[
    {
        "paper": {
            "id": "2509.15221",
            "authors": [
                {
                    "_id": "68ccb2e43df9ac65e93dc5b9",
                    "user": {
                        "_id": "6432c6152bfb2b0ec7572479",
                        "avatarUrl": "/avatars/7c2f908f511cdfa38a19e967d86f9c40.svg",
                        "isPro": false,
                        "fullname": "Zhaoyang Liu",
                        "user": "zyliu",
                        "type": "user"
                    },
                    "name": "Zhaoyang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:49:00.775Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5ba",
                    "user": {
                        "_id": "6502f241b1792803da7e8def",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png",
                        "isPro": false,
                        "fullname": "JingJing Xie",
                        "user": "ownerEli",
                        "type": "user"
                    },
                    "name": "JingJing Xie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:58.643Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5bb",
                    "user": {
                        "_id": "642b9861bb77f8456634b048",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/VrNmmcdgX7FufQmdP5YaG.jpeg",
                        "isPro": false,
                        "fullname": "Zichen Ding",
                        "user": "heroding77",
                        "type": "user"
                    },
                    "name": "Zichen Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:49:03.065Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5bc",
                    "name": "Zehao Li",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5bd",
                    "name": "Bowen Yang",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5be",
                    "name": "Zhenyu Wu",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5bf",
                    "name": "Xuehui Wang",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c0",
                    "user": {
                        "_id": "6064a0eeb1703ddba0d458b9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
                        "isPro": false,
                        "fullname": "Qiushi",
                        "user": "QiushiSun",
                        "type": "user"
                    },
                    "name": "Qiushi Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T10:25:21.623Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c1",
                    "name": "Shi Liu",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c2",
                    "user": {
                        "_id": "619507e7b74b6c591f794340",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
                        "isPro": false,
                        "fullname": "Weiyun Wang",
                        "user": "Weiyun1025",
                        "type": "user"
                    },
                    "name": "Weiyun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:10:25.791Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c3",
                    "user": {
                        "_id": "64804866c7f87934d082bb25",
                        "avatarUrl": "/avatars/41761226c79ac16e48d4c4cb84362adb.svg",
                        "isPro": false,
                        "fullname": "Yeshenglong",
                        "user": "Yeshenglong",
                        "type": "user"
                    },
                    "name": "Shenglong Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:10:17.583Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c4",
                    "name": "Qingyun Li",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c5",
                    "name": "Zeyue Tian",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c6",
                    "name": "Gen Luo",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c7",
                    "user": {
                        "_id": "666a8f24e2990b0cb16b7bf9",
                        "avatarUrl": "/avatars/fcbaf8f1e3e53a2a4a819b7cb2c53aa4.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Yue",
                        "user": "xyyue",
                        "type": "user"
                    },
                    "name": "Xiangyu Yue",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:10:44.945Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c8",
                    "user": {
                        "_id": "645d9c3058f9ee315148116d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645d9c3058f9ee315148116d/uBoAWgrF2Di4WcXVGW9fP.jpeg",
                        "isPro": false,
                        "fullname": "Biqing Qi",
                        "user": "jackqi7",
                        "type": "user"
                    },
                    "name": "Biqing Qi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:10:56.050Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c9",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5ca",
                    "name": "Bowen Zhou",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5cb",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5cc",
                    "name": "Qifeng Chen",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5cd",
                    "name": "Wenhai Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-18T17:59:22.000Z",
            "submittedOnDailyAt": "2025-09-19T00:03:42.415Z",
            "title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  Data",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that\noperate GUIs autonomously, showing great potential, yet progress is limited by\nthe lack of large-scale, open-source computer use data and foundation models.\nIn this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It\noffers a large-scale dataset spanning 6 operating systems and 3 task domains,\nbuilt via a closed-loop pipeline uniting automated agents with human experts.\nTrained on this scaled-up data, ScaleCUA can operate seamlessly across\nplatforms. Specifically, it delivers strong gains over baselines (+26.6 on\nWebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art\nresults (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on\nWebArena-Lite-v2). These findings underscore the power of data-driven scaling\nfor general-purpose computer use agents. We will release data, models, and code\nto advance future research: https://github.com/OpenGVLab/ScaleCUA.",
            "upvotes": 79,
            "discussionId": "68ccb2e43df9ac65e93dc5ce",
            "githubRepo": "https://github.com/OpenGVLab/ScaleCUA",
            "ai_summary": "ScaleCUA, a large-scale dataset and model for computer use agents, achieves state-of-the-art performance across multiple platforms and tasks by leveraging data-driven scaling.",
            "ai_keywords": [
                "Vision-Language Models",
                "computer use agents",
                "GUIs",
                "closed-loop pipeline",
                "automated agents",
                "human experts",
                "WebArena-Lite-v2",
                "ScreenSpot-Pro",
                "MMBench-GUI L1-Hard",
                "OSWorld-G"
            ],
            "githubStars": 82
        },
        "publishedAt": "2025-09-18T13:59:22.000Z",
        "title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  Data",
        "summary": "Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that\noperate GUIs autonomously, showing great potential, yet progress is limited by\nthe lack of large-scale, open-source computer use data and foundation models.\nIn this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It\noffers a large-scale dataset spanning 6 operating systems and 3 task domains,\nbuilt via a closed-loop pipeline uniting automated agents with human experts.\nTrained on this scaled-up data, ScaleCUA can operate seamlessly across\nplatforms. Specifically, it delivers strong gains over baselines (+26.6 on\nWebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art\nresults (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on\nWebArena-Lite-v2). These findings underscore the power of data-driven scaling\nfor general-purpose computer use agents. We will release data, models, and code\nto advance future research: https://github.com/OpenGVLab/ScaleCUA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15221.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 106
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.15207",
            "authors": [
                {
                    "_id": "68ccb7983df9ac65e93dc626",
                    "user": {
                        "_id": "647ffddeb82adfa7cc1a10d9",
                        "avatarUrl": "/avatars/26aa168d6b2068298ebb16584aa52b6c.svg",
                        "isPro": false,
                        "fullname": "zhu",
                        "user": "xuekai",
                        "type": "user"
                    },
                    "name": "Xuekai Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:45.314Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc627",
                    "user": {
                        "_id": "649e6761f9134a06ed1e0cea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg",
                        "isPro": false,
                        "fullname": "Daixuan Cheng",
                        "user": "daixuancheng",
                        "type": "user"
                    },
                    "name": "Daixuan Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:48.340Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc628",
                    "user": {
                        "_id": "64384e18d221ff12edae4c75",
                        "avatarUrl": "/avatars/3a5d2400af0f26091c233d63984df412.svg",
                        "isPro": false,
                        "fullname": "Dinghuai Zhang",
                        "user": "Dinghuai",
                        "type": "user"
                    },
                    "name": "Dinghuai Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:11:24.546Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc629",
                    "name": "Hengli Li",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc62a",
                    "name": "Kaiyan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc62b",
                    "name": "Che Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc62c",
                    "name": "Youbang Sun",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc62d",
                    "name": "Ermo Hua",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc62e",
                    "user": {
                        "_id": "622474f38dc6b0b64f5e903d",
                        "avatarUrl": "/avatars/d6b60a014277a8ec7d564163c5f644aa.svg",
                        "isPro": false,
                        "fullname": "Yuxin Zuo",
                        "user": "yuxinzuo",
                        "type": "user"
                    },
                    "name": "Yuxin Zuo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:11:54.405Z",
                    "hidden": true
                },
                {
                    "_id": "68ccb7983df9ac65e93dc62f",
                    "user": {
                        "_id": "663f07d029be04778ba97871",
                        "avatarUrl": "/avatars/fb7c9d4a2c537d918a3267e7cbc03f04.svg",
                        "isPro": false,
                        "fullname": "Xingtai Lv",
                        "user": "XingtaiHF",
                        "type": "user"
                    },
                    "name": "Xingtai Lv",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:11:46.119Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc630",
                    "user": {
                        "_id": "6663e5b10c54dffcd2a921ca",
                        "avatarUrl": "/avatars/2a4589fef05306ccde06728c752e5601.svg",
                        "isPro": false,
                        "fullname": "Qizheng Zhang",
                        "user": "qizhengz",
                        "type": "user"
                    },
                    "name": "Qizheng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:12:02.816Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc631",
                    "name": "Lin Chen",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc632",
                    "name": "Fanghao Shao",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc633",
                    "name": "Bo Xue",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc634",
                    "name": "Yunchong Song",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc635",
                    "user": {
                        "_id": "65574c70d16b524f1d2345c1",
                        "avatarUrl": "/avatars/8edbe9ebea85c570bc19b63bf3a727d9.svg",
                        "isPro": false,
                        "fullname": "Zhenjie Yang",
                        "user": "jayyoung0802",
                        "type": "user"
                    },
                    "name": "Zhenjie Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:37.248Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc636",
                    "user": {
                        "_id": "650eba9555dc1e841746f132",
                        "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
                        "isPro": false,
                        "fullname": "Ganqu Cui",
                        "user": "ganqu",
                        "type": "user"
                    },
                    "name": "Ganqu Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:12:20.955Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc637",
                    "name": "Ning Ding",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc638",
                    "user": {
                        "_id": "641904caf9d6f1d772ec7af7",
                        "avatarUrl": "/avatars/4a63eac71eb30f70b1a0e9d4708f26c1.svg",
                        "isPro": false,
                        "fullname": "Jianfeng Gao",
                        "user": "wyngjf",
                        "type": "user"
                    },
                    "name": "Jianfeng Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:12:29.442Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc639",
                    "name": "Xiaodong Liu",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc63a",
                    "name": "Bowen Zhou",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc63b",
                    "name": "Hongyuan Mei",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc63c",
                    "name": "Zhouhan Lin",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/wtc3vGNJQlj3FdOsBXwp3.png"
            ],
            "publishedAt": "2025-09-18T17:56:36.000Z",
            "submittedOnDailyAt": "2025-09-19T00:24:38.079Z",
            "title": "FlowRL: Matching Reward Distributions for LLM Reasoning",
            "submittedOnDailyBy": {
                "_id": "649e6761f9134a06ed1e0cea",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg",
                "isPro": false,
                "fullname": "Daixuan Cheng",
                "user": "daixuancheng",
                "type": "user"
            },
            "summary": "We propose FlowRL: matching the full reward distribution via flow balancing\ninstead of maximizing rewards in large language model (LLM) reinforcement\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\ncontrast, we transform scalar rewards into a normalized target distribution\nusing a learnable partition function, and then minimize the reverse KL\ndivergence between the policy and the target distribution. We implement this\nidea as a flow-balanced optimization method that promotes diverse exploration\nand generalizable reasoning trajectories. We conduct experiments on math and\ncode reasoning tasks: FlowRL achieves a significant average improvement of\n10.0% over GRPO and 5.1% over PPO on math benchmarks, and performs\nconsistently better on code reasoning tasks. These results highlight reward\ndistribution-matching as a key step toward efficient exploration and diverse\nreasoning in LLM reinforcement learning.",
            "upvotes": 68,
            "discussionId": "68ccb7983df9ac65e93dc63d",
            "githubRepo": "https://github.com/Xuekai-Zhu/FlowRL",
            "ai_summary": "FlowRL enhances LLM reinforcement learning by matching the full reward distribution through flow balancing, improving diversity and performance over reward-maximizing methods.",
            "ai_keywords": [
                "FlowRL",
                "reward distribution",
                "flow balancing",
                "reinforcement learning",
                "reward-maximizing methods",
                "PPO",
                "GRPO",
                "normalized target distribution",
                "learnable partition function",
                "reverse KL divergence",
                "diverse exploration",
                "generalizable reasoning trajectories",
                "math reasoning",
                "code reasoning"
            ],
            "githubStars": 26
        },
        "publishedAt": "2025-09-18T13:56:36.000Z",
        "title": "FlowRL: Matching Reward Distributions for LLM Reasoning",
        "summary": "We propose FlowRL: matching the full reward distribution via flow balancing\ninstead of maximizing rewards in large language model (LLM) reinforcement\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\ncontrast, we transform scalar rewards into a normalized target distribution\nusing a learnable partition function, and then minimize the reverse KL\ndivergence between the policy and the target distribution. We implement this\nidea as a flow-balanced optimization method that promotes diverse exploration\nand generalizable reasoning trajectories. We conduct experiments on math and\ncode reasoning tasks: FlowRL achieves a significant average improvement of\n10.0% over GRPO and 5.1% over PPO on math benchmarks, and performs\nconsistently better on code reasoning tasks. These results highlight reward\ndistribution-matching as a key step toward efficient exploration and diverse\nreasoning in LLM reinforcement learning.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/wtc3vGNJQlj3FdOsBXwp3.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15207.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649e6761f9134a06ed1e0cea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg",
            "fullname": "Daixuan Cheng",
            "name": "daixuancheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.14760",
            "authors": [
                {
                    "_id": "68ccc6a13df9ac65e93dc66a",
                    "user": {
                        "_id": "689ec537196ab997b13dc977",
                        "avatarUrl": "/avatars/28e0ba07ca26056833e1197a6aa0d14f.svg",
                        "isPro": false,
                        "fullname": "Haoran Zhang",
                        "user": "zzzhr97",
                        "type": "user"
                    },
                    "name": "Haoran Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:23.170Z",
                    "hidden": false
                },
                {
                    "_id": "68ccc6a13df9ac65e93dc66b",
                    "user": {
                        "_id": "63f3502a520c14618925825a",
                        "avatarUrl": "/avatars/e986a2a6625e7be6890616a417f908d2.svg",
                        "isPro": false,
                        "fullname": "Yafu Li",
                        "user": "yaful",
                        "type": "user"
                    },
                    "name": "Yafu Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:12:52.499Z",
                    "hidden": false
                },
                {
                    "_id": "68ccc6a13df9ac65e93dc66c",
                    "user": {
                        "_id": "6498fde776d49ee00f79cbfe",
                        "avatarUrl": "/avatars/4c284a71080150e6cb3b9632dfccef60.svg",
                        "isPro": false,
                        "fullname": "Xuyang Hu",
                        "user": "huxy912",
                        "type": "user"
                    },
                    "name": "Xuyang Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:13:07.959Z",
                    "hidden": false
                },
                {
                    "_id": "68ccc6a13df9ac65e93dc66d",
                    "user": {
                        "_id": "657fe7a8504da7f6f30a2832",
                        "avatarUrl": "/avatars/65987e3cba449b5d250616510ee11f33.svg",
                        "isPro": false,
                        "fullname": "Dongrui Liu",
                        "user": "Max9803",
                        "type": "user"
                    },
                    "name": "Dongrui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:13:15.250Z",
                    "hidden": false
                },
                {
                    "_id": "68ccc6a13df9ac65e93dc66e",
                    "name": "Zhilin Wang",
                    "hidden": false
                },
                {
                    "_id": "68ccc6a13df9ac65e93dc66f",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "68ccc6a13df9ac65e93dc670",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-18T09:08:53.000Z",
            "submittedOnDailyAt": "2025-09-19T02:48:26.705Z",
            "title": "Reasoning over Boundaries: Enhancing Specification Alignment via\n  Test-time Delibration",
            "submittedOnDailyBy": {
                "_id": "63f3502a520c14618925825a",
                "avatarUrl": "/avatars/e986a2a6625e7be6890616a417f908d2.svg",
                "isPro": false,
                "fullname": "Yafu Li",
                "user": "yaful",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are increasingly applied in diverse real-world\nscenarios, each governed by bespoke behavioral and safety specifications (spec)\ncustom-tailored by users or organizations. These spec, categorized into\nsafety-spec and behavioral-spec, vary across scenarios and evolve with changing\npreferences and requirements. We formalize this challenge as specification\nalignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec\nfrom both behavioral and safety perspectives. To address this challenge, we\npropose Align3, a lightweight method that employs Test-Time Deliberation (TTD)\nwith hierarchical reflection and revision to reason over the specification\nboundaries. We further present SpecBench, a unified benchmark for measuring\nspecification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts.\nExperiments on 15 reasoning and 18 instruct models with several TTD methods,\nincluding Self-Refine, TPO, and MoreThink, yield three key findings: (i)\ntest-time deliberation enhances specification alignment; (ii) Align3 advances\nthe safety-helpfulness trade-off frontier with minimal overhead; (iii)\nSpecBench effectively reveals alignment gaps. These results highlight the\npotential of test-time deliberation as an effective strategy for reasoning over\nthe real-world specification boundaries.",
            "upvotes": 44,
            "discussionId": "68ccc6a13df9ac65e93dc671",
            "githubRepo": "https://github.com/zzzhr97/SpecBench",
            "ai_summary": "Align3, a lightweight method using Test-Time Deliberation, enhances specification alignment in large language models across diverse scenarios with minimal overhead.",
            "ai_keywords": [
                "Large language models",
                "specification alignment",
                "Test-Time Deliberation",
                "hierarchical reflection",
                "revision",
                "SpecBench",
                "safety-helpfulness trade-off",
                "Self-Refine",
                "TPO",
                "MoreThink"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-09-18T05:08:53.000Z",
        "title": "Reasoning over Boundaries: Enhancing Specification Alignment via\n  Test-time Delibration",
        "summary": "Large language models (LLMs) are increasingly applied in diverse real-world\nscenarios, each governed by bespoke behavioral and safety specifications (spec)\ncustom-tailored by users or organizations. These spec, categorized into\nsafety-spec and behavioral-spec, vary across scenarios and evolve with changing\npreferences and requirements. We formalize this challenge as specification\nalignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec\nfrom both behavioral and safety perspectives. To address this challenge, we\npropose Align3, a lightweight method that employs Test-Time Deliberation (TTD)\nwith hierarchical reflection and revision to reason over the specification\nboundaries. We further present SpecBench, a unified benchmark for measuring\nspecification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts.\nExperiments on 15 reasoning and 18 instruct models with several TTD methods,\nincluding Self-Refine, TPO, and MoreThink, yield three key findings: (i)\ntest-time deliberation enhances specification alignment; (ii) Align3 advances\nthe safety-helpfulness trade-off frontier with minimal overhead; (iii)\nSpecBench effectively reveals alignment gaps. These results highlight the\npotential of test-time deliberation as an effective strategy for reasoning over\nthe real-world specification boundaries.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14760.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63f3502a520c14618925825a",
            "avatarUrl": "/avatars/e986a2a6625e7be6890616a417f908d2.svg",
            "fullname": "Yafu Li",
            "name": "yaful",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.15194",
            "authors": [
                {
                    "_id": "68ccb4383df9ac65e93dc5d0",
                    "user": {
                        "_id": "65a05abf07184d32fa002d41",
                        "avatarUrl": "/avatars/3a23e7e568d2024381ed31b56c1c461a.svg",
                        "isPro": false,
                        "fullname": "Yujun Zhou",
                        "user": "yujunzhou",
                        "type": "user"
                    },
                    "name": "Yujun Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:53.689Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d1",
                    "user": {
                        "_id": "62ffa3f8311cad266f9af236",
                        "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
                        "isPro": false,
                        "fullname": "Zhenwen Liang",
                        "user": "invokerliang",
                        "type": "user"
                    },
                    "name": "Zhenwen Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:56.281Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d2",
                    "name": "Haolin Liu",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d3",
                    "name": "Wenhao Yu",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d4",
                    "name": "Kishan Panaganti",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d5",
                    "name": "Linfeng Song",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d6",
                    "name": "Dian Yu",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d7",
                    "name": "Xiangliang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d8",
                    "name": "Haitao Mi",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d9",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-18T17:50:04.000Z",
            "submittedOnDailyAt": "2025-09-19T00:09:24.304Z",
            "title": "Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation",
            "submittedOnDailyBy": {
                "_id": "5feab3a28a3201f8e554c969",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660795228685-5feab3a28a3201f8e554c969.png",
                "isPro": false,
                "fullname": "Wenhao Yu",
                "user": "wyu1",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nlabel-free methods, confidence minimization, self-consistency, or majority-vote\nobjectives, stabilize learning but steadily shrink exploration, causing an\nentropy collapse: generations become shorter, less diverse, and brittle. Unlike\nprior approaches such as Test-Time Reinforcement Learning (TTRL), which\nprimarily adapt models to the immediate unlabeled dataset at hand, our goal is\nbroader: to enable general improvements without sacrificing the model's\ninherent exploration capacity and generalization ability, i.e., evolving. We\nformalize this issue and propose EVolution-Oriented and Label-free\nReinforcement Learning (EVOL-RL), a simple rule that couples stability with\nvariation under a label-free setting. EVOL-RL keeps the majority-voted answer\nas a stable anchor (selection) while adding a novelty-aware reward that favors\nresponses whose reasoning differs from what has already been produced\n(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also\nuses asymmetric clipping to preserve strong signals and an entropy regularizer\nto sustain search. This majority-for-selection + novelty-for-variation design\nprevents collapse, maintains longer and more informative chains of thought, and\nimproves both pass@1 and pass@n. EVOL-RL consistently outperforms the\nmajority-only TTRL baseline; e.g., training on label-free AIME24 lifts\nQwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%\nto 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks\nstronger generalization across domains (e.g., GPQA). Furthermore, we\ndemonstrate that EVOL-RL also boosts performance in the RLVR setting,\nhighlighting its broad applicability.",
            "upvotes": 30,
            "discussionId": "68ccb4383df9ac65e93dc5da",
            "githubRepo": "https://github.com/YujunZhou/EVOL-RL",
            "ai_summary": "EVOL-RL, a label-free reinforcement learning method, enhances large language models by balancing stability and variation, preventing entropy collapse and improving generalization.",
            "ai_keywords": [
                "reinforcement learning from verifiable rewards",
                "RLVR",
                "label-free methods",
                "confidence minimization",
                "self-consistency",
                "majority-vote objectives",
                "entropy collapse",
                "Test-Time Reinforcement Learning",
                "TTRL",
                "EVolution-Oriented and Label-free Reinforcement Learning",
                "EVOL-RL",
                "GRPO",
                "asymmetric clipping",
                "entropy regularizer",
                "pass@1",
                "pass@n",
                "AIME24",
                "Qwen3-4B-Base",
                "AIME25",
                "GPQA"
            ],
            "githubStars": 12
        },
        "publishedAt": "2025-09-18T13:50:04.000Z",
        "title": "Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation",
        "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nlabel-free methods, confidence minimization, self-consistency, or majority-vote\nobjectives, stabilize learning but steadily shrink exploration, causing an\nentropy collapse: generations become shorter, less diverse, and brittle. Unlike\nprior approaches such as Test-Time Reinforcement Learning (TTRL), which\nprimarily adapt models to the immediate unlabeled dataset at hand, our goal is\nbroader: to enable general improvements without sacrificing the model's\ninherent exploration capacity and generalization ability, i.e., evolving. We\nformalize this issue and propose EVolution-Oriented and Label-free\nReinforcement Learning (EVOL-RL), a simple rule that couples stability with\nvariation under a label-free setting. EVOL-RL keeps the majority-voted answer\nas a stable anchor (selection) while adding a novelty-aware reward that favors\nresponses whose reasoning differs from what has already been produced\n(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also\nuses asymmetric clipping to preserve strong signals and an entropy regularizer\nto sustain search. This majority-for-selection + novelty-for-variation design\nprevents collapse, maintains longer and more informative chains of thought, and\nimproves both pass@1 and pass@n. EVOL-RL consistently outperforms the\nmajority-only TTRL baseline; e.g., training on label-free AIME24 lifts\nQwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%\nto 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks\nstronger generalization across domains (e.g., GPQA). Furthermore, we\ndemonstrate that EVOL-RL also boosts performance in the RLVR setting,\nhighlighting its broad applicability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15194.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5feab3a28a3201f8e554c969",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660795228685-5feab3a28a3201f8e554c969.png",
            "fullname": "Wenhao Yu",
            "name": "wyu1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.15185",
            "authors": [
                {
                    "_id": "68ccc7243df9ac65e93dc679",
                    "user": {
                        "_id": "6662a450b1fff5575fdf0fbd",
                        "avatarUrl": "/avatars/2a6065269f1980213625a9cfd8d42fbd.svg",
                        "isPro": false,
                        "fullname": "Xiaoyu Yue",
                        "user": "YueXY233",
                        "type": "user"
                    },
                    "name": "Xiaoyu Yue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:19.187Z",
                    "hidden": false
                },
                {
                    "_id": "68ccc7243df9ac65e93dc67a",
                    "user": {
                        "_id": "64b7aa374df206a3ed1947d2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7aa374df206a3ed1947d2/Ostk72ehOR6yUX-PhUvyQ.jpeg",
                        "isPro": false,
                        "fullname": "wzd",
                        "user": "GoodEnough",
                        "type": "user"
                    },
                    "name": "Zidong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T07:00:14.082Z",
                    "hidden": false
                },
                {
                    "_id": "68ccc7243df9ac65e93dc67b",
                    "name": "Yuqing Wang",
                    "hidden": false
                },
                {
                    "_id": "68ccc7243df9ac65e93dc67c",
                    "name": "Wenlong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ccc7243df9ac65e93dc67d",
                    "name": "Xihui Liu",
                    "hidden": false
                },
                {
                    "_id": "68ccc7243df9ac65e93dc67e",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68ccc7243df9ac65e93dc67f",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "68ccc7243df9ac65e93dc680",
                    "name": "Luping Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-18T17:47:40.000Z",
            "submittedOnDailyAt": "2025-09-19T01:31:06.584Z",
            "title": "Understand Before You Generate: Self-Guided Training for Autoregressive\n  Image Generation",
            "submittedOnDailyBy": {
                "_id": "6662a450b1fff5575fdf0fbd",
                "avatarUrl": "/avatars/2a6065269f1980213625a9cfd8d42fbd.svg",
                "isPro": false,
                "fullname": "Xiaoyu Yue",
                "user": "YueXY233",
                "type": "user"
            },
            "summary": "Recent studies have demonstrated the importance of high-quality visual\nrepresentations in image generation and have highlighted the limitations of\ngenerative models in image understanding. As a generative paradigm originally\ndesigned for natural language, autoregressive models face similar challenges.\nIn this work, we present the first systematic investigation into the mechanisms\nof applying the next-token prediction paradigm to the visual domain. We\nidentify three key properties that hinder the learning of high-level visual\nsemantics: local and conditional dependence, inter-step semantic inconsistency,\nand spatial invariance deficiency. We show that these issues can be effectively\naddressed by introducing self-supervised objectives during training, leading to\na novel training framework, Self-guided Training for AutoRegressive models\n(ST-AR). Without relying on pre-trained representation models, ST-AR\nsignificantly enhances the image understanding ability of autoregressive models\nand leads to improved generation quality. Specifically, ST-AR brings\napproximately 42% FID improvement for LlamaGen-L and 49% FID improvement for\nLlamaGen-XL, while maintaining the same sampling strategy.",
            "upvotes": 22,
            "discussionId": "68ccc7253df9ac65e93dc681",
            "ai_summary": "Self-guided Training for AutoRegressive models (ST-AR) enhances image understanding and generation quality in autoregressive models by addressing key visual semantics challenges through self-supervised objectives.",
            "ai_keywords": [
                "autoregressive models",
                "next-token prediction",
                "high-level visual semantics",
                "local and conditional dependence",
                "inter-step semantic inconsistency",
                "spatial invariance deficiency",
                "self-supervised objectives",
                "Self-guided Training for AutoRegressive models",
                "ST-AR",
                "FID improvement",
                "LlamaGen-L",
                "LlamaGen-XL"
            ]
        },
        "publishedAt": "2025-09-18T13:47:40.000Z",
        "title": "Understand Before You Generate: Self-Guided Training for Autoregressive\n  Image Generation",
        "summary": "Recent studies have demonstrated the importance of high-quality visual\nrepresentations in image generation and have highlighted the limitations of\ngenerative models in image understanding. As a generative paradigm originally\ndesigned for natural language, autoregressive models face similar challenges.\nIn this work, we present the first systematic investigation into the mechanisms\nof applying the next-token prediction paradigm to the visual domain. We\nidentify three key properties that hinder the learning of high-level visual\nsemantics: local and conditional dependence, inter-step semantic inconsistency,\nand spatial invariance deficiency. We show that these issues can be effectively\naddressed by introducing self-supervised objectives during training, leading to\na novel training framework, Self-guided Training for AutoRegressive models\n(ST-AR). Without relying on pre-trained representation models, ST-AR\nsignificantly enhances the image understanding ability of autoregressive models\nand leads to improved generation quality. Specifically, ST-AR brings\napproximately 42% FID improvement for LlamaGen-L and 49% FID improvement for\nLlamaGen-XL, while maintaining the same sampling strategy.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15185.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6662a450b1fff5575fdf0fbd",
            "avatarUrl": "/avatars/2a6065269f1980213625a9cfd8d42fbd.svg",
            "fullname": "Xiaoyu Yue",
            "name": "YueXY233",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.13160",
            "authors": [
                {
                    "_id": "68ccb6e23df9ac65e93dc60d",
                    "name": "Liang Hu",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc60e",
                    "name": "Jianpeng Jiao",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc60f",
                    "name": "Jiashuo Liu",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc610",
                    "name": "Yanle Ren",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc611",
                    "name": "Zhoufutu Wen",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc612",
                    "name": "Kaiyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc613",
                    "name": "Xuanliang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc614",
                    "name": "Xiang Gao",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc615",
                    "name": "Tianci He",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc616",
                    "name": "Fei Hu",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc617",
                    "name": "Yali Liao",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc618",
                    "name": "Zaiyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc619",
                    "name": "Chenghao Yang",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc61a",
                    "name": "Qianyu Yang",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc61b",
                    "name": "Mingren Yin",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc61c",
                    "name": "Zhiyuan Zeng",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc61d",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:45:59.923Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc61e",
                    "name": "Xinyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc61f",
                    "name": "Xiying Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc620",
                    "name": "Zhenwei Zhu",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc621",
                    "name": "Hongseok Namkoong",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc622",
                    "user": {
                        "_id": "686c983f8c812deb40e83270",
                        "avatarUrl": "/avatars/7d1e4a38929b7ac79004463fd0060e9b.svg",
                        "isPro": false,
                        "fullname": "Wenhao Huang",
                        "user": "WenhaoHuang",
                        "type": "user"
                    },
                    "name": "Wenhao Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:46:13.644Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb6e23df9ac65e93dc623",
                    "name": "Yuwen Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-16T15:13:13.000Z",
            "submittedOnDailyAt": "2025-09-19T00:20:55.990Z",
            "title": "FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial\n  Search and Reasoning",
            "submittedOnDailyBy": {
                "_id": "638efcf4c67af472d316d424",
                "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                "isPro": false,
                "fullname": "Ge Zhang",
                "user": "zhangysk",
                "type": "user"
            },
            "summary": "Search has emerged as core infrastructure for LLM-based agents and is widely\nviewed as critical on the path toward more general intelligence. Finance is a\nparticularly demanding proving ground: analysts routinely conduct complex,\nmulti-step searches over time-sensitive, domain-specific data, making it ideal\nfor assessing both search proficiency and knowledge-grounded reasoning. Yet no\nexisting open financial datasets evaluate data searching capability of\nend-to-end agents, largely because constructing realistic, complicated tasks\nrequires deep financial expertise and time-sensitive data is hard to evaluate.\nWe present FinSearchComp, the first fully open-source agent benchmark for\nrealistic, open-domain financial search and reasoning. FinSearchComp comprises\nthree tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and\nComplex Historical Investigation -- closely reproduce real-world financial\nanalyst workflows. To ensure difficulty and reliability, we engage 70\nprofessional financial experts for annotation and implement a rigorous\nmulti-stage quality-assurance pipeline. The benchmark includes 635 questions\nspanning global and Greater China markets, and we evaluate 21 models (products)\non it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.\nDouBao (web) leads on the Greater China subset. Experimental analyses show that\nequipping agents with web search and financial plugins substantially improves\nresults on FinSearchComp, and the country origin of models and tools impact\nperformance significantly.By aligning with realistic analyst tasks and\nproviding end-to-end evaluation, FinSearchComp offers a professional,\nhigh-difficulty testbed for complex financial search and reasoning.",
            "upvotes": 22,
            "discussionId": "68ccb6e23df9ac65e93dc624",
            "ai_summary": "FinSearchComp is an open-source benchmark for evaluating financial search and reasoning capabilities of end-to-end agents, featuring realistic tasks and professional annotations.",
            "ai_keywords": [
                "LLM-based agents",
                "search proficiency",
                "knowledge-grounded reasoning",
                "FinSearchComp",
                "Time-Sensitive Data Fetching",
                "Simple Historical Lookup",
                "Complex Historical Investigation",
                "web search",
                "financial plugins"
            ]
        },
        "publishedAt": "2025-09-16T11:13:13.000Z",
        "title": "FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial\n  Search and Reasoning",
        "summary": "Search has emerged as core infrastructure for LLM-based agents and is widely\nviewed as critical on the path toward more general intelligence. Finance is a\nparticularly demanding proving ground: analysts routinely conduct complex,\nmulti-step searches over time-sensitive, domain-specific data, making it ideal\nfor assessing both search proficiency and knowledge-grounded reasoning. Yet no\nexisting open financial datasets evaluate data searching capability of\nend-to-end agents, largely because constructing realistic, complicated tasks\nrequires deep financial expertise and time-sensitive data is hard to evaluate.\nWe present FinSearchComp, the first fully open-source agent benchmark for\nrealistic, open-domain financial search and reasoning. FinSearchComp comprises\nthree tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and\nComplex Historical Investigation -- closely reproduce real-world financial\nanalyst workflows. To ensure difficulty and reliability, we engage 70\nprofessional financial experts for annotation and implement a rigorous\nmulti-stage quality-assurance pipeline. The benchmark includes 635 questions\nspanning global and Greater China markets, and we evaluate 21 models (products)\non it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.\nDouBao (web) leads on the Greater China subset. Experimental analyses show that\nequipping agents with web search and financial plugins substantially improves\nresults on FinSearchComp, and the country origin of models and tools impact\nperformance significantly.By aligning with realistic analyst tasks and\nproviding end-to-end evaluation, FinSearchComp offers a professional,\nhigh-difficulty testbed for complex financial search and reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13160.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "fullname": "Ge Zhang",
            "name": "zhangysk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 67
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.15212",
            "authors": [
                {
                    "_id": "68ccb5443df9ac65e93dc5e8",
                    "user": {
                        "_id": "629c95b7a5d6f5fe10e6ed45",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629c95b7a5d6f5fe10e6ed45/Sy0Ype5snsRookID-gsSm.jpeg",
                        "isPro": false,
                        "fullname": "Yuming Jiang",
                        "user": "yumingj",
                        "type": "user"
                    },
                    "name": "Yuming Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:43:28.955Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb5443df9ac65e93dc5e9",
                    "user": {
                        "_id": "65fd82762bf2cd20ddaa193f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
                        "isPro": false,
                        "fullname": "Siteng Huang",
                        "user": "huangsiteng",
                        "type": "user"
                    },
                    "name": "Siteng Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:43:37.080Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb5443df9ac65e93dc5ea",
                    "name": "Shengke Xue",
                    "hidden": false
                },
                {
                    "_id": "68ccb5443df9ac65e93dc5eb",
                    "user": {
                        "_id": "676124d3b8b1b60a0e5c2d8e",
                        "avatarUrl": "/avatars/350e7b9fb6f171cea7dafa39d3890cb5.svg",
                        "isPro": false,
                        "fullname": "Yaxi Zhao",
                        "user": "yaniii",
                        "type": "user"
                    },
                    "name": "Yaxi Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:44:49.332Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb5443df9ac65e93dc5ec",
                    "name": "Jun Cen",
                    "hidden": false
                },
                {
                    "_id": "68ccb5443df9ac65e93dc5ed",
                    "user": {
                        "_id": "609115c79a8bcaa437b234a9",
                        "avatarUrl": "/avatars/1631a91030703d8397133363cf82c863.svg",
                        "isPro": true,
                        "fullname": "Leng Sicong",
                        "user": "Sicong",
                        "type": "user"
                    },
                    "name": "Sicong Leng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:45:11.063Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb5443df9ac65e93dc5ee",
                    "name": "Kehan Li",
                    "hidden": false
                },
                {
                    "_id": "68ccb5443df9ac65e93dc5ef",
                    "user": {
                        "_id": "66224557c61c7fbd98099079",
                        "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
                        "isPro": false,
                        "fullname": "Jiayan Guo",
                        "user": "SpaceProduct",
                        "type": "user"
                    },
                    "name": "Jiayan Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:45:19.380Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb5443df9ac65e93dc5f0",
                    "name": "Kexiang Wang",
                    "hidden": false
                },
                {
                    "_id": "68ccb5443df9ac65e93dc5f1",
                    "name": "Mingxiu Chen",
                    "hidden": false
                },
                {
                    "_id": "68ccb5443df9ac65e93dc5f2",
                    "name": "Fan Wang",
                    "hidden": false
                },
                {
                    "_id": "68ccb5443df9ac65e93dc5f3",
                    "name": "Deli Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ccb5443df9ac65e93dc5f4",
                    "name": "Xin Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-18T17:58:02.000Z",
            "submittedOnDailyAt": "2025-09-19T00:13:55.962Z",
            "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "This paper presents RynnVLA-001, a vision-language-action(VLA) model built\nupon large-scale video generative pretraining from human demonstrations. We\npropose a novel two-stage pretraining methodology. The first stage, Ego-Centric\nVideo Generative Pretraining, trains an Image-to-Video model on 12M ego-centric\nmanipulation videos to predict future frames conditioned on an initial frame\nand a language instruction. The second stage, Human-Centric Trajectory-Aware\nModeling, extends this by jointly predicting future keypoint trajectories,\nthereby effectively bridging visual frame prediction with action prediction.\nFurthermore, to enhance action representation, we propose ActionVAE, a\nvariational autoencoder that compresses sequences of actions into compact\nlatent embeddings, reducing the complexity of the VLA output space. When\nfinetuned on the same downstream robotics datasets, RynnVLA-001 achieves\nsuperior performance over state-of-the-art baselines, demonstrating that the\nproposed pretraining strategy provides a more effective initialization for VLA\nmodels.",
            "upvotes": 12,
            "discussionId": "68ccb5453df9ac65e93dc5f5",
            "githubRepo": "https://github.com/alibaba-damo-academy/RynnVLA-001",
            "ai_summary": "RynnVLA-001, a vision-language-action model, uses a two-stage pretraining approach and ActionVAE to achieve superior performance on robotics tasks.",
            "ai_keywords": [
                "Ego-Centric Video Generative Pretraining",
                "Human-Centric Trajectory-Aware Modeling",
                "Image-to-Video model",
                "keypoint trajectories",
                "ActionVAE",
                "variational autoencoder",
                "latent embeddings",
                "VLA model"
            ],
            "githubStars": 175
        },
        "publishedAt": "2025-09-18T13:58:02.000Z",
        "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation",
        "summary": "This paper presents RynnVLA-001, a vision-language-action(VLA) model built\nupon large-scale video generative pretraining from human demonstrations. We\npropose a novel two-stage pretraining methodology. The first stage, Ego-Centric\nVideo Generative Pretraining, trains an Image-to-Video model on 12M ego-centric\nmanipulation videos to predict future frames conditioned on an initial frame\nand a language instruction. The second stage, Human-Centric Trajectory-Aware\nModeling, extends this by jointly predicting future keypoint trajectories,\nthereby effectively bridging visual frame prediction with action prediction.\nFurthermore, to enhance action representation, we propose ActionVAE, a\nvariational autoencoder that compresses sequences of actions into compact\nlatent embeddings, reducing the complexity of the VLA output space. When\nfinetuned on the same downstream robotics datasets, RynnVLA-001 achieves\nsuperior performance over state-of-the-art baselines, demonstrating that the\nproposed pretraining strategy provides a more effective initialization for VLA\nmodels.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15212.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 106
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.14476",
            "authors": [
                {
                    "_id": "68ccb6a63df9ac65e93dc603",
                    "user": {
                        "_id": "62b6b0397523238923221df9",
                        "avatarUrl": "/avatars/77068771dd51df7519516cd502a88789.svg",
                        "isPro": false,
                        "fullname": "Jiasenlu",
                        "user": "Jiasenlu",
                        "type": "user"
                    },
                    "name": "Jiasen Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:41:09.966Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb6a63df9ac65e93dc604",
                    "name": "Liangchen Song",
                    "hidden": false
                },
                {
                    "_id": "68ccb6a63df9ac65e93dc605",
                    "name": "Mingze Xu",
                    "hidden": false
                },
                {
                    "_id": "68ccb6a63df9ac65e93dc606",
                    "user": {
                        "_id": "673c383c0a4d127a7f6482d5",
                        "avatarUrl": "/avatars/5f875477a721204c5e3d67c391dfcea4.svg",
                        "isPro": false,
                        "fullname": "Byeongjoo Ahn",
                        "user": "byeongjooahn",
                        "type": "user"
                    },
                    "name": "Byeongjoo Ahn",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:41:32.933Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb6a63df9ac65e93dc607",
                    "name": "Yanjun Wang",
                    "hidden": false
                },
                {
                    "_id": "68ccb6a63df9ac65e93dc608",
                    "name": "Chen Chen",
                    "hidden": false
                },
                {
                    "_id": "68ccb6a63df9ac65e93dc609",
                    "user": {
                        "_id": "684573fcf072c657cac88078",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/OFfIBultMLWfVY9UPlBVK.png",
                        "isPro": false,
                        "fullname": "afshin dehghan",
                        "user": "afshin525",
                        "type": "user"
                    },
                    "name": "Afshin Dehghan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:42:19.278Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb6a63df9ac65e93dc60a",
                    "name": "Yinfei Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-17T23:11:18.000Z",
            "submittedOnDailyAt": "2025-09-19T00:19:42.928Z",
            "title": "AToken: A Unified Tokenizer for Vision",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present AToken, the first unified visual tokenizer that achieves both\nhigh-fidelity reconstruction and semantic understanding across images, videos,\nand 3D assets. Unlike existing tokenizers that specialize in either\nreconstruction or understanding for single modalities, AToken encodes these\ndiverse visual inputs into a shared 4D latent space, unifying both tasks and\nmodalities in a single framework. Specifically, we introduce a pure transformer\narchitecture with 4D rotary position embeddings to process visual inputs of\narbitrary resolutions and temporal durations. To ensure stable training, we\nintroduce an adversarial-free training objective that combines perceptual and\nGram matrix losses, achieving state-of-the-art reconstruction quality. By\nemploying a progressive training curriculum, AToken gradually expands from\nsingle images, videos, and 3D, and supports both continuous and discrete latent\ntokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01\nrFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9%\nclassification accuracy for 3D. In downstream applications, AToken enables both\nvisual generation tasks (e.g., image generation with continuous and discrete\ntokens, text-to-video generation, image-to-3D synthesis) and understanding\ntasks (e.g., multimodal LLMs), achieving competitive performance across all\nbenchmarks. These results shed light on the next-generation multimodal AI\nsystems built upon unified visual tokenization.",
            "upvotes": 11,
            "discussionId": "68ccb6a63df9ac65e93dc60b",
            "ai_summary": "AToken, a unified visual tokenizer, achieves high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets using a 4D transformer architecture with adversarial-free training.",
            "ai_keywords": [
                "unified visual tokenizer",
                "4D latent space",
                "pure transformer architecture",
                "4D rotary position embeddings",
                "adversarial-free training objective",
                "perceptual loss",
                "Gram matrix loss",
                "progressive training curriculum",
                "continuous latent tokens",
                "discrete latent tokens",
                "rFID",
                "ImageNet accuracy",
                "rFVD",
                "MSRVTT retrieval",
                "PSNR",
                "classification accuracy",
                "visual generation tasks",
                "text-to-video generation",
                "image-to-3D synthesis",
                "multimodal LLMs"
            ]
        },
        "publishedAt": "2025-09-17T19:11:18.000Z",
        "title": "AToken: A Unified Tokenizer for Vision",
        "summary": "We present AToken, the first unified visual tokenizer that achieves both\nhigh-fidelity reconstruction and semantic understanding across images, videos,\nand 3D assets. Unlike existing tokenizers that specialize in either\nreconstruction or understanding for single modalities, AToken encodes these\ndiverse visual inputs into a shared 4D latent space, unifying both tasks and\nmodalities in a single framework. Specifically, we introduce a pure transformer\narchitecture with 4D rotary position embeddings to process visual inputs of\narbitrary resolutions and temporal durations. To ensure stable training, we\nintroduce an adversarial-free training objective that combines perceptual and\nGram matrix losses, achieving state-of-the-art reconstruction quality. By\nemploying a progressive training curriculum, AToken gradually expands from\nsingle images, videos, and 3D, and supports both continuous and discrete latent\ntokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01\nrFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9%\nclassification accuracy for 3D. In downstream applications, AToken enables both\nvisual generation tasks (e.g., image generation with continuous and discrete\ntokens, text-to-video generation, image-to-3D synthesis) and understanding\ntasks (e.g., multimodal LLMs), achieving competitive performance across all\nbenchmarks. These results shed light on the next-generation multimodal AI\nsystems built upon unified visual tokenization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14476.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 106
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.15130",
            "authors": [
                {
                    "_id": "68ccb5de3df9ac65e93dc5fc",
                    "user": {
                        "_id": "65cf975bf5a15aa42146b208",
                        "avatarUrl": "/avatars/d62e3907331a24bb899b7a1fdf4299e7.svg",
                        "isPro": false,
                        "fullname": "Song",
                        "user": "ChenxiSong",
                        "type": "user"
                    },
                    "name": "Chenxi Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:40:23.503Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb5de3df9ac65e93dc5fd",
                    "user": {
                        "_id": "66431f02f7c5de57fc858e20",
                        "avatarUrl": "/avatars/bc5f52b2f66cb4ee751d5d676c5b8777.svg",
                        "isPro": false,
                        "fullname": "yanming Yang",
                        "user": "2hiTee",
                        "type": "user"
                    },
                    "name": "Yanming Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:40:32.159Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb5de3df9ac65e93dc5fe",
                    "name": "Tong Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ccb5de3df9ac65e93dc5ff",
                    "name": "Ruibo Li",
                    "hidden": false
                },
                {
                    "_id": "68ccb5de3df9ac65e93dc600",
                    "user": {
                        "_id": "62f058cb0acc344c17163141",
                        "avatarUrl": "/avatars/6368f20bd02a468b3443e7c65ffb9ec7.svg",
                        "isPro": false,
                        "fullname": "Chi Zhang",
                        "user": "ChiZhang",
                        "type": "user"
                    },
                    "name": "Chi Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:40:46.830Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-18T16:40:47.000Z",
            "submittedOnDailyAt": "2025-09-19T00:16:15.925Z",
            "title": "WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model\n  via Training-Free Guidance",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent video diffusion models demonstrate strong potential in spatial\nintelligence tasks due to their rich latent world priors. However, this\npotential is hindered by their limited controllability and geometric\ninconsistency, creating a gap between their strong priors and their practical\nuse in 3D/4D tasks. As a result, current approaches often rely on retraining or\nfine-tuning, which risks degrading pretrained knowledge and incurs high\ncomputational costs. To address this, we propose WorldForge, a training-free,\ninference-time framework composed of three tightly coupled modules. Intra-Step\nRecursive Refinement introduces a recursive refinement mechanism during\ninference, which repeatedly optimizes network predictions within each denoising\nstep to enable precise trajectory injection. Flow-Gated Latent Fusion leverages\noptical flow similarity to decouple motion from appearance in the latent space\nand selectively inject trajectory guidance into motion-related channels.\nDual-Path Self-Corrective Guidance compares guided and unguided denoising paths\nto adaptively correct trajectory drift caused by noisy or misaligned structural\nsignals. Together, these components inject fine-grained, trajectory-aligned\nguidance without training, achieving both accurate motion control and\nphotorealistic content generation. Extensive experiments across diverse\nbenchmarks validate our method's superiority in realism, trajectory\nconsistency, and visual fidelity. This work introduces a novel plug-and-play\nparadigm for controllable video synthesis, offering a new perspective on\nleveraging generative priors for spatial intelligence.",
            "upvotes": 6,
            "discussionId": "68ccb5df3df9ac65e93dc601",
            "projectPage": "https://worldforge-agi.github.io/",
            "ai_summary": "WorldForge, a training-free framework, enhances video diffusion models with precise motion control and photorealistic content generation through recursive refinement, flow-gated latent fusion, and dual-path self-corrective guidance.",
            "ai_keywords": [
                "video diffusion models",
                "spatial intelligence tasks",
                "latent world priors",
                "controllability",
                "geometric inconsistency",
                "retraining",
                "fine-tuning",
                "WorldForge",
                "inference-time framework",
                "Intra-Step Recursive Refinement",
                "denoising step",
                "Flow-Gated Latent Fusion",
                "optical flow similarity",
                "Dual-Path Self-Corrective Guidance",
                "trajectory injection",
                "trajectory drift",
                "photorealistic content generation",
                "trajectory consistency",
                "visual fidelity",
                "plug-and-play paradigm",
                "generative priors"
            ]
        },
        "publishedAt": "2025-09-18T12:40:47.000Z",
        "title": "WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model\n  via Training-Free Guidance",
        "summary": "Recent video diffusion models demonstrate strong potential in spatial\nintelligence tasks due to their rich latent world priors. However, this\npotential is hindered by their limited controllability and geometric\ninconsistency, creating a gap between their strong priors and their practical\nuse in 3D/4D tasks. As a result, current approaches often rely on retraining or\nfine-tuning, which risks degrading pretrained knowledge and incurs high\ncomputational costs. To address this, we propose WorldForge, a training-free,\ninference-time framework composed of three tightly coupled modules. Intra-Step\nRecursive Refinement introduces a recursive refinement mechanism during\ninference, which repeatedly optimizes network predictions within each denoising\nstep to enable precise trajectory injection. Flow-Gated Latent Fusion leverages\noptical flow similarity to decouple motion from appearance in the latent space\nand selectively inject trajectory guidance into motion-related channels.\nDual-Path Self-Corrective Guidance compares guided and unguided denoising paths\nto adaptively correct trajectory drift caused by noisy or misaligned structural\nsignals. Together, these components inject fine-grained, trajectory-aligned\nguidance without training, achieving both accurate motion control and\nphotorealistic content generation. Extensive experiments across diverse\nbenchmarks validate our method's superiority in realism, trajectory\nconsistency, and visual fidelity. This work introduces a novel plug-and-play\nparadigm for controllable video synthesis, offering a new perspective on\nleveraging generative priors for spatial intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15130.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 106
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.14638",
            "authors": [
                {
                    "_id": "68ccb51f3df9ac65e93dc5dc",
                    "user": {
                        "_id": "67ade8d6d53a41cd62ee300c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67ade8d6d53a41cd62ee300c/ThE7nKEkVLmDxq_flbWdj.png",
                        "isPro": false,
                        "fullname": "Li",
                        "user": "Mingsong07",
                        "type": "user"
                    },
                    "name": "Mingsong Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:51.131Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb51f3df9ac65e93dc5dd",
                    "name": "Lin Liu",
                    "hidden": false
                },
                {
                    "_id": "68ccb51f3df9ac65e93dc5de",
                    "user": {
                        "_id": "649bb8158f4b425bdeb13b77",
                        "avatarUrl": "/avatars/64a5173036bf77c2b0e359fb14f41088.svg",
                        "isPro": false,
                        "fullname": "Hongjun Wang",
                        "user": "dreamzz5",
                        "type": "user"
                    },
                    "name": "Hongjun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:38:35.453Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb51f3df9ac65e93dc5df",
                    "user": {
                        "_id": "63898c562a897944ea5f07a2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63898c562a897944ea5f07a2/oG1ia44ICpAk1tReIaX0G.jpeg",
                        "isPro": false,
                        "fullname": "Haoxing chen",
                        "user": "HaoxingChen",
                        "type": "user"
                    },
                    "name": "Haoxing Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:38:44.187Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb51f3df9ac65e93dc5e0",
                    "name": "Xijun Gu",
                    "hidden": false
                },
                {
                    "_id": "68ccb51f3df9ac65e93dc5e1",
                    "name": "Shizhan Liu",
                    "hidden": false
                },
                {
                    "_id": "68ccb51f3df9ac65e93dc5e2",
                    "name": "Dong Gong",
                    "hidden": false
                },
                {
                    "_id": "68ccb51f3df9ac65e93dc5e3",
                    "name": "Junbo Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ccb51f3df9ac65e93dc5e4",
                    "name": "Zhenzhong Lan",
                    "hidden": false
                },
                {
                    "_id": "68ccb51f3df9ac65e93dc5e5",
                    "name": "Jianguo Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-18T05:33:38.000Z",
            "submittedOnDailyAt": "2025-09-19T00:13:05.587Z",
            "title": "MultiEdit: Advancing Instruction-based Image Editing on Diverse and\n  Challenging Tasks",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Current instruction-based image editing (IBIE) methods struggle with\nchallenging editing tasks, as both editing types and sample counts of existing\ndatasets are limited. Moreover, traditional dataset construction often contains\nnoisy image-caption pairs, which may introduce biases and limit model\ncapabilities in complex editing scenarios. To address these limitations, we\nintroduce MultiEdit, a comprehensive dataset featuring over 107K high-quality\nimage editing samples. It encompasses 6 challenging editing tasks through a\ndiverse collection of 18 non-style-transfer editing types and 38 style transfer\noperations, covering a spectrum from sophisticated style transfer to complex\nsemantic operations like person reference editing and in-image text editing. We\nemploy a novel dataset construction pipeline that utilizes two multi-modal\nlarge language models (MLLMs) to generate visual-adaptive editing instructions\nand produce high-fidelity edited images, respectively. Extensive experiments\ndemonstrate that fine-tuning foundational open-source models with our\nMultiEdit-Train set substantially improves models' performance on sophisticated\nediting tasks in our proposed MultiEdit-Test benchmark, while effectively\npreserving their capabilities on the standard editing benchmark. We believe\nMultiEdit provides a valuable resource for advancing research into more diverse\nand challenging IBIE capabilities. Our dataset is available at\nhttps://huggingface.co/datasets/inclusionAI/MultiEdit.",
            "upvotes": 4,
            "discussionId": "68ccb5203df9ac65e93dc5e6",
            "projectPage": "https://huggingface.co/datasets/inclusionAI/MultiEdit",
            "ai_summary": "MultiEdit, a comprehensive dataset with over 107K high-quality image editing samples, improves performance on sophisticated editing tasks using a novel pipeline with multi-modal large language models.",
            "ai_keywords": [
                "instruction-based image editing",
                "IBIE",
                "editing tasks",
                "sample counts",
                "noisy image-caption pairs",
                "biases",
                "dataset construction",
                "editing types",
                "style transfer",
                "semantic operations",
                "person reference editing",
                "in-image text editing",
                "multi-modal large language models",
                "MLLMs",
                "visual-adaptive editing instructions",
                "high-fidelity edited images",
                "fine-tuning",
                "foundational open-source models",
                "MultiEdit-Train",
                "MultiEdit-Test benchmark",
                "standard editing benchmark"
            ]
        },
        "publishedAt": "2025-09-18T01:33:38.000Z",
        "title": "MultiEdit: Advancing Instruction-based Image Editing on Diverse and\n  Challenging Tasks",
        "summary": "Current instruction-based image editing (IBIE) methods struggle with\nchallenging editing tasks, as both editing types and sample counts of existing\ndatasets are limited. Moreover, traditional dataset construction often contains\nnoisy image-caption pairs, which may introduce biases and limit model\ncapabilities in complex editing scenarios. To address these limitations, we\nintroduce MultiEdit, a comprehensive dataset featuring over 107K high-quality\nimage editing samples. It encompasses 6 challenging editing tasks through a\ndiverse collection of 18 non-style-transfer editing types and 38 style transfer\noperations, covering a spectrum from sophisticated style transfer to complex\nsemantic operations like person reference editing and in-image text editing. We\nemploy a novel dataset construction pipeline that utilizes two multi-modal\nlarge language models (MLLMs) to generate visual-adaptive editing instructions\nand produce high-fidelity edited images, respectively. Extensive experiments\ndemonstrate that fine-tuning foundational open-source models with our\nMultiEdit-Train set substantially improves models' performance on sophisticated\nediting tasks in our proposed MultiEdit-Test benchmark, while effectively\npreserving their capabilities on the standard editing benchmark. We believe\nMultiEdit provides a valuable resource for advancing research into more diverse\nand challenging IBIE capabilities. Our dataset is available at\nhttps://huggingface.co/datasets/inclusionAI/MultiEdit.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14638.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 106
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.14233",
            "authors": [
                {
                    "_id": "68cbc02afcdecd257f026cb0",
                    "name": "Alejandro Hernández-Cano",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cb1",
                    "name": "Alexander Hägele",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cb2",
                    "name": "Allen Hao Huang",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cb3",
                    "name": "Angelika Romanou",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cb4",
                    "name": "Antoni-Joan Solergibert",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cb5",
                    "name": "Barna Pasztor",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cb6",
                    "name": "Bettina Messmer",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cb7",
                    "name": "Dhia Garbaya",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cb8",
                    "name": "Eduard Frank Ďurech",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cb9",
                    "name": "Ido Hakimi",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cba",
                    "name": "Juan García Giraldo",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cbb",
                    "name": "Mete Ismayilzada",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cbc",
                    "name": "Negar Foroutan",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cbd",
                    "name": "Skander Moalla",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cbe",
                    "name": "Tiancheng Chen",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cbf",
                    "name": "Vinko Sabolčec",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cc0",
                    "name": "Yixuan Xu",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cc1",
                    "name": "Michael Aerni",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cc2",
                    "name": "Badr AlKhamissi",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cc3",
                    "name": "Ines Altemir Marinas",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cc4",
                    "name": "Mohammad Hossein Amani",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cc5",
                    "name": "Matin Ansaripour",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cc6",
                    "name": "Ilia Badanin",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cc7",
                    "name": "Harold Benoit",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cc8",
                    "name": "Emanuela Boros",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cc9",
                    "name": "Nicholas Browning",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cca",
                    "name": "Fabian Bösch",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026ccb",
                    "name": "Maximilian Böther",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026ccc",
                    "name": "Niklas Canova",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026ccd",
                    "name": "Camille Challier",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cce",
                    "name": "Clement Charmillot",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026ccf",
                    "name": "Jonathan Coles",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cd0",
                    "name": "Jan Deriu",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cd1",
                    "name": "Arnout Devos",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cd2",
                    "name": "Lukas Drescher",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cd3",
                    "name": "Daniil Dzenhaliou",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cd4",
                    "name": "Maud Ehrmann",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cd5",
                    "name": "Dongyang Fan",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cd6",
                    "name": "Simin Fan",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cd7",
                    "name": "Silin Gao",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cd8",
                    "name": "Miguel Gila",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cd9",
                    "user": {
                        "_id": "5f9c00a5777efc07d7f1e4be",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665073337782-5f9c00a5777efc07d7f1e4be.png",
                        "isPro": true,
                        "fullname": "María Grandury",
                        "user": "mariagrandury",
                        "type": "user"
                    },
                    "name": "María Grandury",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:50:06.187Z",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cda",
                    "name": "Diba Hashemi",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cdb",
                    "name": "Alexander Hoyle",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cdc",
                    "name": "Jiaming Jiang",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cdd",
                    "name": "Mark Klein",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cde",
                    "name": "Andrei Kucharavy",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cdf",
                    "name": "Anastasiia Kucherenko",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026ce0",
                    "name": "Frederike Lübeck",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026ce1",
                    "name": "Roman Machacek",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026ce2",
                    "name": "Theofilos Manitaras",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026ce3",
                    "name": "Andreas Marfurt",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026ce4",
                    "name": "Kyle Matoba",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026ce5",
                    "name": "Simon Matrenok",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026ce6",
                    "name": "Henrique Mendoncça",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026ce7",
                    "name": "Fawzi Roberto Mohamed",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026ce8",
                    "name": "Syrielle Montariol",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026ce9",
                    "name": "Luca Mouchel",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cea",
                    "name": "Sven Najem-Meyer",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026ceb",
                    "name": "Jingwei Ni",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cec",
                    "name": "Gennaro Oliva",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026ced",
                    "name": "Matteo Pagliardini",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cee",
                    "name": "Elia Palme",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cef",
                    "name": "Andrei Panferov",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cf0",
                    "name": "Léo Paoletti",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cf1",
                    "name": "Marco Passerini",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cf2",
                    "name": "Ivan Pavlov",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cf3",
                    "name": "Auguste Poiroux",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cf4",
                    "name": "Kaustubh Ponkshe",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cf5",
                    "name": "Nathan Ranchin",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cf6",
                    "name": "Javi Rando",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cf7",
                    "name": "Mathieu Sauser",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cf8",
                    "name": "Jakhongir Saydaliev",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cf9",
                    "name": "Muhammad Ali Sayfiddinov",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cfa",
                    "name": "Marian Schneider",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cfb",
                    "name": "Stefano Schuppli",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cfc",
                    "name": "Marco Scialanga",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cfd",
                    "user": {
                        "_id": "6479f8335f3450e1ded40774",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/iOjE2dcSUS-XYE0WJe6U8.jpeg",
                        "isPro": false,
                        "fullname": "Andrei Semenov",
                        "user": "Andron00e",
                        "type": "user"
                    },
                    "name": "Andrei Semenov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-18T13:25:44.926Z",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cfe",
                    "name": "Kumar Shridhar",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026cff",
                    "name": "Raghav Singhal",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d00",
                    "name": "Anna Sotnikova",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d01",
                    "name": "Alexander Sternfeld",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d02",
                    "name": "Ayush Kumar Tarun",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d03",
                    "name": "Paul Teiletche",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d04",
                    "name": "Jannis Vamvas",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d05",
                    "user": {
                        "_id": "61919e59d221f4281b3833d5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669164541734-61919e59d221f4281b3833d5.jpeg",
                        "isPro": false,
                        "fullname": "Xiaozhe Yao",
                        "user": "xzyao",
                        "type": "user"
                    },
                    "name": "Xiaozhe Yao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:50:03.723Z",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d06",
                    "name": "Hao Zhao Alexander Ilic",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d07",
                    "name": "Ana Klimovic",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d08",
                    "name": "Andreas Krause",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d09",
                    "name": "Caglar Gulcehre",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d0a",
                    "name": "David Rosenthal",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d0b",
                    "name": "Elliott Ash",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d0c",
                    "name": "Florian Tramèr",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d0d",
                    "name": "Joost VandeVondele",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d0e",
                    "name": "Livio Veraldi",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d0f",
                    "name": "Martin Rajman",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d10",
                    "name": "Thomas Schulthess",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d11",
                    "name": "Torsten Hoefler",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d12",
                    "name": "Antoine Bosselut",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d13",
                    "name": "Martin Jaggi",
                    "hidden": false
                },
                {
                    "_id": "68cbc02afcdecd257f026d14",
                    "name": "Imanol Schlag",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-17T17:59:21.000Z",
            "submittedOnDailyAt": "2025-09-19T12:51:17.668Z",
            "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language\n  Environments",
            "submittedOnDailyBy": {
                "_id": "61919e59d221f4281b3833d5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669164541734-61919e59d221f4281b3833d5.jpeg",
                "isPro": false,
                "fullname": "Xiaozhe Yao",
                "user": "xzyao",
                "type": "user"
            },
            "summary": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension.",
            "upvotes": 4,
            "discussionId": "68cbc02afcdecd257f026d15",
            "projectPage": "https://www.swiss-ai.org/apertus",
            "githubRepo": "https://github.com/swiss-ai/apertus-tech-report",
            "ai_summary": "Apertus is a suite of open large language models that ensure data compliance and multilingual representation through ethical data sourcing, the Goldfish objective, and comprehensive artifact release.",
            "ai_keywords": [
                "large language models",
                "data compliance",
                "multilingual representation",
                "openly available data",
                "robots.txt exclusions",
                "Goldfish objective",
                "multilingual benchmarks",
                "data preparation scripts",
                "checkpoints",
                "evaluation suites",
                "training code"
            ],
            "githubStars": 107
        },
        "publishedAt": "2025-09-17T13:59:21.000Z",
        "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language\n  Environments",
        "summary": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14233.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61919e59d221f4281b3833d5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669164541734-61919e59d221f4281b3833d5.jpeg",
            "fullname": "Xiaozhe Yao",
            "name": "xzyao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.10397",
            "authors": [
                {
                    "_id": "68c814fef1d159e090f25e3c",
                    "user": {
                        "_id": "68c2f53e1dcca208f5a5af67",
                        "avatarUrl": "/avatars/ecf0fa3be24d7e90421471b31fcc0c7d.svg",
                        "isPro": false,
                        "fullname": "Fei Liu",
                        "user": "feiliu1",
                        "type": "user"
                    },
                    "name": "Fei Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-15T15:04:43.754Z",
                    "hidden": false
                },
                {
                    "_id": "68c814fef1d159e090f25e3d",
                    "name": "Xinyu Lin",
                    "hidden": false
                },
                {
                    "_id": "68c814fef1d159e090f25e3e",
                    "name": "Hanchao Yu",
                    "hidden": false
                },
                {
                    "_id": "68c814fef1d159e090f25e3f",
                    "name": "Mingyuan Wu",
                    "hidden": false
                },
                {
                    "_id": "68c814fef1d159e090f25e40",
                    "name": "Jianyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68c814fef1d159e090f25e41",
                    "name": "Qiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c814fef1d159e090f25e42",
                    "name": "Zhuokai Zhao",
                    "hidden": false
                },
                {
                    "_id": "68c814fef1d159e090f25e43",
                    "name": "Yinglong Xia",
                    "hidden": false
                },
                {
                    "_id": "68c814fef1d159e090f25e44",
                    "name": "Yao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c814fef1d159e090f25e45",
                    "user": {
                        "_id": "67acc60fe68b373f3ef6e6a7",
                        "avatarUrl": "/avatars/941789c472939e0c08e3e62356e6b278.svg",
                        "isPro": false,
                        "fullname": "Weiwei Li",
                        "user": "VilockLi",
                        "type": "user"
                    },
                    "name": "Weiwei Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:50:32.831Z",
                    "hidden": false
                },
                {
                    "_id": "68c814fef1d159e090f25e46",
                    "name": "Mingze Gao",
                    "hidden": false
                },
                {
                    "_id": "68c814fef1d159e090f25e47",
                    "name": "Qifan Wang",
                    "hidden": false
                },
                {
                    "_id": "68c814fef1d159e090f25e48",
                    "name": "Lizhu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c814fef1d159e090f25e49",
                    "name": "Benyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c814fef1d159e090f25e4a",
                    "name": "Xiangjun Fan",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/68c2f53e1dcca208f5a5af67/VChx-oe4F2oy0yPXHMnSG.png",
                "https://cdn-uploads.huggingface.co/production/uploads/68c2f53e1dcca208f5a5af67/odOPDsi0BcOM8HIAUHBqw.png"
            ],
            "publishedAt": "2025-09-12T16:44:34.000Z",
            "submittedOnDailyAt": "2025-09-19T11:02:06.086Z",
            "title": "RecoWorld: Building Simulated Environments for Agentic Recommender\n  Systems",
            "submittedOnDailyBy": {
                "_id": "68c2f53e1dcca208f5a5af67",
                "avatarUrl": "/avatars/ecf0fa3be24d7e90421471b31fcc0c7d.svg",
                "isPro": false,
                "fullname": "Fei Liu",
                "user": "feiliu1",
                "type": "user"
            },
            "summary": "We present RecoWorld, a blueprint for building simulated environments\ntailored to agentic recommender systems. Such environments give agents a proper\ntraining space where they can learn from errors without impacting real users.\nRecoWorld distinguishes itself with a dual-view architecture: a simulated user\nand an agentic recommender engage in multi-turn interactions aimed at\nmaximizing user retention. The user simulator reviews recommended items,\nupdates its mindset, and when sensing potential user disengagement, generates\nreflective instructions. The agentic recommender adapts its recommendations by\nincorporating these user instructions and reasoning traces, creating a dynamic\nfeedback loop that actively engages users. This process leverages the\nexceptional reasoning capabilities of modern LLMs. We explore diverse content\nrepresentations within the simulator, including text-based, multimodal, and\nsemantic ID modeling, and discuss how multi-turn RL enables the recommender to\nrefine its strategies through iterative interactions. RecoWorld also supports\nmulti-agent simulations, allowing creators to simulate the responses of\ntargeted user populations. It marks an important first step toward recommender\nsystems where users and agents collaboratively shape personalized information\nstreams. We envision new interaction paradigms where \"user instructs,\nrecommender responds,\" jointly optimizing user retention and engagement.",
            "upvotes": 4,
            "discussionId": "68c814fef1d159e090f25e4b",
            "ai_summary": "RecoWorld is a simulated environment for agentic recommender systems that uses a dual-view architecture with user and recommender interactions, leveraging LLMs and multi-turn RL to enhance user retention and engagement.",
            "ai_keywords": [
                "agentic recommender systems",
                "dual-view architecture",
                "user simulator",
                "multi-turn interactions",
                "user retention",
                "reflective instructions",
                "reasoning traces",
                "modern LLMs",
                "content representations",
                "text-based",
                "multimodal",
                "semantic ID modeling",
                "multi-turn RL",
                "multi-agent simulations",
                "personalized information streams"
            ]
        },
        "publishedAt": "2025-09-12T12:44:34.000Z",
        "title": "RecoWorld: Building Simulated Environments for Agentic Recommender\n  Systems",
        "summary": "We present RecoWorld, a blueprint for building simulated environments\ntailored to agentic recommender systems. Such environments give agents a proper\ntraining space where they can learn from errors without impacting real users.\nRecoWorld distinguishes itself with a dual-view architecture: a simulated user\nand an agentic recommender engage in multi-turn interactions aimed at\nmaximizing user retention. The user simulator reviews recommended items,\nupdates its mindset, and when sensing potential user disengagement, generates\nreflective instructions. The agentic recommender adapts its recommendations by\nincorporating these user instructions and reasoning traces, creating a dynamic\nfeedback loop that actively engages users. This process leverages the\nexceptional reasoning capabilities of modern LLMs. We explore diverse content\nrepresentations within the simulator, including text-based, multimodal, and\nsemantic ID modeling, and discuss how multi-turn RL enables the recommender to\nrefine its strategies through iterative interactions. RecoWorld also supports\nmulti-agent simulations, allowing creators to simulate the responses of\ntargeted user populations. It marks an important first step toward recommender\nsystems where users and agents collaboratively shape personalized information\nstreams. We envision new interaction paradigms where \"user instructs,\nrecommender responds,\" jointly optimizing user retention and engagement.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/68c2f53e1dcca208f5a5af67/VChx-oe4F2oy0yPXHMnSG.png",
            "https://cdn-uploads.huggingface.co/production/uploads/68c2f53e1dcca208f5a5af67/odOPDsi0BcOM8HIAUHBqw.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.10397.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68c2f53e1dcca208f5a5af67",
            "avatarUrl": "/avatars/ecf0fa3be24d7e90421471b31fcc0c7d.svg",
            "fullname": "Fei Liu",
            "name": "feiliu1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.15178",
            "authors": [
                {
                    "_id": "68ccc1663df9ac65e93dc664",
                    "user": {
                        "_id": "629c73c588987318a42580a1",
                        "avatarUrl": "/avatars/05ac913fcf7d94d51b75a3fea87d6de2.svg",
                        "isPro": false,
                        "fullname": "Zaiquan yang",
                        "user": "zaiquan",
                        "type": "user"
                    },
                    "name": "Zaiquan Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:37:06.882Z",
                    "hidden": false
                },
                {
                    "_id": "68ccc1663df9ac65e93dc665",
                    "user": {
                        "_id": "66725eb419bebc69b5d7ad05",
                        "avatarUrl": "/avatars/36218f1f19b84c7dfbe8912a620a9dd6.svg",
                        "isPro": false,
                        "fullname": "yuhaoliu",
                        "user": "yuhaoliu",
                        "type": "user"
                    },
                    "name": "Yuhao Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:36:54.136Z",
                    "hidden": false
                },
                {
                    "_id": "68ccc1663df9ac65e93dc666",
                    "name": "Gerhard Hancke",
                    "hidden": false
                },
                {
                    "_id": "68ccc1663df9ac65e93dc667",
                    "name": "Rynson W. H. Lau",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-18T17:35:50.000Z",
            "submittedOnDailyAt": "2025-09-19T01:09:43.882Z",
            "title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot\n  Spatio-Temporal Video Grounding",
            "submittedOnDailyBy": {
                "_id": "6351463b8445bbe32e944f6c",
                "avatarUrl": "/avatars/ec0e8f378d5314d4af97d6c488771b3d.svg",
                "isPro": false,
                "fullname": "Yuhao Liu",
                "user": "LeoLau",
                "type": "user"
            },
            "summary": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal\ntube of a video, as specified by the input text query. In this paper, we\nutilize multimodal large language models (MLLMs) to explore a zero-shot\nsolution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to\ndynamically assign special tokens, referred to as grounding tokens,\nfor grounding the text query; and (2) MLLMs often suffer from suboptimal\ngrounding due to the inability to fully integrate the cues in the text query\n(e.g., attributes, actions) for inference. Based on these insights, we\npropose a MLLM-based zero-shot framework for STVG, which includes novel\ndecomposed spatio-temporal highlighting (DSTH) and temporal-augmented\nassembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH\nstrategy first decouples the original query into attribute and action\nsub-queries for inquiring the existence of the target both spatially and\ntemporally. It then uses a novel logit-guided re-attention (LRA) module to\nlearn latent variables as spatial and temporal prompts, by regularizing token\npredictions for each sub-query. These prompts highlight attribute and action\ncues, respectively, directing the model's attention to reliable spatial and\ntemporal related visual regions. In addition, as the spatial grounding by the\nattribute sub-query should be temporally consistent, we introduce the TAS\nstrategy to assemble the predictions using the original video frames and the\ntemporal-augmented frames as inputs to help improve temporal consistency. We\nevaluate our method on various MLLMs, and show that it outperforms SOTA methods\non three common STVG benchmarks.\n  The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.",
            "upvotes": 3,
            "discussionId": "68ccc1673df9ac65e93dc668",
            "githubRepo": "https://github.com/zaiquanyang/LLaVA_Next_STVG",
            "ai_summary": "A zero-shot framework using multimodal large language models for spatio-temporal video grounding employs decomposed spatio-temporal highlighting and temporal-augmented assembling strategies to improve grounding accuracy.",
            "ai_keywords": [
                "spatio-temporal video grounding",
                "multimodal large language models",
                "zero-shot solution",
                "grounding tokens",
                "decomposed spatio-temporal highlighting",
                "temporal-augmented assembling",
                "logit-guided re-attention",
                "latent variables",
                "spatial prompts",
                "temporal prompts",
                "temporal consistency"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-09-18T13:35:50.000Z",
        "title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot\n  Spatio-Temporal Video Grounding",
        "summary": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal\ntube of a video, as specified by the input text query. In this paper, we\nutilize multimodal large language models (MLLMs) to explore a zero-shot\nsolution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to\ndynamically assign special tokens, referred to as grounding tokens,\nfor grounding the text query; and (2) MLLMs often suffer from suboptimal\ngrounding due to the inability to fully integrate the cues in the text query\n(e.g., attributes, actions) for inference. Based on these insights, we\npropose a MLLM-based zero-shot framework for STVG, which includes novel\ndecomposed spatio-temporal highlighting (DSTH) and temporal-augmented\nassembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH\nstrategy first decouples the original query into attribute and action\nsub-queries for inquiring the existence of the target both spatially and\ntemporally. It then uses a novel logit-guided re-attention (LRA) module to\nlearn latent variables as spatial and temporal prompts, by regularizing token\npredictions for each sub-query. These prompts highlight attribute and action\ncues, respectively, directing the model's attention to reliable spatial and\ntemporal related visual regions. In addition, as the spatial grounding by the\nattribute sub-query should be temporally consistent, we introduce the TAS\nstrategy to assemble the predictions using the original video frames and the\ntemporal-augmented frames as inputs to help improve temporal consistency. We\nevaluate our method on various MLLMs, and show that it outperforms SOTA methods\non three common STVG benchmarks.\n  The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15178.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6351463b8445bbe32e944f6c",
            "avatarUrl": "/avatars/ec0e8f378d5314d4af97d6c488771b3d.svg",
            "fullname": "Yuhao Liu",
            "name": "LeoLau",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.13399",
            "authors": [
                {
                    "_id": "68ccff113df9ac65e93dc734",
                    "user": {
                        "_id": "6630a1da64b627c0b9a57360",
                        "avatarUrl": "/avatars/9070ca03d2f7eef1e7e243c14d89274f.svg",
                        "isPro": false,
                        "fullname": "Tianyu Chen",
                        "user": "C-Tianyu",
                        "type": "user"
                    },
                    "name": "Tianyu Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T10:25:19.099Z",
                    "hidden": false
                },
                {
                    "_id": "68ccff113df9ac65e93dc735",
                    "name": "Yasi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ccff113df9ac65e93dc736",
                    "name": "Zhi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ccff113df9ac65e93dc737",
                    "name": "Peiyu Yu",
                    "hidden": false
                },
                {
                    "_id": "68ccff113df9ac65e93dc738",
                    "name": "Shu Wang",
                    "hidden": false
                },
                {
                    "_id": "68ccff113df9ac65e93dc739",
                    "name": "Zhendong Wang",
                    "hidden": false
                },
                {
                    "_id": "68ccff113df9ac65e93dc73a",
                    "name": "Kevin Lin",
                    "hidden": false
                },
                {
                    "_id": "68ccff113df9ac65e93dc73b",
                    "name": "Xiaofei Wang",
                    "hidden": false
                },
                {
                    "_id": "68ccff113df9ac65e93dc73c",
                    "name": "Zhengyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "68ccff113df9ac65e93dc73d",
                    "name": "Linjie Li",
                    "hidden": false
                },
                {
                    "_id": "68ccff113df9ac65e93dc73e",
                    "name": "Chung-Ching Lin",
                    "hidden": false
                },
                {
                    "_id": "68ccff113df9ac65e93dc73f",
                    "name": "Jianwen Xie",
                    "hidden": false
                },
                {
                    "_id": "68ccff113df9ac65e93dc740",
                    "name": "Oscar Leong",
                    "hidden": false
                },
                {
                    "_id": "68ccff113df9ac65e93dc741",
                    "name": "Lijuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68ccff113df9ac65e93dc742",
                    "name": "Ying Nian Wu",
                    "hidden": false
                },
                {
                    "_id": "68ccff113df9ac65e93dc743",
                    "name": "Mingyuan Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-16T17:45:39.000Z",
            "submittedOnDailyAt": "2025-09-19T18:44:41.086Z",
            "title": "EdiVal-Agent: An Object-Centric Framework for Automated, Scalable,\n  Fine-Grained Evaluation of Multi-Turn Editing",
            "submittedOnDailyBy": {
                "_id": "6630a1da64b627c0b9a57360",
                "avatarUrl": "/avatars/9070ca03d2f7eef1e7e243c14d89274f.svg",
                "isPro": false,
                "fullname": "Tianyu Chen",
                "user": "C-Tianyu",
                "type": "user"
            },
            "summary": "Instruction-based image editing has advanced rapidly, yet reliable and\ninterpretable evaluation remains a bottleneck. Current protocols either (i)\ndepend on paired reference images -- resulting in limited coverage and\ninheriting biases from prior generative models -- or (ii) rely solely on\nzero-shot vision-language models (VLMs), whose prompt-based assessments of\ninstruction following, content consistency, and visual quality are often\nimprecise.\n  To address this, we introduce EdiVal-Agent, an automated, scalable, and\nfine-grained evaluation framework for multi-turn instruction-based editing from\nan object-centric perspective, supported by a suite of expert tools. Given an\nimage, EdiVal-Agent first decomposes it into semantically meaningful objects,\nthen synthesizes diverse, context-aware editing instructions. For evaluation,\nit integrates VLMs with open-vocabulary object detectors to assess instruction\nfollowing, uses semantic-level feature extractors to evaluate content\nconsistency, and leverages human preference models to judge visual quality. We\nshow that combining VLMs with object detectors yields stronger agreement with\nhuman judgments in instruction-following evaluation compared to using VLMs\nalone and CLIP-based metrics. Furthermore, the pipeline's modular design allows\nfuture tools to be seamlessly integrated, enhancing evaluation accuracy over\ntime.\n  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing\nbenchmark covering 9 instruction types and 11 state-of-the-art editing models\nspanning autoregressive (AR) (including Nano Banana, GPT-Image-1),\nflow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be\nused to identify existing failure modes, thereby informing the development of\nthe next generation of editing models. Project page:\nhttps://tianyucodings.github.io/EdiVAL-page/.",
            "upvotes": 3,
            "discussionId": "68ccff113df9ac65e93dc744",
            "ai_summary": "EdiVal-Agent is an automated evaluation framework for instruction-based image editing that integrates VLMs, object detectors, and human preference models to assess instruction following, content consistency, and visual quality.",
            "ai_keywords": [
                "EdiVal-Agent",
                "zero-shot vision-language models",
                "VLMs",
                "open-vocabulary object detectors",
                "semantic-level feature extractors",
                "human preference models",
                "EdiVal-Bench",
                "autoregressive",
                "flow-matching",
                "diffusion paradigms"
            ]
        },
        "publishedAt": "2025-09-16T13:45:39.000Z",
        "title": "EdiVal-Agent: An Object-Centric Framework for Automated, Scalable,\n  Fine-Grained Evaluation of Multi-Turn Editing",
        "summary": "Instruction-based image editing has advanced rapidly, yet reliable and\ninterpretable evaluation remains a bottleneck. Current protocols either (i)\ndepend on paired reference images -- resulting in limited coverage and\ninheriting biases from prior generative models -- or (ii) rely solely on\nzero-shot vision-language models (VLMs), whose prompt-based assessments of\ninstruction following, content consistency, and visual quality are often\nimprecise.\n  To address this, we introduce EdiVal-Agent, an automated, scalable, and\nfine-grained evaluation framework for multi-turn instruction-based editing from\nan object-centric perspective, supported by a suite of expert tools. Given an\nimage, EdiVal-Agent first decomposes it into semantically meaningful objects,\nthen synthesizes diverse, context-aware editing instructions. For evaluation,\nit integrates VLMs with open-vocabulary object detectors to assess instruction\nfollowing, uses semantic-level feature extractors to evaluate content\nconsistency, and leverages human preference models to judge visual quality. We\nshow that combining VLMs with object detectors yields stronger agreement with\nhuman judgments in instruction-following evaluation compared to using VLMs\nalone and CLIP-based metrics. Furthermore, the pipeline's modular design allows\nfuture tools to be seamlessly integrated, enhancing evaluation accuracy over\ntime.\n  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing\nbenchmark covering 9 instruction types and 11 state-of-the-art editing models\nspanning autoregressive (AR) (including Nano Banana, GPT-Image-1),\nflow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be\nused to identify existing failure modes, thereby informing the development of\nthe next generation of editing models. Project page:\nhttps://tianyucodings.github.io/EdiVAL-page/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13399.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6630a1da64b627c0b9a57360",
            "avatarUrl": "/avatars/9070ca03d2f7eef1e7e243c14d89274f.svg",
            "fullname": "Tianyu Chen",
            "name": "C-Tianyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.15020",
            "authors": [
                {
                    "_id": "68cd2c4b3df9ac65e93dc787",
                    "user": {
                        "_id": "64d4b251071f4a335c97264e",
                        "avatarUrl": "/avatars/c14ae3eea034c1b39186a194425e9989.svg",
                        "isPro": false,
                        "fullname": "Mario Sanz",
                        "user": "mario-sanz",
                        "type": "user"
                    },
                    "name": "Mario Sanz-Guerrero",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T10:24:53.862Z",
                    "hidden": false
                },
                {
                    "_id": "68cd2c4b3df9ac65e93dc788",
                    "user": {
                        "_id": "65e0e6fa4394fc3d1b59627a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e0e6fa4394fc3d1b59627a/rlKw_UdH3MpmpgUcchMgB.jpeg",
                        "isPro": false,
                        "fullname": "Minh Duc Bui",
                        "user": "MinhDucBui",
                        "type": "user"
                    },
                    "name": "Minh Duc Bui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:28:19.439Z",
                    "hidden": false
                },
                {
                    "_id": "68cd2c4b3df9ac65e93dc789",
                    "name": "Katharina von der Wense",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-18T14:47:58.000Z",
            "submittedOnDailyAt": "2025-09-19T08:44:21.538Z",
            "title": "Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question\n  Answering with LLMs",
            "submittedOnDailyBy": {
                "_id": "64d4b251071f4a335c97264e",
                "avatarUrl": "/avatars/c14ae3eea034c1b39186a194425e9989.svg",
                "isPro": false,
                "fullname": "Mario Sanz",
                "user": "mario-sanz",
                "type": "user"
            },
            "summary": "When evaluating large language models (LLMs) with multiple-choice question\nanswering (MCQA), it is common to end the prompt with the string \"Answer:\" to\nfacilitate automated answer extraction via next-token probabilities. However,\nthere is no consensus on how to tokenize the space following the colon, often\noverlooked as a trivial choice. In this paper, we uncover accuracy differences\nof up to 11% due to this (seemingly irrelevant) tokenization variation as well\nas reshuffled model rankings, raising concerns about the reliability of LLM\ncomparisons in prior work. Surprisingly, we are able to recommend one specific\nstrategy -- tokenizing the space together with the answer letter -- as we\nobserve consistent and statistically significant performance improvements.\nAdditionally, it improves model calibration, enhancing the reliability of the\nmodel's confidence estimates. Our findings underscore the importance of careful\nevaluation design and highlight the need for standardized, transparent\nevaluation protocols to ensure reliable and comparable results.",
            "upvotes": 2,
            "discussionId": "68cd2c4b3df9ac65e93dc78a",
            "ai_summary": "Tokenizing the space with the answer letter in multiple-choice question answering improves LLM accuracy and calibration.",
            "ai_keywords": [
                "large language models",
                "multiple-choice question answering",
                "tokenization",
                "next-token probabilities",
                "model calibration",
                "evaluation protocols"
            ]
        },
        "publishedAt": "2025-09-18T10:47:58.000Z",
        "title": "Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question\n  Answering with LLMs",
        "summary": "When evaluating large language models (LLMs) with multiple-choice question\nanswering (MCQA), it is common to end the prompt with the string \"Answer:\" to\nfacilitate automated answer extraction via next-token probabilities. However,\nthere is no consensus on how to tokenize the space following the colon, often\noverlooked as a trivial choice. In this paper, we uncover accuracy differences\nof up to 11% due to this (seemingly irrelevant) tokenization variation as well\nas reshuffled model rankings, raising concerns about the reliability of LLM\ncomparisons in prior work. Surprisingly, we are able to recommend one specific\nstrategy -- tokenizing the space together with the answer letter -- as we\nobserve consistent and statistically significant performance improvements.\nAdditionally, it improves model calibration, enhancing the reliability of the\nmodel's confidence estimates. Our findings underscore the importance of careful\nevaluation design and highlight the need for standardized, transparent\nevaluation protocols to ensure reliable and comparable results.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15020.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64d4b251071f4a335c97264e",
            "avatarUrl": "/avatars/c14ae3eea034c1b39186a194425e9989.svg",
            "fullname": "Mario Sanz",
            "name": "mario-sanz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.09307",
            "authors": [
                {
                    "_id": "68cd48333df9ac65e93dc7d0",
                    "user": {
                        "_id": "679f64c6d196d603a98fa831",
                        "avatarUrl": "/avatars/f9d16a2461862bafe8f8509503fb748e.svg",
                        "isPro": false,
                        "fullname": "Zhengzhao Lai",
                        "user": "onlyairnopods",
                        "type": "user"
                    },
                    "name": "Zhengzhao Lai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T13:09:46.842Z",
                    "hidden": false
                },
                {
                    "_id": "68cd48333df9ac65e93dc7d1",
                    "name": "Youbin Zheng",
                    "hidden": false
                },
                {
                    "_id": "68cd48333df9ac65e93dc7d2",
                    "name": "Zhenyang Cai",
                    "hidden": false
                },
                {
                    "_id": "68cd48333df9ac65e93dc7d3",
                    "name": "Haonan Lyu",
                    "hidden": false
                },
                {
                    "_id": "68cd48333df9ac65e93dc7d4",
                    "name": "Jinpu Yang",
                    "hidden": false
                },
                {
                    "_id": "68cd48333df9ac65e93dc7d5",
                    "name": "Hongqing Liang",
                    "hidden": false
                },
                {
                    "_id": "68cd48333df9ac65e93dc7d6",
                    "name": "Yan Hu",
                    "hidden": false
                },
                {
                    "_id": "68cd48333df9ac65e93dc7d7",
                    "name": "Benyou Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-11T09:50:16.000Z",
            "submittedOnDailyAt": "2025-09-19T12:48:23.446Z",
            "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on\n  Materials Characterization",
            "submittedOnDailyBy": {
                "_id": "679f64c6d196d603a98fa831",
                "avatarUrl": "/avatars/f9d16a2461862bafe8f8509503fb748e.svg",
                "isPro": false,
                "fullname": "Zhengzhao Lai",
                "user": "onlyairnopods",
                "type": "user"
            },
            "summary": "Materials characterization is fundamental to acquiring materials information,\nrevealing the processing-microstructure-property relationships that guide\nmaterial design and optimization. While multimodal large language models\n(MLLMs) have recently shown promise in generative and predictive tasks within\nmaterials science, their capacity to understand real-world characterization\nimaging data remains underexplored. To bridge this gap, we present MatCha, the\nfirst benchmark for materials characterization image understanding, comprising\n1,500 questions that demand expert-level domain expertise. MatCha encompasses\nfour key stages of materials research comprising 21 distinct tasks, each\ndesigned to reflect authentic challenges faced by materials scientists. Our\nevaluation of state-of-the-art MLLMs on MatCha reveals a significant\nperformance gap compared to human experts. These models exhibit degradation\nwhen addressing questions requiring higher-level expertise and sophisticated\nvisual perception. Simple few-shot and chain-of-thought prompting struggle to\nalleviate these limitations. These findings highlight that existing MLLMs still\nexhibit limited adaptability to real-world materials characterization\nscenarios. We hope MatCha will facilitate future research in areas such as new\nmaterial discovery and autonomous scientific agents. MatCha is available at\nhttps://github.com/FreedomIntelligence/MatCha.",
            "upvotes": 2,
            "discussionId": "68cd48343df9ac65e93dc7d8",
            "ai_summary": "MatCha is a benchmark for evaluating the performance of multimodal large language models in understanding materials characterization images, revealing significant limitations compared to human experts.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "benchmark",
                "materials characterization image understanding",
                "few-shot prompting",
                "chain-of-thought prompting",
                "autonomous scientific agents"
            ]
        },
        "publishedAt": "2025-09-11T05:50:16.000Z",
        "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on\n  Materials Characterization",
        "summary": "Materials characterization is fundamental to acquiring materials information,\nrevealing the processing-microstructure-property relationships that guide\nmaterial design and optimization. While multimodal large language models\n(MLLMs) have recently shown promise in generative and predictive tasks within\nmaterials science, their capacity to understand real-world characterization\nimaging data remains underexplored. To bridge this gap, we present MatCha, the\nfirst benchmark for materials characterization image understanding, comprising\n1,500 questions that demand expert-level domain expertise. MatCha encompasses\nfour key stages of materials research comprising 21 distinct tasks, each\ndesigned to reflect authentic challenges faced by materials scientists. Our\nevaluation of state-of-the-art MLLMs on MatCha reveals a significant\nperformance gap compared to human experts. These models exhibit degradation\nwhen addressing questions requiring higher-level expertise and sophisticated\nvisual perception. Simple few-shot and chain-of-thought prompting struggle to\nalleviate these limitations. These findings highlight that existing MLLMs still\nexhibit limited adaptability to real-world materials characterization\nscenarios. We hope MatCha will facilitate future research in areas such as new\nmaterial discovery and autonomous scientific agents. MatCha is available at\nhttps://github.com/FreedomIntelligence/MatCha.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09307.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "679f64c6d196d603a98fa831",
            "avatarUrl": "/avatars/f9d16a2461862bafe8f8509503fb748e.svg",
            "fullname": "Zhengzhao Lai",
            "name": "onlyairnopods",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.06216",
            "authors": [
                {
                    "_id": "68cccbb93df9ac65e93dc6a1",
                    "name": "Ahmed E. Hassan",
                    "hidden": false
                },
                {
                    "_id": "68cccbb93df9ac65e93dc6a2",
                    "user": {
                        "_id": "62b4f3b7464e664268bf4e85",
                        "avatarUrl": "/avatars/16e79e11a8734b1d241e0f0c55a54045.svg",
                        "isPro": false,
                        "fullname": "Leo",
                        "user": "hao-li",
                        "type": "user"
                    },
                    "name": "Hao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:14.213Z",
                    "hidden": false
                },
                {
                    "_id": "68cccbb93df9ac65e93dc6a3",
                    "name": "Dayi Lin",
                    "hidden": false
                },
                {
                    "_id": "68cccbb93df9ac65e93dc6a4",
                    "name": "Bram Adams",
                    "hidden": false
                },
                {
                    "_id": "68cccbb93df9ac65e93dc6a5",
                    "name": "Tse-Hsun Chen",
                    "hidden": false
                },
                {
                    "_id": "68cccbb93df9ac65e93dc6a6",
                    "name": "Yutaro Kashiwa",
                    "hidden": false
                },
                {
                    "_id": "68cccbb93df9ac65e93dc6a7",
                    "name": "Dong Qiu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62b4f3b7464e664268bf4e85/xr9FCpbAjOwEaMXPHg5cJ.jpeg"
            ],
            "publishedAt": "2025-09-07T21:40:10.000Z",
            "submittedOnDailyAt": "2025-09-19T11:30:55.379Z",
            "title": "Agentic Software Engineering: Foundational Pillars and a Research\n  Roadmap",
            "submittedOnDailyBy": {
                "_id": "62b4f3b7464e664268bf4e85",
                "avatarUrl": "/avatars/16e79e11a8734b1d241e0f0c55a54045.svg",
                "isPro": false,
                "fullname": "Leo",
                "user": "hao-li",
                "type": "user"
            },
            "summary": "Agentic Software Engineering (SE 3.0) represents a new era where intelligent\nagents are tasked not with simple code generation, but with achieving complex,\ngoal-oriented SE objectives. To harness these new capabilities while ensuring\ntrustworthiness, we must recognize a fundamental duality within the SE field in\nthe Agentic SE era, comprising two symbiotic modalities: SE for Humans and SE\nfor Agents. This duality demands a radical reimagining of the foundational\npillars of SE (actors, processes, tools, and artifacts) which manifest\ndifferently across each modality. We propose two purpose-built workbenches to\nsupport this vision. The Agent Command Environment (ACE) serves as a command\ncenter where humans orchestrate and mentor agent teams, handling outputs such\nas Merge-Readiness Packs (MRPs) and Consultation Request Packs (CRPs). The\nAgent Execution Environment (AEE) is a digital workspace where agents perform\ntasks while invoking human expertise when facing ambiguity or complex\ntrade-offs. This bi-directional partnership, which supports agent-initiated\nhuman callbacks and handovers, gives rise to new, structured engineering\nactivities (i.e., processes) that redefine human-AI collaboration, elevating\nthe practice from agentic coding to true agentic software engineering. This\npaper presents the Structured Agentic Software Engineering (SASE) vision,\noutlining several of the foundational pillars for the future of SE. The paper\nculminates in a research roadmap that identifies a few key challenges and\nopportunities while briefly discussing the resulting impact of this future on\nSE education. Our goal is not to offer a definitive solution, but to provide a\nconceptual scaffold with structured vocabulary to catalyze a community-wide\ndialogue, pushing the SE community to think beyond its classic, human-centric\ntenets toward a disciplined, scalable, and trustworthy agentic future.",
            "upvotes": 2,
            "discussionId": "68cccbba3df9ac65e93dc6a8",
            "ai_summary": "Agentic Software Engineering introduces a dual modality approach with human and agent collaboration, redefining software engineering processes and tools to achieve complex, goal-oriented objectives.",
            "ai_keywords": [
                "Agentic Software Engineering",
                "SE 3.0",
                "intelligent agents",
                "SE for Humans",
                "SE for Agents",
                "Actor Command Environment",
                "Agent Execution Environment",
                "Merge-Readiness Packs",
                "Consultation Request Packs",
                "human-AI collaboration",
                "Structured Agentic Software Engineering"
            ]
        },
        "publishedAt": "2025-09-07T17:40:10.000Z",
        "title": "Agentic Software Engineering: Foundational Pillars and a Research\n  Roadmap",
        "summary": "Agentic Software Engineering (SE 3.0) represents a new era where intelligent\nagents are tasked not with simple code generation, but with achieving complex,\ngoal-oriented SE objectives. To harness these new capabilities while ensuring\ntrustworthiness, we must recognize a fundamental duality within the SE field in\nthe Agentic SE era, comprising two symbiotic modalities: SE for Humans and SE\nfor Agents. This duality demands a radical reimagining of the foundational\npillars of SE (actors, processes, tools, and artifacts) which manifest\ndifferently across each modality. We propose two purpose-built workbenches to\nsupport this vision. The Agent Command Environment (ACE) serves as a command\ncenter where humans orchestrate and mentor agent teams, handling outputs such\nas Merge-Readiness Packs (MRPs) and Consultation Request Packs (CRPs). The\nAgent Execution Environment (AEE) is a digital workspace where agents perform\ntasks while invoking human expertise when facing ambiguity or complex\ntrade-offs. This bi-directional partnership, which supports agent-initiated\nhuman callbacks and handovers, gives rise to new, structured engineering\nactivities (i.e., processes) that redefine human-AI collaboration, elevating\nthe practice from agentic coding to true agentic software engineering. This\npaper presents the Structured Agentic Software Engineering (SASE) vision,\noutlining several of the foundational pillars for the future of SE. The paper\nculminates in a research roadmap that identifies a few key challenges and\nopportunities while briefly discussing the resulting impact of this future on\nSE education. Our goal is not to offer a definitive solution, but to provide a\nconceptual scaffold with structured vocabulary to catalyze a community-wide\ndialogue, pushing the SE community to think beyond its classic, human-centric\ntenets toward a disciplined, scalable, and trustworthy agentic future.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62b4f3b7464e664268bf4e85/xr9FCpbAjOwEaMXPHg5cJ.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06216.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62b4f3b7464e664268bf4e85",
            "avatarUrl": "/avatars/16e79e11a8734b1d241e0f0c55a54045.svg",
            "fullname": "Leo",
            "name": "hao-li",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.14977",
            "authors": [
                {
                    "_id": "68ccaff53df9ac65e93dc5ab",
                    "user": {
                        "_id": "6576dc779091da7dc69a0acd",
                        "avatarUrl": "/avatars/dd825a1308e687b23a349ccb64f9a91a.svg",
                        "isPro": false,
                        "fullname": "chaoyinshe",
                        "user": "chaoyinshe",
                        "type": "user"
                    },
                    "name": "Chaoyin She",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:49:12.488Z",
                    "hidden": false
                },
                {
                    "_id": "68ccaff53df9ac65e93dc5ac",
                    "user": {
                        "_id": "673c011c79b5a48d3749bbc0",
                        "avatarUrl": "/avatars/8ef91fcf988bc64289c67f859a931658.svg",
                        "isPro": false,
                        "fullname": "Rui-Fang Lu",
                        "user": "Ruimed",
                        "type": "user"
                    },
                    "name": "Ruifang Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:27:38.830Z",
                    "hidden": false
                },
                {
                    "_id": "68ccaff53df9ac65e93dc5ad",
                    "user": {
                        "_id": "67d2e10c3b1df5eb08e2fbfc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WG_8c5MUJFptouBODBl2h.png",
                        "isPro": false,
                        "fullname": "lidachen",
                        "user": "lidachen",
                        "type": "user"
                    },
                    "name": "Lida Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:27:47.453Z",
                    "hidden": false
                },
                {
                    "_id": "68ccaff53df9ac65e93dc5ae",
                    "user": {
                        "_id": "62fa0ffe0697d224219a0cb7",
                        "avatarUrl": "/avatars/f0ef59e1c0cf4ab4fe5cee08d488bd03.svg",
                        "isPro": false,
                        "fullname": "Wei Wang",
                        "user": "WeiWang",
                        "type": "user"
                    },
                    "name": "Wei Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:28:07.479Z",
                    "hidden": false
                },
                {
                    "_id": "68ccaff53df9ac65e93dc5af",
                    "name": "Qinghua Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-18T14:07:53.000Z",
            "submittedOnDailyAt": "2025-09-19T05:22:49.128Z",
            "title": "EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal\n  Ultrasound Intelligence",
            "submittedOnDailyBy": {
                "_id": "6576dc779091da7dc69a0acd",
                "avatarUrl": "/avatars/dd825a1308e687b23a349ccb64f9a91a.svg",
                "isPro": false,
                "fullname": "chaoyinshe",
                "user": "chaoyinshe",
                "type": "user"
            },
            "summary": "Ultrasound imaging has become the preferred imaging modality for early cancer\nscreening due to its advantages of non-ionizing radiation, low cost, and\nreal-time imaging capabilities. However, conventional ultrasound diagnosis\nheavily relies on physician expertise, presenting challenges of high\nsubjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer\npromising solutions for this issue, but existing general-purpose models\ndemonstrate limited knowledge in ultrasound medical tasks, with poor\ngeneralization in multi-organ lesion recognition and low efficiency across\nmulti-task diagnostics. To address these limitations, we propose EchoVLM, a\nvision-language model specifically designed for ultrasound medical imaging. The\nmodel employs a Mixture of Experts (MoE) architecture trained on data spanning\nseven anatomical regions. This design enables the model to perform multiple\ntasks, including ultrasound report generation, diagnosis and visual\nquestion-answering (VQA). The experimental results demonstrated that EchoVLM\nachieved significant improvements of 10.15 and 4.77 points in BLEU-1 scores and\nROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report\ngeneration task. These findings suggest that EchoVLM has substantial potential\nto enhance diagnostic accuracy in ultrasound imaging, thereby providing a\nviable technical solution for future clinical applications. Source code and\nmodel weights are available at https://github.com/Asunatan/EchoVLM.",
            "upvotes": 1,
            "discussionId": "68ccaff53df9ac65e93dc5b0",
            "githubRepo": "https://github.com/Asunatan/EchoVLM",
            "ai_summary": "EchoVLM, a vision-language model with a Mixture of Experts architecture, improves ultrasound report generation and diagnosis by leveraging data from multiple anatomical regions.",
            "ai_keywords": [
                "vision-language models",
                "Mixture of Experts",
                "ultrasound report generation",
                "diagnosis",
                "visual question-answering",
                "BLEU-1 scores",
                "ROUGE-1 scores"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-09-18T10:07:53.000Z",
        "title": "EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal\n  Ultrasound Intelligence",
        "summary": "Ultrasound imaging has become the preferred imaging modality for early cancer\nscreening due to its advantages of non-ionizing radiation, low cost, and\nreal-time imaging capabilities. However, conventional ultrasound diagnosis\nheavily relies on physician expertise, presenting challenges of high\nsubjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer\npromising solutions for this issue, but existing general-purpose models\ndemonstrate limited knowledge in ultrasound medical tasks, with poor\ngeneralization in multi-organ lesion recognition and low efficiency across\nmulti-task diagnostics. To address these limitations, we propose EchoVLM, a\nvision-language model specifically designed for ultrasound medical imaging. The\nmodel employs a Mixture of Experts (MoE) architecture trained on data spanning\nseven anatomical regions. This design enables the model to perform multiple\ntasks, including ultrasound report generation, diagnosis and visual\nquestion-answering (VQA). The experimental results demonstrated that EchoVLM\nachieved significant improvements of 10.15 and 4.77 points in BLEU-1 scores and\nROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report\ngeneration task. These findings suggest that EchoVLM has substantial potential\nto enhance diagnostic accuracy in ultrasound imaging, thereby providing a\nviable technical solution for future clinical applications. Source code and\nmodel weights are available at https://github.com/Asunatan/EchoVLM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14977.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6576dc779091da7dc69a0acd",
            "avatarUrl": "/avatars/dd825a1308e687b23a349ccb64f9a91a.svg",
            "fullname": "chaoyinshe",
            "name": "chaoyinshe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.10402",
            "authors": [
                {
                    "_id": "68c81748f1d159e090f25e4d",
                    "user": {
                        "_id": "65dd617096d71c27a19a28d4",
                        "avatarUrl": "/avatars/8f599654d0e84b95cfb20a43e75e4bc9.svg",
                        "isPro": false,
                        "fullname": "Suzhen Zhong",
                        "user": "Suzhen",
                        "type": "user"
                    },
                    "name": "Suzhen Zhong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:50:30.362Z",
                    "hidden": false
                },
                {
                    "_id": "68c81748f1d159e090f25e4e",
                    "name": "Ying Zou",
                    "hidden": false
                },
                {
                    "_id": "68c81748f1d159e090f25e4f",
                    "name": "Bram Adams",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-12T16:52:49.000Z",
            "submittedOnDailyAt": "2025-09-19T11:35:47.400Z",
            "title": "Developer-LLM Conversations: An Empirical Study of Interactions and\n  Generated Code Quality",
            "submittedOnDailyBy": {
                "_id": "65dd617096d71c27a19a28d4",
                "avatarUrl": "/avatars/8f599654d0e84b95cfb20a43e75e4bc9.svg",
                "isPro": false,
                "fullname": "Suzhen Zhong",
                "user": "Suzhen",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) are becoming integral to modern software\ndevelopment workflows, assisting developers with code generation, API\nexplanation, and iterative problem-solving through natural language\nconversations. Despite widespread adoption, there is limited understanding of\nhow developers interact with LLMs in practice and how these conversational\ndynamics influence task outcomes, code quality, and software engineering\nworkflows. To address this, we leverage CodeChat, a large dataset comprising\n82,845 real-world developer-LLM conversations, containing 368,506 code snippets\ngenerated across over 20 programming languages, derived from the WildChat\ndataset. We find that LLM responses are substantially longer than developer\nprompts, with a median token-length ratio of 14:1. Multi-turn conversations\naccount for 68% of the dataset and often evolve due to shifting requirements,\nincomplete prompts, or clarification requests. Topic analysis identifies web\ndesign (9.6% of conversations) and neural network training (8.7% of\nconversations) as the most frequent LLM-assisted tasks. Evaluation across five\nlanguages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and\nlanguage-specific issues in LLM-generated code: generated Python and JavaScript\ncode often include undefined variables (83.4% and 75.3% of code snippets,\nrespectively); Java code lacks required comments (75.9%); C++ code frequently\nomits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a\nconversation, syntax and import errors persist across turns; however,\ndocumentation quality in Java improves by up to 14.7%, and import handling in\nPython improves by 3.7% over 5 turns. Prompts that point out mistakes in code\ngenerated in prior turns and explicitly request a fix are most effective for\nresolving errors.",
            "upvotes": 0,
            "discussionId": "68c81748f1d159e090f25e50",
            "ai_summary": "Analysis of real-world developer-LLM conversations reveals patterns in task outcomes, code quality, and common issues across multiple programming languages.",
            "ai_keywords": [
                "CodeChat",
                "large dataset",
                "developer-LLM conversations",
                "code snippets",
                "WildChat",
                "token-length ratio",
                "multi-turn conversations",
                "topic analysis",
                "web design",
                "neural network training",
                "language-specific issues",
                "undefined variables",
                "required comments",
                "omitted headers",
                "unresolved namespaces",
                "syntax errors",
                "import errors",
                "documentation quality",
                "import handling"
            ]
        },
        "publishedAt": "2025-09-12T12:52:49.000Z",
        "title": "Developer-LLM Conversations: An Empirical Study of Interactions and\n  Generated Code Quality",
        "summary": "Large Language Models (LLMs) are becoming integral to modern software\ndevelopment workflows, assisting developers with code generation, API\nexplanation, and iterative problem-solving through natural language\nconversations. Despite widespread adoption, there is limited understanding of\nhow developers interact with LLMs in practice and how these conversational\ndynamics influence task outcomes, code quality, and software engineering\nworkflows. To address this, we leverage CodeChat, a large dataset comprising\n82,845 real-world developer-LLM conversations, containing 368,506 code snippets\ngenerated across over 20 programming languages, derived from the WildChat\ndataset. We find that LLM responses are substantially longer than developer\nprompts, with a median token-length ratio of 14:1. Multi-turn conversations\naccount for 68% of the dataset and often evolve due to shifting requirements,\nincomplete prompts, or clarification requests. Topic analysis identifies web\ndesign (9.6% of conversations) and neural network training (8.7% of\nconversations) as the most frequent LLM-assisted tasks. Evaluation across five\nlanguages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and\nlanguage-specific issues in LLM-generated code: generated Python and JavaScript\ncode often include undefined variables (83.4% and 75.3% of code snippets,\nrespectively); Java code lacks required comments (75.9%); C++ code frequently\nomits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a\nconversation, syntax and import errors persist across turns; however,\ndocumentation quality in Java improves by up to 14.7%, and import handling in\nPython improves by 3.7% over 5 turns. Prompts that point out mistakes in code\ngenerated in prior turns and explicitly request a fix are most effective for\nresolving errors.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.10402.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65dd617096d71c27a19a28d4",
            "avatarUrl": "/avatars/8f599654d0e84b95cfb20a43e75e4bc9.svg",
            "fullname": "Suzhen Zhong",
            "name": "Suzhen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.06482",
            "authors": [
                {
                    "_id": "68cbb3b45a7803ff3be42ec2",
                    "user": {
                        "_id": "68cbaffdbba5bcf511763e56",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FWcmzd2m3h4NXZp0VYenS.png",
                        "isPro": false,
                        "fullname": "Zhongxiang Xie",
                        "user": "zx-Xie",
                        "type": "user"
                    },
                    "name": "Zhongxiang Xie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-18T13:25:51.321Z",
                    "hidden": false
                },
                {
                    "_id": "68cbb3b45a7803ff3be42ec3",
                    "name": "Shuangxi Miao",
                    "hidden": false
                },
                {
                    "_id": "68cbb3b45a7803ff3be42ec4",
                    "name": "Yuhan Jiang",
                    "hidden": false
                },
                {
                    "_id": "68cbb3b45a7803ff3be42ec5",
                    "name": "Zhewei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68cbb3b45a7803ff3be42ec6",
                    "name": "Jing Yao",
                    "hidden": false
                },
                {
                    "_id": "68cbb3b45a7803ff3be42ec7",
                    "name": "Xuecao Li",
                    "hidden": false
                },
                {
                    "_id": "68cbb3b45a7803ff3be42ec8",
                    "name": "Jianxi Huang",
                    "hidden": false
                },
                {
                    "_id": "68cbb3b45a7803ff3be42ec9",
                    "user": {
                        "_id": "65783559dea7e2122d05a773",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783559dea7e2122d05a773/etMIinzsmbxT9EqO6WpkS.png",
                        "isPro": false,
                        "fullname": "Pedram Ghamisi",
                        "user": "pedramghamisi",
                        "type": "user"
                    },
                    "name": "Pedram Ghamisi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T10:27:01.051Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T09:46:33.000Z",
            "submittedOnDailyAt": "2025-09-19T01:25:17.604Z",
            "title": "FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution\n  Remote Sensing Change Detection",
            "submittedOnDailyBy": {
                "_id": "68cbaffdbba5bcf511763e56",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FWcmzd2m3h4NXZp0VYenS.png",
                "isPro": false,
                "fullname": "Zhongxiang Xie",
                "user": "zx-Xie",
                "type": "user"
            },
            "summary": "Change detection from high-resolution remote sensing images lies as a\ncornerstone of Earth observation applications, yet its efficacy is often\ncompromised by two critical challenges. First, false alarms are prevalent as\nmodels misinterpret radiometric variations from temporal shifts (e.g.,\nillumination, season) as genuine changes. Second, a non-negligible semantic gap\nbetween deep abstract features and shallow detail-rich features tends to\nobstruct their effective fusion, culminating in poorly delineated boundaries.\nTo step further in addressing these issues, we propose the Frequency-Spatial\nSynergistic Gated Network (FSG-Net), a novel paradigm that aims to\nsystematically disentangle semantic changes from nuisance variations.\nSpecifically, FSG-Net first operates in the frequency domain, where a\nDiscrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates\npseudo-changes by discerningly processing different frequency components.\nSubsequently, the refined features are enhanced in the spatial domain by a\nSynergistic Temporal-Spatial Attention Module (STSAM), which amplifies the\nsaliency of genuine change regions. To finally bridge the semantic gap, a\nLightweight Gated Fusion Unit (LGFU) leverages high-level semantics to\nselectively gate and integrate crucial details from shallow layers.\nComprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate\nthe superiority of FSG-Net, establishing a new state-of-the-art with F1-scores\nof 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at\nhttps://github.com/zxXie-Air/FSG-Net after a possible publication.",
            "upvotes": 0,
            "discussionId": "68cbb3b45a7803ff3be42eca",
            "githubRepo": "https://github.com/zxXie-Air/FSG-Net",
            "ai_summary": "FSG-Net addresses false alarms and semantic gaps in change detection by using a frequency-spatial synergistic approach with wavelet interaction, attention mechanisms, and gated fusion.",
            "ai_keywords": [
                "Frequency-Spatial Synergistic Gated Network",
                "FSG-Net",
                "Discrepancy-Aware Wavelet Interaction Module",
                "DAWIM",
                "Synergistic Temporal-Spatial Attention Module",
                "STSAM",
                "Lightweight Gated Fusion Unit",
                "LGFU",
                "change detection",
                "high-resolution remote sensing images",
                "Earth observation",
                "false alarms",
                "semantic gap",
                "high-level semantics",
                "shallow layers",
                "CDD",
                "GZ-CD",
                "LEVIR-CD",
                "F1-scores"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-09-08T05:46:33.000Z",
        "title": "FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution\n  Remote Sensing Change Detection",
        "summary": "Change detection from high-resolution remote sensing images lies as a\ncornerstone of Earth observation applications, yet its efficacy is often\ncompromised by two critical challenges. First, false alarms are prevalent as\nmodels misinterpret radiometric variations from temporal shifts (e.g.,\nillumination, season) as genuine changes. Second, a non-negligible semantic gap\nbetween deep abstract features and shallow detail-rich features tends to\nobstruct their effective fusion, culminating in poorly delineated boundaries.\nTo step further in addressing these issues, we propose the Frequency-Spatial\nSynergistic Gated Network (FSG-Net), a novel paradigm that aims to\nsystematically disentangle semantic changes from nuisance variations.\nSpecifically, FSG-Net first operates in the frequency domain, where a\nDiscrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates\npseudo-changes by discerningly processing different frequency components.\nSubsequently, the refined features are enhanced in the spatial domain by a\nSynergistic Temporal-Spatial Attention Module (STSAM), which amplifies the\nsaliency of genuine change regions. To finally bridge the semantic gap, a\nLightweight Gated Fusion Unit (LGFU) leverages high-level semantics to\nselectively gate and integrate crucial details from shallow layers.\nComprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate\nthe superiority of FSG-Net, establishing a new state-of-the-art with F1-scores\nof 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at\nhttps://github.com/zxXie-Air/FSG-Net after a possible publication.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06482.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68cbaffdbba5bcf511763e56",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FWcmzd2m3h4NXZp0VYenS.png",
            "fullname": "Zhongxiang Xie",
            "name": "zx-Xie",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]