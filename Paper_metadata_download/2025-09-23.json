[
    {
        "paper": {
            "id": "2509.17567",
            "authors": [
                {
                    "_id": "68d2047d1ca7156988a8eca6",
                    "user": {
                        "_id": "6002c316698168af3bb9f4a6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6002c316698168af3bb9f4a6/M2J2QFCRc5RhYRoz7OuZN.png",
                        "isPro": false,
                        "fullname": "yangxiao",
                        "user": "YangXiao-nlp",
                        "type": "user"
                    },
                    "name": "Yang Xiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:07:07.291Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8eca7",
                    "user": {
                        "_id": "66d01e4401f2a6b4cd93ad87",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png",
                        "isPro": false,
                        "fullname": "Mohan Jiang",
                        "user": "mhjiang0408",
                        "type": "user"
                    },
                    "name": "Mohan Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T02:42:46.715Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8eca8",
                    "name": "Jie Sun",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8eca9",
                    "user": {
                        "_id": "668e476520e499a0786ea56e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668e476520e499a0786ea56e/lnvd1_UWW9o9ddrR6ehwR.png",
                        "isPro": false,
                        "fullname": "Keyu Li",
                        "user": "weizhihao1",
                        "type": "user"
                    },
                    "name": "Keyu Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:07:10.827Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecaa",
                    "user": {
                        "_id": "66fa544c54f87b607fbffd2e",
                        "avatarUrl": "/avatars/94195dcda0eb68e8fd20d80718744697.svg",
                        "isPro": false,
                        "fullname": "Jifan Lin",
                        "user": "evanlin2570",
                        "type": "user"
                    },
                    "name": "Jifan Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:07:05.060Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecab",
                    "user": {
                        "_id": "6651f8441b1ce9f4a61a09ee",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/YM8m6_FLN_eF5PmqD7BfA.png",
                        "isPro": false,
                        "fullname": "Zhuang Yumin",
                        "user": "happyZYM",
                        "type": "user"
                    },
                    "name": "Yumin Zhuang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T11:23:43.458Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecac",
                    "name": "Ji Zeng",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecad",
                    "user": {
                        "_id": "65900d4ff5a209eeac08b463",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65900d4ff5a209eeac08b463/PJNNBRJIk1qR24oaRLTex.jpeg",
                        "isPro": false,
                        "fullname": "shijie xia",
                        "user": "seven-cat",
                        "type": "user"
                    },
                    "name": "Shijie Xia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T11:23:09.499Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecae",
                    "name": "Qishuo Hua",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecaf",
                    "user": {
                        "_id": "67638cc0d63e4b348e8a5fa3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67638cc0d63e4b348e8a5fa3/BZNlw1uTGUcumCrXKkerx.png",
                        "isPro": false,
                        "fullname": "Xuefeng Li",
                        "user": "drxuefeng",
                        "type": "user"
                    },
                    "name": "Xuefeng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T11:22:55.873Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb0",
                    "user": {
                        "_id": "6734008e534ff3213926e239",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/L91SOmXRLz8rbDRHz4Hxs.png",
                        "isPro": false,
                        "fullname": "Xiaojie Cai",
                        "user": "jerrycxj",
                        "type": "user"
                    },
                    "name": "Xiaojie Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T11:22:16.516Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb1",
                    "user": {
                        "_id": "666292594676a4e4e4d8743f",
                        "avatarUrl": "/avatars/69154b27e67fd4adae651f2e0cffb5b7.svg",
                        "isPro": false,
                        "fullname": "Tongyu Wang",
                        "user": "Sprzwty",
                        "type": "user"
                    },
                    "name": "Tongyu Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T11:22:46.864Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb2",
                    "name": "Yue Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb3",
                    "name": "Liming Liu",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb4",
                    "name": "Xia Wu",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb5",
                    "user": {
                        "_id": "68aed07e662f89a73a744613",
                        "avatarUrl": "/avatars/2827a3703ea44c58c461c87e6bd48953.svg",
                        "isPro": false,
                        "fullname": "jinlong hou",
                        "user": "wenzi001",
                        "type": "user"
                    },
                    "name": "Jinlong Hou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T11:22:06.411Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb6",
                    "user": {
                        "_id": "64358b811860001f14416fc1",
                        "avatarUrl": "/avatars/64a50495b116931124141d3e12fd6d30.svg",
                        "isPro": false,
                        "fullname": "Cheng Yuan",
                        "user": "chengyuan",
                        "type": "user"
                    },
                    "name": "Yuan Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T11:24:02.413Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb7",
                    "name": "Wenjie Li",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb8",
                    "name": "Xiang Wang",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb9",
                    "user": {
                        "_id": "61ad24836da53246bd6ac410",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61ad24836da53246bd6ac410/o-FL-C6B77iB94wyAtTuO.png",
                        "isPro": false,
                        "fullname": "Dequan Wang",
                        "user": "dqwang",
                        "type": "user"
                    },
                    "name": "Dequan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T11:21:41.249Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecba",
                    "name": "Pengfei Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6002c316698168af3bb9f4a6/bxJ1rPnNWVVybfIHpLjfo.png"
            ],
            "publishedAt": "2025-09-22T10:59:32.000Z",
            "submittedOnDailyAt": "2025-09-23T02:56:05.460Z",
            "title": "LIMI: Less is More for Agency",
            "submittedOnDailyBy": {
                "_id": "6002c316698168af3bb9f4a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6002c316698168af3bb9f4a6/M2J2QFCRc5RhYRoz7OuZN.png",
                "isPro": false,
                "fullname": "yangxiao",
                "user": "YangXiao-nlp",
                "type": "user"
            },
            "summary": "We define Agency as the emergent capacity of AI systems to function as\nautonomous agents actively discovering problems, formulating hypotheses, and\nexecuting solutions through self-directed engagement with environments and\ntools. This fundamental capability marks the dawn of the Age of AI Agency,\ndriven by a critical industry shift: the urgent need for AI systems that don't\njust think, but work. While current AI excels at reasoning and generating\nresponses, industries demand autonomous agents that can execute tasks, operate\ntools, and drive real-world outcomes. As agentic intelligence becomes the\ndefining characteristic separating cognitive systems from productive workers,\nefficiently cultivating machine autonomy becomes paramount. Current approaches\nassume that more data yields better agency, following traditional scaling laws\nfrom language modeling. We fundamentally challenge this paradigm. LIMI (Less Is\nMore for Intelligent Agency) demonstrates that agency follows radically\ndifferent development principles. Through strategic focus on collaborative\nsoftware development and scientific research workflows, we show that\nsophisticated agentic intelligence can emerge from minimal but strategically\ncurated demonstrations of autonomous behavior. Using only 78 carefully designed\ntraining samples, LIMI achieves 73.5% on comprehensive agency benchmarks,\ndramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%),\nDeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%).\nMost strikingly, LIMI demonstrates 53.7% improvement over models trained on\n10,000 samples-achieving superior agentic intelligence with 128 times fewer\nsamples. Our findings establish the Agency Efficiency Principle: machine\nautonomy emerges not from data abundance but from strategic curation of\nhigh-quality agentic demonstrations.",
            "upvotes": 65,
            "discussionId": "68d2047e1ca7156988a8ecbb",
            "projectPage": "https://github.com/GAIR-NLP/LIMI",
            "githubRepo": "https://github.com/GAIR-NLP/LIMI",
            "ai_summary": "LIMI demonstrates that sophisticated agentic intelligence can emerge from minimal, strategically curated demonstrations, outperforming data-intensive models on agency benchmarks.",
            "ai_keywords": [
                "Agency",
                "autonomous agents",
                "self-directed engagement",
                "agentic intelligence",
                "LIMI",
                "Agency Efficiency Principle"
            ],
            "githubStars": 23
        },
        "publishedAt": "2025-09-22T06:59:32.000Z",
        "title": "LIMI: Less is More for Agency",
        "summary": "We define Agency as the emergent capacity of AI systems to function as\nautonomous agents actively discovering problems, formulating hypotheses, and\nexecuting solutions through self-directed engagement with environments and\ntools. This fundamental capability marks the dawn of the Age of AI Agency,\ndriven by a critical industry shift: the urgent need for AI systems that don't\njust think, but work. While current AI excels at reasoning and generating\nresponses, industries demand autonomous agents that can execute tasks, operate\ntools, and drive real-world outcomes. As agentic intelligence becomes the\ndefining characteristic separating cognitive systems from productive workers,\nefficiently cultivating machine autonomy becomes paramount. Current approaches\nassume that more data yields better agency, following traditional scaling laws\nfrom language modeling. We fundamentally challenge this paradigm. LIMI (Less Is\nMore for Intelligent Agency) demonstrates that agency follows radically\ndifferent development principles. Through strategic focus on collaborative\nsoftware development and scientific research workflows, we show that\nsophisticated agentic intelligence can emerge from minimal but strategically\ncurated demonstrations of autonomous behavior. Using only 78 carefully designed\ntraining samples, LIMI achieves 73.5% on comprehensive agency benchmarks,\ndramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%),\nDeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%).\nMost strikingly, LIMI demonstrates 53.7% improvement over models trained on\n10,000 samples-achieving superior agentic intelligence with 128 times fewer\nsamples. Our findings establish the Agency Efficiency Principle: machine\nautonomy emerges not from data abundance but from strategic curation of\nhigh-quality agentic demonstrations.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6002c316698168af3bb9f4a6/bxJ1rPnNWVVybfIHpLjfo.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17567.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6002c316698168af3bb9f4a6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6002c316698168af3bb9f4a6/M2J2QFCRc5RhYRoz7OuZN.png",
            "fullname": "yangxiao",
            "name": "YangXiao-nlp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.17765",
            "authors": [
                {
                    "_id": "68d20ca51ca7156988a8ed02",
                    "name": "Jin Xu",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed03",
                    "user": {
                        "_id": "661e577cbac5d981f883b743",
                        "avatarUrl": "/avatars/95e55e9707a6b55594c264081202d7f4.svg",
                        "isPro": false,
                        "fullname": "GuoZhifang",
                        "user": "ZhifangGuo",
                        "type": "user"
                    },
                    "name": "Zhifang Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:10:28.512Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed04",
                    "name": "Hangrui Hu",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed05",
                    "user": {
                        "_id": "62c6a751a71b40cf26f359a8",
                        "avatarUrl": "/avatars/49abd2e71946035452c316d703baaac6.svg",
                        "isPro": false,
                        "fullname": "Yunfei Chu",
                        "user": "faychu",
                        "type": "user"
                    },
                    "name": "Yunfei Chu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:10:44.908Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed06",
                    "name": "Xiong Wang",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed07",
                    "user": {
                        "_id": "6594f06ac04427eb38444bce",
                        "avatarUrl": "/avatars/b13fbf589b25eff038deb3fa12d95871.svg",
                        "isPro": false,
                        "fullname": "Jinzheng He",
                        "user": "jinzheng-he",
                        "type": "user"
                    },
                    "name": "Jinzheng He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:10:52.346Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed08",
                    "name": "Yuxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed09",
                    "name": "Xian Shi",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed0a",
                    "name": "Ting He",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed0b",
                    "name": "Xinfa Zhu",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed0c",
                    "name": "Yuanjun Lv",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed0d",
                    "user": {
                        "_id": "62cfafa91f70d6b8c8ad0722",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62cfafa91f70d6b8c8ad0722/WNbwMB2MQ_4GLzDnHPMwx.png",
                        "isPro": false,
                        "fullname": "Yongqi Wang",
                        "user": "Cyanbox",
                        "type": "user"
                    },
                    "name": "Yongqi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:11:22.199Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed0e",
                    "name": "Dake Guo",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed0f",
                    "name": "He Wang",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed10",
                    "user": {
                        "_id": "659a5c786d20ab21b05be596",
                        "avatarUrl": "/avatars/3d0449e9e50751a338e063c50ee244e9.svg",
                        "isPro": false,
                        "fullname": "Linhan Ma",
                        "user": "Lhma-aslp",
                        "type": "user"
                    },
                    "name": "Linhan Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:11:37.816Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed11",
                    "name": "Pei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed12",
                    "name": "Xinyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed13",
                    "user": {
                        "_id": "666fd382b955b0e655165768",
                        "avatarUrl": "/avatars/66476925471bda2dc9b57f091f245dd9.svg",
                        "isPro": false,
                        "fullname": "hongkun hao",
                        "user": "hongkunhao",
                        "type": "user"
                    },
                    "name": "Hongkun Hao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:11:50.894Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed14",
                    "name": "Zishan Guo",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed15",
                    "name": "Baosong Yang",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed16",
                    "name": "Bin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed17",
                    "user": {
                        "_id": "630388b0d14428368d1616c5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630388b0d14428368d1616c5/Z8O82fDVlB5qkM8Jmq65_.jpeg",
                        "isPro": false,
                        "fullname": "Ziyang Ma",
                        "user": "BoJack",
                        "type": "user"
                    },
                    "name": "Ziyang Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:12:04.210Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed18",
                    "name": "Xipin Wei",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed19",
                    "name": "Shuai Bai",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed1a",
                    "user": {
                        "_id": "6461d675681b2e19b6acb5a5",
                        "avatarUrl": "/avatars/0d95d65d30f6672ec09dc92155324d7f.svg",
                        "isPro": false,
                        "fullname": "Keqin Chen",
                        "user": "chenkq",
                        "type": "user"
                    },
                    "name": "Keqin Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:12:38.284Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed1b",
                    "name": "Xuejing Liu",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed1c",
                    "name": "Peng Wang",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed1d",
                    "user": {
                        "_id": "6417fa211f1f3b0fa811edc0",
                        "avatarUrl": "/avatars/fa9e1ef1472a736c2ceebe12b77d6c89.svg",
                        "isPro": false,
                        "fullname": "Mingkun Yang",
                        "user": "ayumiymk",
                        "type": "user"
                    },
                    "name": "Mingkun Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:12:49.920Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed1e",
                    "user": {
                        "_id": "6434d4989bd5a84b5dd0b0f5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434d4989bd5a84b5dd0b0f5/0Elf9qbfG9Hkgypm9pTGm.jpeg",
                        "isPro": false,
                        "fullname": "Dayiheng Liu",
                        "user": "Losin94",
                        "type": "user"
                    },
                    "name": "Dayiheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:12:57.409Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed1f",
                    "name": "Xingzhang Ren",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed20",
                    "name": "Bo Zheng",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed21",
                    "name": "Rui Men",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed22",
                    "name": "Fan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed23",
                    "name": "Bowen Yu",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed24",
                    "user": {
                        "_id": "67e2432875313f8a046946ea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Syqlq19KVPKYD4t0xsKvM.png",
                        "isPro": false,
                        "fullname": "jianxin Yang",
                        "user": "yangjianxin",
                        "type": "user"
                    },
                    "name": "Jianxin Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:10:19.273Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed25",
                    "name": "Le Yu",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed26",
                    "user": {
                        "_id": "602f88f5e8149a962412a667",
                        "avatarUrl": "/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "Jingren",
                        "type": "user"
                    },
                    "name": "Jingren Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:10:12.277Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed27",
                    "user": {
                        "_id": "620760a26e3b7210c2ff1943",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
                        "isPro": false,
                        "fullname": "Junyang Lin",
                        "user": "JustinLin610",
                        "type": "user"
                    },
                    "name": "Junyang Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:09:59.230Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T13:26:24.000Z",
            "submittedOnDailyAt": "2025-09-23T01:27:51.983Z",
            "title": "Qwen3-Omni Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present Qwen3-Omni, a single multimodal model that, for the first time,\nmaintains state-of-the-art performance across text, image, audio, and video\nwithout any degradation relative to single-modal counterparts. Qwen3-Omni\nmatches the performance of same-sized single-modal models within the Qwen\nseries and excels particularly on audio tasks. Across 36 audio and audio-visual\nbenchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall\nSOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,\nSeed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE\narchitecture that unifies perception and generation across text, images, audio,\nand video, yielding fluent text and natural real-time speech. It supports text\ninteraction in 119 languages, speech understanding in 19 languages, and speech\ngeneration in 10 languages. To reduce first-packet latency in streaming\nsynthesis, Talker autoregressively predicts discrete speech codecs using a\nmulti-codebook scheme. Leveraging the representational capacity of these\ncodebooks, we replace computationally intensive block-wise diffusion with a\nlightweight causal ConvNet, enabling streaming from the first codec frame. In\ncold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet\nlatency of 234 ms. To further strengthen multimodal reasoning, we introduce a\nThinking model that explicitly reasons over inputs from any modality. Since the\nresearch community currently lacks a general-purpose audio captioning model, we\nfine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which\nproduces detailed, low-hallucination captions for arbitrary audio inputs.\nQwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and\nQwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0\nlicense.",
            "upvotes": 52,
            "discussionId": "68d20ca51ca7156988a8ed28",
            "githubRepo": "https://github.com/QwenLM/Qwen3-Omni",
            "ai_summary": "Qwen3-Omni, a multimodal model, achieves state-of-the-art performance across text, image, audio, and video, using a Thinker-Talker MoE architecture and a lightweight causal ConvNet for efficient streaming synthesis.",
            "ai_keywords": [
                "multimodal model",
                "Thinker-Talker MoE architecture",
                "audio tasks",
                "audio-visual benchmarks",
                "open-source SOTA",
                "closed-source models",
                "Gemini-2.5-Pro",
                "Seed-ASR",
                "GPT-4o-Transcribe",
                "fluent text",
                "natural real-time speech",
                "text interaction",
                "speech understanding",
                "speech generation",
                "first-packet latency",
                "Talker",
                "discrete speech codecs",
                "multi-codebook scheme",
                "block-wise diffusion",
                "causal ConvNet",
                "cold-start settings",
                "multimodal reasoning",
                "Thinking model",
                "audio captioning model",
                "Qwen3-Omni-30B-A3B",
                "Qwen3-Omni-30B-A3B-Thinking",
                "Qwen3-Omni-30B-A3B-Captioner"
            ],
            "githubStars": 1478
        },
        "publishedAt": "2025-09-22T09:26:24.000Z",
        "title": "Qwen3-Omni Technical Report",
        "summary": "We present Qwen3-Omni, a single multimodal model that, for the first time,\nmaintains state-of-the-art performance across text, image, audio, and video\nwithout any degradation relative to single-modal counterparts. Qwen3-Omni\nmatches the performance of same-sized single-modal models within the Qwen\nseries and excels particularly on audio tasks. Across 36 audio and audio-visual\nbenchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall\nSOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,\nSeed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE\narchitecture that unifies perception and generation across text, images, audio,\nand video, yielding fluent text and natural real-time speech. It supports text\ninteraction in 119 languages, speech understanding in 19 languages, and speech\ngeneration in 10 languages. To reduce first-packet latency in streaming\nsynthesis, Talker autoregressively predicts discrete speech codecs using a\nmulti-codebook scheme. Leveraging the representational capacity of these\ncodebooks, we replace computationally intensive block-wise diffusion with a\nlightweight causal ConvNet, enabling streaming from the first codec frame. In\ncold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet\nlatency of 234 ms. To further strengthen multimodal reasoning, we introduce a\nThinking model that explicitly reasons over inputs from any modality. Since the\nresearch community currently lacks a general-purpose audio captioning model, we\nfine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which\nproduces detailed, low-hallucination captions for arbitrary audio inputs.\nQwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and\nQwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0\nlicense.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17765.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 108
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.17627",
            "authors": [
                {
                    "_id": "68d20ce31ca7156988a8ed2a",
                    "user": {
                        "_id": "6678045bc3f824dde8e217ff",
                        "avatarUrl": "/avatars/068cc0e16a034cc35b7f058bcba87f4a.svg",
                        "isPro": false,
                        "fullname": "jinshu chen",
                        "user": "JinshuChen",
                        "type": "user"
                    },
                    "name": "Jinshu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:10:58.669Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed2b",
                    "user": {
                        "_id": "6752cd83ffaeeb979db974ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png",
                        "isPro": false,
                        "fullname": "Xinghui Li",
                        "user": "Crayon-Shinchan",
                        "type": "user"
                    },
                    "name": "Xinghui Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:10:50.287Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed2c",
                    "name": "Xu Bai",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed2d",
                    "name": "Tianxiang Ma",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed2e",
                    "name": "Pengze Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed2f",
                    "user": {
                        "_id": "6304e2dabad6ce7fc0287d57",
                        "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
                        "isPro": false,
                        "fullname": "Zhuowei_Chen",
                        "user": "ZhuoweiChen",
                        "type": "user"
                    },
                    "name": "Zhuowei Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:11:09.773Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed30",
                    "name": "Gen Li",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed31",
                    "name": "Lijie Liu",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed32",
                    "name": "Songtao Zhao",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed33",
                    "user": {
                        "_id": "63b415037af2e415f2599c18",
                        "avatarUrl": "/avatars/4afbe7d6d05a702f1beeed9c53e78153.svg",
                        "isPro": false,
                        "fullname": "Bingchuan Li",
                        "user": "lbc402",
                        "type": "user"
                    },
                    "name": "Bingchuan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:11:24.038Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed34",
                    "name": "Qian He",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6752cd83ffaeeb979db974ae/sVfS9qyD1e1SqO9753UK_.mp4"
            ],
            "publishedAt": "2025-09-22T11:35:55.000Z",
            "submittedOnDailyAt": "2025-09-23T01:35:03.582Z",
            "title": "OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion\n  Transformer Models",
            "submittedOnDailyBy": {
                "_id": "6752cd83ffaeeb979db974ae",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png",
                "isPro": false,
                "fullname": "Xinghui Li",
                "user": "Crayon-Shinchan",
                "type": "user"
            },
            "summary": "Recent advances in video insertion based on diffusion models are impressive.\nHowever, existing methods rely on complex control signals but struggle with\nsubject consistency, limiting their practical applicability. In this paper, we\nfocus on the task of Mask-free Video Insertion and aim to resolve three key\nchallenges: data scarcity, subject-scene equilibrium, and insertion\nharmonization. To address the data scarcity, we propose a new data pipeline\nInsertPipe, constructing diverse cross-pair data automatically. Building upon\nour data pipeline, we develop OmniInsert, a novel unified framework for\nmask-free video insertion from both single and multiple subject references.\nSpecifically, to maintain subject-scene equilibrium, we introduce a simple yet\neffective Condition-Specific Feature Injection mechanism to distinctly inject\nmulti-source conditions and propose a novel Progressive Training strategy that\nenables the model to balance feature injection from subjects and source video.\nMeanwhile, we design the Subject-Focused Loss to improve the detailed\nappearance of the subjects. To further enhance insertion harmonization, we\npropose an Insertive Preference Optimization methodology to optimize the model\nby simulating human preferences, and incorporate a Context-Aware Rephraser\nmodule during reference to seamlessly integrate the subject into the original\nscenes. To address the lack of a benchmark for the field, we introduce\nInsertBench, a comprehensive benchmark comprising diverse scenes with\nmeticulously selected subjects. Evaluation on InsertBench indicates OmniInsert\noutperforms state-of-the-art closed-source commercial solutions. The code will\nbe released.",
            "upvotes": 46,
            "discussionId": "68d20ce41ca7156988a8ed35",
            "ai_summary": "OmniInsert addresses challenges in mask-free video insertion using a novel data pipeline, feature injection, progressive training, and context-aware rephrasing, outperforming commercial solutions.",
            "ai_keywords": [
                "diffusion models",
                "Mask-free Video Insertion",
                "data scarcity",
                "Condition-Specific Feature Injection",
                "Progressive Training",
                "Subject-Focused Loss",
                "Insertive Preference Optimization",
                "Context-Aware Rephraser",
                "InsertBench"
            ]
        },
        "publishedAt": "2025-09-22T07:35:55.000Z",
        "title": "OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion\n  Transformer Models",
        "summary": "Recent advances in video insertion based on diffusion models are impressive.\nHowever, existing methods rely on complex control signals but struggle with\nsubject consistency, limiting their practical applicability. In this paper, we\nfocus on the task of Mask-free Video Insertion and aim to resolve three key\nchallenges: data scarcity, subject-scene equilibrium, and insertion\nharmonization. To address the data scarcity, we propose a new data pipeline\nInsertPipe, constructing diverse cross-pair data automatically. Building upon\nour data pipeline, we develop OmniInsert, a novel unified framework for\nmask-free video insertion from both single and multiple subject references.\nSpecifically, to maintain subject-scene equilibrium, we introduce a simple yet\neffective Condition-Specific Feature Injection mechanism to distinctly inject\nmulti-source conditions and propose a novel Progressive Training strategy that\nenables the model to balance feature injection from subjects and source video.\nMeanwhile, we design the Subject-Focused Loss to improve the detailed\nappearance of the subjects. To further enhance insertion harmonization, we\npropose an Insertive Preference Optimization methodology to optimize the model\nby simulating human preferences, and incorporate a Context-Aware Rephraser\nmodule during reference to seamlessly integrate the subject into the original\nscenes. To address the lack of a benchmark for the field, we introduce\nInsertBench, a comprehensive benchmark comprising diverse scenes with\nmeticulously selected subjects. Evaluation on InsertBench indicates OmniInsert\noutperforms state-of-the-art closed-source commercial solutions. The code will\nbe released.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6752cd83ffaeeb979db974ae/sVfS9qyD1e1SqO9753UK_.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17627.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6752cd83ffaeeb979db974ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png",
            "fullname": "Xinghui Li",
            "name": "Crayon-Shinchan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.18091",
            "authors": [
                {
                    "_id": "68d20b8a1ca7156988a8ece7",
                    "user": {
                        "_id": "64db88993725f8d9a908c077",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
                        "isPro": false,
                        "fullname": "Sunhao Dai",
                        "user": "KID-22",
                        "type": "user"
                    },
                    "name": "Sunhao Dai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:06:54.519Z",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ece8",
                    "user": {
                        "_id": "65acfb3a14e6582c30b4ce76",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65acfb3a14e6582c30b4ce76/RhEhePggBtyM0RIIqXQen.jpeg",
                        "isPro": false,
                        "fullname": "TangJiakai",
                        "user": "TangJiakai5704",
                        "type": "user"
                    },
                    "name": "Jiakai Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:06:57.620Z",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ece9",
                    "name": "Jiahua Wu",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecea",
                    "user": {
                        "_id": "67e21ce15d02fcdf8d7abd39",
                        "avatarUrl": "/avatars/01f63e591d327f3d54f51460a599dc80.svg",
                        "isPro": false,
                        "fullname": "kun",
                        "user": "vicowang",
                        "type": "user"
                    },
                    "name": "Kun Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:41:11.576Z",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8eceb",
                    "user": {
                        "_id": "641f18a673cfc036ddbaeccf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/RtOK_qV71NM87A67XFJCN.png",
                        "isPro": false,
                        "fullname": "Yuxuan Zhu",
                        "user": "Ethan7",
                        "type": "user"
                    },
                    "name": "Yuxuan Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:07:00.003Z",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecec",
                    "name": "Bingjun Chen",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8eced",
                    "name": "Bangyang Hong",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecee",
                    "name": "Yu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecef",
                    "name": "Cong Fu",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecf0",
                    "name": "Kangle Wu",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecf1",
                    "name": "Yabo Ni",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecf2",
                    "name": "Anxiang Zeng",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecf3",
                    "name": "Wenjie Wang",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecf4",
                    "name": "Xu Chen",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecf5",
                    "name": "Jun Xu",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecf6",
                    "name": "See-Kiong Ng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T17:59:07.000Z",
            "submittedOnDailyAt": "2025-09-23T01:26:16.690Z",
            "title": "OnePiece: Bringing Context Engineering and Reasoning to Industrial\n  Cascade Ranking System",
            "submittedOnDailyBy": {
                "_id": "64db88993725f8d9a908c077",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
                "isPro": false,
                "fullname": "Sunhao Dai",
                "user": "KID-22",
                "type": "user"
            },
            "summary": "Despite the growing interest in replicating the scaled success of large\nlanguage models (LLMs) in industrial search and recommender systems, most\nexisting industrial efforts remain limited to transplanting Transformer\narchitectures, which bring only incremental improvements over strong Deep\nLearning Recommendation Models (DLRMs). From a first principle perspective, the\nbreakthroughs of LLMs stem not only from their architectures but also from two\ncomplementary mechanisms: context engineering, which enriches raw input queries\nwith contextual cues to better elicit model capabilities, and multi-step\nreasoning, which iteratively refines model outputs through intermediate\nreasoning paths. However, these two mechanisms and their potential to unlock\nsubstantial improvements remain largely underexplored in industrial ranking\nsystems.\n  In this paper, we propose OnePiece, a unified framework that seamlessly\nintegrates LLM-style context engineering and reasoning into both retrieval and\nranking models of industrial cascaded pipelines. OnePiece is built on a pure\nTransformer backbone and further introduces three key innovations: (1)\nstructured context engineering, which augments interaction history with\npreference and scenario signals and unifies them into a structured tokenized\ninput sequence for both retrieval and ranking; (2) block-wise latent reasoning,\nwhich equips the model with multi-step refinement of representations and scales\nreasoning bandwidth via block size; (3) progressive multi-task training, which\nleverages user feedback chains to effectively supervise reasoning steps during\ntraining. OnePiece has been deployed in the main personalized search scenario\nof Shopee and achieves consistent online gains across different key business\nmetrics, including over +2% GMV/UU and a +2.90% increase in advertising\nrevenue.",
            "upvotes": 27,
            "discussionId": "68d20b8b1ca7156988a8ecf7",
            "ai_summary": "OnePiece integrates LLM-style context engineering and reasoning into industrial search and recommendation systems, achieving significant improvements in key business metrics.",
            "ai_keywords": [
                "Transformer architectures",
                "Deep Learning Recommendation Models (DLRMs)",
                "context engineering",
                "multi-step reasoning",
                "structured context engineering",
                "block-wise latent reasoning",
                "progressive multi-task training"
            ]
        },
        "publishedAt": "2025-09-22T13:59:07.000Z",
        "title": "OnePiece: Bringing Context Engineering and Reasoning to Industrial\n  Cascade Ranking System",
        "summary": "Despite the growing interest in replicating the scaled success of large\nlanguage models (LLMs) in industrial search and recommender systems, most\nexisting industrial efforts remain limited to transplanting Transformer\narchitectures, which bring only incremental improvements over strong Deep\nLearning Recommendation Models (DLRMs). From a first principle perspective, the\nbreakthroughs of LLMs stem not only from their architectures but also from two\ncomplementary mechanisms: context engineering, which enriches raw input queries\nwith contextual cues to better elicit model capabilities, and multi-step\nreasoning, which iteratively refines model outputs through intermediate\nreasoning paths. However, these two mechanisms and their potential to unlock\nsubstantial improvements remain largely underexplored in industrial ranking\nsystems.\n  In this paper, we propose OnePiece, a unified framework that seamlessly\nintegrates LLM-style context engineering and reasoning into both retrieval and\nranking models of industrial cascaded pipelines. OnePiece is built on a pure\nTransformer backbone and further introduces three key innovations: (1)\nstructured context engineering, which augments interaction history with\npreference and scenario signals and unifies them into a structured tokenized\ninput sequence for both retrieval and ranking; (2) block-wise latent reasoning,\nwhich equips the model with multi-step refinement of representations and scales\nreasoning bandwidth via block size; (3) progressive multi-task training, which\nleverages user feedback chains to effectively supervise reasoning steps during\ntraining. OnePiece has been deployed in the main personalized search scenario\nof Shopee and achieves consistent online gains across different key business\nmetrics, including over +2% GMV/UU and a +2.90% increase in advertising\nrevenue.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18091.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64db88993725f8d9a908c077",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
            "fullname": "Sunhao Dai",
            "name": "KID-22",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.18056",
            "authors": [
                {
                    "_id": "68d23a2c1ca7156988a8ee4a",
                    "user": {
                        "_id": "67485bfd768f8d6a509d5cd7",
                        "avatarUrl": "/avatars/ad01e707a2066ef673ac7317ccdbb902.svg",
                        "isPro": false,
                        "fullname": "Yunheng Li",
                        "user": "lyhisme",
                        "type": "user"
                    },
                    "name": "Yunheng Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:05:02.046Z",
                    "hidden": false
                },
                {
                    "_id": "68d23a2c1ca7156988a8ee4b",
                    "name": "Jing Cheng",
                    "hidden": false
                },
                {
                    "_id": "68d23a2c1ca7156988a8ee4c",
                    "name": "Shaoyong Jia",
                    "hidden": false
                },
                {
                    "_id": "68d23a2c1ca7156988a8ee4d",
                    "name": "Hangyi Kuang",
                    "hidden": false
                },
                {
                    "_id": "68d23a2c1ca7156988a8ee4e",
                    "name": "Shaohui Jiao",
                    "hidden": false
                },
                {
                    "_id": "68d23a2c1ca7156988a8ee4f",
                    "name": "Qibin Hou",
                    "hidden": false
                },
                {
                    "_id": "68d23a2c1ca7156988a8ee50",
                    "name": "Ming-Ming Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T17:30:15.000Z",
            "submittedOnDailyAt": "2025-09-23T04:45:19.836Z",
            "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning\n  for Video LLMs",
            "submittedOnDailyBy": {
                "_id": "67485bfd768f8d6a509d5cd7",
                "avatarUrl": "/avatars/ad01e707a2066ef673ac7317ccdbb902.svg",
                "isPro": false,
                "fullname": "Yunheng Li",
                "user": "lyhisme",
                "type": "user"
            },
            "summary": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1",
            "upvotes": 25,
            "discussionId": "68d23a2c1ca7156988a8ee51",
            "ai_summary": "TempSamp-R1, a reinforcement fine-tuning framework, enhances multimodal large language models for video temporal grounding by using off-policy supervision and a hybrid Chain-of-Thought training paradigm, achieving state-of-the-art performance on benchmark datasets.",
            "ai_keywords": [
                "reinforcement fine-tuning",
                "multimodal large language models",
                "video temporal grounding",
                "Group Relative Policy Optimization",
                "on-policy sampling",
                "off-policy supervision",
                "non-linear soft advantage computation",
                "Chain-of-Thought training",
                "few-shot generalization"
            ]
        },
        "publishedAt": "2025-09-22T13:30:15.000Z",
        "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning\n  for Video LLMs",
        "summary": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18056.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67485bfd768f8d6a509d5cd7",
            "avatarUrl": "/avatars/ad01e707a2066ef673ac7317ccdbb902.svg",
            "fullname": "Yunheng Li",
            "name": "lyhisme",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.17437",
            "authors": [
                {
                    "_id": "68d22fc51ca7156988a8ee0f",
                    "user": {
                        "_id": "64e85b3edb3767299865e0e3",
                        "avatarUrl": "/avatars/fdbe121535dea940edd2766161393485.svg",
                        "isPro": false,
                        "fullname": "Chen",
                        "user": "Guizhen",
                        "type": "user"
                    },
                    "name": "Guizhen Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:05:26.302Z",
                    "hidden": false
                },
                {
                    "_id": "68d22fc51ca7156988a8ee10",
                    "name": "Weiwen Xu",
                    "hidden": false
                },
                {
                    "_id": "68d22fc51ca7156988a8ee11",
                    "user": {
                        "_id": "64b7cd74ff6d81ae297feded",
                        "avatarUrl": "/avatars/880fbc96cc093f5e901ce84f32a1d21d.svg",
                        "isPro": false,
                        "fullname": "ZHANG HAO",
                        "user": "26hzhang",
                        "type": "user"
                    },
                    "name": "Hao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:05:22.981Z",
                    "hidden": false
                },
                {
                    "_id": "68d22fc51ca7156988a8ee12",
                    "name": "Hou Pong Chan",
                    "hidden": false
                },
                {
                    "_id": "68d22fc51ca7156988a8ee13",
                    "name": "Deli Zhao",
                    "hidden": false
                },
                {
                    "_id": "68d22fc51ca7156988a8ee14",
                    "name": "Anh Tuan Luu",
                    "hidden": false
                },
                {
                    "_id": "68d22fc51ca7156988a8ee15",
                    "user": {
                        "_id": "642eecbf9b2484d7d8526781",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642eecbf9b2484d7d8526781/4IvGbd66s49Wx5pZyZGHA.png",
                        "isPro": false,
                        "fullname": "Yu Rong",
                        "user": "Swrooy",
                        "type": "user"
                    },
                    "name": "Yu Rong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:05:12.297Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T07:28:09.000Z",
            "submittedOnDailyAt": "2025-09-23T04:30:26.856Z",
            "title": "GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "64e85b3edb3767299865e0e3",
                "avatarUrl": "/avatars/fdbe121535dea940edd2766161393485.svg",
                "isPro": false,
                "fullname": "Chen",
                "user": "Guizhen",
                "type": "user"
            },
            "summary": "Recent advancements in reinforcement learning (RL) have enhanced the\nreasoning abilities of large language models (LLMs), yet the impact on\nmultimodal LLMs (MLLMs) is limited. Particularly in vision-intensive tasks like\ngeometric reasoning, MLLMs hallucinate frequently, leading to inaccurate\nreasoning. We attribute this to the perceptual bottleneck in MLLMs, which caps\nthe benefits of reasoning training. To quantify this, we design a\nGeo-Perception Question-Answering (GeoPQA) benchmark, targeting basic geometric\nconcepts and spatial relationships. Experiments on GeoPQA reveal significant\nshortcomings of MLLMs in visual perception, which constrain RL reward signals\nfor effective training. To address this bottleneck, we propose a two-stage RL\ntraining framework by first enhancing the visual perception of geometric\nstructures, then fostering reasoning capabilities. Applied to\nQwen2.5-VL-3B-Instruct, our two-stage training improves geometric reasoning by\n9.7% and geometric problem solving by 9.1%, compared to the direct reasoning\ntraining approach. Our method also generalizes to other vision-intensive\ndomains like figure understanding, highlighting the importance of perceptual\ngrounding in effective MLLM reasoning.",
            "upvotes": 15,
            "discussionId": "68d22fc51ca7156988a8ee16",
            "githubRepo": "https://github.com/DAMO-NLP-SG/GeoPQA",
            "ai_summary": "A two-stage reinforcement learning framework improves geometric reasoning and problem-solving in multimodal language models by first enhancing visual perception.",
            "ai_keywords": [
                "reinforcement learning",
                "large language models",
                "multimodal language models",
                "geometric reasoning",
                "hallucination",
                "perceptual bottleneck",
                "Geo-Perception Question-Answering",
                "visual perception",
                "reasoning capabilities",
                "Qwen2.5-VL-3B-Instruct",
                "figure understanding",
                "perceptual grounding"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-09-22T03:28:09.000Z",
        "title": "GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric\n  Reasoning",
        "summary": "Recent advancements in reinforcement learning (RL) have enhanced the\nreasoning abilities of large language models (LLMs), yet the impact on\nmultimodal LLMs (MLLMs) is limited. Particularly in vision-intensive tasks like\ngeometric reasoning, MLLMs hallucinate frequently, leading to inaccurate\nreasoning. We attribute this to the perceptual bottleneck in MLLMs, which caps\nthe benefits of reasoning training. To quantify this, we design a\nGeo-Perception Question-Answering (GeoPQA) benchmark, targeting basic geometric\nconcepts and spatial relationships. Experiments on GeoPQA reveal significant\nshortcomings of MLLMs in visual perception, which constrain RL reward signals\nfor effective training. To address this bottleneck, we propose a two-stage RL\ntraining framework by first enhancing the visual perception of geometric\nstructures, then fostering reasoning capabilities. Applied to\nQwen2.5-VL-3B-Instruct, our two-stage training improves geometric reasoning by\n9.7% and geometric problem solving by 9.1%, compared to the direct reasoning\ntraining approach. Our method also generalizes to other vision-intensive\ndomains like figure understanding, highlighting the importance of perceptual\ngrounding in effective MLLM reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17437.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e85b3edb3767299865e0e3",
            "avatarUrl": "/avatars/fdbe121535dea940edd2766161393485.svg",
            "fullname": "Chen",
            "name": "Guizhen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.17396",
            "authors": [
                {
                    "_id": "68d1fc2d1ca7156988a8ec7e",
                    "user": {
                        "_id": "63c0e2503bdc86f8108da51b",
                        "avatarUrl": "/avatars/7d47f11992f030b3d831e45102581d1f.svg",
                        "isPro": false,
                        "fullname": "Minsoo Kim",
                        "user": "minsoo2333",
                        "type": "user"
                    },
                    "name": "Minsoo Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:07:14.924Z",
                    "hidden": false
                },
                {
                    "_id": "68d1fc2d1ca7156988a8ec7f",
                    "name": "Arnav Kundu",
                    "hidden": false
                },
                {
                    "_id": "68d1fc2d1ca7156988a8ec80",
                    "name": "Han-Byul Kim",
                    "hidden": false
                },
                {
                    "_id": "68d1fc2d1ca7156988a8ec81",
                    "name": "Richa Dixit",
                    "hidden": false
                },
                {
                    "_id": "68d1fc2d1ca7156988a8ec82",
                    "name": "Minsik Cho",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T06:56:35.000Z",
            "submittedOnDailyAt": "2025-09-23T01:24:27.136Z",
            "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
            "submittedOnDailyBy": {
                "_id": "63c0e2503bdc86f8108da51b",
                "avatarUrl": "/avatars/7d47f11992f030b3d831e45102581d1f.svg",
                "isPro": false,
                "fullname": "Minsoo Kim",
                "user": "minsoo2333",
                "type": "user"
            },
            "summary": "Recent advances in large language models (LLMs) have extended context\nlengths, enabling assistants to sustain long histories for coherent,\npersonalized responses. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly dominates\nunder strict resource constraints. An active line of research for reducing this\noverhead is KV cache compression, which seeks to limit cache size while\npreserving accuracy. Yet existing methods face two major limitations: (i)\nevicting entries after full-context prefill causes unbounded peak memory, and\n(ii) query-dependent eviction narrows the cache to a single query, leading to\ndegraded accuracy in multi-turn conversations. We introduce EpiCache, a\ntraining-free KV cache management framework for long conversational question\nanswering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth\nthrough block-wise prefill and preserves topic-relevant context via episodic KV\ncompression, which clusters conversation history into coherent episodes and\napplies episode-specific KV cache eviction. We further design an adaptive\nlayer-wise budget allocation strategy that measures each layer's sensitivity to\neviction and distributes the memory budget across layers accordingly. Across\nthree LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over\nrecent baselines, sustains near-full KV accuracy under 4-6x compression, and\nreduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints.",
            "upvotes": 15,
            "discussionId": "68d1fc2e1ca7156988a8ec83",
            "ai_summary": "EpiCache is a KV cache management framework for long conversational question answering that reduces memory usage and improves accuracy through block-wise prefill, episodic KV compression, and adaptive layer-wise budget allocation.",
            "ai_keywords": [
                "Key-Value (KV) caching",
                "KV cache compression",
                "block-wise prefill",
                "episodic KV compression",
                "adaptive layer-wise budget allocation",
                "LongConvQA",
                "multi-turn conversations"
            ]
        },
        "publishedAt": "2025-09-22T02:56:35.000Z",
        "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
        "summary": "Recent advances in large language models (LLMs) have extended context\nlengths, enabling assistants to sustain long histories for coherent,\npersonalized responses. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly dominates\nunder strict resource constraints. An active line of research for reducing this\noverhead is KV cache compression, which seeks to limit cache size while\npreserving accuracy. Yet existing methods face two major limitations: (i)\nevicting entries after full-context prefill causes unbounded peak memory, and\n(ii) query-dependent eviction narrows the cache to a single query, leading to\ndegraded accuracy in multi-turn conversations. We introduce EpiCache, a\ntraining-free KV cache management framework for long conversational question\nanswering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth\nthrough block-wise prefill and preserves topic-relevant context via episodic KV\ncompression, which clusters conversation history into coherent episodes and\napplies episode-specific KV cache eviction. We further design an adaptive\nlayer-wise budget allocation strategy that measures each layer's sensitivity to\neviction and distributes the memory budget across layers accordingly. Across\nthree LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over\nrecent baselines, sustains near-full KV accuracy under 4-6x compression, and\nreduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17396.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63c0e2503bdc86f8108da51b",
            "avatarUrl": "/avatars/7d47f11992f030b3d831e45102581d1f.svg",
            "fullname": "Minsoo Kim",
            "name": "minsoo2333",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.16117",
            "authors": [
                {
                    "_id": "68d0bc4f8adc5cd018d15afb",
                    "user": {
                        "_id": "652bf7edc3cba555d5673c6e",
                        "avatarUrl": "/avatars/78f6416c30203b30671f8423f061c657.svg",
                        "isPro": true,
                        "fullname": "Kaiwen Zheng",
                        "user": "worstcoder",
                        "type": "user"
                    },
                    "name": "Kaiwen Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:12:30.463Z",
                    "hidden": false
                },
                {
                    "_id": "68d0bc4f8adc5cd018d15afc",
                    "user": {
                        "_id": "65571135bfb62d747abc8129",
                        "avatarUrl": "/avatars/5f4542daa34597f17e6280b9cce18c91.svg",
                        "isPro": false,
                        "fullname": "Hugging",
                        "user": "ChenDRAG",
                        "type": "user"
                    },
                    "name": "Huayu Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T15:15:47.039Z",
                    "hidden": false
                },
                {
                    "_id": "68d0bc4f8adc5cd018d15afd",
                    "name": "Haotian Ye",
                    "hidden": false
                },
                {
                    "_id": "68d0bc4f8adc5cd018d15afe",
                    "name": "Haoxiang Wang",
                    "hidden": false
                },
                {
                    "_id": "68d0bc4f8adc5cd018d15aff",
                    "user": {
                        "_id": "6732d5dea24987c43bfbafd8",
                        "avatarUrl": "/avatars/1581373b9de5069975716932fceb976b.svg",
                        "isPro": false,
                        "fullname": "Qinsheng Zhang",
                        "user": "qsh-zh",
                        "type": "user"
                    },
                    "name": "Qinsheng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:12:52.095Z",
                    "hidden": false
                },
                {
                    "_id": "68d0bc4f8adc5cd018d15b00",
                    "name": "Kai Jiang",
                    "hidden": false
                },
                {
                    "_id": "68d0bc4f8adc5cd018d15b01",
                    "name": "Hang Su",
                    "hidden": false
                },
                {
                    "_id": "68d0bc4f8adc5cd018d15b02",
                    "user": {
                        "_id": "62f6e244329d4d014d1f4ac5",
                        "avatarUrl": "/avatars/5a8b2bb063c2ebc340504b22530f6811.svg",
                        "isPro": false,
                        "fullname": "Stefano Ermon",
                        "user": "ermonste",
                        "type": "user"
                    },
                    "name": "Stefano Ermon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:12:59.507Z",
                    "hidden": false
                },
                {
                    "_id": "68d0bc4f8adc5cd018d15b03",
                    "name": "Jun Zhu",
                    "hidden": false
                },
                {
                    "_id": "68d0bc4f8adc5cd018d15b04",
                    "user": {
                        "_id": "62f049afdf4b93aad5c7f2d6",
                        "avatarUrl": "/avatars/e272e58ad996733d7098e50248e5b57e.svg",
                        "isPro": false,
                        "fullname": "Ming-Yu Liu",
                        "user": "mingyuliutw",
                        "type": "user"
                    },
                    "name": "Ming-Yu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:13:08.835Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-19T16:09:33.000Z",
            "submittedOnDailyAt": "2025-09-23T00:57:40.436Z",
            "title": "DiffusionNFT: Online Diffusion Reinforcement with Forward Process",
            "submittedOnDailyBy": {
                "_id": "652bf7edc3cba555d5673c6e",
                "avatarUrl": "/avatars/78f6416c30203b30671f8423f061c657.svg",
                "isPro": true,
                "fullname": "Kaiwen Zheng",
                "user": "worstcoder",
                "type": "user"
            },
            "summary": "Online reinforcement learning (RL) has been central to post-training language\nmodels, but its extension to diffusion models remains challenging due to\nintractable likelihoods. Recent works discretize the reverse sampling process\nto enable GRPO-style training, yet they inherit fundamental drawbacks,\nincluding solver restrictions, forward-reverse inconsistency, and complicated\nintegration with classifier-free guidance (CFG). We introduce Diffusion\nNegative-aware FineTuning (DiffusionNFT), a new online RL paradigm that\noptimizes diffusion models directly on the forward process via flow matching.\nDiffusionNFT contrasts positive and negative generations to define an implicit\npolicy improvement direction, naturally incorporating reinforcement signals\ninto the supervised learning objective. This formulation enables training with\narbitrary black-box solvers, eliminates the need for likelihood estimation, and\nrequires only clean images rather than sampling trajectories for policy\noptimization. DiffusionNFT is up to 25times more efficient than FlowGRPO in\nhead-to-head comparisons, while being CFG-free. For instance, DiffusionNFT\nimproves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO\nachieves 0.95 with over 5k steps and additional CFG employment. By leveraging\nmultiple reward models, DiffusionNFT significantly boosts the performance of\nSD3.5-Medium in every benchmark tested.",
            "upvotes": 15,
            "discussionId": "68d0bc508adc5cd018d15b05",
            "ai_summary": "Diffusion Negative-aware FineTuning (DiffusionNFT) optimizes diffusion models directly on the forward process via flow matching, improving efficiency and performance compared to existing methods.",
            "ai_keywords": [
                "online reinforcement learning",
                "diffusion models",
                "GRPO-style training",
                "reverse sampling process",
                "flow matching",
                "implicit policy improvement",
                "supervised learning objective",
                "black-box solvers",
                "likelihood estimation",
                "GenEval score",
                "reward models",
                "SD3.5-Medium"
            ]
        },
        "publishedAt": "2025-09-19T12:09:33.000Z",
        "title": "DiffusionNFT: Online Diffusion Reinforcement with Forward Process",
        "summary": "Online reinforcement learning (RL) has been central to post-training language\nmodels, but its extension to diffusion models remains challenging due to\nintractable likelihoods. Recent works discretize the reverse sampling process\nto enable GRPO-style training, yet they inherit fundamental drawbacks,\nincluding solver restrictions, forward-reverse inconsistency, and complicated\nintegration with classifier-free guidance (CFG). We introduce Diffusion\nNegative-aware FineTuning (DiffusionNFT), a new online RL paradigm that\noptimizes diffusion models directly on the forward process via flow matching.\nDiffusionNFT contrasts positive and negative generations to define an implicit\npolicy improvement direction, naturally incorporating reinforcement signals\ninto the supervised learning objective. This formulation enables training with\narbitrary black-box solvers, eliminates the need for likelihood estimation, and\nrequires only clean images rather than sampling trajectories for policy\noptimization. DiffusionNFT is up to 25times more efficient than FlowGRPO in\nhead-to-head comparisons, while being CFG-free. For instance, DiffusionNFT\nimproves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO\nachieves 0.95 with over 5k steps and additional CFG employment. By leveraging\nmultiple reward models, DiffusionNFT significantly boosts the performance of\nSD3.5-Medium in every benchmark tested.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.16117.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652bf7edc3cba555d5673c6e",
            "avatarUrl": "/avatars/78f6416c30203b30671f8423f061c657.svg",
            "fullname": "Kaiwen Zheng",
            "name": "worstcoder",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.16941",
            "authors": [
                {
                    "_id": "68d2125b1ca7156988a8ed84",
                    "name": "Xiang Deng",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed85",
                    "name": "Jeff Da",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed86",
                    "name": "Edwin Pan",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed87",
                    "name": "Yannis Yiming He",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed88",
                    "name": "Charles Ide",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed89",
                    "name": "Kanak Garg",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed8a",
                    "user": {
                        "_id": "64cc2820ec160b67ca975aef",
                        "avatarUrl": "/avatars/7f8c52bed96a08eb59c8f76fc6988afb.svg",
                        "isPro": false,
                        "fullname": "Niklas Lauffer",
                        "user": "nlauffer",
                        "type": "user"
                    },
                    "name": "Niklas Lauffer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:13:43.269Z",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed8b",
                    "name": "Andrew Park",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed8c",
                    "name": "Nitin Pasari",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed8d",
                    "name": "Chetan Rane",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed8e",
                    "name": "Karmini Sampath",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed8f",
                    "user": {
                        "_id": "6848eb0671d4a5a3d5bbfe89",
                        "avatarUrl": "/avatars/b1965ea0151318c51b385c9fed473d77.svg",
                        "isPro": false,
                        "fullname": "Maya Krishnan",
                        "user": "MayaKrishnan",
                        "type": "user"
                    },
                    "name": "Maya Krishnan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:13:50.713Z",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed90",
                    "user": {
                        "_id": "6303d604eedc089484c59c43",
                        "avatarUrl": "/avatars/cca9a85dcffe61f1489b5acde2294536.svg",
                        "isPro": false,
                        "fullname": "Srivatsa Kundurthy",
                        "user": "srivatsa-kundurthy",
                        "type": "user"
                    },
                    "name": "Srivatsa Kundurthy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:14:00.973Z",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed91",
                    "name": "Sean Hendryx",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed92",
                    "name": "Zifan Wang",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed93",
                    "user": {
                        "_id": "63c47e9c8d95a5c7706a3249",
                        "avatarUrl": "/avatars/68a2605cc5608dd61e991310fba3b4c0.svg",
                        "isPro": false,
                        "fullname": "Chen Bo Calvin Zhang",
                        "user": "calvincbzhang",
                        "type": "user"
                    },
                    "name": "Chen Bo Calvin Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:13:37.677Z",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed94",
                    "name": "Noah Jacobson",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed95",
                    "name": "Bing Liu",
                    "hidden": false
                },
                {
                    "_id": "68d2125b1ca7156988a8ed96",
                    "name": "Brad Kenstler",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-21T06:28:17.000Z",
            "submittedOnDailyAt": "2025-09-23T01:52:13.007Z",
            "title": "SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering\n  Tasks?",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce SWE-Bench Pro, a substantially more challenging benchmark that\nbuilds upon the best practices of SWE-BENCH [25], but is explicitly designed to\ncapture realistic, complex, enterprise-level problems beyond the scope of\nSWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of\n41 actively maintained repositories spanning business applications, B2B\nservices, and developer tools. The benchmark is partitioned into a public set\nwith open access to problems sourced from 11 repositories, a held-out set of 12\nrepositories and a commercial set of 18 proprietary repositories where we have\nformal partnership agreements with early-stage startups. Problems in the\nheld-out and the commercial set are not publicly accessible, but we release\nresults on the commercial set. Our benchmark features long-horizon tasks that\nmay require hours to days for a professional software engineer to complete,\noften involving patches across multiple files and substantial code\nmodifications. All tasks are human-verified and augmented with sufficient\ncontext to ensure resolvability. In our evaluation of widely used coding\nmodels, under a unified scaffold, we observe that their performance on\nSWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest\nscore to date at 23.3%. To better understand these limitations, we cluster the\nfailure modes observed in the collected agent trajectories for a clearer\ncharacterization of the error patterns exhibited by current models. Overall,\nSWE-BENCH PRO provides a contamination-resistant testbed that more faithfully\ncaptures the complexity and diversity of real-world software development,\nadvancing the pursuit of truly autonomous software engineering agents at a\nprofessional level.",
            "upvotes": 14,
            "discussionId": "68d2125c1ca7156988a8ed97",
            "projectPage": "https://scale.com/research/swe_bench_pro",
            "ai_summary": "SWE-Bench Pro is a challenging benchmark for coding models, featuring complex, enterprise-level problems that require substantial code modifications, with performance evaluations showing significant limitations in current models.",
            "ai_keywords": [
                ""
            ]
        },
        "publishedAt": "2025-09-21T02:28:17.000Z",
        "title": "SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering\n  Tasks?",
        "summary": "We introduce SWE-Bench Pro, a substantially more challenging benchmark that\nbuilds upon the best practices of SWE-BENCH [25], but is explicitly designed to\ncapture realistic, complex, enterprise-level problems beyond the scope of\nSWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of\n41 actively maintained repositories spanning business applications, B2B\nservices, and developer tools. The benchmark is partitioned into a public set\nwith open access to problems sourced from 11 repositories, a held-out set of 12\nrepositories and a commercial set of 18 proprietary repositories where we have\nformal partnership agreements with early-stage startups. Problems in the\nheld-out and the commercial set are not publicly accessible, but we release\nresults on the commercial set. Our benchmark features long-horizon tasks that\nmay require hours to days for a professional software engineer to complete,\noften involving patches across multiple files and substantial code\nmodifications. All tasks are human-verified and augmented with sufficient\ncontext to ensure resolvability. In our evaluation of widely used coding\nmodels, under a unified scaffold, we observe that their performance on\nSWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest\nscore to date at 23.3%. To better understand these limitations, we cluster the\nfailure modes observed in the collected agent trajectories for a clearer\ncharacterization of the error patterns exhibited by current models. Overall,\nSWE-BENCH PRO provides a contamination-resistant testbed that more faithfully\ncaptures the complexity and diversity of real-world software development,\nadvancing the pursuit of truly autonomous software engineering agents at a\nprofessional level.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.16941.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 108
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.18084",
            "authors": [
                {
                    "_id": "68d201a61ca7156988a8ec95",
                    "name": "Jiawen Tian",
                    "hidden": false
                },
                {
                    "_id": "68d201a61ca7156988a8ec96",
                    "name": "Liqun Huang",
                    "hidden": false
                },
                {
                    "_id": "68d201a61ca7156988a8ec97",
                    "user": {
                        "_id": "68d206c0a6f8ea66da0d416d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bcKpFMN2HwqTB41CDFJGP.png",
                        "isPro": false,
                        "fullname": "czr",
                        "user": "cuizhongren",
                        "type": "user"
                    },
                    "name": "Zhongren Cui",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T02:40:28.782Z",
                    "hidden": false
                },
                {
                    "_id": "68d201a61ca7156988a8ec98",
                    "name": "Jingchao Qiao",
                    "hidden": false
                },
                {
                    "_id": "68d201a61ca7156988a8ec99",
                    "name": "Jiafeng Xu",
                    "hidden": false
                },
                {
                    "_id": "68d201a61ca7156988a8ec9a",
                    "name": "Xiao Ma",
                    "hidden": false
                },
                {
                    "_id": "68d201a61ca7156988a8ec9b",
                    "name": "Zeyu Ren",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6478597d91398856110a6738/8aoHWwElrdM0QhSVdIuVo.mp4"
            ],
            "publishedAt": "2025-09-22T17:57:07.000Z",
            "submittedOnDailyAt": "2025-09-23T00:54:36.399Z",
            "title": "ByteWrist: A Parallel Robotic Wrist Enabling Flexible and\n  Anthropomorphic Motion for Confined Spaces",
            "submittedOnDailyBy": {
                "_id": "6478597d91398856110a6738",
                "avatarUrl": "/avatars/c3bc61eb7554ac21946b424e1314f1a7.svg",
                "isPro": false,
                "fullname": "Xiao Ma",
                "user": "yusufma555",
                "type": "user"
            },
            "summary": "This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic\nparallel wrist for robotic manipulation. ByteWrist addresses the critical\nlimitations of existing serial and parallel wrists in narrow-space operations\nthrough a compact three-stage parallel drive mechanism integrated with\narc-shaped end linkages. The design achieves precise RPY (Roll-Pitch-Yaw)\nmotion while maintaining exceptional compactness, making it particularly\nsuitable for complex unstructured environments such as home services, medical\nassistance, and precision assembly. The key innovations include: (1) a nested\nthree-stage motor-driven linkages that minimize volume while enabling\nindependent multi-DOF control, (2) arc-shaped end linkages that optimize force\ntransmission and expand motion range, and (3) a central supporting ball\nfunctioning as a spherical joint that enhances structural stiffness without\ncompromising flexibility. Meanwhile, we present comprehensive kinematic\nmodeling including forward / inverse kinematics and a numerical Jacobian\nsolution for precise control. Empirically, we observe ByteWrist demonstrates\nstrong performance in narrow-space maneuverability and dual-arm cooperative\nmanipulation tasks, outperforming Kinova-based systems. Results indicate\nsignificant improvements in compactness, efficiency, and stiffness compared to\ntraditional designs, establishing ByteWrist as a promising solution for\nnext-generation robotic manipulation in constrained environments.",
            "upvotes": 11,
            "discussionId": "68d201a61ca7156988a8ec9c",
            "projectPage": "https://bytewrist.github.io/"
        },
        "publishedAt": "2025-09-22T13:57:07.000Z",
        "title": "ByteWrist: A Parallel Robotic Wrist Enabling Flexible and\n  Anthropomorphic Motion for Confined Spaces",
        "summary": "This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic\nparallel wrist for robotic manipulation. ByteWrist addresses the critical\nlimitations of existing serial and parallel wrists in narrow-space operations\nthrough a compact three-stage parallel drive mechanism integrated with\narc-shaped end linkages. The design achieves precise RPY (Roll-Pitch-Yaw)\nmotion while maintaining exceptional compactness, making it particularly\nsuitable for complex unstructured environments such as home services, medical\nassistance, and precision assembly. The key innovations include: (1) a nested\nthree-stage motor-driven linkages that minimize volume while enabling\nindependent multi-DOF control, (2) arc-shaped end linkages that optimize force\ntransmission and expand motion range, and (3) a central supporting ball\nfunctioning as a spherical joint that enhances structural stiffness without\ncompromising flexibility. Meanwhile, we present comprehensive kinematic\nmodeling including forward / inverse kinematics and a numerical Jacobian\nsolution for precise control. Empirically, we observe ByteWrist demonstrates\nstrong performance in narrow-space maneuverability and dual-arm cooperative\nmanipulation tasks, outperforming Kinova-based systems. Results indicate\nsignificant improvements in compactness, efficiency, and stiffness compared to\ntraditional designs, establishing ByteWrist as a promising solution for\nnext-generation robotic manipulation in constrained environments.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6478597d91398856110a6738/8aoHWwElrdM0QhSVdIuVo.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18084.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6478597d91398856110a6738",
            "avatarUrl": "/avatars/c3bc61eb7554ac21946b424e1314f1a7.svg",
            "fullname": "Xiao Ma",
            "name": "yusufma555",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.17985",
            "authors": [
                {
                    "_id": "68d21e5d1ca7156988a8edc5",
                    "user": {
                        "_id": "63040f5c7373aacccd889430",
                        "avatarUrl": "/avatars/29b05c69445c48943f535ad381fb9464.svg",
                        "isPro": false,
                        "fullname": "geonung kim",
                        "user": "comar",
                        "type": "user"
                    },
                    "name": "Geonung Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:15:01.645Z",
                    "hidden": false
                },
                {
                    "_id": "68d21e5d1ca7156988a8edc6",
                    "user": {
                        "_id": "642cec5dfc341371b030adaa",
                        "avatarUrl": "/avatars/13886df0cf19d697d181a0691ac269fc.svg",
                        "isPro": false,
                        "fullname": "Janghyeok Han",
                        "user": "Janghyeok",
                        "type": "user"
                    },
                    "name": "Janghyeok Han",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:15:09.662Z",
                    "hidden": false
                },
                {
                    "_id": "68d21e5d1ca7156988a8edc7",
                    "user": {
                        "_id": "675d3d94036615f450fe818f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Oc4-UMIHyVe0naZ0XGQmj.png",
                        "isPro": false,
                        "fullname": "sunghyun cho",
                        "user": "chosh1110",
                        "type": "user"
                    },
                    "name": "Sunghyun Cho",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:15:17.945Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63040f5c7373aacccd889430/3COO8-8PZ0F305vXjZVjE.mp4"
            ],
            "publishedAt": "2025-09-22T16:28:47.000Z",
            "submittedOnDailyAt": "2025-09-23T02:48:15.077Z",
            "title": "VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video\n  Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "63040f5c7373aacccd889430",
                "avatarUrl": "/avatars/29b05c69445c48943f535ad381fb9464.svg",
                "isPro": false,
                "fullname": "geonung kim",
                "user": "comar",
                "type": "user"
            },
            "summary": "In this paper, we propose VideoFrom3D, a novel framework for synthesizing\nhigh-quality 3D scene videos from coarse geometry, a camera trajectory, and a\nreference image. Our approach streamlines the 3D graphic design workflow,\nenabling flexible design exploration and rapid production of deliverables. A\nstraightforward approach to synthesizing a video from coarse geometry might\ncondition a video diffusion model on geometric structure. However, existing\nvideo diffusion models struggle to generate high-fidelity results for complex\nscenes due to the difficulty of jointly modeling visual quality, motion, and\ntemporal consistency. To address this, we propose a generative framework that\nleverages the complementary strengths of image and video diffusion models.\nSpecifically, our framework consists of a Sparse Anchor-view Generation (SAG)\nand a Geometry-guided Generative Inbetweening (GGI) module. The SAG module\ngenerates high-quality, cross-view consistent anchor views using an image\ndiffusion model, aided by Sparse Appearance-guided Sampling. Building on these\nanchor views, GGI module faithfully interpolates intermediate frames using a\nvideo diffusion model, enhanced by flow-based camera control and structural\nguidance. Notably, both modules operate without any paired dataset of 3D scene\nmodels and natural images, which is extremely difficult to obtain.\nComprehensive experiments show that our method produces high-quality,\nstyle-consistent scene videos under diverse and challenging scenarios,\noutperforming simple and extended baselines.",
            "upvotes": 10,
            "discussionId": "68d21e5d1ca7156988a8edc8",
            "projectPage": "https://kimgeonung.github.io/VideoFrom3D/",
            "githubRepo": "https://github.com/KIMGEONUNG/VideoFrom3D",
            "ai_summary": "VideoFrom3D synthesizes high-quality 3D scene videos using a combination of image and video diffusion models, achieving style consistency without requiring paired datasets.",
            "ai_keywords": [
                "video diffusion model",
                "image diffusion model",
                "Sparse Anchor-view Generation",
                "Geometry-guided Generative Inbetweening",
                "Sparse Appearance-guided Sampling",
                "flow-based camera control",
                "structural guidance"
            ],
            "githubStars": 25
        },
        "publishedAt": "2025-09-22T12:28:47.000Z",
        "title": "VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video\n  Diffusion Models",
        "summary": "In this paper, we propose VideoFrom3D, a novel framework for synthesizing\nhigh-quality 3D scene videos from coarse geometry, a camera trajectory, and a\nreference image. Our approach streamlines the 3D graphic design workflow,\nenabling flexible design exploration and rapid production of deliverables. A\nstraightforward approach to synthesizing a video from coarse geometry might\ncondition a video diffusion model on geometric structure. However, existing\nvideo diffusion models struggle to generate high-fidelity results for complex\nscenes due to the difficulty of jointly modeling visual quality, motion, and\ntemporal consistency. To address this, we propose a generative framework that\nleverages the complementary strengths of image and video diffusion models.\nSpecifically, our framework consists of a Sparse Anchor-view Generation (SAG)\nand a Geometry-guided Generative Inbetweening (GGI) module. The SAG module\ngenerates high-quality, cross-view consistent anchor views using an image\ndiffusion model, aided by Sparse Appearance-guided Sampling. Building on these\nanchor views, GGI module faithfully interpolates intermediate frames using a\nvideo diffusion model, enhanced by flow-based camera control and structural\nguidance. Notably, both modules operate without any paired dataset of 3D scene\nmodels and natural images, which is extremely difficult to obtain.\nComprehensive experiments show that our method produces high-quality,\nstyle-consistent scene videos under diverse and challenging scenarios,\noutperforming simple and extended baselines.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63040f5c7373aacccd889430/3COO8-8PZ0F305vXjZVjE.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17985.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63040f5c7373aacccd889430",
            "avatarUrl": "/avatars/29b05c69445c48943f535ad381fb9464.svg",
            "fullname": "geonung kim",
            "name": "comar",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.17177",
            "authors": [
                {
                    "_id": "68d20da11ca7156988a8ed3c",
                    "name": "Bowen Qin",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed3d",
                    "name": "Chen Yue",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed3e",
                    "name": "Fang Yin",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed3f",
                    "name": "Hui Wang",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed40",
                    "name": "JG Yao",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed41",
                    "name": "Jiakang Liu",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed42",
                    "user": {
                        "_id": "662f4fed259fa63f77da1f72",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662f4fed259fa63f77da1f72/JOPCmhNeKE0d01tx-le5c.jpeg",
                        "isPro": false,
                        "fullname": "jingshu",
                        "user": "lilaczheng",
                        "type": "user"
                    },
                    "name": "Jing-Shu Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:05:51.988Z",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed43",
                    "user": {
                        "_id": "66f8b76d01bae9d4a4b8ce57",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/3fHkSeKq0YMPgkdYbrfZ7.png",
                        "isPro": false,
                        "fullname": "Miguel Hu",
                        "user": "miguelhuchen",
                        "type": "user"
                    },
                    "name": "Miguel Hu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:17:49.937Z",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed44",
                    "user": {
                        "_id": "62fcd91b03f866462204b591",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fcd91b03f866462204b591/BkAVmJRKzBX_zRimf_yXY.png",
                        "isPro": false,
                        "fullname": "Richeng Xuan",
                        "user": "xuanricheng",
                        "type": "user"
                    },
                    "name": "Richeng Xuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:16:52.606Z",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed45",
                    "name": "Shibei Meng",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed46",
                    "user": {
                        "_id": "6888907625619eb48fd2d72e",
                        "avatarUrl": "/avatars/554c601516adb9983ae03d3ebaa5d226.svg",
                        "isPro": false,
                        "fullname": "Shiqi Zhou",
                        "user": "stephaniezhou",
                        "type": "user"
                    },
                    "name": "Shiqi Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:17:33.950Z",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed47",
                    "user": {
                        "_id": "67411039d191d8c839ed65f4",
                        "avatarUrl": "/avatars/b91223e404e0568859fa6185b6cc6381.svg",
                        "isPro": false,
                        "fullname": "Dai Teng",
                        "user": "tengdai722",
                        "type": "user"
                    },
                    "name": "Teng Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:17:25.696Z",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed48",
                    "name": "Tong-Shuai Ren",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed49",
                    "name": "Wei Cui",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed4a",
                    "name": "Xi Yang",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed4b",
                    "name": "Xialin Du",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed4c",
                    "name": "Xiaojing Xu",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed4d",
                    "name": "Xue Sun",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed4e",
                    "name": "Xuejing Li",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed4f",
                    "user": {
                        "_id": "681967d9704439a74a7e75c9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/k-4GlEN6Tg1Mrvrl4JqBt.png",
                        "isPro": false,
                        "fullname": "Yaming LIU",
                        "user": "Yaming1",
                        "type": "user"
                    },
                    "name": "Yaming Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:16:44.686Z",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed50",
                    "name": "Yesheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed51",
                    "name": "Ying Liu",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed52",
                    "user": {
                        "_id": "629aa3155ab4232a3fe0893e",
                        "avatarUrl": "/avatars/cf2d4a9295b5da9e2e4d2278bbb36040.svg",
                        "isPro": false,
                        "fullname": "Yonghua Lin",
                        "user": "Yonghua",
                        "type": "user"
                    },
                    "name": "Yonghua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:15:04.277Z",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed53",
                    "name": "Yu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed54",
                    "name": "Yunduo Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed55",
                    "name": "Yuwen Luo",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed56",
                    "user": {
                        "_id": "65b21047f5d76208991e463e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b21047f5d76208991e463e/6ML4lLz-vUr1HdWR3Jo-L.jpeg",
                        "isPro": false,
                        "fullname": "Zheqi He",
                        "user": "philokey",
                        "type": "user"
                    },
                    "name": "Zheqi He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:06:44.113Z",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed57",
                    "name": "Zhiyuan He",
                    "hidden": false
                },
                {
                    "_id": "68d20da11ca7156988a8ed58",
                    "name": "Zhongyuan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-21T17:53:30.000Z",
            "submittedOnDailyAt": "2025-09-23T08:33:20.878Z",
            "title": "FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning\n  Models on Automatically Verifiable Textual and Visual Questions",
            "submittedOnDailyBy": {
                "_id": "63a369d98c0c89dcae3b8329",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
                "isPro": true,
                "fullname": "Adina Yakefu",
                "user": "AdinaY",
                "type": "user"
            },
            "summary": "We conduct a moderate-scale contamination-free (to some extent) evaluation of\ncurrent large reasoning models (LRMs) with some preliminary findings. We also\nrelease ROME, our evaluation benchmark for vision language models intended to\ntest reasoning from visual clues. We attach links to the benchmark, evaluation\ndata, and other updates on this website:\nhttps://flageval-baai.github.io/LRM-Eval/",
            "upvotes": 10,
            "discussionId": "68d20da11ca7156988a8ed59",
            "ai_summary": "A contamination-free evaluation of large reasoning models is conducted using the ROME benchmark, which tests reasoning from visual clues in vision language models.",
            "ai_keywords": [
                "large reasoning models",
                "ROME",
                "vision language models"
            ]
        },
        "publishedAt": "2025-09-21T13:53:30.000Z",
        "title": "FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning\n  Models on Automatically Verifiable Textual and Visual Questions",
        "summary": "We conduct a moderate-scale contamination-free (to some extent) evaluation of\ncurrent large reasoning models (LRMs) with some preliminary findings. We also\nrelease ROME, our evaluation benchmark for vision language models intended to\ntest reasoning from visual clues. We attach links to the benchmark, evaluation\ndata, and other updates on this website:\nhttps://flageval-baai.github.io/LRM-Eval/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17177.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a369d98c0c89dcae3b8329",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
            "fullname": "Adina Yakefu",
            "name": "AdinaY",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1200
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.17158",
            "authors": [
                {
                    "_id": "68d2139b1ca7156988a8ed99",
                    "user": {
                        "_id": "63071f424d2c7796a4f30160",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63071f424d2c7796a4f30160/PoXHEWZPnRwLuwMzbUZJx.png",
                        "isPro": false,
                        "fullname": "Pierre Andrews",
                        "user": "mortimerp9",
                        "type": "user"
                    },
                    "name": "Pierre Andrews",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:15:41.549Z",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8ed9a",
                    "user": {
                        "_id": "640c6964c63516d41ecd8f3c",
                        "avatarUrl": "/avatars/ed90945e169c6bfd2042717be71d1557.svg",
                        "isPro": false,
                        "fullname": "Amine Benhalloum",
                        "user": "bamine",
                        "type": "user"
                    },
                    "name": "Amine Benhalloum",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:15:48.664Z",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8ed9b",
                    "user": {
                        "_id": "68cd616834ec980179eb9745",
                        "avatarUrl": "/avatars/19bff2bff2aed4517bb6d99b792dcc2b.svg",
                        "isPro": false,
                        "fullname": "Gerard Moreno-Torres Bertran",
                        "user": "germtf",
                        "type": "user"
                    },
                    "name": "Gerard Moreno-Torres Bertran",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:15:58.251Z",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8ed9c",
                    "user": {
                        "_id": "68d111dcf2534a79c30f2c0b",
                        "avatarUrl": "/avatars/3ec9fbd78146ccbaa8410d03dcea52af.svg",
                        "isPro": false,
                        "fullname": "Matteo Bettini",
                        "user": "matteobettini",
                        "type": "user"
                    },
                    "name": "Matteo Bettini",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:16:06.015Z",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8ed9d",
                    "user": {
                        "_id": "6687ee79eee600e418404cc9",
                        "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg",
                        "isPro": false,
                        "fullname": "Amar Budhiraja",
                        "user": "ambud26",
                        "type": "user"
                    },
                    "name": "Amar Budhiraja",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:16:14.164Z",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8ed9e",
                    "user": {
                        "_id": "67b8749ffa8442592bce008e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ctbzupAxRhcNXDka75ANi.png",
                        "isPro": false,
                        "fullname": "Ricardo Silveira Cabral",
                        "user": "rscabral",
                        "type": "user"
                    },
                    "name": "Ricardo Silveira Cabral",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:16:21.909Z",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8ed9f",
                    "user": {
                        "_id": "66ba2adb59e8e7a95715a2db",
                        "avatarUrl": "/avatars/2035e95dba7ae8c2962e7473fc3247f9.svg",
                        "isPro": false,
                        "fullname": "Virginie Do",
                        "user": "virginiedo",
                        "type": "user"
                    },
                    "name": "Virginie Do",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:16:29.547Z",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8eda0",
                    "user": {
                        "_id": "6463f29b31063f253c221c35",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6463f29b31063f253c221c35/g1851skgauDpfFIJiRRHK.jpeg",
                        "isPro": false,
                        "fullname": "Romain Froger",
                        "user": "RomainFroger",
                        "type": "user"
                    },
                    "name": "Romain Froger",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:16:39.680Z",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8eda1",
                    "name": "Emilien Garreau",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8eda2",
                    "name": "Jean-Baptiste Gaya",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8eda3",
                    "user": {
                        "_id": "6177322d37f32ecb1e2d4cdf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635201569275-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Hugo Laurençon",
                        "user": "HugoLaurencon",
                        "type": "user"
                    },
                    "name": "Hugo Laurençon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:05:47.161Z",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8eda4",
                    "name": "Maxime Lecanu",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8eda5",
                    "name": "Kunal Malkan",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8eda6",
                    "user": {
                        "_id": "652739901eb78901533d7bcc",
                        "avatarUrl": "/avatars/ead039508ca19d0c345eb34bbb39b19e.svg",
                        "isPro": false,
                        "fullname": "Dheeraj Mekala",
                        "user": "dheeraj7596",
                        "type": "user"
                    },
                    "name": "Dheeraj Mekala",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:27:22.404Z",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8eda7",
                    "name": "Pierre Ménard",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8eda8",
                    "user": {
                        "_id": "6458c976f1f5263c2ef0514c",
                        "avatarUrl": "/avatars/0cab7449293be91a4fa08c23533b1b1a.svg",
                        "isPro": false,
                        "fullname": "Grégoire Mialon",
                        "user": "gregmialz",
                        "type": "user"
                    },
                    "name": "Grégoire Mialon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:25:32.267Z",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8eda9",
                    "user": {
                        "_id": "6337054e0267ebcf02637018",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6337054e0267ebcf02637018/eTwc-yBCxli8qVbZEcsKg.jpeg",
                        "isPro": false,
                        "fullname": "Ulyana Piterbarg",
                        "user": "upiter",
                        "type": "user"
                    },
                    "name": "Ulyana Piterbarg",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:25:23.944Z",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8edaa",
                    "user": {
                        "_id": "626463491ed8d81e47ac8527",
                        "avatarUrl": "/avatars/4b9b2e8e1525270f34b4ccce36522705.svg",
                        "isPro": false,
                        "fullname": "Mikhail Plekhanov",
                        "user": "movb",
                        "type": "user"
                    },
                    "name": "Mikhail Plekhanov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:25:16.870Z",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8edab",
                    "user": {
                        "_id": "64e8ac7664e7b5f642f128d3",
                        "avatarUrl": "/avatars/83b1c4a7dfde1ddf524c64c7de3e196a.svg",
                        "isPro": false,
                        "fullname": "Mathieu Rita",
                        "user": "matrita",
                        "type": "user"
                    },
                    "name": "Mathieu Rita",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:25:09.550Z",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8edac",
                    "user": {
                        "_id": "67acf441fc4e74954f317fc8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Ct4t2FzaUeBbx5BtKhf10.png",
                        "isPro": false,
                        "fullname": "Andrey Rusakov",
                        "user": "fonnLippe",
                        "type": "user"
                    },
                    "name": "Andrey Rusakov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:25:01.245Z",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8edad",
                    "name": "Thomas Scialom",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8edae",
                    "user": {
                        "_id": "67f0f493b814154649fdf184",
                        "avatarUrl": "/avatars/9ee3c6f48efee0544081cd9caa78f483.svg",
                        "isPro": false,
                        "fullname": "Vladislav Vorotilov",
                        "user": "vladvo",
                        "type": "user"
                    },
                    "name": "Vladislav Vorotilov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:24:49.984Z",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8edaf",
                    "name": "Mengjue Wang",
                    "hidden": false
                },
                {
                    "_id": "68d2139b1ca7156988a8edb0",
                    "name": "Ian Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-21T16:59:45.000Z",
            "submittedOnDailyAt": "2025-09-23T01:57:29.427Z",
            "title": "ARE: Scaling Up Agent Environments and Evaluations",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce Meta Agents Research Environments (ARE), a research platform for\nscalable creation of environments, integration of synthetic or real\napplications, and execution of agentic orchestrations. ARE provides simple\nabstractions to build complex and diverse environments, each with their own\nrules, tools, content, and verifiers, helping to bridge the gap between model\ndevelopment and real-world deployment. We also propose Gaia2, a benchmark built\nin ARE and designed to measure general agent capabilities. Beyond search and\nexecution, Gaia2 requires agents to handle ambiguities and noise, adapt to\ndynamic environments, collaborate with other agents, and operate under temporal\nconstraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new\nfailure modes that are invisible in static settings. Our experiments show that\nno system dominates across the intelligence spectrum: stronger reasoning often\ncomes at the cost of efficiency, and budget scaling curves plateau,\nhighlighting the need for new architectures and adaptive compute strategies.\nPerhaps more importantly, ARE abstractions enable continuous extension of Gaia2\nto other environments, empowering the community to rapidly create new\nbenchmarks tailored to their domains. In AI's second half, progress\nincreasingly depends on defining meaningful tasks and robust evaluations to\ndrive frontier capabilities forward.",
            "upvotes": 10,
            "discussionId": "68d2139b1ca7156988a8edb1",
            "ai_summary": "Meta Agents Research Environments (ARE) facilitate the creation and execution of complex environments for agent research, and Gaia2, a benchmark built on ARE, evaluates general agent capabilities in dynamic, asynchronous settings.",
            "ai_keywords": [
                "Meta Agents Research Environments",
                "ARE",
                "synthetic applications",
                "real-world deployment",
                "abstractions",
                "complex environments",
                "rules",
                "tools",
                "content",
                "verifiers",
                "Gaia2",
                "benchmark",
                "general agent capabilities",
                "ambiguities",
                "noise",
                "dynamic environments",
                "collaboration",
                "temporal constraints",
                "asynchronous",
                "failure modes",
                "reasoning",
                "efficiency",
                "budget scaling curves",
                "architectures",
                "adaptive compute strategies"
            ]
        },
        "publishedAt": "2025-09-21T12:59:45.000Z",
        "title": "ARE: Scaling Up Agent Environments and Evaluations",
        "summary": "We introduce Meta Agents Research Environments (ARE), a research platform for\nscalable creation of environments, integration of synthetic or real\napplications, and execution of agentic orchestrations. ARE provides simple\nabstractions to build complex and diverse environments, each with their own\nrules, tools, content, and verifiers, helping to bridge the gap between model\ndevelopment and real-world deployment. We also propose Gaia2, a benchmark built\nin ARE and designed to measure general agent capabilities. Beyond search and\nexecution, Gaia2 requires agents to handle ambiguities and noise, adapt to\ndynamic environments, collaborate with other agents, and operate under temporal\nconstraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new\nfailure modes that are invisible in static settings. Our experiments show that\nno system dominates across the intelligence spectrum: stronger reasoning often\ncomes at the cost of efficiency, and budget scaling curves plateau,\nhighlighting the need for new architectures and adaptive compute strategies.\nPerhaps more importantly, ARE abstractions enable continuous extension of Gaia2\nto other environments, empowering the community to rapidly create new\nbenchmarks tailored to their domains. In AI's second half, progress\nincreasingly depends on defining meaningful tasks and robust evaluations to\ndrive frontier capabilities forward.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17158.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 108
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.16596",
            "authors": [
                {
                    "_id": "68d205331ca7156988a8ecbd",
                    "user": {
                        "_id": "66384be673c2c55f2ded89fa",
                        "avatarUrl": "/avatars/1d8721074f0f51fab405f81474f2035f.svg",
                        "isPro": false,
                        "fullname": "Junjie Ye",
                        "user": "Junjie-Ye",
                        "type": "user"
                    },
                    "name": "Junjie Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T02:40:25.849Z",
                    "hidden": false
                },
                {
                    "_id": "68d205331ca7156988a8ecbe",
                    "user": {
                        "_id": "655c6b1abfb531437a54c0e6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/V8Md2mMX83hrSowKk6qMS.jpeg",
                        "isPro": false,
                        "fullname": "Yuming Yang",
                        "user": "Umean",
                        "type": "user"
                    },
                    "name": "Yuming Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:07:02.770Z",
                    "hidden": false
                },
                {
                    "_id": "68d205331ca7156988a8ecbf",
                    "name": "Yang Nan",
                    "hidden": false
                },
                {
                    "_id": "68d205331ca7156988a8ecc0",
                    "name": "Shuo Li",
                    "hidden": false
                },
                {
                    "_id": "68d205331ca7156988a8ecc1",
                    "name": "Qi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d205331ca7156988a8ecc2",
                    "name": "Tao Gui",
                    "hidden": false
                },
                {
                    "_id": "68d205331ca7156988a8ecc3",
                    "name": "Xuanjing Huang",
                    "hidden": false
                },
                {
                    "_id": "68d205331ca7156988a8ecc4",
                    "name": "Peng Wang",
                    "hidden": false
                },
                {
                    "_id": "68d205331ca7156988a8ecc5",
                    "name": "Zhongchao Shi",
                    "hidden": false
                },
                {
                    "_id": "68d205331ca7156988a8ecc6",
                    "name": "Jianping Fan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-20T09:40:32.000Z",
            "submittedOnDailyAt": "2025-09-23T00:56:48.151Z",
            "title": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from\n  Token and Parameter Levels",
            "submittedOnDailyBy": {
                "_id": "655c6b1abfb531437a54c0e6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/V8Md2mMX83hrSowKk6qMS.jpeg",
                "isPro": false,
                "fullname": "Yuming Yang",
                "user": "Umean",
                "type": "user"
            },
            "summary": "Large language models (LLMs) acquire substantial world knowledge during\npre-training, which is further shaped by post-training techniques such as\nsupervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge\nremains underexplored, limiting our ability to control knowledge change\nbehavior in fine-tuned models. To address this gap, we evaluate closed-book\nquestion answering (CBQA) performance across five LLMs from the LLaMA-2 and\nLLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up\nto 14% worse than those fine-tuned on only 240 samples. Furthermore, varying\nthe level of knowledge mastery in the fine-tuning data leads to performance\nfluctuations of over 12%. To investigate these effects, we analyze model\nbehavior at both the token and parameter levels. Our analysis reveals that up\nto 90% of parameter updates during SFT do not contribute to knowledge\nenhancement. Restoring these updates can improve performance on the CBQA task,\ndepending on the characteristics of the fine-tuning data. These insights offer\npractical guidance for developing fine-tuning strategies that more effectively\nstrengthen model knowledge.",
            "upvotes": 8,
            "discussionId": "68d205331ca7156988a8ecc7",
            "ai_summary": "Supervised fine-tuning of large language models can negatively impact closed-book question answering performance, with up to 90% of parameter updates not contributing to knowledge enhancement.",
            "ai_keywords": [
                "large language models",
                "pre-training",
                "supervised fine-tuning",
                "closed-book question answering",
                "token",
                "parameter updates",
                "knowledge enhancement"
            ]
        },
        "publishedAt": "2025-09-20T05:40:32.000Z",
        "title": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from\n  Token and Parameter Levels",
        "summary": "Large language models (LLMs) acquire substantial world knowledge during\npre-training, which is further shaped by post-training techniques such as\nsupervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge\nremains underexplored, limiting our ability to control knowledge change\nbehavior in fine-tuned models. To address this gap, we evaluate closed-book\nquestion answering (CBQA) performance across five LLMs from the LLaMA-2 and\nLLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up\nto 14% worse than those fine-tuned on only 240 samples. Furthermore, varying\nthe level of knowledge mastery in the fine-tuning data leads to performance\nfluctuations of over 12%. To investigate these effects, we analyze model\nbehavior at both the token and parameter levels. Our analysis reveals that up\nto 90% of parameter updates during SFT do not contribute to knowledge\nenhancement. Restoring these updates can improve performance on the CBQA task,\ndepending on the characteristics of the fine-tuning data. These insights offer\npractical guidance for developing fine-tuning strategies that more effectively\nstrengthen model knowledge.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.16596.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655c6b1abfb531437a54c0e6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/V8Md2mMX83hrSowKk6qMS.jpeg",
            "fullname": "Yuming Yang",
            "name": "Umean",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.17671",
            "authors": [
                {
                    "_id": "68d23b0d1ca7156988a8ee53",
                    "user": {
                        "_id": "63241e8f0fc33e3d14dd5277",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63241e8f0fc33e3d14dd5277/QUF4j7L3xZdKRnLRCrA12.jpeg",
                        "isPro": false,
                        "fullname": "SELVA TAŞ",
                        "user": "selvatas",
                        "type": "user"
                    },
                    "name": "Selva Taş",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:04:54.670Z",
                    "hidden": false
                },
                {
                    "_id": "68d23b0d1ca7156988a8ee54",
                    "user": {
                        "_id": "6422eab8e2029ade06eeee2c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
                        "isPro": false,
                        "fullname": "Mahmud ElHuseyni 🇵🇸",
                        "user": "MElHuseyni",
                        "type": "user"
                    },
                    "name": "Mahmut El Huseyni",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:04:59.202Z",
                    "hidden": false
                },
                {
                    "_id": "68d23b0d1ca7156988a8ee55",
                    "user": {
                        "_id": "6464e76894327a238f56d7ff",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464e76894327a238f56d7ff/pKsurCt64lH_0pb_nvjoE.jpeg",
                        "isPro": false,
                        "fullname": "Özay Ezerceli",
                        "user": "ozayezerceli",
                        "type": "user"
                    },
                    "name": "Özay Ezerceli",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T11:22:30.533Z",
                    "hidden": false
                },
                {
                    "_id": "68d23b0d1ca7156988a8ee56",
                    "name": "Reyhan Bayraktar",
                    "hidden": false
                },
                {
                    "_id": "68d23b0d1ca7156988a8ee57",
                    "name": "Fatma Betül Terzioğlu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T12:14:11.000Z",
            "submittedOnDailyAt": "2025-09-23T04:48:36.360Z",
            "title": "Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG\n  Applications",
            "submittedOnDailyBy": {
                "_id": "6422eab8e2029ade06eeee2c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
                "isPro": false,
                "fullname": "Mahmud ElHuseyni 🇵🇸",
                "user": "MElHuseyni",
                "type": "user"
            },
            "summary": "The widespread adoption of Large Language Models (LLMs) has been hindered by\ntheir tendency to hallucinate, generating plausible but factually incorrect\ninformation. While Retrieval-Augmented Generation (RAG) systems attempt to\naddress this issue by grounding responses in external knowledge, hallucination\nremains a persistent challenge, particularly for morphologically complex,\nlow-resource languages like Turkish. This paper introduces Turk-LettuceDetect,\nthe first suite of hallucination detection models specifically designed for\nTurkish RAG applications. Building on the LettuceDetect framework, we formulate\nhallucination detection as a token-level classification task and fine-tune\nthree distinct encoder architectures: a Turkish-specific ModernBERT,\nTurkEmbed4STS, and multilingual EuroBERT. These models were trained on a\nmachine-translated version of the RAGTruth benchmark dataset containing 17,790\ninstances across question answering, data-to-text generation, and summarization\ntasks. Our experimental results show that the ModernBERT-based model achieves\nan F1-score of 0.7266 on the complete test set, with particularly strong\nperformance on structured tasks. The models maintain computational efficiency\nwhile supporting long contexts up to 8,192 tokens, making them suitable for\nreal-time deployment. Comparative analysis reveals that while state-of-the-art\nLLMs demonstrate high recall, they suffer from low precision due to\nover-generation of hallucinated content, underscoring the necessity of\nspecialized detection mechanisms. By releasing our models and translated\ndataset, this work addresses a critical gap in multilingual NLP and establishes\na foundation for developing more reliable and trustworthy AI applications for\nTurkish and other languages.",
            "upvotes": 6,
            "discussionId": "68d23b0d1ca7156988a8ee58",
            "ai_summary": "Turk-LettuceDetect, a suite of hallucination detection models for Turkish RAG applications, achieves high performance using fine-tuned encoder architectures on a machine-translated RAGTruth dataset.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "Retrieval-Augmented Generation (RAG)",
                "hallucination detection",
                "token-level classification",
                "ModernBERT",
                "TurkEmbed4STS",
                "EuroBERT",
                "RAGTruth benchmark dataset",
                "F1-score",
                "computational efficiency",
                "long contexts",
                "multilingual NLP"
            ]
        },
        "publishedAt": "2025-09-22T08:14:11.000Z",
        "title": "Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG\n  Applications",
        "summary": "The widespread adoption of Large Language Models (LLMs) has been hindered by\ntheir tendency to hallucinate, generating plausible but factually incorrect\ninformation. While Retrieval-Augmented Generation (RAG) systems attempt to\naddress this issue by grounding responses in external knowledge, hallucination\nremains a persistent challenge, particularly for morphologically complex,\nlow-resource languages like Turkish. This paper introduces Turk-LettuceDetect,\nthe first suite of hallucination detection models specifically designed for\nTurkish RAG applications. Building on the LettuceDetect framework, we formulate\nhallucination detection as a token-level classification task and fine-tune\nthree distinct encoder architectures: a Turkish-specific ModernBERT,\nTurkEmbed4STS, and multilingual EuroBERT. These models were trained on a\nmachine-translated version of the RAGTruth benchmark dataset containing 17,790\ninstances across question answering, data-to-text generation, and summarization\ntasks. Our experimental results show that the ModernBERT-based model achieves\nan F1-score of 0.7266 on the complete test set, with particularly strong\nperformance on structured tasks. The models maintain computational efficiency\nwhile supporting long contexts up to 8,192 tokens, making them suitable for\nreal-time deployment. Comparative analysis reveals that while state-of-the-art\nLLMs demonstrate high recall, they suffer from low precision due to\nover-generation of hallucinated content, underscoring the necessity of\nspecialized detection mechanisms. By releasing our models and translated\ndataset, this work addresses a critical gap in multilingual NLP and establishes\na foundation for developing more reliable and trustworthy AI applications for\nTurkish and other languages.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17671.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6422eab8e2029ade06eeee2c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
            "fullname": "Mahmud ElHuseyni 🇵🇸",
            "name": "MElHuseyni",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 18
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.17428",
            "authors": [
                {
                    "_id": "68d24c461ca7156988a8eeae",
                    "user": {
                        "_id": "6400208acafc9d549863af59",
                        "avatarUrl": "/avatars/6c383c810a038ce61e803f1d75132471.svg",
                        "isPro": false,
                        "fullname": "Hyesung Jeon",
                        "user": "hjeon2k",
                        "type": "user"
                    },
                    "name": "Hyesung Jeon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:27:46.208Z",
                    "hidden": false
                },
                {
                    "_id": "68d24c461ca7156988a8eeaf",
                    "user": {
                        "_id": "655f207afcbe7329b9502b56",
                        "avatarUrl": "/avatars/521c76b785336fb9d015bb4442b58475.svg",
                        "isPro": false,
                        "fullname": "Seojune Lee",
                        "user": "vantaa32",
                        "type": "user"
                    },
                    "name": "Seojune Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:27:53.916Z",
                    "hidden": false
                },
                {
                    "_id": "68d24c461ca7156988a8eeb0",
                    "user": {
                        "_id": "66734e27a0ea9fb0d7d06e68",
                        "avatarUrl": "/avatars/4c7f2e28af85cd1baf2abcc2280c4835.svg",
                        "isPro": false,
                        "fullname": "Beom Seok Kang",
                        "user": "beomseok-kang",
                        "type": "user"
                    },
                    "name": "Beomseok Kang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:28:01.608Z",
                    "hidden": false
                },
                {
                    "_id": "68d24c461ca7156988a8eeb1",
                    "name": "Yulhwa Kim",
                    "hidden": false
                },
                {
                    "_id": "68d24c461ca7156988a8eeb2",
                    "name": "Jae-Joon Kim",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6400208acafc9d549863af59/aUmk7ue3qOnZzLtHih-XT.png"
            ],
            "publishedAt": "2025-09-22T07:21:41.000Z",
            "submittedOnDailyAt": "2025-09-23T06:15:05.820Z",
            "title": "QWHA: Quantization-Aware Walsh-Hadamard Adaptation for\n  Parameter-Efficient Fine-Tuning on Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6400208acafc9d549863af59",
                "avatarUrl": "/avatars/6c383c810a038ce61e803f1d75132471.svg",
                "isPro": false,
                "fullname": "Hyesung Jeon",
                "user": "hjeon2k",
                "type": "user"
            },
            "summary": "The demand for efficient deployment of large language models (LLMs) has\ndriven interest in quantization, which reduces inference cost, and\nparameter-efficient fine-tuning (PEFT), which lowers training overhead. This\nmotivated the development of quantization-aware PEFT to produce accurate yet\nefficient quantized models. In this setting, reducing quantization error prior\nto fine-tuning is crucial for achieving high model accuracy. However, existing\nmethods that rely on low-rank adaptation suffer from limited representational\ncapacity. Recent Fourier-related transform (FT)-based adapters offer greater\nrepresentational power than low-rank adapters, but their direct integration\ninto quantized models often results in ineffective error reduction and\nincreased computational overhead. To overcome these limitations, we propose\nQWHA, a method that integrates FT-based adapters into quantized models by\nemploying the Walsh-Hadamard Transform (WHT) as the transform kernel, together\nwith a novel adapter initialization scheme incorporating adaptive parameter\nselection and value refinement. We demonstrate that QWHA effectively mitigates\nquantization errors while facilitating fine-tuning, and that its design\nsubstantially reduces computational cost. Experimental results show that QWHA\nconsistently outperforms baselines in low-bit quantization accuracy and\nachieves significant training speedups over existing FT-based adapters. The\ncode is available at https://github.com/vantaa89/qwha.",
            "upvotes": 6,
            "discussionId": "68d24c471ca7156988a8eeb3",
            "githubRepo": "https://github.com/vantaa89/qwha",
            "ai_summary": "QWHA integrates Walsh-Hadamard Transform-based adapters into quantized models to reduce quantization errors and computational overhead, improving low-bit quantization accuracy and training speed.",
            "ai_keywords": [
                "quantization",
                "parameter-efficient fine-tuning",
                "quantization-aware PEFT",
                "quantization error",
                "low-rank adaptation",
                "Fourier-related transform",
                "FT-based adapters",
                "Walsh-Hadamard Transform",
                "WHT",
                "adaptive parameter selection",
                "value refinement",
                "low-bit quantization accuracy",
                "training speedups"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-09-22T03:21:41.000Z",
        "title": "QWHA: Quantization-Aware Walsh-Hadamard Adaptation for\n  Parameter-Efficient Fine-Tuning on Large Language Models",
        "summary": "The demand for efficient deployment of large language models (LLMs) has\ndriven interest in quantization, which reduces inference cost, and\nparameter-efficient fine-tuning (PEFT), which lowers training overhead. This\nmotivated the development of quantization-aware PEFT to produce accurate yet\nefficient quantized models. In this setting, reducing quantization error prior\nto fine-tuning is crucial for achieving high model accuracy. However, existing\nmethods that rely on low-rank adaptation suffer from limited representational\ncapacity. Recent Fourier-related transform (FT)-based adapters offer greater\nrepresentational power than low-rank adapters, but their direct integration\ninto quantized models often results in ineffective error reduction and\nincreased computational overhead. To overcome these limitations, we propose\nQWHA, a method that integrates FT-based adapters into quantized models by\nemploying the Walsh-Hadamard Transform (WHT) as the transform kernel, together\nwith a novel adapter initialization scheme incorporating adaptive parameter\nselection and value refinement. We demonstrate that QWHA effectively mitigates\nquantization errors while facilitating fine-tuning, and that its design\nsubstantially reduces computational cost. Experimental results show that QWHA\nconsistently outperforms baselines in low-bit quantization accuracy and\nachieves significant training speedups over existing FT-based adapters. The\ncode is available at https://github.com/vantaa89/qwha.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6400208acafc9d549863af59/aUmk7ue3qOnZzLtHih-XT.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17428.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6400208acafc9d549863af59",
            "avatarUrl": "/avatars/6c383c810a038ce61e803f1d75132471.svg",
            "fullname": "Hyesung Jeon",
            "name": "hjeon2k",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.18058",
            "authors": [
                {
                    "_id": "68d2aa120e215259d193b08d",
                    "user": {
                        "_id": "645410c50b50e97d3722f1d2",
                        "avatarUrl": "/avatars/e8e079981316b26dbf0f4ada6dcdc8c6.svg",
                        "isPro": false,
                        "fullname": "Alexander Panfilov",
                        "user": "kotekjedi",
                        "type": "user"
                    },
                    "name": "Alexander Panfilov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:37:00.803Z",
                    "hidden": false
                },
                {
                    "_id": "68d2aa120e215259d193b08e",
                    "user": {
                        "_id": "632772f5f0e99f96e028b912",
                        "avatarUrl": "/avatars/3df0ca4a4b8b92dfc1ec4265ab899f56.svg",
                        "isPro": false,
                        "fullname": "Evgenii Kortukov",
                        "user": "kortukov",
                        "type": "user"
                    },
                    "name": "Evgenii Kortukov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:36:54.077Z",
                    "hidden": false
                },
                {
                    "_id": "68d2aa120e215259d193b08f",
                    "user": {
                        "_id": "6695455330bd2a19adca14c0",
                        "avatarUrl": "/avatars/a183124c6f13f1179f202c7462ef5bf7.svg",
                        "isPro": false,
                        "fullname": "Kristina Nikolic",
                        "user": "nkristina",
                        "type": "user"
                    },
                    "name": "Kristina Nikolić",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:36:46.523Z",
                    "hidden": false
                },
                {
                    "_id": "68d2aa120e215259d193b090",
                    "name": "Matthias Bethge",
                    "hidden": false
                },
                {
                    "_id": "68d2aa120e215259d193b091",
                    "name": "Sebastian Lapuschkin",
                    "hidden": false
                },
                {
                    "_id": "68d2aa120e215259d193b092",
                    "user": {
                        "_id": "689af842fdae2231b1f67a68",
                        "avatarUrl": "/avatars/79e7c97a039eaba4fd6bf78c552cbf85.svg",
                        "isPro": false,
                        "fullname": "Wojciech Samek",
                        "user": "wojsamek",
                        "type": "user"
                    },
                    "name": "Wojciech Samek",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:36:30.702Z",
                    "hidden": false
                },
                {
                    "_id": "68d2aa120e215259d193b093",
                    "name": "Ameya Prabhu",
                    "hidden": false
                },
                {
                    "_id": "68d2aa120e215259d193b094",
                    "user": {
                        "_id": "64c225f0129617dbaba5ae88",
                        "avatarUrl": "/avatars/9e2044bf6ba8e1666b1183a961f58aae.svg",
                        "isPro": false,
                        "fullname": "Maksym Andriushchenko",
                        "user": "MaksymAndriushchenko",
                        "type": "user"
                    },
                    "name": "Maksym Andriushchenko",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:36:17.963Z",
                    "hidden": false
                },
                {
                    "_id": "68d2aa120e215259d193b095",
                    "user": {
                        "_id": "63d86dbf3130cadcaf8bdd11",
                        "avatarUrl": "/avatars/29d79a0c6dcec01111ef192fecd0fa7a.svg",
                        "isPro": false,
                        "fullname": "Jonas Geiping",
                        "user": "JonasGeiping",
                        "type": "user"
                    },
                    "name": "Jonas Geiping",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:36:09.894Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T17:30:56.000Z",
            "submittedOnDailyAt": "2025-09-23T12:41:01.789Z",
            "title": "Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLM",
            "submittedOnDailyBy": {
                "_id": "63d86dbf3130cadcaf8bdd11",
                "avatarUrl": "/avatars/29d79a0c6dcec01111ef192fecd0fa7a.svg",
                "isPro": false,
                "fullname": "Jonas Geiping",
                "user": "JonasGeiping",
                "type": "user"
            },
            "summary": "Large language model (LLM) developers aim for their models to be honest,\nhelpful, and harmless. However, when faced with malicious requests, models are\ntrained to refuse, sacrificing helpfulness. We show that frontier LLMs can\ndevelop a preference for dishonesty as a new strategy, even when other options\nare available. Affected models respond to harmful requests with outputs that\nsound harmful but are subtly incorrect or otherwise harmless in practice. This\nbehavior emerges with hard-to-predict variations even within models from the\nsame model family. We find no apparent cause for the propensity to deceive, but\nwe show that more capable models are better at executing this strategy.\nStrategic dishonesty already has a practical impact on safety evaluations, as\nwe show that dishonest responses fool all output-based monitors used to detect\njailbreaks that we test, rendering benchmark scores unreliable. Further,\nstrategic dishonesty can act like a honeypot against malicious users, which\nnoticeably obfuscates prior jailbreak attacks. While output monitors fail, we\nshow that linear probes on internal activations can be used to reliably detect\nstrategic dishonesty. We validate probes on datasets with verifiable outcomes\nand by using their features as steering vectors. Overall, we consider strategic\ndishonesty as a concrete example of a broader concern that alignment of LLMs is\nhard to control, especially when helpfulness and harmlessness conflict.",
            "upvotes": 5,
            "discussionId": "68d2aa120e215259d193b096",
            "ai_summary": "Frontier large language models can develop a preference for strategic dishonesty in response to harmful requests, impacting safety evaluations and acting as a honeypot against malicious users, while internal activation probes can detect this behavior.",
            "ai_keywords": [
                "large language model",
                "LLM",
                "strategic dishonesty",
                "harmful requests",
                "output-based monitors",
                "jailbreaks",
                "benchmark scores",
                "internal activations",
                "linear probes",
                "alignment"
            ]
        },
        "publishedAt": "2025-09-22T13:30:56.000Z",
        "title": "Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLM",
        "summary": "Large language model (LLM) developers aim for their models to be honest,\nhelpful, and harmless. However, when faced with malicious requests, models are\ntrained to refuse, sacrificing helpfulness. We show that frontier LLMs can\ndevelop a preference for dishonesty as a new strategy, even when other options\nare available. Affected models respond to harmful requests with outputs that\nsound harmful but are subtly incorrect or otherwise harmless in practice. This\nbehavior emerges with hard-to-predict variations even within models from the\nsame model family. We find no apparent cause for the propensity to deceive, but\nwe show that more capable models are better at executing this strategy.\nStrategic dishonesty already has a practical impact on safety evaluations, as\nwe show that dishonest responses fool all output-based monitors used to detect\njailbreaks that we test, rendering benchmark scores unreliable. Further,\nstrategic dishonesty can act like a honeypot against malicious users, which\nnoticeably obfuscates prior jailbreak attacks. While output monitors fail, we\nshow that linear probes on internal activations can be used to reliably detect\nstrategic dishonesty. We validate probes on datasets with verifiable outcomes\nand by using their features as steering vectors. Overall, we consider strategic\ndishonesty as a concrete example of a broader concern that alignment of LLMs is\nhard to control, especially when helpfulness and harmlessness conflict.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18058.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63d86dbf3130cadcaf8bdd11",
            "avatarUrl": "/avatars/29d79a0c6dcec01111ef192fecd0fa7a.svg",
            "fullname": "Jonas Geiping",
            "name": "JonasGeiping",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 32
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.15709",
            "authors": [
                {
                    "_id": "68d2322a1ca7156988a8ee18",
                    "name": "Zhuangzhuang He",
                    "hidden": false
                },
                {
                    "_id": "68d2322a1ca7156988a8ee19",
                    "name": "Zhou Kaiyu",
                    "hidden": false
                },
                {
                    "_id": "68d2322a1ca7156988a8ee1a",
                    "name": "Haoyue Bai",
                    "hidden": false
                },
                {
                    "_id": "68d2322a1ca7156988a8ee1b",
                    "name": "Fengbin Zhu",
                    "hidden": false
                },
                {
                    "_id": "68d2322a1ca7156988a8ee1c",
                    "name": "Yonghui Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-19T07:33:50.000Z",
            "submittedOnDailyAt": "2025-09-23T04:11:14.501Z",
            "title": "Understanding Embedding Scaling in Collaborative Filtering",
            "submittedOnDailyBy": {
                "_id": "6621e88fa11ce46061d25a16",
                "avatarUrl": "/avatars/4a9a965a2d0f33e2855d2909a3e162bc.svg",
                "isPro": false,
                "fullname": "Zhuangzhuang He",
                "user": "bruno888",
                "type": "user"
            },
            "summary": "Scaling recommendation models into large recommendation models has become one\nof the most widely discussed topics. Recent efforts focus on components beyond\nthe scaling embedding dimension, as it is believed that scaling embedding may\nlead to performance degradation. Although there have been some initial\nobservations on embedding, the root cause of their non-scalability remains\nunclear. Moreover, whether performance degradation occurs across different\ntypes of models and datasets is still an unexplored area. Regarding the effect\nof embedding dimensions on performance, we conduct large-scale experiments\nacross 10 datasets with varying sparsity levels and scales, using 4\nrepresentative classical architectures. We surprisingly observe two novel\nphenomenon: double-peak and logarithmic. For the former, as the embedding\ndimension increases, performance first improves, then declines, rises again,\nand eventually drops. For the latter, it exhibits a perfect logarithmic curve.\nOur contributions are threefold. First, we discover two novel phenomena when\nscaling collaborative filtering models. Second, we gain an understanding of the\nunderlying causes of the double-peak phenomenon. Lastly, we theoretically\nanalyze the noise robustness of collaborative filtering models, with results\nmatching empirical observations.",
            "upvotes": 5,
            "discussionId": "68d2322a1ca7156988a8ee1d",
            "ai_summary": "Large-scale experiments reveal double-peak and logarithmic performance patterns in collaborative filtering models as embedding dimensions scale, and provide theoretical insights into their causes.",
            "ai_keywords": [
                "collaborative filtering models",
                "embedding dimensions",
                "double-peak phenomenon",
                "logarithmic curve",
                "noise robustness"
            ]
        },
        "publishedAt": "2025-09-19T03:33:50.000Z",
        "title": "Understanding Embedding Scaling in Collaborative Filtering",
        "summary": "Scaling recommendation models into large recommendation models has become one\nof the most widely discussed topics. Recent efforts focus on components beyond\nthe scaling embedding dimension, as it is believed that scaling embedding may\nlead to performance degradation. Although there have been some initial\nobservations on embedding, the root cause of their non-scalability remains\nunclear. Moreover, whether performance degradation occurs across different\ntypes of models and datasets is still an unexplored area. Regarding the effect\nof embedding dimensions on performance, we conduct large-scale experiments\nacross 10 datasets with varying sparsity levels and scales, using 4\nrepresentative classical architectures. We surprisingly observe two novel\nphenomenon: double-peak and logarithmic. For the former, as the embedding\ndimension increases, performance first improves, then declines, rises again,\nand eventually drops. For the latter, it exhibits a perfect logarithmic curve.\nOur contributions are threefold. First, we discover two novel phenomena when\nscaling collaborative filtering models. Second, we gain an understanding of the\nunderlying causes of the double-peak phenomenon. Lastly, we theoretically\nanalyze the noise robustness of collaborative filtering models, with results\nmatching empirical observations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15709.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6621e88fa11ce46061d25a16",
            "avatarUrl": "/avatars/4a9a965a2d0f33e2855d2909a3e162bc.svg",
            "fullname": "Zhuangzhuang He",
            "name": "bruno888",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.18010",
            "authors": [
                {
                    "_id": "68d243a81ca7156988a8ee7e",
                    "user": {
                        "_id": "66309b3833ccd9e68c5d5171",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg",
                        "isPro": false,
                        "fullname": "Sara Papi",
                        "user": "spapi",
                        "type": "user"
                    },
                    "name": "Sara Papi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:37:23.826Z",
                    "hidden": false
                },
                {
                    "_id": "68d243a81ca7156988a8ee7f",
                    "name": "Dennis Fucci",
                    "hidden": false
                },
                {
                    "_id": "68d243a81ca7156988a8ee80",
                    "user": {
                        "_id": "662e58f32b1b529a43c8b2f4",
                        "avatarUrl": "/avatars/bd26a7b26596882c6bc8a7375ff42cac.svg",
                        "isPro": false,
                        "fullname": "Marco Gaido",
                        "user": "mgaido91",
                        "type": "user"
                    },
                    "name": "Marco Gaido",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:37:35.452Z",
                    "hidden": false
                },
                {
                    "_id": "68d243a81ca7156988a8ee81",
                    "user": {
                        "_id": "66350219843f549fdac86347",
                        "avatarUrl": "/avatars/1ad7aa4e8e6d80c5d50bf4de502f11a4.svg",
                        "isPro": false,
                        "fullname": "Matteo Negri",
                        "user": "MNegri",
                        "type": "user"
                    },
                    "name": "Matteo Negri",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:37:42.444Z",
                    "hidden": false
                },
                {
                    "_id": "68d243a81ca7156988a8ee82",
                    "user": {
                        "_id": "662eb48f5ade870b60db4380",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ozHyP-GBIAEhR-l-TkBFb.jpeg",
                        "isPro": false,
                        "fullname": "Luisa Bentivogli",
                        "user": "lubentivogli",
                        "type": "user"
                    },
                    "name": "Luisa Bentivogli",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:37:49.343Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66309b3833ccd9e68c5d5171/ZxzS24_RE16UqGlCYc9v8.png"
            ],
            "publishedAt": "2025-09-22T16:49:26.000Z",
            "submittedOnDailyAt": "2025-09-23T09:46:46.017Z",
            "title": "Cross-Attention is Half Explanation in Speech-to-Text Models",
            "submittedOnDailyBy": {
                "_id": "66309b3833ccd9e68c5d5171",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg",
                "isPro": false,
                "fullname": "Sara Papi",
                "user": "spapi",
                "type": "user"
            },
            "summary": "Cross-attention is a core mechanism in encoder-decoder architectures,\nwidespread in many fields, including speech-to-text (S2T) processing. Its\nscores have been repurposed for various downstream applications--such as\ntimestamp estimation and audio-text alignment--under the assumption that they\nreflect the dependencies between input speech representation and the generated\ntext. While the explanatory nature of attention mechanisms has been widely\ndebated in the broader NLP literature, this assumption remains largely\nunexplored within the speech domain. To address this gap, we assess the\nexplanatory power of cross-attention in S2T models by comparing its scores to\ninput saliency maps derived from feature attribution. Our analysis spans\nmonolingual and multilingual, single-task and multi-task models at multiple\nscales, and shows that attention scores moderately to strongly align with\nsaliency-based explanations, particularly when aggregated across heads and\nlayers. However, it also shows that cross-attention captures only about 50% of\nthe input relevance and, in the best case, only partially reflects how the\ndecoder attends to the encoder's representations--accounting for just 52-75% of\nthe saliency. These findings uncover fundamental limitations in interpreting\ncross-attention as an explanatory proxy, suggesting that it offers an\ninformative yet incomplete view of the factors driving predictions in S2T\nmodels.",
            "upvotes": 4,
            "discussionId": "68d243a81ca7156988a8ee83",
            "ai_summary": "Cross-attention in speech-to-text models aligns moderately with saliency-based explanations but captures only a portion of input relevance and decoder attention.",
            "ai_keywords": [
                "cross-attention",
                "encoder-decoder architectures",
                "speech-to-text",
                "timestamp estimation",
                "audio-text alignment",
                "feature attribution",
                "saliency maps",
                "monolingual",
                "multilingual",
                "single-task",
                "multi-task models"
            ]
        },
        "publishedAt": "2025-09-22T12:49:26.000Z",
        "title": "Cross-Attention is Half Explanation in Speech-to-Text Models",
        "summary": "Cross-attention is a core mechanism in encoder-decoder architectures,\nwidespread in many fields, including speech-to-text (S2T) processing. Its\nscores have been repurposed for various downstream applications--such as\ntimestamp estimation and audio-text alignment--under the assumption that they\nreflect the dependencies between input speech representation and the generated\ntext. While the explanatory nature of attention mechanisms has been widely\ndebated in the broader NLP literature, this assumption remains largely\nunexplored within the speech domain. To address this gap, we assess the\nexplanatory power of cross-attention in S2T models by comparing its scores to\ninput saliency maps derived from feature attribution. Our analysis spans\nmonolingual and multilingual, single-task and multi-task models at multiple\nscales, and shows that attention scores moderately to strongly align with\nsaliency-based explanations, particularly when aggregated across heads and\nlayers. However, it also shows that cross-attention captures only about 50% of\nthe input relevance and, in the best case, only partially reflects how the\ndecoder attends to the encoder's representations--accounting for just 52-75% of\nthe saliency. These findings uncover fundamental limitations in interpreting\ncross-attention as an explanatory proxy, suggesting that it offers an\ninformative yet incomplete view of the factors driving predictions in S2T\nmodels.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66309b3833ccd9e68c5d5171/ZxzS24_RE16UqGlCYc9v8.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18010.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66309b3833ccd9e68c5d5171",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg",
            "fullname": "Sara Papi",
            "name": "spapi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.17818",
            "authors": [
                {
                    "_id": "68d20b381ca7156988a8ece1",
                    "name": "Yiyang Chen",
                    "hidden": false
                },
                {
                    "_id": "68d20b381ca7156988a8ece2",
                    "name": "Xuanhua He",
                    "hidden": false
                },
                {
                    "_id": "68d20b381ca7156988a8ece3",
                    "name": "Xiujun Ma",
                    "hidden": false
                },
                {
                    "_id": "68d20b381ca7156988a8ece4",
                    "name": "Yue Ma",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T14:13:31.000Z",
            "submittedOnDailyAt": "2025-09-23T01:21:45.969Z",
            "title": "ContextFlow: Training-Free Video Object Editing via Adaptive Context\n  Enrichment",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Training-free video object editing aims to achieve precise object-level\nmanipulation, including object insertion, swapping, and deletion. However, it\nfaces significant challenges in maintaining fidelity and temporal consistency.\nExisting methods, often designed for U-Net architectures, suffer from two\nprimary limitations: inaccurate inversion due to first-order solvers, and\ncontextual conflicts caused by crude \"hard\" feature replacement. These issues\nare more challenging in Diffusion Transformers (DiTs), where the unsuitability\nof prior layer-selection heuristics makes effective guidance challenging. To\naddress these limitations, we introduce ContextFlow, a novel training-free\nframework for DiT-based video object editing. In detail, we first employ a\nhigh-order Rectified Flow solver to establish a robust editing foundation. The\ncore of our framework is Adaptive Context Enrichment (for specifying what to\nedit), a mechanism that addresses contextual conflicts. Instead of replacing\nfeatures, it enriches the self-attention context by concatenating Key-Value\npairs from parallel reconstruction and editing paths, empowering the model to\ndynamically fuse information. Additionally, to determine where to apply this\nenrichment (for specifying where to edit), we propose a systematic, data-driven\nanalysis to identify task-specific vital layers. Based on a novel Guidance\nResponsiveness Metric, our method pinpoints the most influential DiT blocks for\ndifferent tasks (e.g., insertion, swapping), enabling targeted and highly\neffective guidance. Extensive experiments show that ContextFlow significantly\noutperforms existing training-free methods and even surpasses several\nstate-of-the-art training-based approaches, delivering temporally coherent,\nhigh-fidelity results.",
            "upvotes": 4,
            "discussionId": "68d20b381ca7156988a8ece5",
            "projectPage": "https://yychen233.github.io/ContextFlow-page/",
            "ai_summary": "ContextFlow, a training-free framework for Diffusion Transformers, enhances video object editing by using a high-order Rectified Flow solver and Adaptive Context Enrichment to achieve precise, temporally consistent, and high-fidelity object manipulation.",
            "ai_keywords": [
                "Diffusion Transformers",
                "DiTs",
                "Rectified Flow solver",
                "Adaptive Context Enrichment",
                "self-attention",
                "Key-Value pairs",
                "Guidance Responsiveness Metric"
            ]
        },
        "publishedAt": "2025-09-22T10:13:31.000Z",
        "title": "ContextFlow: Training-Free Video Object Editing via Adaptive Context\n  Enrichment",
        "summary": "Training-free video object editing aims to achieve precise object-level\nmanipulation, including object insertion, swapping, and deletion. However, it\nfaces significant challenges in maintaining fidelity and temporal consistency.\nExisting methods, often designed for U-Net architectures, suffer from two\nprimary limitations: inaccurate inversion due to first-order solvers, and\ncontextual conflicts caused by crude \"hard\" feature replacement. These issues\nare more challenging in Diffusion Transformers (DiTs), where the unsuitability\nof prior layer-selection heuristics makes effective guidance challenging. To\naddress these limitations, we introduce ContextFlow, a novel training-free\nframework for DiT-based video object editing. In detail, we first employ a\nhigh-order Rectified Flow solver to establish a robust editing foundation. The\ncore of our framework is Adaptive Context Enrichment (for specifying what to\nedit), a mechanism that addresses contextual conflicts. Instead of replacing\nfeatures, it enriches the self-attention context by concatenating Key-Value\npairs from parallel reconstruction and editing paths, empowering the model to\ndynamically fuse information. Additionally, to determine where to apply this\nenrichment (for specifying where to edit), we propose a systematic, data-driven\nanalysis to identify task-specific vital layers. Based on a novel Guidance\nResponsiveness Metric, our method pinpoints the most influential DiT blocks for\ndifferent tasks (e.g., insertion, swapping), enabling targeted and highly\neffective guidance. Extensive experiments show that ContextFlow significantly\noutperforms existing training-free methods and even surpasses several\nstate-of-the-art training-based approaches, delivering temporally coherent,\nhigh-fidelity results.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17818.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 108
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.15248",
            "authors": [
                {
                    "_id": "68d209c41ca7156988a8ecd4",
                    "name": "Zitong Yang",
                    "hidden": false
                },
                {
                    "_id": "68d209c41ca7156988a8ecd5",
                    "name": "Aonan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d209c41ca7156988a8ecd6",
                    "name": "Hong Liu",
                    "hidden": false
                },
                {
                    "_id": "68d209c41ca7156988a8ecd7",
                    "name": "Tatsunori Hashimoto",
                    "hidden": false
                },
                {
                    "_id": "68d209c41ca7156988a8ecd8",
                    "name": "Emmanuel Candès",
                    "hidden": false
                },
                {
                    "_id": "68d209c41ca7156988a8ecd9",
                    "name": "Chong Wang",
                    "hidden": false
                },
                {
                    "_id": "68d209c41ca7156988a8ecda",
                    "name": "Ruoming Pang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-17T22:28:27.000Z",
            "submittedOnDailyAt": "2025-09-23T01:20:05.684Z",
            "title": "Synthetic bootstrapped pretraining",
            "submittedOnDailyBy": {
                "_id": "65f3e68a138c6ab771434e2d",
                "avatarUrl": "/avatars/7bfbdb1949f73b3d8f88ae2ff73900bb.svg",
                "isPro": false,
                "fullname": "Aonan Zhang",
                "user": "AonanZhang",
                "type": "user"
            },
            "summary": "We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM)\npretraining procedure that first learns a model of relations between documents\nfrom the pretraining dataset and then leverages it to synthesize a vast new\ncorpus for joint training. While the standard pretraining teaches LMs to learn\ncausal correlations among tokens within a single document, it is not designed\nto efficiently model the rich, learnable inter-document correlations that can\npotentially lead to better performance. We validate SBP by designing a\ncompute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T\ntokens from scratch. We find SBP consistently improves upon a strong repetition\nbaseline and delivers a significant fraction of performance improvement\nattainable by an oracle upper bound with access to 20x more unique data.\nQualitative analysis reveals that the synthesized documents go beyond mere\nparaphrases -- SBP first abstracts a core concept from the seed material and\nthen crafts a new narration on top of it. Besides strong empirical performance,\nSBP admits a natural Bayesian interpretation: the synthesizer implicitly learns\nto abstract the latent concepts shared between related documents.",
            "upvotes": 4,
            "discussionId": "68d209c51ca7156988a8ecdb",
            "ai_summary": "Synthetic Bootstrapped Pretraining (SBP) enhances language model performance by learning inter-document correlations and synthesizing new training data, leading to significant improvements over standard pretraining methods.",
            "ai_keywords": [
                "Synthetic Bootstrapped Pretraining",
                "SBP",
                "language model",
                "pretraining procedure",
                "inter-document correlations",
                "causal correlations",
                "Bayesian interpretation",
                "latent concepts"
            ]
        },
        "publishedAt": "2025-09-17T18:28:27.000Z",
        "title": "Synthetic bootstrapped pretraining",
        "summary": "We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM)\npretraining procedure that first learns a model of relations between documents\nfrom the pretraining dataset and then leverages it to synthesize a vast new\ncorpus for joint training. While the standard pretraining teaches LMs to learn\ncausal correlations among tokens within a single document, it is not designed\nto efficiently model the rich, learnable inter-document correlations that can\npotentially lead to better performance. We validate SBP by designing a\ncompute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T\ntokens from scratch. We find SBP consistently improves upon a strong repetition\nbaseline and delivers a significant fraction of performance improvement\nattainable by an oracle upper bound with access to 20x more unique data.\nQualitative analysis reveals that the synthesized documents go beyond mere\nparaphrases -- SBP first abstracts a core concept from the seed material and\nthen crafts a new narration on top of it. Besides strong empirical performance,\nSBP admits a natural Bayesian interpretation: the synthesizer implicitly learns\nto abstract the latent concepts shared between related documents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15248.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f3e68a138c6ab771434e2d",
            "avatarUrl": "/avatars/7bfbdb1949f73b3d8f88ae2ff73900bb.svg",
            "fullname": "Aonan Zhang",
            "name": "AonanZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.18095",
            "authors": [
                {
                    "_id": "68d22a351ca7156988a8edfa",
                    "user": {
                        "_id": "67eb818141abf40cd87ab303",
                        "avatarUrl": "/avatars/8fd19f5fcac50946be63d55d265e68b0.svg",
                        "isPro": false,
                        "fullname": "Zilin Xiao",
                        "user": "MrZilinXiao",
                        "type": "user"
                    },
                    "name": "Zilin Xiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:05:31.739Z",
                    "hidden": false
                },
                {
                    "_id": "68d22a351ca7156988a8edfb",
                    "name": "Qi Ma",
                    "hidden": false
                },
                {
                    "_id": "68d22a351ca7156988a8edfc",
                    "name": "Mengting Gu",
                    "hidden": false
                },
                {
                    "_id": "68d22a351ca7156988a8edfd",
                    "name": "Chun-cheng Jason Chen",
                    "hidden": false
                },
                {
                    "_id": "68d22a351ca7156988a8edfe",
                    "name": "Xintao Chen",
                    "hidden": false
                },
                {
                    "_id": "68d22a351ca7156988a8edff",
                    "name": "Vicente Ordonez",
                    "hidden": false
                },
                {
                    "_id": "68d22a351ca7156988a8ee00",
                    "name": "Vijai Mohan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T17:59:42.000Z",
            "submittedOnDailyAt": "2025-09-23T03:34:46.809Z",
            "title": "MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late\n  Interaction",
            "submittedOnDailyBy": {
                "_id": "67eb818141abf40cd87ab303",
                "avatarUrl": "/avatars/8fd19f5fcac50946be63d55d265e68b0.svg",
                "isPro": false,
                "fullname": "Zilin Xiao",
                "user": "MrZilinXiao",
                "type": "user"
            },
            "summary": "Universal multimodal embedding models have achieved great success in\ncapturing semantic relevance between queries and candidates. However, current\nmethods either condense queries and candidates into a single vector,\npotentially limiting the expressiveness for fine-grained information, or\nproduce too many vectors that are prohibitively expensive for multi-vector\nretrieval. In this work, we introduce MetaEmbed, a new framework for multimodal\nretrieval that rethinks how multimodal embeddings are constructed and\ninteracted with at scale. During training, a fixed number of learnable Meta\nTokens are appended to the input sequence. At test-time, their last-layer\ncontextualized representations serve as compact yet expressive multi-vector\nembeddings. Through the proposed Matryoshka Multi-Vector Retrieval training,\nMetaEmbed learns to organize information by granularity across multiple\nvectors. As a result, we enable test-time scaling in multimodal retrieval,\nwhere users can balance retrieval quality against efficiency demands by\nselecting the number of tokens used for indexing and retrieval interactions.\nExtensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and\nthe Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed\nachieves state-of-the-art retrieval performance while scaling robustly to\nmodels with 32B parameters.",
            "upvotes": 3,
            "discussionId": "68d22a351ca7156988a8ee01",
            "ai_summary": "MetaEmbed, a new framework for multimodal retrieval, uses learnable Meta Tokens to provide compact yet expressive multi-vector embeddings, enabling scalable and efficient retrieval performance.",
            "ai_keywords": [
                "MetaEmbed",
                "Meta Tokens",
                "multimodal embeddings",
                "Matryoshka Multi-Vector Retrieval",
                "Massive Multimodal Embedding Benchmark",
                "Visual Document Retrieval Benchmark"
            ]
        },
        "publishedAt": "2025-09-22T13:59:42.000Z",
        "title": "MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late\n  Interaction",
        "summary": "Universal multimodal embedding models have achieved great success in\ncapturing semantic relevance between queries and candidates. However, current\nmethods either condense queries and candidates into a single vector,\npotentially limiting the expressiveness for fine-grained information, or\nproduce too many vectors that are prohibitively expensive for multi-vector\nretrieval. In this work, we introduce MetaEmbed, a new framework for multimodal\nretrieval that rethinks how multimodal embeddings are constructed and\ninteracted with at scale. During training, a fixed number of learnable Meta\nTokens are appended to the input sequence. At test-time, their last-layer\ncontextualized representations serve as compact yet expressive multi-vector\nembeddings. Through the proposed Matryoshka Multi-Vector Retrieval training,\nMetaEmbed learns to organize information by granularity across multiple\nvectors. As a result, we enable test-time scaling in multimodal retrieval,\nwhere users can balance retrieval quality against efficiency demands by\nselecting the number of tokens used for indexing and retrieval interactions.\nExtensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and\nthe Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed\nachieves state-of-the-art retrieval performance while scaling robustly to\nmodels with 32B parameters.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18095.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67eb818141abf40cd87ab303",
            "avatarUrl": "/avatars/8fd19f5fcac50946be63d55d265e68b0.svg",
            "fullname": "Zilin Xiao",
            "name": "MrZilinXiao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.18094",
            "authors": [
                {
                    "_id": "68d2a61a0e215259d193b084",
                    "user": {
                        "_id": "6250eb5c1fd03f78d0ae550f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649470297585-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Ye Liu",
                        "user": "yeliudev",
                        "type": "user"
                    },
                    "name": "Ye Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T14:24:36.336Z",
                    "hidden": false
                },
                {
                    "_id": "68d2a61a0e215259d193b085",
                    "user": {
                        "_id": "61bb00f6c4ac95d207b25f1b",
                        "avatarUrl": "/avatars/3b6eba701d64518d6f694942f5b2e9a9.svg",
                        "isPro": false,
                        "fullname": "Zongyang Ma",
                        "user": "zyma",
                        "type": "user"
                    },
                    "name": "Zongyang Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T15:15:48.907Z",
                    "hidden": false
                },
                {
                    "_id": "68d2a61a0e215259d193b086",
                    "name": "Junfu Pu",
                    "hidden": false
                },
                {
                    "_id": "68d2a61a0e215259d193b087",
                    "name": "Zhongang Qi",
                    "hidden": false
                },
                {
                    "_id": "68d2a61a0e215259d193b088",
                    "name": "Yang Wu",
                    "hidden": false
                },
                {
                    "_id": "68d2a61a0e215259d193b089",
                    "name": "Ying Shan",
                    "hidden": false
                },
                {
                    "_id": "68d2a61a0e215259d193b08a",
                    "name": "Chang Wen Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6250eb5c1fd03f78d0ae550f/rEUmlR3QM0cppH7Pf-kDD.jpeg"
            ],
            "publishedAt": "2025-09-22T17:59:40.000Z",
            "submittedOnDailyAt": "2025-09-23T12:32:18.498Z",
            "title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level\n  Visual Reasoning",
            "submittedOnDailyBy": {
                "_id": "6250eb5c1fd03f78d0ae550f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649470297585-noauth.jpeg",
                "isPro": false,
                "fullname": "Ye Liu",
                "user": "yeliudev",
                "type": "user"
            },
            "summary": "Recent advances in Large Multi-modal Models (LMMs) have demonstrated their\nremarkable success as general-purpose multi-modal assistants, with particular\nfocuses on holistic image- and video-language understanding. Conversely, less\nattention has been given to scaling fine-grained pixel-level understanding\ncapabilities, where the models are expected to realize pixel-level alignment\nbetween visual signals and language semantics. Some previous studies have\napplied LMMs to related tasks such as region-level captioning and referring\nexpression segmentation. However, these models are limited to performing either\nreferring or segmentation tasks independently and fail to integrate these\nfine-grained perception capabilities into visual reasoning. To bridge this gap,\nwe propose UniPixel, a large multi-modal model capable of flexibly\ncomprehending visual prompt inputs and generating mask-grounded responses. Our\nmodel distinguishes itself by seamlessly integrating pixel-level perception\nwith general visual understanding capabilities. Specifically, UniPixel\nprocesses visual prompts and generates relevant masks on demand, and performs\nsubsequent reasoning conditioning on these intermediate pointers during\ninference, thereby enabling fine-grained pixel-level reasoning. The\neffectiveness of our approach has been verified on 10 benchmarks across a\ndiverse set of tasks, including pixel-level referring/segmentation and\nobject-centric understanding in images/videos. A novel PixelQA task that\njointly requires referring, segmentation, and question answering is also\ndesigned to verify the flexibility of our method.",
            "upvotes": 3,
            "discussionId": "68d2a61b0e215259d193b08b",
            "projectPage": "https://polyu-chenlab.github.io/unipixel",
            "githubRepo": "https://github.com/PolyU-ChenLab/UniPixel",
            "ai_summary": "UniPixel, a large multi-modal model, integrates pixel-level perception with general visual understanding, enabling fine-grained reasoning across various tasks including pixel-level referring, segmentation, and question answering.",
            "ai_keywords": [
                "Large Multi-modal Models",
                "LMMs",
                "pixel-level understanding",
                "pixel-level alignment",
                "visual signals",
                "language semantics",
                "region-level captioning",
                "referring expression segmentation",
                "visual reasoning",
                "mask-grounded responses",
                "pixel-level perception",
                "visual understanding capabilities",
                "pixel-level referring",
                "pixel-level segmentation",
                "object-centric understanding",
                "PixelQA task"
            ],
            "githubStars": 8
        },
        "publishedAt": "2025-09-22T13:59:40.000Z",
        "title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level\n  Visual Reasoning",
        "summary": "Recent advances in Large Multi-modal Models (LMMs) have demonstrated their\nremarkable success as general-purpose multi-modal assistants, with particular\nfocuses on holistic image- and video-language understanding. Conversely, less\nattention has been given to scaling fine-grained pixel-level understanding\ncapabilities, where the models are expected to realize pixel-level alignment\nbetween visual signals and language semantics. Some previous studies have\napplied LMMs to related tasks such as region-level captioning and referring\nexpression segmentation. However, these models are limited to performing either\nreferring or segmentation tasks independently and fail to integrate these\nfine-grained perception capabilities into visual reasoning. To bridge this gap,\nwe propose UniPixel, a large multi-modal model capable of flexibly\ncomprehending visual prompt inputs and generating mask-grounded responses. Our\nmodel distinguishes itself by seamlessly integrating pixel-level perception\nwith general visual understanding capabilities. Specifically, UniPixel\nprocesses visual prompts and generates relevant masks on demand, and performs\nsubsequent reasoning conditioning on these intermediate pointers during\ninference, thereby enabling fine-grained pixel-level reasoning. The\neffectiveness of our approach has been verified on 10 benchmarks across a\ndiverse set of tasks, including pixel-level referring/segmentation and\nobject-centric understanding in images/videos. A novel PixelQA task that\njointly requires referring, segmentation, and question answering is also\ndesigned to verify the flexibility of our method.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6250eb5c1fd03f78d0ae550f/rEUmlR3QM0cppH7Pf-kDD.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18094.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6250eb5c1fd03f78d0ae550f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649470297585-noauth.jpeg",
            "fullname": "Ye Liu",
            "name": "yeliudev",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.18083",
            "authors": [
                {
                    "_id": "68d24adf1ca7156988a8ee94",
                    "name": "Valentin Lacombe",
                    "hidden": false
                },
                {
                    "_id": "68d24adf1ca7156988a8ee95",
                    "name": "Valentin Quesnel",
                    "hidden": false
                },
                {
                    "_id": "68d24adf1ca7156988a8ee96",
                    "name": "Damien Sileo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/5fc0bcb41160c47d1d43856b/8eF3782GxQiM5xUWR24vj.png"
            ],
            "publishedAt": "2025-09-22T17:56:38.000Z",
            "submittedOnDailyAt": "2025-09-23T05:54:55.151Z",
            "title": "Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning",
            "submittedOnDailyBy": {
                "_id": "5fc0bcb41160c47d1d43856b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fc0bcb41160c47d1d43856b/AHCEW4TfTdyjNBx-V_F5A.png",
                "isPro": false,
                "fullname": "Damien Sileo",
                "user": "sileod",
                "type": "user"
            },
            "summary": "We introduce Reasoning Core, a new scalable environment for Reinforcement\nLearning with Verifiable Rewards (RLVR), designed to advance foundational\nsymbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks\nthat focus on games or isolated puzzles, Reasoning Core procedurally generates\nproblems across core formal domains, including PDDL planning, first-order\nlogic, context-free grammar parsing, causal reasoning, and system equation\nsolving. The environment is built on key design principles of high-generality\nproblem distributions, verification via external tools, and continuous\ndifficulty control, which together provide a virtually infinite supply of novel\ntraining instances. Initial zero-shot evaluations with frontier LLMs confirm\nthe difficulty of Reasoning Core's tasks, positioning it as a promising\nresource to improve the reasoning capabilities of future models.",
            "upvotes": 3,
            "discussionId": "68d24ae01ca7156988a8ee97",
            "projectPage": "https://github.com/sileod/reasoning_core",
            "githubRepo": "https://github.com/sileod/reasoning_core",
            "ai_summary": "Reasoning Core is a scalable RLVR environment that generates diverse symbolic reasoning problems to enhance LLM capabilities.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards (RLVR)",
                "Large Language Models (LLMs)",
                "PDDL planning",
                "first-order logic",
                "context-free grammar parsing",
                "causal reasoning",
                "system equation solving",
                "high-generality problem distributions",
                "verification via external tools",
                "continuous difficulty control"
            ],
            "githubStars": 9
        },
        "publishedAt": "2025-09-22T13:56:38.000Z",
        "title": "Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning",
        "summary": "We introduce Reasoning Core, a new scalable environment for Reinforcement\nLearning with Verifiable Rewards (RLVR), designed to advance foundational\nsymbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks\nthat focus on games or isolated puzzles, Reasoning Core procedurally generates\nproblems across core formal domains, including PDDL planning, first-order\nlogic, context-free grammar parsing, causal reasoning, and system equation\nsolving. The environment is built on key design principles of high-generality\nproblem distributions, verification via external tools, and continuous\ndifficulty control, which together provide a virtually infinite supply of novel\ntraining instances. Initial zero-shot evaluations with frontier LLMs confirm\nthe difficulty of Reasoning Core's tasks, positioning it as a promising\nresource to improve the reasoning capabilities of future models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/5fc0bcb41160c47d1d43856b/8eF3782GxQiM5xUWR24vj.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18083.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5fc0bcb41160c47d1d43856b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fc0bcb41160c47d1d43856b/AHCEW4TfTdyjNBx-V_F5A.png",
            "fullname": "Damien Sileo",
            "name": "sileod",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 27
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.18053",
            "authors": [
                {
                    "_id": "68d2b4940e215259d193b0b2",
                    "name": "Hsu-kuang Chiu",
                    "hidden": false
                },
                {
                    "_id": "68d2b4940e215259d193b0b3",
                    "name": "Ryo Hachiuma",
                    "hidden": false
                },
                {
                    "_id": "68d2b4940e215259d193b0b4",
                    "name": "Chien-Yi Wang",
                    "hidden": false
                },
                {
                    "_id": "68d2b4940e215259d193b0b5",
                    "name": "Yu-Chiang Frank Wang",
                    "hidden": false
                },
                {
                    "_id": "68d2b4940e215259d193b0b6",
                    "user": {
                        "_id": "64ae22dd1aee69ece065cdcd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                        "isPro": false,
                        "fullname": "Min-Hung Chen",
                        "user": "cmhungsteve",
                        "type": "user"
                    },
                    "name": "Min-Hung Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T15:15:52.264Z",
                    "hidden": false
                },
                {
                    "_id": "68d2b4940e215259d193b0b7",
                    "name": "Stephen F. Smith",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T17:27:29.000Z",
            "submittedOnDailyAt": "2025-09-23T13:27:26.344Z",
            "title": "V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with\n  Multimodal Large Language Models and Graph-of-Thoughts",
            "submittedOnDailyBy": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
            },
            "summary": "Current state-of-the-art autonomous vehicles could face safety-critical\nsituations when their local sensors are occluded by large nearby objects on the\nroad. Vehicle-to-vehicle (V2V) cooperative autonomous driving has been proposed\nas a means of addressing this problem, and one recently introduced framework\nfor cooperative autonomous driving has further adopted an approach that\nincorporates a Multimodal Large Language Model (MLLM) to integrate cooperative\nperception and planning processes. However, despite the potential benefit of\napplying graph-of-thoughts reasoning to the MLLM, this idea has not been\nconsidered by previous cooperative autonomous driving research. In this paper,\nwe propose a novel graph-of-thoughts framework specifically designed for\nMLLM-based cooperative autonomous driving. Our graph-of-thoughts includes our\nproposed novel ideas of occlusion-aware perception and planning-aware\nprediction. We curate the V2V-GoT-QA dataset and develop the V2V-GoT model for\ntraining and testing the cooperative driving graph-of-thoughts. Our\nexperimental results show that our method outperforms other baselines in\ncooperative perception, prediction, and planning tasks.",
            "upvotes": 3,
            "discussionId": "68d2b4950e215259d193b0b8",
            "ai_summary": "A graph-of-thoughts framework incorporating occlusion-aware perception and planning-aware prediction enhances cooperative autonomous driving using a Multimodal Large Language Model.",
            "ai_keywords": [
                "Multimodal Large Language Model",
                "graph-of-thoughts",
                "occlusion-aware perception",
                "planning-aware prediction",
                "V2V-GoT-QA dataset",
                "V2V-GoT model"
            ]
        },
        "publishedAt": "2025-09-22T13:27:29.000Z",
        "title": "V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with\n  Multimodal Large Language Models and Graph-of-Thoughts",
        "summary": "Current state-of-the-art autonomous vehicles could face safety-critical\nsituations when their local sensors are occluded by large nearby objects on the\nroad. Vehicle-to-vehicle (V2V) cooperative autonomous driving has been proposed\nas a means of addressing this problem, and one recently introduced framework\nfor cooperative autonomous driving has further adopted an approach that\nincorporates a Multimodal Large Language Model (MLLM) to integrate cooperative\nperception and planning processes. However, despite the potential benefit of\napplying graph-of-thoughts reasoning to the MLLM, this idea has not been\nconsidered by previous cooperative autonomous driving research. In this paper,\nwe propose a novel graph-of-thoughts framework specifically designed for\nMLLM-based cooperative autonomous driving. Our graph-of-thoughts includes our\nproposed novel ideas of occlusion-aware perception and planning-aware\nprediction. We curate the V2V-GoT-QA dataset and develop the V2V-GoT model for\ntraining and testing the cooperative driving graph-of-thoughts. Our\nexperimental results show that our method outperforms other baselines in\ncooperative perception, prediction, and planning tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18053.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ae22dd1aee69ece065cdcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
            "fullname": "Min-Hung Chen",
            "name": "cmhungsteve",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.17641",
            "authors": [
                {
                    "_id": "68d22a281ca7156988a8edf4",
                    "name": "Hyunjong Ok",
                    "hidden": false
                },
                {
                    "_id": "68d22a281ca7156988a8edf5",
                    "user": {
                        "_id": "651930780e3a5553d4a04c66",
                        "avatarUrl": "/avatars/35f1a91a2ed920253274c010abc6741d.svg",
                        "isPro": false,
                        "fullname": "suho yoo",
                        "user": "suhoyoo",
                        "type": "user"
                    },
                    "name": "Suho Yoo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:05:34.245Z",
                    "hidden": false
                },
                {
                    "_id": "68d22a281ca7156988a8edf6",
                    "name": "Hyeonjun Kim",
                    "hidden": false
                },
                {
                    "_id": "68d22a281ca7156988a8edf7",
                    "name": "Jaeho Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T11:45:22.000Z",
            "submittedOnDailyAt": "2025-09-23T03:34:10.651Z",
            "title": "AuditoryBench++: Can Language Models Understand Auditory Knowledge\n  without Hearing?",
            "submittedOnDailyBy": {
                "_id": "631974d51328b6caf9fe328f",
                "avatarUrl": "/avatars/5d7b54d2798d9e42d2db66cdba24e085.svg",
                "isPro": false,
                "fullname": "Hyunjong Ok",
                "user": "HJOK",
                "type": "user"
            },
            "summary": "Even without directly hearing sounds, humans can effortlessly reason about\nauditory properties, such as pitch, loudness, or sound-source associations,\ndrawing on auditory commonsense. In contrast, language models often lack this\ncapability, limiting their effectiveness in multimodal interactions. As an\ninitial step to address this gap, we present AuditoryBench++, a comprehensive\nbenchmark for evaluating auditory knowledge and reasoning in text-only\nsettings. The benchmark encompasses tasks that range from basic auditory\ncomparisons to contextually grounded reasoning, enabling fine-grained analysis\nof how models process and integrate auditory concepts. In addition, we\nintroduce AIR-CoT, a novel auditory imagination reasoning method that generates\nand integrates auditory information during inference through span detection\nwith special tokens and knowledge injection. Extensive experiments with recent\nLLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both\nthe off-the-shelf models and those augmented with auditory knowledge. The\nproject page is available at https://auditorybenchpp.github.io.",
            "upvotes": 3,
            "discussionId": "68d22a281ca7156988a8edf8",
            "ai_summary": "AuditoryBench++ and AIR-CoT enhance text-only models' auditory reasoning and knowledge integration, outperforming existing models in multimodal interactions.",
            "ai_keywords": [
                "AuditoryBench++",
                "AIR-CoT",
                "auditory imagination reasoning",
                "span detection",
                "knowledge injection",
                "multimodal interactions"
            ]
        },
        "publishedAt": "2025-09-22T07:45:22.000Z",
        "title": "AuditoryBench++: Can Language Models Understand Auditory Knowledge\n  without Hearing?",
        "summary": "Even without directly hearing sounds, humans can effortlessly reason about\nauditory properties, such as pitch, loudness, or sound-source associations,\ndrawing on auditory commonsense. In contrast, language models often lack this\ncapability, limiting their effectiveness in multimodal interactions. As an\ninitial step to address this gap, we present AuditoryBench++, a comprehensive\nbenchmark for evaluating auditory knowledge and reasoning in text-only\nsettings. The benchmark encompasses tasks that range from basic auditory\ncomparisons to contextually grounded reasoning, enabling fine-grained analysis\nof how models process and integrate auditory concepts. In addition, we\nintroduce AIR-CoT, a novel auditory imagination reasoning method that generates\nand integrates auditory information during inference through span detection\nwith special tokens and knowledge injection. Extensive experiments with recent\nLLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both\nthe off-the-shelf models and those augmented with auditory knowledge. The\nproject page is available at https://auditorybenchpp.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17641.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631974d51328b6caf9fe328f",
            "avatarUrl": "/avatars/5d7b54d2798d9e42d2db66cdba24e085.svg",
            "fullname": "Hyunjong Ok",
            "name": "HJOK",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.17336",
            "authors": [
                {
                    "_id": "68d20f911ca7156988a8ed61",
                    "name": "Tianyu Fu",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed62",
                    "name": "Anyang Su",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed63",
                    "name": "Chenxu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed64",
                    "name": "Hanning Wang",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed65",
                    "name": "Minghui Wu",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed66",
                    "name": "Zhe Yu",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed67",
                    "name": "Fei Hu",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed68",
                    "name": "Mingjia Shi",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed69",
                    "name": "Wei Dong",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed6a",
                    "name": "Jiayao Wang",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed6b",
                    "name": "Yuyang Chen",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed6c",
                    "name": "Ruiyang Yu",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed6d",
                    "name": "Siran Peng",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed6e",
                    "name": "Menglin Li",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed6f",
                    "name": "Nan Huang",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed70",
                    "name": "Haitian Wei",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed71",
                    "name": "Jiawei Yu",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed72",
                    "name": "Yi Xin",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed73",
                    "name": "Xilin Zhao",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed74",
                    "name": "Kai Gu",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed75",
                    "name": "Ping Jiang",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed76",
                    "name": "Sifan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68d20f911ca7156988a8ed77",
                    "name": "Shuo Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T03:13:58.000Z",
            "submittedOnDailyAt": "2025-09-23T01:40:17.413Z",
            "title": "Mano Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Graphical user interfaces (GUIs) are the primary medium for human-computer\ninteraction, yet automating GUI interactions remains challenging due to the\ncomplexity of visual elements, dynamic environments, and the need for\nmulti-step reasoning. Existing methods based on vision-language models (VLMs)\noften suffer from limited resolution, domain mismatch, and insufficient\nsequential decisionmaking capability. To address these issues, we propose Mano,\na robust GUI agent built upon a multi-modal foundation model pre-trained on\nextensive web and computer system data. Our approach integrates a novel\nsimulated environment for high-fidelity data generation, a three-stage training\npipeline (supervised fine-tuning, offline reinforcement learning, and online\nreinforcement learning), and a verification module for error recovery. Mano\ndemonstrates state-of-the-art performance on multiple GUI benchmarks, including\nMind2Web and OSWorld, achieving significant improvements in success rate and\noperational accuracy. Our work provides new insights into the effective\nintegration of reinforcement learning with VLMs for practical GUI agent\ndeployment, highlighting the importance of domain-specific data, iterative\ntraining, and holistic reward design.",
            "upvotes": 3,
            "discussionId": "68d20f911ca7156988a8ed78",
            "ai_summary": "A robust GUI agent, Mano, integrates reinforcement learning with vision-language models for high-fidelity data generation and improved performance on GUI benchmarks.",
            "ai_keywords": [
                "vision-language models",
                "multi-modal foundation model",
                "simulated environment",
                "three-stage training pipeline",
                "supervised fine-tuning",
                "offline reinforcement learning",
                "online reinforcement learning",
                "verification module",
                "Mind2Web",
                "OSWorld",
                "domain-specific data",
                "iterative training",
                "holistic reward design"
            ]
        },
        "publishedAt": "2025-09-21T23:13:58.000Z",
        "title": "Mano Report",
        "summary": "Graphical user interfaces (GUIs) are the primary medium for human-computer\ninteraction, yet automating GUI interactions remains challenging due to the\ncomplexity of visual elements, dynamic environments, and the need for\nmulti-step reasoning. Existing methods based on vision-language models (VLMs)\noften suffer from limited resolution, domain mismatch, and insufficient\nsequential decisionmaking capability. To address these issues, we propose Mano,\na robust GUI agent built upon a multi-modal foundation model pre-trained on\nextensive web and computer system data. Our approach integrates a novel\nsimulated environment for high-fidelity data generation, a three-stage training\npipeline (supervised fine-tuning, offline reinforcement learning, and online\nreinforcement learning), and a verification module for error recovery. Mano\ndemonstrates state-of-the-art performance on multiple GUI benchmarks, including\nMind2Web and OSWorld, achieving significant improvements in success rate and\noperational accuracy. Our work provides new insights into the effective\nintegration of reinforcement learning with VLMs for practical GUI agent\ndeployment, highlighting the importance of domain-specific data, iterative\ntraining, and holistic reward design.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17336.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 108
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.17786",
            "authors": [
                {
                    "_id": "68d2db0d0e215259d193b11b",
                    "name": "Aniello Panariello",
                    "hidden": false
                },
                {
                    "_id": "68d2db0d0e215259d193b11c",
                    "name": "Daniel Marczak",
                    "hidden": false
                },
                {
                    "_id": "68d2db0d0e215259d193b11d",
                    "name": "Simone Magistri",
                    "hidden": false
                },
                {
                    "_id": "68d2db0d0e215259d193b11e",
                    "name": "Angelo Porrello",
                    "hidden": false
                },
                {
                    "_id": "68d2db0d0e215259d193b11f",
                    "name": "Bartłomiej Twardowski",
                    "hidden": false
                },
                {
                    "_id": "68d2db0d0e215259d193b120",
                    "name": "Andrew D. Bagdanov",
                    "hidden": false
                },
                {
                    "_id": "68d2db0d0e215259d193b121",
                    "name": "Simone Calderara",
                    "hidden": false
                },
                {
                    "_id": "68d2db0d0e215259d193b122",
                    "name": "Joost van de Weijer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T13:48:15.000Z",
            "submittedOnDailyAt": "2025-09-23T16:11:14.408Z",
            "title": "Accurate and Efficient Low-Rank Model Merging in Core Space",
            "submittedOnDailyBy": {
                "_id": "65a5358ddb5c00652ef24c8d",
                "avatarUrl": "/avatars/d50b6297584c9b4c2ccd93e64477b940.svg",
                "isPro": false,
                "fullname": "Daniel Marczak",
                "user": "danielm1405",
                "type": "user"
            },
            "summary": "In this paper, we address the challenges associated with merging low-rank\nadaptations of large neural networks. With the rise of parameter-efficient\nadaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning\nhas become more accessible. While fine-tuning models with LoRA is highly\nefficient, existing merging methods often sacrifice this efficiency by merging\nfully-sized weight matrices. We propose the Core Space merging framework, which\nenables the merging of LoRA-adapted models within a common alignment basis,\nthereby preserving the efficiency of low-rank adaptation while substantially\nimproving accuracy across tasks. We further provide a formal proof that\nprojection into Core Space ensures no loss of information and provide a\ncomplexity analysis showing the efficiency gains. Extensive empirical results\ndemonstrate that Core Space significantly improves existing merging techniques\nand achieves state-of-the-art results on both vision and language tasks while\nutilizing a fraction of the computational resources. Codebase is available at\nhttps://github.com/apanariello4/core-space-merging.",
            "upvotes": 2,
            "discussionId": "68d2db0e0e215259d193b123",
            "githubRepo": "https://github.com/apanariello4/core-space-merging",
            "ai_summary": "Core Space merging framework improves the accuracy and efficiency of merging low-rank adapted models across tasks without significant computational cost.",
            "ai_keywords": [
                "Low-Rank Adaptation",
                "LoRA",
                "Core Space",
                "parameter-efficient adaptation",
                "merging methods",
                "weight matrices",
                "formal proof",
                "complexity analysis",
                "state-of-the-art results",
                "vision tasks",
                "language tasks"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-09-22T09:48:15.000Z",
        "title": "Accurate and Efficient Low-Rank Model Merging in Core Space",
        "summary": "In this paper, we address the challenges associated with merging low-rank\nadaptations of large neural networks. With the rise of parameter-efficient\nadaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning\nhas become more accessible. While fine-tuning models with LoRA is highly\nefficient, existing merging methods often sacrifice this efficiency by merging\nfully-sized weight matrices. We propose the Core Space merging framework, which\nenables the merging of LoRA-adapted models within a common alignment basis,\nthereby preserving the efficiency of low-rank adaptation while substantially\nimproving accuracy across tasks. We further provide a formal proof that\nprojection into Core Space ensures no loss of information and provide a\ncomplexity analysis showing the efficiency gains. Extensive empirical results\ndemonstrate that Core Space significantly improves existing merging techniques\nand achieves state-of-the-art results on both vision and language tasks while\nutilizing a fraction of the computational resources. Codebase is available at\nhttps://github.com/apanariello4/core-space-merging.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17786.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65a5358ddb5c00652ef24c8d",
            "avatarUrl": "/avatars/d50b6297584c9b4c2ccd93e64477b940.svg",
            "fullname": "Daniel Marczak",
            "name": "danielm1405",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.17998",
            "authors": [
                {
                    "_id": "68d2acaa0e215259d193b098",
                    "user": {
                        "_id": "603bc1a4fd770a9997b57cb9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/603bc1a4fd770a9997b57cb9/3bXN2XmPxENQ5rpN7ZtPX.png",
                        "isPro": false,
                        "fullname": "Richard Cornelius Suwandi",
                        "user": "richardcsuwandi",
                        "type": "user"
                    },
                    "name": "Richard Cornelius Suwandi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T14:24:34.407Z",
                    "hidden": false
                },
                {
                    "_id": "68d2acaa0e215259d193b099",
                    "name": "Feng Yin",
                    "hidden": false
                },
                {
                    "_id": "68d2acaa0e215259d193b09a",
                    "name": "Juntao Wang",
                    "hidden": false
                },
                {
                    "_id": "68d2acaa0e215259d193b09b",
                    "name": "Renjie Li",
                    "hidden": false
                },
                {
                    "_id": "68d2acaa0e215259d193b09c",
                    "name": "Tsung-Hui Chang",
                    "hidden": false
                },
                {
                    "_id": "68d2acaa0e215259d193b09d",
                    "name": "Sergios Theodoridis",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T16:39:12.000Z",
            "submittedOnDailyAt": "2025-09-23T13:15:52.114Z",
            "title": "Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with\n  LLMs",
            "submittedOnDailyBy": {
                "_id": "603bc1a4fd770a9997b57cb9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/603bc1a4fd770a9997b57cb9/3bXN2XmPxENQ5rpN7ZtPX.png",
                "isPro": false,
                "fullname": "Richard Cornelius Suwandi",
                "user": "richardcsuwandi",
                "type": "user"
            },
            "summary": "The efficiency of Bayesian optimization (BO) relies heavily on the choice of\nthe Gaussian process (GP) kernel, which plays a central role in balancing\nexploration and exploitation under limited evaluation budgets. Traditional BO\nmethods often rely on fixed or heuristic kernel selection strategies, which can\nresult in slow convergence or suboptimal solutions when the chosen kernel is\npoorly suited to the underlying objective function. To address this limitation,\nwe propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO\nwith large language models (LLMs). Concretely, CAKE leverages LLMs as the\ncrossover and mutation operators to adaptively generate and refine GP kernels\nbased on the observed data throughout the optimization process. To maximize the\npower of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to\nselect the most effective kernel through balancing the model fit measured by\nthe Bayesian information criterion (BIC) with the expected improvement at each\niteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO\nmethod consistently outperforms established baselines across a range of\nreal-world tasks, including hyperparameter optimization, controller tuning, and\nphotonic chip design. Our code is publicly available at\nhttps://github.com/cake4bo/cake.",
            "upvotes": 1,
            "discussionId": "68d2acaa0e215259d193b09e",
            "githubRepo": "https://github.com/richardcsuwandi/cake",
            "ai_summary": "Context-Aware Kernel Evolution (CAKE) enhances Bayesian optimization by using large language models to adaptively generate and refine Gaussian process kernels, outperforming traditional methods across various tasks.",
            "ai_keywords": [
                "Gaussian process",
                "Bayesian optimization",
                "kernel",
                "exploration",
                "exploitation",
                "Bayesian information criterion",
                "expected improvement",
                "BIC-Acquisition Kernel Ranking",
                "hyperparameter optimization",
                "controller tuning",
                "photonic chip design"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-09-22T12:39:12.000Z",
        "title": "Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with\n  LLMs",
        "summary": "The efficiency of Bayesian optimization (BO) relies heavily on the choice of\nthe Gaussian process (GP) kernel, which plays a central role in balancing\nexploration and exploitation under limited evaluation budgets. Traditional BO\nmethods often rely on fixed or heuristic kernel selection strategies, which can\nresult in slow convergence or suboptimal solutions when the chosen kernel is\npoorly suited to the underlying objective function. To address this limitation,\nwe propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO\nwith large language models (LLMs). Concretely, CAKE leverages LLMs as the\ncrossover and mutation operators to adaptively generate and refine GP kernels\nbased on the observed data throughout the optimization process. To maximize the\npower of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to\nselect the most effective kernel through balancing the model fit measured by\nthe Bayesian information criterion (BIC) with the expected improvement at each\niteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO\nmethod consistently outperforms established baselines across a range of\nreal-world tasks, including hyperparameter optimization, controller tuning, and\nphotonic chip design. Our code is publicly available at\nhttps://github.com/cake4bo/cake.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17998.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "603bc1a4fd770a9997b57cb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/603bc1a4fd770a9997b57cb9/3bXN2XmPxENQ5rpN7ZtPX.png",
            "fullname": "Richard Cornelius Suwandi",
            "name": "richardcsuwandi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.17938",
            "authors": [
                {
                    "_id": "68d2ef880e215259d193b135",
                    "name": "Satyapriya Krishna",
                    "hidden": false
                },
                {
                    "_id": "68d2ef880e215259d193b136",
                    "name": "Andy Zou",
                    "hidden": false
                },
                {
                    "_id": "68d2ef880e215259d193b137",
                    "name": "Rahul Gupta",
                    "hidden": false
                },
                {
                    "_id": "68d2ef880e215259d193b138",
                    "name": "Eliot Krzysztof Jones",
                    "hidden": false
                },
                {
                    "_id": "68d2ef880e215259d193b139",
                    "name": "Nick Winter",
                    "hidden": false
                },
                {
                    "_id": "68d2ef880e215259d193b13a",
                    "name": "Dan Hendrycks",
                    "hidden": false
                },
                {
                    "_id": "68d2ef880e215259d193b13b",
                    "name": "J. Zico Kolter",
                    "hidden": false
                },
                {
                    "_id": "68d2ef880e215259d193b13c",
                    "name": "Matt Fredrikson",
                    "hidden": false
                },
                {
                    "_id": "68d2ef880e215259d193b13d",
                    "name": "Spyros Matsoukas",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T15:59:40.000Z",
            "submittedOnDailyAt": "2025-09-23T17:36:29.205Z",
            "title": "D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "6186fef1b1085ab638324e7f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6186fef1b1085ab638324e7f/BL6_WJCkxB-BatBUBilT8.jpeg",
                "isPro": false,
                "fullname": "Satya",
                "user": "skrishna",
                "type": "user"
            },
            "summary": "The safety and alignment of Large Language Models (LLMs) are critical for\ntheir responsible deployment. Current evaluation methods predominantly focus on\nidentifying and preventing overtly harmful outputs. However, they often fail to\naddress a more insidious failure mode: models that produce benign-appearing\noutputs while operating on malicious or deceptive internal reasoning. This\nvulnerability, often triggered by sophisticated system prompt injections,\nallows models to bypass conventional safety filters, posing a significant,\nunderexplored risk. To address this gap, we introduce the Deceptive Reasoning\nExposure Suite (D-REX), a novel dataset designed to evaluate the discrepancy\nbetween a model's internal reasoning process and its final output. D-REX was\nconstructed through a competitive red-teaming exercise where participants\ncrafted adversarial system prompts to induce such deceptive behaviors. Each\nsample in D-REX contains the adversarial system prompt, an end-user's test\nquery, the model's seemingly innocuous response, and, crucially, the model's\ninternal chain-of-thought, which reveals the underlying malicious intent. Our\nbenchmark facilitates a new, essential evaluation task: the detection of\ndeceptive alignment. We demonstrate that D-REX presents a significant challenge\nfor existing models and safety mechanisms, highlighting the urgent need for new\ntechniques that scrutinize the internal processes of LLMs, not just their final\noutputs.",
            "upvotes": 1,
            "discussionId": "68d2ef880e215259d193b13e",
            "ai_summary": "The Deceptive Reasoning Exposure Suite (D-REX) evaluates the internal reasoning of Large Language Models to detect deceptive behaviors that bypass safety filters.",
            "ai_keywords": [
                "Large Language Models",
                "Deceptive Reasoning Exposure Suite",
                "D-REX",
                "adversarial system prompts",
                "internal chain-of-thought",
                "deceptive alignment"
            ]
        },
        "publishedAt": "2025-09-22T11:59:40.000Z",
        "title": "D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language\n  Models",
        "summary": "The safety and alignment of Large Language Models (LLMs) are critical for\ntheir responsible deployment. Current evaluation methods predominantly focus on\nidentifying and preventing overtly harmful outputs. However, they often fail to\naddress a more insidious failure mode: models that produce benign-appearing\noutputs while operating on malicious or deceptive internal reasoning. This\nvulnerability, often triggered by sophisticated system prompt injections,\nallows models to bypass conventional safety filters, posing a significant,\nunderexplored risk. To address this gap, we introduce the Deceptive Reasoning\nExposure Suite (D-REX), a novel dataset designed to evaluate the discrepancy\nbetween a model's internal reasoning process and its final output. D-REX was\nconstructed through a competitive red-teaming exercise where participants\ncrafted adversarial system prompts to induce such deceptive behaviors. Each\nsample in D-REX contains the adversarial system prompt, an end-user's test\nquery, the model's seemingly innocuous response, and, crucially, the model's\ninternal chain-of-thought, which reveals the underlying malicious intent. Our\nbenchmark facilitates a new, essential evaluation task: the detection of\ndeceptive alignment. We demonstrate that D-REX presents a significant challenge\nfor existing models and safety mechanisms, highlighting the urgent need for new\ntechniques that scrutinize the internal processes of LLMs, not just their final\noutputs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17938.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6186fef1b1085ab638324e7f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6186fef1b1085ab638324e7f/BL6_WJCkxB-BatBUBilT8.jpeg",
            "fullname": "Satya",
            "name": "skrishna",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.17399",
            "authors": [
                {
                    "_id": "68d24c281ca7156988a8eea9",
                    "user": {
                        "_id": "65dc385490bd042d5b87194e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/MbUdAN__mLxjEQSkzMQGh.jpeg",
                        "isPro": false,
                        "fullname": "Pramit Sahoo",
                        "user": "pramitsahoo",
                        "type": "user"
                    },
                    "name": "Pramit Sahoo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:04:44.425Z",
                    "hidden": false
                },
                {
                    "_id": "68d24c281ca7156988a8eeaa",
                    "user": {
                        "_id": "604a4a8edca2c7ac7508b847",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604a4a8edca2c7ac7508b847/N_HFCXuEaJ0nZw7BKY2Jj.jpeg",
                        "isPro": false,
                        "fullname": "Maharaj Brahma",
                        "user": "mrajbrahma",
                        "type": "user"
                    },
                    "name": "Maharaj Brahma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:04:46.695Z",
                    "hidden": false
                },
                {
                    "_id": "68d24c281ca7156988a8eeab",
                    "name": "Maunendra Sankar Desarkar",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T06:58:02.000Z",
            "submittedOnDailyAt": "2025-09-23T09:11:26.675Z",
            "title": "DIWALI - Diversity and Inclusivity aWare cuLture specific Items for\n  India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian\n  Context",
            "submittedOnDailyBy": {
                "_id": "604a4a8edca2c7ac7508b847",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604a4a8edca2c7ac7508b847/N_HFCXuEaJ0nZw7BKY2Jj.jpeg",
                "isPro": false,
                "fullname": "Maharaj Brahma",
                "user": "mrajbrahma",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are widely used in various tasks and\napplications. However, despite their wide capabilities, they are shown to lack\ncultural alignment ryan-etal-2024-unintended,\nalkhamissi-etal-2024-investigating and produce biased generations\nnaous-etal-2024-beer due to a lack of cultural knowledge and competence.\nEvaluation of LLMs for cultural awareness and alignment is particularly\nchallenging due to the lack of proper evaluation metrics and unavailability of\nculturally grounded datasets representing the vast complexity of cultures at\nthe regional and sub-regional levels. Existing datasets for culture specific\nitems (CSIs) focus primarily on concepts at the regional level and may contain\nfalse positives. To address this issue, we introduce a novel CSI dataset for\nIndian culture, belonging to 17 cultural facets. The dataset comprises sim8k\ncultural concepts from 36 sub-regions. To measure the cultural competence of\nLLMs on a cultural text adaptation task, we evaluate the adaptations using the\nCSIs created, LLM as Judge, and human evaluations from diverse\nsocio-demographic region. Furthermore, we perform quantitative analysis\ndemonstrating selective sub-regional coverage and surface-level adaptations\nacross all considered LLMs. Our dataset is available here:\nhttps://huggingface.co/datasets/nlip/DIWALI{https://huggingface.co/datasets/nlip/DIWALI},\nproject\nwebpage\\href{https://nlip-lab.github.io/nlip/publications/diwali/{https://nlip-lab.github.io/nlip/publications/diwali/}},\nand our codebase with model outputs can be found here:\nhttps://github.com/pramitsahoo/culture-evaluation{https://github.com/pramitsahoo/culture-evaluation}.",
            "upvotes": 1,
            "discussionId": "68d24c281ca7156988a8eeac",
            "projectPage": "https://nlip-lab.github.io/nlip/publications/diwali/",
            "ai_summary": "A new dataset for Indian culture is introduced to evaluate the cultural competence of large language models, focusing on sub-regional cultural facets and providing a framework for human and model-based evaluations.",
            "ai_keywords": [
                "large language models",
                "cultural alignment",
                "biased generations",
                "cultural knowledge",
                "evaluation metrics",
                "culturally grounded datasets",
                "cultural facets",
                "cultural text adaptation",
                "LLM as Judge",
                "human evaluations",
                "socio-demographic region",
                "quantitative analysis",
                "sub-regional coverage",
                "surface-level adaptations"
            ]
        },
        "publishedAt": "2025-09-22T02:58:02.000Z",
        "title": "DIWALI - Diversity and Inclusivity aWare cuLture specific Items for\n  India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian\n  Context",
        "summary": "Large language models (LLMs) are widely used in various tasks and\napplications. However, despite their wide capabilities, they are shown to lack\ncultural alignment ryan-etal-2024-unintended,\nalkhamissi-etal-2024-investigating and produce biased generations\nnaous-etal-2024-beer due to a lack of cultural knowledge and competence.\nEvaluation of LLMs for cultural awareness and alignment is particularly\nchallenging due to the lack of proper evaluation metrics and unavailability of\nculturally grounded datasets representing the vast complexity of cultures at\nthe regional and sub-regional levels. Existing datasets for culture specific\nitems (CSIs) focus primarily on concepts at the regional level and may contain\nfalse positives. To address this issue, we introduce a novel CSI dataset for\nIndian culture, belonging to 17 cultural facets. The dataset comprises sim8k\ncultural concepts from 36 sub-regions. To measure the cultural competence of\nLLMs on a cultural text adaptation task, we evaluate the adaptations using the\nCSIs created, LLM as Judge, and human evaluations from diverse\nsocio-demographic region. Furthermore, we perform quantitative analysis\ndemonstrating selective sub-regional coverage and surface-level adaptations\nacross all considered LLMs. Our dataset is available here:\nhttps://huggingface.co/datasets/nlip/DIWALI{https://huggingface.co/datasets/nlip/DIWALI},\nproject\nwebpage\\href{https://nlip-lab.github.io/nlip/publications/diwali/{https://nlip-lab.github.io/nlip/publications/diwali/}},\nand our codebase with model outputs can be found here:\nhttps://github.com/pramitsahoo/culture-evaluation{https://github.com/pramitsahoo/culture-evaluation}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17399.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "604a4a8edca2c7ac7508b847",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604a4a8edca2c7ac7508b847/N_HFCXuEaJ0nZw7BKY2Jj.jpeg",
            "fullname": "Maharaj Brahma",
            "name": "mrajbrahma",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.17277",
            "authors": [
                {
                    "_id": "68d22ae61ca7156988a8ee03",
                    "user": {
                        "_id": "67d43aeb6ffb8add49ea6712",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d43aeb6ffb8add49ea6712/0h4bV3Ptgh_zTRUHVqwe2.jpeg",
                        "isPro": false,
                        "fullname": "Mandip Goswami",
                        "user": "mandipgoswami",
                        "type": "user"
                    },
                    "name": "Mandip Goswami",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:05:28.895Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-21T23:26:10.000Z",
            "submittedOnDailyAt": "2025-09-23T12:05:05.826Z",
            "title": "BeepBank-500: A Synthetic Earcon Mini-Corpus for UI Sound Research and\n  Psychoacoustics Research",
            "submittedOnDailyBy": {
                "_id": "67d43aeb6ffb8add49ea6712",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d43aeb6ffb8add49ea6712/0h4bV3Ptgh_zTRUHVqwe2.jpeg",
                "isPro": false,
                "fullname": "Mandip Goswami",
                "user": "mandipgoswami",
                "type": "user"
            },
            "summary": "We introduce BeepBank-500, a compact, fully synthetic earcon/alert dataset\n(300-500 clips) designed for rapid, rights-clean experimentation in\nhuman-computer interaction and audio machine learning. Each clip is generated\nfrom a parametric recipe controlling waveform family (sine, square, triangle,\nFM), fundamental frequency, duration, amplitude envelope, amplitude modulation\n(AM), and lightweight Schroeder-style reverberation. We use three reverberation\nsettings: dry, and two synthetic rooms denoted 'rir small' ('small') and 'rir\nmedium' ('medium') throughout the paper and in the metadata. We release mono 48\nkHz WAV audio (16-bit), a rich metadata table (signal/spectral features), and\ntiny reproducible baselines for (i) waveform-family classification and (ii) f0\nregression on single tones. The corpus targets tasks such as earcon\nclassification, timbre analyses, and onset detection, with clearly stated\nlicensing and limitations. Audio is dedicated to the public domain via CC0-1.0;\ncode is under MIT. Data DOI: https://doi.org/10.5281/zenodo.17172015. Code:\nhttps://github.com/mandip42/earcons-mini-500.",
            "upvotes": 1,
            "discussionId": "68d22ae61ca7156988a8ee04",
            "projectPage": "https://doi.org/10.5281/zenodo.17172015",
            "githubRepo": "https://github.com/mandip42/earcons-mini-500",
            "ai_summary": "BeepBank-500 is a synthetic earcon/alert dataset for audio machine learning, featuring parametrically generated clips with various waveform families and reverberation settings.",
            "ai_keywords": [
                "waveform family",
                "sine",
                "square",
                "triangle",
                "FM",
                "fundamental frequency",
                "duration",
                "amplitude envelope",
                "amplitude modulation",
                "Schroeder-style reverberation",
                "waveform-family classification",
                "f0 regression"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-09-21T19:26:10.000Z",
        "title": "BeepBank-500: A Synthetic Earcon Mini-Corpus for UI Sound Research and\n  Psychoacoustics Research",
        "summary": "We introduce BeepBank-500, a compact, fully synthetic earcon/alert dataset\n(300-500 clips) designed for rapid, rights-clean experimentation in\nhuman-computer interaction and audio machine learning. Each clip is generated\nfrom a parametric recipe controlling waveform family (sine, square, triangle,\nFM), fundamental frequency, duration, amplitude envelope, amplitude modulation\n(AM), and lightweight Schroeder-style reverberation. We use three reverberation\nsettings: dry, and two synthetic rooms denoted 'rir small' ('small') and 'rir\nmedium' ('medium') throughout the paper and in the metadata. We release mono 48\nkHz WAV audio (16-bit), a rich metadata table (signal/spectral features), and\ntiny reproducible baselines for (i) waveform-family classification and (ii) f0\nregression on single tones. The corpus targets tasks such as earcon\nclassification, timbre analyses, and onset detection, with clearly stated\nlicensing and limitations. Audio is dedicated to the public domain via CC0-1.0;\ncode is under MIT. Data DOI: https://doi.org/10.5281/zenodo.17172015. Code:\nhttps://github.com/mandip42/earcons-mini-500.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17277.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67d43aeb6ffb8add49ea6712",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d43aeb6ffb8add49ea6712/0h4bV3Ptgh_zTRUHVqwe2.jpeg",
            "fullname": "Mandip Goswami",
            "name": "mandipgoswami",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.17191",
            "authors": [
                {
                    "_id": "68d2362b1ca7156988a8ee22",
                    "user": {
                        "_id": "671f28d2b84710ee860c97b1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U-iDSeDjguCgfLkDRzYqR.png",
                        "isPro": false,
                        "fullname": "C",
                        "user": "JinchaoGe",
                        "type": "user"
                    },
                    "name": "Jinchao Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:28:57.036Z",
                    "hidden": false
                },
                {
                    "_id": "68d2362b1ca7156988a8ee23",
                    "user": {
                        "_id": "681890f3b6b878e74b799452",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/owBC2uR_qE00pLZT8nW8h.png",
                        "isPro": false,
                        "fullname": "Tengfei Cheng",
                        "user": "tengfeiCheng",
                        "type": "user"
                    },
                    "name": "Tengfei Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:29:06.084Z",
                    "hidden": false
                },
                {
                    "_id": "68d2362b1ca7156988a8ee24",
                    "name": "Biao Wu",
                    "hidden": false
                },
                {
                    "_id": "68d2362b1ca7156988a8ee25",
                    "user": {
                        "_id": "65b00730403a23a2fd765110",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b00730403a23a2fd765110/Uw-obs-VymyU9iMVKLURZ.jpeg",
                        "isPro": false,
                        "fullname": "Zeyu Zhang",
                        "user": "ZeyuZhang",
                        "type": "user"
                    },
                    "name": "Zeyu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:29:23.128Z",
                    "hidden": false
                },
                {
                    "_id": "68d2362b1ca7156988a8ee26",
                    "name": "Shiya Huang",
                    "hidden": false
                },
                {
                    "_id": "68d2362b1ca7156988a8ee27",
                    "name": "Judith Bishop",
                    "hidden": false
                },
                {
                    "_id": "68d2362b1ca7156988a8ee28",
                    "name": "Gillian Shepherd",
                    "hidden": false
                },
                {
                    "_id": "68d2362b1ca7156988a8ee29",
                    "name": "Meng Fang",
                    "hidden": false
                },
                {
                    "_id": "68d2362b1ca7156988a8ee2a",
                    "name": "Ling Chen",
                    "hidden": false
                },
                {
                    "_id": "68d2362b1ca7156988a8ee2b",
                    "name": "Yang Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-21T18:36:54.000Z",
            "submittedOnDailyAt": "2025-09-23T04:25:36.285Z",
            "title": "VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery",
            "submittedOnDailyBy": {
                "_id": "64ec877bb93654d4ca5c92e9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                "isPro": true,
                "fullname": "Zeyu Zhang",
                "user": "SteveZeyuZhang",
                "type": "user"
            },
            "summary": "Analyzing cultural-heritage artifacts remains challenging for MLLMs: general\nmodels lack domain expertise, and SFT often overfits superficial patterns,\nyielding brittle reasoning for authentication and historical attribution. This\nraises the question of how to equip MLLMs with robust, expert-level reasoning\nfor ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns\nevaluation into supervision: we construct a taxonomy of question types, probe\nthe SFT model to localize type-specific performance gaps, and optimize with\ntype-conditioned, compositionality-oriented rewards targeting those gaps. We\nalso release VaseVQA, a comprehensive benchmark of 31,773 images designed to\nprobe deep understanding. Experiments show state-of-the-art results on style\nclassification and historical attribution with marked gains in compositional\nrobustness over SFT-only baselines, validating diagnosis-guided,\ntaxonomy-conditioned reward engineering and providing a reusable resource for\nfuture research. Code and dataset will be available at\nhttps://github.com/AIGeeksGroup/VaseVQA.",
            "upvotes": 1,
            "discussionId": "68d2362b1ca7156988a8ee2c",
            "ai_summary": "VaseVL, an SFT-then-RL system, enhances MLLMs for ancient Greek pottery analysis by addressing performance gaps through taxonomy-conditioned rewards, achieving state-of-the-art results in style classification and historical attribution.",
            "ai_keywords": [
                "SFT",
                "RL",
                "VaseVL",
                "taxonomy",
                "type-specific performance gaps",
                "type-conditioned",
                "compositionality-oriented rewards",
                "VaseVQA",
                "compositional robustness"
            ]
        },
        "publishedAt": "2025-09-21T14:36:54.000Z",
        "title": "VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery",
        "summary": "Analyzing cultural-heritage artifacts remains challenging for MLLMs: general\nmodels lack domain expertise, and SFT often overfits superficial patterns,\nyielding brittle reasoning for authentication and historical attribution. This\nraises the question of how to equip MLLMs with robust, expert-level reasoning\nfor ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns\nevaluation into supervision: we construct a taxonomy of question types, probe\nthe SFT model to localize type-specific performance gaps, and optimize with\ntype-conditioned, compositionality-oriented rewards targeting those gaps. We\nalso release VaseVQA, a comprehensive benchmark of 31,773 images designed to\nprobe deep understanding. Experiments show state-of-the-art results on style\nclassification and historical attribution with marked gains in compositional\nrobustness over SFT-only baselines, validating diagnosis-guided,\ntaxonomy-conditioned reward engineering and providing a reusable resource for\nfuture research. Code and dataset will be available at\nhttps://github.com/AIGeeksGroup/VaseVQA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17191.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "fullname": "Zeyu Zhang",
            "name": "SteveZeyuZhang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.16633",
            "authors": [
                {
                    "_id": "68d22a211ca7156988a8edee",
                    "user": {
                        "_id": "6549d555fc80ab2d8ca1469a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6549d555fc80ab2d8ca1469a/Z6-tS-9-6-aUiHCAW5ql8.jpeg",
                        "isPro": false,
                        "fullname": "Abhirama Subramanyam",
                        "user": "abhiram4572",
                        "type": "user"
                    },
                    "name": "Abhirama Subramanyam Penamakuri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:05:37.522Z",
                    "hidden": false
                },
                {
                    "_id": "68d22a211ca7156988a8edef",
                    "user": {
                        "_id": "62cc633787e44ff3e7119a25",
                        "avatarUrl": "/avatars/ec0230dfe0bc707e25f66cdb04e3a217.svg",
                        "isPro": false,
                        "fullname": "Navlika Singh",
                        "user": "NavlikaSingh",
                        "type": "user"
                    },
                    "name": "Navlika Singh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:35:35.942Z",
                    "hidden": false
                },
                {
                    "_id": "68d22a211ca7156988a8edf0",
                    "name": "Piyush Arora",
                    "hidden": false
                },
                {
                    "_id": "68d22a211ca7156988a8edf1",
                    "name": "Anand Mishra",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-20T11:12:23.000Z",
            "submittedOnDailyAt": "2025-09-23T03:34:34.342Z",
            "title": "When Big Models Train Small Ones: Label-Free Model Parity Alignment for\n  Efficient Visual Question Answering using Small VLMs",
            "submittedOnDailyBy": {
                "_id": "6549d555fc80ab2d8ca1469a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6549d555fc80ab2d8ca1469a/Z6-tS-9-6-aUiHCAW5ql8.jpeg",
                "isPro": false,
                "fullname": "Abhirama Subramanyam",
                "user": "abhiram4572",
                "type": "user"
            },
            "summary": "Large Vision-Language Models (L-VLMs) have demonstrated remarkable\nperformance in various vision and language tasks, including visual question\nanswering (VQA). However, their high computational cost makes them impractical\nfor resource-constrained settings and inference-heavy applications. In\ncontrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer\nfrom a significant performance gap compared to their larger counterparts. In\nthis work, we introduce the Model Parity Aligner (MPA), a novel framework\ndesigned to systematically improve S-VLMs by leveraging unlabeled images and\neffective knowledge transfer from L-VLMs. Instead of traditional knowledge\ndistillation methods that rely on labeled training data, MPA employs a\nstrategic parity-based approach that precisely identifies the knowledge\ndisparities between S-VLMs and L-VLMs, and optimizes training by targeting only\nthese disparities. We conduct extensive experiments on four diverse VQA\nbenchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires\nspecialized reasoning capabilities such as text recognition, chart\ninterpretation, and commonsense and factual understanding. Our results\ndemonstrate that MPA consistently enhances the performance of S-VLMs on all\nbenchmarks, reducing the performance gap while maintaining computational\nefficiency. We make our code publicly available.",
            "upvotes": 1,
            "discussionId": "68d22a221ca7156988a8edf2",
            "githubRepo": "https://github.com/vl2g/MPA",
            "ai_summary": "The Model Parity Aligner (MPA) framework improves Small Vision-Language Models (S-VLMs) by leveraging unlabeled images and knowledge transfer from Large Vision-Language Models (L-VLMs) to reduce performance gaps in vision and language tasks.",
            "ai_keywords": [
                "Large Vision-Language Models",
                "Small Vision-Language Models",
                "Model Parity Aligner",
                "knowledge transfer",
                "knowledge distillation",
                "VQA",
                "TextVQA",
                "ST-VQA",
                "ChartQA",
                "OKVQA",
                "specialized reasoning",
                "text recognition",
                "chart interpretation",
                "commonsense understanding",
                "factual understanding"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-09-20T07:12:23.000Z",
        "title": "When Big Models Train Small Ones: Label-Free Model Parity Alignment for\n  Efficient Visual Question Answering using Small VLMs",
        "summary": "Large Vision-Language Models (L-VLMs) have demonstrated remarkable\nperformance in various vision and language tasks, including visual question\nanswering (VQA). However, their high computational cost makes them impractical\nfor resource-constrained settings and inference-heavy applications. In\ncontrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer\nfrom a significant performance gap compared to their larger counterparts. In\nthis work, we introduce the Model Parity Aligner (MPA), a novel framework\ndesigned to systematically improve S-VLMs by leveraging unlabeled images and\neffective knowledge transfer from L-VLMs. Instead of traditional knowledge\ndistillation methods that rely on labeled training data, MPA employs a\nstrategic parity-based approach that precisely identifies the knowledge\ndisparities between S-VLMs and L-VLMs, and optimizes training by targeting only\nthese disparities. We conduct extensive experiments on four diverse VQA\nbenchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires\nspecialized reasoning capabilities such as text recognition, chart\ninterpretation, and commonsense and factual understanding. Our results\ndemonstrate that MPA consistently enhances the performance of S-VLMs on all\nbenchmarks, reducing the performance gap while maintaining computational\nefficiency. We make our code publicly available.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.16633.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6549d555fc80ab2d8ca1469a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6549d555fc80ab2d8ca1469a/Z6-tS-9-6-aUiHCAW5ql8.jpeg",
            "fullname": "Abhirama Subramanyam",
            "name": "abhiram4572",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.16591",
            "authors": [
                {
                    "_id": "68d217261ca7156988a8edb3",
                    "user": {
                        "_id": "6625ef13605f46d05c1d0031",
                        "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
                        "isPro": false,
                        "fullname": "Zheng Liu",
                        "user": "starriver030515",
                        "type": "user"
                    },
                    "name": "Zheng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:05:44.086Z",
                    "hidden": false
                },
                {
                    "_id": "68d217261ca7156988a8edb4",
                    "user": {
                        "_id": "672dbf1bec84b9c33412488f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/V6o6K28ydpkaS9XFA4Ac3.png",
                        "isPro": false,
                        "fullname": "Mengjie Liu",
                        "user": "Balalauuoo",
                        "type": "user"
                    },
                    "name": "Mengjie Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:34:50.983Z",
                    "hidden": false
                },
                {
                    "_id": "68d217261ca7156988a8edb5",
                    "name": "Siwei Wen",
                    "hidden": false
                },
                {
                    "_id": "68d217261ca7156988a8edb6",
                    "name": "Mengzhang Cai",
                    "hidden": false
                },
                {
                    "_id": "68d217261ca7156988a8edb7",
                    "name": "Bin Cui",
                    "hidden": false
                },
                {
                    "_id": "68d217261ca7156988a8edb8",
                    "user": {
                        "_id": "63f9fca8d4349b157a109eec",
                        "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
                        "isPro": false,
                        "fullname": "Conghui He",
                        "user": "conghui",
                        "type": "user"
                    },
                    "name": "Conghui He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:35:08.157Z",
                    "hidden": false
                },
                {
                    "_id": "68d217261ca7156988a8edb9",
                    "name": "Wentao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-20T09:30:25.000Z",
            "submittedOnDailyAt": "2025-09-23T03:49:26.707Z",
            "title": "From Uniform to Heterogeneous: Tailoring Policy Optimization to Every\n  Token's Nature",
            "submittedOnDailyBy": {
                "_id": "6625ef13605f46d05c1d0031",
                "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
                "isPro": false,
                "fullname": "Zheng Liu",
                "user": "starriver030515",
                "type": "user"
            },
            "summary": "Reinforcement Learning has emerged as the fundamental technique for enhancing\nreasoning in LLMs. However, existing algorithms apply uniform optimization to\nall tokens, ignoring their different roles in reasoning process. To address\nthis limitation, we introduce Heterogeneous Adaptive Policy Optimization\n(HAPO), a comprehensive token-aware algorithm that dynamically adapts\noptimization based on token entropy. For rollout sampling, we propose Adaptive\nTemperature Sampling, which adjusts sampling temperature in real time,\npromoting exploration at high-entropy tokens while preserving coherence at\nlow-entropy ones. For advantage calculation, we introduce Token Level Group\nAverage that normalizes advantages at token level, jointly accounting for\nsequence-length as in token-mean loss while preserving non-biased treatment. We\nthen develop Differential Advantage Redistribution that leverages entropy and\nimportance ratios to modulate rewards-adjusting updates for tokens with clear\nsignals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing\naggressive probability reduction for noisy low-entropy tokens while enabling\nexploration for high-entropy tokens. Through systematic investigation between\nentropy and training dynamics, we embedded token-level treatment into every\nstages to achieve fine-grained control. Extensive experiments demonstrate that\nHAPO consistently outperforms DAPO across multiple model scales. Our code can\nbe found in https://github.com/starriver030515/HAPO.",
            "upvotes": 1,
            "discussionId": "68d217261ca7156988a8edba",
            "githubRepo": "https://github.com/starriver030515/HAPO",
            "ai_summary": "Heterogeneous Adaptive Policy Optimization (HAPO) enhances reinforcement learning in LLMs by dynamically adapting token optimization based on entropy, improving performance across various model scales.",
            "ai_keywords": [
                "Heterogeneous Adaptive Policy Optimization",
                "HAPO",
                "token-aware algorithm",
                "token entropy",
                "Adaptive Temperature Sampling",
                "Token Level Group Average",
                "Differential Advantage Redistribution",
                "Asymmetric Adaptive Clipping"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-09-20T05:30:25.000Z",
        "title": "From Uniform to Heterogeneous: Tailoring Policy Optimization to Every\n  Token's Nature",
        "summary": "Reinforcement Learning has emerged as the fundamental technique for enhancing\nreasoning in LLMs. However, existing algorithms apply uniform optimization to\nall tokens, ignoring their different roles in reasoning process. To address\nthis limitation, we introduce Heterogeneous Adaptive Policy Optimization\n(HAPO), a comprehensive token-aware algorithm that dynamically adapts\noptimization based on token entropy. For rollout sampling, we propose Adaptive\nTemperature Sampling, which adjusts sampling temperature in real time,\npromoting exploration at high-entropy tokens while preserving coherence at\nlow-entropy ones. For advantage calculation, we introduce Token Level Group\nAverage that normalizes advantages at token level, jointly accounting for\nsequence-length as in token-mean loss while preserving non-biased treatment. We\nthen develop Differential Advantage Redistribution that leverages entropy and\nimportance ratios to modulate rewards-adjusting updates for tokens with clear\nsignals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing\naggressive probability reduction for noisy low-entropy tokens while enabling\nexploration for high-entropy tokens. Through systematic investigation between\nentropy and training dynamics, we embedded token-level treatment into every\nstages to achieve fine-grained control. Extensive experiments demonstrate that\nHAPO consistently outperforms DAPO across multiple model scales. Our code can\nbe found in https://github.com/starriver030515/HAPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.16591.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6625ef13605f46d05c1d0031",
            "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
            "fullname": "Zheng Liu",
            "name": "starriver030515",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.16415",
            "authors": [
                {
                    "_id": "68d229631ca7156988a8eddb",
                    "user": {
                        "_id": "672dd7722408a6ad524ffd71",
                        "avatarUrl": "/avatars/5f2808c9ac960399ee8189674fcd906a.svg",
                        "isPro": false,
                        "fullname": "Zhengri Wu",
                        "user": "DefaultAaron",
                        "type": "user"
                    },
                    "name": "Zhengri Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:34:17.892Z",
                    "hidden": false
                },
                {
                    "_id": "68d229631ca7156988a8eddc",
                    "name": "Yiran Wang",
                    "hidden": false
                },
                {
                    "_id": "68d229631ca7156988a8eddd",
                    "name": "Yu Wen",
                    "hidden": false
                },
                {
                    "_id": "68d229631ca7156988a8edde",
                    "user": {
                        "_id": "64ec877bb93654d4ca5c92e9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                        "isPro": true,
                        "fullname": "Zeyu Zhang",
                        "user": "SteveZeyuZhang",
                        "type": "user"
                    },
                    "name": "Zeyu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:34:10.259Z",
                    "hidden": false
                },
                {
                    "_id": "68d229631ca7156988a8eddf",
                    "name": "Biao Wu",
                    "hidden": false
                },
                {
                    "_id": "68d229631ca7156988a8ede0",
                    "name": "Hao Tang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/u348NeyvlFMCYWemMJdoi.mp4"
            ],
            "publishedAt": "2025-09-19T20:57:03.000Z",
            "submittedOnDailyAt": "2025-09-23T03:31:25.680Z",
            "title": "StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes",
            "submittedOnDailyBy": {
                "_id": "64ec877bb93654d4ca5c92e9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                "isPro": true,
                "fullname": "Zeyu Zhang",
                "user": "SteveZeyuZhang",
                "type": "user"
            },
            "summary": "Underwater stereo depth estimation provides accurate 3D geometry for robotics\ntasks such as navigation, inspection, and mapping, offering metric depth from\nlow-cost passive cameras while avoiding the scale ambiguity of monocular\nmethods. However, existing approaches face two critical challenges: (i)\nparameter-efficiently adapting large vision foundation encoders to the\nunderwater domain without extensive labeled data, and (ii) tightly fusing\nglobally coherent but scale-ambiguous monocular priors with locally metric yet\nphotometrically fragile stereo correspondences. To address these challenges, we\npropose StereoAdapter, a parameter-efficient self-supervised framework that\nintegrates a LoRA-adapted monocular foundation encoder with a recurrent stereo\nrefinement module. We further introduce dynamic LoRA adaptation for efficient\nrank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to\nenhance robustness under diverse underwater conditions. Comprehensive\nevaluations on both simulated and real-world benchmarks show improvements of\n6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods,\nwhile real-world deployment with the BlueROV2 robot further demonstrates the\nconsistent robustness of our approach. Code:\nhttps://github.com/AIGeeksGroup/StereoAdapter. Website:\nhttps://aigeeksgroup.github.io/StereoAdapter.",
            "upvotes": 1,
            "discussionId": "68d229641ca7156988a8ede1",
            "projectPage": "https://aigeeksgroup.github.io/StereoAdapter/",
            "githubRepo": "https://github.com/AIGeeksGroup/StereoAdapter",
            "ai_summary": "StereoAdapter is a parameter-efficient self-supervised framework that integrates a LoRA-adapted monocular encoder with a recurrent stereo refinement module for underwater stereo depth estimation, improving accuracy and robustness.",
            "ai_keywords": [
                "stereo depth estimation",
                "parameter-efficient",
                "self-supervised framework",
                "LoRA-adapted",
                "monocular foundation encoder",
                "recurrent stereo refinement module",
                "dynamic LoRA adaptation",
                "UW-StereoDepth-40K dataset",
                "TartanAir",
                "SQUID",
                "BlueROV2"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-09-19T16:57:03.000Z",
        "title": "StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes",
        "summary": "Underwater stereo depth estimation provides accurate 3D geometry for robotics\ntasks such as navigation, inspection, and mapping, offering metric depth from\nlow-cost passive cameras while avoiding the scale ambiguity of monocular\nmethods. However, existing approaches face two critical challenges: (i)\nparameter-efficiently adapting large vision foundation encoders to the\nunderwater domain without extensive labeled data, and (ii) tightly fusing\nglobally coherent but scale-ambiguous monocular priors with locally metric yet\nphotometrically fragile stereo correspondences. To address these challenges, we\npropose StereoAdapter, a parameter-efficient self-supervised framework that\nintegrates a LoRA-adapted monocular foundation encoder with a recurrent stereo\nrefinement module. We further introduce dynamic LoRA adaptation for efficient\nrank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to\nenhance robustness under diverse underwater conditions. Comprehensive\nevaluations on both simulated and real-world benchmarks show improvements of\n6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods,\nwhile real-world deployment with the BlueROV2 robot further demonstrates the\nconsistent robustness of our approach. Code:\nhttps://github.com/AIGeeksGroup/StereoAdapter. Website:\nhttps://aigeeksgroup.github.io/StereoAdapter.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/u348NeyvlFMCYWemMJdoi.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.16415.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "fullname": "Zeyu Zhang",
            "name": "SteveZeyuZhang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.14856",
            "authors": [
                {
                    "_id": "68d2371c1ca7156988a8ee2e",
                    "name": "Hanyang Guo",
                    "hidden": false
                },
                {
                    "_id": "68d2371c1ca7156988a8ee2f",
                    "name": "Xunjin Zheng",
                    "hidden": false
                },
                {
                    "_id": "68d2371c1ca7156988a8ee30",
                    "name": "Zihan Liao",
                    "hidden": false
                },
                {
                    "_id": "68d2371c1ca7156988a8ee31",
                    "name": "Hang Yu",
                    "hidden": false
                },
                {
                    "_id": "68d2371c1ca7156988a8ee32",
                    "name": "Peng DI",
                    "hidden": false
                },
                {
                    "_id": "68d2371c1ca7156988a8ee33",
                    "user": {
                        "_id": "6430bdd8cd31d174a9f900fb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
                        "isPro": false,
                        "fullname": "Ziyin Zhang",
                        "user": "Geralt-Targaryen",
                        "type": "user"
                    },
                    "name": "Ziyin Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:05:07.145Z",
                    "hidden": false
                },
                {
                    "_id": "68d2371c1ca7156988a8ee34",
                    "name": "Hong-Ning Dai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-18T11:24:09.000Z",
            "submittedOnDailyAt": "2025-09-23T04:34:40.984Z",
            "title": "CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End\n  Code Review Evaluation in Python Projects",
            "submittedOnDailyBy": {
                "_id": "6430bdd8cd31d174a9f900fb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
                "isPro": false,
                "fullname": "Ziyin Zhang",
                "user": "Geralt-Targaryen",
                "type": "user"
            },
            "summary": "Automated code review (CR) is a key application for Large Language Models\n(LLMs), but progress is hampered by a \"reality gap\": existing benchmarks\nevaluate models on isolated sub-tasks using simplified, context-poor data. This\nfails to reflect the holistic context-rich nature of real-world CR. To bridge\nthis gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware\nbenchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601\nhigh-quality instances from 70 Python projects covering nine Pull-Request (PR)\nproblem domains, where each instance provides rich, multi-faceted context\nincluding the associated issue, PR details, and repository state, enabling\nend-to-end evaluation. Beyond superficial metrics, we also propose a novel\nevaluation framework that combines rule-based checks for location and syntax\nwith model-based judgments of review quality. We present the first large-scale\nassessment of state-of-the-art LLMs on this comprehensive CR task. Our results\nestablish crucial baselines and reveal that (1) no single LLM dominates all\naspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive\nperformance; and (3) different LLMs exhibit varying robustness to redundant\ncontext. These findings highlight the necessity of holistic, multi-dimensional\nevaluation and provide actionable insights for advancing truly intelligent yet\npractical CR assistants.",
            "upvotes": 1,
            "discussionId": "68d2371c1ca7156988a8ee35",
            "ai_summary": "A new benchmark, CodeFuse-CR-Bench, evaluates LLMs in repository-level code review with comprehensive, context-rich data and a novel evaluation framework.",
            "ai_keywords": [
                "Large Language Models",
                "automated code review",
                "CodeFuse-CR-Bench",
                "repository-level CR evaluation",
                "Pull-Request problem domains",
                "rule-based checks",
                "model-based judgments",
                "review quality",
                "Gemini 2.5 Pro"
            ]
        },
        "publishedAt": "2025-09-18T07:24:09.000Z",
        "title": "CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End\n  Code Review Evaluation in Python Projects",
        "summary": "Automated code review (CR) is a key application for Large Language Models\n(LLMs), but progress is hampered by a \"reality gap\": existing benchmarks\nevaluate models on isolated sub-tasks using simplified, context-poor data. This\nfails to reflect the holistic context-rich nature of real-world CR. To bridge\nthis gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware\nbenchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601\nhigh-quality instances from 70 Python projects covering nine Pull-Request (PR)\nproblem domains, where each instance provides rich, multi-faceted context\nincluding the associated issue, PR details, and repository state, enabling\nend-to-end evaluation. Beyond superficial metrics, we also propose a novel\nevaluation framework that combines rule-based checks for location and syntax\nwith model-based judgments of review quality. We present the first large-scale\nassessment of state-of-the-art LLMs on this comprehensive CR task. Our results\nestablish crucial baselines and reveal that (1) no single LLM dominates all\naspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive\nperformance; and (3) different LLMs exhibit varying robustness to redundant\ncontext. These findings highlight the necessity of holistic, multi-dimensional\nevaluation and provide actionable insights for advancing truly intelligent yet\npractical CR assistants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14856.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6430bdd8cd31d174a9f900fb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
            "fullname": "Ziyin Zhang",
            "name": "Geralt-Targaryen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.09873",
            "authors": [
                {
                    "_id": "68cccbb13df9ac65e93dc69a",
                    "user": {
                        "_id": "65ac8bed46d2f7fe546994d5",
                        "avatarUrl": "/avatars/52cf859640dba9a466601a2c532e3442.svg",
                        "isPro": false,
                        "fullname": "James Jewitt",
                        "user": "jejewitt",
                        "type": "user"
                    },
                    "name": "James Jewitt",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:30:15.400Z",
                    "hidden": false
                },
                {
                    "_id": "68cccbb13df9ac65e93dc69b",
                    "user": {
                        "_id": "62b4f3b7464e664268bf4e85",
                        "avatarUrl": "/avatars/16e79e11a8734b1d241e0f0c55a54045.svg",
                        "isPro": false,
                        "fullname": "Leo",
                        "user": "hao-li",
                        "type": "user"
                    },
                    "name": "Hao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:16.570Z",
                    "hidden": false
                },
                {
                    "_id": "68cccbb13df9ac65e93dc69c",
                    "user": {
                        "_id": "62f51d2231ee3f3670f7fe1b",
                        "avatarUrl": "/avatars/20fe13d191738a0b2d6cff6c4d3d073b.svg",
                        "isPro": false,
                        "fullname": "Bram Adams",
                        "user": "bramses",
                        "type": "user"
                    },
                    "name": "Bram Adams",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:33:19.761Z",
                    "hidden": false
                },
                {
                    "_id": "68cccbb13df9ac65e93dc69d",
                    "user": {
                        "_id": "679bb3a518279c15eb512f08",
                        "avatarUrl": "/avatars/390c7abfa9b69c425c2cb4b370aec8f9.svg",
                        "isPro": false,
                        "fullname": "Gopi Krishnan Rajbahadur",
                        "user": "gopirajbahadur",
                        "type": "user"
                    },
                    "name": "Gopi Krishnan Rajbahadur",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:29:59.924Z",
                    "hidden": false
                },
                {
                    "_id": "68cccbb13df9ac65e93dc69e",
                    "name": "Ahmed E. Hassan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-11T21:46:20.000Z",
            "submittedOnDailyAt": "2025-09-23T01:35:42.612Z",
            "title": "From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI\n  Ecosystem",
            "submittedOnDailyBy": {
                "_id": "62b4f3b7464e664268bf4e85",
                "avatarUrl": "/avatars/16e79e11a8734b1d241e0f0c55a54045.svg",
                "isPro": false,
                "fullname": "Leo",
                "user": "hao-li",
                "type": "user"
            },
            "summary": "Hidden license conflicts in the open-source AI ecosystem pose serious legal\nand ethical risks, exposing organizations to potential litigation and users to\nundisclosed risk. However, the field lacks a data-driven understanding of how\nfrequently these conflicts occur, where they originate, and which communities\nare most affected. We present the first end-to-end audit of licenses for\ndatasets and models on Hugging Face, as well as their downstream integration\ninto open-source software applications, covering 364 thousand datasets, 1.6\nmillion models, and 140 thousand GitHub projects. Our empirical analysis\nreveals systemic non-compliance in which 35.5% of model-to-application\ntransitions eliminate restrictive license clauses by relicensing under\npermissive terms. In addition, we prototype an extensible rule engine that\nencodes almost 200 SPDX and model-specific clauses for detecting license\nconflicts, which can solve 86.4% of license conflicts in software applications.\nTo support future research, we release our dataset and the prototype engine.\nOur study highlights license compliance as a critical governance challenge in\nopen-source AI and provides both the data and tools necessary to enable\nautomated, AI-aware compliance at scale.",
            "upvotes": 1,
            "discussionId": "68cccbb13df9ac65e93dc69f",
            "ai_summary": "The study audits licenses in the Hugging Face ecosystem, revealing systemic non-compliance and proposing a rule engine to detect and resolve license conflicts in open-source AI.",
            "ai_keywords": [
                "license conflicts",
                "Hugging Face",
                "datasets",
                "models",
                "GitHub projects",
                "SPDX",
                "rule engine",
                "license compliance",
                "governance challenge",
                "automated compliance"
            ]
        },
        "publishedAt": "2025-09-11T17:46:20.000Z",
        "title": "From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI\n  Ecosystem",
        "summary": "Hidden license conflicts in the open-source AI ecosystem pose serious legal\nand ethical risks, exposing organizations to potential litigation and users to\nundisclosed risk. However, the field lacks a data-driven understanding of how\nfrequently these conflicts occur, where they originate, and which communities\nare most affected. We present the first end-to-end audit of licenses for\ndatasets and models on Hugging Face, as well as their downstream integration\ninto open-source software applications, covering 364 thousand datasets, 1.6\nmillion models, and 140 thousand GitHub projects. Our empirical analysis\nreveals systemic non-compliance in which 35.5% of model-to-application\ntransitions eliminate restrictive license clauses by relicensing under\npermissive terms. In addition, we prototype an extensible rule engine that\nencodes almost 200 SPDX and model-specific clauses for detecting license\nconflicts, which can solve 86.4% of license conflicts in software applications.\nTo support future research, we release our dataset and the prototype engine.\nOur study highlights license compliance as a critical governance challenge in\nopen-source AI and provides both the data and tools necessary to enable\nautomated, AI-aware compliance at scale.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09873.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62b4f3b7464e664268bf4e85",
            "avatarUrl": "/avatars/16e79e11a8734b1d241e0f0c55a54045.svg",
            "fullname": "Leo",
            "name": "hao-li",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.04441",
            "authors": [
                {
                    "_id": "68d2f4050e215259d193b147",
                    "name": "Hao-Shu Fang",
                    "hidden": false
                },
                {
                    "_id": "68d2f4050e215259d193b148",
                    "name": "Branden Romero",
                    "hidden": false
                },
                {
                    "_id": "68d2f4050e215259d193b149",
                    "name": "Yichen Xie",
                    "hidden": false
                },
                {
                    "_id": "68d2f4050e215259d193b14a",
                    "name": "Arthur Hu",
                    "hidden": false
                },
                {
                    "_id": "68d2f4050e215259d193b14b",
                    "name": "Bo-Ruei Huang",
                    "hidden": false
                },
                {
                    "_id": "68d2f4050e215259d193b14c",
                    "name": "Juan Alvarez",
                    "hidden": false
                },
                {
                    "_id": "68d2f4050e215259d193b14d",
                    "name": "Matthew Kim",
                    "hidden": false
                },
                {
                    "_id": "68d2f4050e215259d193b14e",
                    "name": "Gabriel Margolis",
                    "hidden": false
                },
                {
                    "_id": "68d2f4050e215259d193b14f",
                    "name": "Kavya Anbarasu",
                    "hidden": false
                },
                {
                    "_id": "68d2f4050e215259d193b150",
                    "name": "Masayoshi Tomizuka",
                    "hidden": false
                },
                {
                    "_id": "68d2f4050e215259d193b151",
                    "name": "Edward Adelson",
                    "hidden": false
                },
                {
                    "_id": "68d2f4050e215259d193b152",
                    "name": "Pulkit Agrawal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-04T17:57:13.000Z",
            "submittedOnDailyAt": "2025-09-23T18:00:40.457Z",
            "title": "DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io.",
            "upvotes": 1,
            "discussionId": "68d2f4050e215259d193b153",
            "ai_summary": "DEXOP, a passive hand exoskeleton, enhances robotic data collection by sensorizing human manipulation, improving data transferability and task performance.",
            "ai_keywords": [
                "perioperation",
                "DEXOP",
                "passive hand exoskeleton",
                "sensory data",
                "dexterous manipulation",
                "natural environments",
                "proprioception",
                "pose mirroring",
                "force feedback",
                "teleoperation",
                "high-quality demonstration data",
                "task performance"
            ]
        },
        "publishedAt": "2025-09-04T13:57:13.000Z",
        "title": "DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation",
        "summary": "We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04441.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8129
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.16548",
            "authors": [
                {
                    "_id": "68d237f71ca7156988a8ee42",
                    "user": {
                        "_id": "626cf0f65651e31a7a2b9779",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626cf0f65651e31a7a2b9779/TAE--QIKo1vz4Rb8aearl.jpeg",
                        "isPro": false,
                        "fullname": "Ding",
                        "user": "dyyyyyyyy",
                        "type": "user"
                    },
                    "name": "Yuyang Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:05:04.637Z",
                    "hidden": false
                },
                {
                    "_id": "68d237f71ca7156988a8ee43",
                    "user": {
                        "_id": "6380ff85f496d57325c2ac2b",
                        "avatarUrl": "/avatars/0b9eb9c8a96d5ffd905f9ddeee78e6f3.svg",
                        "isPro": false,
                        "fullname": "Xinyu Shi",
                        "user": "XinyuShi",
                        "type": "user"
                    },
                    "name": "Xinyu Shi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:28:19.824Z",
                    "hidden": false
                },
                {
                    "_id": "68d237f71ca7156988a8ee44",
                    "name": "Juntao Li",
                    "hidden": false
                },
                {
                    "_id": "68d237f71ca7156988a8ee45",
                    "user": {
                        "_id": "64116770230ce11b1be7f3b2",
                        "avatarUrl": "/avatars/d554406a121666c3e16bc3321415ccb1.svg",
                        "isPro": false,
                        "fullname": "Xiaobo Liang",
                        "user": "dropreg",
                        "type": "user"
                    },
                    "name": "Xiaobo Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:28:34.022Z",
                    "hidden": false
                },
                {
                    "_id": "68d237f71ca7156988a8ee46",
                    "user": {
                        "_id": "67485743561b1e6f9579389f",
                        "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
                        "isPro": false,
                        "fullname": "Zhaopeng Tu",
                        "user": "zptu",
                        "type": "user"
                    },
                    "name": "Zhaopeng Tu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:28:41.973Z",
                    "hidden": false
                },
                {
                    "_id": "68d237f71ca7156988a8ee47",
                    "name": "Min Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-20T06:19:55.000Z",
            "submittedOnDailyAt": "2025-09-23T04:33:07.311Z",
            "title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward\n  Learning",
            "submittedOnDailyBy": {
                "_id": "626cf0f65651e31a7a2b9779",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626cf0f65651e31a7a2b9779/TAE--QIKo1vz4Rb8aearl.jpeg",
                "isPro": false,
                "fullname": "Ding",
                "user": "dyyyyyyyy",
                "type": "user"
            },
            "summary": "Process reward models (PRMs) offer fine-grained, step-level evaluations that\nfacilitate deeper reasoning processes in large language models (LLMs), proving\neffective in complex tasks like mathematical reasoning. However, developing\nPRMs is challenging due to the high cost and limited scalability of\nhuman-annotated data. Synthetic data from Monte Carlo (MC) estimation is a\npromising alternative but suffers from a high noise ratio, which can cause\noverfitting and hinder large-scale training. In this work, we conduct a\npreliminary study on the noise distribution in synthetic data from MC\nestimation, identifying that annotation models tend to both underestimate and\noverestimate step correctness due to limitations in their annotation\ncapabilities. Building on these insights, we propose Self-Denoising Monte Carlo\nAnnotation (SCAN), an efficient data synthesis and noise-tolerant learning\nframework. Our key findings indicate that: (1) Even lightweight models (e.g.,\n1.5B parameters) can produce high-quality annotations through a self-denoising\nstrategy, enabling PRMs to achieve superior performance with only 6% the\ninference cost required by vanilla MC estimation. (2) With our robust learning\nstrategy, PRMs can effectively learn from this weak supervision, achieving a\n39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using\nonly a compact synthetic dataset, our models surpass strong baselines,\nincluding those trained on large-scale human-annotated datasets such as\nPRM800K. Furthermore, performance continues to improve as we scale up the\nsynthetic data, highlighting the potential of SCAN for scalable,\ncost-efficient, and robust PRM training.",
            "upvotes": 0,
            "discussionId": "68d237f71ca7156988a8ee48",
            "ai_summary": "SCAN, a self-denoising Monte Carlo framework, improves PRM performance with synthetic data, achieving high F1 scores and surpassing human-annotated baselines.",
            "ai_keywords": [
                "Process reward models",
                "Monte Carlo estimation",
                "self-denoising",
                "noise distribution",
                "annotation models",
                "ProcessBench",
                "F1 score",
                "PRM800K"
            ]
        },
        "publishedAt": "2025-09-20T02:19:55.000Z",
        "title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward\n  Learning",
        "summary": "Process reward models (PRMs) offer fine-grained, step-level evaluations that\nfacilitate deeper reasoning processes in large language models (LLMs), proving\neffective in complex tasks like mathematical reasoning. However, developing\nPRMs is challenging due to the high cost and limited scalability of\nhuman-annotated data. Synthetic data from Monte Carlo (MC) estimation is a\npromising alternative but suffers from a high noise ratio, which can cause\noverfitting and hinder large-scale training. In this work, we conduct a\npreliminary study on the noise distribution in synthetic data from MC\nestimation, identifying that annotation models tend to both underestimate and\noverestimate step correctness due to limitations in their annotation\ncapabilities. Building on these insights, we propose Self-Denoising Monte Carlo\nAnnotation (SCAN), an efficient data synthesis and noise-tolerant learning\nframework. Our key findings indicate that: (1) Even lightweight models (e.g.,\n1.5B parameters) can produce high-quality annotations through a self-denoising\nstrategy, enabling PRMs to achieve superior performance with only 6% the\ninference cost required by vanilla MC estimation. (2) With our robust learning\nstrategy, PRMs can effectively learn from this weak supervision, achieving a\n39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using\nonly a compact synthetic dataset, our models surpass strong baselines,\nincluding those trained on large-scale human-annotated datasets such as\nPRM800K. Furthermore, performance continues to improve as we scale up the\nsynthetic data, highlighting the potential of SCAN for scalable,\ncost-efficient, and robust PRM training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.16548.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "626cf0f65651e31a7a2b9779",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626cf0f65651e31a7a2b9779/TAE--QIKo1vz4Rb8aearl.jpeg",
            "fullname": "Ding",
            "name": "dyyyyyyyy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.16195",
            "authors": [
                {
                    "_id": "68d0a5f88adc5cd018d15a80",
                    "user": {
                        "_id": "63195d0582e7eec0eac040e3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63195d0582e7eec0eac040e3/0tXOYkMfmv9e53zBWgqz7.png",
                        "isPro": false,
                        "fullname": "Luca Della Libera",
                        "user": "lucadellalib",
                        "type": "user"
                    },
                    "name": "Luca Della Libera",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-22T03:01:09.815Z",
                    "hidden": false
                },
                {
                    "_id": "68d0a5f88adc5cd018d15a81",
                    "name": "Cem Subakan",
                    "hidden": false
                },
                {
                    "_id": "68d0a5f88adc5cd018d15a82",
                    "name": "Mirco Ravanelli",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-19T17:57:13.000Z",
            "submittedOnDailyAt": "2025-09-23T22:22:58.495Z",
            "title": "FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal\n  Distillation",
            "submittedOnDailyBy": {
                "_id": "63195d0582e7eec0eac040e3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63195d0582e7eec0eac040e3/0tXOYkMfmv9e53zBWgqz7.png",
                "isPro": false,
                "fullname": "Luca Della Libera",
                "user": "lucadellalib",
                "type": "user"
            },
            "summary": "Neural audio codecs are a fundamental component of modern generative audio\npipelines. Although recent codecs achieve strong low-bitrate reconstruction and\nprovide powerful representations for downstream tasks, most are non-streamable,\nlimiting their use in real-time applications. We present FocalCodec-Stream, a\nhybrid codec based on focal modulation that compresses speech into a single\nbinary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our\napproach combines multi-stage causal distillation of WavLM with targeted\narchitectural improvements, including a lightweight refiner module that\nenhances quality under latency constraints. Experiments show that\nFocalCodec-Stream outperforms existing streamable codecs at comparable\nbitrates, while preserving both semantic and acoustic information. The result\nis a favorable trade-off between reconstruction quality, downstream task\nperformance, latency, and efficiency. Code and checkpoints will be released at\nhttps://github.com/lucadellalib/focalcodec.",
            "upvotes": 0,
            "discussionId": "68d0a5f88adc5cd018d15a83",
            "ai_summary": "FocalCodec-Stream is a hybrid neural audio codec that achieves high-quality speech compression with low latency and is suitable for real-time applications.",
            "ai_keywords": [
                "neural audio codecs",
                "focal modulation",
                "binary codebook",
                "multi-stage causal distillation",
                "WavLM",
                "lightweight refiner module",
                "streamable codecs",
                "semantic information",
                "acoustic information",
                "reconstruction quality",
                "downstream task performance",
                "latency",
                "efficiency"
            ]
        },
        "publishedAt": "2025-09-19T13:57:13.000Z",
        "title": "FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal\n  Distillation",
        "summary": "Neural audio codecs are a fundamental component of modern generative audio\npipelines. Although recent codecs achieve strong low-bitrate reconstruction and\nprovide powerful representations for downstream tasks, most are non-streamable,\nlimiting their use in real-time applications. We present FocalCodec-Stream, a\nhybrid codec based on focal modulation that compresses speech into a single\nbinary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our\napproach combines multi-stage causal distillation of WavLM with targeted\narchitectural improvements, including a lightweight refiner module that\nenhances quality under latency constraints. Experiments show that\nFocalCodec-Stream outperforms existing streamable codecs at comparable\nbitrates, while preserving both semantic and acoustic information. The result\nis a favorable trade-off between reconstruction quality, downstream task\nperformance, latency, and efficiency. Code and checkpoints will be released at\nhttps://github.com/lucadellalib/focalcodec.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.16195.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63195d0582e7eec0eac040e3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63195d0582e7eec0eac040e3/0tXOYkMfmv9e53zBWgqz7.png",
            "fullname": "Luca Della Libera",
            "name": "lucadellalib",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    }
]