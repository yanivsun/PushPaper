[
    {
        "paper": {
            "id": "2509.18174",
            "authors": [
                {
                    "_id": "68d38bec0e215259d193b388",
                    "user": {
                        "_id": "65276c7911a8a521c91bc10f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
                        "isPro": false,
                        "fullname": "Khalil Hennara",
                        "user": "Hennara",
                        "type": "user"
                    },
                    "name": "Khalil Hennara",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:52:41.543Z",
                    "hidden": false
                },
                {
                    "_id": "68d38bec0e215259d193b389",
                    "user": {
                        "_id": "6496df4b3c64d75523a11973",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6496df4b3c64d75523a11973/I_Qn5-3Czngle-NsGmabO.jpeg",
                        "isPro": false,
                        "fullname": "Muhammad Hreden",
                        "user": "muhammad0-0hreden",
                        "type": "user"
                    },
                    "name": "Muhammad Hreden",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:52:34.689Z",
                    "hidden": false
                },
                {
                    "_id": "68d38bec0e215259d193b38a",
                    "user": {
                        "_id": "63aa7667769a10efc404fbbc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63aa7667769a10efc404fbbc/tn8ZxUmTEMS0Gze7_F7JL.jpeg",
                        "isPro": false,
                        "fullname": "Mohamed Motasim Hamed",
                        "user": "Moatasem444",
                        "type": "user"
                    },
                    "name": "Mohamed Motasim Hamed",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:52:31.754Z",
                    "hidden": false
                },
                {
                    "_id": "68d38bec0e215259d193b38b",
                    "user": {
                        "_id": "662a4615cf541c92cf0062b9",
                        "avatarUrl": "/avatars/1f0e4307edb6b441c3a807de317a6953.svg",
                        "isPro": false,
                        "fullname": "Ahmad Bastati",
                        "user": "Bastati",
                        "type": "user"
                    },
                    "name": "Ahmad Bastati",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:25:38.733Z",
                    "hidden": false
                },
                {
                    "_id": "68d38bec0e215259d193b38c",
                    "user": {
                        "_id": "65704741e1cfce1764ce652e",
                        "avatarUrl": "/avatars/9189aaf417426af4ebe381ed364a6c0e.svg",
                        "isPro": false,
                        "fullname": "Zeina Aldallal",
                        "user": "ZeinaD",
                        "type": "user"
                    },
                    "name": "Zeina Aldallal",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:52:37.383Z",
                    "hidden": false
                },
                {
                    "_id": "68d38bec0e215259d193b38d",
                    "name": "Sara Chrouf",
                    "hidden": false
                },
                {
                    "_id": "68d38bec0e215259d193b38e",
                    "name": "Safwan AlModhayan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-17T15:07:29.000Z",
            "submittedOnDailyAt": "2025-09-24T04:44:39.971Z",
            "title": "Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR",
            "submittedOnDailyBy": {
                "_id": "65276c7911a8a521c91bc10f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
                "isPro": false,
                "fullname": "Khalil Hennara",
                "user": "Hennara",
                "type": "user"
            },
            "summary": "Arabic document OCR remains a challenging task due to the language's cursive\nscript, diverse fonts, diacritics, and right-to-left orientation. While modern\nMultimodal Large Language Models (MLLMs) have advanced document understanding\nfor high-resource languages, their performance on Arabic remains limited. In\nthis work, we introduce Baseer, a vision-language model fine- tuned\nspecifically for Arabic document OCR. Leveraging a large-scale dataset\ncombining synthetic and real-world documents, Baseer is trained using a\ndecoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving\ngeneral visual features. We also present Misraj-DocOCR, a high-quality,\nexpert-verified benchmark designed for rigorous evaluation of Arabic OCR\nsystems. Our experiments show that Baseer significantly outperforms existing\nopen-source and commercial solutions, achieving a WER of 0.25 and establishing\na new state-of-the-art in the domain of Arabic document OCR. Our results\nhighlight the benefits of domain-specific adaptation of general-purpose MLLMs\nand establish a strong baseline for high-accuracy OCR on morphologically rich\nlanguages like Arabic.",
            "upvotes": 83,
            "discussionId": "68d38bec0e215259d193b38f",
            "projectPage": "https://oinsight.ai/",
            "ai_summary": "Baseer, a vision-language model fine-tuned for Arabic document OCR, achieves state-of-the-art performance using a decoder-only strategy and a large-scale dataset, outperforming existing solutions with a WER of 0.25.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "vision-language model",
                "decoder-only fine-tuning",
                "Misraj-DocOCR",
                "WER",
                "Arabic document OCR"
            ]
        },
        "publishedAt": "2025-09-17T11:07:29.000Z",
        "title": "Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR",
        "summary": "Arabic document OCR remains a challenging task due to the language's cursive\nscript, diverse fonts, diacritics, and right-to-left orientation. While modern\nMultimodal Large Language Models (MLLMs) have advanced document understanding\nfor high-resource languages, their performance on Arabic remains limited. In\nthis work, we introduce Baseer, a vision-language model fine- tuned\nspecifically for Arabic document OCR. Leveraging a large-scale dataset\ncombining synthetic and real-world documents, Baseer is trained using a\ndecoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving\ngeneral visual features. We also present Misraj-DocOCR, a high-quality,\nexpert-verified benchmark designed for rigorous evaluation of Arabic OCR\nsystems. Our experiments show that Baseer significantly outperforms existing\nopen-source and commercial solutions, achieving a WER of 0.25 and establishing\na new state-of-the-art in the domain of Arabic document OCR. Our results\nhighlight the benefits of domain-specific adaptation of general-purpose MLLMs\nand establish a strong baseline for high-accuracy OCR on morphologically rich\nlanguages like Arabic.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18174.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65276c7911a8a521c91bc10f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
            "fullname": "Khalil Hennara",
            "name": "Hennara",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.19249",
            "authors": [
                {
                    "_id": "68d352680e215259d193b1fa",
                    "user": {
                        "_id": "66d45a8de5837f38ce3b73f7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d45a8de5837f38ce3b73f7/3omslNRb8wV_c1xbrCmQC.jpeg",
                        "isPro": false,
                        "fullname": "SihengLi",
                        "user": "Siheng99",
                        "type": "user"
                    },
                    "name": "Siheng Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:53:23.476Z",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b1fb",
                    "name": "Kejiao Li",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b1fc",
                    "user": {
                        "_id": "66dd5a7468f47ec63e502794",
                        "avatarUrl": "/avatars/6497fd124e6009777167feef2558f058.svg",
                        "isPro": false,
                        "fullname": "xu",
                        "user": "xavier-z",
                        "type": "user"
                    },
                    "name": "Zenan Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:53:18.104Z",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b1fd",
                    "name": "Guanhua Huang",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b1fe",
                    "name": "Evander Yang",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b1ff",
                    "name": "Kun Li",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b200",
                    "user": {
                        "_id": "6445e7b1b272430bdbf64e80",
                        "avatarUrl": "/avatars/d3e59a3b488f8539966c944bb16f7b90.svg",
                        "isPro": false,
                        "fullname": "Haoyuan WU",
                        "user": "hywu",
                        "type": "user"
                    },
                    "name": "Haoyuan Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:53:15.202Z",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b201",
                    "name": "Jiajia Wu",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b202",
                    "name": "Zihao Zheng",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b203",
                    "name": "Chenchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b204",
                    "name": "Kun Shi",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b205",
                    "name": "Kyrierl Deng",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b206",
                    "name": "Qi Yi",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b207",
                    "name": "Ruibin Xiong",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b208",
                    "name": "Tingqiang Xu",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b209",
                    "name": "Yuhao Jiang",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b20a",
                    "name": "Jianfeng Yan",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b20b",
                    "name": "Yuyuan Zeng",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b20c",
                    "name": "Guanghui Xu",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b20d",
                    "name": "Jinbao Xue",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b20e",
                    "name": "Zhijiang Xu",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b20f",
                    "name": "Zheng Fang",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b210",
                    "user": {
                        "_id": "6525511b3f8d02205bf1e9ef",
                        "avatarUrl": "/avatars/7dfc3020337f3c403ecf133fa312b5d1.svg",
                        "isPro": false,
                        "fullname": "Shuai Li",
                        "user": "DiveBlue",
                        "type": "user"
                    },
                    "name": "Shuai Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:53:20.582Z",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b211",
                    "name": "Qibin Liu",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b212",
                    "name": "Xiaoxue Li",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b213",
                    "name": "Zhuoyu Li",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b214",
                    "name": "Yangyu Tao",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b215",
                    "name": "Fei Gao",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b216",
                    "name": "Cheng Jiang",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b217",
                    "name": "Bo Chao Wang",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b218",
                    "name": "Kai Liu",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b219",
                    "name": "Jianchen Zhu",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b21a",
                    "name": "Wai Lam",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b21b",
                    "name": "Wayyt Wang",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b21c",
                    "name": "Bo Zhou",
                    "hidden": false
                },
                {
                    "_id": "68d352680e215259d193b21d",
                    "name": "Di Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-23T17:10:40.000Z",
            "submittedOnDailyAt": "2025-09-24T00:37:49.095Z",
            "title": "Reinforcement Learning on Pre-Training Data",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The growing disparity between the exponential scaling of computational\nresources and the finite growth of high-quality text data now constrains\nconventional scaling approaches for large language models (LLMs). To address\nthis challenge, we introduce Reinforcement Learning on Pre-Training data\n(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast\nto prior approaches that scale training primarily through supervised learning,\nRLPT enables the policy to autonomously explore meaningful trajectories to\nlearn from pre-training data and improve its capability through reinforcement\nlearning (RL). While existing RL strategies such as reinforcement learning from\nhuman feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)\nrely on human annotation for reward construction, RLPT eliminates this\ndependency by deriving reward signals directly from pre-training data.\nSpecifically, it adopts a next-segment reasoning objective, rewarding the\npolicy for accurately predicting subsequent text segments conditioned on the\npreceding context. This formulation allows RL to be scaled on pre-training\ndata, encouraging the exploration of richer trajectories across broader\ncontexts and thereby fostering more generalizable reasoning skills. Extensive\nexperiments on both general-domain and mathematical reasoning benchmarks across\nmultiple models validate the effectiveness of RLPT. For example, when applied\nto Qwen3-4B-Base, RLPT yields absolute improvements of 3.0, 5.1, 8.1,\n6.0, 6.6, and 5.3 on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and\nAIME25, respectively. The results further demonstrate favorable scaling\nbehavior, suggesting strong potential for continued gains with more compute. In\naddition, RLPT provides a solid foundation, extending the reasoning boundaries\nof LLMs and enhancing RLVR performance.",
            "upvotes": 43,
            "discussionId": "68d352680e215259d193b21e",
            "ai_summary": "Reinforcement Learning on Pre-Training data (RLPT) optimizes large language models by autonomously exploring meaningful trajectories in pre-training data, improving generalizable reasoning skills without human annotation.",
            "ai_keywords": [
                "Reinforcement Learning on Pre-Training data",
                "RLPT",
                "large language models",
                "LLMs",
                "reinforcement learning",
                "RL",
                "reinforcement learning from human feedback",
                "RLHF",
                "reinforcement learning with verifiable rewards",
                "RLVR",
                "next-segment reasoning objective",
                "MMLU",
                "MMLU-Pro",
                "GPQA-Diamond",
                "KOR-Bench",
                "AIME24",
                "AIME25"
            ]
        },
        "publishedAt": "2025-09-23T13:10:40.000Z",
        "title": "Reinforcement Learning on Pre-Training Data",
        "summary": "The growing disparity between the exponential scaling of computational\nresources and the finite growth of high-quality text data now constrains\nconventional scaling approaches for large language models (LLMs). To address\nthis challenge, we introduce Reinforcement Learning on Pre-Training data\n(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast\nto prior approaches that scale training primarily through supervised learning,\nRLPT enables the policy to autonomously explore meaningful trajectories to\nlearn from pre-training data and improve its capability through reinforcement\nlearning (RL). While existing RL strategies such as reinforcement learning from\nhuman feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)\nrely on human annotation for reward construction, RLPT eliminates this\ndependency by deriving reward signals directly from pre-training data.\nSpecifically, it adopts a next-segment reasoning objective, rewarding the\npolicy for accurately predicting subsequent text segments conditioned on the\npreceding context. This formulation allows RL to be scaled on pre-training\ndata, encouraging the exploration of richer trajectories across broader\ncontexts and thereby fostering more generalizable reasoning skills. Extensive\nexperiments on both general-domain and mathematical reasoning benchmarks across\nmultiple models validate the effectiveness of RLPT. For example, when applied\nto Qwen3-4B-Base, RLPT yields absolute improvements of 3.0, 5.1, 8.1,\n6.0, 6.6, and 5.3 on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and\nAIME25, respectively. The results further demonstrate favorable scaling\nbehavior, suggesting strong potential for continued gains with more compute. In\naddition, RLPT provides a solid foundation, extending the reasoning boundaries\nof LLMs and enhancing RLVR performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19249.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 109
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.18644",
            "authors": [
                {
                    "_id": "68d387b50e215259d193b364",
                    "user": {
                        "_id": "63217b064b3a874743797fa5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63217b064b3a874743797fa5/FBaU2EiZFO2R_6alDcox8.png",
                        "isPro": false,
                        "fullname": "JT Zhao",
                        "user": "JTZhaoSJTU",
                        "type": "user"
                    },
                    "name": "Juntu Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:52:57.974Z",
                    "hidden": false
                },
                {
                    "_id": "68d387b50e215259d193b365",
                    "name": "Wenbo Lu",
                    "hidden": false
                },
                {
                    "_id": "68d387b50e215259d193b366",
                    "name": "Di Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d387b50e215259d193b367",
                    "user": {
                        "_id": "65d72c946a36b5b354f80cf8",
                        "avatarUrl": "/avatars/2cfc99ccd4f52f4a60c7fa40fe4313b7.svg",
                        "isPro": false,
                        "fullname": "lyfeng",
                        "user": "lyfeng001",
                        "type": "user"
                    },
                    "name": "Yufeng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:52:54.957Z",
                    "hidden": false
                },
                {
                    "_id": "68d387b50e215259d193b368",
                    "user": {
                        "_id": "683fb536ef97de05eb2b62f2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/cAO8ffm_uin1me1zVdTuk.png",
                        "isPro": false,
                        "fullname": "Yushen Liang",
                        "user": "Mirage415",
                        "type": "user"
                    },
                    "name": "Yushen Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:26:00.067Z",
                    "hidden": false
                },
                {
                    "_id": "68d387b50e215259d193b369",
                    "name": "Tianluo Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d387b50e215259d193b36a",
                    "user": {
                        "_id": "68d38f3398cf2f221b971293",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xk5Ew0vVsPyEY0fqirsSO.png",
                        "isPro": false,
                        "fullname": "Yifeng Cao",
                        "user": "MattC401",
                        "type": "user"
                    },
                    "name": "Yifeng Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:26:18.034Z",
                    "hidden": false
                },
                {
                    "_id": "68d387b50e215259d193b36b",
                    "user": {
                        "_id": "67cfea72ad91643b5cb92b26",
                        "avatarUrl": "/avatars/11937ee700bf6a743cb627928bebd214.svg",
                        "isPro": false,
                        "fullname": "Junyuan Xie",
                        "user": "piiswrong",
                        "type": "user"
                    },
                    "name": "Junyuan Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:26:25.322Z",
                    "hidden": false
                },
                {
                    "_id": "68d387b50e215259d193b36c",
                    "name": "Yingdong Hu",
                    "hidden": false
                },
                {
                    "_id": "68d387b50e215259d193b36d",
                    "name": "Shengjie Wang",
                    "hidden": false
                },
                {
                    "_id": "68d387b50e215259d193b36e",
                    "user": {
                        "_id": "66868659ccb9539da85c4e14",
                        "avatarUrl": "/avatars/515a49363872c23d57a6f75063606348.svg",
                        "isPro": false,
                        "fullname": "Junliang Guo",
                        "user": "leo-guo",
                        "type": "user"
                    },
                    "name": "Junliang Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:26:42.060Z",
                    "hidden": false
                },
                {
                    "_id": "68d387b50e215259d193b36f",
                    "user": {
                        "_id": "61ad24836da53246bd6ac410",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61ad24836da53246bd6ac410/o-FL-C6B77iB94wyAtTuO.png",
                        "isPro": false,
                        "fullname": "Dequan Wang",
                        "user": "dqwang",
                        "type": "user"
                    },
                    "name": "Dequan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:52:50.291Z",
                    "hidden": false
                },
                {
                    "_id": "68d387b50e215259d193b370",
                    "name": "Yang Gao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-23T04:56:59.000Z",
            "submittedOnDailyAt": "2025-09-24T04:41:47.105Z",
            "title": "Do You Need Proprioceptive States in Visuomotor Policies?",
            "submittedOnDailyBy": {
                "_id": "66d01e4401f2a6b4cd93ad87",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png",
                "isPro": false,
                "fullname": "Mohan Jiang",
                "user": "mhjiang0408",
                "type": "user"
            },
            "summary": "Imitation-learning-based visuomotor policies have been widely used in robot\nmanipulation, where both visual observations and proprioceptive states are\ntypically adopted together for precise control. However, in this study, we find\nthat this common practice makes the policy overly reliant on the proprioceptive\nstate input, which causes overfitting to the training trajectories and results\nin poor spatial generalization. On the contrary, we propose the State-free\nPolicy, removing the proprioceptive state input and predicting actions only\nconditioned on visual observations. The State-free Policy is built in the\nrelative end-effector action space, and should ensure the full task-relevant\nvisual observations, here provided by dual wide-angle wrist cameras. Empirical\nresults demonstrate that the State-free policy achieves significantly stronger\nspatial generalization than the state-based policy: in real-world tasks such as\npick-and-place, challenging shirt-folding, and complex whole-body manipulation,\nspanning multiple robot embodiments, the average success rate improves from 0\\%\nto 85\\% in height generalization and from 6\\% to 64\\% in horizontal\ngeneralization. Furthermore, they also show advantages in data efficiency and\ncross-embodiment adaptation, enhancing their practicality for real-world\ndeployment.",
            "upvotes": 43,
            "discussionId": "68d387b50e215259d193b371",
            "projectPage": "https://statefreepolicy.github.io",
            "ai_summary": "A state-free policy using only visual observations achieves better spatial generalization and data efficiency in robot manipulation tasks compared to state-based policies.",
            "ai_keywords": [
                "imitation-learning-based visuomotor policies",
                "proprioceptive state input",
                "overfitting",
                "spatial generalization",
                "relative end-effector action space",
                "dual wide-angle wrist cameras",
                "pick-and-place",
                "shirt-folding",
                "whole-body manipulation",
                "cross-embodiment adaptation"
            ]
        },
        "publishedAt": "2025-09-23T00:56:59.000Z",
        "title": "Do You Need Proprioceptive States in Visuomotor Policies?",
        "summary": "Imitation-learning-based visuomotor policies have been widely used in robot\nmanipulation, where both visual observations and proprioceptive states are\ntypically adopted together for precise control. However, in this study, we find\nthat this common practice makes the policy overly reliant on the proprioceptive\nstate input, which causes overfitting to the training trajectories and results\nin poor spatial generalization. On the contrary, we propose the State-free\nPolicy, removing the proprioceptive state input and predicting actions only\nconditioned on visual observations. The State-free Policy is built in the\nrelative end-effector action space, and should ensure the full task-relevant\nvisual observations, here provided by dual wide-angle wrist cameras. Empirical\nresults demonstrate that the State-free policy achieves significantly stronger\nspatial generalization than the state-based policy: in real-world tasks such as\npick-and-place, challenging shirt-folding, and complex whole-body manipulation,\nspanning multiple robot embodiments, the average success rate improves from 0\\%\nto 85\\% in height generalization and from 6\\% to 64\\% in horizontal\ngeneralization. Furthermore, they also show advantages in data efficiency and\ncross-embodiment adaptation, enhancing their practicality for real-world\ndeployment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18644.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66d01e4401f2a6b4cd93ad87",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png",
            "fullname": "Mohan Jiang",
            "name": "mhjiang0408",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.18154",
            "authors": [
                {
                    "_id": "68d351d70e215259d193b1d6",
                    "name": "Tianyu Yu",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1d7",
                    "user": {
                        "_id": "669e50bc5bc23a062865b4e4",
                        "avatarUrl": "/avatars/22f046a3806b0940bc9b0250c0678efd.svg",
                        "isPro": false,
                        "fullname": "Zefan Wang",
                        "user": "ZefanW",
                        "type": "user"
                    },
                    "name": "Zefan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:21:00.236Z",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1d8",
                    "user": {
                        "_id": "6350fa8385bdb764f6a9aa82",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6350fa8385bdb764f6a9aa82/QBp92D7x9_XImYTigBHsu.jpeg",
                        "isPro": false,
                        "fullname": "chongyi",
                        "user": "yuzaa",
                        "type": "user"
                    },
                    "name": "Chongyi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:53:26.793Z",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1d9",
                    "name": "Fuwei Huang",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1da",
                    "name": "Wenshuo Ma",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1db",
                    "user": {
                        "_id": "659fa38f83abded48e3fd315",
                        "avatarUrl": "/avatars/0daeaafc96306e061db411fec62ce40a.svg",
                        "isPro": false,
                        "fullname": "Zhihui He",
                        "user": "HwwwH",
                        "type": "user"
                    },
                    "name": "Zhihui He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:21:38.844Z",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1dc",
                    "user": {
                        "_id": "65164295d467ffdc05278d1f",
                        "avatarUrl": "/avatars/37a7fe0088e5b9eca5479a4538242073.svg",
                        "isPro": false,
                        "fullname": "Cai",
                        "user": "tianchicai",
                        "type": "user"
                    },
                    "name": "Tianchi Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:22:32.319Z",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1dd",
                    "user": {
                        "_id": "648312243b7fe59c876c0dca",
                        "avatarUrl": "/avatars/c26ad76cd213529e4670bb599b8199bb.svg",
                        "isPro": false,
                        "fullname": "weize",
                        "user": "weizechen",
                        "type": "user"
                    },
                    "name": "Weize Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:22:41.186Z",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1de",
                    "name": "Yuxiang Huang",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1df",
                    "name": "Yuanqian Zhao",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1e0",
                    "user": {
                        "_id": "6415818a986557e8cac252bf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6415818a986557e8cac252bf/T4u9qjRt8P4clF4nOTA4W.jpeg",
                        "isPro": false,
                        "fullname": "Bokai Xu",
                        "user": "bokesyo",
                        "type": "user"
                    },
                    "name": "Bokai Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:23:33.165Z",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1e1",
                    "name": "Junbo Cui",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1e2",
                    "name": "Yingjing Xu",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1e3",
                    "user": {
                        "_id": "64cb89c4aa31c5d4ec59468f",
                        "avatarUrl": "/avatars/a2ceea672f263aeba195a231b9276522.svg",
                        "isPro": false,
                        "fullname": "Liqing Ruan",
                        "user": "LiqingRuan",
                        "type": "user"
                    },
                    "name": "Liqing Ruan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:23:54.623Z",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1e4",
                    "name": "Luoyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1e5",
                    "name": "Hanyu Liu",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1e6",
                    "name": "Jingkun Tang",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1e7",
                    "name": "Hongyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1e8",
                    "user": {
                        "_id": "6697679343d9faa413b14a89",
                        "avatarUrl": "/avatars/8f99d284696850285b4dbfe5d18d0035.svg",
                        "isPro": false,
                        "fullname": "Qining Guo",
                        "user": "CGQN",
                        "type": "user"
                    },
                    "name": "Qining Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:24:40.938Z",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1e9",
                    "user": {
                        "_id": "6497f9d0a9a46fd69e17b8de",
                        "avatarUrl": "/avatars/d497ed3bc1c339cc6afc6c836e7954c8.svg",
                        "isPro": false,
                        "fullname": "Wenhao Hu",
                        "user": "fumihwh",
                        "type": "user"
                    },
                    "name": "Wenhao Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:24:49.196Z",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1ea",
                    "user": {
                        "_id": "64c5e944979493279b700cb2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vjFuPWw8Vl7b7gXB19Sk-.jpeg",
                        "isPro": false,
                        "fullname": "Bingxiang He",
                        "user": "hbx",
                        "type": "user"
                    },
                    "name": "Bingxiang He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:24:56.273Z",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1eb",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1ec",
                    "name": "Jie Cai",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1ed",
                    "name": "Ji Qi",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1ee",
                    "user": {
                        "_id": "6491af36c1741666238f3bff",
                        "avatarUrl": "/avatars/0ee7d2ec1566e2cc5e8f144140e17f00.svg",
                        "isPro": false,
                        "fullname": "Zonghao Guo",
                        "user": "guozonghao96",
                        "type": "user"
                    },
                    "name": "Zonghao Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:25:03.767Z",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1ef",
                    "name": "Chi Chen",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1f0",
                    "user": {
                        "_id": "60cd679ef3c0385f86cb07c2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624074137194-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Guoyang Zeng",
                        "user": "losfen",
                        "type": "user"
                    },
                    "name": "Guoyang Zeng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:25:11.023Z",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1f1",
                    "name": "Yuxuan Li",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1f2",
                    "user": {
                        "_id": "650eba9555dc1e841746f132",
                        "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
                        "isPro": false,
                        "fullname": "Ganqu Cui",
                        "user": "ganqu",
                        "type": "user"
                    },
                    "name": "Ganqu Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:25:18.654Z",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1f3",
                    "name": "Ning Ding",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1f4",
                    "name": "Xu Han",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1f5",
                    "name": "Yuan Yao",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1f6",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68d351d70e215259d193b1f7",
                    "name": "Maosong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-16T19:41:48.000Z",
            "submittedOnDailyAt": "2025-09-24T00:35:18.789Z",
            "title": "MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and\n  Training Recipe",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) are undergoing rapid progress and\nrepresent the frontier of AI development. However, their training and inference\nefficiency have emerged as a core bottleneck in making MLLMs more accessible\nand scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B\nparameter model designed for high efficiency and strong performance. We\nintroduce three core improvements in model architecture, data strategy and\ntraining method: a unified 3D-Resampler model architecture for highly compact\nencoding over images and videos, a unified learning paradigm for document\nknowledge and text recognition without heavy data engineering, and a hybrid\nreinforcement learning strategy for proficiency in both short and long\nreasoning modes. Comprehensive experimental results in OpenCompass evaluation\nshow that MiniCPM-V 4.5 surpasses widely used proprietary models such as\nGPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL\n72B. Notably, the strong performance is achieved with remarkable efficiency.\nFor example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves\nstate-of-the-art performance among models under 30B size, using just 46.7\\% GPU\nmemory cost and 8.7\\% inference time of Qwen2.5-VL 7B.",
            "upvotes": 32,
            "discussionId": "68d351d80e215259d193b1f8",
            "githubRepo": "https://github.com/OpenBMB/MiniCPM-V",
            "ai_summary": "MiniCPM-V 4.5, a 8B parameter multimodal large language model, achieves high performance and efficiency through a unified 3D-Resampler architecture, a unified learning paradigm, and a hybrid reinforcement learning strategy.",
            "ai_keywords": [
                "3D-Resampler",
                "unified learning paradigm",
                "hybrid reinforcement learning strategy",
                "multimodal large language models",
                "OpenCompass evaluation",
                "VideoMME benchmark"
            ],
            "githubStars": 21955
        },
        "publishedAt": "2025-09-16T15:41:48.000Z",
        "title": "MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and\n  Training Recipe",
        "summary": "Multimodal Large Language Models (MLLMs) are undergoing rapid progress and\nrepresent the frontier of AI development. However, their training and inference\nefficiency have emerged as a core bottleneck in making MLLMs more accessible\nand scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B\nparameter model designed for high efficiency and strong performance. We\nintroduce three core improvements in model architecture, data strategy and\ntraining method: a unified 3D-Resampler model architecture for highly compact\nencoding over images and videos, a unified learning paradigm for document\nknowledge and text recognition without heavy data engineering, and a hybrid\nreinforcement learning strategy for proficiency in both short and long\nreasoning modes. Comprehensive experimental results in OpenCompass evaluation\nshow that MiniCPM-V 4.5 surpasses widely used proprietary models such as\nGPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL\n72B. Notably, the strong performance is achieved with remarkable efficiency.\nFor example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves\nstate-of-the-art performance among models under 30B size, using just 46.7\\% GPU\nmemory cost and 8.7\\% inference time of Qwen2.5-VL 7B.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18154.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 109
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.18849",
            "authors": [
                {
                    "_id": "68d363d20e215259d193b257",
                    "user": {
                        "_id": "66b1dd6b93121096ffcfdab1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b1dd6b93121096ffcfdab1/sOrbqdZhbp3ZP_cUGjhlN.jpeg",
                        "isPro": false,
                        "fullname": "Wenke Huang",
                        "user": "WilliamHuang91",
                        "type": "user"
                    },
                    "name": "Wenke Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:53:06.262Z",
                    "hidden": false
                },
                {
                    "_id": "68d363d20e215259d193b258",
                    "name": "Quan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d363d20e215259d193b259",
                    "name": "Yiyang Fang",
                    "hidden": false
                },
                {
                    "_id": "68d363d20e215259d193b25a",
                    "name": "Jian Liang",
                    "hidden": false
                },
                {
                    "_id": "68d363d20e215259d193b25b",
                    "name": "Xuankun Rong",
                    "hidden": false
                },
                {
                    "_id": "68d363d20e215259d193b25c",
                    "name": "Huanjin Yao",
                    "hidden": false
                },
                {
                    "_id": "68d363d20e215259d193b25d",
                    "name": "Guancheng Wan",
                    "hidden": false
                },
                {
                    "_id": "68d363d20e215259d193b25e",
                    "name": "Ke Liang",
                    "hidden": false
                },
                {
                    "_id": "68d363d20e215259d193b25f",
                    "name": "Wenwen He",
                    "hidden": false
                },
                {
                    "_id": "68d363d20e215259d193b260",
                    "name": "Mingjun Li",
                    "hidden": false
                },
                {
                    "_id": "68d363d20e215259d193b261",
                    "name": "Leszek Rutkowski",
                    "hidden": false
                },
                {
                    "_id": "68d363d20e215259d193b262",
                    "name": "Mang Ye",
                    "hidden": false
                },
                {
                    "_id": "68d363d20e215259d193b263",
                    "name": "Bo Du",
                    "hidden": false
                },
                {
                    "_id": "68d363d20e215259d193b264",
                    "name": "Dacheng Tao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-23T09:37:16.000Z",
            "submittedOnDailyAt": "2025-09-24T01:52:30.743Z",
            "title": "MAPO: Mixed Advantage Policy Optimization",
            "submittedOnDailyBy": {
                "_id": "66b1dd6b93121096ffcfdab1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b1dd6b93121096ffcfdab1/sOrbqdZhbp3ZP_cUGjhlN.jpeg",
                "isPro": false,
                "fullname": "Wenke Huang",
                "user": "WilliamHuang91",
                "type": "user"
            },
            "summary": "Recent advances in reinforcement learning for foundation models, such as\nGroup Relative Policy Optimization (GRPO), have significantly improved the\nperformance of foundation models on reasoning tasks. Notably, the advantage\nfunction serves as a central mechanism in GRPO for ranking the trajectory\nimportance. However, existing explorations encounter both advantage reversion\nand advantage mirror problems, which hinder the reasonable advantage allocation\nacross different query samples. In this work, we propose an easy but effective\nGRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the\ntrajectory appears with different certainty and propose the advantage percent\ndeviation for samples with high-certainty trajectories. Furthermore, we\ndynamically reweight the advantage function for samples with varying trajectory\ncertainty, thereby adaptively configuring the advantage function to account for\nsample-specific characteristics. Comparison with related state-of-the-art\nmethods, along with ablation studies on different advantage variants, validates\nthe effectiveness of our approach.",
            "upvotes": 18,
            "discussionId": "68d363d30e215259d193b265",
            "githubRepo": "https://github.com/WenkeHuang/MAPO",
            "ai_summary": "Mixed Advantage Policy Optimization (MAPO) dynamically reweights the advantage function to improve trajectory ranking in reinforcement learning for foundation models.",
            "ai_keywords": [
                "Group Relative Policy Optimization (GRPO)",
                "advantage function",
                "trajectory importance",
                "advantage reversion",
                "advantage mirror problems",
                "Mixed Advantage Policy Optimization (MAPO)",
                "trajectory certainty",
                "advantage percent deviation",
                "sample-specific characteristics"
            ],
            "githubStars": 27
        },
        "publishedAt": "2025-09-23T05:37:16.000Z",
        "title": "MAPO: Mixed Advantage Policy Optimization",
        "summary": "Recent advances in reinforcement learning for foundation models, such as\nGroup Relative Policy Optimization (GRPO), have significantly improved the\nperformance of foundation models on reasoning tasks. Notably, the advantage\nfunction serves as a central mechanism in GRPO for ranking the trajectory\nimportance. However, existing explorations encounter both advantage reversion\nand advantage mirror problems, which hinder the reasonable advantage allocation\nacross different query samples. In this work, we propose an easy but effective\nGRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the\ntrajectory appears with different certainty and propose the advantage percent\ndeviation for samples with high-certainty trajectories. Furthermore, we\ndynamically reweight the advantage function for samples with varying trajectory\ncertainty, thereby adaptively configuring the advantage function to account for\nsample-specific characteristics. Comparison with related state-of-the-art\nmethods, along with ablation studies on different advantage variants, validates\nthe effectiveness of our approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18849.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66b1dd6b93121096ffcfdab1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b1dd6b93121096ffcfdab1/sOrbqdZhbp3ZP_cUGjhlN.jpeg",
            "fullname": "Wenke Huang",
            "name": "WilliamHuang91",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.18824",
            "authors": [
                {
                    "_id": "68d350cf0e215259d193b1c4",
                    "user": {
                        "_id": "6614cbd40bbea65e71db4e1f",
                        "avatarUrl": "/avatars/ca8ff74887bbf8eb3f5b04ae9bb6d05b.svg",
                        "isPro": false,
                        "fullname": "Yanzuo Lu",
                        "user": "oliveryanzuolu",
                        "type": "user"
                    },
                    "name": "Yanzuo Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:53:29.233Z",
                    "hidden": false
                },
                {
                    "_id": "68d350cf0e215259d193b1c5",
                    "user": {
                        "_id": "63089a78ff78e2aead8d10e7",
                        "avatarUrl": "/avatars/f326fd08abee7e31599a78923be30003.svg",
                        "isPro": false,
                        "fullname": "XinXia",
                        "user": "XiaXin-Aloys",
                        "type": "user"
                    },
                    "name": "Xin Xia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:53:32.477Z",
                    "hidden": false
                },
                {
                    "_id": "68d350cf0e215259d193b1c6",
                    "user": {
                        "_id": "670dd3b3488201d7944325b9",
                        "avatarUrl": "/avatars/25f2db41615f3ed87907a8735acc1249.svg",
                        "isPro": false,
                        "fullname": "Manlin Zhang",
                        "user": "cclim",
                        "type": "user"
                    },
                    "name": "Manlin Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:26:55.019Z",
                    "hidden": false
                },
                {
                    "_id": "68d350cf0e215259d193b1c7",
                    "name": "Huafeng Kuang",
                    "hidden": false
                },
                {
                    "_id": "68d350cf0e215259d193b1c8",
                    "user": {
                        "_id": "641870135d6f3d15c64d074e",
                        "avatarUrl": "/avatars/35f4b33fd1d4db326e4ee4300b26db72.svg",
                        "isPro": false,
                        "fullname": "Jianbin Zheng",
                        "user": "jabir-zheng",
                        "type": "user"
                    },
                    "name": "Jianbin Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:27:11.042Z",
                    "hidden": false
                },
                {
                    "_id": "68d350cf0e215259d193b1c9",
                    "name": "Yuxi Ren",
                    "hidden": false
                },
                {
                    "_id": "68d350cf0e215259d193b1ca",
                    "name": "Xuefeng Xiao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-23T09:12:46.000Z",
            "submittedOnDailyAt": "2025-09-24T00:30:55.687Z",
            "title": "Hyper-Bagel: A Unified Acceleration Framework for Multimodal\n  Understanding and Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Unified multimodal models have recently attracted considerable attention for\ntheir remarkable abilities in jointly understanding and generating diverse\ncontent. However, as contexts integrate increasingly numerous interleaved\nmultimodal tokens, the iterative processes of diffusion denoising and\nautoregressive decoding impose significant computational overhead. To address\nthis, we propose Hyper-Bagel, a unified acceleration framework designed to\nsimultaneously speed up both multimodal understanding and generation tasks. Our\napproach uses a divide-and-conquer strategy, employing speculative decoding for\nnext-token prediction and a multi-stage distillation process for diffusion\ndenoising. The framework delivers substantial performance gains, achieving over\na 2x speedup in multimodal understanding. For generative tasks, our resulting\nlossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a\n22x speedup in image editing, all while preserving the high-quality output of\nthe original model. We further develop a highly efficient 1-NFE model that\nenables near real-time interactive editing and generation. By combining\nadvanced adversarial distillation with human feedback learning, this model\nachieves ultimate cost-effectiveness and responsiveness, making complex\nmultimodal interactions seamless and instantaneous.",
            "upvotes": 17,
            "discussionId": "68d350cf0e215259d193b1cb",
            "projectPage": "https://hyper-bagel.github.io/",
            "ai_summary": "Hyper-Bagel accelerates multimodal understanding and generation tasks using speculative decoding and multi-stage distillation, achieving significant speedups while maintaining high-quality outputs.",
            "ai_keywords": [
                "diffusion denoising",
                "autoregressive decoding",
                "speculative decoding",
                "multi-stage distillation",
                "text-to-image generation",
                "image editing",
                "adversarial distillation",
                "human feedback learning"
            ]
        },
        "publishedAt": "2025-09-23T05:12:46.000Z",
        "title": "Hyper-Bagel: A Unified Acceleration Framework for Multimodal\n  Understanding and Generation",
        "summary": "Unified multimodal models have recently attracted considerable attention for\ntheir remarkable abilities in jointly understanding and generating diverse\ncontent. However, as contexts integrate increasingly numerous interleaved\nmultimodal tokens, the iterative processes of diffusion denoising and\nautoregressive decoding impose significant computational overhead. To address\nthis, we propose Hyper-Bagel, a unified acceleration framework designed to\nsimultaneously speed up both multimodal understanding and generation tasks. Our\napproach uses a divide-and-conquer strategy, employing speculative decoding for\nnext-token prediction and a multi-stage distillation process for diffusion\ndenoising. The framework delivers substantial performance gains, achieving over\na 2x speedup in multimodal understanding. For generative tasks, our resulting\nlossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a\n22x speedup in image editing, all while preserving the high-quality output of\nthe original model. We further develop a highly efficient 1-NFE model that\nenables near real-time interactive editing and generation. By combining\nadvanced adversarial distillation with human feedback learning, this model\nachieves ultimate cost-effectiveness and responsiveness, making complex\nmultimodal interactions seamless and instantaneous.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18824.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 109
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.19297",
            "authors": [
                {
                    "_id": "68d360650e215259d193b245",
                    "user": {
                        "_id": "66699aa8a33847217b5a49c7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                        "isPro": false,
                        "fullname": "Weijie Wang",
                        "user": "lhmd",
                        "type": "user"
                    },
                    "name": "Weijie Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:53:11.784Z",
                    "hidden": false
                },
                {
                    "_id": "68d360650e215259d193b246",
                    "name": "Yeqing Chen",
                    "hidden": false
                },
                {
                    "_id": "68d360650e215259d193b247",
                    "user": {
                        "_id": "64ec877bb93654d4ca5c92e9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                        "isPro": true,
                        "fullname": "Zeyu Zhang",
                        "user": "SteveZeyuZhang",
                        "type": "user"
                    },
                    "name": "Zeyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:53:09.030Z",
                    "hidden": false
                },
                {
                    "_id": "68d360650e215259d193b248",
                    "name": "Hengyu Liu",
                    "hidden": false
                },
                {
                    "_id": "68d360650e215259d193b249",
                    "name": "Haoxiao Wang",
                    "hidden": false
                },
                {
                    "_id": "68d360650e215259d193b24a",
                    "name": "Zhiyuan Feng",
                    "hidden": false
                },
                {
                    "_id": "68d360650e215259d193b24b",
                    "name": "Wenkang Qin",
                    "hidden": false
                },
                {
                    "_id": "68d360650e215259d193b24c",
                    "name": "Zheng Zhu",
                    "hidden": false
                },
                {
                    "_id": "68d360650e215259d193b24d",
                    "name": "Donny Y. Chen",
                    "hidden": false
                },
                {
                    "_id": "68d360650e215259d193b24e",
                    "name": "Bohan Zhuang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66699aa8a33847217b5a49c7/6Nma90jNFIDRa6TqQnV9o.mp4"
            ],
            "publishedAt": "2025-09-23T17:59:02.000Z",
            "submittedOnDailyAt": "2025-09-24T01:46:40.615Z",
            "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with\n  Voxel-Aligned Prediction",
            "submittedOnDailyBy": {
                "_id": "66699aa8a33847217b5a49c7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                "isPro": false,
                "fullname": "Weijie Wang",
                "user": "lhmd",
                "type": "user"
            },
            "summary": "Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective\nsolution for novel view synthesis. Existing methods predominantly rely on a\npixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a\n3D Gaussian. We rethink this widely adopted formulation and identify several\ninherent limitations: it renders the reconstructed 3D models heavily dependent\non the number of input views, leads to view-biased density distributions, and\nintroduces alignment errors, particularly when source views contain occlusions\nor low texture. To address these challenges, we introduce VolSplat, a new\nmulti-view feed-forward paradigm that replaces pixel alignment with\nvoxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D\nvoxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature\nmatching, ensuring robust multi-view consistency. Furthermore, it enables\nadaptive control over Gaussian density based on 3D scene complexity, yielding\nmore faithful Gaussian point clouds, improved geometric consistency, and\nenhanced novel-view rendering quality. Experiments on widely used benchmarks\nincluding RealEstate10K and ScanNet demonstrate that VolSplat achieves\nstate-of-the-art performance while producing more plausible and view-consistent\nGaussian reconstructions. In addition to superior results, our approach\nestablishes a more scalable framework for feed-forward 3D reconstruction with\ndenser and more robust representations, paving the way for further research in\nwider communities. The video results, code and trained models are available on\nour project page: https://lhmd.top/volsplat.",
            "upvotes": 14,
            "discussionId": "68d360650e215259d193b24f",
            "projectPage": "https://lhmd.top/volsplat/",
            "githubRepo": "https://github.com/ziplab/VolSplat",
            "ai_summary": "VolSplat, a voxel-aligned Gaussian prediction method, improves novel view synthesis by overcoming pixel alignment limitations and enhancing 3D reconstruction quality.",
            "ai_keywords": [
                "feed-forward 3D Gaussian Splatting",
                "pixel-aligned Gaussian prediction",
                "voxel-aligned Gaussians",
                "3D voxel grid",
                "Gaussian point clouds",
                "geometric consistency",
                "novel-view rendering quality",
                "RealEstate10K",
                "ScanNet"
            ],
            "githubStars": 40
        },
        "publishedAt": "2025-09-23T13:59:02.000Z",
        "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with\n  Voxel-Aligned Prediction",
        "summary": "Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective\nsolution for novel view synthesis. Existing methods predominantly rely on a\npixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a\n3D Gaussian. We rethink this widely adopted formulation and identify several\ninherent limitations: it renders the reconstructed 3D models heavily dependent\non the number of input views, leads to view-biased density distributions, and\nintroduces alignment errors, particularly when source views contain occlusions\nor low texture. To address these challenges, we introduce VolSplat, a new\nmulti-view feed-forward paradigm that replaces pixel alignment with\nvoxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D\nvoxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature\nmatching, ensuring robust multi-view consistency. Furthermore, it enables\nadaptive control over Gaussian density based on 3D scene complexity, yielding\nmore faithful Gaussian point clouds, improved geometric consistency, and\nenhanced novel-view rendering quality. Experiments on widely used benchmarks\nincluding RealEstate10K and ScanNet demonstrate that VolSplat achieves\nstate-of-the-art performance while producing more plausible and view-consistent\nGaussian reconstructions. In addition to superior results, our approach\nestablishes a more scalable framework for feed-forward 3D reconstruction with\ndenser and more robust representations, paving the way for further research in\nwider communities. The video results, code and trained models are available on\nour project page: https://lhmd.top/volsplat.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66699aa8a33847217b5a49c7/6Nma90jNFIDRa6TqQnV9o.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19297.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "fullname": "Weijie Wang",
            "name": "lhmd",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.19296",
            "authors": [
                {
                    "_id": "68d34f1a0e215259d193b19c",
                    "user": {
                        "_id": "647ef66c45baf21ad707b291",
                        "avatarUrl": "/avatars/4cd941cdca6dd829fdc9cb3fb788a99c.svg",
                        "isPro": false,
                        "fullname": "Sherwin Bahmani",
                        "user": "sherwinbahmani",
                        "type": "user"
                    },
                    "name": "Sherwin Bahmani",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:27:57.343Z",
                    "hidden": false
                },
                {
                    "_id": "68d34f1a0e215259d193b19d",
                    "name": "Tianchang Shen",
                    "hidden": false
                },
                {
                    "_id": "68d34f1a0e215259d193b19e",
                    "name": "Jiawei Ren",
                    "hidden": false
                },
                {
                    "_id": "68d34f1a0e215259d193b19f",
                    "name": "Jiahui Huang",
                    "hidden": false
                },
                {
                    "_id": "68d34f1a0e215259d193b1a0",
                    "name": "Yifeng Jiang",
                    "hidden": false
                },
                {
                    "_id": "68d34f1a0e215259d193b1a1",
                    "user": {
                        "_id": "656e000253703dd78fd072a9",
                        "avatarUrl": "/avatars/6702ba8fabe3d08884aa757f90cea333.svg",
                        "isPro": false,
                        "fullname": "Haithem Turki",
                        "user": "hturki",
                        "type": "user"
                    },
                    "name": "Haithem Turki",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:29:09.869Z",
                    "hidden": false
                },
                {
                    "_id": "68d34f1a0e215259d193b1a2",
                    "name": "Andrea Tagliasacchi",
                    "hidden": false
                },
                {
                    "_id": "68d34f1a0e215259d193b1a3",
                    "name": "David B. Lindell",
                    "hidden": false
                },
                {
                    "_id": "68d34f1a0e215259d193b1a4",
                    "user": {
                        "_id": "6366cda3361a96184dc22139",
                        "avatarUrl": "/avatars/d8a88c84cb5f69e69dd038674a29be89.svg",
                        "isPro": false,
                        "fullname": "Zan Gojcic",
                        "user": "zgojcic",
                        "type": "user"
                    },
                    "name": "Zan Gojcic",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:28:51.842Z",
                    "hidden": false
                },
                {
                    "_id": "68d34f1a0e215259d193b1a5",
                    "name": "Sanja Fidler",
                    "hidden": false
                },
                {
                    "_id": "68d34f1a0e215259d193b1a6",
                    "name": "Huan Ling",
                    "hidden": false
                },
                {
                    "_id": "68d34f1a0e215259d193b1a7",
                    "name": "Jun Gao",
                    "hidden": false
                },
                {
                    "_id": "68d34f1a0e215259d193b1a8",
                    "name": "Xuanchi Ren",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-23T17:58:01.000Z",
            "submittedOnDailyAt": "2025-09-24T00:23:39.732Z",
            "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model\n  Self-Distillation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The ability to generate virtual environments is crucial for applications\nranging from gaming to physical AI domains such as robotics, autonomous\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\nrely on the availability of captured real-world multi-view data, which is not\nalways readily available. Recent advancements in video diffusion models have\nshown remarkable imagination capabilities, yet their 2D nature limits the\napplications to simulation where a robot needs to navigate and interact with\nthe environment. In this paper, we propose a self-distillation framework that\naims to distill the implicit 3D knowledge in the video diffusion models into an\nexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for\nmulti-view training data. Specifically, we augment the typical RGB decoder with\na 3DGS decoder, which is supervised by the output of the RGB decoder. In this\napproach, the 3DGS decoder can be purely trained with synthetic data generated\nby video diffusion models. At inference time, our model can synthesize 3D\nscenes from either a text prompt or a single image for real-time rendering. Our\nframework further extends to dynamic 3D scene generation from a monocular input\nvideo. Experimental results show that our framework achieves state-of-the-art\nperformance in static and dynamic 3D scene generation.",
            "upvotes": 11,
            "discussionId": "68d34f1b0e215259d193b1a9",
            "projectPage": "https://research.nvidia.com/labs/toronto-ai/lyra/",
            "githubRepo": "https://github.com/nv-tlabs/lyra",
            "ai_summary": "A self-distillation framework converts implicit 3D knowledge from video diffusion models into an explicit 3D Gaussian Splatting representation, enabling 3D scene generation from text or images.",
            "ai_keywords": [
                "video diffusion models",
                "3D Gaussian Splatting",
                "3DGS",
                "RGB decoder",
                "3D scene generation",
                "text prompt",
                "monocular input video",
                "dynamic 3D scene generation"
            ],
            "githubStars": 135
        },
        "publishedAt": "2025-09-23T13:58:01.000Z",
        "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model\n  Self-Distillation",
        "summary": "The ability to generate virtual environments is crucial for applications\nranging from gaming to physical AI domains such as robotics, autonomous\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\nrely on the availability of captured real-world multi-view data, which is not\nalways readily available. Recent advancements in video diffusion models have\nshown remarkable imagination capabilities, yet their 2D nature limits the\napplications to simulation where a robot needs to navigate and interact with\nthe environment. In this paper, we propose a self-distillation framework that\naims to distill the implicit 3D knowledge in the video diffusion models into an\nexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for\nmulti-view training data. Specifically, we augment the typical RGB decoder with\na 3DGS decoder, which is supervised by the output of the RGB decoder. In this\napproach, the 3DGS decoder can be purely trained with synthetic data generated\nby video diffusion models. At inference time, our model can synthesize 3D\nscenes from either a text prompt or a single image for real-time rendering. Our\nframework further extends to dynamic 3D scene generation from a monocular input\nvideo. Experimental results show that our framework achieves state-of-the-art\nperformance in static and dynamic 3D scene generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19296.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 109
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.19284",
            "authors": [
                {
                    "_id": "68d3538e0e215259d193b220",
                    "user": {
                        "_id": "65cbfa6c968742be942e6cba",
                        "avatarUrl": "/avatars/1a6cc0983edc28fa92178d3abc283ba1.svg",
                        "isPro": false,
                        "fullname": "Feng",
                        "user": "Yunzhen",
                        "type": "user"
                    },
                    "name": "Yunzhen Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:29:48.477Z",
                    "hidden": false
                },
                {
                    "_id": "68d3538e0e215259d193b221",
                    "user": {
                        "_id": "65ce30e06da01df536eded5a",
                        "avatarUrl": "/avatars/04c32cba7a3bbaf9ea5dee88c96cf87b.svg",
                        "isPro": false,
                        "fullname": "Julia Kempe",
                        "user": "Knykny",
                        "type": "user"
                    },
                    "name": "Julia Kempe",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:29:55.754Z",
                    "hidden": false
                },
                {
                    "_id": "68d3538e0e215259d193b222",
                    "name": "Cheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d3538e0e215259d193b223",
                    "name": "Parag Jain",
                    "hidden": false
                },
                {
                    "_id": "68d3538e0e215259d193b224",
                    "user": {
                        "_id": "66d56e479000766c36b29119",
                        "avatarUrl": "/avatars/b92ef991185591120674c94966fc7ab8.svg",
                        "isPro": false,
                        "fullname": "Anthony Hartshorn",
                        "user": "AHartshorn",
                        "type": "user"
                    },
                    "name": "Anthony Hartshorn",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:30:03.470Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-23T17:50:54.000Z",
            "submittedOnDailyAt": "2025-09-24T00:43:19.278Z",
            "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and\n  Structure of CoT",
            "submittedOnDailyBy": {
                "_id": "65cbfa6c968742be942e6cba",
                "avatarUrl": "/avatars/1a6cc0983edc28fa92178d3abc283ba1.svg",
                "isPro": false,
                "fullname": "Feng",
                "user": "Yunzhen",
                "type": "user"
            },
            "summary": "Large reasoning models (LRMs) spend substantial test-time compute on long\nchain-of-thought (CoT) traces, but what *characterizes* an effective CoT\nremains unclear. While prior work reports gains from lengthening CoTs and\nincreasing review (revisiting earlier steps) via appended *wait* tokens, recent\nstudies suggest that shorter thinking can outperform longer traces. We\ntherefore conduct a systematic evaluation across ten LRMs on math and\nscientific reasoning. Contrary to the \"longer-is-better\" narrative, we find\nthat both naive CoT lengthening and increased review are associated with\n*lower* accuracy.\n  As CoT unfolds step by step, token-level metrics can conflate verbosity with\nprocess quality. We introduce a graph view of CoT to extract structure and\nidentify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of\nsteps in abandoned branches-that consistently outpredicts length and review\nratio for correctness across models. To probe causality, we design two\ninterventions. First, we rank candidate CoTs by each metric at test time, where\nFSF yields the largest pass@1 gains; second, we edit CoTs to remove failed\nbranches, which significantly improves accuracy, indicating that failed\nbranches bias subsequent reasoning. Taken together, these results characterize\neffective CoTs as those that *fail less* and support *structure-aware*\ntest-time scaling over indiscriminately generating long CoT.",
            "upvotes": 11,
            "discussionId": "68d3538e0e215259d193b225",
            "ai_summary": "Effective chain-of-thoughts in large reasoning models are characterized by fewer failed steps and better structural quality, not necessarily by length or review.",
            "ai_keywords": [
                "chain-of-thought",
                "CoT",
                "large reasoning models",
                "LRMs",
                "wait tokens",
                "Failed-Step Fraction",
                "FSF",
                "structure-aware",
                "test-time scaling"
            ]
        },
        "publishedAt": "2025-09-23T13:50:54.000Z",
        "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and\n  Structure of CoT",
        "summary": "Large reasoning models (LRMs) spend substantial test-time compute on long\nchain-of-thought (CoT) traces, but what *characterizes* an effective CoT\nremains unclear. While prior work reports gains from lengthening CoTs and\nincreasing review (revisiting earlier steps) via appended *wait* tokens, recent\nstudies suggest that shorter thinking can outperform longer traces. We\ntherefore conduct a systematic evaluation across ten LRMs on math and\nscientific reasoning. Contrary to the \"longer-is-better\" narrative, we find\nthat both naive CoT lengthening and increased review are associated with\n*lower* accuracy.\n  As CoT unfolds step by step, token-level metrics can conflate verbosity with\nprocess quality. We introduce a graph view of CoT to extract structure and\nidentify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of\nsteps in abandoned branches-that consistently outpredicts length and review\nratio for correctness across models. To probe causality, we design two\ninterventions. First, we rank candidate CoTs by each metric at test time, where\nFSF yields the largest pass@1 gains; second, we edit CoTs to remove failed\nbranches, which significantly improves accuracy, indicating that failed\nbranches bias subsequent reasoning. Taken together, these results characterize\neffective CoTs as those that *fail less* and support *structure-aware*\ntest-time scaling over indiscriminately generating long CoT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19284.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65cbfa6c968742be942e6cba",
            "avatarUrl": "/avatars/1a6cc0983edc28fa92178d3abc283ba1.svg",
            "fullname": "Feng",
            "name": "Yunzhen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.19170",
            "authors": [
                {
                    "_id": "68d378190e215259d193b34d",
                    "name": "Natasha Butt",
                    "hidden": false
                },
                {
                    "_id": "68d378190e215259d193b34e",
                    "name": "Ariel Kwiatkowski",
                    "hidden": false
                },
                {
                    "_id": "68d378190e215259d193b34f",
                    "name": "Ismail Labiad",
                    "hidden": false
                },
                {
                    "_id": "68d378190e215259d193b350",
                    "name": "Julia Kempe",
                    "hidden": false
                },
                {
                    "_id": "68d378190e215259d193b351",
                    "name": "Yann Ollivier",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-23T15:43:47.000Z",
            "submittedOnDailyAt": "2025-09-24T15:13:14.305Z",
            "title": "Soft Tokens, Hard Truths",
            "submittedOnDailyBy": {
                "_id": "67235358177d9eae488ff01b",
                "avatarUrl": "/avatars/b94e228ceae5155898af4a6d1bb06ed3.svg",
                "isPro": false,
                "fullname": "Natasha Butt",
                "user": "NatashaEB",
                "type": "user"
            },
            "summary": "The use of continuous instead of discrete tokens during the Chain-of-Thought\n(CoT) phase of reasoning LLMs has garnered attention recently, based on the\nintuition that a continuous mixture of discrete tokens could simulate a\nsuperposition of several reasoning paths simultaneously. Theoretical results\nhave formally proven that continuous tokens have much greater expressivity and\ncan solve specific problems more efficiently. However, practical use of\ncontinuous tokens has been limited by strong training difficulties: previous\nworks either just use continuous tokens at inference time on a pre-trained\ndiscrete-token model, or must distill the continuous CoT from ground-truth\ndiscrete CoTs and face computational costs that limit the CoT to very few\ntokens.\n  This is the first work introducing a scalable method to learn continuous CoTs\nvia reinforcement learning (RL), without distilling from reference discrete\nCoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input\nembedding to provide RL exploration. Computational overhead is minimal,\nenabling us to learn continuous CoTs with hundreds of tokens. On math reasoning\nbenchmarks with Llama and Qwen models up to 8B, training with continuous CoTs\nmatch discrete-token CoTs for pass@1 and surpass them for pass@32, showing\ngreater CoT diversity. In systematic comparisons, the best-performing scenario\nis to train with continuous CoT tokens then use discrete tokens for inference,\nmeaning the \"soft\" models can be deployed in a standard way. Finally, we show\ncontinuous CoT RL training better preserves the predictions of the base model\non out-of-domain tasks, thus providing a softer touch to the base model.",
            "upvotes": 5,
            "discussionId": "68d378190e215259d193b352",
            "ai_summary": "A scalable reinforcement learning method for learning continuous chain-of-thought tokens in large language models improves performance and diversity over discrete tokens while preserving out-of-domain predictions.",
            "ai_keywords": [
                "Chain-of-Thought (CoT)",
                "continuous tokens",
                "discrete tokens",
                "reinforcement learning (RL)",
                "soft tokens",
                "noise on input embedding",
                "math reasoning benchmarks",
                "pass@1",
                "pass@32",
                "out-of-domain tasks"
            ]
        },
        "publishedAt": "2025-09-23T11:43:47.000Z",
        "title": "Soft Tokens, Hard Truths",
        "summary": "The use of continuous instead of discrete tokens during the Chain-of-Thought\n(CoT) phase of reasoning LLMs has garnered attention recently, based on the\nintuition that a continuous mixture of discrete tokens could simulate a\nsuperposition of several reasoning paths simultaneously. Theoretical results\nhave formally proven that continuous tokens have much greater expressivity and\ncan solve specific problems more efficiently. However, practical use of\ncontinuous tokens has been limited by strong training difficulties: previous\nworks either just use continuous tokens at inference time on a pre-trained\ndiscrete-token model, or must distill the continuous CoT from ground-truth\ndiscrete CoTs and face computational costs that limit the CoT to very few\ntokens.\n  This is the first work introducing a scalable method to learn continuous CoTs\nvia reinforcement learning (RL), without distilling from reference discrete\nCoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input\nembedding to provide RL exploration. Computational overhead is minimal,\nenabling us to learn continuous CoTs with hundreds of tokens. On math reasoning\nbenchmarks with Llama and Qwen models up to 8B, training with continuous CoTs\nmatch discrete-token CoTs for pass@1 and surpass them for pass@32, showing\ngreater CoT diversity. In systematic comparisons, the best-performing scenario\nis to train with continuous CoT tokens then use discrete tokens for inference,\nmeaning the \"soft\" models can be deployed in a standard way. Finally, we show\ncontinuous CoT RL training better preserves the predictions of the base model\non out-of-domain tasks, thus providing a softer touch to the base model.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19170.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67235358177d9eae488ff01b",
            "avatarUrl": "/avatars/b94e228ceae5155898af4a6d1bb06ed3.svg",
            "fullname": "Natasha Butt",
            "name": "NatashaEB",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.13835",
            "authors": [
                {
                    "_id": "68d3913f0e215259d193b3c1",
                    "user": {
                        "_id": "65e0e6fa4394fc3d1b59627a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e0e6fa4394fc3d1b59627a/rlKw_UdH3MpmpgUcchMgB.jpeg",
                        "isPro": false,
                        "fullname": "Minh Duc Bui",
                        "user": "MinhDucBui",
                        "type": "user"
                    },
                    "name": "Minh Duc Bui",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:47:16.416Z",
                    "hidden": false
                },
                {
                    "_id": "68d3913f0e215259d193b3c2",
                    "name": "Carolin Holtermann",
                    "hidden": false
                },
                {
                    "_id": "68d3913f0e215259d193b3c3",
                    "user": {
                        "_id": "659553cd2a36333c0bf23f75",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659553cd2a36333c0bf23f75/7ojOQ_26rRg3cn0_qGNSC.jpeg",
                        "isPro": false,
                        "fullname": "Valentin Hofmann",
                        "user": "valentinhofmann",
                        "type": "user"
                    },
                    "name": "Valentin Hofmann",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:36:15.504Z",
                    "hidden": false
                },
                {
                    "_id": "68d3913f0e215259d193b3c4",
                    "user": {
                        "_id": "626c02e7703f3b27dd590896",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654503075060-626c02e7703f3b27dd590896.jpeg",
                        "isPro": false,
                        "fullname": "Anne Lauscher",
                        "user": "anlausch",
                        "type": "user"
                    },
                    "name": "Anne Lauscher",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:36:21.971Z",
                    "hidden": false
                },
                {
                    "_id": "68d3913f0e215259d193b3c5",
                    "name": "Katharina von der Wense",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65e0e6fa4394fc3d1b59627a/28_BpeIrXFYdBsMeu6h1G.png"
            ],
            "publishedAt": "2025-09-17T09:05:37.000Z",
            "submittedOnDailyAt": "2025-09-24T05:08:18.551Z",
            "title": "Large Language Models Discriminate Against Speakers of German Dialects",
            "submittedOnDailyBy": {
                "_id": "65e0e6fa4394fc3d1b59627a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e0e6fa4394fc3d1b59627a/rlKw_UdH3MpmpgUcchMgB.jpeg",
                "isPro": false,
                "fullname": "Minh Duc Bui",
                "user": "MinhDucBui",
                "type": "user"
            },
            "summary": "Dialects represent a significant component of human culture and are found\nacross all regions of the world. In Germany, more than 40% of the population\nspeaks a regional dialect (Adler and Hansen, 2022). However, despite cultural\nimportance, individuals speaking dialects often face negative societal\nstereotypes. We examine whether such stereotypes are mirrored by large language\nmodels (LLMs). We draw on the sociolinguistic literature on dialect perception\nto analyze traits commonly associated with dialect speakers. Based on these\ntraits, we assess the dialect naming bias and dialect usage bias expressed by\nLLMs in two tasks: an association task and a decision task. To assess a model's\ndialect usage bias, we construct a novel evaluation corpus that pairs sentences\nfrom seven regional German dialects (e.g., Alemannic and Bavarian) with their\nstandard German counterparts. We find that: (1) in the association task, all\nevaluated LLMs exhibit significant dialect naming and dialect usage bias\nagainst German dialect speakers, reflected in negative adjective associations;\n(2) all models reproduce these dialect naming and dialect usage biases in their\ndecision making; and (3) contrary to prior work showing minimal bias with\nexplicit demographic mentions, we find that explicitly labeling linguistic\ndemographics--German dialect speakers--amplifies bias more than implicit cues\nlike dialect usage.",
            "upvotes": 5,
            "discussionId": "68d3913f0e215259d193b3c6",
            "ai_summary": "Large language models exhibit significant dialect naming and usage bias against German dialect speakers, which is amplified when linguistic demographics are explicitly labeled.",
            "ai_keywords": [
                "large language models",
                "dialect naming bias",
                "dialect usage bias",
                "association task",
                "decision task",
                "regional German dialects",
                "standard German",
                "negative adjective associations"
            ]
        },
        "publishedAt": "2025-09-17T05:05:37.000Z",
        "title": "Large Language Models Discriminate Against Speakers of German Dialects",
        "summary": "Dialects represent a significant component of human culture and are found\nacross all regions of the world. In Germany, more than 40% of the population\nspeaks a regional dialect (Adler and Hansen, 2022). However, despite cultural\nimportance, individuals speaking dialects often face negative societal\nstereotypes. We examine whether such stereotypes are mirrored by large language\nmodels (LLMs). We draw on the sociolinguistic literature on dialect perception\nto analyze traits commonly associated with dialect speakers. Based on these\ntraits, we assess the dialect naming bias and dialect usage bias expressed by\nLLMs in two tasks: an association task and a decision task. To assess a model's\ndialect usage bias, we construct a novel evaluation corpus that pairs sentences\nfrom seven regional German dialects (e.g., Alemannic and Bavarian) with their\nstandard German counterparts. We find that: (1) in the association task, all\nevaluated LLMs exhibit significant dialect naming and dialect usage bias\nagainst German dialect speakers, reflected in negative adjective associations;\n(2) all models reproduce these dialect naming and dialect usage biases in their\ndecision making; and (3) contrary to prior work showing minimal bias with\nexplicit demographic mentions, we find that explicitly labeling linguistic\ndemographics--German dialect speakers--amplifies bias more than implicit cues\nlike dialect usage.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65e0e6fa4394fc3d1b59627a/28_BpeIrXFYdBsMeu6h1G.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13835.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e0e6fa4394fc3d1b59627a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e0e6fa4394fc3d1b59627a/rlKw_UdH3MpmpgUcchMgB.jpeg",
            "fullname": "Minh Duc Bui",
            "name": "MinhDucBui",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.18905",
            "authors": [
                {
                    "_id": "68d3474a0e215259d193b175",
                    "user": {
                        "_id": "664da5a094234f3c17df8d3b",
                        "avatarUrl": "/avatars/ef0ca66050bbb172b02b049c2f0a4b1a.svg",
                        "isPro": false,
                        "fullname": "Songsong Yu",
                        "user": "Two-hot",
                        "type": "user"
                    },
                    "name": "Songsong Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:53:37.350Z",
                    "hidden": false
                },
                {
                    "_id": "68d3474a0e215259d193b176",
                    "user": {
                        "_id": "669f3b098c65c172c4d64039",
                        "avatarUrl": "/avatars/d85158964853ab87b9b677fa16df90f8.svg",
                        "isPro": false,
                        "fullname": "Yuxin Chen",
                        "user": "Uasonchen",
                        "type": "user"
                    },
                    "name": "Yuxin Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:53:34.885Z",
                    "hidden": false
                },
                {
                    "_id": "68d3474a0e215259d193b177",
                    "name": "Hao Ju",
                    "hidden": false
                },
                {
                    "_id": "68d3474a0e215259d193b178",
                    "name": "Lianjie Jia",
                    "hidden": false
                },
                {
                    "_id": "68d3474a0e215259d193b179",
                    "name": "Fuxi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d3474a0e215259d193b17a",
                    "name": "Shaofei Huang",
                    "hidden": false
                },
                {
                    "_id": "68d3474a0e215259d193b17b",
                    "name": "Yuhan Wu",
                    "hidden": false
                },
                {
                    "_id": "68d3474a0e215259d193b17c",
                    "name": "Rundi Cui",
                    "hidden": false
                },
                {
                    "_id": "68d3474a0e215259d193b17d",
                    "name": "Binghao Ran",
                    "hidden": false
                },
                {
                    "_id": "68d3474a0e215259d193b17e",
                    "name": "Zaibin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d3474a0e215259d193b17f",
                    "name": "Zhedong Zheng",
                    "hidden": false
                },
                {
                    "_id": "68d3474a0e215259d193b180",
                    "name": "Zhipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d3474a0e215259d193b181",
                    "name": "Yifan Wang",
                    "hidden": false
                },
                {
                    "_id": "68d3474a0e215259d193b182",
                    "name": "Lin Song",
                    "hidden": false
                },
                {
                    "_id": "68d3474a0e215259d193b183",
                    "name": "Lijun Wang",
                    "hidden": false
                },
                {
                    "_id": "68d3474a0e215259d193b184",
                    "name": "Yanwei Li",
                    "hidden": false
                },
                {
                    "_id": "68d3474a0e215259d193b185",
                    "name": "Ying Shan",
                    "hidden": false
                },
                {
                    "_id": "68d3474a0e215259d193b186",
                    "name": "Huchuan Lu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/664da5a094234f3c17df8d3b/2QE30oZZaHEAhtObCYj8e.png"
            ],
            "publishedAt": "2025-09-23T12:00:14.000Z",
            "submittedOnDailyAt": "2025-09-24T23:56:18.114Z",
            "title": "How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven\n  Perspective",
            "submittedOnDailyBy": {
                "_id": "664da5a094234f3c17df8d3b",
                "avatarUrl": "/avatars/ef0ca66050bbb172b02b049c2f0a4b1a.svg",
                "isPro": false,
                "fullname": "Songsong Yu",
                "user": "Two-hot",
                "type": "user"
            },
            "summary": "Visual Spatial Reasoning (VSR) is a core human cognitive ability and a\ncritical requirement for advancing embodied intelligence and autonomous\nsystems. Despite recent progress in Vision-Language Models (VLMs), achieving\nhuman-level VSR remains highly challenging due to the complexity of\nrepresenting and reasoning over three-dimensional space. In this paper, we\npresent a systematic investigation of VSR in VLMs, encompassing a review of\nexisting methodologies across input modalities, model architectures, training\nstrategies, and reasoning mechanisms. Furthermore, we categorize spatial\nintelligence into three levels of capability, ie, basic perception, spatial\nunderstanding, spatial planning, and curate SIBench, a spatial intelligence\nbenchmark encompassing nearly 20 open-source datasets across 23 task settings.\nExperiments with state-of-the-art VLMs reveal a pronounced gap between\nperception and reasoning, as models show competence in basic perceptual tasks\nbut consistently underperform in understanding and planning tasks, particularly\nin numerical estimation, multi-view reasoning, temporal dynamics, and spatial\nimagination. These findings underscore the substantial challenges that remain\nin achieving spatial intelligence, while providing both a systematic roadmap\nand a comprehensive benchmark to drive future research in the field. The\nrelated resources of this study are accessible at\nhttps://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.",
            "upvotes": 4,
            "discussionId": "68d3474a0e215259d193b187",
            "ai_summary": "Research investigates visual spatial reasoning in vision-language models, highlighting gaps between perceptual and reasoning capabilities, and introduces SIBench as a benchmark for future research.",
            "ai_keywords": [
                "Visual Spatial Reasoning",
                "Vision-Language Models",
                "spatial intelligence",
                "basic perception",
                "spatial understanding",
                "spatial planning",
                "SIBench",
                "numerical estimation",
                "multi-view reasoning",
                "temporal dynamics",
                "spatial imagination"
            ]
        },
        "publishedAt": "2025-09-23T08:00:14.000Z",
        "title": "How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven\n  Perspective",
        "summary": "Visual Spatial Reasoning (VSR) is a core human cognitive ability and a\ncritical requirement for advancing embodied intelligence and autonomous\nsystems. Despite recent progress in Vision-Language Models (VLMs), achieving\nhuman-level VSR remains highly challenging due to the complexity of\nrepresenting and reasoning over three-dimensional space. In this paper, we\npresent a systematic investigation of VSR in VLMs, encompassing a review of\nexisting methodologies across input modalities, model architectures, training\nstrategies, and reasoning mechanisms. Furthermore, we categorize spatial\nintelligence into three levels of capability, ie, basic perception, spatial\nunderstanding, spatial planning, and curate SIBench, a spatial intelligence\nbenchmark encompassing nearly 20 open-source datasets across 23 task settings.\nExperiments with state-of-the-art VLMs reveal a pronounced gap between\nperception and reasoning, as models show competence in basic perceptual tasks\nbut consistently underperform in understanding and planning tasks, particularly\nin numerical estimation, multi-view reasoning, temporal dynamics, and spatial\nimagination. These findings underscore the substantial challenges that remain\nin achieving spatial intelligence, while providing both a systematic roadmap\nand a comprehensive benchmark to drive future research in the field. The\nrelated resources of this study are accessible at\nhttps://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/664da5a094234f3c17df8d3b/2QE30oZZaHEAhtObCYj8e.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18905.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "664da5a094234f3c17df8d3b",
            "avatarUrl": "/avatars/ef0ca66050bbb172b02b049c2f0a4b1a.svg",
            "fullname": "Songsong Yu",
            "name": "Two-hot",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.19300",
            "authors": [
                {
                    "_id": "68d34ffd0e215259d193b1ab",
                    "user": {
                        "_id": "65409e53d07194a81ecdd427",
                        "avatarUrl": "/avatars/73a88ffe7f78f05b6fc8eb57e9f1e587.svg",
                        "isPro": false,
                        "fullname": "Chen Chen",
                        "user": "ultra7chen",
                        "type": "user"
                    },
                    "name": "Chen Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:34:36.964Z",
                    "hidden": false
                },
                {
                    "_id": "68d34ffd0e215259d193b1ac",
                    "name": "Pengsheng Guo",
                    "hidden": false
                },
                {
                    "_id": "68d34ffd0e215259d193b1ad",
                    "user": {
                        "_id": "63d9ea2aaefaa7352587151c",
                        "avatarUrl": "/avatars/2f02ee1b2a0018c74ef1d47883782bd5.svg",
                        "isPro": false,
                        "fullname": "Liangchen Song",
                        "user": "lsongx",
                        "type": "user"
                    },
                    "name": "Liangchen Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:34:53.552Z",
                    "hidden": false
                },
                {
                    "_id": "68d34ffd0e215259d193b1ae",
                    "user": {
                        "_id": "62b6b0397523238923221df9",
                        "avatarUrl": "/avatars/77068771dd51df7519516cd502a88789.svg",
                        "isPro": false,
                        "fullname": "Jiasenlu",
                        "user": "Jiasenlu",
                        "type": "user"
                    },
                    "name": "Jiasen Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:35:00.187Z",
                    "hidden": false
                },
                {
                    "_id": "68d34ffd0e215259d193b1af",
                    "name": "Rui Qian",
                    "hidden": false
                },
                {
                    "_id": "68d34ffd0e215259d193b1b0",
                    "user": {
                        "_id": "64980a5be35ff69cfd9df735",
                        "avatarUrl": "/avatars/ec0c84611cc600e2f81c528d633090bc.svg",
                        "isPro": false,
                        "fullname": "wangxinze",
                        "user": "wangxinze",
                        "type": "user"
                    },
                    "name": "Xinze Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:35:12.106Z",
                    "hidden": false
                },
                {
                    "_id": "68d34ffd0e215259d193b1b1",
                    "user": {
                        "_id": "638cb2efc8912be69c149a46",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638cb2efc8912be69c149a46/qxMNOWoSUG525DgNPpHcK.jpeg",
                        "isPro": false,
                        "fullname": "Tsu-Jui Fu",
                        "user": "tsujuifu",
                        "type": "user"
                    },
                    "name": "Tsu-Jui Fu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:35:18.721Z",
                    "hidden": false
                },
                {
                    "_id": "68d34ffd0e215259d193b1b2",
                    "name": "Wei Liu",
                    "hidden": false
                },
                {
                    "_id": "68d34ffd0e215259d193b1b3",
                    "name": "Yinfei Yang",
                    "hidden": false
                },
                {
                    "_id": "68d34ffd0e215259d193b1b4",
                    "name": "Alex Schwing",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65409e53d07194a81ecdd427/Esx1jPSPMVHM3uXjKVzR6.png"
            ],
            "publishedAt": "2025-09-23T17:59:31.000Z",
            "submittedOnDailyAt": "2025-09-24T03:03:54.232Z",
            "title": "CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target\n  for Better Flow Matching",
            "submittedOnDailyBy": {
                "_id": "65409e53d07194a81ecdd427",
                "avatarUrl": "/avatars/73a88ffe7f78f05b6fc8eb57e9f1e587.svg",
                "isPro": false,
                "fullname": "Chen Chen",
                "user": "ultra7chen",
                "type": "user"
            },
            "summary": "Conditional generative modeling aims to learn a conditional data distribution\nfrom samples containing data-condition pairs. For this, diffusion and\nflow-based methods have attained compelling results. These methods use a\nlearned (flow) model to transport an initial standard Gaussian noise that\nignores the condition to the conditional data distribution. The model is hence\nrequired to learn both mass transport and conditional injection. To ease the\ndemand on the model, we propose Condition-Aware Reparameterization for Flow\nMatching (CAR-Flow) -- a lightweight, learned shift that conditions the source,\nthe target, or both distributions. By relocating these distributions, CAR-Flow\nshortens the probability path the model must learn, leading to faster training\nin practice. On low-dimensional synthetic data, we visualize and quantify the\neffects of CAR. On higher-dimensional natural image data (ImageNet-256),\nequipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while\nintroducing less than 0.6% additional parameters.",
            "upvotes": 3,
            "discussionId": "68d34ffd0e215259d193b1b5",
            "ai_summary": "Condition-Aware Reparameterization for Flow Matching (CAR-Flow) enhances conditional generative modeling by repositioning distributions, leading to faster training and improved performance on image data.",
            "ai_keywords": [
                "conditional generative modeling",
                "diffusion",
                "flow-based methods",
                "learned shift",
                "source distribution",
                "target distribution",
                "probability path",
                "SiT-XL/2",
                "FID"
            ]
        },
        "publishedAt": "2025-09-23T13:59:31.000Z",
        "title": "CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target\n  for Better Flow Matching",
        "summary": "Conditional generative modeling aims to learn a conditional data distribution\nfrom samples containing data-condition pairs. For this, diffusion and\nflow-based methods have attained compelling results. These methods use a\nlearned (flow) model to transport an initial standard Gaussian noise that\nignores the condition to the conditional data distribution. The model is hence\nrequired to learn both mass transport and conditional injection. To ease the\ndemand on the model, we propose Condition-Aware Reparameterization for Flow\nMatching (CAR-Flow) -- a lightweight, learned shift that conditions the source,\nthe target, or both distributions. By relocating these distributions, CAR-Flow\nshortens the probability path the model must learn, leading to faster training\nin practice. On low-dimensional synthetic data, we visualize and quantify the\neffects of CAR. On higher-dimensional natural image data (ImageNet-256),\nequipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while\nintroducing less than 0.6% additional parameters.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65409e53d07194a81ecdd427/Esx1jPSPMVHM3uXjKVzR6.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19300.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65409e53d07194a81ecdd427",
            "avatarUrl": "/avatars/73a88ffe7f78f05b6fc8eb57e9f1e587.svg",
            "fullname": "Chen Chen",
            "name": "ultra7chen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.17321",
            "authors": [
                {
                    "_id": "68d2cb3b0e215259d193b0db",
                    "user": {
                        "_id": "6273c47ef6d63a28483eaba2",
                        "avatarUrl": "/avatars/f50e8c37f070846814985091609aa23f.svg",
                        "isPro": false,
                        "fullname": "Pawe Budzianowski",
                        "user": "pfb30",
                        "type": "user"
                    },
                    "name": "Pawe Budzianowski",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:35:47.604Z",
                    "hidden": false
                },
                {
                    "_id": "68d2cb3b0e215259d193b0dc",
                    "user": {
                        "_id": "669fd3b34d95f13eb6e9dbd5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669fd3b34d95f13eb6e9dbd5/01VPhZO7bqgvCCaFFZbP3.jpeg",
                        "isPro": true,
                        "fullname": "Emilia Winios",
                        "user": "emilia-wisnios",
                        "type": "user"
                    },
                    "name": "Emilia Winios",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:54:14.638Z",
                    "hidden": false
                },
                {
                    "_id": "68d2cb3b0e215259d193b0dd",
                    "name": "Gracjan Gral",
                    "hidden": false
                },
                {
                    "_id": "68d2cb3b0e215259d193b0de",
                    "name": "Igor Kulakov",
                    "hidden": false
                },
                {
                    "_id": "68d2cb3b0e215259d193b0df",
                    "name": "Viktor Petrenko",
                    "hidden": false
                },
                {
                    "_id": "68d2cb3b0e215259d193b0e0",
                    "name": "Krzysztof Walas",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T02:52:55.000Z",
            "submittedOnDailyAt": "2025-09-24T04:02:10.466Z",
            "title": "OpenGVL - Benchmarking Visual Temporal Progress for Data Curation",
            "submittedOnDailyBy": {
                "_id": "669fd3b34d95f13eb6e9dbd5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669fd3b34d95f13eb6e9dbd5/01VPhZO7bqgvCCaFFZbP3.jpeg",
                "isPro": true,
                "fullname": "Emilia Winios",
                "user": "emilia-wisnios",
                "type": "user"
            },
            "summary": "Data scarcity remains one of the most limiting factors in driving progress in\nrobotics. However, the amount of available robotics data in the wild is growing\nexponentially, creating new opportunities for large-scale data utilization.\nReliable temporal task completion prediction could help automatically annotate\nand curate this data at scale. The Generative Value Learning (GVL) approach was\nrecently proposed, leveraging the knowledge embedded in vision-language models\n(VLMs) to predict task progress from visual observations. Building upon GVL, we\npropose OpenGVL, a comprehensive benchmark for estimating task progress across\ndiverse challenging manipulation tasks involving both robotic and human\nembodiments. We evaluate the capabilities of publicly available open-source\nfoundation models, showing that open-source model families significantly\nunderperform closed-source counterparts, achieving only approximately 70% of\ntheir performance on temporal progress prediction tasks. Furthermore, we\ndemonstrate how OpenGVL can serve as a practical tool for automated data\ncuration and filtering, enabling efficient quality assessment of large-scale\nrobotics datasets. We release the benchmark along with the complete codebase at\ngithub.com/budzianowski/opengvl{OpenGVL}.",
            "upvotes": 3,
            "discussionId": "68d2cb3b0e215259d193b0e1",
            "ai_summary": "OpenGVL is a benchmark for task progress prediction in robotics using vision-language models, showing open-source models underperform compared to closed-source ones and enabling automated data curation.",
            "ai_keywords": [
                "Generative Value Learning",
                "GVL",
                "vision-language models",
                "VLMs",
                "task progress prediction",
                "manipulation tasks",
                "open-source foundation models",
                "closed-source counterparts",
                "automated data curation",
                "quality assessment",
                "robotics datasets"
            ]
        },
        "publishedAt": "2025-09-21T22:52:55.000Z",
        "title": "OpenGVL - Benchmarking Visual Temporal Progress for Data Curation",
        "summary": "Data scarcity remains one of the most limiting factors in driving progress in\nrobotics. However, the amount of available robotics data in the wild is growing\nexponentially, creating new opportunities for large-scale data utilization.\nReliable temporal task completion prediction could help automatically annotate\nand curate this data at scale. The Generative Value Learning (GVL) approach was\nrecently proposed, leveraging the knowledge embedded in vision-language models\n(VLMs) to predict task progress from visual observations. Building upon GVL, we\npropose OpenGVL, a comprehensive benchmark for estimating task progress across\ndiverse challenging manipulation tasks involving both robotic and human\nembodiments. We evaluate the capabilities of publicly available open-source\nfoundation models, showing that open-source model families significantly\nunderperform closed-source counterparts, achieving only approximately 70% of\ntheir performance on temporal progress prediction tasks. Furthermore, we\ndemonstrate how OpenGVL can serve as a practical tool for automated data\ncuration and filtering, enabling efficient quality assessment of large-scale\nrobotics datasets. We release the benchmark along with the complete codebase at\ngithub.com/budzianowski/opengvl{OpenGVL}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17321.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "669fd3b34d95f13eb6e9dbd5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669fd3b34d95f13eb6e9dbd5/01VPhZO7bqgvCCaFFZbP3.jpeg",
            "fullname": "Emilia Winios",
            "name": "emilia-wisnios",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.17083",
            "authors": [
                {
                    "_id": "68d211241ca7156988a8ed80",
                    "user": {
                        "_id": "65d31df8e3667040af7bc945",
                        "avatarUrl": "/avatars/abe1137fccc45da0e1bd41ef0c172899.svg",
                        "isPro": false,
                        "fullname": "Zipeng Wang",
                        "user": "ZipW",
                        "type": "user"
                    },
                    "name": "Zipeng Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:05:49.538Z",
                    "hidden": false
                },
                {
                    "_id": "68d211241ca7156988a8ed81",
                    "user": {
                        "_id": "66feab48651e00e22f33222e",
                        "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
                        "isPro": false,
                        "fullname": "Dan Xu",
                        "user": "danxuhk",
                        "type": "user"
                    },
                    "name": "Dan Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:55:37.112Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-21T13:59:26.000Z",
            "submittedOnDailyAt": "2025-09-24T01:06:44.788Z",
            "title": "HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel\n  View Synthesis",
            "submittedOnDailyBy": {
                "_id": "65d31df8e3667040af7bc945",
                "avatarUrl": "/avatars/abe1137fccc45da0e1bd41ef0c172899.svg",
                "isPro": false,
                "fullname": "Zipeng Wang",
                "user": "ZipW",
                "type": "user"
            },
            "summary": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative\nto NeRF-based approaches, enabling real-time, high-quality novel view synthesis\nthrough explicit, optimizable 3D Gaussians. However, 3DGS suffers from\nsignificant memory overhead due to its reliance on per-Gaussian parameters to\nmodel view-dependent effects and anisotropic shapes. While recent works propose\ncompressing 3DGS with neural fields, these methods struggle to capture\nhigh-frequency spatial variations in Gaussian properties, leading to degraded\nreconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a\nnovel scene representation that combines the strengths of explicit Gaussians\nand neural fields. HyRF decomposes the scene into (1) a compact set of explicit\nGaussians storing only critical high-frequency parameters and (2) grid-based\nneural fields that predict remaining properties. To enhance representational\ncapacity, we introduce a decoupled neural field architecture, separately\nmodeling geometry (scale, opacity, rotation) and view-dependent color.\nAdditionally, we propose a hybrid rendering scheme that composites Gaussian\nsplatting with a neural field-predicted background, addressing limitations in\ndistant scene representation. Experiments demonstrate that HyRF achieves\nstate-of-the-art rendering quality while reducing model size by over 20 times\ncompared to 3DGS and maintaining real-time performance. Our project page is\navailable at https://wzpscott.github.io/hyrf/.",
            "upvotes": 3,
            "discussionId": "68d211241ca7156988a8ed82",
            "projectPage": "https://wzpscott.github.io/hyrf/",
            "githubRepo": "https://github.com/wzpscott/hybrid-radiance-fields",
            "ai_summary": "Hybrid Radiance Fields combine explicit Gaussians and neural fields to achieve high-quality rendering with reduced memory usage and real-time performance.",
            "ai_keywords": [
                "3D Gaussian Splatting",
                "NeRF",
                "explicit Gaussians",
                "neural fields",
                "high-frequency parameters",
                "grid-based neural fields",
                "decoupled neural field architecture",
                "hybrid rendering scheme",
                "Gaussian splatting",
                "neural field-predicted background"
            ],
            "githubStars": 26
        },
        "publishedAt": "2025-09-21T09:59:26.000Z",
        "title": "HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel\n  View Synthesis",
        "summary": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative\nto NeRF-based approaches, enabling real-time, high-quality novel view synthesis\nthrough explicit, optimizable 3D Gaussians. However, 3DGS suffers from\nsignificant memory overhead due to its reliance on per-Gaussian parameters to\nmodel view-dependent effects and anisotropic shapes. While recent works propose\ncompressing 3DGS with neural fields, these methods struggle to capture\nhigh-frequency spatial variations in Gaussian properties, leading to degraded\nreconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a\nnovel scene representation that combines the strengths of explicit Gaussians\nand neural fields. HyRF decomposes the scene into (1) a compact set of explicit\nGaussians storing only critical high-frequency parameters and (2) grid-based\nneural fields that predict remaining properties. To enhance representational\ncapacity, we introduce a decoupled neural field architecture, separately\nmodeling geometry (scale, opacity, rotation) and view-dependent color.\nAdditionally, we propose a hybrid rendering scheme that composites Gaussian\nsplatting with a neural field-predicted background, addressing limitations in\ndistant scene representation. Experiments demonstrate that HyRF achieves\nstate-of-the-art rendering quality while reducing model size by over 20 times\ncompared to 3DGS and maintaining real-time performance. Our project page is\navailable at https://wzpscott.github.io/hyrf/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17083.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d31df8e3667040af7bc945",
            "avatarUrl": "/avatars/abe1137fccc45da0e1bd41ef0c172899.svg",
            "fullname": "Zipeng Wang",
            "name": "ZipW",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.14635",
            "authors": [
                {
                    "_id": "68ccd8ed3df9ac65e93dc6f6",
                    "name": "Weihan Peng",
                    "hidden": false
                },
                {
                    "_id": "68ccd8ed3df9ac65e93dc6f7",
                    "user": {
                        "_id": "645b0c3ec35da9c7afd95421",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
                        "isPro": false,
                        "fullname": "Yuling",
                        "user": "YerbaPage",
                        "type": "user"
                    },
                    "name": "Yuling Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:47:58.582Z",
                    "hidden": false
                },
                {
                    "_id": "68ccd8ed3df9ac65e93dc6f8",
                    "name": "Yuhang Wang",
                    "hidden": false
                },
                {
                    "_id": "68ccd8ed3df9ac65e93dc6f9",
                    "name": "Xinyun Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ccd8ed3df9ac65e93dc6fa",
                    "name": "Beijun Shen",
                    "hidden": false
                },
                {
                    "_id": "68ccd8ed3df9ac65e93dc6fb",
                    "name": "Xiaodong Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-18T05:25:32.000Z",
            "submittedOnDailyAt": "2025-09-24T23:35:29.152Z",
            "title": "SWE-QA: Can Language Models Answer Repository-level Code Questions?",
            "submittedOnDailyBy": {
                "_id": "65684c80a9a1a6a50d779f58",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65684c80a9a1a6a50d779f58/it534ZdH5LxRub1M_o3uM.jpeg",
                "isPro": false,
                "fullname": "Silin Chen",
                "user": "Silin-Chen",
                "type": "user"
            },
            "summary": "Understanding and reasoning about entire software repositories is an\nessential capability for intelligent software engineering tools. While existing\nbenchmarks such as CoSQA and CodeQA have advanced the field, they predominantly\nfocus on small, self-contained code snippets. These setups fail to capture the\ncomplexity of real-world repositories, where effective understanding and\nreasoning often require navigating multiple files, understanding software\narchitecture, and grounding answers in long-range code dependencies. In this\npaper, we present SWE-QA, a repository-level code question answering (QA)\nbenchmark designed to facilitate research on automated QA systems in realistic\ncode environments. SWE-QA involves 576 high-quality question-answer pairs\nspanning diverse categories, including intention understanding, cross-file\nreasoning, and multi-hop dependency analysis. To construct SWE-QA, we first\ncrawled 77,100 GitHub issues from 11 popular repositories. Based on an analysis\nof naturally occurring developer questions extracted from these issues, we\ndeveloped a two-level taxonomy of repository-level questions and constructed a\nset of seed questions for each category. For each category, we manually curated\nand validated questions and collected their corresponding answers. As a\nprototype application, we further develop SWE-QA-Agent, an agentic framework in\nwhich LLM agents reason and act to find answers automatically. We evaluate six\nadvanced LLMs on SWE-QA under various context augmentation strategies.\nExperimental results highlight the promise of LLMs, particularly our\nSWE-QA-Agent framework, in addressing repository-level QA, while also revealing\nopen challenges and pointing to future research directions.",
            "upvotes": 3,
            "discussionId": "68ccd8ed3df9ac65e93dc6fc",
            "githubRepo": "https://github.com/peng-weihan/SWE-QA-Bench",
            "ai_summary": "SWE-QA is a repository-level code question answering benchmark that addresses the complexity of real-world software repositories, evaluated using advanced LLMs and an agentic framework.",
            "ai_keywords": [
                "code question answering",
                "repository-level QA",
                "software repositories",
                "cross-file reasoning",
                "multi-hop dependency analysis",
                "LLM agents",
                "SWE-QA-Agent",
                "GitHub issues"
            ],
            "githubStars": 16
        },
        "publishedAt": "2025-09-18T01:25:32.000Z",
        "title": "SWE-QA: Can Language Models Answer Repository-level Code Questions?",
        "summary": "Understanding and reasoning about entire software repositories is an\nessential capability for intelligent software engineering tools. While existing\nbenchmarks such as CoSQA and CodeQA have advanced the field, they predominantly\nfocus on small, self-contained code snippets. These setups fail to capture the\ncomplexity of real-world repositories, where effective understanding and\nreasoning often require navigating multiple files, understanding software\narchitecture, and grounding answers in long-range code dependencies. In this\npaper, we present SWE-QA, a repository-level code question answering (QA)\nbenchmark designed to facilitate research on automated QA systems in realistic\ncode environments. SWE-QA involves 576 high-quality question-answer pairs\nspanning diverse categories, including intention understanding, cross-file\nreasoning, and multi-hop dependency analysis. To construct SWE-QA, we first\ncrawled 77,100 GitHub issues from 11 popular repositories. Based on an analysis\nof naturally occurring developer questions extracted from these issues, we\ndeveloped a two-level taxonomy of repository-level questions and constructed a\nset of seed questions for each category. For each category, we manually curated\nand validated questions and collected their corresponding answers. As a\nprototype application, we further develop SWE-QA-Agent, an agentic framework in\nwhich LLM agents reason and act to find answers automatically. We evaluate six\nadvanced LLMs on SWE-QA under various context augmentation strategies.\nExperimental results highlight the promise of LLMs, particularly our\nSWE-QA-Agent framework, in addressing repository-level QA, while also revealing\nopen challenges and pointing to future research directions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14635.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65684c80a9a1a6a50d779f58",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65684c80a9a1a6a50d779f58/it534ZdH5LxRub1M_o3uM.jpeg",
            "fullname": "Silin Chen",
            "name": "Silin-Chen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.17349",
            "authors": [
                {
                    "_id": "68d2871f0e215259d193b041",
                    "name": "Peter Polk",
                    "hidden": false
                },
                {
                    "_id": "68d2871f0e215259d193b042",
                    "user": {
                        "_id": "66309b3833ccd9e68c5d5171",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg",
                        "isPro": false,
                        "fullname": "Sara Papi",
                        "user": "spapi",
                        "type": "user"
                    },
                    "name": "Sara Papi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:54:25.376Z",
                    "hidden": false
                },
                {
                    "_id": "68d2871f0e215259d193b043",
                    "user": {
                        "_id": "662eb48f5ade870b60db4380",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ozHyP-GBIAEhR-l-TkBFb.jpeg",
                        "isPro": false,
                        "fullname": "Luisa Bentivogli",
                        "user": "lubentivogli",
                        "type": "user"
                    },
                    "name": "Luisa Bentivogli",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:31:19.179Z",
                    "hidden": false
                },
                {
                    "_id": "68d2871f0e215259d193b044",
                    "user": {
                        "_id": "6551481e685ba4c13da2e244",
                        "avatarUrl": "/avatars/2455e45268b2b3cfd92bf7fa516ca229.svg",
                        "isPro": false,
                        "fullname": "Ondrej Bojar",
                        "user": "OndrejBojar",
                        "type": "user"
                    },
                    "name": "Ondej Bojar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:31:25.721Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T04:21:19.000Z",
            "submittedOnDailyAt": "2025-09-24T11:54:02.143Z",
            "title": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous\n  Speech-to-Text Translation",
            "submittedOnDailyBy": {
                "_id": "66309b3833ccd9e68c5d5171",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg",
                "isPro": false,
                "fullname": "Sara Papi",
                "user": "spapi",
                "type": "user"
            },
            "summary": "Simultaneous speech-to-text translation (SimulST) systems have to balance\ntranslation quality with latency--the delay between speech input and the\ntranslated output. While quality evaluation is well established, accurate\nlatency measurement remains a challenge. Existing metrics often produce\ninconsistent or misleading results, especially in the widely used short-form\nsetting, where speech is artificially presegmented. In this paper, we present\nthe first comprehensive analysis of SimulST latency metrics across language\npairs, systems, and both short- and long-form regimes. We uncover a structural\nbias in current metrics related to segmentation that undermines fair and\nmeaningful comparisons. To address this, we introduce YAAL (Yet Another Average\nLagging), a refined latency metric that delivers more accurate evaluations in\nthe short-form regime. We extend YAAL to LongYAAL for unsegmented audio and\npropose SoftSegmenter, a novel resegmentation tool based on word-level\nalignment. Our experiments show that YAAL and LongYAAL outperform popular\nlatency metrics, while SoftSegmenter enhances alignment quality in long-form\nevaluation, together enabling more reliable assessments of SimulST systems.",
            "upvotes": 2,
            "discussionId": "68d2871f0e215259d193b045",
            "ai_summary": "The paper analyzes SimulST latency metrics, identifies segmentation bias, and introduces YAAL and LongYAAL for more accurate latency evaluation, along with SoftSegmenter for improved alignment quality.",
            "ai_keywords": [
                "SimulST",
                "latency metrics",
                "short-form",
                "long-form",
                "segmentation bias",
                "YAAL",
                "LongYAAL",
                "SoftSegmenter",
                "word-level alignment"
            ]
        },
        "publishedAt": "2025-09-22T00:21:19.000Z",
        "title": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous\n  Speech-to-Text Translation",
        "summary": "Simultaneous speech-to-text translation (SimulST) systems have to balance\ntranslation quality with latency--the delay between speech input and the\ntranslated output. While quality evaluation is well established, accurate\nlatency measurement remains a challenge. Existing metrics often produce\ninconsistent or misleading results, especially in the widely used short-form\nsetting, where speech is artificially presegmented. In this paper, we present\nthe first comprehensive analysis of SimulST latency metrics across language\npairs, systems, and both short- and long-form regimes. We uncover a structural\nbias in current metrics related to segmentation that undermines fair and\nmeaningful comparisons. To address this, we introduce YAAL (Yet Another Average\nLagging), a refined latency metric that delivers more accurate evaluations in\nthe short-form regime. We extend YAAL to LongYAAL for unsegmented audio and\npropose SoftSegmenter, a novel resegmentation tool based on word-level\nalignment. Our experiments show that YAAL and LongYAAL outperform popular\nlatency metrics, while SoftSegmenter enhances alignment quality in long-form\nevaluation, together enabling more reliable assessments of SimulST systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17349.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66309b3833ccd9e68c5d5171",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg",
            "fullname": "Sara Papi",
            "name": "spapi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.16506",
            "authors": [
                {
                    "_id": "68d41837b1702be9357f4133",
                    "name": "Joe Barrow",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-20T02:55:40.000Z",
            "submittedOnDailyAt": "2025-09-24T14:44:16.968Z",
            "title": "CommonForms: A Large, Diverse Dataset for Form Field Detection",
            "submittedOnDailyBy": {
                "_id": "64b71b089ebb7e6c7dce4d38",
                "avatarUrl": "/avatars/b5601682b6500f30369ad7660e88bb55.svg",
                "isPro": false,
                "fullname": "Joe Barrow",
                "user": "jbarrow",
                "type": "user"
            },
            "summary": "This paper introduces CommonForms, a web-scale dataset for form field\ndetection. It casts the problem of form field detection as object detection:\ngiven an image of a page, predict the location and type (Text Input, Choice\nButton, Signature) of form fields. The dataset is constructed by filtering\nCommon Crawl to find PDFs that have fillable elements. Starting with 8 million\ndocuments, the filtering process is used to arrive at a final dataset of\nroughly 55k documents that have over 450k pages. Analysis shows that the\ndataset contains a diverse mixture of languages and domains; one third of the\npages are non-English, and among the 14 classified domains, no domain makes up\nmore than 25% of the dataset.\n  In addition, this paper presents a family of form field detectors,\nFFDNet-Small and FFDNet-Large, which attain a very high average precision on\nthe CommonForms test set. Each model cost less than $500 to train. Ablation\nresults show that high-resolution inputs are crucial for high-quality form\nfield detection, and that the cleaning process improves data efficiency over\nusing all PDFs that have fillable fields in Common Crawl. A qualitative\nanalysis shows that they outperform a popular, commercially available PDF\nreader that can prepare forms. Unlike the most popular commercially available\nsolutions, FFDNet can predict checkboxes in addition to text and signature\nfields. This is, to our knowledge, the first large scale dataset released for\nform field detection, as well as the first open source models. The dataset,\nmodels, and code will be released at https://github.com/jbarrow/commonforms",
            "upvotes": 2,
            "discussionId": "68d41837b1702be9357f4134",
            "ai_summary": "A web-scale dataset and models for form field detection are introduced, achieving high precision and supporting diverse languages and domains.",
            "ai_keywords": [
                "object detection",
                "form field detection",
                "CommonForms",
                "FFDNet-Small",
                "FFDNet-Large",
                "high-resolution inputs",
                "data efficiency",
                "qualitative analysis",
                "checkboxes",
                "text fields",
                "signature fields"
            ]
        },
        "publishedAt": "2025-09-19T22:55:40.000Z",
        "title": "CommonForms: A Large, Diverse Dataset for Form Field Detection",
        "summary": "This paper introduces CommonForms, a web-scale dataset for form field\ndetection. It casts the problem of form field detection as object detection:\ngiven an image of a page, predict the location and type (Text Input, Choice\nButton, Signature) of form fields. The dataset is constructed by filtering\nCommon Crawl to find PDFs that have fillable elements. Starting with 8 million\ndocuments, the filtering process is used to arrive at a final dataset of\nroughly 55k documents that have over 450k pages. Analysis shows that the\ndataset contains a diverse mixture of languages and domains; one third of the\npages are non-English, and among the 14 classified domains, no domain makes up\nmore than 25% of the dataset.\n  In addition, this paper presents a family of form field detectors,\nFFDNet-Small and FFDNet-Large, which attain a very high average precision on\nthe CommonForms test set. Each model cost less than $500 to train. Ablation\nresults show that high-resolution inputs are crucial for high-quality form\nfield detection, and that the cleaning process improves data efficiency over\nusing all PDFs that have fillable fields in Common Crawl. A qualitative\nanalysis shows that they outperform a popular, commercially available PDF\nreader that can prepare forms. Unlike the most popular commercially available\nsolutions, FFDNet can predict checkboxes in addition to text and signature\nfields. This is, to our knowledge, the first large scale dataset released for\nform field detection, as well as the first open source models. The dataset,\nmodels, and code will be released at https://github.com/jbarrow/commonforms",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.16506.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b71b089ebb7e6c7dce4d38",
            "avatarUrl": "/avatars/b5601682b6500f30369ad7660e88bb55.svg",
            "fullname": "Joe Barrow",
            "name": "jbarrow",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.19087",
            "authors": [
                {
                    "_id": "68d350120e215259d193b1b7",
                    "user": {
                        "_id": "67b500597fa5921119aa5358",
                        "avatarUrl": "/avatars/33c283de346ec83f6ddc98d6d3b8538d.svg",
                        "isPro": false,
                        "fullname": "Ganesh Mallya",
                        "user": "gmallya",
                        "type": "user"
                    },
                    "name": "Ganesh Mallya",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:33:44.714Z",
                    "hidden": false
                },
                {
                    "_id": "68d350120e215259d193b1b8",
                    "name": "Yotam Gigi",
                    "hidden": false
                },
                {
                    "_id": "68d350120e215259d193b1b9",
                    "user": {
                        "_id": "6461d65b5ea1bc9a9da98b88",
                        "avatarUrl": "/avatars/fd6e3715ca8f918ca074356244bb9ce9.svg",
                        "isPro": false,
                        "fullname": "Dahun Kim",
                        "user": "mcahny",
                        "type": "user"
                    },
                    "name": "Dahun Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:33:55.555Z",
                    "hidden": false
                },
                {
                    "_id": "68d350120e215259d193b1ba",
                    "name": "Maxim Neumann",
                    "hidden": false
                },
                {
                    "_id": "68d350120e215259d193b1bb",
                    "user": {
                        "_id": "6763fb464cbdc8085793d563",
                        "avatarUrl": "/avatars/2b68f44da4379052ed87ed3970acf43e.svg",
                        "isPro": false,
                        "fullname": "Genady Beryozkin",
                        "user": "genadyb",
                        "type": "user"
                    },
                    "name": "Genady Beryozkin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:34:07.003Z",
                    "hidden": false
                },
                {
                    "_id": "68d350120e215259d193b1bc",
                    "name": "Tomer Shekel",
                    "hidden": false
                },
                {
                    "_id": "68d350120e215259d193b1bd",
                    "name": "Anelia Angelova",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-23T14:40:52.000Z",
            "submittedOnDailyAt": "2025-09-24T00:27:49.250Z",
            "title": "Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal\n  Gemini 2.5 Model for Remote Sensing Applications",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Multi-spectral imagery plays a crucial role in diverse Remote Sensing\napplications including land-use classification, environmental monitoring and\nurban planning. These images are widely adopted because their additional\nspectral bands correlate strongly with physical materials on the ground, such\nas ice, water, and vegetation. This allows for more accurate identification,\nand their public availability from missions, such as Sentinel-2 and Landsat,\nonly adds to their value. Currently, the automatic analysis of such data is\npredominantly managed through machine learning models specifically trained for\nmulti-spectral input, which are costly to train and support. Furthermore,\nalthough providing a lot of utility for Remote Sensing, such additional inputs\ncannot be used with powerful generalist large multimodal models, which are\ncapable of solving many visual problems, but are not able to understand\nspecialized multi-spectral signals.\n  To address this, we propose a training-free approach which introduces new\nmulti-spectral data in a Zero-Shot-only mode, as inputs to generalist\nmultimodal models, trained on RGB-only inputs. Our approach leverages the\nmultimodal models' understanding of the visual space, and proposes to adapt to\ninputs to that space, and to inject domain-specific information as instructions\ninto the model. We exemplify this idea with the Gemini2.5 model and observe\nstrong Zero-Shot performance gains of the approach on popular Remote Sensing\nbenchmarks for land cover and land use classification and demonstrate the easy\nadaptability of Gemini2.5 to new inputs. These results highlight the potential\nfor geospatial professionals, working with non-standard specialized inputs, to\neasily leverage powerful multimodal models, such as Gemini2.5, to accelerate\ntheir work, benefiting from their rich reasoning and contextual capabilities,\ngrounded in the specialized sensor data.",
            "upvotes": 1,
            "discussionId": "68d350120e215259d193b1be",
            "ai_summary": "A training-free method enables generalist multimodal models to process multi-spectral imagery in a zero-shot manner, enhancing performance on remote sensing tasks.",
            "ai_keywords": [
                "multi-spectral imagery",
                "remote sensing",
                "land-use classification",
                "environmental monitoring",
                "urban planning",
                "machine learning models",
                "multimodal models",
                "zero-shot learning",
                "Gemini2.5",
                "land cover",
                "land use classification"
            ]
        },
        "publishedAt": "2025-09-23T10:40:52.000Z",
        "title": "Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal\n  Gemini 2.5 Model for Remote Sensing Applications",
        "summary": "Multi-spectral imagery plays a crucial role in diverse Remote Sensing\napplications including land-use classification, environmental monitoring and\nurban planning. These images are widely adopted because their additional\nspectral bands correlate strongly with physical materials on the ground, such\nas ice, water, and vegetation. This allows for more accurate identification,\nand their public availability from missions, such as Sentinel-2 and Landsat,\nonly adds to their value. Currently, the automatic analysis of such data is\npredominantly managed through machine learning models specifically trained for\nmulti-spectral input, which are costly to train and support. Furthermore,\nalthough providing a lot of utility for Remote Sensing, such additional inputs\ncannot be used with powerful generalist large multimodal models, which are\ncapable of solving many visual problems, but are not able to understand\nspecialized multi-spectral signals.\n  To address this, we propose a training-free approach which introduces new\nmulti-spectral data in a Zero-Shot-only mode, as inputs to generalist\nmultimodal models, trained on RGB-only inputs. Our approach leverages the\nmultimodal models' understanding of the visual space, and proposes to adapt to\ninputs to that space, and to inject domain-specific information as instructions\ninto the model. We exemplify this idea with the Gemini2.5 model and observe\nstrong Zero-Shot performance gains of the approach on popular Remote Sensing\nbenchmarks for land cover and land use classification and demonstrate the easy\nadaptability of Gemini2.5 to new inputs. These results highlight the potential\nfor geospatial professionals, working with non-standard specialized inputs, to\neasily leverage powerful multimodal models, such as Gemini2.5, to accelerate\ntheir work, benefiting from their rich reasoning and contextual capabilities,\ngrounded in the specialized sensor data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19087.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 109
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.19002",
            "authors": [
                {
                    "_id": "68d3903b0e215259d193b3ad",
                    "user": {
                        "_id": "625cfd862f4600c018fcb8a3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650261345491-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Hao Wang",
                        "user": "conan1024hao",
                        "type": "user"
                    },
                    "name": "Hao Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-24T13:47:21.213Z",
                    "hidden": false
                },
                {
                    "_id": "68d3903b0e215259d193b3ae",
                    "user": {
                        "_id": "62e3ef9cb8b56529407b481f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e3ef9cb8b56529407b481f/s-m-iUP8cVHLc8NknULxJ.png",
                        "isPro": false,
                        "fullname": "Eiki Murata",
                        "user": "Eiki",
                        "type": "user"
                    },
                    "name": "Eiki Murata",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:32:28.453Z",
                    "hidden": false
                },
                {
                    "_id": "68d3903b0e215259d193b3af",
                    "user": {
                        "_id": "6847c1fee176d94459c545bd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6847c1fee176d94459c545bd/ruAMzH28FQVTP11x2fk1J.jpeg",
                        "isPro": false,
                        "fullname": "Lingfang Zhang",
                        "user": "mamezlf",
                        "type": "user"
                    },
                    "name": "Lingfang Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:32:35.274Z",
                    "hidden": false
                },
                {
                    "_id": "68d3903b0e215259d193b3b0",
                    "name": "Ayako Sato",
                    "hidden": false
                },
                {
                    "_id": "68d3903b0e215259d193b3b1",
                    "name": "So Fukuda",
                    "hidden": false
                },
                {
                    "_id": "68d3903b0e215259d193b3b2",
                    "user": {
                        "_id": "642a897a1fde05202fc041c4",
                        "avatarUrl": "/avatars/3e3d28969642b47e245ab0eb0573b5ca.svg",
                        "isPro": false,
                        "fullname": "Ziqi Yin",
                        "user": "Zikky",
                        "type": "user"
                    },
                    "name": "Ziqi Yin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:32:46.087Z",
                    "hidden": false
                },
                {
                    "_id": "68d3903b0e215259d193b3b3",
                    "user": {
                        "_id": "6583ea04a76333af9f59f8f6",
                        "avatarUrl": "/avatars/73ff345965db3793b9367b69fc8d5906.svg",
                        "isPro": false,
                        "fullname": "Wentao Hu",
                        "user": "wentaohu",
                        "type": "user"
                    },
                    "name": "Wentao Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:32:52.936Z",
                    "hidden": false
                },
                {
                    "_id": "68d3903b0e215259d193b3b4",
                    "name": "Keisuke Nakao",
                    "hidden": false
                },
                {
                    "_id": "68d3903b0e215259d193b3b5",
                    "name": "Yusuke Nakamura",
                    "hidden": false
                },
                {
                    "_id": "68d3903b0e215259d193b3b6",
                    "user": {
                        "_id": "66645b18f4b20b92ca10ca98",
                        "avatarUrl": "/avatars/c1909e9ccc366c86b77730c39a7580a5.svg",
                        "isPro": false,
                        "fullname": "Sebastian Zwirner",
                        "user": "SebastianZw",
                        "type": "user"
                    },
                    "name": "Sebastian Zwirner",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:33:30.658Z",
                    "hidden": false
                },
                {
                    "_id": "68d3903b0e215259d193b3b7",
                    "name": "Yi-Chia Chen",
                    "hidden": false
                },
                {
                    "_id": "68d3903b0e215259d193b3b8",
                    "name": "Hiroyuki Otomo",
                    "hidden": false
                },
                {
                    "_id": "68d3903b0e215259d193b3b9",
                    "user": {
                        "_id": "662b2dc272e3e3e4d8f4643c",
                        "avatarUrl": "/avatars/2c204cedbe74b85d8fe2494ce9bd846c.svg",
                        "isPro": false,
                        "fullname": "Hiroki Ouchi",
                        "user": "hiroki-o",
                        "type": "user"
                    },
                    "name": "Hiroki Ouchi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:33:14.758Z",
                    "hidden": false
                },
                {
                    "_id": "68d3903b0e215259d193b3ba",
                    "name": "Daisuke Kawahara",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/625cfd862f4600c018fcb8a3/zkmvSAO48wifsIQajtJFL.png"
            ],
            "publishedAt": "2025-09-23T13:46:31.000Z",
            "submittedOnDailyAt": "2025-09-24T05:02:27.764Z",
            "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via\n  Travel Video Itinerary Reconstruction",
            "submittedOnDailyBy": {
                "_id": "625cfd862f4600c018fcb8a3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650261345491-noauth.jpeg",
                "isPro": false,
                "fullname": "Hao Wang",
                "user": "conan1024hao",
                "type": "user"
            },
            "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced video understanding capabilities, opening new\npossibilities for practical applications. Yet current video benchmarks focus\nlargely on indoor scenes or short-range outdoor activities, leaving the\nchallenges associated with long-distance travel largely unexplored. Mastering\nextended geospatial-temporal trajectories is critical for next-generation\nMLLMs, underpinning real-world tasks such as embodied-AI planning and\nnavigation. To bridge this gap, we present VIR-Bench, a novel benchmark\nconsisting of 200 travel videos that frames itinerary reconstruction as a\nchallenging task designed to evaluate and push forward MLLMs'\ngeospatial-temporal intelligence. Experimental results reveal that\nstate-of-the-art MLLMs, including proprietary ones, struggle to achieve high\nscores, underscoring the difficulty of handling videos that span extended\nspatial and temporal scales. Moreover, we conduct an in-depth case study in\nwhich we develop a prototype travel-planning agent that leverages the insights\ngained from VIR-Bench. The agent's markedly improved itinerary recommendations\nverify that our evaluation protocol not only benchmarks models effectively but\nalso translates into concrete performance gains in user-facing applications.",
            "upvotes": 1,
            "discussionId": "68d3903c0e215259d193b3bb",
            "githubRepo": "https://github.com/nlp-waseda/VIR-Bench",
            "ai_summary": "VIR-Bench, a new benchmark for travel videos, evaluates and enhances MLLMs' geospatial-temporal intelligence, improving itinerary recommendations in real-world applications.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "video understanding",
                "video benchmarks",
                "indoor scenes",
                "outdoor activities",
                "long-distance travel",
                "geospatial-temporal trajectories",
                "embodied-AI planning",
                "navigation",
                "itinerary reconstruction",
                "geospatial-temporal intelligence",
                "travel-planning agent",
                "itinerary recommendations"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-09-23T09:46:31.000Z",
        "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via\n  Travel Video Itinerary Reconstruction",
        "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced video understanding capabilities, opening new\npossibilities for practical applications. Yet current video benchmarks focus\nlargely on indoor scenes or short-range outdoor activities, leaving the\nchallenges associated with long-distance travel largely unexplored. Mastering\nextended geospatial-temporal trajectories is critical for next-generation\nMLLMs, underpinning real-world tasks such as embodied-AI planning and\nnavigation. To bridge this gap, we present VIR-Bench, a novel benchmark\nconsisting of 200 travel videos that frames itinerary reconstruction as a\nchallenging task designed to evaluate and push forward MLLMs'\ngeospatial-temporal intelligence. Experimental results reveal that\nstate-of-the-art MLLMs, including proprietary ones, struggle to achieve high\nscores, underscoring the difficulty of handling videos that span extended\nspatial and temporal scales. Moreover, we conduct an in-depth case study in\nwhich we develop a prototype travel-planning agent that leverages the insights\ngained from VIR-Bench. The agent's markedly improved itinerary recommendations\nverify that our evaluation protocol not only benchmarks models effectively but\nalso translates into concrete performance gains in user-facing applications.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/625cfd862f4600c018fcb8a3/zkmvSAO48wifsIQajtJFL.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19002.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "625cfd862f4600c018fcb8a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650261345491-noauth.jpeg",
            "fullname": "Hao Wang",
            "name": "conan1024hao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.18090",
            "authors": [
                {
                    "_id": "68d3e32c4402066be56e1a90",
                    "user": {
                        "_id": "66eec3889cbe309858abb830",
                        "avatarUrl": "/avatars/aeaee6ee34c867d246b1c9334048351d.svg",
                        "isPro": false,
                        "fullname": "Jiahe Li",
                        "user": "Fictionary",
                        "type": "user"
                    },
                    "name": "Jiahe Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:31:49.547Z",
                    "hidden": false
                },
                {
                    "_id": "68d3e32c4402066be56e1a91",
                    "name": "Jiawei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d3e32c4402066be56e1a92",
                    "user": {
                        "_id": "65e054827409351736166d73",
                        "avatarUrl": "/avatars/d56b8af16b47cdac7a47d7bc0563687a.svg",
                        "isPro": false,
                        "fullname": "Youmin Zhang",
                        "user": "Rock-youmi",
                        "type": "user"
                    },
                    "name": "Youmin Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:32:01.994Z",
                    "hidden": false
                },
                {
                    "_id": "68d3e32c4402066be56e1a93",
                    "name": "Xiao Bai",
                    "hidden": false
                },
                {
                    "_id": "68d3e32c4402066be56e1a94",
                    "name": "Jin Zheng",
                    "hidden": false
                },
                {
                    "_id": "68d3e32c4402066be56e1a95",
                    "user": {
                        "_id": "68be6f5c0e70474611c18ce1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FPbaWvBxwvR7m6pABhTFo.png",
                        "isPro": false,
                        "fullname": "Xiaohan Yu",
                        "user": "XiaoHan630",
                        "type": "user"
                    },
                    "name": "Xiaohan Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:32:16.933Z",
                    "hidden": false
                },
                {
                    "_id": "68d3e32c4402066be56e1a96",
                    "user": {
                        "_id": "633fe5ced4fc96dd3a054db5",
                        "avatarUrl": "/avatars/f3f2810b97f5ef0e4f72d40278f00204.svg",
                        "isPro": false,
                        "fullname": "Lin Gu",
                        "user": "LinGu",
                        "type": "user"
                    },
                    "name": "Lin Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:32:10.065Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T17:58:48.000Z",
            "submittedOnDailyAt": "2025-09-24T10:57:10.183Z",
            "title": "GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface\n  Reconstruction",
            "submittedOnDailyBy": {
                "_id": "66eec3889cbe309858abb830",
                "avatarUrl": "/avatars/aeaee6ee34c867d246b1c9334048351d.svg",
                "isPro": false,
                "fullname": "Jiahe Li",
                "user": "Fictionary",
                "type": "user"
            },
            "summary": "Reconstructing accurate surfaces with radiance fields has achieved remarkable\nprogress in recent years. However, prevailing approaches, primarily based on\nGaussian Splatting, are increasingly constrained by representational\nbottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based\nframework that explores and extends the under-investigated potential of sparse\nvoxels for achieving accurate, detailed, and complete surface reconstruction.\nAs strengths, sparse voxels support preserving the coverage completeness and\ngeometric clarity, while corresponding challenges also arise from absent scene\nconstraints and locality in surface refinement. To ensure correct scene\nconvergence, we first propose a Voxel-Uncertainty Depth Constraint that\nmaximizes the effect of monocular depth cues while presenting a voxel-oriented\nuncertainty to avoid quality degradation, enabling effective and robust scene\nconstraints yet preserving highly accurate geometries. Subsequently, Sparse\nVoxel Surface Regularization is designed to enhance geometric consistency for\ntiny voxels and facilitate the voxel-based formation of sharp and accurate\nsurfaces. Extensive experiments demonstrate our superior performance compared\nto existing methods across diverse challenging scenarios, excelling in\ngeometric accuracy, detail preservation, and reconstruction completeness while\nmaintaining high efficiency. Code is available at\nhttps://github.com/Fictionarry/GeoSVR.",
            "upvotes": 1,
            "discussionId": "68d3e32c4402066be56e1a97",
            "projectPage": "https://fictionarry.github.io/GeoSVR-project/",
            "githubRepo": "https://github.com/Fictionarry/GeoSVR",
            "ai_summary": "GeoSVR, a voxel-based framework, improves surface reconstruction accuracy and detail using sparse voxels with depth constraints and surface regularization.",
            "ai_keywords": [
                "Gaussian Splatting",
                "GeoSVR",
                "voxel-based framework",
                "sparse voxels",
                "Voxel-Uncertainty Depth Constraint",
                "Sparse Voxel Surface Regularization",
                "geometric accuracy",
                "detail preservation",
                "reconstruction completeness"
            ],
            "githubStars": 29
        },
        "publishedAt": "2025-09-22T13:58:48.000Z",
        "title": "GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface\n  Reconstruction",
        "summary": "Reconstructing accurate surfaces with radiance fields has achieved remarkable\nprogress in recent years. However, prevailing approaches, primarily based on\nGaussian Splatting, are increasingly constrained by representational\nbottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based\nframework that explores and extends the under-investigated potential of sparse\nvoxels for achieving accurate, detailed, and complete surface reconstruction.\nAs strengths, sparse voxels support preserving the coverage completeness and\ngeometric clarity, while corresponding challenges also arise from absent scene\nconstraints and locality in surface refinement. To ensure correct scene\nconvergence, we first propose a Voxel-Uncertainty Depth Constraint that\nmaximizes the effect of monocular depth cues while presenting a voxel-oriented\nuncertainty to avoid quality degradation, enabling effective and robust scene\nconstraints yet preserving highly accurate geometries. Subsequently, Sparse\nVoxel Surface Regularization is designed to enhance geometric consistency for\ntiny voxels and facilitate the voxel-based formation of sharp and accurate\nsurfaces. Extensive experiments demonstrate our superior performance compared\nto existing methods across diverse challenging scenarios, excelling in\ngeometric accuracy, detail preservation, and reconstruction completeness while\nmaintaining high efficiency. Code is available at\nhttps://github.com/Fictionarry/GeoSVR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18090.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66eec3889cbe309858abb830",
            "avatarUrl": "/avatars/aeaee6ee34c867d246b1c9334048351d.svg",
            "fullname": "Jiahe Li",
            "name": "Fictionary",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.18030",
            "authors": [
                {
                    "_id": "68d40c58e151e844b496c5fb",
                    "name": "Justin Xu",
                    "hidden": false
                },
                {
                    "_id": "68d40c58e151e844b496c5fc",
                    "name": "Xi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d40c58e151e844b496c5fd",
                    "name": "Javid Abderezaei",
                    "hidden": false
                },
                {
                    "_id": "68d40c58e151e844b496c5fe",
                    "name": "Julie Bauml",
                    "hidden": false
                },
                {
                    "_id": "68d40c58e151e844b496c5ff",
                    "name": "Roger Boodoo",
                    "hidden": false
                },
                {
                    "_id": "68d40c58e151e844b496c600",
                    "name": "Fatemeh Haghighi",
                    "hidden": false
                },
                {
                    "_id": "68d40c58e151e844b496c601",
                    "name": "Ali Ganjizadeh",
                    "hidden": false
                },
                {
                    "_id": "68d40c58e151e844b496c602",
                    "name": "Eric Brattain",
                    "hidden": false
                },
                {
                    "_id": "68d40c58e151e844b496c603",
                    "name": "Dave Van Veen",
                    "hidden": false
                },
                {
                    "_id": "68d40c58e151e844b496c604",
                    "name": "Zaiqiao Meng",
                    "hidden": false
                },
                {
                    "_id": "68d40c58e151e844b496c605",
                    "name": "David Eyre",
                    "hidden": false
                },
                {
                    "_id": "68d40c58e151e844b496c606",
                    "name": "Jean-Benoit Delbrouck",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T17:03:48.000Z",
            "submittedOnDailyAt": "2025-09-24T22:25:33.486Z",
            "title": "RadEval: A framework for radiology text evaluation",
            "submittedOnDailyBy": {
                "_id": "652ebdcc76365388909b06cf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ebdcc76365388909b06cf/MnIVVuTphX8buEER9EkCY.jpeg",
                "isPro": false,
                "fullname": "Xi Zhang",
                "user": "X-iZhang",
                "type": "user"
            },
            "summary": "We introduce RadEval, a unified, open-source framework for evaluating\nradiology texts. RadEval consolidates a diverse range of metrics, from classic\nn-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical\nconcept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT,\nTemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and\nstandardize implementations, extend GREEN to support multiple imaging\nmodalities with a more lightweight model, and pretrain a domain-specific\nradiology encoder, demonstrating strong zero-shot retrieval performance. We\nalso release a richly annotated expert dataset with over 450 clinically\nsignificant error labels and show how different metrics correlate with\nradiologist judgment. Finally, RadEval provides statistical testing tools and\nbaseline model evaluations across multiple publicly available datasets,\nfacilitating reproducibility and robust benchmarking in radiology report\ngeneration.",
            "upvotes": 1,
            "discussionId": "68d40c59e151e844b496c607",
            "githubRepo": "https://github.com/jbdel/RadEval",
            "ai_summary": "RadEval is a comprehensive framework for evaluating radiology texts using a variety of metrics, including n-gram overlap, contextual measures, clinical concept-based scores, and advanced LLM-based evaluators, with a focus on reproducibility and robust benchmarking.",
            "ai_keywords": [
                "n-gram overlap",
                "BLEU",
                "ROUGE",
                "contextual measures",
                "BERTScore",
                "clinical concept-based scores",
                "F1CheXbert",
                "F1RadGraph",
                "RaTEScore",
                "SRR-BERT",
                "TemporalEntityF1",
                "advanced LLM-based evaluators",
                "GREEN",
                "zero-shot retrieval",
                "radiology encoder",
                "expert dataset",
                "radiologist judgment",
                "statistical testing tools",
                "baseline model evaluations"
            ],
            "githubStars": 15
        },
        "publishedAt": "2025-09-22T13:03:48.000Z",
        "title": "RadEval: A framework for radiology text evaluation",
        "summary": "We introduce RadEval, a unified, open-source framework for evaluating\nradiology texts. RadEval consolidates a diverse range of metrics, from classic\nn-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical\nconcept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT,\nTemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and\nstandardize implementations, extend GREEN to support multiple imaging\nmodalities with a more lightweight model, and pretrain a domain-specific\nradiology encoder, demonstrating strong zero-shot retrieval performance. We\nalso release a richly annotated expert dataset with over 450 clinically\nsignificant error labels and show how different metrics correlate with\nradiologist judgment. Finally, RadEval provides statistical testing tools and\nbaseline model evaluations across multiple publicly available datasets,\nfacilitating reproducibility and robust benchmarking in radiology report\ngeneration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18030.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652ebdcc76365388909b06cf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ebdcc76365388909b06cf/MnIVVuTphX8buEER9EkCY.jpeg",
            "fullname": "Xi Zhang",
            "name": "X-iZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.19274",
            "authors": [
                {
                    "_id": "68d3e8ae4402066be56e1a99",
                    "user": {
                        "_id": "66b725125ce4c02bda63e7f0",
                        "avatarUrl": "/avatars/6f8c6b0bbfdb2d429263dea95ab32bea.svg",
                        "isPro": false,
                        "fullname": "Arijit Maji",
                        "user": "13ari",
                        "type": "user"
                    },
                    "name": "Arijit Maji",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:30:28.360Z",
                    "hidden": false
                },
                {
                    "_id": "68d3e8ae4402066be56e1a9a",
                    "user": {
                        "_id": "6790ea9a766b27e112b9fc1a",
                        "avatarUrl": "/avatars/72502cc892c90ee2e9b825aab94ba9a0.svg",
                        "isPro": false,
                        "fullname": "Raghvendra Kumar",
                        "user": "procodz",
                        "type": "user"
                    },
                    "name": "Raghvendra Kumar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-24T15:30:35.497Z",
                    "hidden": false
                },
                {
                    "_id": "68d3e8ae4402066be56e1a9b",
                    "name": "Akash Ghosh",
                    "hidden": false
                },
                {
                    "_id": "68d3e8ae4402066be56e1a9c",
                    "name": "Anushka",
                    "hidden": false
                },
                {
                    "_id": "68d3e8ae4402066be56e1a9d",
                    "name": "Nemil Shah",
                    "hidden": false
                },
                {
                    "_id": "68d3e8ae4402066be56e1a9e",
                    "name": "Abhilekh Borah",
                    "hidden": false
                },
                {
                    "_id": "68d3e8ae4402066be56e1a9f",
                    "name": "Vanshika Shah",
                    "hidden": false
                },
                {
                    "_id": "68d3e8ae4402066be56e1aa0",
                    "name": "Nishant Mishra",
                    "hidden": false
                },
                {
                    "_id": "68d3e8ae4402066be56e1aa1",
                    "name": "Sriparna Saha",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-23T17:40:43.000Z",
            "submittedOnDailyAt": "2025-09-24T11:22:22.925Z",
            "title": "DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language\n  Models' Understanding on Indian Culture",
            "submittedOnDailyBy": {
                "_id": "65425237ea69bcb6203c8d76",
                "avatarUrl": "/avatars/42953b27288faac8eb1397f194cecc66.svg",
                "isPro": false,
                "fullname": "Abhilekh Borah",
                "user": "abhilekhborah",
                "type": "user"
            },
            "summary": "We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual\nbenchmark centered exclusively on Indian culture, designed to evaluate the\ncultural understanding of generative AI systems. Unlike existing benchmarks\nwith a generic or global scope, DRISHTIKON offers deep, fine-grained coverage\nacross India's diverse regions, spanning 15 languages, covering all states and\nunion territories, and incorporating over 64,000 aligned text-image pairs. The\ndataset captures rich cultural themes including festivals, attire, cuisines,\nart forms, and historical heritage amongst many more. We evaluate a wide range\nof vision-language models (VLMs), including open-source small and large models,\nproprietary systems, reasoning-specialized VLMs, and Indic-focused models,\nacross zero-shot and chain-of-thought settings. Our results expose key\nlimitations in current models' ability to reason over culturally grounded,\nmultimodal inputs, particularly for low-resource languages and less-documented\ntraditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a\nrobust testbed to advance culturally aware, multimodally competent language\ntechnologies.",
            "upvotes": 0,
            "discussionId": "68d3e8ae4402066be56e1aa2",
            "ai_summary": "DRISHTIKON is a multimodal and multilingual benchmark for evaluating generative AI systems' cultural understanding across India's diverse regions and languages.",
            "ai_keywords": [
                "multimodal",
                "multilingual",
                "benchmark",
                "cultural understanding",
                "generative AI systems",
                "text-image pairs",
                "vision-language models",
                "zero-shot",
                "chain-of-thought",
                "culturally grounded",
                "low-resource languages",
                "inclusive AI research",
                "culturally aware",
                "multimodally competent language technologies"
            ]
        },
        "publishedAt": "2025-09-23T13:40:43.000Z",
        "title": "DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language\n  Models' Understanding on Indian Culture",
        "summary": "We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual\nbenchmark centered exclusively on Indian culture, designed to evaluate the\ncultural understanding of generative AI systems. Unlike existing benchmarks\nwith a generic or global scope, DRISHTIKON offers deep, fine-grained coverage\nacross India's diverse regions, spanning 15 languages, covering all states and\nunion territories, and incorporating over 64,000 aligned text-image pairs. The\ndataset captures rich cultural themes including festivals, attire, cuisines,\nart forms, and historical heritage amongst many more. We evaluate a wide range\nof vision-language models (VLMs), including open-source small and large models,\nproprietary systems, reasoning-specialized VLMs, and Indic-focused models,\nacross zero-shot and chain-of-thought settings. Our results expose key\nlimitations in current models' ability to reason over culturally grounded,\nmultimodal inputs, particularly for low-resource languages and less-documented\ntraditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a\nrobust testbed to advance culturally aware, multimodally competent language\ntechnologies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19274.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65425237ea69bcb6203c8d76",
            "avatarUrl": "/avatars/42953b27288faac8eb1397f194cecc66.svg",
            "fullname": "Abhilekh Borah",
            "name": "abhilekhborah",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.18282",
            "authors": [
                {
                    "_id": "68d4213ab1702be9357f4148",
                    "name": "Jesse Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d4213ab1702be9357f4149",
                    "name": "Marius Memmel",
                    "hidden": false
                },
                {
                    "_id": "68d4213ab1702be9357f414a",
                    "name": "Kevin Kim",
                    "hidden": false
                },
                {
                    "_id": "68d4213ab1702be9357f414b",
                    "name": "Dieter Fox",
                    "hidden": false
                },
                {
                    "_id": "68d4213ab1702be9357f414c",
                    "name": "Jesse Thomason",
                    "hidden": false
                },
                {
                    "_id": "68d4213ab1702be9357f414d",
                    "name": "Fabio Ramos",
                    "hidden": false
                },
                {
                    "_id": "68d4213ab1702be9357f414e",
                    "name": "Erdem Byk",
                    "hidden": false
                },
                {
                    "_id": "68d4213ab1702be9357f414f",
                    "name": "Abhishek Gupta",
                    "hidden": false
                },
                {
                    "_id": "68d4213ab1702be9357f4150",
                    "name": "Anqi Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T18:10:14.000Z",
            "submittedOnDailyAt": "2025-09-24T15:25:07.964Z",
            "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot\n  Generalization of Robot Manipulation Policies",
            "submittedOnDailyBy": {
                "_id": "62cf279e3200bfd438e80941",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62cf279e3200bfd438e80941/4Sd_qWMqliNz02Y33SWHn.jpeg",
                "isPro": true,
                "fullname": "Jesse Zhang",
                "user": "jesbu1",
                "type": "user"
            },
            "summary": "Robotic manipulation policies often fail to generalize because they must\nsimultaneously learn where to attend, what actions to take, and how to execute\nthem. We argue that high-level reasoning about where and what can be offloaded\nto vision-language models (VLMs), leaving policies to specialize in how to act.\nWe present PEEK (Policy-agnostic Extraction of Essential Keypoints), which\nfine-tunes VLMs to predict a unified point-based intermediate representation:\n1. end-effector paths specifying what actions to take, and 2. task-relevant\nmasks indicating where to focus. These annotations are directly overlaid onto\nrobot observations, making the representation policy-agnostic and transferable\nacross architectures. To enable scalable training, we introduce an automatic\nannotation pipeline, generating labeled data across 20+ robot datasets spanning\n9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot\ngeneralization, including a 41.4x real-world improvement for a 3D policy\ntrained only in simulation, and 2-3.5x gains for both large VLAs and small\nmanipulation policies. By letting VLMs absorb semantic and visual complexity,\nPEEK equips manipulation policies with the minimal cues they need--where, what,\nand how. Website at https://peek-robot.github.io/.",
            "upvotes": 0,
            "discussionId": "68d4213ab1702be9357f4151",
            "projectPage": "https://peek-robot.github.io/",
            "githubRepo": "https://github.com/peek-robot/peek",
            "ai_summary": "PEEK fine-tunes vision-language models to predict essential keypoints for robotic manipulation, enhancing zero-shot generalization across different policies and robot embodiments.",
            "ai_keywords": [
                "vision-language models",
                "VLMs",
                "end-effector paths",
                "task-relevant masks",
                "policy-agnostic",
                "automatic annotation pipeline",
                "zero-shot generalization",
                "3D policy",
                "VLAs",
                "manipulation policies"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-09-22T14:10:14.000Z",
        "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot\n  Generalization of Robot Manipulation Policies",
        "summary": "Robotic manipulation policies often fail to generalize because they must\nsimultaneously learn where to attend, what actions to take, and how to execute\nthem. We argue that high-level reasoning about where and what can be offloaded\nto vision-language models (VLMs), leaving policies to specialize in how to act.\nWe present PEEK (Policy-agnostic Extraction of Essential Keypoints), which\nfine-tunes VLMs to predict a unified point-based intermediate representation:\n1. end-effector paths specifying what actions to take, and 2. task-relevant\nmasks indicating where to focus. These annotations are directly overlaid onto\nrobot observations, making the representation policy-agnostic and transferable\nacross architectures. To enable scalable training, we introduce an automatic\nannotation pipeline, generating labeled data across 20+ robot datasets spanning\n9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot\ngeneralization, including a 41.4x real-world improvement for a 3D policy\ntrained only in simulation, and 2-3.5x gains for both large VLAs and small\nmanipulation policies. By letting VLMs absorb semantic and visual complexity,\nPEEK equips manipulation policies with the minimal cues they need--where, what,\nand how. Website at https://peek-robot.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18282.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62cf279e3200bfd438e80941",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62cf279e3200bfd438e80941/4Sd_qWMqliNz02Y33SWHn.jpeg",
            "fullname": "Jesse Zhang",
            "name": "jesbu1",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    }
]