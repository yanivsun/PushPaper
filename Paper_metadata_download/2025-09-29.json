[
    {
        "paper": {
            "id": "2509.22622",
            "authors": [
                {
                    "_id": "68d9e2a20177a6054b013a05",
                    "name": "Shuai Yang",
                    "hidden": false
                },
                {
                    "_id": "68d9e2a20177a6054b013a06",
                    "name": "Wei Huang",
                    "hidden": false
                },
                {
                    "_id": "68d9e2a20177a6054b013a07",
                    "name": "Ruihang Chu",
                    "hidden": false
                },
                {
                    "_id": "68d9e2a20177a6054b013a08",
                    "name": "Yicheng Xiao",
                    "hidden": false
                },
                {
                    "_id": "68d9e2a20177a6054b013a09",
                    "name": "Yuyang Zhao",
                    "hidden": false
                },
                {
                    "_id": "68d9e2a20177a6054b013a0a",
                    "name": "Xianbang Wang",
                    "hidden": false
                },
                {
                    "_id": "68d9e2a20177a6054b013a0b",
                    "user": {
                        "_id": "63129589bbaa385279d1826e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63129589bbaa385279d1826e/0AUs3q4ngRZ-wXuY1jP9G.jpeg",
                        "isPro": true,
                        "fullname": "Muyang Li",
                        "user": "Lmxyy",
                        "type": "user"
                    },
                    "name": "Muyang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:59:10.279Z",
                    "hidden": false
                },
                {
                    "_id": "68d9e2a20177a6054b013a0c",
                    "name": "Enze Xie",
                    "hidden": false
                },
                {
                    "_id": "68d9e2a20177a6054b013a0d",
                    "name": "Yingcong Chen",
                    "hidden": false
                },
                {
                    "_id": "68d9e2a20177a6054b013a0e",
                    "name": "Yao Lu",
                    "hidden": false
                },
                {
                    "_id": "68d9e2a20177a6054b013a0f",
                    "name": "Song Han",
                    "hidden": false
                },
                {
                    "_id": "68d9e2a20177a6054b013a10",
                    "name": "Yukang Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T17:48:24.000Z",
            "submittedOnDailyAt": "2025-09-29T00:52:38.341Z",
            "title": "LongLive: Real-time Interactive Long Video Generation",
            "submittedOnDailyBy": {
                "_id": "634ce90e741a5e37886a19e3",
                "avatarUrl": "/avatars/0d1579039136b37db5b67282b0a34c33.svg",
                "isPro": false,
                "fullname": "Syang",
                "user": "Andyson",
                "type": "user"
            },
            "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
            "upvotes": 132,
            "discussionId": "68d9e2a20177a6054b013a11",
            "projectPage": "https://nvlabs.github.io/LongLive/",
            "githubRepo": "https://github.com/NVlabs/LongLive",
            "ai_summary": "LongLive is a frame-level autoregressive framework for real-time and interactive long video generation, addressing efficiency and quality challenges through causal attention, KV-recache, streaming long tuning, and short window attention.",
            "ai_keywords": [
                "frame-level autoregressive",
                "diffusion models",
                "diffusion-forcing models",
                "bidirectional attention",
                "causal attention",
                "KV caching",
                "interactive capabilities",
                "streaming prompt inputs",
                "KV-recache mechanism",
                "streaming long tuning",
                "short window attention",
                "frame-level attention sink",
                "frame sink",
                "long-range consistency",
                "VBench",
                "INT8-quantized inference"
            ],
            "githubStars": 364
        },
        "publishedAt": "2025-09-26T13:48:24.000Z",
        "title": "LongLive: Real-time Interactive Long Video Generation",
        "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22622.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "634ce90e741a5e37886a19e3",
            "avatarUrl": "/avatars/0d1579039136b37db5b67282b0a34c33.svg",
            "fullname": "Syang",
            "name": "Andyson",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "submitterOrganization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22611",
            "authors": [
                {
                    "_id": "68d9ecb50177a6054b013a97",
                    "user": {
                        "_id": "664764e5d834283e7ff96d37",
                        "avatarUrl": "/avatars/ac1d0d2c0ece1fc572e8c43f869bfdc6.svg",
                        "isPro": false,
                        "fullname": "Junkang Wu",
                        "user": "junkang0909",
                        "type": "user"
                    },
                    "name": "Junkang Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:58:39.458Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ecb50177a6054b013a98",
                    "user": {
                        "_id": "6547a247dbce6bd2be168d33",
                        "avatarUrl": "/avatars/23ae384f7c4fc1a89a556a37f5e75acf.svg",
                        "isPro": false,
                        "fullname": "Kexin Huang",
                        "user": "737443h",
                        "type": "user"
                    },
                    "name": "Kexin Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T11:26:45.936Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ecb50177a6054b013a99",
                    "name": "Jiancan Wu",
                    "hidden": false
                },
                {
                    "_id": "68d9ecb50177a6054b013a9a",
                    "name": "An Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d9ecb50177a6054b013a9b",
                    "user": {
                        "_id": "65fca775fa59bdf4737b1a84",
                        "avatarUrl": "/avatars/a161b510bde8f57e7686cbb0b4aa6a52.svg",
                        "isPro": false,
                        "fullname": "Xiang Wang",
                        "user": "xiangwang1223",
                        "type": "user"
                    },
                    "name": "Xiang Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:58:37.188Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ecb50177a6054b013a9c",
                    "name": "Xiangnan He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T17:37:52.000Z",
            "submittedOnDailyAt": "2025-09-29T00:55:10.422Z",
            "title": "Quantile Advantage Estimation for Entropy-Safe Reasoning",
            "submittedOnDailyBy": {
                "_id": "664764e5d834283e7ff96d37",
                "avatarUrl": "/avatars/ac1d0d2c0ece1fc572e8c43f869bfdc6.svg",
                "isPro": false,
                "fullname": "Junkang Wu",
                "user": "junkang0909",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM\nreasoning, but training often oscillates between {entropy collapse} and\n{entropy explosion}. We trace both hazards to the mean baseline used in\nvalue-free RL (e.g., GRPO and DAPO), which improperly penalizes\nnegative-advantage samples under reward outliers. We propose {Quantile\nAdvantage Estimation} (QAE), replacing the mean with a group-wise K-quantile\nbaseline. QAE induces a response-level, two-regime gate: on hard queries (p <=\n1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it\ntargets remaining failures. Under first-order softmax updates, we prove\n{two-sided entropy safety}, giving lower and upper bounds on one-step entropy\nchange that curb explosion and prevent collapse. Empirically, this minimal\nmodification stabilizes entropy, sparsifies credit assignment (with tuned K,\nroughly 80% of responses receive zero advantage), and yields sustained pass@1\ngains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results\nidentify {baseline design} -- rather than token-level heuristics -- as the\nprimary mechanism for scaling RLVR.",
            "upvotes": 100,
            "discussionId": "68d9ecb60177a6054b013a9d",
            "githubRepo": "https://github.com/junkangwu/QAE",
            "ai_summary": "Quantile Advantage Estimation stabilizes reinforcement learning with verifiable rewards by addressing entropy issues and improving performance on large language models.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards",
                "entropy collapse",
                "entropy explosion",
                "value-free RL",
                "GRPO",
                "DAPO",
                "Quantile Advantage Estimation",
                "K-quantile baseline",
                "two-sided entropy safety",
                "pass@1",
                "AIME",
                "AMC",
                "baseline design"
            ],
            "githubStars": 9
        },
        "publishedAt": "2025-09-26T13:37:52.000Z",
        "title": "Quantile Advantage Estimation for Entropy-Safe Reasoning",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM\nreasoning, but training often oscillates between {entropy collapse} and\n{entropy explosion}. We trace both hazards to the mean baseline used in\nvalue-free RL (e.g., GRPO and DAPO), which improperly penalizes\nnegative-advantage samples under reward outliers. We propose {Quantile\nAdvantage Estimation} (QAE), replacing the mean with a group-wise K-quantile\nbaseline. QAE induces a response-level, two-regime gate: on hard queries (p <=\n1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it\ntargets remaining failures. Under first-order softmax updates, we prove\n{two-sided entropy safety}, giving lower and upper bounds on one-step entropy\nchange that curb explosion and prevent collapse. Empirically, this minimal\nmodification stabilizes entropy, sparsifies credit assignment (with tuned K,\nroughly 80% of responses receive zero advantage), and yields sustained pass@1\ngains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results\nidentify {baseline design} -- rather than token-level heuristics -- as the\nprimary mechanism for scaling RLVR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22611.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "664764e5d834283e7ff96d37",
            "avatarUrl": "/avatars/ac1d0d2c0ece1fc572e8c43f869bfdc6.svg",
            "fullname": "Junkang Wu",
            "name": "junkang0909",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.22576",
            "authors": [
                {
                    "_id": "68d9e47c0177a6054b013a13",
                    "user": {
                        "_id": "678523aee670c62966974feb",
                        "avatarUrl": "/avatars/0a0619232f170e517e05657e0ca84699.svg",
                        "isPro": false,
                        "fullname": "Wujiang Xu",
                        "user": "Iscarrot",
                        "type": "user"
                    },
                    "name": "Xu Wujiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:59:06.613Z",
                    "hidden": false
                },
                {
                    "_id": "68d9e47c0177a6054b013a14",
                    "name": "Wentian Zhao",
                    "hidden": false
                },
                {
                    "_id": "68d9e47c0177a6054b013a15",
                    "user": {
                        "_id": "64dfcc62e8b6f3f3baa950e0",
                        "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
                        "isPro": false,
                        "fullname": "Zhenting Wang",
                        "user": "ztwang",
                        "type": "user"
                    },
                    "name": "Zhenting Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T11:27:35.286Z",
                    "hidden": false
                },
                {
                    "_id": "68d9e47c0177a6054b013a16",
                    "name": "Li Yu-Jhe",
                    "hidden": false
                },
                {
                    "_id": "68d9e47c0177a6054b013a17",
                    "name": "Jin Can",
                    "hidden": false
                },
                {
                    "_id": "68d9e47c0177a6054b013a18",
                    "name": "Jin Mingyu",
                    "hidden": false
                },
                {
                    "_id": "68d9e47c0177a6054b013a19",
                    "name": "Mei Kai",
                    "hidden": false
                },
                {
                    "_id": "68d9e47c0177a6054b013a1a",
                    "user": {
                        "_id": "66274e02348a5304435dc9cc",
                        "avatarUrl": "/avatars/bda87559cd497c310597c2fc8430b31f.svg",
                        "isPro": false,
                        "fullname": "Kun Wan",
                        "user": "timecuriosity",
                        "type": "user"
                    },
                    "name": "Wan Kun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:59:02.080Z",
                    "hidden": false
                },
                {
                    "_id": "68d9e47c0177a6054b013a1b",
                    "name": "Metaxas Dimitris",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T16:51:44.000Z",
            "submittedOnDailyAt": "2025-09-29T00:52:23.571Z",
            "title": "EPO: Entropy-regularized Policy Optimization for LLM Agents\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "64dfcc62e8b6f3f3baa950e0",
                "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
                "isPro": false,
                "fullname": "Zhenting Wang",
                "user": "ztwang",
                "type": "user"
            },
            "summary": "Training LLM agents in multi-turn environments with sparse rewards, where\ncompleting a single task requires 30+ turns of interaction within an episode,\npresents a fundamental challenge for reinforcement learning. We identify a\ncritical failure mode unique to this setting: the exploration-exploitation\ncascade failure. This cascade begins with early-stage policy premature\nconvergence, where sparse feedback causes agents to commit to flawed,\nlow-entropy strategies. Subsequently, agents enter late-stage policy collapse,\nwhere conventional entropy regularization becomes counterproductive, promoting\nchaotic exploration that destabilizes training. We propose Entropy-regularized\nPolicy Optimization (EPO), a general framework that breaks this failure cycle\nthrough three synergistic mechanisms: (1) adopting entropy regularization in\nmulti-turn settings to enhance exploration, (2) an entropy smoothing\nregularizer that bounds policy entropy within historical averages to prevent\nabrupt fluctuations, and (3) adaptive phase-based weighting that balances\nexploration and exploitation across training. Our analysis justifies that EPO\nguarantees monotonically decreasing entropy variance while maintaining\nconvergence. EPO achieves up to 152% performance improvement on ScienceWorld\nand up to 19.8% on ALFWorld. Our work demonstrates that multi-turn\nsparse-reward settings require fundamentally different entropy control than\ntraditional RL, with broad implications for LLM agent training.",
            "upvotes": 87,
            "discussionId": "68d9e47c0177a6054b013a1c",
            "githubRepo": "https://github.com/WujiangXu/EPO",
            "ai_summary": "Entropy-regularized Policy Optimization (EPO) addresses exploration-exploitation challenges in multi-turn environments with sparse rewards, improving performance in tasks like ScienceWorld and ALFWorld.",
            "ai_keywords": [
                "reinforcement learning",
                "exploration-exploitation cascade failure",
                "premature convergence",
                "policy collapse",
                "entropy regularization",
                "entropy smoothing regularizer",
                "adaptive phase-based weighting",
                "entropy variance",
                "ScienceWorld",
                "ALFWorld"
            ],
            "githubStars": 12
        },
        "publishedAt": "2025-09-26T12:51:44.000Z",
        "title": "EPO: Entropy-regularized Policy Optimization for LLM Agents\n  Reinforcement Learning",
        "summary": "Training LLM agents in multi-turn environments with sparse rewards, where\ncompleting a single task requires 30+ turns of interaction within an episode,\npresents a fundamental challenge for reinforcement learning. We identify a\ncritical failure mode unique to this setting: the exploration-exploitation\ncascade failure. This cascade begins with early-stage policy premature\nconvergence, where sparse feedback causes agents to commit to flawed,\nlow-entropy strategies. Subsequently, agents enter late-stage policy collapse,\nwhere conventional entropy regularization becomes counterproductive, promoting\nchaotic exploration that destabilizes training. We propose Entropy-regularized\nPolicy Optimization (EPO), a general framework that breaks this failure cycle\nthrough three synergistic mechanisms: (1) adopting entropy regularization in\nmulti-turn settings to enhance exploration, (2) an entropy smoothing\nregularizer that bounds policy entropy within historical averages to prevent\nabrupt fluctuations, and (3) adaptive phase-based weighting that balances\nexploration and exploitation across training. Our analysis justifies that EPO\nguarantees monotonically decreasing entropy variance while maintaining\nconvergence. EPO achieves up to 152% performance improvement on ScienceWorld\nand up to 19.8% on ALFWorld. Our work demonstrates that multi-turn\nsparse-reward settings require fundamentally different entropy control than\ntraditional RL, with broad implications for LLM agent training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22576.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64dfcc62e8b6f3f3baa950e0",
            "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
            "fullname": "Zhenting Wang",
            "name": "ztwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.22186",
            "authors": [
                {
                    "_id": "68d9ebf80177a6054b013a58",
                    "name": "Junbo Niu",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a59",
                    "user": {
                        "_id": "6625ef13605f46d05c1d0031",
                        "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
                        "isPro": false,
                        "fullname": "Zheng Liu",
                        "user": "starriver030515",
                        "type": "user"
                    },
                    "name": "Zheng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T11:26:48.737Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a5a",
                    "name": "Zhuangcheng Gu",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a5b",
                    "user": {
                        "_id": "63ae9ff5557befe297a76f90",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672388558183-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Bin Wang",
                        "user": "wanderkid",
                        "type": "user"
                    },
                    "name": "Bin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:58:46.158Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a5c",
                    "user": {
                        "_id": "66753c556f2ac48ee625d7d1",
                        "avatarUrl": "/avatars/8f7c252675fd8a096794d12971903722.svg",
                        "isPro": false,
                        "fullname": "Linke Ouyang",
                        "user": "ouyanglinke",
                        "type": "user"
                    },
                    "name": "Linke Ouyang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T11:26:51.347Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a5d",
                    "name": "Zhiyuan Zhao",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a5e",
                    "name": "Tao Chu",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a5f",
                    "user": {
                        "_id": "65b8c55130839a0db8cdc496",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b8c55130839a0db8cdc496/REpRsa1ts84yjk1GyyfnT.png",
                        "isPro": false,
                        "fullname": "Tianyao He",
                        "user": "hotelll",
                        "type": "user"
                    },
                    "name": "Tianyao He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T11:27:08.370Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a60",
                    "name": "Fan Wu",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a61",
                    "name": "Qintong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a62",
                    "name": "Zhenjiang Jin",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a63",
                    "name": "Guang Liang",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a64",
                    "name": "Rui Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a65",
                    "name": "Wenzheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a66",
                    "name": "Yuan Qu",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a67",
                    "name": "Zhifei Ren",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a68",
                    "user": {
                        "_id": "68231f20feab9d28e37f17e1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68231f20feab9d28e37f17e1/hikWkWYg1zmKfU3kLSKeu.jpeg",
                        "isPro": false,
                        "fullname": "Yuefeng Sun",
                        "user": "SunYuefeng",
                        "type": "user"
                    },
                    "name": "Yuefeng Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T11:27:04.835Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a69",
                    "name": "Yuanhong Zheng",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a6a",
                    "name": "Dongsheng Ma",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a6b",
                    "name": "Zirui Tang",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a6c",
                    "name": "Boyu Niu",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a6d",
                    "name": "Ziyang Miao",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a6e",
                    "name": "Hejun Dong",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a6f",
                    "name": "Siyi Qian",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a70",
                    "name": "Junyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a71",
                    "name": "Jingzhou Chen",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a72",
                    "name": "Fangdong Wang",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a73",
                    "name": "Xiaomeng Zhao",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a74",
                    "name": "Liqun Wei",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a75",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a76",
                    "name": "Shasha Wang",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a77",
                    "name": "Ruiliang Xu",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a78",
                    "name": "Yuanyuan Cao",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a79",
                    "name": "Lu Chen",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a7a",
                    "name": "Qianqian Wu",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a7b",
                    "name": "Huaiyu Gu",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a7c",
                    "name": "Lindong Lu",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a7d",
                    "name": "Keming Wang",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a7e",
                    "name": "Dechen Lin",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a7f",
                    "name": "Guanlin Shen",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a80",
                    "name": "Xuanhe Zhou",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a81",
                    "name": "Linfeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a82",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/kajwuVzd4pDucSPlwghxo.png",
                        "isPro": false,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:58:52.431Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a83",
                    "name": "Xiaoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a84",
                    "user": {
                        "_id": "64b4eec4faa3181a5eab9c46",
                        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
                        "isPro": true,
                        "fullname": "Jiaqi Wang",
                        "user": "myownskyW7",
                        "type": "user"
                    },
                    "name": "Jiaqi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:58:49.303Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a85",
                    "name": "Bo Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a86",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a87",
                    "user": {
                        "_id": "64c9beb2904317f42de06dd8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c9beb2904317f42de06dd8/he3rxfyzfwEd1vLuK6_o2.jpeg",
                        "isPro": false,
                        "fullname": "Pei Chu",
                        "user": "chupei",
                        "type": "user"
                    },
                    "name": "Pei Chu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:58:41.995Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a88",
                    "name": "Weijia Li",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a89",
                    "name": "Jiang Wu",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a8a",
                    "name": "Lijun Wu",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a8b",
                    "name": "Zhenxiang Li",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a8c",
                    "name": "Guangyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a8d",
                    "name": "Zhongying Tu",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a8e",
                    "name": "Chao Xu",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a8f",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a90",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a91",
                    "name": "Bowen Zhou",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a92",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a93",
                    "name": "Wentao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d9ebf80177a6054b013a94",
                    "name": "Conghui He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T10:45:48.000Z",
            "submittedOnDailyAt": "2025-09-29T00:46:33.156Z",
            "title": "MinerU2.5: A Decoupled Vision-Language Model for Efficient\n  High-Resolution Document Parsing",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language\nmodel that achieves state-of-the-art recognition accuracy while maintaining\nexceptional computational efficiency. Our approach employs a coarse-to-fine,\ntwo-stage parsing strategy that decouples global layout analysis from local\ncontent recognition. In the first stage, the model performs efficient layout\nanalysis on downsampled images to identify structural elements, circumventing\nthe computational overhead of processing high-resolution inputs. In the second\nstage, guided by the global layout, it performs targeted content recognition on\nnative-resolution crops extracted from the original image, preserving\nfine-grained details in dense text, complex formulas, and tables. To support\nthis strategy, we developed a comprehensive data engine that generates diverse,\nlarge-scale training corpora for both pretraining and fine-tuning. Ultimately,\nMinerU2.5 demonstrates strong document parsing ability, achieving\nstate-of-the-art performance on multiple benchmarks, surpassing both\ngeneral-purpose and domain-specific models across various recognition tasks,\nwhile maintaining significantly lower computational overhead.",
            "upvotes": 77,
            "discussionId": "68d9ebf80177a6054b013a95",
            "projectPage": "https://opendatalab.github.io/MinerU/",
            "githubRepo": "https://github.com/opendatalab/MinerU",
            "ai_summary": "MinerU2.5, a 1.2B-parameter document parsing vision-language model, achieves state-of-the-art recognition accuracy with computational efficiency through a coarse-to-fine parsing strategy.",
            "ai_keywords": [
                "document parsing",
                "vision-language model",
                "coarse-to-fine",
                "two-stage parsing",
                "layout analysis",
                "content recognition",
                "downsampled images",
                "native-resolution crops",
                "data engine",
                "pretraining",
                "fine-tuning",
                "state-of-the-art performance",
                "computational overhead"
            ],
            "githubStars": 44962
        },
        "publishedAt": "2025-09-26T06:45:48.000Z",
        "title": "MinerU2.5: A Decoupled Vision-Language Model for Efficient\n  High-Resolution Document Parsing",
        "summary": "We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language\nmodel that achieves state-of-the-art recognition accuracy while maintaining\nexceptional computational efficiency. Our approach employs a coarse-to-fine,\ntwo-stage parsing strategy that decouples global layout analysis from local\ncontent recognition. In the first stage, the model performs efficient layout\nanalysis on downsampled images to identify structural elements, circumventing\nthe computational overhead of processing high-resolution inputs. In the second\nstage, guided by the global layout, it performs targeted content recognition on\nnative-resolution crops extracted from the original image, preserving\nfine-grained details in dense text, complex formulas, and tables. To support\nthis strategy, we developed a comprehensive data engine that generates diverse,\nlarge-scale training corpora for both pretraining and fine-tuning. Ultimately,\nMinerU2.5 demonstrates strong document parsing ability, achieving\nstate-of-the-art performance on multiple benchmarks, surpassing both\ngeneral-purpose and domain-specific models across various recognition tasks,\nwhile maintaining significantly lower computational overhead.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22186.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 112
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.21679",
            "authors": [
                {
                    "_id": "68d9f9bd0177a6054b013b5f",
                    "user": {
                        "_id": "666d506fc0f3d5afc24dd5ca",
                        "avatarUrl": "/avatars/eeb98947415d08a26815fd139c76a071.svg",
                        "isPro": false,
                        "fullname": "Hyun Ryu",
                        "user": "hyun1905",
                        "type": "user"
                    },
                    "name": "Hyun Ryu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:57:59.016Z",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b60",
                    "user": {
                        "_id": "62845957b410bd779033759c",
                        "avatarUrl": "/avatars/4feef73c06f2f7de6abf7a4789ac13f9.svg",
                        "isPro": false,
                        "fullname": "Doohyuk Jang",
                        "user": "jadohu",
                        "type": "user"
                    },
                    "name": "Doohyuk Jang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T11:26:32.034Z",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b61",
                    "name": "Hyemin S. Lee",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b62",
                    "name": "Joonhyun Jeong",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b63",
                    "name": "Gyeongman Kim",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b64",
                    "name": "Donghyeon Cho",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b65",
                    "name": "Gyouk Chu",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b66",
                    "name": "Minyeong Hwang",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b67",
                    "name": "Hyeongwon Jang",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b68",
                    "name": "Changhun Kim",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b69",
                    "name": "Haechan Kim",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b6a",
                    "name": "Jina Kim",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b6b",
                    "user": {
                        "_id": "6666b61eaf95872a03a0a673",
                        "avatarUrl": "/avatars/fc0c144cf6307357d45d7ca2d6ba8d2f.svg",
                        "isPro": false,
                        "fullname": "Joowon",
                        "user": "kjwispro",
                        "type": "user"
                    },
                    "name": "Joowon Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:57:56.550Z",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b6c",
                    "name": "Yoonjeon Kim",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b6d",
                    "name": "Kwanhyung Lee",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b6e",
                    "name": "Chanjae Park",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b6f",
                    "name": "Heecheol Yun",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b70",
                    "name": "Gregor Betz",
                    "hidden": false
                },
                {
                    "_id": "68d9f9bd0177a6054b013b71",
                    "name": "Eunho Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-25T22:55:05.000Z",
            "submittedOnDailyAt": "2025-09-29T01:55:27.888Z",
            "title": "ReviewScore: Misinformed Peer Review Detection with Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "666d506fc0f3d5afc24dd5ca",
                "avatarUrl": "/avatars/eeb98947415d08a26815fd139c76a071.svg",
                "isPro": false,
                "fullname": "Hyun Ryu",
                "user": "hyun1905",
                "type": "user"
            },
            "summary": "Peer review serves as a backbone of academic research, but in most AI\nconferences, the review quality is degrading as the number of submissions\nexplodes. To reliably detect low-quality reviews, we define misinformed review\npoints as either \"weaknesses\" in a review that contain incorrect premises, or\n\"questions\" in a review that can be already answered by the paper. We verify\nthat 15.2% of weaknesses and 26.4% of questions are misinformed and introduce\nReviewScore indicating if a review point is misinformed. To evaluate the\nfactuality of each premise of weaknesses, we propose an automated engine that\nreconstructs every explicit and implicit premise from a weakness. We build a\nhuman expert-annotated ReviewScore dataset to check the ability of LLMs to\nautomate ReviewScore evaluation. Then, we measure human-model agreements on\nReviewScore using eight current state-of-the-art LLMs and verify moderate\nagreements. We also prove that evaluating premise-level factuality shows\nsignificantly higher agreements than evaluating weakness-level factuality. A\nthorough disagreement analysis further supports a potential of fully automated\nReviewScore evaluation.",
            "upvotes": 54,
            "discussionId": "68d9f9be0177a6054b013b72",
            "ai_summary": "An automated engine evaluates the factuality of review points in AI conference papers, demonstrating moderate agreement with human experts and higher accuracy at the premise level.",
            "ai_keywords": [
                "misinformed review points",
                "weaknesses",
                "questions",
                "ReviewScore",
                "automated engine",
                "premise-level factuality",
                "weakness-level factuality",
                "human expert-annotated dataset",
                "state-of-the-art LLMs",
                "human-model agreements"
            ]
        },
        "publishedAt": "2025-09-25T18:55:05.000Z",
        "title": "ReviewScore: Misinformed Peer Review Detection with Large Language\n  Models",
        "summary": "Peer review serves as a backbone of academic research, but in most AI\nconferences, the review quality is degrading as the number of submissions\nexplodes. To reliably detect low-quality reviews, we define misinformed review\npoints as either \"weaknesses\" in a review that contain incorrect premises, or\n\"questions\" in a review that can be already answered by the paper. We verify\nthat 15.2% of weaknesses and 26.4% of questions are misinformed and introduce\nReviewScore indicating if a review point is misinformed. To evaluate the\nfactuality of each premise of weaknesses, we propose an automated engine that\nreconstructs every explicit and implicit premise from a weakness. We build a\nhuman expert-annotated ReviewScore dataset to check the ability of LLMs to\nautomate ReviewScore evaluation. Then, we measure human-model agreements on\nReviewScore using eight current state-of-the-art LLMs and verify moderate\nagreements. We also prove that evaluating premise-level factuality shows\nsignificantly higher agreements than evaluating weakness-level factuality. A\nthorough disagreement analysis further supports a potential of fully automated\nReviewScore evaluation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21679.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "666d506fc0f3d5afc24dd5ca",
            "avatarUrl": "/avatars/eeb98947415d08a26815fd139c76a071.svg",
            "fullname": "Hyun Ryu",
            "name": "hyun1905",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "submitterOrganization": {
            "_id": "6475760c33192631bad2bb38",
            "name": "kaist-ai",
            "fullname": "KAIST AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.22637",
            "authors": [
                {
                    "_id": "68d9ef7a0177a6054b013ad7",
                    "user": {
                        "_id": "66129c7b50350afe76757262",
                        "avatarUrl": "/avatars/a2f4fac076b9d658a0d904ed54960f6f.svg",
                        "isPro": false,
                        "fullname": "Xiangxin Zhou",
                        "user": "zhouxiangxin",
                        "type": "user"
                    },
                    "name": "Xiangxin Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:58:24.272Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ef7a0177a6054b013ad8",
                    "name": "Zichen Liu",
                    "hidden": false
                },
                {
                    "_id": "68d9ef7a0177a6054b013ad9",
                    "name": "Haonan Wang",
                    "hidden": false
                },
                {
                    "_id": "68d9ef7a0177a6054b013ada",
                    "name": "Chao Du",
                    "hidden": false
                },
                {
                    "_id": "68d9ef7a0177a6054b013adb",
                    "name": "Min Lin",
                    "hidden": false
                },
                {
                    "_id": "68d9ef7a0177a6054b013adc",
                    "user": {
                        "_id": "64c07b488e2612254361153b",
                        "avatarUrl": "/avatars/ade0f783cc4c2d3e73f402637f595471.svg",
                        "isPro": false,
                        "fullname": "chongxuan li",
                        "user": "zhenxuan00",
                        "type": "user"
                    },
                    "name": "Chongxuan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-29T13:10:55.814Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ef7a0177a6054b013add",
                    "name": "Liang Wang",
                    "hidden": false
                },
                {
                    "_id": "68d9ef7a0177a6054b013ade",
                    "name": "Tianyu Pang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T17:58:10.000Z",
            "submittedOnDailyAt": "2025-09-29T01:02:16.493Z",
            "title": "Variational Reasoning for Language Models",
            "submittedOnDailyBy": {
                "_id": "63d91b6d255ef6add20e1b38",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
                "isPro": false,
                "fullname": "Tianyu Pang",
                "user": "P2333",
                "type": "user"
            },
            "summary": "We introduce a variational reasoning framework for language models that\ntreats thinking traces as latent variables and optimizes them through\nvariational inference. Starting from the evidence lower bound (ELBO), we extend\nit to a multi-trace objective for tighter bounds and propose a forward-KL\nformulation that stabilizes the training of the variational posterior. We\nfurther show that rejection sampling finetuning and binary-reward RL, including\nGRPO, can be interpreted as local forward-KL objectives, where an implicit\nweighting by model accuracy naturally arises from the derivation and reveals a\npreviously unnoticed bias toward easier questions. We empirically validate our\nmethod on the Qwen 2.5 and Qwen 3 model families across a wide range of\nreasoning tasks. Overall, our work provides a principled probabilistic\nperspective that unifies variational inference with RL-style methods and yields\nstable objectives for improving the reasoning ability of language models. Our\ncode is available at https://github.com/sail-sg/variational-reasoning.",
            "upvotes": 51,
            "discussionId": "68d9ef7b0177a6054b013adf",
            "githubRepo": "https://github.com/sail-sg/variational-reasoning",
            "ai_summary": "A variational reasoning framework treats thinking traces as latent variables, optimizing them through variational inference to improve language model reasoning.",
            "ai_keywords": [
                "variational reasoning framework",
                "latent variables",
                "variational inference",
                "evidence lower bound (ELBO)",
                "multi-trace objective",
                "forward-KL formulation",
                "rejection sampling finetuning",
                "binary-reward RL",
                "GRPO",
                "implicit weighting",
                "model accuracy",
                "probabilistic perspective",
                "RL-style methods"
            ],
            "githubStars": 25
        },
        "publishedAt": "2025-09-26T13:58:10.000Z",
        "title": "Variational Reasoning for Language Models",
        "summary": "We introduce a variational reasoning framework for language models that\ntreats thinking traces as latent variables and optimizes them through\nvariational inference. Starting from the evidence lower bound (ELBO), we extend\nit to a multi-trace objective for tighter bounds and propose a forward-KL\nformulation that stabilizes the training of the variational posterior. We\nfurther show that rejection sampling finetuning and binary-reward RL, including\nGRPO, can be interpreted as local forward-KL objectives, where an implicit\nweighting by model accuracy naturally arises from the derivation and reveals a\npreviously unnoticed bias toward easier questions. We empirically validate our\nmethod on the Qwen 2.5 and Qwen 3 model families across a wide range of\nreasoning tasks. Overall, our work provides a principled probabilistic\nperspective that unifies variational inference with RL-style methods and yields\nstable objectives for improving the reasoning ability of language models. Our\ncode is available at https://github.com/sail-sg/variational-reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22637.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63d91b6d255ef6add20e1b38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
            "fullname": "Tianyu Pang",
            "name": "P2333",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "submitterOrganization": {
            "_id": "61f4e841c771e23a1abb61ff",
            "name": "sail",
            "fullname": "Sea AI Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643440185801-5df833bdda6d0311fd3d5403.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22638",
            "authors": [
                {
                    "_id": "68d9ef140177a6054b013acd",
                    "user": {
                        "_id": "66f2aa5dc13823293e03b9bb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/7ggb86W70FZy1iPE4IVAY.png",
                        "isPro": false,
                        "fullname": "RenJie",
                        "user": "LuoRenjie",
                        "type": "user"
                    },
                    "name": "Renjie Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-29T13:14:39.469Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ef140177a6054b013ace",
                    "name": "Zichen Liu",
                    "hidden": false
                },
                {
                    "_id": "68d9ef140177a6054b013acf",
                    "name": "Xiangyan Liu",
                    "hidden": false
                },
                {
                    "_id": "68d9ef140177a6054b013ad0",
                    "name": "Chao Du",
                    "hidden": false
                },
                {
                    "_id": "68d9ef140177a6054b013ad1",
                    "name": "Min Lin",
                    "hidden": false
                },
                {
                    "_id": "68d9ef140177a6054b013ad2",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-29T13:14:20.560Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ef140177a6054b013ad3",
                    "name": "Wei Lu",
                    "hidden": false
                },
                {
                    "_id": "68d9ef140177a6054b013ad4",
                    "name": "Tianyu Pang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T17:58:27.000Z",
            "submittedOnDailyAt": "2025-09-29T01:01:00.665Z",
            "title": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards",
            "submittedOnDailyBy": {
                "_id": "63d91b6d255ef6add20e1b38",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
                "isPro": false,
                "fullname": "Tianyu Pang",
                "user": "P2333",
                "type": "user"
            },
            "summary": "LLMs are often trained with RL from human or AI feedback, yet such methods\ntypically compress nuanced feedback into scalar rewards, discarding much of\ntheir richness and inducing scale imbalance. We propose treating verbal\nfeedback as a conditioning signal. Inspired by language priors in text-to-image\ngeneration, which enable novel outputs from unseen prompts, we introduce the\nfeedback-conditional policy (FCP). FCP learns directly from response-feedback\npairs, approximating the feedback-conditional posterior through maximum\nlikelihood training on offline data. We further develop an online bootstrapping\nstage where the policy generates under positive conditions and receives fresh\nfeedback to refine itself. This reframes feedback-driven learning as\nconditional generation rather than reward optimization, offering a more\nexpressive way for LLMs to directly learn from verbal feedback. Our code is\navailable at https://github.com/sail-sg/feedback-conditional-policy.",
            "upvotes": 48,
            "discussionId": "68d9ef140177a6054b013ad5",
            "ai_summary": "Feedback-conditional policy (FCP) enables LLMs to learn from verbal feedback by treating it as a conditioning signal, improving expressiveness over scalar rewards.",
            "ai_keywords": [
                "LLMs",
                "RL",
                "human feedback",
                "AI feedback",
                "scalar rewards",
                "feedback-conditional policy",
                "language priors",
                "text-to-image generation",
                "response-feedback pairs",
                "maximum likelihood training",
                "offline data",
                "online bootstrapping",
                "conditional generation",
                "reward optimization"
            ]
        },
        "publishedAt": "2025-09-26T13:58:27.000Z",
        "title": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards",
        "summary": "LLMs are often trained with RL from human or AI feedback, yet such methods\ntypically compress nuanced feedback into scalar rewards, discarding much of\ntheir richness and inducing scale imbalance. We propose treating verbal\nfeedback as a conditioning signal. Inspired by language priors in text-to-image\ngeneration, which enable novel outputs from unseen prompts, we introduce the\nfeedback-conditional policy (FCP). FCP learns directly from response-feedback\npairs, approximating the feedback-conditional posterior through maximum\nlikelihood training on offline data. We further develop an online bootstrapping\nstage where the policy generates under positive conditions and receives fresh\nfeedback to refine itself. This reframes feedback-driven learning as\nconditional generation rather than reward optimization, offering a more\nexpressive way for LLMs to directly learn from verbal feedback. Our code is\navailable at https://github.com/sail-sg/feedback-conditional-policy.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22638.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63d91b6d255ef6add20e1b38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
            "fullname": "Tianyu Pang",
            "name": "P2333",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "submitterOrganization": {
            "_id": "61f4e841c771e23a1abb61ff",
            "name": "sail",
            "fullname": "Sea AI Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643440185801-5df833bdda6d0311fd3d5403.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22647",
            "authors": [
                {
                    "_id": "68d9f33e0177a6054b013afc",
                    "name": "Long Xing",
                    "hidden": false
                },
                {
                    "_id": "68d9f33e0177a6054b013afd",
                    "name": "Xiaoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "68d9f33e0177a6054b013afe",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/kajwuVzd4pDucSPlwghxo.png",
                        "isPro": false,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:58:18.935Z",
                    "hidden": false
                },
                {
                    "_id": "68d9f33e0177a6054b013aff",
                    "name": "Yuhang Cao",
                    "hidden": false
                },
                {
                    "_id": "68d9f33e0177a6054b013b00",
                    "name": "Jianze Liang",
                    "hidden": false
                },
                {
                    "_id": "68d9f33e0177a6054b013b01",
                    "name": "Qidong Huang",
                    "hidden": false
                },
                {
                    "_id": "68d9f33e0177a6054b013b02",
                    "user": {
                        "_id": "64b4eec4faa3181a5eab9c46",
                        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
                        "isPro": true,
                        "fullname": "Jiaqi Wang",
                        "user": "myownskyW7",
                        "type": "user"
                    },
                    "name": "Jiaqi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:58:16.337Z",
                    "hidden": false
                },
                {
                    "_id": "68d9f33e0177a6054b013b03",
                    "name": "Feng Wu",
                    "hidden": false
                },
                {
                    "_id": "68d9f33e0177a6054b013b04",
                    "name": "Dahua Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T17:59:55.000Z",
            "submittedOnDailyAt": "2025-09-29T01:19:15.716Z",
            "title": "CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement\n  Learning",
            "submittedOnDailyBy": {
                "_id": "62eb70462f0f5e54df42f778",
                "avatarUrl": "/avatars/456049dba67638d3cdb330cdf383f272.svg",
                "isPro": false,
                "fullname": "Xilin Wei",
                "user": "Wiselnn",
                "type": "user"
            },
            "summary": "Image captioning is a fundamental task that bridges the visual and linguistic\ndomains, playing a critical role in pre-training Large Vision-Language Models\n(LVLMs). Current state-of-the-art captioning models are typically trained with\nSupervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable\ndata annotated by humans or proprietary models. This approach often leads to\nmodels that memorize specific ground-truth answers, limiting their generality\nand ability to generate diverse, creative descriptions. To overcome the\nlimitation of SFT, we propose applying the Reinforcement Learning with\nVerifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.\nA primary challenge, however, is designing an objective reward function for the\ninherently subjective nature of what constitutes a \"good\" caption. We introduce\nCaptioning Reinforcement Learning (CapRL), a novel training framework that\nredefines caption quality through its utility: a high-quality caption should\nenable a non-visual language model to accurately answer questions about the\ncorresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM\ngenerates a caption, and the objective reward is derived from the accuracy of a\nseparate, vision-free LLM answering Multiple-Choice Questions based solely on\nthat caption. As the first study to apply RLVR to the subjective image\ncaptioning task, we demonstrate that CapRL significantly enhances multiple\nsettings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B\nresults in substantial gains across 12 benchmarks. Moreover, within the Prism\nFramework for caption quality evaluation, CapRL achieves performance comparable\nto Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%.\nCode is available here: https://github.com/InternLM/CapRL.",
            "upvotes": 28,
            "discussionId": "68d9f33e0177a6054b013b05",
            "githubRepo": "https://github.com/InternLM/CapRL",
            "ai_summary": "CapRL, a novel reinforcement learning framework, enhances image captioning by using a vision-free language model to evaluate caption quality through multiple-choice questions, leading to improved performance across benchmarks.",
            "ai_keywords": [
                "Large Vision-Language Models",
                "Supervised Fine-Tuning",
                "Reinforcement Learning with Verifiable Rewards",
                "Captioning Reinforcement Learning",
                "decoupled two-stage pipeline",
                "vision-free language model",
                "Multiple-Choice Questions",
                "Prism Framework"
            ],
            "githubStars": 46
        },
        "publishedAt": "2025-09-26T13:59:55.000Z",
        "title": "CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement\n  Learning",
        "summary": "Image captioning is a fundamental task that bridges the visual and linguistic\ndomains, playing a critical role in pre-training Large Vision-Language Models\n(LVLMs). Current state-of-the-art captioning models are typically trained with\nSupervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable\ndata annotated by humans or proprietary models. This approach often leads to\nmodels that memorize specific ground-truth answers, limiting their generality\nand ability to generate diverse, creative descriptions. To overcome the\nlimitation of SFT, we propose applying the Reinforcement Learning with\nVerifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.\nA primary challenge, however, is designing an objective reward function for the\ninherently subjective nature of what constitutes a \"good\" caption. We introduce\nCaptioning Reinforcement Learning (CapRL), a novel training framework that\nredefines caption quality through its utility: a high-quality caption should\nenable a non-visual language model to accurately answer questions about the\ncorresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM\ngenerates a caption, and the objective reward is derived from the accuracy of a\nseparate, vision-free LLM answering Multiple-Choice Questions based solely on\nthat caption. As the first study to apply RLVR to the subjective image\ncaptioning task, we demonstrate that CapRL significantly enhances multiple\nsettings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B\nresults in substantial gains across 12 benchmarks. Moreover, within the Prism\nFramework for caption quality evaluation, CapRL achieves performance comparable\nto Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%.\nCode is available here: https://github.com/InternLM/CapRL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22647.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62eb70462f0f5e54df42f778",
            "avatarUrl": "/avatars/456049dba67638d3cdb330cdf383f272.svg",
            "fullname": "Xilin Wei",
            "name": "Wiselnn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "submitterOrganization": {
            "_id": "64a2d5fa81252883206f24c9",
            "name": "internlm",
            "fullname": "Intern Large Models",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22281",
            "authors": [
                {
                    "_id": "68da105b0177a6054b013ba7",
                    "user": {
                        "_id": "64edb581067fbb625f893628",
                        "avatarUrl": "/avatars/d59893d6f1f752bb73255bd78b325fe9.svg",
                        "isPro": false,
                        "fullname": "hao",
                        "user": "wuzhi-hao",
                        "type": "user"
                    },
                    "name": "Jinkun Hao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:52:19.633Z",
                    "hidden": false
                },
                {
                    "_id": "68da105b0177a6054b013ba8",
                    "user": {
                        "_id": "68da238ef2f999edd0fed048",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/57HzxM8PgI18NOXQT7OIS.jpeg",
                        "isPro": false,
                        "fullname": "Naifu Liang",
                        "user": "nfliang",
                        "type": "user"
                    },
                    "name": "Naifu Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T11:26:29.306Z",
                    "hidden": false
                },
                {
                    "_id": "68da105b0177a6054b013ba9",
                    "name": "Zhen Luo",
                    "hidden": false
                },
                {
                    "_id": "68da105b0177a6054b013baa",
                    "name": "Xudong Xu",
                    "hidden": false
                },
                {
                    "_id": "68da105b0177a6054b013bab",
                    "name": "Weipeng Zhong",
                    "hidden": false
                },
                {
                    "_id": "68da105b0177a6054b013bac",
                    "name": "Ran Yi",
                    "hidden": false
                },
                {
                    "_id": "68da105b0177a6054b013bad",
                    "name": "Yichen Jin",
                    "hidden": false
                },
                {
                    "_id": "68da105b0177a6054b013bae",
                    "name": "Zhaoyang Lyu",
                    "hidden": false
                },
                {
                    "_id": "68da105b0177a6054b013baf",
                    "name": "Feng Zheng",
                    "hidden": false
                },
                {
                    "_id": "68da105b0177a6054b013bb0",
                    "name": "Lizhuang Ma",
                    "hidden": false
                },
                {
                    "_id": "68da105b0177a6054b013bb1",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T12:46:00.000Z",
            "submittedOnDailyAt": "2025-09-29T04:01:06.731Z",
            "title": "MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "65220fedc709aaca9aa63061",
                "avatarUrl": "/avatars/36e56199fa8c98ff3430636567a0ed62.svg",
                "isPro": false,
                "fullname": "Lixing Xiao",
                "user": "lxxiao",
                "type": "user"
            },
            "summary": "The ability of robots to interpret human instructions and execute\nmanipulation tasks necessitates the availability of task-relevant tabletop\nscenes for training. However, traditional methods for creating these scenes\nrely on time-consuming manual layout design or purely randomized layouts, which\nare limited in terms of plausibility or alignment with the tasks. In this\npaper, we formulate a novel task, namely task-oriented tabletop scene\ngeneration, which poses significant challenges due to the substantial gap\nbetween high-level task instructions and the tabletop scenes. To support\nresearch on such a challenging task, we introduce MesaTask-10K, a large-scale\ndataset comprising approximately 10,700 synthetic tabletop scenes with manually\ncrafted layouts that ensure realistic layouts and intricate inter-object\nrelations. To bridge the gap between tasks and scenes, we propose a Spatial\nReasoning Chain that decomposes the generation process into object inference,\nspatial interrelation reasoning, and scene graph construction for the final 3D\nlayout. We present MesaTask, an LLM-based framework that utilizes this\nreasoning chain and is further enhanced with DPO algorithms to generate\nphysically plausible tabletop scenes that align well with given task\ndescriptions. Exhaustive experiments demonstrate the superior performance of\nMesaTask compared to baselines in generating task-conforming tabletop scenes\nwith realistic layouts. Project page is at https://mesatask.github.io/",
            "upvotes": 27,
            "discussionId": "68da105b0177a6054b013bb2",
            "projectPage": "https://mesatask.github.io/",
            "githubRepo": "https://github.com/InternRobotics/MesaTask",
            "ai_summary": "MesaTask, an LLM-based framework with a Spatial Reasoning Chain, generates realistic tabletop scenes aligned with task descriptions using DPO algorithms.",
            "ai_keywords": [
                "LLM-based framework",
                "Spatial Reasoning Chain",
                "object inference",
                "spatial interrelation reasoning",
                "scene graph construction",
                "DPO algorithms"
            ],
            "githubStars": 24
        },
        "publishedAt": "2025-09-26T08:46:00.000Z",
        "title": "MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial\n  Reasoning",
        "summary": "The ability of robots to interpret human instructions and execute\nmanipulation tasks necessitates the availability of task-relevant tabletop\nscenes for training. However, traditional methods for creating these scenes\nrely on time-consuming manual layout design or purely randomized layouts, which\nare limited in terms of plausibility or alignment with the tasks. In this\npaper, we formulate a novel task, namely task-oriented tabletop scene\ngeneration, which poses significant challenges due to the substantial gap\nbetween high-level task instructions and the tabletop scenes. To support\nresearch on such a challenging task, we introduce MesaTask-10K, a large-scale\ndataset comprising approximately 10,700 synthetic tabletop scenes with manually\ncrafted layouts that ensure realistic layouts and intricate inter-object\nrelations. To bridge the gap between tasks and scenes, we propose a Spatial\nReasoning Chain that decomposes the generation process into object inference,\nspatial interrelation reasoning, and scene graph construction for the final 3D\nlayout. We present MesaTask, an LLM-based framework that utilizes this\nreasoning chain and is further enhanced with DPO algorithms to generate\nphysically plausible tabletop scenes that align well with given task\ndescriptions. Exhaustive experiments demonstrate the superior performance of\nMesaTask compared to baselines in generating task-conforming tabletop scenes\nwith realistic layouts. Project page is at https://mesatask.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22281.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65220fedc709aaca9aa63061",
            "avatarUrl": "/avatars/36e56199fa8c98ff3430636567a0ed62.svg",
            "fullname": "Lixing Xiao",
            "name": "lxxiao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.21880",
            "authors": [
                {
                    "_id": "68da20030177a6054b013bd4",
                    "user": {
                        "_id": "670a457314d30c4f56eb76dc",
                        "avatarUrl": "/avatars/e4644b1755db9a546afa1ac9fd11895d.svg",
                        "isPro": false,
                        "fullname": "Thanh-Long V. Le",
                        "user": "bltnynk",
                        "type": "user"
                    },
                    "name": "Thanh-Long V. Le",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:59:41.349Z",
                    "hidden": false
                },
                {
                    "_id": "68da20030177a6054b013bd5",
                    "name": "Myeongho Jeon",
                    "hidden": false
                },
                {
                    "_id": "68da20030177a6054b013bd6",
                    "name": "Kim Vu",
                    "hidden": false
                },
                {
                    "_id": "68da20030177a6054b013bd7",
                    "name": "Viet Lai",
                    "hidden": false
                },
                {
                    "_id": "68da20030177a6054b013bd8",
                    "name": "Eunho Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T05:03:54.000Z",
            "submittedOnDailyAt": "2025-09-29T04:32:20.231Z",
            "title": "No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM\n  Reinforcement Learning via Entropy-Guided Advantage Shaping",
            "submittedOnDailyBy": {
                "_id": "670a457314d30c4f56eb76dc",
                "avatarUrl": "/avatars/e4644b1755db9a546afa1ac9fd11895d.svg",
                "isPro": false,
                "fullname": "Thanh-Long V. Le",
                "user": "bltnynk",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework\nfor improving the reasoning abilities of Large Language Models (LLMs). However,\ncurrent methods such as GRPO rely only on problems where the model responses to\nthe same input differ in correctness, while ignoring those where all responses\nreceive the same reward - so-called zero-variance prompts. In this work, we\nargue that such prompts are not useless but can, in fact, provide meaningful\nfeedback for policy optimization. To this end, we introduce RL with\nZero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals\nfrom zero-variance prompts. RL-ZVP directly rewards correctness and penalizes\nerrors even without contrasting responses, modulating feedback with token-level\ncharacteristics to preserve informative, nuanced signals. Across six math\nreasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61\npoints in accuracy and 7.77 points in pass rate over GRPO, while consistently\noutperforming other baselines that filter out zero-variance prompts. These\nresults highlight the untapped potential of learning from zero-variance prompts\nin RLVR.",
            "upvotes": 27,
            "discussionId": "68da20030177a6054b013bd9",
            "ai_summary": "RL-ZVP, a novel reinforcement learning algorithm, leverages zero-variance prompts to improve the accuracy and pass rate of Large Language Models in math reasoning tasks.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards",
                "Large Language Models",
                "GRPO",
                "zero-variance prompts",
                "policy optimization",
                "token-level characteristics",
                "math reasoning benchmarks"
            ]
        },
        "publishedAt": "2025-09-26T01:03:54.000Z",
        "title": "No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM\n  Reinforcement Learning via Entropy-Guided Advantage Shaping",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework\nfor improving the reasoning abilities of Large Language Models (LLMs). However,\ncurrent methods such as GRPO rely only on problems where the model responses to\nthe same input differ in correctness, while ignoring those where all responses\nreceive the same reward - so-called zero-variance prompts. In this work, we\nargue that such prompts are not useless but can, in fact, provide meaningful\nfeedback for policy optimization. To this end, we introduce RL with\nZero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals\nfrom zero-variance prompts. RL-ZVP directly rewards correctness and penalizes\nerrors even without contrasting responses, modulating feedback with token-level\ncharacteristics to preserve informative, nuanced signals. Across six math\nreasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61\npoints in accuracy and 7.77 points in pass rate over GRPO, while consistently\noutperforming other baselines that filter out zero-variance prompts. These\nresults highlight the untapped potential of learning from zero-variance prompts\nin RLVR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21880.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "670a457314d30c4f56eb76dc",
            "avatarUrl": "/avatars/e4644b1755db9a546afa1ac9fd11895d.svg",
            "fullname": "Thanh-Long V. Le",
            "name": "bltnynk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "submitterOrganization": {
            "_id": "6475760c33192631bad2bb38",
            "name": "kaist-ai",
            "fullname": "KAIST AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.19894",
            "authors": [
                {
                    "_id": "68d6885d8ccd91bdd39fff60",
                    "user": {
                        "_id": "66b291a2d4db4314da3f7664",
                        "avatarUrl": "/avatars/3944b62f0be9e1894603bb082c1efc7f.svg",
                        "isPro": false,
                        "fullname": "Xueliang Zhao",
                        "user": "xl-zhao",
                        "type": "user"
                    },
                    "name": "Xueliang Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T06:01:33.721Z",
                    "hidden": false
                },
                {
                    "_id": "68d6885d8ccd91bdd39fff61",
                    "name": "Wei Wu",
                    "hidden": false
                },
                {
                    "_id": "68d6885d8ccd91bdd39fff62",
                    "name": "Jian Guan",
                    "hidden": false
                },
                {
                    "_id": "68d6885d8ccd91bdd39fff63",
                    "name": "Zhuocheng Gong",
                    "hidden": false
                },
                {
                    "_id": "68d6885d8ccd91bdd39fff64",
                    "name": "Lingpeng Kong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-24T08:46:29.000Z",
            "submittedOnDailyAt": "2025-09-29T08:14:56.735Z",
            "title": "PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "66b291a2d4db4314da3f7664",
                "avatarUrl": "/avatars/3944b62f0be9e1894603bb082c1efc7f.svg",
                "isPro": false,
                "fullname": "Xueliang Zhao",
                "user": "xl-zhao",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are evolving from conversational systems into\nstrong reasoners for tasks such as Olympiad mathematics and competitive\nprogramming. While scaling parameters and test-time computation has driven\nprogress, a key bottleneck is the lack of high-quality training problems:\nhuman-curated datasets are costly and limited, while existing synthetic corpora\nare often too easy or narrow. PromptCoT 1.0 showed that injecting rationales\ninto prompt synthesis increases problem difficulty. Building on this, we\npresent PromptCoT 2.0, a scalable framework that replaces hand-crafted\nheuristics with an expectation-maximization (EM) loop, where rationales are\niteratively refined to guide prompt construction. This produces problems that\nare both harder and more diverse than prior corpora. The synthetic prompts\nsupport two post-training regimes: (1) Self-Play, where strong models improve\nautonomously via verifiable feedback without stronger teachers; and (2)\nSupervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled\ntraces. Extensive experiments demonstrate the effectiveness of this approach.\nIn self-play, applying PromptCoT 2.0 to Qwen3-30B-A3B-Thinking-2507 sets new\nstate-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 on AIME\n24/25 and HMMT 25, +6.1 and +5.0 on LiveCodeBench v5/v6, and +35 Elo on\nCodeforces. In SFT, training Qwen2.5-7B-Instruct solely on synthetic prompts\nboosts accuracy to 73.1 (AIME 24), 65.6 (AIME 25), and 53.4 (LiveCodeBench v5),\nsurpassing models trained on human or hybrid data. Analyses further confirm\nthat PromptCoT 2.0 yields fundamentally harder and distributionally distinct\nproblems. These results establish prompt synthesis as a new axis for scaling\nreasoning and position PromptCoT 2.0 as a scalable foundation for future\nopen-source models. The implementation is available at\nhttps://github.com/inclusionAI/PromptCoT.",
            "upvotes": 24,
            "discussionId": "68d6885d8ccd91bdd39fff65",
            "githubRepo": "https://github.com/inclusionAI/PromptCoT",
            "ai_summary": "PromptCoT 2.0 uses an EM loop to generate harder and more diverse synthetic prompts, improving reasoning capabilities in large language models through self-play and supervised fine-tuning.",
            "ai_keywords": [
                "LLMs",
                "reasoners",
                "Olympiad mathematics",
                "competitive programming",
                "prompt synthesis",
                "rationales",
                "expectation-maximization (EM) loop",
                "self-play",
                "supervised fine-tuning (SFT)",
                "Qwen3-30B-A3B-Thinking-2507",
                "Qwen2.5-7B-Instruct",
                "AIME",
                "HMMT",
                "LiveCodeBench",
                "Codeforces",
                "prompt synthesis",
                "open-source models"
            ],
            "githubStars": 89
        },
        "publishedAt": "2025-09-24T04:46:29.000Z",
        "title": "PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model\n  Reasoning",
        "summary": "Large language models (LLMs) are evolving from conversational systems into\nstrong reasoners for tasks such as Olympiad mathematics and competitive\nprogramming. While scaling parameters and test-time computation has driven\nprogress, a key bottleneck is the lack of high-quality training problems:\nhuman-curated datasets are costly and limited, while existing synthetic corpora\nare often too easy or narrow. PromptCoT 1.0 showed that injecting rationales\ninto prompt synthesis increases problem difficulty. Building on this, we\npresent PromptCoT 2.0, a scalable framework that replaces hand-crafted\nheuristics with an expectation-maximization (EM) loop, where rationales are\niteratively refined to guide prompt construction. This produces problems that\nare both harder and more diverse than prior corpora. The synthetic prompts\nsupport two post-training regimes: (1) Self-Play, where strong models improve\nautonomously via verifiable feedback without stronger teachers; and (2)\nSupervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled\ntraces. Extensive experiments demonstrate the effectiveness of this approach.\nIn self-play, applying PromptCoT 2.0 to Qwen3-30B-A3B-Thinking-2507 sets new\nstate-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 on AIME\n24/25 and HMMT 25, +6.1 and +5.0 on LiveCodeBench v5/v6, and +35 Elo on\nCodeforces. In SFT, training Qwen2.5-7B-Instruct solely on synthetic prompts\nboosts accuracy to 73.1 (AIME 24), 65.6 (AIME 25), and 53.4 (LiveCodeBench v5),\nsurpassing models trained on human or hybrid data. Analyses further confirm\nthat PromptCoT 2.0 yields fundamentally harder and distributionally distinct\nproblems. These results establish prompt synthesis as a new axis for scaling\nreasoning and position PromptCoT 2.0 as a scalable foundation for future\nopen-source models. The implementation is available at\nhttps://github.com/inclusionAI/PromptCoT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19894.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "66b291a2d4db4314da3f7664",
            "avatarUrl": "/avatars/3944b62f0be9e1894603bb082c1efc7f.svg",
            "fullname": "Xueliang Zhao",
            "name": "xl-zhao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.21766",
            "authors": [
                {
                    "_id": "68d9f1fa0177a6054b013ae1",
                    "name": "Haotian Luo",
                    "hidden": false
                },
                {
                    "_id": "68d9f1fa0177a6054b013ae2",
                    "name": "Huaisong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d9f1fa0177a6054b013ae3",
                    "name": "Xuelin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d9f1fa0177a6054b013ae4",
                    "name": "Haoyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68d9f1fa0177a6054b013ae5",
                    "name": "Zeyu Qin",
                    "hidden": false
                },
                {
                    "_id": "68d9f1fa0177a6054b013ae6",
                    "name": "Wenjie Lu",
                    "hidden": false
                },
                {
                    "_id": "68d9f1fa0177a6054b013ae7",
                    "name": "Guozheng Ma",
                    "hidden": false
                },
                {
                    "_id": "68d9f1fa0177a6054b013ae8",
                    "name": "Haiying He",
                    "hidden": false
                },
                {
                    "_id": "68d9f1fa0177a6054b013ae9",
                    "name": "Yingsha Xie",
                    "hidden": false
                },
                {
                    "_id": "68d9f1fa0177a6054b013aea",
                    "name": "Qiyang Zhou",
                    "hidden": false
                },
                {
                    "_id": "68d9f1fa0177a6054b013aeb",
                    "name": "Zixuan Hu",
                    "hidden": false
                },
                {
                    "_id": "68d9f1fa0177a6054b013aec",
                    "name": "Hongze Mi",
                    "hidden": false
                },
                {
                    "_id": "68d9f1fa0177a6054b013aed",
                    "name": "Yibo Wang",
                    "hidden": false
                },
                {
                    "_id": "68d9f1fa0177a6054b013aee",
                    "name": "Naiqiang Tan",
                    "hidden": false
                },
                {
                    "_id": "68d9f1fa0177a6054b013aef",
                    "name": "Hong Chen",
                    "hidden": false
                },
                {
                    "_id": "68d9f1fa0177a6054b013af0",
                    "name": "Yi R. Fung",
                    "hidden": false
                },
                {
                    "_id": "68d9f1fa0177a6054b013af1",
                    "name": "Chun Yuan",
                    "hidden": false
                },
                {
                    "_id": "68d9f1fa0177a6054b013af2",
                    "name": "Li Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T02:04:00.000Z",
            "submittedOnDailyAt": "2025-09-29T01:14:51.415Z",
            "title": "UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon\n  Scenarios",
            "submittedOnDailyBy": {
                "_id": "632ab8f5a968c34257da5c52",
                "avatarUrl": "/avatars/59df09e6c9e1e633170514d950ad7981.svg",
                "isPro": false,
                "fullname": "Haotian Luo",
                "user": "LordNoah",
                "type": "user"
            },
            "summary": "Autonomous agents have recently achieved remarkable progress across diverse\ndomains, yet most evaluations focus on short-horizon, fully observable tasks.\nIn contrast, many critical real-world tasks, such as large-scale software\ndevelopment, commercial investment, and scientific discovery, unfold in\nlong-horizon and partially observable scenarios where success hinges on\nsustained reasoning, planning, memory management, and tool use. Existing\nbenchmarks rarely capture these long-horizon challenges, leaving a gap in\nsystematic evaluation. To bridge this gap, we introduce UltraHorizon a\nnovel benchmark that measures the foundational capabilities essential for\ncomplex real-world challenges. We use exploration as a unifying task across\nthree distinct environments to validate these core competencies. Agents are\ndesigned in long-horizon discovery tasks where they must iteratively uncover\nhidden rules through sustained reasoning, planning, memory and tools\nmanagement, and interaction with environments. Under the heaviest scale\nsetting, trajectories average 200k+ tokens and 400+ tool\ncalls, whereas in standard configurations they still exceed 35k tokens\nand involve more than 60 tool calls on average. Our extensive\nexperiments reveal that LLM-agents consistently underperform in these settings,\nwhereas human participants achieve higher scores, underscoring a persistent gap\nin agents' long-horizon abilities. We also observe that simple scaling fails in\nour task. To better illustrate the failure of agents, we conduct an in-depth\nanalysis of collected trajectories. We identify eight types of errors and\nattribute them to two primary causes: in-context locking and functional\nfundamental capability gaps.\nhttps://github.com/StarDewXXX/UltraHorizon{Our code will be available\nhere.}",
            "upvotes": 21,
            "discussionId": "68d9f1fa0177a6054b013af3",
            "githubRepo": "https://github.com/StarDewXXX/UltraHorizon",
            "ai_summary": "UltraHorizon is a new benchmark that evaluates long-horizon and partially observable tasks for autonomous agents, highlighting gaps in their sustained reasoning, planning, memory, and tool use capabilities.",
            "ai_keywords": [
                "UltraHorizon",
                "long-horizon",
                "partially observable",
                "sustained reasoning",
                "planning",
                "memory management",
                "tool use",
                "exploration",
                "trajectories",
                "tokens",
                "tool calls",
                "LLM-agents",
                "in-context locking",
                "functional fundamental capability gaps"
            ],
            "githubStars": 12
        },
        "publishedAt": "2025-09-25T22:04:00.000Z",
        "title": "UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon\n  Scenarios",
        "summary": "Autonomous agents have recently achieved remarkable progress across diverse\ndomains, yet most evaluations focus on short-horizon, fully observable tasks.\nIn contrast, many critical real-world tasks, such as large-scale software\ndevelopment, commercial investment, and scientific discovery, unfold in\nlong-horizon and partially observable scenarios where success hinges on\nsustained reasoning, planning, memory management, and tool use. Existing\nbenchmarks rarely capture these long-horizon challenges, leaving a gap in\nsystematic evaluation. To bridge this gap, we introduce UltraHorizon a\nnovel benchmark that measures the foundational capabilities essential for\ncomplex real-world challenges. We use exploration as a unifying task across\nthree distinct environments to validate these core competencies. Agents are\ndesigned in long-horizon discovery tasks where they must iteratively uncover\nhidden rules through sustained reasoning, planning, memory and tools\nmanagement, and interaction with environments. Under the heaviest scale\nsetting, trajectories average 200k+ tokens and 400+ tool\ncalls, whereas in standard configurations they still exceed 35k tokens\nand involve more than 60 tool calls on average. Our extensive\nexperiments reveal that LLM-agents consistently underperform in these settings,\nwhereas human participants achieve higher scores, underscoring a persistent gap\nin agents' long-horizon abilities. We also observe that simple scaling fails in\nour task. To better illustrate the failure of agents, we conduct an in-depth\nanalysis of collected trajectories. We identify eight types of errors and\nattribute them to two primary causes: in-context locking and functional\nfundamental capability gaps.\nhttps://github.com/StarDewXXX/UltraHorizon{Our code will be available\nhere.}",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21766.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "632ab8f5a968c34257da5c52",
            "avatarUrl": "/avatars/59df09e6c9e1e633170514d950ad7981.svg",
            "fullname": "Haotian Luo",
            "name": "LordNoah",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22075",
            "authors": [
                {
                    "_id": "68da98cfd2bf1f4b15ec7137",
                    "name": "Dmitriy Shopkhoev",
                    "hidden": false
                },
                {
                    "_id": "68da98cfd2bf1f4b15ec7138",
                    "name": "Denis Makhov",
                    "hidden": false
                },
                {
                    "_id": "68da98cfd2bf1f4b15ec7139",
                    "name": "Magauiya Zhussip",
                    "hidden": false
                },
                {
                    "_id": "68da98cfd2bf1f4b15ec713a",
                    "name": "Ammar Ali",
                    "hidden": false
                },
                {
                    "_id": "68da98cfd2bf1f4b15ec713b",
                    "name": "Stamatios Lefkimmiatis",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T08:55:09.000Z",
            "submittedOnDailyAt": "2025-09-29T13:39:52.181Z",
            "title": "COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary\n  Learning",
            "submittedOnDailyBy": {
                "_id": "6166db59f78a267701a78c2a",
                "avatarUrl": "/avatars/8784efc36f67719e9455b1f081340ed9.svg",
                "isPro": false,
                "fullname": "Ammar Ali",
                "user": "ammarali32",
                "type": "user"
            },
            "summary": "Post-training compression of large language models (LLMs) largely relies on\nlow-rank weight approximation, which represents each column of a weight matrix\nin a shared low-dimensional subspace. While this is a computationally efficient\nstrategy, the imposed structural constraint is rigid and can lead to a\nnoticeable model accuracy drop. In this work, we propose CoSpaDi (Compression\nvia Sparse Dictionary Learning), a novel training-free compression framework\nthat replaces low-rank decomposition with a more flexible structured sparse\nfactorization in which each weight matrix is represented with a dense\ndictionary and a column-sparse coefficient matrix. This formulation enables a\nunion-of-subspaces representation: different columns of the original weight\nmatrix are approximated in distinct subspaces spanned by adaptively selected\ndictionary atoms, offering greater expressiveness than a single invariant\nbasis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the\nfactorization such that the output activations of compressed projection layers\nclosely match those of the original ones, thereby minimizing functional\nreconstruction error rather than mere weight approximation. This data-aware\nstrategy preserves better model fidelity without any fine-tuning under\nreasonable compression ratios. Moreover, the resulting structured sparsity\nallows efficient sparse-dense matrix multiplication and is compatible with\npost-training quantization for further memory and latency gains. We evaluate\nCoSpaDi across multiple Llama and Qwen models under per-layer and per-group\nsettings at 20-50\\% compression ratios, demonstrating consistent superiority\nover state-of-the-art data-aware low-rank methods both in accuracy and\nperplexity. Our results establish structured sparse dictionary learning as a\npowerful alternative to conventional low-rank approaches for efficient LLM\ndeployment.",
            "upvotes": 20,
            "discussionId": "68da98cfd2bf1f4b15ec713c",
            "ai_summary": "CoSpaDi, a training-free compression framework, uses structured sparse dictionary learning to compress large language models with better accuracy and efficiency compared to low-rank methods.",
            "ai_keywords": [
                "low-rank weight approximation",
                "CoSpaDi",
                "structured sparse factorization",
                "dense dictionary",
                "column-sparse coefficient matrix",
                "union-of-subspaces representation",
                "dictionary atoms",
                "calibration dataset",
                "functional reconstruction error",
                "sparse-dense matrix multiplication",
                "post-training quantization",
                "per-layer compression",
                "per-group compression",
                "accuracy",
                "perplexity"
            ]
        },
        "publishedAt": "2025-09-26T04:55:09.000Z",
        "title": "COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary\n  Learning",
        "summary": "Post-training compression of large language models (LLMs) largely relies on\nlow-rank weight approximation, which represents each column of a weight matrix\nin a shared low-dimensional subspace. While this is a computationally efficient\nstrategy, the imposed structural constraint is rigid and can lead to a\nnoticeable model accuracy drop. In this work, we propose CoSpaDi (Compression\nvia Sparse Dictionary Learning), a novel training-free compression framework\nthat replaces low-rank decomposition with a more flexible structured sparse\nfactorization in which each weight matrix is represented with a dense\ndictionary and a column-sparse coefficient matrix. This formulation enables a\nunion-of-subspaces representation: different columns of the original weight\nmatrix are approximated in distinct subspaces spanned by adaptively selected\ndictionary atoms, offering greater expressiveness than a single invariant\nbasis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the\nfactorization such that the output activations of compressed projection layers\nclosely match those of the original ones, thereby minimizing functional\nreconstruction error rather than mere weight approximation. This data-aware\nstrategy preserves better model fidelity without any fine-tuning under\nreasonable compression ratios. Moreover, the resulting structured sparsity\nallows efficient sparse-dense matrix multiplication and is compatible with\npost-training quantization for further memory and latency gains. We evaluate\nCoSpaDi across multiple Llama and Qwen models under per-layer and per-group\nsettings at 20-50\\% compression ratios, demonstrating consistent superiority\nover state-of-the-art data-aware low-rank methods both in accuracy and\nperplexity. Our results establish structured sparse dictionary learning as a\npowerful alternative to conventional low-rank approaches for efficient LLM\ndeployment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22075.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6166db59f78a267701a78c2a",
            "avatarUrl": "/avatars/8784efc36f67719e9455b1f081340ed9.svg",
            "fullname": "Ammar Ali",
            "name": "ammarali32",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "submitterOrganization": {
            "_id": "65f1bb3789aedc3dbe201d53",
            "name": "MTSAIR",
            "fullname": "MTSAIR",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6166db59f78a267701a78c2a/kTtlRzMs3RxfOT11vhhG4.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22651",
            "authors": [
                {
                    "_id": "68d9ebbd0177a6054b013a51",
                    "user": {
                        "_id": "64d592c28767727dffa1f002",
                        "avatarUrl": "/avatars/fe38bcac944a2742dc12c624e62d24ef.svg",
                        "isPro": false,
                        "fullname": "WangKe",
                        "user": "scikkk",
                        "type": "user"
                    },
                    "name": "Ke Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:58:58.098Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ebbd0177a6054b013a52",
                    "name": "Houxing Ren",
                    "hidden": false
                },
                {
                    "_id": "68d9ebbd0177a6054b013a53",
                    "name": "Zimu Lu",
                    "hidden": false
                },
                {
                    "_id": "68d9ebbd0177a6054b013a54",
                    "name": "Mingjie Zhan",
                    "hidden": false
                },
                {
                    "_id": "68d9ebbd0177a6054b013a55",
                    "name": "Hongsheng Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T17:59:59.000Z",
            "submittedOnDailyAt": "2025-09-29T00:55:30.907Z",
            "title": "VoiceAssistant-Eval: Benchmarking AI Assistants across Listening,\n  Speaking, and Viewing",
            "submittedOnDailyBy": {
                "_id": "64d592c28767727dffa1f002",
                "avatarUrl": "/avatars/fe38bcac944a2742dc12c624e62d24ef.svg",
                "isPro": false,
                "fullname": "WangKe",
                "user": "scikkk",
                "type": "user"
            },
            "summary": "The growing capabilities of large language models and multimodal systems have\nspurred interest in voice-first AI assistants, yet existing benchmarks are\ninadequate for evaluating the full range of these systems' capabilities. We\nintroduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI\nassistants across listening, speaking, and viewing. VoiceAssistant-Eval\ncomprises 10,497 curated examples spanning 13 task categories. These tasks\ninclude natural sounds, music, and spoken dialogue for listening; multi-turn\ndialogue, role-play imitation, and various scenarios for speaking; and highly\nheterogeneous images for viewing. To demonstrate its utility, we evaluate 21\nopen-source models and GPT-4o-Audio, measuring the quality of the response\ncontent and speech, as well as their consistency. The results reveal three key\nfindings: (1) proprietary models do not universally outperform open-source\nmodels; (2) most models excel at speaking tasks but lag in audio understanding;\nand (3) well-designed smaller models can rival much larger ones. Notably, the\nmid-sized Step-Audio-2-mini (7B) achieves more than double the listening\naccuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal\n(audio plus visual) input and role-play voice imitation tasks are difficult for\ncurrent models, and significant gaps persist in robustness and safety\nalignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous\nframework for evaluating and guiding the development of next-generation AI\nassistants. Code and data will be released at\nhttps://mathllm.github.io/VoiceAssistantEval/ .",
            "upvotes": 19,
            "discussionId": "68d9ebbe0177a6054b013a56",
            "projectPage": "https://mathllm.github.io/VoiceAssistantEval/",
            "githubRepo": "https://github.com/mathllm/VoiceAssistant-Eval",
            "ai_summary": "VoiceAssistant-Eval is a benchmark for evaluating AI assistants across listening, speaking, and viewing tasks, revealing insights into model performance and identifying areas for improvement.",
            "ai_keywords": [
                "large language models",
                "multimodal systems",
                "voice-first AI assistants",
                "VoiceAssistant-Eval",
                "listening",
                "speaking",
                "viewing",
                "natural sounds",
                "music",
                "spoken dialogue",
                "multi-turn dialogue",
                "role-play imitation",
                "highly heterogeneous images",
                "response content",
                "speech",
                "consistency",
                "proprietary models",
                "open-source models",
                "GPT-4o-Audio",
                "listening accuracy",
                "Step-Audio-2-mini",
                "LLaMA-Omni2-32B-Bilingual",
                "multimodal input",
                "role-play voice imitation",
                "robustness",
                "safety alignment"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-09-26T13:59:59.000Z",
        "title": "VoiceAssistant-Eval: Benchmarking AI Assistants across Listening,\n  Speaking, and Viewing",
        "summary": "The growing capabilities of large language models and multimodal systems have\nspurred interest in voice-first AI assistants, yet existing benchmarks are\ninadequate for evaluating the full range of these systems' capabilities. We\nintroduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI\nassistants across listening, speaking, and viewing. VoiceAssistant-Eval\ncomprises 10,497 curated examples spanning 13 task categories. These tasks\ninclude natural sounds, music, and spoken dialogue for listening; multi-turn\ndialogue, role-play imitation, and various scenarios for speaking; and highly\nheterogeneous images for viewing. To demonstrate its utility, we evaluate 21\nopen-source models and GPT-4o-Audio, measuring the quality of the response\ncontent and speech, as well as their consistency. The results reveal three key\nfindings: (1) proprietary models do not universally outperform open-source\nmodels; (2) most models excel at speaking tasks but lag in audio understanding;\nand (3) well-designed smaller models can rival much larger ones. Notably, the\nmid-sized Step-Audio-2-mini (7B) achieves more than double the listening\naccuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal\n(audio plus visual) input and role-play voice imitation tasks are difficult for\ncurrent models, and significant gaps persist in robustness and safety\nalignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous\nframework for evaluating and guiding the development of next-generation AI\nassistants. Code and data will be released at\nhttps://mathllm.github.io/VoiceAssistantEval/ .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22651.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d592c28767727dffa1f002",
            "avatarUrl": "/avatars/fe38bcac944a2742dc12c624e62d24ef.svg",
            "fullname": "WangKe",
            "name": "scikkk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "submitterOrganization": {
            "_id": "664a29f4f604081903ad5bfe",
            "name": "MathLLMs",
            "fullname": "LLMs for Reasoning",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/650d6b7ce31a6f18f010a400/Kd6cdUtXv3QW02kqQjQuR.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.22653",
            "authors": [
                {
                    "_id": "68d9f3590177a6054b013b0f",
                    "name": "Chih Yao Hu",
                    "hidden": false
                },
                {
                    "_id": "68d9f3590177a6054b013b10",
                    "name": "Yang-Sen Lin",
                    "hidden": false
                },
                {
                    "_id": "68d9f3590177a6054b013b11",
                    "user": {
                        "_id": "631c073e9edb6d320a175b50",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662781228166-noauth.png",
                        "isPro": false,
                        "fullname": "yuna0x0",
                        "user": "yuna0x0",
                        "type": "user"
                    },
                    "name": "Yuna Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:58:13.657Z",
                    "hidden": false
                },
                {
                    "_id": "68d9f3590177a6054b013b12",
                    "name": "Chih-Hai Su",
                    "hidden": false
                },
                {
                    "_id": "68d9f3590177a6054b013b13",
                    "user": {
                        "_id": "655f1770f74fa124d1172ec1",
                        "avatarUrl": "/avatars/e4413693c34974fac75a438ffe2cc630.svg",
                        "isPro": false,
                        "fullname": "Jie-Ying Lee",
                        "user": "jayinnn",
                        "type": "user"
                    },
                    "name": "Jie-Ying Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:58:09.121Z",
                    "hidden": false
                },
                {
                    "_id": "68d9f3590177a6054b013b14",
                    "name": "Shr-Ruei Tsai",
                    "hidden": false
                },
                {
                    "_id": "68d9f3590177a6054b013b15",
                    "name": "Chin-Yang Lin",
                    "hidden": false
                },
                {
                    "_id": "68d9f3590177a6054b013b16",
                    "name": "Kuan-Wen Chen",
                    "hidden": false
                },
                {
                    "_id": "68d9f3590177a6054b013b17",
                    "name": "Tsung-Wei Ke",
                    "hidden": false
                },
                {
                    "_id": "68d9f3590177a6054b013b18",
                    "name": "Yu-Lun Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/jXIPd_rI57wyRFXn8MMFs.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/rlG4NDb8KERdfXisRyQJy.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/KGKl_yyeneO29hL9teJJC.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/81Uj8UQYR_1umBg-mldYE.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/hmCXAyVFo7mnbnWgKw-M6.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/IVKIIO41x6_uBbdy5nNpP.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/dnIwAHYoHvT1LwdodKnNg.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/nQ-YlUkTZ3IeRDPHHOnQp.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/CkJJBYuN3Cgh1XrARrWOp.mp4"
            ],
            "publishedAt": "2025-09-26T17:59:59.000Z",
            "submittedOnDailyAt": "2025-09-29T04:51:47.994Z",
            "title": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned\n  Aerial Navigation",
            "submittedOnDailyBy": {
                "_id": "631c073e9edb6d320a175b50",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662781228166-noauth.png",
                "isPro": false,
                "fullname": "yuna0x0",
                "user": "yuna0x0",
                "type": "user"
            },
            "summary": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language\nnavigation (AVLN) framework built atop vision-language models (VLMs). SPF is\ncapable of navigating to any goal based on any type of free-form instructions\nin any kind of environment. In contrast to existing VLM-based approaches that\ntreat action prediction as a text generation task, our key insight is to\nconsider action prediction for AVLN as a 2D spatial grounding task. SPF\nharnesses VLMs to decompose vague language instructions into iterative\nannotation of 2D waypoints on the input image. Along with the predicted\ntraveling distance, SPF transforms predicted 2D waypoints into 3D displacement\nvectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the\ntraveling distance to facilitate more efficient navigation. Notably, SPF\nperforms navigation in a closed-loop control manner, enabling UAVs to follow\ndynamic targets in dynamic environments. SPF sets a new state of the art in DRL\nsimulation benchmark, outperforming the previous best method by an absolute\nmargin of 63%. In extensive real-world evaluations, SPF outperforms strong\nbaselines by a large margin. We also conduct comprehensive ablation studies to\nhighlight the effectiveness of our design choice. Lastly, SPF shows remarkable\ngeneralization to different VLMs. Project page: https://spf-web.pages.dev",
            "upvotes": 17,
            "discussionId": "68d9f35a0177a6054b013b19",
            "projectPage": "https://spf-web.pages.dev",
            "githubRepo": "https://github.com/Hu-chih-yao/see-point-fly",
            "ai_summary": "See, Point, Fly (SPF) is a training-free aerial vision-and-language navigation framework that treats action prediction as a 2D spatial grounding task, outperforming existing methods in both simulation and real-world evaluations.",
            "ai_keywords": [
                "vision-language models",
                "VLMs",
                "action prediction",
                "2D spatial grounding",
                "2D waypoints",
                "3D displacement vectors",
                "UAVs",
                "closed-loop control",
                "DRL simulation benchmark",
                "ablation studies"
            ],
            "githubStars": 10
        },
        "publishedAt": "2025-09-26T13:59:59.000Z",
        "title": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned\n  Aerial Navigation",
        "summary": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language\nnavigation (AVLN) framework built atop vision-language models (VLMs). SPF is\ncapable of navigating to any goal based on any type of free-form instructions\nin any kind of environment. In contrast to existing VLM-based approaches that\ntreat action prediction as a text generation task, our key insight is to\nconsider action prediction for AVLN as a 2D spatial grounding task. SPF\nharnesses VLMs to decompose vague language instructions into iterative\nannotation of 2D waypoints on the input image. Along with the predicted\ntraveling distance, SPF transforms predicted 2D waypoints into 3D displacement\nvectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the\ntraveling distance to facilitate more efficient navigation. Notably, SPF\nperforms navigation in a closed-loop control manner, enabling UAVs to follow\ndynamic targets in dynamic environments. SPF sets a new state of the art in DRL\nsimulation benchmark, outperforming the previous best method by an absolute\nmargin of 63%. In extensive real-world evaluations, SPF outperforms strong\nbaselines by a large margin. We also conduct comprehensive ablation studies to\nhighlight the effectiveness of our design choice. Lastly, SPF shows remarkable\ngeneralization to different VLMs. Project page: https://spf-web.pages.dev",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/jXIPd_rI57wyRFXn8MMFs.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/rlG4NDb8KERdfXisRyQJy.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/KGKl_yyeneO29hL9teJJC.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/81Uj8UQYR_1umBg-mldYE.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/hmCXAyVFo7mnbnWgKw-M6.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/IVKIIO41x6_uBbdy5nNpP.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/dnIwAHYoHvT1LwdodKnNg.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/nQ-YlUkTZ3IeRDPHHOnQp.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/631c073e9edb6d320a175b50/CkJJBYuN3Cgh1XrARrWOp.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22653.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631c073e9edb6d320a175b50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662781228166-noauth.png",
            "fullname": "yuna0x0",
            "name": "yuna0x0",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.22414",
            "authors": [
                {
                    "_id": "68d9f8180177a6054b013b59",
                    "name": "Song Fei",
                    "hidden": false
                },
                {
                    "_id": "68d9f8180177a6054b013b5a",
                    "name": "Tian Ye",
                    "hidden": false
                },
                {
                    "_id": "68d9f8180177a6054b013b5b",
                    "name": "Lujia Wang",
                    "hidden": false
                },
                {
                    "_id": "68d9f8180177a6054b013b5c",
                    "name": "Lei Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T14:39:08.000Z",
            "submittedOnDailyAt": "2025-09-29T01:44:29.750Z",
            "title": "LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale\n  Diffusion Transformer",
            "submittedOnDailyBy": {
                "_id": "66015e8aa4d296af07de538e",
                "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
                "isPro": false,
                "fullname": "Owen",
                "user": "Owen777",
                "type": "user"
            },
            "summary": "Universal image restoration (UIR) aims to recover images degraded by unknown\nmixtures while preserving semantics -- conditions under which discriminative\nrestorers and UNet-based diffusion priors often oversmooth, hallucinate, or\ndrift. We present LucidFlux, a caption-free UIR framework that adapts a large\ndiffusion transformer (Flux.1) without image captions. LucidFlux introduces a\nlightweight dual-branch conditioner that injects signals from the degraded\ninput and a lightly restored proxy to respectively anchor geometry and suppress\nartifacts. Then, a timestep- and layer-adaptive modulation schedule is designed\nto route these cues across the backbone's hierarchy, in order to yield\ncoarse-to-fine and context-aware updates that protect the global structure\nwhile recovering texture. After that, to avoid the latency and instability of\ntext prompts or MLLM captions, we enforce caption-free semantic alignment via\nSigLIP features extracted from the proxy. A scalable curation pipeline further\nfilters large-scale data for structure-rich supervision. Across synthetic and\nin-the-wild benchmarks, LucidFlux consistently outperforms strong open-source\nand commercial baselines, and ablation studies verify the necessity of each\ncomponent. LucidFlux shows that, for large DiTs, when, where, and what to\ncondition on -- rather than adding parameters or relying on text prompts -- is\nthe governing lever for robust and caption-free universal image restoration in\nthe wild.",
            "upvotes": 17,
            "discussionId": "68d9f8190177a6054b013b5d",
            "projectPage": "https://w2genai-lab.github.io/LucidFlux/",
            "githubRepo": "https://github.com/W2GenAI-Lab/LucidFlux",
            "ai_summary": "LucidFlux, a caption-free UIR framework using a diffusion transformer, achieves robust image restoration through adaptive conditioning and SigLIP features without text prompts.",
            "ai_keywords": [
                "diffusion transformer",
                "dual-branch conditioner",
                "timestep- and layer-adaptive modulation",
                "SigLIP features",
                "scalable curation pipeline",
                "universal image restoration",
                "DiTs"
            ],
            "githubStars": 63
        },
        "publishedAt": "2025-09-26T10:39:08.000Z",
        "title": "LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale\n  Diffusion Transformer",
        "summary": "Universal image restoration (UIR) aims to recover images degraded by unknown\nmixtures while preserving semantics -- conditions under which discriminative\nrestorers and UNet-based diffusion priors often oversmooth, hallucinate, or\ndrift. We present LucidFlux, a caption-free UIR framework that adapts a large\ndiffusion transformer (Flux.1) without image captions. LucidFlux introduces a\nlightweight dual-branch conditioner that injects signals from the degraded\ninput and a lightly restored proxy to respectively anchor geometry and suppress\nartifacts. Then, a timestep- and layer-adaptive modulation schedule is designed\nto route these cues across the backbone's hierarchy, in order to yield\ncoarse-to-fine and context-aware updates that protect the global structure\nwhile recovering texture. After that, to avoid the latency and instability of\ntext prompts or MLLM captions, we enforce caption-free semantic alignment via\nSigLIP features extracted from the proxy. A scalable curation pipeline further\nfilters large-scale data for structure-rich supervision. Across synthetic and\nin-the-wild benchmarks, LucidFlux consistently outperforms strong open-source\nand commercial baselines, and ablation studies verify the necessity of each\ncomponent. LucidFlux shows that, for large DiTs, when, where, and what to\ncondition on -- rather than adding parameters or relying on text prompts -- is\nthe governing lever for robust and caption-free universal image restoration in\nthe wild.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22414.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66015e8aa4d296af07de538e",
            "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
            "fullname": "Owen",
            "name": "Owen777",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 33
        },
        "submitterOrganization": {
            "_id": "68b2b2167f881fc640bb9d21",
            "name": "W2GenAI",
            "fullname": "W2GenAI Lab"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22644",
            "authors": [
                {
                    "_id": "68d9f3c80177a6054b013b25",
                    "name": "Zimu Lu",
                    "hidden": false
                },
                {
                    "_id": "68d9f3c80177a6054b013b26",
                    "name": "Houxing Ren",
                    "hidden": false
                },
                {
                    "_id": "68d9f3c80177a6054b013b27",
                    "name": "Yunqiao Yang",
                    "hidden": false
                },
                {
                    "_id": "68d9f3c80177a6054b013b28",
                    "user": {
                        "_id": "64d592c28767727dffa1f002",
                        "avatarUrl": "/avatars/fe38bcac944a2742dc12c624e62d24ef.svg",
                        "isPro": false,
                        "fullname": "WangKe",
                        "user": "scikkk",
                        "type": "user"
                    },
                    "name": "Ke Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:58:03.512Z",
                    "hidden": false
                },
                {
                    "_id": "68d9f3c80177a6054b013b29",
                    "name": "Zhuofan Zong",
                    "hidden": false
                },
                {
                    "_id": "68d9f3c80177a6054b013b2a",
                    "name": "Junting Pan",
                    "hidden": false
                },
                {
                    "_id": "68d9f3c80177a6054b013b2b",
                    "name": "Mingjie Zhan",
                    "hidden": false
                },
                {
                    "_id": "68d9f3c80177a6054b013b2c",
                    "name": "Hongsheng Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T17:59:51.000Z",
            "submittedOnDailyAt": "2025-09-29T01:24:05.846Z",
            "title": "WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level\n  Feedback and Step-Level Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "64b0bfef2f2f9c345b87e673",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/7O3zCr_IyGvtz0BUdKNKi.jpeg",
                "isPro": false,
                "fullname": "Zimu Lu",
                "user": "luzimu",
                "type": "user"
            },
            "summary": "Agent systems powered by large language models (LLMs) have demonstrated\nimpressive performance on repository-level code-generation tasks. However, for\ntasks such as website codebase generation, which depend heavily on visual\neffects and user-interaction feedback, current code agents rely only on simple\ncode execution for feedback and verification. This approach fails to capture\nthe actual quality of the generated code. In this paper, we propose\nWebGen-Agent, a novel website-generation agent that leverages comprehensive and\nmulti-level visual feedback to iteratively generate and refine the website\ncodebase. Detailed and expressive text descriptions and suggestions regarding\nthe screenshots and GUI-agent testing of the websites are generated by a visual\nlanguage model (VLM), together with scores that quantify their quality. The\nscreenshot and GUI-agent scores are further integrated with a backtracking and\nselect-best mechanism, enhancing the performance of the agent. Utilizing the\naccurate visual scores inherent in the WebGen-Agent workflow, we further\nintroduce Step-GRPO with Screenshot and GUI-agent Feedback to improve\nthe ability of LLMs to act as the reasoning engine of WebGen-Agent. By using\nthe screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we\nprovide a dense and reliable process supervision signal, which effectively\nimproves the model's website-generation ability. On the WebGen-Bench dataset,\nWebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%\nand its appearance score from 3.0 to 3.9, outperforming the previous\nstate-of-the-art agent system. Additionally, our Step-GRPO training approach\nincreases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and\nraises the appearance score from 3.4 to 3.7.",
            "upvotes": 15,
            "discussionId": "68d9f3c90177a6054b013b2d",
            "ai_summary": "WebGen-Agent enhances website code generation by integrating visual feedback and GUI-agent testing with a backtracking mechanism and Step-GRPO training to improve accuracy and appearance scores.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "website codebase generation",
                "visual feedback",
                "GUI-agent testing",
                "visual language model",
                "VLM",
                "backtracking",
                "select-best mechanism",
                "Step-GRPO",
                "WebGen-Bench dataset",
                "Claude-3.5-Sonnet",
                "Qwen2.5-Coder-7B-Instruct"
            ]
        },
        "publishedAt": "2025-09-26T13:59:51.000Z",
        "title": "WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level\n  Feedback and Step-Level Reinforcement Learning",
        "summary": "Agent systems powered by large language models (LLMs) have demonstrated\nimpressive performance on repository-level code-generation tasks. However, for\ntasks such as website codebase generation, which depend heavily on visual\neffects and user-interaction feedback, current code agents rely only on simple\ncode execution for feedback and verification. This approach fails to capture\nthe actual quality of the generated code. In this paper, we propose\nWebGen-Agent, a novel website-generation agent that leverages comprehensive and\nmulti-level visual feedback to iteratively generate and refine the website\ncodebase. Detailed and expressive text descriptions and suggestions regarding\nthe screenshots and GUI-agent testing of the websites are generated by a visual\nlanguage model (VLM), together with scores that quantify their quality. The\nscreenshot and GUI-agent scores are further integrated with a backtracking and\nselect-best mechanism, enhancing the performance of the agent. Utilizing the\naccurate visual scores inherent in the WebGen-Agent workflow, we further\nintroduce Step-GRPO with Screenshot and GUI-agent Feedback to improve\nthe ability of LLMs to act as the reasoning engine of WebGen-Agent. By using\nthe screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we\nprovide a dense and reliable process supervision signal, which effectively\nimproves the model's website-generation ability. On the WebGen-Bench dataset,\nWebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%\nand its appearance score from 3.0 to 3.9, outperforming the previous\nstate-of-the-art agent system. Additionally, our Step-GRPO training approach\nincreases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and\nraises the appearance score from 3.4 to 3.7.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22644.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b0bfef2f2f9c345b87e673",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/7O3zCr_IyGvtz0BUdKNKi.jpeg",
            "fullname": "Zimu Lu",
            "name": "luzimu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22624",
            "authors": [
                {
                    "_id": "68d9ed460177a6054b013aab",
                    "name": "Ziyu Liu",
                    "hidden": false
                },
                {
                    "_id": "68d9ed460177a6054b013aac",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/kajwuVzd4pDucSPlwghxo.png",
                        "isPro": false,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:58:32.038Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ed460177a6054b013aad",
                    "name": "Shengyuan Ding",
                    "hidden": false
                },
                {
                    "_id": "68d9ed460177a6054b013aae",
                    "name": "Yuhang Cao",
                    "hidden": false
                },
                {
                    "_id": "68d9ed460177a6054b013aaf",
                    "name": "Xiaoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "68d9ed460177a6054b013ab0",
                    "name": "Haodong Duan",
                    "hidden": false
                },
                {
                    "_id": "68d9ed460177a6054b013ab1",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "68d9ed460177a6054b013ab2",
                    "user": {
                        "_id": "64b4eec4faa3181a5eab9c46",
                        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
                        "isPro": true,
                        "fullname": "Jiaqi Wang",
                        "user": "myownskyW7",
                        "type": "user"
                    },
                    "name": "Jiaqi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:58:29.391Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T17:50:12.000Z",
            "submittedOnDailyAt": "2025-09-29T02:25:47.351Z",
            "title": "SPARK: Synergistic Policy And Reward Co-Evolving Framework",
            "submittedOnDailyBy": {
                "_id": "63859cf3b2906edaf83af9f0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/kajwuVzd4pDucSPlwghxo.png",
                "isPro": false,
                "fullname": "Yuhang Zang",
                "user": "yuhangzang",
                "type": "user"
            },
            "summary": "Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)\nincreasingly use Reinforcement Learning (RL) for post-pretraining, such as RL\nwith Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback\n(RLHF) for subjective tasks. However, RLHF incurs high costs and potential\nreward-policy mismatch due to reliance on human preferences, while RLVR still\nwastes supervision by discarding rollouts and correctness signals after each\nupdate. To address these challenges, we introduce the Synergistic Policy And\nReward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable\nmethod that builds on RLVR. Instead of discarding rollouts and correctness\ndata, SPARK recycles this valuable information to simultaneously train the\nmodel itself as a generative reward model. This auxiliary training uses a mix\nof objectives, such as pointwise reward score, pairwise comparison, and\nevaluation conditioned on further-reflection responses, to teach the model to\nevaluate and improve its own responses. Our process eliminates the need for a\nseparate reward model and costly human preference data. SPARK creates a\npositive co-evolving feedback loop: improved reward accuracy yields better\npolicy gradients, which in turn produce higher-quality rollouts that further\nrefine the reward model. Our unified framework supports test-time scaling via\nself-reflection without external reward models and their associated costs. We\nshow that SPARK achieves significant performance gains on multiple LLM and LVLM\nmodels and multiple reasoning, reward models, and general benchmarks. For\nexample, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks,\n12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the\nbaselines, demonstrating robustness and broad generalization.",
            "upvotes": 15,
            "discussionId": "68d9ed460177a6054b013ab3",
            "projectPage": "https://github.com/InternLM/Spark",
            "githubRepo": "https://github.com/InternLM/Spark",
            "ai_summary": "SPARK, a synergistic policy and reward co-evolving framework, enhances LLMs and LVLMs by recycling rollouts and correctness data to train a generative reward model, reducing reliance on human preferences and external reward models.",
            "ai_keywords": [
                "Reinforcement Learning",
                "RL with Verifiable Rewards",
                "RL from Human Feedback",
                "Synergistic Policy And Reward Co-Evolving Framework",
                "SPARK",
                "on-policy",
                "generative reward model",
                "pointwise reward score",
                "pairwise comparison",
                "self-reflection",
                "policy gradients",
                "rollouts",
                "reward accuracy",
                "performance gains",
                "reasoning benchmarks",
                "reward benchmarks",
                "general benchmarks"
            ],
            "githubStars": 16
        },
        "publishedAt": "2025-09-26T13:50:12.000Z",
        "title": "SPARK: Synergistic Policy And Reward Co-Evolving Framework",
        "summary": "Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)\nincreasingly use Reinforcement Learning (RL) for post-pretraining, such as RL\nwith Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback\n(RLHF) for subjective tasks. However, RLHF incurs high costs and potential\nreward-policy mismatch due to reliance on human preferences, while RLVR still\nwastes supervision by discarding rollouts and correctness signals after each\nupdate. To address these challenges, we introduce the Synergistic Policy And\nReward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable\nmethod that builds on RLVR. Instead of discarding rollouts and correctness\ndata, SPARK recycles this valuable information to simultaneously train the\nmodel itself as a generative reward model. This auxiliary training uses a mix\nof objectives, such as pointwise reward score, pairwise comparison, and\nevaluation conditioned on further-reflection responses, to teach the model to\nevaluate and improve its own responses. Our process eliminates the need for a\nseparate reward model and costly human preference data. SPARK creates a\npositive co-evolving feedback loop: improved reward accuracy yields better\npolicy gradients, which in turn produce higher-quality rollouts that further\nrefine the reward model. Our unified framework supports test-time scaling via\nself-reflection without external reward models and their associated costs. We\nshow that SPARK achieves significant performance gains on multiple LLM and LVLM\nmodels and multiple reasoning, reward models, and general benchmarks. For\nexample, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks,\n12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the\nbaselines, demonstrating robustness and broad generalization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22624.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/kajwuVzd4pDucSPlwghxo.png",
            "fullname": "Yuhang Zang",
            "name": "yuhangzang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "submitterOrganization": {
            "_id": "64a2d5fa81252883206f24c9",
            "name": "internlm",
            "fullname": "Intern Large Models",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.21989",
            "authors": [
                {
                    "_id": "68da1cfe0177a6054b013bc5",
                    "user": {
                        "_id": "6577f3bacfb2207f11e847bb",
                        "avatarUrl": "/avatars/825998cfebc47d8106f633be5ad10964.svg",
                        "isPro": false,
                        "fullname": "Abdelrahman Eldesokey",
                        "user": "abdo-eldesokey",
                        "type": "user"
                    },
                    "name": "Abdelrahman Eldesokey",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T11:26:25.220Z",
                    "hidden": false
                },
                {
                    "_id": "68da1cfe0177a6054b013bc6",
                    "name": "Aleksandar Cvejic",
                    "hidden": false
                },
                {
                    "_id": "68da1cfe0177a6054b013bc7",
                    "name": "Bernard Ghanem",
                    "hidden": false
                },
                {
                    "_id": "68da1cfe0177a6054b013bc8",
                    "name": "Peter Wonka",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6577f3bacfb2207f11e847bb/bT2HVKlLIeZzkJz3kFw-S.mp4"
            ],
            "publishedAt": "2025-09-26T07:11:55.000Z",
            "submittedOnDailyAt": "2025-09-29T04:16:41.968Z",
            "title": "Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in\n  Subject-Driven Generation",
            "submittedOnDailyBy": {
                "_id": "6577f3bacfb2207f11e847bb",
                "avatarUrl": "/avatars/825998cfebc47d8106f633be5ad10964.svg",
                "isPro": false,
                "fullname": "Abdelrahman Eldesokey",
                "user": "abdo-eldesokey",
                "type": "user"
            },
            "summary": "We propose a novel approach for disentangling visual and semantic features\nfrom the backbones of pre-trained diffusion models, enabling visual\ncorrespondence in a manner analogous to the well-established semantic\ncorrespondence. While diffusion model backbones are known to encode\nsemantically rich features, they must also contain visual features to support\ntheir image synthesis capabilities. However, isolating these visual features is\nchallenging due to the absence of annotated datasets. To address this, we\nintroduce an automated pipeline that constructs image pairs with annotated\nsemantic and visual correspondences based on existing subject-driven image\ngeneration datasets, and design a contrastive architecture to separate the two\nfeature types. Leveraging the disentangled representations, we propose a new\nmetric, Visual Semantic Matching (VSM), that quantifies visual inconsistencies\nin subject-driven image generation. Empirical results show that our approach\noutperforms global feature-based metrics such as CLIP, DINO, and\nvision--language models in quantifying visual inconsistencies while also\nenabling spatial localization of inconsistent regions. To our knowledge, this\nis the first method that supports both quantification and localization of\ninconsistencies in subject-driven generation, offering a valuable tool for\nadvancing this task. Project\nPage:https://abdo-eldesokey.github.io/mind-the-glitch/",
            "upvotes": 15,
            "discussionId": "68da1cff0177a6054b013bc9",
            "projectPage": "https://abdo-eldesokey.github.io/mind-the-glitch/",
            "githubRepo": "https://github.com/abdo-eldesokey/mind-the-glitch",
            "ai_summary": "A novel method disentangles visual and semantic features from diffusion model backbones to quantify and localize visual inconsistencies in subject-driven image generation.",
            "ai_keywords": [
                "diffusion models",
                "visual features",
                "semantic features",
                "image pairs",
                "contrastive architecture",
                "Visual Semantic Matching (VSM)",
                "CLIP",
                "DINO",
                "vision--language models",
                "subject-driven image generation"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-09-26T03:11:55.000Z",
        "title": "Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in\n  Subject-Driven Generation",
        "summary": "We propose a novel approach for disentangling visual and semantic features\nfrom the backbones of pre-trained diffusion models, enabling visual\ncorrespondence in a manner analogous to the well-established semantic\ncorrespondence. While diffusion model backbones are known to encode\nsemantically rich features, they must also contain visual features to support\ntheir image synthesis capabilities. However, isolating these visual features is\nchallenging due to the absence of annotated datasets. To address this, we\nintroduce an automated pipeline that constructs image pairs with annotated\nsemantic and visual correspondences based on existing subject-driven image\ngeneration datasets, and design a contrastive architecture to separate the two\nfeature types. Leveraging the disentangled representations, we propose a new\nmetric, Visual Semantic Matching (VSM), that quantifies visual inconsistencies\nin subject-driven image generation. Empirical results show that our approach\noutperforms global feature-based metrics such as CLIP, DINO, and\nvision--language models in quantifying visual inconsistencies while also\nenabling spatial localization of inconsistent regions. To our knowledge, this\nis the first method that supports both quantification and localization of\ninconsistencies in subject-driven generation, offering a valuable tool for\nadvancing this task. Project\nPage:https://abdo-eldesokey.github.io/mind-the-glitch/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6577f3bacfb2207f11e847bb/bT2HVKlLIeZzkJz3kFw-S.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21989.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6577f3bacfb2207f11e847bb",
            "avatarUrl": "/avatars/825998cfebc47d8106f633be5ad10964.svg",
            "fullname": "Abdelrahman Eldesokey",
            "name": "abdo-eldesokey",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.21710",
            "authors": [
                {
                    "_id": "68d9f6800177a6054b013b3e",
                    "user": {
                        "_id": "6352637d0f9bdb641c44e52d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6352637d0f9bdb641c44e52d/mSBRPzcH5pIV68PUmcsHV.png",
                        "isPro": false,
                        "fullname": "wuxiaojun",
                        "user": "wuxiaojun",
                        "type": "user"
                    },
                    "name": "Xiaojun Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:58:01.305Z",
                    "hidden": false
                },
                {
                    "_id": "68d9f6800177a6054b013b3f",
                    "name": "Cehao Yang",
                    "hidden": false
                },
                {
                    "_id": "68d9f6800177a6054b013b40",
                    "name": "Xueyuan Lin",
                    "hidden": false
                },
                {
                    "_id": "68d9f6800177a6054b013b41",
                    "name": "Chengjin Xu",
                    "hidden": false
                },
                {
                    "_id": "68d9f6800177a6054b013b42",
                    "name": "Xuhui Jiang",
                    "hidden": false
                },
                {
                    "_id": "68d9f6800177a6054b013b43",
                    "name": "Yuanliang Sun",
                    "hidden": false
                },
                {
                    "_id": "68d9f6800177a6054b013b44",
                    "name": "Hui Xiong",
                    "hidden": false
                },
                {
                    "_id": "68d9f6800177a6054b013b45",
                    "name": "Jia Li",
                    "hidden": false
                },
                {
                    "_id": "68d9f6800177a6054b013b46",
                    "name": "Jian Guo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6352637d0f9bdb641c44e52d/YCja6QXrg88J1RAsObhCs.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6352637d0f9bdb641c44e52d/NzKmT8H9nEDX6mhKe1Z1q.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6352637d0f9bdb641c44e52d/hznDaJE6_uY7Y0HId9_x6.png"
            ],
            "publishedAt": "2025-09-26T00:13:10.000Z",
            "submittedOnDailyAt": "2025-09-29T01:39:30.005Z",
            "title": "Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on\n  Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval",
            "submittedOnDailyBy": {
                "_id": "6352637d0f9bdb641c44e52d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6352637d0f9bdb641c44e52d/mSBRPzcH5pIV68PUmcsHV.png",
                "isPro": false,
                "fullname": "wuxiaojun",
                "user": "wuxiaojun",
                "type": "user"
            },
            "summary": "Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the\nimportant paradigm for enhancing Large Language Models (LLMs) with external\nknowledge. However, existing approaches face a fundamental trade-off. While\ngraph-based methods are inherently dependent on high-quality graph structures,\nthey face significant practical constraints: manually constructed knowledge\ngraphs are prohibitively expensive to scale, while automatically extracted\ngraphs from corpora are limited by the performance of the underlying LLM\nextractors, especially when using smaller, local-deployed models. This paper\npresents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces\nMulti-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these\nlimitations. Our core innovation is the dynamic construction and refinement of\na Chunk-Triplets-Community heterogeneous graph index, which pioneeringly\nincorporates a dual-evolution mechanism of Evolving Query and Evolving\nSub-Graph for precise evidence retrieval. This approach addresses a critical\nlimitation of prior Graph-based RAG methods, which typically construct a static\ngraph index in a single pass without adapting to the actual query. A\nmulti-agent system, comprising Constructor, Retriever, Reflector, and Responser\nagents, collaboratively engages in an iterative process of evidence retrieval,\nanswer generation, sufficiency reflection, and, crucially, evolving query and\nsubgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively\nbuild a targeted graph index during reasoning, mitigating the inherent\ndrawbacks of static, one-time graph construction and enabling deep, precise\nreasoning even with lightweight LLMs. Extensive experiments demonstrate that\nToG-3 outperforms compared baselines on both deep and broad reasoning\nbenchmarks, and ablation studies confirm the efficacy of the components of\nMACER framework.",
            "upvotes": 14,
            "discussionId": "68d9f6800177a6054b013b47",
            "githubRepo": "https://github.com/DataArcTech/RAG-Factory",
            "ai_summary": "ToG-3, a novel framework, enhances LLMs with external knowledge using a dynamic, multi-agent system that evolves queries and subgraphs for precise evidence retrieval and reasoning.",
            "ai_keywords": [
                "Retrieval-Augmented Generation",
                "Graph-based RAG",
                "Large Language Models",
                "Multi-Agent Context Evolution and Retrieval",
                "Chunk-Triplets-Community",
                "Evolving Query",
                "Evolving Sub-Graph",
                "multi-agent system",
                "Constructor",
                "Retriever",
                "Reflector",
                "Responser",
                "deep reasoning",
                "broad reasoning"
            ],
            "githubStars": 18
        },
        "publishedAt": "2025-09-25T20:13:10.000Z",
        "title": "Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on\n  Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval",
        "summary": "Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the\nimportant paradigm for enhancing Large Language Models (LLMs) with external\nknowledge. However, existing approaches face a fundamental trade-off. While\ngraph-based methods are inherently dependent on high-quality graph structures,\nthey face significant practical constraints: manually constructed knowledge\ngraphs are prohibitively expensive to scale, while automatically extracted\ngraphs from corpora are limited by the performance of the underlying LLM\nextractors, especially when using smaller, local-deployed models. This paper\npresents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces\nMulti-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these\nlimitations. Our core innovation is the dynamic construction and refinement of\na Chunk-Triplets-Community heterogeneous graph index, which pioneeringly\nincorporates a dual-evolution mechanism of Evolving Query and Evolving\nSub-Graph for precise evidence retrieval. This approach addresses a critical\nlimitation of prior Graph-based RAG methods, which typically construct a static\ngraph index in a single pass without adapting to the actual query. A\nmulti-agent system, comprising Constructor, Retriever, Reflector, and Responser\nagents, collaboratively engages in an iterative process of evidence retrieval,\nanswer generation, sufficiency reflection, and, crucially, evolving query and\nsubgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively\nbuild a targeted graph index during reasoning, mitigating the inherent\ndrawbacks of static, one-time graph construction and enabling deep, precise\nreasoning even with lightweight LLMs. Extensive experiments demonstrate that\nToG-3 outperforms compared baselines on both deep and broad reasoning\nbenchmarks, and ablation studies confirm the efficacy of the components of\nMACER framework.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6352637d0f9bdb641c44e52d/YCja6QXrg88J1RAsObhCs.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6352637d0f9bdb641c44e52d/NzKmT8H9nEDX6mhKe1Z1q.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6352637d0f9bdb641c44e52d/hznDaJE6_uY7Y0HId9_x6.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21710.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6352637d0f9bdb641c44e52d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6352637d0f9bdb641c44e52d/mSBRPzcH5pIV68PUmcsHV.png",
            "fullname": "wuxiaojun",
            "name": "wuxiaojun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "submitterOrganization": {
            "_id": "68d9ffdfbca0cbde59529d88",
            "name": "DataArcTech",
            "fullname": "DataArcTech Ltd.",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b56d96a6566c6039b7f1d8/4ZhVvQ1O0IkLEmHKDXek6.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.21388",
            "authors": [
                {
                    "_id": "68da66cacfa03a5158a1edae",
                    "name": "Anton Konushin",
                    "hidden": false
                },
                {
                    "_id": "68da66cacfa03a5158a1edaf",
                    "name": "Nikita Drozdov",
                    "hidden": false
                },
                {
                    "_id": "68da66cacfa03a5158a1edb0",
                    "name": "Bulat Gabdullin",
                    "hidden": false
                },
                {
                    "_id": "68da66cacfa03a5158a1edb1",
                    "name": "Alexey Zakharov",
                    "hidden": false
                },
                {
                    "_id": "68da66cacfa03a5158a1edb2",
                    "name": "Anna Vorontsova",
                    "hidden": false
                },
                {
                    "_id": "68da66cacfa03a5158a1edb3",
                    "name": "Danila Rukhovich",
                    "hidden": false
                },
                {
                    "_id": "68da66cacfa03a5158a1edb4",
                    "name": "Maksim Kolodiazhnyi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-23T20:24:07.000Z",
            "submittedOnDailyAt": "2025-09-29T09:35:34.285Z",
            "title": "TUN3D: Towards Real-World Scene Understanding from Unposed Images",
            "submittedOnDailyBy": {
                "_id": "665b10fb270e47e678f2ddf1",
                "avatarUrl": "/avatars/1bc7a9211acf767f7bfca998c24315a0.svg",
                "isPro": false,
                "fullname": "max",
                "user": "maksimko123",
                "type": "user"
            },
            "summary": "Layout estimation and 3D object detection are two fundamental tasks in indoor\nscene understanding. When combined, they enable the creation of a compact yet\nsemantically rich spatial representation of a scene. Existing approaches\ntypically rely on point cloud input, which poses a major limitation since most\nconsumer cameras lack depth sensors and visual-only data remains far more\ncommon. We address this issue with TUN3D, the first method that tackles joint\nlayout estimation and 3D object detection in real scans, given multi-view\nimages as input, and does not require ground-truth camera poses or depth\nsupervision. Our approach builds on a lightweight sparse-convolutional backbone\nand employs two dedicated heads: one for 3D object detection and one for layout\nestimation, leveraging a novel and effective parametric wall representation.\nExtensive experiments show that TUN3D achieves state-of-the-art performance\nacross three challenging scene understanding benchmarks: (i) using ground-truth\npoint clouds, (ii) using posed images, and (iii) using unposed images. While\nperforming on par with specialized 3D object detection methods, TUN3D\nsignificantly advances layout estimation, setting a new benchmark in holistic\nindoor scene understanding. Code is available at\nhttps://github.com/col14m/tun3d .",
            "upvotes": 12,
            "discussionId": "68da66cbcfa03a5158a1edb5",
            "projectPage": "https://bulatko.github.io/tun3d/",
            "githubRepo": "https://github.com/col14m/TUN3D",
            "ai_summary": "TUN3D is a method for joint layout estimation and 3D object detection using multi-view images without depth sensors or ground-truth camera poses, achieving state-of-the-art performance on indoor scene understanding benchmarks.",
            "ai_keywords": [
                "sparse-convolutional backbone",
                "parametric wall representation",
                "3D object detection",
                "layout estimation",
                "indoor scene understanding",
                "multi-view images"
            ],
            "githubStars": 11
        },
        "publishedAt": "2025-09-23T16:24:07.000Z",
        "title": "TUN3D: Towards Real-World Scene Understanding from Unposed Images",
        "summary": "Layout estimation and 3D object detection are two fundamental tasks in indoor\nscene understanding. When combined, they enable the creation of a compact yet\nsemantically rich spatial representation of a scene. Existing approaches\ntypically rely on point cloud input, which poses a major limitation since most\nconsumer cameras lack depth sensors and visual-only data remains far more\ncommon. We address this issue with TUN3D, the first method that tackles joint\nlayout estimation and 3D object detection in real scans, given multi-view\nimages as input, and does not require ground-truth camera poses or depth\nsupervision. Our approach builds on a lightweight sparse-convolutional backbone\nand employs two dedicated heads: one for 3D object detection and one for layout\nestimation, leveraging a novel and effective parametric wall representation.\nExtensive experiments show that TUN3D achieves state-of-the-art performance\nacross three challenging scene understanding benchmarks: (i) using ground-truth\npoint clouds, (ii) using posed images, and (iii) using unposed images. While\nperforming on par with specialized 3D object detection methods, TUN3D\nsignificantly advances layout estimation, setting a new benchmark in holistic\nindoor scene understanding. Code is available at\nhttps://github.com/col14m/tun3d .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21388.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "665b10fb270e47e678f2ddf1",
            "avatarUrl": "/avatars/1bc7a9211acf767f7bfca998c24315a0.svg",
            "fullname": "max",
            "name": "maksimko123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22072",
            "authors": [
                {
                    "_id": "68d9e8d90177a6054b013a2a",
                    "name": "Wanli Yang",
                    "hidden": false
                },
                {
                    "_id": "68d9e8d90177a6054b013a2b",
                    "name": "Fei Sun",
                    "hidden": false
                },
                {
                    "_id": "68d9e8d90177a6054b013a2c",
                    "name": "Rui Tang",
                    "hidden": false
                },
                {
                    "_id": "68d9e8d90177a6054b013a2d",
                    "name": "Hongyu Zang",
                    "hidden": false
                },
                {
                    "_id": "68d9e8d90177a6054b013a2e",
                    "name": "Du Su",
                    "hidden": false
                },
                {
                    "_id": "68d9e8d90177a6054b013a2f",
                    "name": "Qi Cao",
                    "hidden": false
                },
                {
                    "_id": "68d9e8d90177a6054b013a30",
                    "name": "Jingang Wang",
                    "hidden": false
                },
                {
                    "_id": "68d9e8d90177a6054b013a31",
                    "name": "Huawei Shen",
                    "hidden": false
                },
                {
                    "_id": "68d9e8d90177a6054b013a32",
                    "name": "Xueqi Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T08:53:13.000Z",
            "submittedOnDailyAt": "2025-09-29T10:32:23.190Z",
            "title": "Fine-tuning Done Right in Model Editing",
            "submittedOnDailyBy": {
                "_id": "68da4c0442241d2b33e8c221",
                "avatarUrl": "/avatars/4a4846028e5963cabf0f15111631a77a.svg",
                "isPro": false,
                "fullname": "Fei Sun",
                "user": "feisun-sf",
                "type": "user"
            },
            "summary": "Fine-tuning, a foundational method for adapting large language models, has\nlong been considered ineffective for model editing. Here, we challenge this\nbelief, arguing that the reported failure arises not from the inherent\nlimitation of fine-tuning itself, but from adapting it to the sequential nature\nof the editing task, a single-pass depth-first pipeline that optimizes each\nsample to convergence before moving on. While intuitive, this depth-first\npipeline coupled with sample-wise updating over-optimizes each edit and induces\ninterference across edits. Our controlled experiments reveal that simply\nrestoring fine-tuning to the standard breadth-first (i.e., epoch-based)\npipeline with mini-batch optimization substantially improves its effectiveness\nfor model editing. Moreover, fine-tuning in editing also suffers from\nsuboptimal tuning parameter locations inherited from prior methods. Through\nsystematic analysis of tuning locations, we derive LocFT-BF, a simple and\neffective localized editing method built on the restored fine-tuning framework.\nExtensive experiments across diverse LLMs and datasets demonstrate that\nLocFT-BF outperforms state-of-the-art methods by large margins. Notably, to our\nknowledge, it is the first to sustain 100K edits and 72B-parameter models,10 x\nbeyond prior practice, without sacrificing general capabilities. By clarifying\na long-standing misconception and introducing a principled localized tuning\nstrategy, we advance fine-tuning from an underestimated baseline to a leading\nmethod for model editing, establishing a solid foundation for future research.",
            "upvotes": 11,
            "discussionId": "68d9e8da0177a6054b013a33",
            "ai_summary": "Restoring fine-tuning to a breadth-first pipeline with mini-batch optimization and localized tuning parameters improves its effectiveness for model editing, outperforming state-of-the-art methods.",
            "ai_keywords": [
                "fine-tuning",
                "model editing",
                "depth-first pipeline",
                "breadth-first pipeline",
                "mini-batch optimization",
                "LocFT-BF",
                "localized editing",
                "LLMs",
                "parameter-efficient fine-tuning"
            ]
        },
        "publishedAt": "2025-09-26T04:53:13.000Z",
        "title": "Fine-tuning Done Right in Model Editing",
        "summary": "Fine-tuning, a foundational method for adapting large language models, has\nlong been considered ineffective for model editing. Here, we challenge this\nbelief, arguing that the reported failure arises not from the inherent\nlimitation of fine-tuning itself, but from adapting it to the sequential nature\nof the editing task, a single-pass depth-first pipeline that optimizes each\nsample to convergence before moving on. While intuitive, this depth-first\npipeline coupled with sample-wise updating over-optimizes each edit and induces\ninterference across edits. Our controlled experiments reveal that simply\nrestoring fine-tuning to the standard breadth-first (i.e., epoch-based)\npipeline with mini-batch optimization substantially improves its effectiveness\nfor model editing. Moreover, fine-tuning in editing also suffers from\nsuboptimal tuning parameter locations inherited from prior methods. Through\nsystematic analysis of tuning locations, we derive LocFT-BF, a simple and\neffective localized editing method built on the restored fine-tuning framework.\nExtensive experiments across diverse LLMs and datasets demonstrate that\nLocFT-BF outperforms state-of-the-art methods by large margins. Notably, to our\nknowledge, it is the first to sustain 100K edits and 72B-parameter models,10 x\nbeyond prior practice, without sacrificing general capabilities. By clarifying\na long-standing misconception and introducing a principled localized tuning\nstrategy, we advance fine-tuning from an underestimated baseline to a leading\nmethod for model editing, establishing a solid foundation for future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22072.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68da4c0442241d2b33e8c221",
            "avatarUrl": "/avatars/4a4846028e5963cabf0f15111631a77a.svg",
            "fullname": "Fei Sun",
            "name": "feisun-sf",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "submitterOrganization": {
            "_id": "632fea4a9c9aa2bfdf5982f8",
            "name": "UCAS",
            "fullname": "ucas",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632fe99f2a6ef6fb4ad7ba08/QiMtq1UkcKsI9yy1ZCU1m.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.21799",
            "authors": [
                {
                    "_id": "68d9f61a0177a6054b013b2f",
                    "name": "Hongze Mi",
                    "hidden": false
                },
                {
                    "_id": "68d9f61a0177a6054b013b30",
                    "name": "Yibo Feng",
                    "hidden": false
                },
                {
                    "_id": "68d9f61a0177a6054b013b31",
                    "name": "Wenjie Lu",
                    "hidden": false
                },
                {
                    "_id": "68d9f61a0177a6054b013b32",
                    "name": "Yuqi Wang",
                    "hidden": false
                },
                {
                    "_id": "68d9f61a0177a6054b013b33",
                    "name": "Jinyuan Li",
                    "hidden": false
                },
                {
                    "_id": "68d9f61a0177a6054b013b34",
                    "name": "Song Cao",
                    "hidden": false
                },
                {
                    "_id": "68d9f61a0177a6054b013b35",
                    "name": "He Cui",
                    "hidden": false
                },
                {
                    "_id": "68d9f61a0177a6054b013b36",
                    "name": "Tengfei Tian",
                    "hidden": false
                },
                {
                    "_id": "68d9f61a0177a6054b013b37",
                    "name": "Xuelin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d9f61a0177a6054b013b38",
                    "name": "Haotian Luo",
                    "hidden": false
                },
                {
                    "_id": "68d9f61a0177a6054b013b39",
                    "name": "Di Sun",
                    "hidden": false
                },
                {
                    "_id": "68d9f61a0177a6054b013b3a",
                    "name": "Naiqiang Tan",
                    "hidden": false
                },
                {
                    "_id": "68d9f61a0177a6054b013b3b",
                    "name": "Gang Pan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T02:56:19.000Z",
            "submittedOnDailyAt": "2025-09-29T01:30:08.939Z",
            "title": "D-Artemis: A Deliberative Cognitive Framework for Mobile GUI\n  Multi-Agents",
            "submittedOnDailyBy": {
                "_id": "632ab8f5a968c34257da5c52",
                "avatarUrl": "/avatars/59df09e6c9e1e633170514d950ad7981.svg",
                "isPro": false,
                "fullname": "Haotian Luo",
                "user": "LordNoah",
                "type": "user"
            },
            "summary": "Graphical User Interface (GUI) agents aim to automate a wide spectrum of\nhuman tasks by emulating user interaction. Despite rapid advancements, current\napproaches are hindered by several critical challenges: data bottleneck in\nend-to-end training, high cost of delayed error detection, and risk of\ncontradictory guidance. Inspired by the human cognitive loop of Thinking,\nAlignment, and Reflection, we present D-Artemis -- a novel deliberative\nframework in this paper. D-Artemis leverages a fine-grained, app-specific tip\nretrieval mechanism to inform its decision-making process. It also employs a\nproactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC)\nCheck module and Action Correction Agent (ACA) work in concert to mitigate the\nrisk of execution failures. A post-execution Status Reflection Agent (SRA)\ncompletes the cognitive loop, enabling strategic learning from experience.\nCrucially, D-Artemis enhances the capabilities of general-purpose Multimodal\nlarge language models (MLLMs) for GUI tasks without the need for training on\ncomplex trajectory datasets, demonstrating strong generalization. D-Artemis\nestablishes new state-of-the-art (SOTA) results across both major benchmarks,\nachieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2.\nExtensive ablation studies further demonstrate the significant contribution of\neach component to the framework.",
            "upvotes": 8,
            "discussionId": "68d9f61a0177a6054b013b3c",
            "ai_summary": "D-Artemis, a novel deliberative framework, enhances GUI automation by leveraging app-specific tips, proactive alignment, and reflection, achieving state-of-the-art results with general-purpose multimodal large language models.",
            "ai_keywords": [
                "deliberative framework",
                "fine-grained",
                "app-specific tip retrieval",
                "Thought-Action Consistency (TAC) Check",
                "Action Correction Agent (ACA)",
                "Status Reflection Agent (SRA)",
                "Multimodal large language models (MLLMs)",
                "GUI tasks",
                "AndroidWorld",
                "ScreenSpot-V2"
            ]
        },
        "publishedAt": "2025-09-25T22:56:19.000Z",
        "title": "D-Artemis: A Deliberative Cognitive Framework for Mobile GUI\n  Multi-Agents",
        "summary": "Graphical User Interface (GUI) agents aim to automate a wide spectrum of\nhuman tasks by emulating user interaction. Despite rapid advancements, current\napproaches are hindered by several critical challenges: data bottleneck in\nend-to-end training, high cost of delayed error detection, and risk of\ncontradictory guidance. Inspired by the human cognitive loop of Thinking,\nAlignment, and Reflection, we present D-Artemis -- a novel deliberative\nframework in this paper. D-Artemis leverages a fine-grained, app-specific tip\nretrieval mechanism to inform its decision-making process. It also employs a\nproactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC)\nCheck module and Action Correction Agent (ACA) work in concert to mitigate the\nrisk of execution failures. A post-execution Status Reflection Agent (SRA)\ncompletes the cognitive loop, enabling strategic learning from experience.\nCrucially, D-Artemis enhances the capabilities of general-purpose Multimodal\nlarge language models (MLLMs) for GUI tasks without the need for training on\ncomplex trajectory datasets, demonstrating strong generalization. D-Artemis\nestablishes new state-of-the-art (SOTA) results across both major benchmarks,\nachieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2.\nExtensive ablation studies further demonstrate the significant contribution of\neach component to the framework.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21799.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "632ab8f5a968c34257da5c52",
            "avatarUrl": "/avatars/59df09e6c9e1e633170514d950ad7981.svg",
            "fullname": "Haotian Luo",
            "name": "LordNoah",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.21760",
            "authors": [
                {
                    "_id": "68d9e7890177a6054b013a25",
                    "name": "Lan Chen",
                    "hidden": false
                },
                {
                    "_id": "68d9e7890177a6054b013a26",
                    "name": "Yuchao Gu",
                    "hidden": false
                },
                {
                    "_id": "68d9e7890177a6054b013a27",
                    "user": {
                        "_id": "6388a7e98a5dbe2f3dc61faa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6388a7e98a5dbe2f3dc61faa/Zj8pR7BG_wOQjsYzosb5p.jpeg",
                        "isPro": false,
                        "fullname": "Qi Mao",
                        "user": "HelenMao",
                        "type": "user"
                    },
                    "name": "Qi Mao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T06:00:42.049Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T01:43:40.000Z",
            "submittedOnDailyAt": "2025-09-29T00:29:37.279Z",
            "title": "UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models",
            "submittedOnDailyBy": {
                "_id": "640d704c8036cc2142299c19",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
                "isPro": false,
                "fullname": "Lan Chen",
                "user": "Orannue",
                "type": "user"
            },
            "summary": "Large language models, trained on extensive corpora, successfully unify\ndiverse linguistic tasks within a single generative framework. Inspired by\nthis, recent works like Large Vision Model (LVM) extend this paradigm to vision\nby organizing tasks into sequential visual sentences, where visual prompts\nserve as the context to guide outputs. However, such modeling requires\ntask-specific pre-training across modalities and sources, which is costly and\nlimits scalability to unseen tasks. Given that pre-trained video generation\nmodels inherently capture temporal sequence dependencies, we explore a more\nunified and scalable alternative: can a pre-trained video generation model\nadapt to diverse image and video tasks? To answer this, we propose UniVid, a\nframework that fine-tunes a video diffusion transformer to handle various\nvision tasks without task-specific modifications. Tasks are represented as\nvisual sentences, where the context sequence defines both the task and the\nexpected output modality. We evaluate the generalization of UniVid from two\nperspectives: (1) cross-modal inference with contexts composed of both images\nand videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks\nfrom natural to annotated data, without multi-source pre-training. Despite\nbeing trained solely on natural video data, UniVid generalizes well in both\nsettings. Notably, understanding and generation tasks can easily switch by\nsimply reversing the visual sentence order in this paradigm. These findings\nhighlight the potential of pre-trained video generation models to serve as a\nscalable and unified foundation for vision modeling. Our code will be released\nat https://github.com/CUC-MIPG/UniVid.",
            "upvotes": 8,
            "discussionId": "68d9e78a0177a6054b013a28",
            "githubRepo": "https://github.com/CUC-MIPG/UniVid",
            "ai_summary": "A pre-trained video diffusion transformer, UniVid, is fine-tuned to handle diverse vision tasks without task-specific modifications, demonstrating cross-modal and cross-source generalization.",
            "ai_keywords": [
                "Large Vision Model",
                "visual sentences",
                "video diffusion transformer",
                "UniVid",
                "cross-modal inference",
                "cross-source tasks",
                "natural video data",
                "understanding and generation tasks"
            ],
            "githubStars": 19
        },
        "publishedAt": "2025-09-25T21:43:40.000Z",
        "title": "UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models",
        "summary": "Large language models, trained on extensive corpora, successfully unify\ndiverse linguistic tasks within a single generative framework. Inspired by\nthis, recent works like Large Vision Model (LVM) extend this paradigm to vision\nby organizing tasks into sequential visual sentences, where visual prompts\nserve as the context to guide outputs. However, such modeling requires\ntask-specific pre-training across modalities and sources, which is costly and\nlimits scalability to unseen tasks. Given that pre-trained video generation\nmodels inherently capture temporal sequence dependencies, we explore a more\nunified and scalable alternative: can a pre-trained video generation model\nadapt to diverse image and video tasks? To answer this, we propose UniVid, a\nframework that fine-tunes a video diffusion transformer to handle various\nvision tasks without task-specific modifications. Tasks are represented as\nvisual sentences, where the context sequence defines both the task and the\nexpected output modality. We evaluate the generalization of UniVid from two\nperspectives: (1) cross-modal inference with contexts composed of both images\nand videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks\nfrom natural to annotated data, without multi-source pre-training. Despite\nbeing trained solely on natural video data, UniVid generalizes well in both\nsettings. Notably, understanding and generation tasks can easily switch by\nsimply reversing the visual sentence order in this paradigm. These findings\nhighlight the potential of pre-trained video generation models to serve as a\nscalable and unified foundation for vision modeling. Our code will be released\nat https://github.com/CUC-MIPG/UniVid.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21760.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640d704c8036cc2142299c19",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
            "fullname": "Lan Chen",
            "name": "Orannue",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.21500",
            "authors": [
                {
                    "_id": "68da0e7e0177a6054b013b9b",
                    "name": "Junkai Zhang",
                    "hidden": false
                },
                {
                    "_id": "68da0e7e0177a6054b013b9c",
                    "name": "Zihao Wang",
                    "hidden": false
                },
                {
                    "_id": "68da0e7e0177a6054b013b9d",
                    "name": "Lin Gui",
                    "hidden": false
                },
                {
                    "_id": "68da0e7e0177a6054b013b9e",
                    "name": "Swarnashree Mysore Sathyendra",
                    "hidden": false
                },
                {
                    "_id": "68da0e7e0177a6054b013b9f",
                    "name": "Jaehwan Jeong",
                    "hidden": false
                },
                {
                    "_id": "68da0e7e0177a6054b013ba0",
                    "name": "Victor Veitch",
                    "hidden": false
                },
                {
                    "_id": "68da0e7e0177a6054b013ba1",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "68da0e7e0177a6054b013ba2",
                    "name": "Yunzhong He",
                    "hidden": false
                },
                {
                    "_id": "68da0e7e0177a6054b013ba3",
                    "name": "Bing Liu",
                    "hidden": false
                },
                {
                    "_id": "68da0e7e0177a6054b013ba4",
                    "name": "Lifeng Jin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-25T19:57:39.000Z",
            "submittedOnDailyAt": "2025-09-29T03:18:17.265Z",
            "title": "Chasing the Tail: Effective Rubric-based Reward Modeling for Large\n  Language Model Post-Training",
            "submittedOnDailyBy": {
                "_id": "64e7bb81b159a6f87be99459",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e7bb81b159a6f87be99459/obeVMow9SRRNt722T6ZvU.jpeg",
                "isPro": false,
                "fullname": "Junkai Zhang",
                "user": "JunkaiZ",
                "type": "user"
            },
            "summary": "Reinforcement fine-tuning (RFT) often suffers from reward\nover-optimization, where a policy model hacks the reward signals to achieve\nhigh scores while producing low-quality outputs. Our theoretical analysis shows\nthat the key lies in reward misspecification at the high-reward tail: the\ninability to reliably distinguish Excellent responses from merely Great ones.\nThis motivate us to focus on the high-reward region. However, such tail\nexamples are scarce under the base LLM. While off-policy exemplars (e.g. from\nstronger models or rewrites) are easier to obtain, naively training on them\nyields a misspecified reward for the policy we aim to align. To address this,\nwe study rubric-based rewards. By design, rubrics can leverage off-policy\nexamples while remaining insensitive to their artifacts. To elicit rubrics that\ncapture the high-reward tail, we highlight the importance of distinguishing\namong great and diverse responses, and introduce a workflow to implement this\nidea. We empirically demonstrate that rubric-based rewards substantially\nmitigate reward over-optimization and deliver effective LLM post-training\nimprovements. Our code can be accessed at\nhttps://github.com/Jun-Kai-Zhang/rubrics.git .",
            "upvotes": 8,
            "discussionId": "68da0e7f0177a6054b013ba5",
            "ai_summary": "Rubric-based rewards mitigate reward over-optimization in reinforcement fine-tuning by leveraging off-policy examples while maintaining reward reliability.",
            "ai_keywords": [
                "reinforcement fine-tuning",
                "reward over-optimization",
                "policy model",
                "reward misspecification",
                "high-reward region",
                "base LLM",
                "off-policy exemplars",
                "rubric-based rewards",
                "reward alignment",
                "LLM post-training improvements"
            ]
        },
        "publishedAt": "2025-09-25T15:57:39.000Z",
        "title": "Chasing the Tail: Effective Rubric-based Reward Modeling for Large\n  Language Model Post-Training",
        "summary": "Reinforcement fine-tuning (RFT) often suffers from reward\nover-optimization, where a policy model hacks the reward signals to achieve\nhigh scores while producing low-quality outputs. Our theoretical analysis shows\nthat the key lies in reward misspecification at the high-reward tail: the\ninability to reliably distinguish Excellent responses from merely Great ones.\nThis motivate us to focus on the high-reward region. However, such tail\nexamples are scarce under the base LLM. While off-policy exemplars (e.g. from\nstronger models or rewrites) are easier to obtain, naively training on them\nyields a misspecified reward for the policy we aim to align. To address this,\nwe study rubric-based rewards. By design, rubrics can leverage off-policy\nexamples while remaining insensitive to their artifacts. To elicit rubrics that\ncapture the high-reward tail, we highlight the importance of distinguishing\namong great and diverse responses, and introduce a workflow to implement this\nidea. We empirically demonstrate that rubric-based rewards substantially\nmitigate reward over-optimization and deliver effective LLM post-training\nimprovements. Our code can be accessed at\nhttps://github.com/Jun-Kai-Zhang/rubrics.git .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21500.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e7bb81b159a6f87be99459",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e7bb81b159a6f87be99459/obeVMow9SRRNt722T6ZvU.jpeg",
            "fullname": "Junkai Zhang",
            "name": "JunkaiZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "submitterOrganization": {
            "_id": "6677220f8a4064c02bc81217",
            "name": "ScaleAI",
            "fullname": "Scale AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65d6a5f94c28026a003581b4/uqHyTuNQ8fX7LheVhzPeO.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22601",
            "authors": [
                {
                    "_id": "68d9ee7d0177a6054b013abb",
                    "name": "Yulei Qin",
                    "hidden": false
                },
                {
                    "_id": "68d9ee7d0177a6054b013abc",
                    "name": "Xiaoyu Tan",
                    "hidden": false
                },
                {
                    "_id": "68d9ee7d0177a6054b013abd",
                    "name": "Zhengbao He",
                    "hidden": false
                },
                {
                    "_id": "68d9ee7d0177a6054b013abe",
                    "name": "Gang Li",
                    "hidden": false
                },
                {
                    "_id": "68d9ee7d0177a6054b013abf",
                    "name": "Haojia Lin",
                    "hidden": false
                },
                {
                    "_id": "68d9ee7d0177a6054b013ac0",
                    "name": "Zongyi Li",
                    "hidden": false
                },
                {
                    "_id": "68d9ee7d0177a6054b013ac1",
                    "name": "Zihan Xu",
                    "hidden": false
                },
                {
                    "_id": "68d9ee7d0177a6054b013ac2",
                    "name": "Yuchen Shi",
                    "hidden": false
                },
                {
                    "_id": "68d9ee7d0177a6054b013ac3",
                    "name": "Siqi Cai",
                    "hidden": false
                },
                {
                    "_id": "68d9ee7d0177a6054b013ac4",
                    "name": "Renting Rui",
                    "hidden": false
                },
                {
                    "_id": "68d9ee7d0177a6054b013ac5",
                    "name": "Shaofei Cai",
                    "hidden": false
                },
                {
                    "_id": "68d9ee7d0177a6054b013ac6",
                    "name": "Yuzheng Cai",
                    "hidden": false
                },
                {
                    "_id": "68d9ee7d0177a6054b013ac7",
                    "name": "Xuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d9ee7d0177a6054b013ac8",
                    "name": "Sheng Ye",
                    "hidden": false
                },
                {
                    "_id": "68d9ee7d0177a6054b013ac9",
                    "name": "Ke Li",
                    "hidden": false
                },
                {
                    "_id": "68d9ee7d0177a6054b013aca",
                    "user": {
                        "_id": "647401e50da364bd0d002f2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png",
                        "isPro": false,
                        "fullname": "XING SUN",
                        "user": "tedsun",
                        "type": "user"
                    },
                    "name": "Xing Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T11:26:42.729Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T17:20:38.000Z",
            "submittedOnDailyAt": "2025-09-29T00:57:22.284Z",
            "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive\n  Exploration for Agentic Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic\ntool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,\nyet it faces a fundamental challenge of exploration-exploitation trade-off.\nExisting studies stimulate exploration through the lens of policy entropy, but\nsuch mechanical entropy maximization is prone to RL training instability due to\nthe multi-turn distribution shifting. In this paper, we target the progressive\nexploration-exploitation balance under the guidance of the agent own\nexperiences without succumbing to either entropy collapsing or runaway\ndivergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)\nrecipe for training agentic LLMs. It extends the vanilla SIL framework, where a\nreplay buffer stores self-generated promising trajectories for off-policy\nupdate, by gradually steering the policy evolution within a well-balanced range\nof entropy across stages. Specifically, our approach incorporates a curriculum\nto manage the exploration process, utilizing intrinsic rewards to foster\nskill-level exploration and facilitating action-level exploration through SIL.\nAt first, the auxiliary tool call reward plays a critical role in the\naccumulation of tool-use skills, enabling broad exposure to the unfamiliar\ndistributions of the environment feedback with an upward entropy trend. As\ntraining progresses, self-imitation gets strengthened to exploit existing\nsuccessful patterns from replayed experiences for comparative action-level\nexploration, accelerating solution iteration without unbounded entropy growth.\nTo further stabilize training, we recalibrate the advantages of experiences in\nthe replay buffer to address the potential policy drift. Reugularizations such\nas the clipping of tokens with high covariance between probability and\nadvantage are introduced to the trajectory-level entropy control to curb\nover-confidence.",
            "upvotes": 5,
            "discussionId": "68d9ee7e0177a6054b013acb",
            "ai_summary": "SPEAR, a curriculum-based self-imitation learning method, manages exploration-exploitation balance in reinforcement learning for LLMs by using intrinsic rewards and trajectory-level entropy control.",
            "ai_keywords": [
                "reinforcement learning",
                "exploration-exploitation trade-off",
                "policy entropy",
                "curriculum-based self-imitation learning",
                "replay buffer",
                "off-policy update",
                "intrinsic rewards",
                "self-imitation",
                "entropy control",
                "policy drift",
                "token clipping"
            ]
        },
        "publishedAt": "2025-09-26T13:20:38.000Z",
        "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive\n  Exploration for Agentic Reinforcement Learning",
        "summary": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic\ntool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,\nyet it faces a fundamental challenge of exploration-exploitation trade-off.\nExisting studies stimulate exploration through the lens of policy entropy, but\nsuch mechanical entropy maximization is prone to RL training instability due to\nthe multi-turn distribution shifting. In this paper, we target the progressive\nexploration-exploitation balance under the guidance of the agent own\nexperiences without succumbing to either entropy collapsing or runaway\ndivergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)\nrecipe for training agentic LLMs. It extends the vanilla SIL framework, where a\nreplay buffer stores self-generated promising trajectories for off-policy\nupdate, by gradually steering the policy evolution within a well-balanced range\nof entropy across stages. Specifically, our approach incorporates a curriculum\nto manage the exploration process, utilizing intrinsic rewards to foster\nskill-level exploration and facilitating action-level exploration through SIL.\nAt first, the auxiliary tool call reward plays a critical role in the\naccumulation of tool-use skills, enabling broad exposure to the unfamiliar\ndistributions of the environment feedback with an upward entropy trend. As\ntraining progresses, self-imitation gets strengthened to exploit existing\nsuccessful patterns from replayed experiences for comparative action-level\nexploration, accelerating solution iteration without unbounded entropy growth.\nTo further stabilize training, we recalibrate the advantages of experiences in\nthe replay buffer to address the potential policy drift. Reugularizations such\nas the clipping of tokens with high covariance between probability and\nadvantage are introduced to the trajectory-level entropy control to curb\nover-confidence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22601.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 112
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.21574",
            "authors": [
                {
                    "_id": "68d9ed0d0177a6054b013a9f",
                    "name": "You Xie",
                    "hidden": false
                },
                {
                    "_id": "68d9ed0d0177a6054b013aa0",
                    "user": {
                        "_id": "6435ec65f3b08e267d9c9bbd",
                        "avatarUrl": "/avatars/7e846e0c91241898171aa35efd7264c0.svg",
                        "isPro": false,
                        "fullname": "Tianpei Gu",
                        "user": "gutianpei",
                        "type": "user"
                    },
                    "name": "Tianpei Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:58:34.638Z",
                    "hidden": false
                },
                {
                    "_id": "68d9ed0d0177a6054b013aa1",
                    "name": "Zenan Li",
                    "hidden": false
                },
                {
                    "_id": "68d9ed0d0177a6054b013aa2",
                    "name": "Chenxu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d9ed0d0177a6054b013aa3",
                    "name": "Guoxian Song",
                    "hidden": false
                },
                {
                    "_id": "68d9ed0d0177a6054b013aa4",
                    "name": "Xiaochen Zhao",
                    "hidden": false
                },
                {
                    "_id": "68d9ed0d0177a6054b013aa5",
                    "name": "Chao Liang",
                    "hidden": false
                },
                {
                    "_id": "68d9ed0d0177a6054b013aa6",
                    "name": "Jianwen Jiang",
                    "hidden": false
                },
                {
                    "_id": "68d9ed0d0177a6054b013aa7",
                    "name": "Hongyi Xu",
                    "hidden": false
                },
                {
                    "_id": "68d9ed0d0177a6054b013aa8",
                    "name": "Linjie Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-25T20:53:27.000Z",
            "submittedOnDailyAt": "2025-09-29T00:51:11.933Z",
            "title": "X-Streamer: Unified Human World Modeling with Audiovisual Interaction",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce X-Streamer, an end-to-end multimodal human world modeling\nframework for building digital human agents capable of infinite interactions\nacross text, speech, and video within a single unified architecture. Starting\nfrom a single portrait, X-Streamer enables real-time, open-ended video calls\ndriven by streaming multimodal inputs. At its core is a Thinker-Actor\ndual-transformer architecture that unifies multimodal understanding and\ngeneration, turning a static portrait into persistent and intelligent\naudiovisual interactions. The Thinker module perceives and reasons over\nstreaming user inputs, while its hidden states are translated by the Actor into\nsynchronized multimodal streams in real time. Concretely, the Thinker leverages\na pretrained large language-speech model, while the Actor employs a chunk-wise\nautoregressive diffusion model that cross-attends to the Thinker's hidden\nstates to produce time-aligned multimodal responses with interleaved discrete\ntext and audio tokens and continuous video latents. To ensure long-horizon\nstability, we design inter- and intra-chunk attentions with time-aligned\nmultimodal positional embeddings for fine-grained cross-modality alignment and\ncontext retention, further reinforced by chunk-wise diffusion forcing and\nglobal identity referencing. X-Streamer runs in real time on two A100 GPUs,\nsustaining hours-long consistent video chat experiences from arbitrary\nportraits and paving the way toward unified world modeling of interactive\ndigital humans.",
            "upvotes": 5,
            "discussionId": "68d9ed0d0177a6054b013aa9",
            "projectPage": "https://byteaigc.github.io/X-Streamer/",
            "ai_summary": "X-Streamer is a unified multimodal framework using dual-transformer architecture for real-time, open-ended interactions across text, speech, and video, leveraging large language-speech models and autoregressive diffusion models.",
            "ai_keywords": [
                "multimodal human world modeling",
                "Thinker-Actor dual-transformer architecture",
                "Thinker module",
                "Actor module",
                "pretrained large language-speech model",
                "chunk-wise autoregressive diffusion model",
                "cross-attention",
                "time-aligned multimodal responses",
                "discrete text and audio tokens",
                "continuous video latents",
                "inter-chunk attentions",
                "intra-chunk attentions",
                "time-aligned multimodal positional embeddings",
                "chunk-wise diffusion forcing",
                "global identity referencing"
            ]
        },
        "publishedAt": "2025-09-25T16:53:27.000Z",
        "title": "X-Streamer: Unified Human World Modeling with Audiovisual Interaction",
        "summary": "We introduce X-Streamer, an end-to-end multimodal human world modeling\nframework for building digital human agents capable of infinite interactions\nacross text, speech, and video within a single unified architecture. Starting\nfrom a single portrait, X-Streamer enables real-time, open-ended video calls\ndriven by streaming multimodal inputs. At its core is a Thinker-Actor\ndual-transformer architecture that unifies multimodal understanding and\ngeneration, turning a static portrait into persistent and intelligent\naudiovisual interactions. The Thinker module perceives and reasons over\nstreaming user inputs, while its hidden states are translated by the Actor into\nsynchronized multimodal streams in real time. Concretely, the Thinker leverages\na pretrained large language-speech model, while the Actor employs a chunk-wise\nautoregressive diffusion model that cross-attends to the Thinker's hidden\nstates to produce time-aligned multimodal responses with interleaved discrete\ntext and audio tokens and continuous video latents. To ensure long-horizon\nstability, we design inter- and intra-chunk attentions with time-aligned\nmultimodal positional embeddings for fine-grained cross-modality alignment and\ncontext retention, further reinforced by chunk-wise diffusion forcing and\nglobal identity referencing. X-Streamer runs in real time on two A100 GPUs,\nsustaining hours-long consistent video chat experiences from arbitrary\nportraits and paving the way toward unified world modeling of interactive\ndigital humans.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21574.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 112
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.20787",
            "authors": [
                {
                    "_id": "68da43a90177a6054b013c66",
                    "name": "Shihua Huang",
                    "hidden": false
                },
                {
                    "_id": "68da43a90177a6054b013c67",
                    "name": "Yongjie Hou",
                    "hidden": false
                },
                {
                    "_id": "68da43a90177a6054b013c68",
                    "name": "Longfei Liu",
                    "hidden": false
                },
                {
                    "_id": "68da43a90177a6054b013c69",
                    "name": "Xuanlong Yu",
                    "hidden": false
                },
                {
                    "_id": "68da43a90177a6054b013c6a",
                    "name": "Xi Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-25T06:14:00.000Z",
            "submittedOnDailyAt": "2025-09-29T07:01:11.580Z",
            "title": "Real-Time Object Detection Meets DINOv3",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": true,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "Benefiting from the simplicity and effectiveness of Dense O2O and MAL, DEIM\nhas become the mainstream training framework for real-time DETRs, significantly\noutperforming the YOLO series. In this work, we extend it with DINOv3 features,\nresulting in DEIMv2. DEIMv2 spans eight model sizes from X to Atto, covering\nGPU, edge, and mobile deployment. For the X, L, M, and S variants, we adopt\nDINOv3-pretrained or distilled backbones and introduce a Spatial Tuning Adapter\n(STA), which efficiently converts DINOv3's single-scale output into multi-scale\nfeatures and complements strong semantics with fine-grained details to enhance\ndetection. For ultra-lightweight models (Nano, Pico, Femto, and Atto), we\nemploy HGNetv2 with depth and width pruning to meet strict resource budgets.\nTogether with a simplified decoder and an upgraded Dense O2O, this unified\ndesign enables DEIMv2 to achieve a superior performance-cost trade-off across\ndiverse scenarios, establishing new state-of-the-art results. Notably, our\nlargest model, DEIMv2-X, achieves 57.8 AP with only 50.3 million parameters,\nsurpassing prior X-scale models that require over 60 million parameters for\njust 56.5 AP. On the compact side, DEIMv2-S is the first sub-10 million model\n(9.71 million) to exceed the 50 AP milestone on COCO, reaching 50.9 AP. Even\nthe ultra-lightweight DEIMv2-Pico, with just 1.5 million parameters, delivers\n38.5 AP, matching YOLOv10-Nano (2.3 million) with around 50 percent fewer\nparameters. Our code and pre-trained models are available at\nhttps://github.com/Intellindust-AI-Lab/DEIMv2",
            "upvotes": 4,
            "discussionId": "68da43aa0177a6054b013c6b",
            "projectPage": "https://intellindust-ai-lab.github.io/projects/DEIMv2/",
            "githubRepo": "https://github.com/Intellindust-AI-Lab/DEIMv2",
            "ai_summary": "DEIMv2, an extended version of DEIM with DINOv3 features, achieves superior performance-cost trade-offs across diverse deployment scenarios, including GPU, edge, and mobile, by using Spatial Tuning Adapter and HGNetv2 with pruning.",
            "ai_keywords": [
                "Dense O2O",
                "MAL",
                "DETRs",
                "YOLO",
                "DINOv3",
                "Spatial Tuning Adapter",
                "STA",
                "HGNetv2",
                "depth and width pruning",
                "Dense O2O",
                "AP",
                "COCO"
            ],
            "githubStars": 370
        },
        "publishedAt": "2025-09-25T02:14:00.000Z",
        "title": "Real-Time Object Detection Meets DINOv3",
        "summary": "Benefiting from the simplicity and effectiveness of Dense O2O and MAL, DEIM\nhas become the mainstream training framework for real-time DETRs, significantly\noutperforming the YOLO series. In this work, we extend it with DINOv3 features,\nresulting in DEIMv2. DEIMv2 spans eight model sizes from X to Atto, covering\nGPU, edge, and mobile deployment. For the X, L, M, and S variants, we adopt\nDINOv3-pretrained or distilled backbones and introduce a Spatial Tuning Adapter\n(STA), which efficiently converts DINOv3's single-scale output into multi-scale\nfeatures and complements strong semantics with fine-grained details to enhance\ndetection. For ultra-lightweight models (Nano, Pico, Femto, and Atto), we\nemploy HGNetv2 with depth and width pruning to meet strict resource budgets.\nTogether with a simplified decoder and an upgraded Dense O2O, this unified\ndesign enables DEIMv2 to achieve a superior performance-cost trade-off across\ndiverse scenarios, establishing new state-of-the-art results. Notably, our\nlargest model, DEIMv2-X, achieves 57.8 AP with only 50.3 million parameters,\nsurpassing prior X-scale models that require over 60 million parameters for\njust 56.5 AP. On the compact side, DEIMv2-S is the first sub-10 million model\n(9.71 million) to exceed the 50 AP milestone on COCO, reaching 50.9 AP. Even\nthe ultra-lightweight DEIMv2-Pico, with just 1.5 million parameters, delivers\n38.5 AP, matching YOLOv10-Nano (2.3 million) with around 50 percent fewer\nparameters. Our code and pre-trained models are available at\nhttps://github.com/Intellindust-AI-Lab/DEIMv2",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.20787.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 978
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22650",
            "authors": [
                {
                    "_id": "68da30c00177a6054b013c28",
                    "name": "Anna Kukleva",
                    "hidden": false
                },
                {
                    "_id": "68da30c00177a6054b013c29",
                    "user": {
                        "_id": "63412f2add8853dc7e306a4f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/nqbteeQhS2dPA5LVy7dTD.png",
                        "isPro": false,
                        "fullname": "Enis Simsar",
                        "user": "enisimsar",
                        "type": "user"
                    },
                    "name": "Enis Simsar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T11:26:19.128Z",
                    "hidden": false
                },
                {
                    "_id": "68da30c00177a6054b013c2a",
                    "name": "Alessio Tonioni",
                    "hidden": false
                },
                {
                    "_id": "68da30c00177a6054b013c2b",
                    "name": "Muhammad Ferjad Naeem",
                    "hidden": false
                },
                {
                    "_id": "68da30c00177a6054b013c2c",
                    "name": "Federico Tombari",
                    "hidden": false
                },
                {
                    "_id": "68da30c00177a6054b013c2d",
                    "name": "Jan Eric Lenssen",
                    "hidden": false
                },
                {
                    "_id": "68da30c00177a6054b013c2e",
                    "name": "Bernt Schiele",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T17:59:57.000Z",
            "submittedOnDailyAt": "2025-09-29T05:41:57.243Z",
            "title": "RefAM: Attention Magnets for Zero-Shot Referral Segmentation",
            "submittedOnDailyBy": {
                "_id": "63412f2add8853dc7e306a4f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/nqbteeQhS2dPA5LVy7dTD.png",
                "isPro": false,
                "fullname": "Enis Simsar",
                "user": "enisimsar",
                "type": "user"
            },
            "summary": "Most existing approaches to referring segmentation achieve strong performance\nonly through fine-tuning or by composing multiple pre-trained models, often at\nthe cost of additional training and architectural modifications. Meanwhile,\nlarge-scale generative diffusion models encode rich semantic information,\nmaking them attractive as general-purpose feature extractors. In this work, we\nintroduce a new method that directly exploits features, attention scores, from\ndiffusion transformers for downstream tasks, requiring neither architectural\nmodifications nor additional training. To systematically evaluate these\nfeatures, we extend benchmarks with vision-language grounding tasks spanning\nboth images and videos. Our key insight is that stop words act as attention\nmagnets: they accumulate surplus attention and can be filtered to reduce noise.\nMoreover, we identify global attention sinks (GAS) emerging in deeper layers\nand show that they can be safely suppressed or redirected onto auxiliary\ntokens, leading to sharper and more accurate grounding maps. We further propose\nan attention redistribution strategy, where appended stop words partition\nbackground activations into smaller clusters, yielding sharper and more\nlocalized heatmaps. Building on these findings, we develop RefAM, a simple\ntraining-free grounding framework that combines cross-attention maps, GAS\nhandling, and redistribution. Across zero-shot referring image and video\nsegmentation benchmarks, our approach consistently outperforms prior methods,\nestablishing a new state of the art without fine-tuning or additional\ncomponents.",
            "upvotes": 3,
            "discussionId": "68da30c00177a6054b013c2f",
            "projectPage": "https://refam-diffusion.github.io/",
            "ai_summary": "A new method leverages diffusion transformers' attention scores for referring segmentation without fine-tuning or additional training, improving performance through stop word filtering and attention redistribution.",
            "ai_keywords": [
                "diffusion transformers",
                "attention scores",
                "vision-language grounding",
                "global attention sinks",
                "attention redistribution",
                "cross-attention maps",
                "RefAM",
                "zero-shot referring image and video segmentation"
            ]
        },
        "publishedAt": "2025-09-26T13:59:57.000Z",
        "title": "RefAM: Attention Magnets for Zero-Shot Referral Segmentation",
        "summary": "Most existing approaches to referring segmentation achieve strong performance\nonly through fine-tuning or by composing multiple pre-trained models, often at\nthe cost of additional training and architectural modifications. Meanwhile,\nlarge-scale generative diffusion models encode rich semantic information,\nmaking them attractive as general-purpose feature extractors. In this work, we\nintroduce a new method that directly exploits features, attention scores, from\ndiffusion transformers for downstream tasks, requiring neither architectural\nmodifications nor additional training. To systematically evaluate these\nfeatures, we extend benchmarks with vision-language grounding tasks spanning\nboth images and videos. Our key insight is that stop words act as attention\nmagnets: they accumulate surplus attention and can be filtered to reduce noise.\nMoreover, we identify global attention sinks (GAS) emerging in deeper layers\nand show that they can be safely suppressed or redirected onto auxiliary\ntokens, leading to sharper and more accurate grounding maps. We further propose\nan attention redistribution strategy, where appended stop words partition\nbackground activations into smaller clusters, yielding sharper and more\nlocalized heatmaps. Building on these findings, we develop RefAM, a simple\ntraining-free grounding framework that combines cross-attention maps, GAS\nhandling, and redistribution. Across zero-shot referring image and video\nsegmentation benchmarks, our approach consistently outperforms prior methods,\nestablishing a new state of the art without fine-tuning or additional\ncomponents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22650.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63412f2add8853dc7e306a4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/nqbteeQhS2dPA5LVy7dTD.png",
            "fullname": "Enis Simsar",
            "name": "enisimsar",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.22642",
            "authors": [
                {
                    "_id": "68da280f0177a6054b013bf1",
                    "name": "Xiaowei Chi",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013bf2",
                    "user": {
                        "_id": "64ec26d02f70f2a4c7893367",
                        "avatarUrl": "/avatars/68957db5fa47bccbf28b59d0c721f1e6.svg",
                        "isPro": false,
                        "fullname": "Peidong Jia",
                        "user": "pdjia",
                        "type": "user"
                    },
                    "name": "Peidong Jia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-29T13:07:29.529Z",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013bf3",
                    "name": "Chun-Kai Fan",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013bf4",
                    "name": "Xiaozhu Ju",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013bf5",
                    "user": {
                        "_id": "646d84a4534e52f8c3075b28",
                        "avatarUrl": "/avatars/d6c8d0d1492cd2a140e1b1935be86b5c.svg",
                        "isPro": false,
                        "fullname": "Weishi MI",
                        "user": "hhhwmws",
                        "type": "user"
                    },
                    "name": "Weishi Mi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-29T13:07:47.562Z",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013bf6",
                    "name": "Kevin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013bf7",
                    "name": "Zhiyuan Qin",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013bf8",
                    "user": {
                        "_id": "64e850ae2c506d28d4864890",
                        "avatarUrl": "/avatars/e407cb310d96b50fce83bc8cf3071d1c.svg",
                        "isPro": false,
                        "fullname": "Wanxin Tian",
                        "user": "TianWanxin",
                        "type": "user"
                    },
                    "name": "Wanxin Tian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-29T13:08:09.998Z",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013bf9",
                    "user": {
                        "_id": "64c26e772152fe05f050d5a0",
                        "avatarUrl": "/avatars/72c773b0ca10d2f960af9ef0ecf5bfd5.svg",
                        "isPro": false,
                        "fullname": "Kuangzhi Ge",
                        "user": "MAMBA4L",
                        "type": "user"
                    },
                    "name": "Kuangzhi Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-29T13:08:43.658Z",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013bfa",
                    "name": "Hao Li",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013bfb",
                    "user": {
                        "_id": "67852e5a3d49517c54365945",
                        "avatarUrl": "/avatars/3ab8df5628e91bf89ac39f6fd6955c3e.svg",
                        "isPro": false,
                        "fullname": "Zezhong Qian",
                        "user": "XuWuLingYu",
                        "type": "user"
                    },
                    "name": "Zezhong Qian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-29T13:08:53.947Z",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013bfc",
                    "name": "Anthony Chen",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013bfd",
                    "name": "Qiang Zhou",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013bfe",
                    "name": "Yueru Jia",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013bff",
                    "name": "Jiaming Liu",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c00",
                    "name": "Yong Dai",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c01",
                    "name": "Qingpo Wuwu",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c02",
                    "user": {
                        "_id": "681402ff51f8723e48c6b829",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eOEpcYnxTFip7avJoOE12.jpeg",
                        "isPro": false,
                        "fullname": "Chengyu Bai",
                        "user": "Bryson0123",
                        "type": "user"
                    },
                    "name": "Chengyu Bai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-29T13:10:09.117Z",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c03",
                    "name": "Yu-Kai Wang",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c04",
                    "name": "Ying Li",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c05",
                    "name": "Lizhang Chen",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c06",
                    "name": "Yong Bao",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c07",
                    "name": "Zhiyuan Jiang",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c08",
                    "name": "Jiacheng Zhu",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c09",
                    "name": "Kai Tang",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c0a",
                    "name": "Ruichuan An",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c0b",
                    "name": "Yulin Luo",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c0c",
                    "name": "Qiuxuan Feng",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c0d",
                    "name": "Siyuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c0e",
                    "name": "Chi-min Chan",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c0f",
                    "name": "Chengkai Hou",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c10",
                    "name": "Wei Xue",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c11",
                    "user": {
                        "_id": "65444eddc21c8d9179319aaf",
                        "avatarUrl": "/avatars/9a432c87ca751ad38321163af33698bf.svg",
                        "isPro": false,
                        "fullname": "Sirui Han",
                        "user": "Anna759",
                        "type": "user"
                    },
                    "name": "Sirui Han",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-29T13:10:20.407Z",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c12",
                    "name": "Yike Guo",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c13",
                    "name": "Shanghang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68da280f0177a6054b013c14",
                    "name": "Jian Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T17:59:07.000Z",
            "submittedOnDailyAt": "2025-09-29T05:51:17.798Z",
            "title": "WoW: Towards a World omniscient World model Through Embodied Interaction",
            "submittedOnDailyBy": {
                "_id": "64368a740c77d7c50369a37a",
                "avatarUrl": "/avatars/fbb45eb385d2e28b4743077fc5143c02.svg",
                "isPro": false,
                "fullname": "yong",
                "user": "dyong",
                "type": "user"
            },
            "summary": "Humans develop an understanding of intuitive physics through active\ninteraction with the world. This approach is in stark contrast to current video\nmodels, such as Sora, which rely on passive observation and therefore struggle\nwith grasping physical causality. This observation leads to our central\nhypothesis: authentic physical intuition of the world model must be grounded in\nextensive, causally rich interactions with the real world. To test this\nhypothesis, we present WoW, a 14-billion-parameter generative world model\ntrained on 2 million robot interaction trajectories. Our findings reveal that\nthe model's understanding of physics is a probabilistic distribution of\nplausible outcomes, leading to stochastic instabilities and physical\nhallucinations. Furthermore, we demonstrate that this emergent capability can\nbe actively constrained toward physical realism by SOPHIA, where\nvision-language model agents evaluate the DiT-generated output and guide its\nrefinement by iteratively evolving the language instructions. In addition, a\nco-trained Inverse Dynamics Model translates these refined plans into\nexecutable robotic actions, thus closing the imagination-to-action loop. We\nestablish WoWBench, a new benchmark focused on physical consistency and causal\nreasoning in video, where WoW achieves state-of-the-art performance in both\nhuman and autonomous evaluation, demonstrating strong ability in physical\ncausality, collision dynamics, and object permanence. Our work provides\nsystematic evidence that large-scale, real-world interaction is a cornerstone\nfor developing physical intuition in AI. Models, data, and benchmarks will be\nopen-sourced.",
            "upvotes": 3,
            "discussionId": "68da280f0177a6054b013c15",
            "ai_summary": "WoW, a 14-billion-parameter generative world model trained on robot interactions, demonstrates improved physical intuition through SOPHIA's guidance and achieves state-of-the-art performance on physical consistency and causal reasoning in video.",
            "ai_keywords": [
                "generative world model",
                "probabilistic distribution",
                "stochastic instabilities",
                "physical hallucinations",
                "vision-language model",
                "DiT-generated output",
                "Inverse Dynamics Model",
                "imagination-to-action loop",
                "WoWBench",
                "physical consistency",
                "causal reasoning",
                "physical causality",
                "collision dynamics",
                "object permanence"
            ]
        },
        "publishedAt": "2025-09-26T13:59:07.000Z",
        "title": "WoW: Towards a World omniscient World model Through Embodied Interaction",
        "summary": "Humans develop an understanding of intuitive physics through active\ninteraction with the world. This approach is in stark contrast to current video\nmodels, such as Sora, which rely on passive observation and therefore struggle\nwith grasping physical causality. This observation leads to our central\nhypothesis: authentic physical intuition of the world model must be grounded in\nextensive, causally rich interactions with the real world. To test this\nhypothesis, we present WoW, a 14-billion-parameter generative world model\ntrained on 2 million robot interaction trajectories. Our findings reveal that\nthe model's understanding of physics is a probabilistic distribution of\nplausible outcomes, leading to stochastic instabilities and physical\nhallucinations. Furthermore, we demonstrate that this emergent capability can\nbe actively constrained toward physical realism by SOPHIA, where\nvision-language model agents evaluate the DiT-generated output and guide its\nrefinement by iteratively evolving the language instructions. In addition, a\nco-trained Inverse Dynamics Model translates these refined plans into\nexecutable robotic actions, thus closing the imagination-to-action loop. We\nestablish WoWBench, a new benchmark focused on physical consistency and causal\nreasoning in video, where WoW achieves state-of-the-art performance in both\nhuman and autonomous evaluation, demonstrating strong ability in physical\ncausality, collision dynamics, and object permanence. Our work provides\nsystematic evidence that large-scale, real-world interaction is a cornerstone\nfor developing physical intuition in AI. Models, data, and benchmarks will be\nopen-sourced.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22642.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64368a740c77d7c50369a37a",
            "avatarUrl": "/avatars/fbb45eb385d2e28b4743077fc5143c02.svg",
            "fullname": "yong",
            "name": "dyong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22244",
            "authors": [
                {
                    "_id": "68d9ebab0177a6054b013a48",
                    "name": "Junyi Wu",
                    "hidden": false
                },
                {
                    "_id": "68d9ebab0177a6054b013a49",
                    "name": "Zhiteng Li",
                    "hidden": false
                },
                {
                    "_id": "68d9ebab0177a6054b013a4a",
                    "name": "Haotong Qin",
                    "hidden": false
                },
                {
                    "_id": "68d9ebab0177a6054b013a4b",
                    "name": "Xiaohong Liu",
                    "hidden": false
                },
                {
                    "_id": "68d9ebab0177a6054b013a4c",
                    "name": "Linghe Kong",
                    "hidden": false
                },
                {
                    "_id": "68d9ebab0177a6054b013a4d",
                    "name": "Yulun Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d9ebab0177a6054b013a4e",
                    "name": "Xiaokang Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T11:59:30.000Z",
            "submittedOnDailyAt": "2025-09-29T00:45:23.659Z",
            "title": "FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image\n  Editing",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Text-guided image editing with diffusion models has achieved remarkable\nquality but suffers from prohibitive latency, hindering real-world\napplications. We introduce FlashEdit, a novel framework designed to enable\nhigh-fidelity, real-time image editing. Its efficiency stems from three key\ninnovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses\ncostly iterative processes; (2) a Background Shield (BG-Shield) technique that\nguarantees background preservation by selectively modifying features only\nwithin the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA)\nmechanism that ensures precise, localized edits by suppressing semantic leakage\nto the background. Extensive experiments demonstrate that FlashEdit maintains\nsuperior background consistency and structural integrity, while performing\nedits in under 0.2 seconds, which is an over 150times speedup compared to\nprior multi-step methods. Our code will be made publicly available at\nhttps://github.com/JunyiWuCode/FlashEdit.",
            "upvotes": 3,
            "discussionId": "68d9ebab0177a6054b013a4f",
            "ai_summary": "FlashEdit enables real-time, high-fidelity image editing with diffusion models through efficient inversion, background preservation, and localized attention mechanisms.",
            "ai_keywords": [
                "One-Step Inversion-and-Editing",
                "OSIE",
                "Background Shield",
                "BG-Shield",
                "Sparsified Spatial Cross-Attention",
                "SSCA",
                "diffusion models",
                "image editing",
                "background consistency",
                "structural integrity"
            ]
        },
        "publishedAt": "2025-09-26T07:59:30.000Z",
        "title": "FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image\n  Editing",
        "summary": "Text-guided image editing with diffusion models has achieved remarkable\nquality but suffers from prohibitive latency, hindering real-world\napplications. We introduce FlashEdit, a novel framework designed to enable\nhigh-fidelity, real-time image editing. Its efficiency stems from three key\ninnovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses\ncostly iterative processes; (2) a Background Shield (BG-Shield) technique that\nguarantees background preservation by selectively modifying features only\nwithin the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA)\nmechanism that ensures precise, localized edits by suppressing semantic leakage\nto the background. Extensive experiments demonstrate that FlashEdit maintains\nsuperior background consistency and structural integrity, while performing\nedits in under 0.2 seconds, which is an over 150times speedup compared to\nprior multi-step methods. Our code will be made publicly available at\nhttps://github.com/JunyiWuCode/FlashEdit.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22244.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 112
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.21991",
            "authors": [
                {
                    "_id": "68da629acfa03a5158a1ed9c",
                    "user": {
                        "_id": "642d395784bf892b8fa91816",
                        "avatarUrl": "/avatars/2db2202641ad7ed9aeaea7e8bfc3acca.svg",
                        "isPro": false,
                        "fullname": "Jewon Lee",
                        "user": "je1lee",
                        "type": "user"
                    },
                    "name": "Jewon Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T13:05:11.586Z",
                    "hidden": false
                },
                {
                    "_id": "68da629acfa03a5158a1ed9d",
                    "user": {
                        "_id": "67a0a0d48f047b67c3172e9e",
                        "avatarUrl": "/avatars/fa2ee986071f3642c8c7cd6ff693dbfd.svg",
                        "isPro": false,
                        "fullname": "Wooksu Shin",
                        "user": "dnrtn1101",
                        "type": "user"
                    },
                    "name": "Wooksu Shin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T13:05:14.261Z",
                    "hidden": false
                },
                {
                    "_id": "68da629acfa03a5158a1ed9e",
                    "name": "Seungmin Yang",
                    "hidden": false
                },
                {
                    "_id": "68da629acfa03a5158a1ed9f",
                    "name": "Ki-Ung Song",
                    "hidden": false
                },
                {
                    "_id": "68da629acfa03a5158a1eda0",
                    "name": "DongUk Lim",
                    "hidden": false
                },
                {
                    "_id": "68da629acfa03a5158a1eda1",
                    "name": "Jaeyeon Kim",
                    "hidden": false
                },
                {
                    "_id": "68da629acfa03a5158a1eda2",
                    "name": "Tae-Ho Kim",
                    "hidden": false
                },
                {
                    "_id": "68da629acfa03a5158a1eda3",
                    "user": {
                        "_id": "64374f10cd93f4c9a34c1239",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64374f10cd93f4c9a34c1239/tdkdcgl18rKXLvyTEhfnG.jpeg",
                        "isPro": false,
                        "fullname": "Bo-Kyeong Kim",
                        "user": "bokyeong1015",
                        "type": "user"
                    },
                    "name": "Bo-Kyeong Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T11:28:06.616Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T07:15:19.000Z",
            "submittedOnDailyAt": "2025-09-29T09:13:37.380Z",
            "title": "ERGO: Efficient High-Resolution Visual Understanding for Vision-Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "642d395784bf892b8fa91816",
                "avatarUrl": "/avatars/2db2202641ad7ed9aeaea7e8bfc3acca.svg",
                "isPro": false,
                "fullname": "Jewon Lee",
                "user": "je1lee",
                "type": "user"
            },
            "summary": "Efficient processing of high-resolution images is crucial for real-world\nvision-language applications. However, existing Large Vision-Language Models\n(LVLMs) incur substantial computational overhead due to the large number of\nvision tokens. With the advent of \"thinking with images\" models, reasoning now\nextends beyond text to the visual domain. This capability motivates our\ntwo-stage \"coarse-to-fine\" reasoning pipeline: first, a downsampled image is\nanalyzed to identify task-relevant regions; then, only these regions are\ncropped at full resolution and processed in a subsequent reasoning stage. This\napproach reduces computational cost while preserving fine-grained visual\ndetails where necessary. A major challenge lies in inferring which regions are\ntruly relevant to a given query. Recent related methods often fail in the first\nstage after input-image downsampling, due to perception-driven reasoning, where\nclear visual information is required for effective reasoning. To address this\nissue, we propose ERGO (Efficient Reasoning & Guided Observation) that performs\nreasoning-driven perception-leveraging multimodal context to determine where to\nfocus. Our model can account for perceptual uncertainty, expanding the cropped\nregion to cover visually ambiguous areas for answering questions. To this end,\nwe develop simple yet effective reward components in a reinforcement learning\nframework for coarse-to-fine perception. Across multiple datasets, our approach\ndelivers higher accuracy than the original model and competitive methods, with\ngreater efficiency. For instance, ERGO surpasses Qwen2.5-VL-7B on the V*\nbenchmark by 4.7 points while using only 23% of the vision tokens, achieving a\n3x inference speedup. The code and models can be found at:\nhttps://github.com/nota-github/ERGO.",
            "upvotes": 3,
            "discussionId": "68da629bcfa03a5158a1eda4",
            "githubRepo": "https://github.com/nota-github/ERGO",
            "ai_summary": "ERGO, a two-stage reasoning pipeline, efficiently processes high-resolution images by focusing on task-relevant regions, improving accuracy and speed compared to existing models.",
            "ai_keywords": [
                "Large Vision-Language Models",
                "LVLMs",
                "coarse-to-fine reasoning",
                "downsampled image",
                "full resolution",
                "reasoning-driven perception",
                "multimodal context",
                "perceptual uncertainty",
                "reinforcement learning",
                "V* benchmark",
                "vision tokens",
                "inference speedup"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-09-26T03:15:19.000Z",
        "title": "ERGO: Efficient High-Resolution Visual Understanding for Vision-Language\n  Models",
        "summary": "Efficient processing of high-resolution images is crucial for real-world\nvision-language applications. However, existing Large Vision-Language Models\n(LVLMs) incur substantial computational overhead due to the large number of\nvision tokens. With the advent of \"thinking with images\" models, reasoning now\nextends beyond text to the visual domain. This capability motivates our\ntwo-stage \"coarse-to-fine\" reasoning pipeline: first, a downsampled image is\nanalyzed to identify task-relevant regions; then, only these regions are\ncropped at full resolution and processed in a subsequent reasoning stage. This\napproach reduces computational cost while preserving fine-grained visual\ndetails where necessary. A major challenge lies in inferring which regions are\ntruly relevant to a given query. Recent related methods often fail in the first\nstage after input-image downsampling, due to perception-driven reasoning, where\nclear visual information is required for effective reasoning. To address this\nissue, we propose ERGO (Efficient Reasoning & Guided Observation) that performs\nreasoning-driven perception-leveraging multimodal context to determine where to\nfocus. Our model can account for perceptual uncertainty, expanding the cropped\nregion to cover visually ambiguous areas for answering questions. To this end,\nwe develop simple yet effective reward components in a reinforcement learning\nframework for coarse-to-fine perception. Across multiple datasets, our approach\ndelivers higher accuracy than the original model and competitive methods, with\ngreater efficiency. For instance, ERGO surpasses Qwen2.5-VL-7B on the V*\nbenchmark by 4.7 points while using only 23% of the vision tokens, achieving a\n3x inference speedup. The code and models can be found at:\nhttps://github.com/nota-github/ERGO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21991.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642d395784bf892b8fa91816",
            "avatarUrl": "/avatars/2db2202641ad7ed9aeaea7e8bfc3acca.svg",
            "fullname": "Jewon Lee",
            "name": "je1lee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.23661",
            "authors": [
                {
                    "_id": "68db2f1cd2bf1f4b15ec7251",
                    "name": "Xiang An",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec7252",
                    "name": "Yin Xie",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec7253",
                    "name": "Kaicheng Yang",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec7254",
                    "name": "Wenkang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec7255",
                    "name": "Xiuwei Zhao",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec7256",
                    "name": "Zheng Cheng",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec7257",
                    "name": "Yirui Wang",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec7258",
                    "name": "Songcen Xu",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec7259",
                    "name": "Changrui Chen",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec725a",
                    "name": "Chunsheng Wu",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec725b",
                    "name": "Huajie Tan",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec725c",
                    "name": "Chunyuan Li",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec725d",
                    "name": "Jing Yang",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec725e",
                    "name": "Jie Yu",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec725f",
                    "name": "Xiyao Wang",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec7260",
                    "name": "Bin Qin",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec7261",
                    "name": "Yumeng Wang",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec7262",
                    "name": "Zizhen Yan",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec7263",
                    "name": "Ziyong Feng",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec7264",
                    "name": "Ziwei Liu",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec7265",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "68db2f1cd2bf1f4b15ec7266",
                    "name": "Jiankang Deng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T05:52:55.000Z",
            "submittedOnDailyAt": "2025-09-29T23:45:40.089Z",
            "title": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal\n  Training",
            "submittedOnDailyBy": {
                "_id": "6478679d7b370854241b2ad8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478679d7b370854241b2ad8/dBczWYYdfEt9tQcnVGhQk.jpeg",
                "isPro": false,
                "fullname": "xiangan",
                "user": "xiangan",
                "type": "user"
            },
            "summary": "We present LLaVA-OneVision-1.5, a novel family of Large Multimodal Models\n(LMMs) that achieve state-of-the-art performance with significantly reduced\ncomputational and financial costs. Different from the existing works,\nLLaVA-OneVision-1.5 provides an open, efficient, and reproducible framework for\nbuilding high-quality vision-language models entirely from scratch. The\nLLaVA-OneVision-1.5 release comprises three primary components: (1) Large-Scale\nCurated Datasets: We construct an 85M concept-balanced pretraining dataset\nLLaVA-OneVision-1.5-Mid-Traning and a meticulously curated 26M instruction\ndataset LLaVA-OneVision-1.5-Instruct, collectively encompassing 64B compressed\nmultimodal tokens. (2) Efficient Training Framework: We develop a complete\nend-to-end efficient training framework leveraging an offline parallel data\npacking strategy to facilitate the training of LLaVA-OneVision-1.5 within a\n$16,000 budget. (3) State-of-the-art Performance: Experimental results\ndemonstrate that LLaVA-OneVision1.5 yields exceptionally competitive\nperformance across a broad range of downstream tasks. Specifically,\nLLaVA-OneVision-1.5-8B outperforms Qwen2.5-VL-7B on 18 of 27 benchmarks, and\nLLaVA-OneVision-1.5-4B surpasses Qwen2.5-VL-3B on all 27 benchmarks. We\nanticipate releasing LLaVA-OneVision-1.5-RL shortly and encourage the community\nto await further updates.",
            "upvotes": 2,
            "discussionId": "68db2f1cd2bf1f4b15ec7267",
            "githubRepo": "https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5",
            "ai_summary": "LLaVA-OneVision-1.5 is a family of large multimodal models that achieves state-of-the-art performance with reduced costs through efficient training and high-quality datasets.",
            "ai_keywords": [
                "Large Multimodal Models",
                "LMMs",
                "Large-Scale Curated Datasets",
                "pretraining dataset",
                "instruction dataset",
                "multimodal tokens",
                "efficient training framework",
                "offline parallel data packing strategy",
                "state-of-the-art performance",
                "downstream tasks",
                "Qwen2.5-VL-7B",
                "Qwen2.5-VL-3B"
            ]
        },
        "publishedAt": "2025-09-28T01:52:55.000Z",
        "title": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal\n  Training",
        "summary": "We present LLaVA-OneVision-1.5, a novel family of Large Multimodal Models\n(LMMs) that achieve state-of-the-art performance with significantly reduced\ncomputational and financial costs. Different from the existing works,\nLLaVA-OneVision-1.5 provides an open, efficient, and reproducible framework for\nbuilding high-quality vision-language models entirely from scratch. The\nLLaVA-OneVision-1.5 release comprises three primary components: (1) Large-Scale\nCurated Datasets: We construct an 85M concept-balanced pretraining dataset\nLLaVA-OneVision-1.5-Mid-Traning and a meticulously curated 26M instruction\ndataset LLaVA-OneVision-1.5-Instruct, collectively encompassing 64B compressed\nmultimodal tokens. (2) Efficient Training Framework: We develop a complete\nend-to-end efficient training framework leveraging an offline parallel data\npacking strategy to facilitate the training of LLaVA-OneVision-1.5 within a\n$16,000 budget. (3) State-of-the-art Performance: Experimental results\ndemonstrate that LLaVA-OneVision1.5 yields exceptionally competitive\nperformance across a broad range of downstream tasks. Specifically,\nLLaVA-OneVision-1.5-8B outperforms Qwen2.5-VL-7B on 18 of 27 benchmarks, and\nLLaVA-OneVision-1.5-4B surpasses Qwen2.5-VL-3B on all 27 benchmarks. We\nanticipate releasing LLaVA-OneVision-1.5-RL shortly and encourage the community\nto await further updates.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23661.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6478679d7b370854241b2ad8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478679d7b370854241b2ad8/dBczWYYdfEt9tQcnVGhQk.jpeg",
            "fullname": "xiangan",
            "name": "xiangan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22636",
            "authors": [
                {
                    "_id": "68da7a37cfa03a5158a1edbf",
                    "user": {
                        "_id": "646bbce4628e5b50b2dfdc1c",
                        "avatarUrl": "/avatars/451361495c67be90a07698c3ad358dc8.svg",
                        "isPro": false,
                        "fullname": "Amandeep",
                        "user": "Aman015",
                        "type": "user"
                    },
                    "name": "Amandeep Kumar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T13:05:09.208Z",
                    "hidden": false
                },
                {
                    "_id": "68da7a37cfa03a5158a1edc0",
                    "name": "Nithin Gopalakrishnan Nair",
                    "hidden": false
                },
                {
                    "_id": "68da7a37cfa03a5158a1edc1",
                    "name": "Vishal M. Patel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T17:58:04.000Z",
            "submittedOnDailyAt": "2025-09-29T12:03:57.817Z",
            "title": "Scale-Wise VAR is Secretly Discrete Diffusion",
            "submittedOnDailyBy": {
                "_id": "646bbce4628e5b50b2dfdc1c",
                "avatarUrl": "/avatars/451361495c67be90a07698c3ad358dc8.svg",
                "isPro": false,
                "fullname": "Amandeep",
                "user": "Aman015",
                "type": "user"
            },
            "summary": "Autoregressive (AR) transformers have emerged as a powerful paradigm for\nvisual generation, largely due to their scalability, computational efficiency\nand unified architecture with language and vision. Among them, next scale\nprediction Visual Autoregressive Generation (VAR) has recently demonstrated\nremarkable performance, even surpassing diffusion-based models. In this work,\nwe revisit VAR and uncover a theoretical insight: when equipped with a\nMarkovian attention mask, VAR is mathematically equivalent to a discrete\ndiffusion. We term this reinterpretation as Scalable Visual Refinement with\nDiscrete Diffusion (SRDD), establishing a principled bridge between AR\ntransformers and diffusion models. Leveraging this new perspective, we show how\none can directly import the advantages of diffusion such as iterative\nrefinement and reduce architectural inefficiencies into VAR, yielding faster\nconvergence, lower inference cost, and improved zero-shot reconstruction.\nAcross multiple datasets, we show that the diffusion based perspective of VAR\nleads to consistent gains in efficiency and generation.",
            "upvotes": 2,
            "discussionId": "68da7a37cfa03a5158a1edc2",
            "projectPage": "https://virobo-15.github.io/srdd.github.io/",
            "ai_summary": "VAR, an autoregressive transformer for visual generation, is shown to be equivalent to a discrete diffusion when using a Markovian attention mask, leading to improved efficiency and generation quality.",
            "ai_keywords": [
                "autoregressive transformers",
                "visual generation",
                "next scale prediction",
                "VAR",
                "diffusion-based models",
                "Markovian attention mask",
                "discrete diffusion",
                "Scalable Visual Refinement with Discrete Diffusion",
                "SRDD",
                "iterative refinement",
                "zero-shot reconstruction"
            ]
        },
        "publishedAt": "2025-09-26T13:58:04.000Z",
        "title": "Scale-Wise VAR is Secretly Discrete Diffusion",
        "summary": "Autoregressive (AR) transformers have emerged as a powerful paradigm for\nvisual generation, largely due to their scalability, computational efficiency\nand unified architecture with language and vision. Among them, next scale\nprediction Visual Autoregressive Generation (VAR) has recently demonstrated\nremarkable performance, even surpassing diffusion-based models. In this work,\nwe revisit VAR and uncover a theoretical insight: when equipped with a\nMarkovian attention mask, VAR is mathematically equivalent to a discrete\ndiffusion. We term this reinterpretation as Scalable Visual Refinement with\nDiscrete Diffusion (SRDD), establishing a principled bridge between AR\ntransformers and diffusion models. Leveraging this new perspective, we show how\none can directly import the advantages of diffusion such as iterative\nrefinement and reduce architectural inefficiencies into VAR, yielding faster\nconvergence, lower inference cost, and improved zero-shot reconstruction.\nAcross multiple datasets, we show that the diffusion based perspective of VAR\nleads to consistent gains in efficiency and generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22636.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646bbce4628e5b50b2dfdc1c",
            "avatarUrl": "/avatars/451361495c67be90a07698c3ad358dc8.svg",
            "fullname": "Amandeep",
            "name": "Aman015",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.22496",
            "authors": [
                {
                    "_id": "68d9eb0f0177a6054b013a3e",
                    "name": "Ruoyu Chen",
                    "hidden": false
                },
                {
                    "_id": "68d9eb0f0177a6054b013a3f",
                    "name": "Xiaoqing Guo",
                    "hidden": false
                },
                {
                    "_id": "68d9eb0f0177a6054b013a40",
                    "name": "Kangwei Liu",
                    "hidden": false
                },
                {
                    "_id": "68d9eb0f0177a6054b013a41",
                    "name": "Siyuan Liang",
                    "hidden": false
                },
                {
                    "_id": "68d9eb0f0177a6054b013a42",
                    "name": "Shiming Liu",
                    "hidden": false
                },
                {
                    "_id": "68d9eb0f0177a6054b013a43",
                    "name": "Qunli Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d9eb0f0177a6054b013a44",
                    "name": "Hua Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d9eb0f0177a6054b013a45",
                    "name": "Xiaochun Cao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T15:38:42.000Z",
            "submittedOnDailyAt": "2025-09-29T00:42:49.775Z",
            "title": "Where MLLMs Attend and What They Rely On: Explaining Autoregressive\n  Token Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable\ncapabilities in aligning visual inputs with natural language outputs. Yet, the\nextent to which generated tokens depend on visual modalities remains poorly\nunderstood, limiting interpretability and reliability. In this work, we present\nEAGLE, a lightweight black-box framework for explaining autoregressive token\ngeneration in MLLMs. EAGLE attributes any selected tokens to compact perceptual\nregions while quantifying the relative influence of language priors and\nperceptual evidence. The framework introduces an objective function that\nunifies sufficiency (insight score) and indispensability (necessity score),\noptimized via greedy search over sparsified image regions for faithful and\nefficient attribution. Beyond spatial attribution, EAGLE performs\nmodality-aware analysis that disentangles what tokens rely on, providing\nfine-grained interpretability of model decisions. Extensive experiments across\nopen-source MLLMs show that EAGLE consistently outperforms existing methods in\nfaithfulness, localization, and hallucination diagnosis, while requiring\nsubstantially less GPU memory. These results highlight its effectiveness and\npracticality for advancing the interpretability of MLLMs. The code is available\nat https://github.com/RuoyuChen10/EAGLE.",
            "upvotes": 2,
            "discussionId": "68d9eb0f0177a6054b013a46",
            "ai_summary": "EAGLE is a lightweight framework that explains token generation in multimodal large language models by attributing tokens to visual regions and quantifying the influence of language and perceptual evidence.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "EAGLE",
                "autoregressive token generation",
                "perceptual regions",
                "language priors",
                "perceptual evidence",
                "objective function",
                "insight score",
                "necessity score",
                "greedy search",
                "spatial attribution",
                "modality-aware analysis",
                "disentanglement",
                "faithfulness",
                "localization",
                "hallucination diagnosis"
            ]
        },
        "publishedAt": "2025-09-26T11:38:42.000Z",
        "title": "Where MLLMs Attend and What They Rely On: Explaining Autoregressive\n  Token Generation",
        "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable\ncapabilities in aligning visual inputs with natural language outputs. Yet, the\nextent to which generated tokens depend on visual modalities remains poorly\nunderstood, limiting interpretability and reliability. In this work, we present\nEAGLE, a lightweight black-box framework for explaining autoregressive token\ngeneration in MLLMs. EAGLE attributes any selected tokens to compact perceptual\nregions while quantifying the relative influence of language priors and\nperceptual evidence. The framework introduces an objective function that\nunifies sufficiency (insight score) and indispensability (necessity score),\noptimized via greedy search over sparsified image regions for faithful and\nefficient attribution. Beyond spatial attribution, EAGLE performs\nmodality-aware analysis that disentangles what tokens rely on, providing\nfine-grained interpretability of model decisions. Extensive experiments across\nopen-source MLLMs show that EAGLE consistently outperforms existing methods in\nfaithfulness, localization, and hallucination diagnosis, while requiring\nsubstantially less GPU memory. These results highlight its effectiveness and\npracticality for advancing the interpretability of MLLMs. The code is available\nat https://github.com/RuoyuChen10/EAGLE.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22496.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 112
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22300",
            "authors": [
                {
                    "_id": "68da4e15cfa03a5158a1ed0d",
                    "user": {
                        "_id": "63b4b02a103617b0a5b0ee2e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
                        "isPro": false,
                        "fullname": "Seyedmorteza Sadat",
                        "user": "msadat97",
                        "type": "user"
                    },
                    "name": "Seyedmorteza Sadat",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T11:26:12.223Z",
                    "hidden": false
                },
                {
                    "_id": "68da4e15cfa03a5158a1ed0e",
                    "name": "Farnood Salehi",
                    "hidden": false
                },
                {
                    "_id": "68da4e15cfa03a5158a1ed0f",
                    "name": "Romann M. Weber",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T13:01:10.000Z",
            "submittedOnDailyAt": "2025-09-29T07:46:04.594Z",
            "title": "HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion\n  Models",
            "submittedOnDailyBy": {
                "_id": "63b4b02a103617b0a5b0ee2e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
                "isPro": false,
                "fullname": "Seyedmorteza Sadat",
                "user": "msadat97",
                "type": "user"
            },
            "summary": "While diffusion models have made remarkable progress in image generation,\ntheir outputs can still appear unrealistic and lack fine details, especially\nwhen using fewer number of neural function evaluations (NFEs) or lower guidance\nscales. To address this issue, we propose a novel momentum-based sampling\ntechnique, termed history-guided sampling (HiGS), which enhances quality and\nefficiency of diffusion sampling by integrating recent model predictions into\neach inference step. Specifically, HiGS leverages the difference between the\ncurrent prediction and a weighted average of past predictions to steer the\nsampling process toward more realistic outputs with better details and\nstructure. Our approach introduces practically no additional computation and\nintegrates seamlessly into existing diffusion frameworks, requiring neither\nextra training nor fine-tuning. Extensive experiments show that HiGS\nconsistently improves image quality across diverse models and architectures and\nunder varying sampling budgets and guidance scales. Moreover, using a\npretrained SiT model, HiGS achieves a new state-of-the-art FID of 1.61 for\nunguided ImageNet generation at 256times256 with only 30 sampling steps\n(instead of the standard 250). We thus present HiGS as a plug-and-play\nenhancement to standard diffusion sampling that enables faster generation with\nhigher fidelity.",
            "upvotes": 2,
            "discussionId": "68da4e15cfa03a5158a1ed10",
            "ai_summary": "A momentum-based sampling technique called history-guided sampling (HiGS) improves the quality and efficiency of diffusion models in image generation without additional computation or training.",
            "ai_keywords": [
                "diffusion models",
                "image generation",
                "neural function evaluations",
                "NFEs",
                "guidance scales",
                "history-guided sampling",
                "HiGS",
                "model predictions",
                "inference step",
                "sampling process",
                "image quality",
                "sampling budgets",
                "SiT model",
                "FID",
                "ImageNet generation"
            ]
        },
        "publishedAt": "2025-09-26T09:01:10.000Z",
        "title": "HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion\n  Models",
        "summary": "While diffusion models have made remarkable progress in image generation,\ntheir outputs can still appear unrealistic and lack fine details, especially\nwhen using fewer number of neural function evaluations (NFEs) or lower guidance\nscales. To address this issue, we propose a novel momentum-based sampling\ntechnique, termed history-guided sampling (HiGS), which enhances quality and\nefficiency of diffusion sampling by integrating recent model predictions into\neach inference step. Specifically, HiGS leverages the difference between the\ncurrent prediction and a weighted average of past predictions to steer the\nsampling process toward more realistic outputs with better details and\nstructure. Our approach introduces practically no additional computation and\nintegrates seamlessly into existing diffusion frameworks, requiring neither\nextra training nor fine-tuning. Extensive experiments show that HiGS\nconsistently improves image quality across diverse models and architectures and\nunder varying sampling budgets and guidance scales. Moreover, using a\npretrained SiT model, HiGS achieves a new state-of-the-art FID of 1.61 for\nunguided ImageNet generation at 256times256 with only 30 sampling steps\n(instead of the standard 250). We thus present HiGS as a plug-and-play\nenhancement to standard diffusion sampling that enables faster generation with\nhigher fidelity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22300.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63b4b02a103617b0a5b0ee2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
            "fullname": "Seyedmorteza Sadat",
            "name": "msadat97",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.21294",
            "authors": [
                {
                    "_id": "68d6a3310177a6054b013678",
                    "user": {
                        "_id": "648585606ab1aad693277a96",
                        "avatarUrl": "/avatars/17b50f59c1841d69bd84329422eb09e1.svg",
                        "isPro": false,
                        "fullname": "Pranjal A. Chitale",
                        "user": "pranjalchitale",
                        "type": "user"
                    },
                    "name": "Pranjal A. Chitale",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-26T15:36:00.535Z",
                    "hidden": false
                },
                {
                    "_id": "68d6a3310177a6054b013679",
                    "name": "Varun Gumma",
                    "hidden": false
                },
                {
                    "_id": "68d6a3310177a6054b01367a",
                    "name": "Sanchit Ahuja",
                    "hidden": false
                },
                {
                    "_id": "68d6a3310177a6054b01367b",
                    "name": "Prashant Kodali",
                    "hidden": false
                },
                {
                    "_id": "68d6a3310177a6054b01367c",
                    "name": "Manan Uppadhyay",
                    "hidden": false
                },
                {
                    "_id": "68d6a3310177a6054b01367d",
                    "name": "Deepthi Sudharsan",
                    "hidden": false
                },
                {
                    "_id": "68d6a3310177a6054b01367e",
                    "name": "Sunayana Sitaram",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-25T15:13:00.000Z",
            "submittedOnDailyAt": "2025-09-29T13:15:49.096Z",
            "title": "The role of synthetic data in Multilingual, Multi-cultural AI systems:\n  Lessons from Indic Languages",
            "submittedOnDailyBy": {
                "_id": "648585606ab1aad693277a96",
                "avatarUrl": "/avatars/17b50f59c1841d69bd84329422eb09e1.svg",
                "isPro": false,
                "fullname": "Pranjal A. Chitale",
                "user": "pranjalchitale",
                "type": "user"
            },
            "summary": "Developing AI systems that operate effectively across languages while\nremaining culturally grounded is a long-standing challenge, particularly in\nlow-resource settings. Synthetic data provides a promising avenue, yet its\neffectiveness in multilingual and multicultural contexts remains underexplored.\nWe investigate the creation and impact of synthetic, culturally contextualized\ndatasets for Indian languages through a bottom-up generation strategy that\nprompts large open-source LLMs (>= 235B parameters) to ground data generation\nin language-specific Wikipedia content. This approach complements the dominant\ntop-down paradigm of translating synthetic datasets from high-resource\nlanguages such as English. We introduce Updesh, a high-quality large-scale\nsynthetic instruction-following dataset comprising 9.5M data points across 13\nIndian languages, encompassing diverse reasoning and generative tasks with an\nemphasis on long-context, multi-turn capabilities, and alignment with Indian\ncultural contexts. A comprehensive evaluation incorporating both automated\nmetrics and human annotation across 10k assessments indicates that generated\ndata is high quality; though, human evaluation highlights areas for further\nimprovement. Additionally, we perform downstream evaluations by fine-tuning\nmodels on our dataset and assessing the performance across 15 diverse\nmultilingual datasets. Models trained on Updesh consistently achieve\nsignificant gains on generative tasks and remain competitive on multiple-choice\nstyle NLU tasks. Notably, relative improvements are most pronounced in low and\nmedium-resource languages, narrowing their gap with high-resource languages.\nThese findings provide empirical evidence that effective multilingual AI\nrequires multi-faceted data curation and generation strategies that incorporate\ncontext-aware, culturally grounded methodologies.",
            "upvotes": 2,
            "discussionId": "68d6a3320177a6054b01367f",
            "ai_summary": "Synthetic, culturally contextualized datasets for Indian languages improve multilingual AI performance, especially in low and medium-resource languages, through a bottom-up generation strategy using large open-source LLMs.",
            "ai_keywords": [
                "LLMs",
                "large-scale synthetic instruction-following dataset",
                "Updesh",
                "long-context",
                "multi-turn capabilities",
                "cultural contexts",
                "automated metrics",
                "human annotation",
                "fine-tuning",
                "generative tasks",
                "multiple-choice style NLU tasks",
                "multilingual datasets",
                "low-resource languages",
                "medium-resource languages",
                "high-resource languages"
            ]
        },
        "publishedAt": "2025-09-25T11:13:00.000Z",
        "title": "The role of synthetic data in Multilingual, Multi-cultural AI systems:\n  Lessons from Indic Languages",
        "summary": "Developing AI systems that operate effectively across languages while\nremaining culturally grounded is a long-standing challenge, particularly in\nlow-resource settings. Synthetic data provides a promising avenue, yet its\neffectiveness in multilingual and multicultural contexts remains underexplored.\nWe investigate the creation and impact of synthetic, culturally contextualized\ndatasets for Indian languages through a bottom-up generation strategy that\nprompts large open-source LLMs (>= 235B parameters) to ground data generation\nin language-specific Wikipedia content. This approach complements the dominant\ntop-down paradigm of translating synthetic datasets from high-resource\nlanguages such as English. We introduce Updesh, a high-quality large-scale\nsynthetic instruction-following dataset comprising 9.5M data points across 13\nIndian languages, encompassing diverse reasoning and generative tasks with an\nemphasis on long-context, multi-turn capabilities, and alignment with Indian\ncultural contexts. A comprehensive evaluation incorporating both automated\nmetrics and human annotation across 10k assessments indicates that generated\ndata is high quality; though, human evaluation highlights areas for further\nimprovement. Additionally, we perform downstream evaluations by fine-tuning\nmodels on our dataset and assessing the performance across 15 diverse\nmultilingual datasets. Models trained on Updesh consistently achieve\nsignificant gains on generative tasks and remain competitive on multiple-choice\nstyle NLU tasks. Notably, relative improvements are most pronounced in low and\nmedium-resource languages, narrowing their gap with high-resource languages.\nThese findings provide empirical evidence that effective multilingual AI\nrequires multi-faceted data curation and generation strategies that incorporate\ncontext-aware, culturally grounded methodologies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21294.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648585606ab1aad693277a96",
            "avatarUrl": "/avatars/17b50f59c1841d69bd84329422eb09e1.svg",
            "fullname": "Pranjal A. Chitale",
            "name": "pranjalchitale",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "submitterOrganization": {
            "_id": "5e6485f787403103f9f1055e",
            "name": "microsoft",
            "fullname": "Microsoft",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.22630",
            "authors": [
                {
                    "_id": "68da035b0177a6054b013b85",
                    "name": "Xingyu Shen",
                    "hidden": false
                },
                {
                    "_id": "68da035b0177a6054b013b86",
                    "user": {
                        "_id": "6144e4667f2544bb450787b2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6144e4667f2544bb450787b2/5wSDDqJbI4TGtBLP9IvjY.png",
                        "isPro": false,
                        "fullname": "Yingfa Chen",
                        "user": "chen-yingfa",
                        "type": "user"
                    },
                    "name": "Yingfa Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T05:57:43.162Z",
                    "hidden": false
                },
                {
                    "_id": "68da035b0177a6054b013b87",
                    "name": "Zhen Leng Thai",
                    "hidden": false
                },
                {
                    "_id": "68da035b0177a6054b013b88",
                    "name": "Xu Han",
                    "hidden": false
                },
                {
                    "_id": "68da035b0177a6054b013b89",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68da035b0177a6054b013b8a",
                    "name": "Maosong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T17:55:22.000Z",
            "submittedOnDailyAt": "2025-09-29T02:38:01.619Z",
            "title": "StateX: Enhancing RNN Recall via Post-training State Expansion",
            "submittedOnDailyBy": {
                "_id": "6144e4667f2544bb450787b2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6144e4667f2544bb450787b2/5wSDDqJbI4TGtBLP9IvjY.png",
                "isPro": false,
                "fullname": "Yingfa Chen",
                "user": "chen-yingfa",
                "type": "user"
            },
            "summary": "While Transformer-based models have demonstrated remarkable language modeling\nperformance, their high complexities result in high costs when processing long\ncontexts. In contrast, recurrent neural networks (RNNs) such as linear\nattention and state space models have gained popularity due to their constant\nper-token complexities. However, these recurrent models struggle with tasks\nthat require accurate recall of contextual information from long contexts,\nbecause all contextual information is compressed into a constant-size recurrent\nstate. Previous works have shown that recall ability is positively correlated\nwith the recurrent state size, yet directly training RNNs with larger recurrent\nstates results in high training costs. In this paper, we introduce StateX, a\ntraining pipeline for efficiently expanding the states of pre-trained RNNs\nthrough post-training. For two popular classes of RNNs, linear attention and\nstate space models, we design post-training architectural modifications to\nscale up the state size with no or negligible increase in model parameters.\nExperiments on models up to 1.3B parameters demonstrate that StateX efficiently\nenhances the recall and in-context learning ability of RNNs without incurring\nhigh post-training costs or compromising other capabilities.",
            "upvotes": 1,
            "discussionId": "68da035b0177a6054b013b8b",
            "githubRepo": "https://github.com/thunlp/StateX",
            "ai_summary": "StateX is a post-training pipeline that expands the state size of pre-trained RNNs, enhancing recall and in-context learning without significant additional costs.",
            "ai_keywords": [
                "Transformer-based models",
                "language modeling",
                "recurrent neural networks",
                "RNNs",
                "linear attention",
                "state space models",
                "recurrent state",
                "StateX",
                "post-training",
                "in-context learning"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-09-26T13:55:22.000Z",
        "title": "StateX: Enhancing RNN Recall via Post-training State Expansion",
        "summary": "While Transformer-based models have demonstrated remarkable language modeling\nperformance, their high complexities result in high costs when processing long\ncontexts. In contrast, recurrent neural networks (RNNs) such as linear\nattention and state space models have gained popularity due to their constant\nper-token complexities. However, these recurrent models struggle with tasks\nthat require accurate recall of contextual information from long contexts,\nbecause all contextual information is compressed into a constant-size recurrent\nstate. Previous works have shown that recall ability is positively correlated\nwith the recurrent state size, yet directly training RNNs with larger recurrent\nstates results in high training costs. In this paper, we introduce StateX, a\ntraining pipeline for efficiently expanding the states of pre-trained RNNs\nthrough post-training. For two popular classes of RNNs, linear attention and\nstate space models, we design post-training architectural modifications to\nscale up the state size with no or negligible increase in model parameters.\nExperiments on models up to 1.3B parameters demonstrate that StateX efficiently\nenhances the recall and in-context learning ability of RNNs without incurring\nhigh post-training costs or compromising other capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22630.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6144e4667f2544bb450787b2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6144e4667f2544bb450787b2/5wSDDqJbI4TGtBLP9IvjY.png",
            "fullname": "Yingfa Chen",
            "name": "chen-yingfa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.21559",
            "authors": [
                {
                    "_id": "68da237c0177a6054b013be3",
                    "user": {
                        "_id": "68090a58090c3a12a2806615",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0dMsD4hSMkQ1Vo5vkOiQd.png",
                        "isPro": false,
                        "fullname": "PRASANNA REDDY PULAKURTHI",
                        "user": "prasannareddyp",
                        "type": "user"
                    },
                    "name": "Prasanna Reddy Pulakurthi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T11:26:22.375Z",
                    "hidden": false
                },
                {
                    "_id": "68da237c0177a6054b013be4",
                    "name": "Jiamian Wang",
                    "hidden": false
                },
                {
                    "_id": "68da237c0177a6054b013be5",
                    "name": "Majid Rabbani",
                    "hidden": false
                },
                {
                    "_id": "68da237c0177a6054b013be6",
                    "name": "Sohail Dianat",
                    "hidden": false
                },
                {
                    "_id": "68da237c0177a6054b013be7",
                    "name": "Raghuveer Rao",
                    "hidden": false
                },
                {
                    "_id": "68da237c0177a6054b013be8",
                    "name": "Zhiqiang Tao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-25T20:39:45.000Z",
            "submittedOnDailyAt": "2025-09-29T04:44:55.508Z",
            "title": "X-CoT: Explainable Text-to-Video Retrieval via LLM-based\n  Chain-of-Thought Reasoning",
            "submittedOnDailyBy": {
                "_id": "68090a58090c3a12a2806615",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0dMsD4hSMkQ1Vo5vkOiQd.png",
                "isPro": false,
                "fullname": "PRASANNA REDDY PULAKURTHI",
                "user": "prasannareddyp",
                "type": "user"
            },
            "summary": "Prevalent text-to-video retrieval systems mainly adopt embedding models for\nfeature extraction and compute cosine similarities for ranking. However, this\ndesign presents two limitations. Low-quality text-video data pairs could\ncompromise the retrieval, yet are hard to identify and examine. Cosine\nsimilarity alone provides no explanation for the ranking results, limiting the\ninterpretability. We ask that can we interpret the ranking results, so as to\nassess the retrieval models and examine the text-video data? This work proposes\nX-CoT, an explainable retrieval framework upon LLM CoT reasoning in place of\nthe embedding model-based similarity ranking. We first expand the existing\nbenchmarks with additional video annotations to support semantic understanding\nand reduce data bias. We also devise a retrieval CoT consisting of pairwise\ncomparison steps, yielding detailed reasoning and complete ranking. X-CoT\nempirically improves the retrieval performance and produces detailed\nrationales. It also facilitates the model behavior and data quality analysis.\nCode and data are available at: https://github.com/PrasannaPulakurthi/X-CoT.",
            "upvotes": 1,
            "discussionId": "68da237c0177a6054b013be9",
            "projectPage": "https://prasannapulakurthi.github.io/X-CoT/",
            "githubRepo": "https://github.com/PrasannaPulakurthi/X-CoT",
            "ai_summary": "X-CoT, an explainable retrieval framework using LLM CoT reasoning, enhances text-to-video retrieval by providing detailed rationales and improving performance.",
            "ai_keywords": [
                "embedding models",
                "cosine similarities",
                "X-CoT",
                "LLM CoT reasoning",
                "video annotations",
                "semantic understanding",
                "data bias",
                "retrieval CoT",
                "pairwise comparison",
                "detailed rationales",
                "model behavior",
                "data quality analysis"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-09-25T16:39:45.000Z",
        "title": "X-CoT: Explainable Text-to-Video Retrieval via LLM-based\n  Chain-of-Thought Reasoning",
        "summary": "Prevalent text-to-video retrieval systems mainly adopt embedding models for\nfeature extraction and compute cosine similarities for ranking. However, this\ndesign presents two limitations. Low-quality text-video data pairs could\ncompromise the retrieval, yet are hard to identify and examine. Cosine\nsimilarity alone provides no explanation for the ranking results, limiting the\ninterpretability. We ask that can we interpret the ranking results, so as to\nassess the retrieval models and examine the text-video data? This work proposes\nX-CoT, an explainable retrieval framework upon LLM CoT reasoning in place of\nthe embedding model-based similarity ranking. We first expand the existing\nbenchmarks with additional video annotations to support semantic understanding\nand reduce data bias. We also devise a retrieval CoT consisting of pairwise\ncomparison steps, yielding detailed reasoning and complete ranking. X-CoT\nempirically improves the retrieval performance and produces detailed\nrationales. It also facilitates the model behavior and data quality analysis.\nCode and data are available at: https://github.com/PrasannaPulakurthi/X-CoT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21559.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68090a58090c3a12a2806615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0dMsD4hSMkQ1Vo5vkOiQd.png",
            "fullname": "PRASANNA REDDY PULAKURTHI",
            "name": "prasannareddyp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.21319",
            "authors": [
                {
                    "_id": "68dab42fd2bf1f4b15ec7178",
                    "name": "Zhilin Wang",
                    "hidden": false
                },
                {
                    "_id": "68dab42fd2bf1f4b15ec7179",
                    "name": "Jiaqi Zeng",
                    "hidden": false
                },
                {
                    "_id": "68dab42fd2bf1f4b15ec717a",
                    "name": "Olivier Delalleau",
                    "hidden": false
                },
                {
                    "_id": "68dab42fd2bf1f4b15ec717b",
                    "name": "Ellie Evans",
                    "hidden": false
                },
                {
                    "_id": "68dab42fd2bf1f4b15ec717c",
                    "name": "Daniel Egert",
                    "hidden": false
                },
                {
                    "_id": "68dab42fd2bf1f4b15ec717d",
                    "name": "Hoo-Chang Shin",
                    "hidden": false
                },
                {
                    "_id": "68dab42fd2bf1f4b15ec717e",
                    "name": "Felipe Soares",
                    "hidden": false
                },
                {
                    "_id": "68dab42fd2bf1f4b15ec717f",
                    "name": "Yi Dong",
                    "hidden": false
                },
                {
                    "_id": "68dab42fd2bf1f4b15ec7180",
                    "name": "Oleksii Kuchaiev",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-25T16:19:06.000Z",
            "submittedOnDailyAt": "2025-09-29T15:01:57.688Z",
            "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards",
            "submittedOnDailyBy": {
                "_id": "635865c8aff68f72ac039a4e",
                "avatarUrl": "/avatars/3fa9bee2e9e211723fcf44bddc6110c2.svg",
                "isPro": false,
                "fullname": "Zhilin Wang",
                "user": "zhilinw",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).",
            "upvotes": 1,
            "discussionId": "68dab42fd2bf1f4b15ec7181",
            "ai_summary": "RLBFF combines human feedback and rule-based verification to improve reward models, achieving top performance on benchmarks with customizable principles.",
            "ai_keywords": [
                "Reinforcement Learning with Human Feedback (RLHF)",
                "Reinforcement Learning with Verifiable Rewards (RLVR)",
                "Reinforcement Learning with Binary Flexible Feedback (RLBFF)",
                "reward models",
                "entailment task",
                "RM-Bench",
                "JudgeBench",
                "MT-Bench",
                "WildBench",
                "Arena Hard v2"
            ]
        },
        "publishedAt": "2025-09-25T12:19:06.000Z",
        "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards",
        "summary": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21319.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635865c8aff68f72ac039a4e",
            "avatarUrl": "/avatars/3fa9bee2e9e211723fcf44bddc6110c2.svg",
            "fullname": "Zhilin Wang",
            "name": "zhilinw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "submitterOrganization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.21150",
            "authors": [
                {
                    "_id": "68da804fd2bf1f4b15ec7122",
                    "user": {
                        "_id": "67a30a243d5a32b36c7d7d0b",
                        "avatarUrl": "/avatars/4aa43f0d79797be1063f246d63638a85.svg",
                        "isPro": false,
                        "fullname": "Ruiyu Wang",
                        "user": "rywang37",
                        "type": "user"
                    },
                    "name": "Ruiyu Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T13:05:05.871Z",
                    "hidden": false
                },
                {
                    "_id": "68da804fd2bf1f4b15ec7123",
                    "name": "Shizhao Sun",
                    "hidden": false
                },
                {
                    "_id": "68da804fd2bf1f4b15ec7124",
                    "name": "Weijian Ma",
                    "hidden": false
                },
                {
                    "_id": "68da804fd2bf1f4b15ec7125",
                    "name": "Jiang Bian",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-25T13:38:36.000Z",
            "submittedOnDailyAt": "2025-09-29T11:25:09.793Z",
            "title": "CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific\n  Tokenization",
            "submittedOnDailyBy": {
                "_id": "67a30a243d5a32b36c7d7d0b",
                "avatarUrl": "/avatars/4aa43f0d79797be1063f246d63638a85.svg",
                "isPro": false,
                "fullname": "Ruiyu Wang",
                "user": "rywang37",
                "type": "user"
            },
            "summary": "Computer-Aided Design (CAD) is a foundational component of industrial\nprototyping, where models are defined not by raw coordinates but by\nconstruction sequences such as sketches and extrusions. This sequential\nstructure enables both efficient prototype initialization and subsequent\nediting. Text-guided CAD prototyping, which unifies Text-to-CAD generation and\nCAD editing, has the potential to streamline the entire design pipeline.\nHowever, prior work has not explored this setting, largely because standard\nlarge language model (LLM) tokenizers decompose CAD sequences into\nnatural-language word pieces, failing to capture primitive-level CAD semantics\nand hindering attention modules from modeling geometric structure. We\nconjecture that a multimodal tokenization strategy, aligned with CAD's\nprimitive and structural nature, can provide more effective representations. To\nthis end, we propose CAD-Tokenizer, a framework that represents CAD data with\nmodality-specific tokens using a sequence-based VQ-VAE with primitive-level\npooling and constrained decoding. This design produces compact, primitive-aware\nrepresentations that align with CAD's structural nature. Applied to unified\ntext-guided CAD prototyping, CAD-Tokenizer significantly improves instruction\nfollowing and generation quality, achieving better quantitative and qualitative\nperformance over both general-purpose LLMs and task-specific baselines.",
            "upvotes": 1,
            "discussionId": "68da804fd2bf1f4b15ec7126",
            "ai_summary": "CAD-Tokenizer, a multimodal tokenization framework using VQ-VAE, enhances text-guided CAD prototyping by improving instruction following and generation quality.",
            "ai_keywords": [
                "VQ-VAE",
                "multimodal tokenization",
                "primitive-level pooling",
                "constrained decoding",
                "text-guided CAD prototyping"
            ]
        },
        "publishedAt": "2025-09-25T09:38:36.000Z",
        "title": "CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific\n  Tokenization",
        "summary": "Computer-Aided Design (CAD) is a foundational component of industrial\nprototyping, where models are defined not by raw coordinates but by\nconstruction sequences such as sketches and extrusions. This sequential\nstructure enables both efficient prototype initialization and subsequent\nediting. Text-guided CAD prototyping, which unifies Text-to-CAD generation and\nCAD editing, has the potential to streamline the entire design pipeline.\nHowever, prior work has not explored this setting, largely because standard\nlarge language model (LLM) tokenizers decompose CAD sequences into\nnatural-language word pieces, failing to capture primitive-level CAD semantics\nand hindering attention modules from modeling geometric structure. We\nconjecture that a multimodal tokenization strategy, aligned with CAD's\nprimitive and structural nature, can provide more effective representations. To\nthis end, we propose CAD-Tokenizer, a framework that represents CAD data with\nmodality-specific tokens using a sequence-based VQ-VAE with primitive-level\npooling and constrained decoding. This design produces compact, primitive-aware\nrepresentations that align with CAD's structural nature. Applied to unified\ntext-guided CAD prototyping, CAD-Tokenizer significantly improves instruction\nfollowing and generation quality, achieving better quantitative and qualitative\nperformance over both general-purpose LLMs and task-specific baselines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21150.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67a30a243d5a32b36c7d7d0b",
            "avatarUrl": "/avatars/4aa43f0d79797be1063f246d63638a85.svg",
            "fullname": "Ruiyu Wang",
            "name": "rywang37",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "submitterOrganization": {
            "_id": "5e6485f787403103f9f1055e",
            "name": "microsoft",
            "fullname": "Microsoft",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.19768",
            "authors": [
                {
                    "_id": "68d7bda90177a6054b0137dc",
                    "user": {
                        "_id": "62ed66743d88d075d72515fe",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659725419224-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Sina S",
                        "user": "s-jse",
                        "type": "user"
                    },
                    "name": "Sina J. Semnani",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T06:00:24.247Z",
                    "hidden": false
                },
                {
                    "_id": "68d7bda90177a6054b0137dd",
                    "name": "Han Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d7bda90177a6054b0137de",
                    "name": "Xinyan He",
                    "hidden": false
                },
                {
                    "_id": "68d7bda90177a6054b0137df",
                    "name": "Merve Tekgrler",
                    "hidden": false
                },
                {
                    "_id": "68d7bda90177a6054b0137e0",
                    "name": "Monica S. Lam",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-24T05:38:45.000Z",
            "submittedOnDailyAt": "2025-09-29T00:47:35.179Z",
            "title": "CHURRO: Making History Readable with an Open-Weight Large\n  Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition",
            "submittedOnDailyBy": {
                "_id": "62ed66743d88d075d72515fe",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659725419224-noauth.jpeg",
                "isPro": false,
                "fullname": "Sina S",
                "user": "s-jse",
                "type": "user"
            },
            "summary": "Accurate text recognition for historical documents can greatly advance the\nstudy and preservation of cultural heritage. Existing vision-language models\n(VLMs), however, are designed for modern, standardized texts and are not\nequipped to read the diverse languages and scripts, irregular layouts, and\nfrequent degradation found in historical materials.\n  This paper presents CHURRO, a 3B-parameter open-weight VLM specialized for\nhistorical text recognition. The model is trained on CHURRO-DS, the largest\nhistorical text recognition dataset to date. CHURRO-DS unifies 155 historical\ncorpora comprising 99,491 pages, spanning 22 centuries of textual heritage\nacross 46 language clusters, including historical variants and dead languages.\n  We evaluate several open-weight and closed VLMs and optical character\nrecognition (OCR) systems on CHURRO-DS and find that CHURRO outperforms all\nother VLMs. On the CHURRO-DS test set, CHURRO achieves 82.3% (printed) and\n70.1% (handwritten) normalized Levenshtein similarity, surpassing the\nsecond-best model, Gemini 2.5 Pro, by 1.4% and 6.5%, respectively, while being\n15.5 times more cost-effective.\n  By releasing the model and dataset, we aim to enable community-driven\nresearch to improve the readability of historical texts and accelerate\nscholarship.",
            "upvotes": 1,
            "discussionId": "68d7bdaa0177a6054b0137e1",
            "ai_summary": "CHURRO, a 3B-parameter open-weight vision-language model, outperforms existing models in historical text recognition using the largest dataset to date, CHURRO-DS, and is more cost-effective.",
            "ai_keywords": [
                "vision-language models",
                "historical text recognition",
                "CHURRO",
                "CHURRO-DS",
                "historical corpora",
                "language clusters",
                "optical character recognition",
                "Levenshtein similarity",
                "Gemini 2.5 Pro"
            ]
        },
        "publishedAt": "2025-09-24T01:38:45.000Z",
        "title": "CHURRO: Making History Readable with an Open-Weight Large\n  Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition",
        "summary": "Accurate text recognition for historical documents can greatly advance the\nstudy and preservation of cultural heritage. Existing vision-language models\n(VLMs), however, are designed for modern, standardized texts and are not\nequipped to read the diverse languages and scripts, irregular layouts, and\nfrequent degradation found in historical materials.\n  This paper presents CHURRO, a 3B-parameter open-weight VLM specialized for\nhistorical text recognition. The model is trained on CHURRO-DS, the largest\nhistorical text recognition dataset to date. CHURRO-DS unifies 155 historical\ncorpora comprising 99,491 pages, spanning 22 centuries of textual heritage\nacross 46 language clusters, including historical variants and dead languages.\n  We evaluate several open-weight and closed VLMs and optical character\nrecognition (OCR) systems on CHURRO-DS and find that CHURRO outperforms all\nother VLMs. On the CHURRO-DS test set, CHURRO achieves 82.3% (printed) and\n70.1% (handwritten) normalized Levenshtein similarity, surpassing the\nsecond-best model, Gemini 2.5 Pro, by 1.4% and 6.5%, respectively, while being\n15.5 times more cost-effective.\n  By releasing the model and dataset, we aim to enable community-driven\nresearch to improve the readability of historical texts and accelerate\nscholarship.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19768.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62ed66743d88d075d72515fe",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659725419224-noauth.jpeg",
            "fullname": "Sina S",
            "name": "s-jse",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "submitterOrganization": {
            "_id": "62ed67f67c64f97bcf2c708a",
            "name": "stanford-oval",
            "fullname": "Stanford Open Virtual Assistant Lab (OVAL)",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1659726066453-62ed66743d88d075d72515fe.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.20906",
            "authors": [
                {
                    "_id": "68da4dadcfa03a5158a1ed08",
                    "user": {
                        "_id": "66964ee4417bbfcd51e3ce20",
                        "avatarUrl": "/avatars/705b5dd98890e4fb7ebd9b91c9d5dc38.svg",
                        "isPro": false,
                        "fullname": "Julius Pesonen",
                        "user": "Julppe1",
                        "type": "user"
                    },
                    "name": "Julius Pesonen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T11:26:14.579Z",
                    "hidden": false
                },
                {
                    "_id": "68da4dadcfa03a5158a1ed09",
                    "name": "Arno Solin",
                    "hidden": false
                },
                {
                    "_id": "68da4dadcfa03a5158a1ed0a",
                    "name": "Eija Honkavaara",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-25T08:46:37.000Z",
            "submittedOnDailyAt": "2025-09-29T10:16:10.540Z",
            "title": "Finding 3D Positions of Distant Objects from Noisy Camera Movement and\n  Semantic Segmentation Sequences",
            "submittedOnDailyBy": {
                "_id": "66964ee4417bbfcd51e3ce20",
                "avatarUrl": "/avatars/705b5dd98890e4fb7ebd9b91c9d5dc38.svg",
                "isPro": false,
                "fullname": "Julius Pesonen",
                "user": "Julppe1",
                "type": "user"
            },
            "summary": "3D object localisation based on a sequence of camera measurements is\nessential for safety-critical surveillance tasks, such as drone-based wildfire\nmonitoring. Localisation of objects detected with a camera can typically be\nsolved with dense depth estimation or 3D scene reconstruction. However, in the\ncontext of distant objects or tasks limited by the amount of available\ncomputational resources, neither solution is feasible. In this paper, we show\nthat the task can be solved using particle filters for both single and multiple\ntarget scenarios. The method was studied using a 3D simulation and a\ndrone-based image segmentation sequence with global navigation satellite system\n(GNSS)-based camera pose estimates. The results showed that a particle filter\ncan be used to solve practical localisation tasks based on camera poses and\nimage segments in these situations where other solutions fail. The particle\nfilter is independent of the detection method, making it flexible for new\ntasks. The study also demonstrates that drone-based wildfire monitoring can be\nconducted using the proposed method paired with a pre-existing image\nsegmentation model.",
            "upvotes": 0,
            "discussionId": "68da4dadcfa03a5158a1ed0b",
            "ai_summary": "Particle filters enable 3D object localization using camera poses and image segments, suitable for drone-based surveillance with limited computational resources.",
            "ai_keywords": [
                "particle filters",
                "3D object localization",
                "camera measurements",
                "dense depth estimation",
                "3D scene reconstruction",
                "GNSS-based camera pose estimates",
                "image segmentation",
                "drone-based wildfire monitoring"
            ]
        },
        "publishedAt": "2025-09-25T04:46:37.000Z",
        "title": "Finding 3D Positions of Distant Objects from Noisy Camera Movement and\n  Semantic Segmentation Sequences",
        "summary": "3D object localisation based on a sequence of camera measurements is\nessential for safety-critical surveillance tasks, such as drone-based wildfire\nmonitoring. Localisation of objects detected with a camera can typically be\nsolved with dense depth estimation or 3D scene reconstruction. However, in the\ncontext of distant objects or tasks limited by the amount of available\ncomputational resources, neither solution is feasible. In this paper, we show\nthat the task can be solved using particle filters for both single and multiple\ntarget scenarios. The method was studied using a 3D simulation and a\ndrone-based image segmentation sequence with global navigation satellite system\n(GNSS)-based camera pose estimates. The results showed that a particle filter\ncan be used to solve practical localisation tasks based on camera poses and\nimage segments in these situations where other solutions fail. The particle\nfilter is independent of the detection method, making it flexible for new\ntasks. The study also demonstrates that drone-based wildfire monitoring can be\nconducted using the proposed method paired with a pre-existing image\nsegmentation model.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.20906.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66964ee4417bbfcd51e3ce20",
            "avatarUrl": "/avatars/705b5dd98890e4fb7ebd9b91c9d5dc38.svg",
            "fullname": "Julius Pesonen",
            "name": "Julppe1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.18420",
            "authors": [
                {
                    "_id": "68d7cca60177a6054b013828",
                    "user": {
                        "_id": "68d7c8e0f2f999edd0cfcbb4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5SyJU2oALWxHj4yKJtDk3.jpeg",
                        "isPro": false,
                        "fullname": "Nikolai Skripko",
                        "user": "NikolaiSkripko",
                        "type": "user"
                    },
                    "name": "Nikolai Skripko",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-29T06:00:16.219Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T21:04:39.000Z",
            "submittedOnDailyAt": "2025-09-29T07:52:33.469Z",
            "title": "Instruction-Following Evaluation in Function Calling for Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "68d7c8e0f2f999edd0cfcbb4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5SyJU2oALWxHj4yKJtDk3.jpeg",
                "isPro": false,
                "fullname": "Nikolai Skripko",
                "user": "NikolaiSkripko",
                "type": "user"
            },
            "summary": "Function calling is a core capability of large language models, essential for\nAI agents. Existing benchmarks such as the Berkeley Function Calling\nLeaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench\n(arXiv:2501.12851) evaluate argument correctness but do not test adherence to\nformat instructions embedded in parameter descriptions, such as enclosing\nvalues in double quotes or using ISO date formats.\n  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)\nthat assesses precise instruction following in function calling. IFEval-FC\nencodes verifiable formats directly within JSON schema descriptions, for\nexample specifying that a value must not contain punctuation. It includes 750\ntest cases, each consisting of a function with an embedded format for one of\nits input parameters and a corresponding user query. Evaluation is fully\nalgorithmic, ensuring objectivity, reproducibility, and scalability.\n  Our results show that even state-of-the-art proprietary models, including\nGPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,\nhighlighting a practical limitation for real-world agent systems. The complete\ncodebase and data are publicly available at\nhttps://github.com/Skripkon/IFEval-FC.",
            "upvotes": 0,
            "discussionId": "68d7cca60177a6054b013829",
            "githubRepo": "https://github.com/Skripkon/IFEval-FC",
            "ai_summary": "IFEval-FC is a benchmark that evaluates function calling by assessing adherence to format instructions within JSON schema descriptions, revealing that even advanced models often fail to follow basic formatting rules.",
            "ai_keywords": [
                "function calling",
                "large language models",
                "AI agents",
                "Berkeley Function Calling Leaderboard",
                "tau^2-Bench",
                "ACEBench",
                "IFEval",
                "JSON schema descriptions"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-09-22T17:04:39.000Z",
        "title": "Instruction-Following Evaluation in Function Calling for Large Language\n  Models",
        "summary": "Function calling is a core capability of large language models, essential for\nAI agents. Existing benchmarks such as the Berkeley Function Calling\nLeaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench\n(arXiv:2501.12851) evaluate argument correctness but do not test adherence to\nformat instructions embedded in parameter descriptions, such as enclosing\nvalues in double quotes or using ISO date formats.\n  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)\nthat assesses precise instruction following in function calling. IFEval-FC\nencodes verifiable formats directly within JSON schema descriptions, for\nexample specifying that a value must not contain punctuation. It includes 750\ntest cases, each consisting of a function with an embedded format for one of\nits input parameters and a corresponding user query. Evaluation is fully\nalgorithmic, ensuring objectivity, reproducibility, and scalability.\n  Our results show that even state-of-the-art proprietary models, including\nGPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,\nhighlighting a practical limitation for real-world agent systems. The complete\ncodebase and data are publicly available at\nhttps://github.com/Skripkon/IFEval-FC.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18420.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "68d7c8e0f2f999edd0cfcbb4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5SyJU2oALWxHj4yKJtDk3.jpeg",
            "fullname": "Nikolai Skripko",
            "name": "NikolaiSkripko",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]