[
    {
        "paper": {
            "id": "2509.24006",
            "authors": [
                {
                    "_id": "68db424ed2bf1f4b15ec730a",
                    "user": {
                        "_id": "66c0a08bac74db25de8427ec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
                        "isPro": false,
                        "fullname": "Jintao Zhang",
                        "user": "jt-zhang",
                        "type": "user"
                    },
                    "name": "Jintao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:32:43.935Z",
                    "hidden": false
                },
                {
                    "_id": "68db424ed2bf1f4b15ec730b",
                    "name": "Haoxu Wang",
                    "hidden": false
                },
                {
                    "_id": "68db424ed2bf1f4b15ec730c",
                    "name": "Kai Jiang",
                    "hidden": false
                },
                {
                    "_id": "68db424ed2bf1f4b15ec730d",
                    "name": "Shuo Yang",
                    "hidden": false
                },
                {
                    "_id": "68db424ed2bf1f4b15ec730e",
                    "name": "Kaiwen Zheng",
                    "hidden": false
                },
                {
                    "_id": "68db424ed2bf1f4b15ec730f",
                    "name": "Haocheng Xi",
                    "hidden": false
                },
                {
                    "_id": "68db424ed2bf1f4b15ec7310",
                    "name": "Ziteng Wang",
                    "hidden": false
                },
                {
                    "_id": "68db424ed2bf1f4b15ec7311",
                    "name": "Hongzhou Zhu",
                    "hidden": false
                },
                {
                    "_id": "68db424ed2bf1f4b15ec7312",
                    "name": "Min Zhao",
                    "hidden": false
                },
                {
                    "_id": "68db424ed2bf1f4b15ec7313",
                    "name": "Ion Stoica",
                    "hidden": false
                },
                {
                    "_id": "68db424ed2bf1f4b15ec7314",
                    "name": "Joseph E. Gonzalez",
                    "hidden": false
                },
                {
                    "_id": "68db424ed2bf1f4b15ec7315",
                    "name": "Jun Zhu",
                    "hidden": false
                },
                {
                    "_id": "68db424ed2bf1f4b15ec7316",
                    "name": "Jianfei Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T17:58:59.000Z",
            "submittedOnDailyAt": "2025-09-30T01:13:12.259Z",
            "title": "SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable\n  Sparse-Linear Attention",
            "submittedOnDailyBy": {
                "_id": "66c0a08bac74db25de8427ec",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
                "isPro": false,
                "fullname": "Jintao Zhang",
                "user": "jt-zhang",
                "type": "user"
            },
            "summary": "In Diffusion Transformer (DiT) models, particularly for video generation,\nattention latency is a major bottleneck due to the long sequence length and the\nquadratic complexity. We find that attention weights can be separated into two\nparts: a small fraction of large weights with high rank and the remaining\nweights with very low rank. This naturally suggests applying sparse\nacceleration to the first part and low-rank acceleration to the second. Based\non this finding, we propose SLA (Sparse-Linear Attention), a trainable\nattention method that fuses sparse and linear attention to accelerate diffusion\nmodels. SLA classifies attention weights into critical, marginal, and\nnegligible categories, applying O(N^2) attention to critical weights, O(N)\nattention to marginal weights, and skipping negligible ones. SLA combines these\ncomputations into a single GPU kernel and supports both forward and backward\npasses. With only a few fine-tuning steps using SLA, DiT models achieve a 20x\nreduction in attention computation, resulting in significant acceleration\nwithout loss of generation quality. Experiments show that SLA reduces attention\ncomputation by 95% without degrading end-to-end generation quality,\noutperforming baseline methods. In addition, we implement an efficient GPU\nkernel for SLA, which yields a 13.7x speedup in attention computation and a\n2.2x end-to-end speedup in video generation on Wan2.1-1.3B.",
            "upvotes": 98,
            "discussionId": "68db424fd2bf1f4b15ec7317",
            "ai_summary": "SLA, a trainable attention method combining sparse and linear attention, accelerates Diffusion Transformer models for video generation with minimal quality loss.",
            "ai_keywords": [
                "Diffusion Transformer",
                "attention latency",
                "sequence length",
                "quadratic complexity",
                "sparse acceleration",
                "low-rank acceleration",
                "SLA",
                "critical weights",
                "marginal weights",
                "negligible weights",
                "GPU kernel",
                "fine-tuning",
                "attention computation",
                "generation quality",
                "end-to-end speedup"
            ]
        },
        "publishedAt": "2025-09-28T13:58:59.000Z",
        "title": "SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable\n  Sparse-Linear Attention",
        "summary": "In Diffusion Transformer (DiT) models, particularly for video generation,\nattention latency is a major bottleneck due to the long sequence length and the\nquadratic complexity. We find that attention weights can be separated into two\nparts: a small fraction of large weights with high rank and the remaining\nweights with very low rank. This naturally suggests applying sparse\nacceleration to the first part and low-rank acceleration to the second. Based\non this finding, we propose SLA (Sparse-Linear Attention), a trainable\nattention method that fuses sparse and linear attention to accelerate diffusion\nmodels. SLA classifies attention weights into critical, marginal, and\nnegligible categories, applying O(N^2) attention to critical weights, O(N)\nattention to marginal weights, and skipping negligible ones. SLA combines these\ncomputations into a single GPU kernel and supports both forward and backward\npasses. With only a few fine-tuning steps using SLA, DiT models achieve a 20x\nreduction in attention computation, resulting in significant acceleration\nwithout loss of generation quality. Experiments show that SLA reduces attention\ncomputation by 95% without degrading end-to-end generation quality,\noutperforming baseline methods. In addition, we implement an efficient GPU\nkernel for SLA, which yields a 13.7x speedup in attention computation and a\n2.2x end-to-end speedup in video generation on Wan2.1-1.3B.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24006.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "fullname": "Jintao Zhang",
            "name": "jt-zhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 28
        },
        "submitterOrganization": {
            "_id": "628735cbc83a2d6ab8d14a66",
            "name": "Tsinghua",
            "fullname": "Tsinghua University"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.22220",
            "authors": [
                {
                    "_id": "68dba222d2bf1f4b15ec792e",
                    "user": {
                        "_id": "62b076f6bcd06fe282983c36",
                        "avatarUrl": "/avatars/0086306236782ac1ce241b296ad215b2.svg",
                        "isPro": false,
                        "fullname": "Yuhan SONG",
                        "user": "QbethQ",
                        "type": "user"
                    },
                    "name": "Yuhan Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T10:17:34.115Z",
                    "hidden": false
                },
                {
                    "_id": "68dba222d2bf1f4b15ec792f",
                    "name": "Linhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dba222d2bf1f4b15ec7930",
                    "name": "Chuhan Wu",
                    "hidden": false
                },
                {
                    "_id": "68dba222d2bf1f4b15ec7931",
                    "name": "Aiwei Liu",
                    "hidden": false
                },
                {
                    "_id": "68dba222d2bf1f4b15ec7932",
                    "name": "Wei Jia",
                    "hidden": false
                },
                {
                    "_id": "68dba222d2bf1f4b15ec7933",
                    "name": "Houfeng Wang",
                    "hidden": false
                },
                {
                    "_id": "68dba222d2bf1f4b15ec7934",
                    "name": "Xiao Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T11:32:51.000Z",
            "submittedOnDailyAt": "2025-09-30T09:11:35.111Z",
            "title": "StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient\n  SpeechLLMs",
            "submittedOnDailyBy": {
                "_id": "62b076f6bcd06fe282983c36",
                "avatarUrl": "/avatars/0086306236782ac1ce241b296ad215b2.svg",
                "isPro": false,
                "fullname": "Yuhan SONG",
                "user": "QbethQ",
                "type": "user"
            },
            "summary": "Prevalent semantic speech tokenizers, designed to capture linguistic content,\nare surprisingly fragile. We find they are not robust to meaning-irrelevant\nacoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech\nis perfectly intelligible, their output token sequences can change drastically,\nincreasing the learning burden for downstream LLMs. This instability stems from\ntwo flaws: a brittle single-path quantization architecture and a distant\ntraining signal indifferent to intermediate token stability. To address this,\nwe introduce StableToken, a tokenizer that achieves stability through a\nconsensus-driven mechanism. Its multi-branch architecture processes audio in\nparallel, and these representations are merged via a powerful bit-wise voting\nmechanism to form a single, stable token sequence. StableToken sets a new\nstate-of-the-art in token stability, drastically reducing Unit Edit Distance\n(UED) under diverse noise conditions. This foundational stability translates\ndirectly to downstream benefits, significantly improving the robustness of\nSpeechLLMs on a variety of tasks.",
            "upvotes": 57,
            "discussionId": "68dba222d2bf1f4b15ec7935",
            "ai_summary": "StableToken, a multi-branch consensus-driven tokenizer, enhances token stability and robustness in speech processing, improving SpeechLLMs' performance under noisy conditions.",
            "ai_keywords": [
                "semantic speech tokenizers",
                "acoustic perturbations",
                "Signal-to-Noise Ratios",
                "SNRs",
                "learning burden",
                "downstream LLMs",
                "brittle single-path quantization",
                "training signal",
                "multi-branch architecture",
                "bit-wise voting mechanism",
                "token stability",
                "Unit Edit Distance",
                "UED",
                "SpeechLLMs"
            ]
        },
        "publishedAt": "2025-09-26T07:32:51.000Z",
        "title": "StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient\n  SpeechLLMs",
        "summary": "Prevalent semantic speech tokenizers, designed to capture linguistic content,\nare surprisingly fragile. We find they are not robust to meaning-irrelevant\nacoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech\nis perfectly intelligible, their output token sequences can change drastically,\nincreasing the learning burden for downstream LLMs. This instability stems from\ntwo flaws: a brittle single-path quantization architecture and a distant\ntraining signal indifferent to intermediate token stability. To address this,\nwe introduce StableToken, a tokenizer that achieves stability through a\nconsensus-driven mechanism. Its multi-branch architecture processes audio in\nparallel, and these representations are merged via a powerful bit-wise voting\nmechanism to form a single, stable token sequence. StableToken sets a new\nstate-of-the-art in token stability, drastically reducing Unit Edit Distance\n(UED) under diverse noise conditions. This foundational stability translates\ndirectly to downstream benefits, significantly improving the robustness of\nSpeechLLMs on a variety of tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22220.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62b076f6bcd06fe282983c36",
            "avatarUrl": "/avatars/0086306236782ac1ce241b296ad215b2.svg",
            "fullname": "Yuhan SONG",
            "name": "QbethQ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.23102",
            "authors": [
                {
                    "_id": "68db42f2d2bf1f4b15ec7323",
                    "name": "Fang Wu",
                    "hidden": false
                },
                {
                    "_id": "68db42f2d2bf1f4b15ec7324",
                    "name": "Xu Huang",
                    "hidden": false
                },
                {
                    "_id": "68db42f2d2bf1f4b15ec7325",
                    "user": {
                        "_id": "65b8909c89eb3dfbe8d26780",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b8909c89eb3dfbe8d26780/aJExXsCN9oQOdd9bsQOEO.jpeg",
                        "isPro": false,
                        "fullname": "Weihao XUAN",
                        "user": "weihao1115",
                        "type": "user"
                    },
                    "name": "Weihao Xuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:32:41.015Z",
                    "hidden": false
                },
                {
                    "_id": "68db42f2d2bf1f4b15ec7326",
                    "name": "Zhiwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db42f2d2bf1f4b15ec7327",
                    "name": "Yijia Xiao",
                    "hidden": false
                },
                {
                    "_id": "68db42f2d2bf1f4b15ec7328",
                    "name": "Guancheng Wan",
                    "hidden": false
                },
                {
                    "_id": "68db42f2d2bf1f4b15ec7329",
                    "name": "Xiaomin Li",
                    "hidden": false
                },
                {
                    "_id": "68db42f2d2bf1f4b15ec732a",
                    "name": "Bing Hu",
                    "hidden": false
                },
                {
                    "_id": "68db42f2d2bf1f4b15ec732b",
                    "name": "Peng Xia",
                    "hidden": false
                },
                {
                    "_id": "68db42f2d2bf1f4b15ec732c",
                    "name": "Jure Leskovec",
                    "hidden": false
                },
                {
                    "_id": "68db42f2d2bf1f4b15ec732d",
                    "name": "Yejin Choi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-27T04:18:33.000Z",
            "submittedOnDailyAt": "2025-09-30T01:12:14.832Z",
            "title": "Multiplayer Nash Preference Optimization",
            "submittedOnDailyBy": {
                "_id": "675e0d5cdd3e9eeed6954f5a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/7oMEoBmaFiCR9K2q9Z_7q.png",
                "isPro": false,
                "fullname": "Fang Wu",
                "user": "fangwu97",
                "type": "user"
            },
            "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the standard\nparadigm for aligning large language models (LLMs) with human preferences.\nHowever, reward-based methods built on the Bradley-Terry assumption struggle to\ncapture the non-transitive and heterogeneous nature of real-world preferences.\nTo address this, recent studies have reframed alignment as a two-player Nash\ngame, giving rise to Nash learning from human feedback (NLHF). While this\nperspective has inspired algorithms such as INPO, ONPO, and EGPO with strong\ntheoretical and empirical guarantees, they remain fundamentally restricted to\ntwo-player interactions, creating a single-opponent bias that fails to capture\nthe full complexity of realistic preference structures. In this work, we\nintroduce Multiplayer Nash Preference Optimization (MNPO), a novel framework\nthat generalizes NLHF to the multiplayer regime. It formulates alignment as an\nn-player game, where each policy competes against a population of opponents\nwhile being regularized toward a reference model. Our framework establishes\nwell-defined Nash equilibria in multiplayer settings and extends the concept of\nduality gap to quantify approximation quality. We demonstrate that MNPO\ninherits the equilibrium guarantees of two-player methods while enabling richer\ncompetitive dynamics and improved coverage of diverse preference structures.\nThrough comprehensive empirical evaluation, we show that MNPO consistently\noutperforms existing NLHF baselines on instruction-following benchmarks,\nachieving superior alignment quality under heterogeneous annotator conditions\nand mixed-policy evaluation scenarios. Together, these results establish MNPO\nas a principled and scalable framework for aligning LLMs with complex,\nnon-transitive human preferences. Code is available at\nhttps://github.com/smiles724/MNPO.",
            "upvotes": 52,
            "discussionId": "68db42f3d2bf1f4b15ec732e",
            "ai_summary": "Multiplayer Nash Preference Optimization (MNPO) extends Nash learning from human feedback to handle complex, non-transitive human preferences by formulating alignment as an n-player game.",
            "ai_keywords": [
                "Reinforcement learning from human feedback",
                "RLHF",
                "large language models",
                "LLMs",
                "Bradley-Terry assumption",
                "Nash learning from human feedback",
                "NLHF",
                "INPO",
                "ONPO",
                "EGPO",
                "Multiplayer Nash Preference Optimization",
                "MNPO",
                "Nash equilibria",
                "duality gap",
                "instruction-following benchmarks",
                "heterogeneous annotator conditions",
                "mixed-policy evaluation"
            ]
        },
        "publishedAt": "2025-09-27T00:18:33.000Z",
        "title": "Multiplayer Nash Preference Optimization",
        "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the standard\nparadigm for aligning large language models (LLMs) with human preferences.\nHowever, reward-based methods built on the Bradley-Terry assumption struggle to\ncapture the non-transitive and heterogeneous nature of real-world preferences.\nTo address this, recent studies have reframed alignment as a two-player Nash\ngame, giving rise to Nash learning from human feedback (NLHF). While this\nperspective has inspired algorithms such as INPO, ONPO, and EGPO with strong\ntheoretical and empirical guarantees, they remain fundamentally restricted to\ntwo-player interactions, creating a single-opponent bias that fails to capture\nthe full complexity of realistic preference structures. In this work, we\nintroduce Multiplayer Nash Preference Optimization (MNPO), a novel framework\nthat generalizes NLHF to the multiplayer regime. It formulates alignment as an\nn-player game, where each policy competes against a population of opponents\nwhile being regularized toward a reference model. Our framework establishes\nwell-defined Nash equilibria in multiplayer settings and extends the concept of\nduality gap to quantify approximation quality. We demonstrate that MNPO\ninherits the equilibrium guarantees of two-player methods while enabling richer\ncompetitive dynamics and improved coverage of diverse preference structures.\nThrough comprehensive empirical evaluation, we show that MNPO consistently\noutperforms existing NLHF baselines on instruction-following benchmarks,\nachieving superior alignment quality under heterogeneous annotator conditions\nand mixed-policy evaluation scenarios. Together, these results establish MNPO\nas a principled and scalable framework for aligning LLMs with complex,\nnon-transitive human preferences. Code is available at\nhttps://github.com/smiles724/MNPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23102.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "675e0d5cdd3e9eeed6954f5a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/7oMEoBmaFiCR9K2q9Z_7q.png",
            "fullname": "Fang Wu",
            "name": "fangwu97",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "submitterOrganization": {
            "_id": "6112d84f8c2e1f4060908c9e",
            "name": "stanfordnlp",
            "fullname": "Stanford NLP",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1628624969199-6032802e1f993496bc14d9e3.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.24897",
            "authors": [
                {
                    "_id": "68db4c16d2bf1f4b15ec744e",
                    "user": {
                        "_id": "673c7319d11b1c2e246ead9c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
                        "isPro": false,
                        "fullname": "Yang Shi",
                        "user": "DogNeverSleep",
                        "type": "user"
                    },
                    "name": "Yang Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:29:34.746Z",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec744f",
                    "user": {
                        "_id": "652965773a416e1f2173443b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
                        "isPro": false,
                        "fullname": "Yuhao Dong",
                        "user": "THUdyh",
                        "type": "user"
                    },
                    "name": "Yuhao Dong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T10:18:06.242Z",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7450",
                    "name": "Yue Ding",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7451",
                    "name": "Yuran Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7452",
                    "name": "Xuanyu Zhu",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7453",
                    "name": "Sheng Zhou",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7454",
                    "name": "Wenting Liu",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7455",
                    "name": "Haochen Tian",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7456",
                    "name": "Rundong Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7457",
                    "name": "Huanqian Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7458",
                    "name": "Zuyan Liu",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7459",
                    "name": "Bohan Zeng",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec745a",
                    "name": "Ruizhe Chen",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec745b",
                    "name": "Qixun Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec745c",
                    "name": "Zhuoran Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec745d",
                    "name": "Xinlong Chen",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec745e",
                    "name": "Chengzhuo Tong",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec745f",
                    "user": {
                        "_id": "661e62c6bac5d981f886f77b",
                        "avatarUrl": "/avatars/f1eb51ed4499ca434c8939573dfbd5e2.svg",
                        "isPro": false,
                        "fullname": "Bozhou Li",
                        "user": "zooblastlbz",
                        "type": "user"
                    },
                    "name": "Bozhou Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T07:12:22.595Z",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7460",
                    "name": "Chaoyou Fu",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7461",
                    "name": "Qiang Liu",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7462",
                    "name": "Haotian Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7463",
                    "name": "Wenjing Yang",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7464",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7465",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7466",
                    "name": "Yi-Fan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4c16d2bf1f4b15ec7467",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T15:07:28.000Z",
            "submittedOnDailyAt": "2025-09-30T01:50:34.924Z",
            "title": "RealUnify: Do Unified Models Truly Benefit from Unification? A\n  Comprehensive Benchmark",
            "submittedOnDailyBy": {
                "_id": "673c7319d11b1c2e246ead9c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
                "isPro": false,
                "fullname": "Yang Shi",
                "user": "DogNeverSleep",
                "type": "user"
            },
            "summary": "The integration of visual understanding and generation into unified\nmultimodal models represents a significant stride toward general-purpose AI.\nHowever, a fundamental question remains unanswered by existing benchmarks: does\nthis architectural unification actually enable synergetic interaction between\nthe constituent capabilities? Existing evaluation paradigms, which primarily\nassess understanding and generation in isolation, are insufficient for\ndetermining whether a unified model can leverage its understanding to enhance\nits generation, or use generative simulation to facilitate deeper\ncomprehension. To address this critical gap, we introduce RealUnify, a\nbenchmark specifically designed to evaluate bidirectional capability synergy.\nRealUnify comprises 1,000 meticulously human-annotated instances spanning 10\ncategories and 32 subtasks. It is structured around two core axes: 1)\nUnderstanding Enhances Generation, which requires reasoning (e.g., commonsense,\nlogic) to guide image generation, and 2) Generation Enhances Understanding,\nwhich necessitates mental simulation or reconstruction (e.g., of transformed or\ndisordered visual inputs) to solve reasoning tasks. A key contribution is our\ndual-evaluation protocol, which combines direct end-to-end assessment with a\ndiagnostic stepwise evaluation that decomposes tasks into distinct\nunderstanding and generation phases. This protocol allows us to precisely\ndiscern whether performance bottlenecks stem from deficiencies in core\nabilities or from a failure to integrate them. Through large-scale evaluations\nof 12 leading unified models and 6 specialized baselines, we find that current\nunified models still struggle to achieve effective synergy, indicating that\narchitectural unification alone is insufficient. These results highlight the\nneed for new training strategies and inductive biases to fully unlock the\npotential of unified modeling.",
            "upvotes": 41,
            "discussionId": "68db4c17d2bf1f4b15ec7468",
            "githubRepo": "https://github.com/FrankYang-17/RealUnify",
            "ai_summary": "RealUnify evaluates the bidirectional synergy between understanding and generation in unified multimodal models, revealing that current models lack effective integration despite architectural unification.",
            "ai_keywords": [
                "multimodal models",
                "RealUnify",
                "bidirectional capability synergy",
                "understanding enhances generation",
                "generation enhances understanding",
                "dual-evaluation protocol",
                "unified models",
                "specialized baselines"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-09-29T11:07:28.000Z",
        "title": "RealUnify: Do Unified Models Truly Benefit from Unification? A\n  Comprehensive Benchmark",
        "summary": "The integration of visual understanding and generation into unified\nmultimodal models represents a significant stride toward general-purpose AI.\nHowever, a fundamental question remains unanswered by existing benchmarks: does\nthis architectural unification actually enable synergetic interaction between\nthe constituent capabilities? Existing evaluation paradigms, which primarily\nassess understanding and generation in isolation, are insufficient for\ndetermining whether a unified model can leverage its understanding to enhance\nits generation, or use generative simulation to facilitate deeper\ncomprehension. To address this critical gap, we introduce RealUnify, a\nbenchmark specifically designed to evaluate bidirectional capability synergy.\nRealUnify comprises 1,000 meticulously human-annotated instances spanning 10\ncategories and 32 subtasks. It is structured around two core axes: 1)\nUnderstanding Enhances Generation, which requires reasoning (e.g., commonsense,\nlogic) to guide image generation, and 2) Generation Enhances Understanding,\nwhich necessitates mental simulation or reconstruction (e.g., of transformed or\ndisordered visual inputs) to solve reasoning tasks. A key contribution is our\ndual-evaluation protocol, which combines direct end-to-end assessment with a\ndiagnostic stepwise evaluation that decomposes tasks into distinct\nunderstanding and generation phases. This protocol allows us to precisely\ndiscern whether performance bottlenecks stem from deficiencies in core\nabilities or from a failure to integrate them. Through large-scale evaluations\nof 12 leading unified models and 6 specialized baselines, we find that current\nunified models still struggle to achieve effective synergy, indicating that\narchitectural unification alone is insufficient. These results highlight the\nneed for new training strategies and inductive biases to fully unlock the\npotential of unified modeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24897.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "673c7319d11b1c2e246ead9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
            "fullname": "Yang Shi",
            "name": "DogNeverSleep",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.24900",
            "authors": [
                {
                    "_id": "68db4e25d2bf1f4b15ec747c",
                    "name": "Zhihong Chen",
                    "hidden": false
                },
                {
                    "_id": "68db4e25d2bf1f4b15ec747d",
                    "name": "Xuehai Bai",
                    "hidden": false
                },
                {
                    "_id": "68db4e25d2bf1f4b15ec747e",
                    "user": {
                        "_id": "673c7319d11b1c2e246ead9c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
                        "isPro": false,
                        "fullname": "Yang Shi",
                        "user": "DogNeverSleep",
                        "type": "user"
                    },
                    "name": "Yang Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:34:34.223Z",
                    "hidden": false
                },
                {
                    "_id": "68db4e25d2bf1f4b15ec747f",
                    "name": "Chaoyou Fu",
                    "hidden": false
                },
                {
                    "_id": "68db4e25d2bf1f4b15ec7480",
                    "name": "Huanyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4e25d2bf1f4b15ec7481",
                    "name": "Haotian Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4e25d2bf1f4b15ec7482",
                    "name": "Xiaoyan Sun",
                    "hidden": false
                },
                {
                    "_id": "68db4e25d2bf1f4b15ec7483",
                    "name": "Zhang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4e25d2bf1f4b15ec7484",
                    "name": "Liang Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4e25d2bf1f4b15ec7485",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4e25d2bf1f4b15ec7486",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "68db4e25d2bf1f4b15ec7487",
                    "name": "Yi-Fan Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T15:11:09.000Z",
            "submittedOnDailyAt": "2025-09-30T01:57:57.108Z",
            "title": "OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation\n  and Editing",
            "submittedOnDailyBy": {
                "_id": "673c7319d11b1c2e246ead9c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
                "isPro": false,
                "fullname": "Yang Shi",
                "user": "DogNeverSleep",
                "type": "user"
            },
            "summary": "The performance of unified multimodal models for image generation and editing\nis fundamentally constrained by the quality and comprehensiveness of their\ntraining data. While existing datasets have covered basic tasks like style\ntransfer and simple object manipulation, they often lack the systematic\nstructure and challenging scenarios required for real-world applications. To\naddress this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset\nconstructed using a novel methodology that combines hierarchical task taxonomy\nwith automated data generation. Our taxonomy not only includes fundamental\ncapabilities such as text rendering and style control but also introduces\nhighly practical yet challenging categories like scientific imagery for\nchemistry illustrations and complex instruction editing requiring simultaneous\nexecution of multiple operations. Through an automated pipeline leveraging\nstructured resource pools and GPT-4o, we generate 80k high-quality\ninstruction-image pairs with controlled diversity, covering 11 major domains\nand 51 subtasks. Extensive experiments show that fine-tuning leading models on\nour dataset achieves significant performance gains across multiple benchmarks,\nwith improvements of up to 18\\% on editing tasks (UniWorld-V1 on ImgEdit-Bench)\nand 13% on generation tasks (Harmon on GenEval). Our work demonstrates that\nsystematic data construction is key to advancing multimodal AI capabilities.",
            "upvotes": 38,
            "discussionId": "68db4e25d2bf1f4b15ec7488",
            "ai_summary": "OpenGPT-4o-Image, a large-scale dataset with hierarchical task taxonomy and automated generation, significantly improves performance in image generation and editing tasks.",
            "ai_keywords": [
                "unified multimodal models",
                "image generation",
                "image editing",
                "training data",
                "hierarchical task taxonomy",
                "automated data generation",
                "text rendering",
                "style control",
                "scientific imagery",
                "complex instruction editing",
                "instruction-image pairs",
                "controlled diversity",
                "fine-tuning",
                "UniWorld-V1",
                "ImgEdit-Bench",
                "Harmon",
                "GenEval"
            ]
        },
        "publishedAt": "2025-09-29T11:11:09.000Z",
        "title": "OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation\n  and Editing",
        "summary": "The performance of unified multimodal models for image generation and editing\nis fundamentally constrained by the quality and comprehensiveness of their\ntraining data. While existing datasets have covered basic tasks like style\ntransfer and simple object manipulation, they often lack the systematic\nstructure and challenging scenarios required for real-world applications. To\naddress this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset\nconstructed using a novel methodology that combines hierarchical task taxonomy\nwith automated data generation. Our taxonomy not only includes fundamental\ncapabilities such as text rendering and style control but also introduces\nhighly practical yet challenging categories like scientific imagery for\nchemistry illustrations and complex instruction editing requiring simultaneous\nexecution of multiple operations. Through an automated pipeline leveraging\nstructured resource pools and GPT-4o, we generate 80k high-quality\ninstruction-image pairs with controlled diversity, covering 11 major domains\nand 51 subtasks. Extensive experiments show that fine-tuning leading models on\nour dataset achieves significant performance gains across multiple benchmarks,\nwith improvements of up to 18\\% on editing tasks (UniWorld-V1 on ImgEdit-Bench)\nand 13% on generation tasks (Harmon on GenEval). Our work demonstrates that\nsystematic data construction is key to advancing multimodal AI capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24900.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "673c7319d11b1c2e246ead9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
            "fullname": "Yang Shi",
            "name": "DogNeverSleep",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.23808",
            "authors": [
                {
                    "_id": "68db671fd2bf1f4b15ec762e",
                    "user": {
                        "_id": "666060eefff0e904a291c9ca",
                        "avatarUrl": "/avatars/56c43cc0ee3376a3e4d2d78fc641804e.svg",
                        "isPro": false,
                        "fullname": "Fanding Huang",
                        "user": "Niugan",
                        "type": "user"
                    },
                    "name": "Fanding Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:29:22.925Z",
                    "hidden": false
                },
                {
                    "_id": "68db671fd2bf1f4b15ec762f",
                    "user": {
                        "_id": "68db7b19a72e7113caffb1cb",
                        "avatarUrl": "/avatars/355871f30bdc97141004d15b2dc2378c.svg",
                        "isPro": false,
                        "fullname": "Guanbo huang",
                        "user": "Gambel",
                        "type": "user"
                    },
                    "name": "Guanbo Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T07:12:27.555Z",
                    "hidden": false
                },
                {
                    "_id": "68db671fd2bf1f4b15ec7630",
                    "user": {
                        "_id": "670bd8e972c44e17cd6900bd",
                        "avatarUrl": "/avatars/898137782df8762e3a6ca4451dd871ea.svg",
                        "isPro": false,
                        "fullname": "Fan Xiao",
                        "user": "AnikiFan",
                        "type": "user"
                    },
                    "name": "Xiao Fan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:29:26.824Z",
                    "hidden": false
                },
                {
                    "_id": "68db671fd2bf1f4b15ec7631",
                    "name": "Yi He",
                    "hidden": false
                },
                {
                    "_id": "68db671fd2bf1f4b15ec7632",
                    "user": {
                        "_id": "6560763e152b659e623865ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6560763e152b659e623865ae/cTT2jGnPU_8XMrUTvqZ2h.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Liang",
                        "user": "MasterVito",
                        "type": "user"
                    },
                    "name": "Xiao Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:00:17.152Z",
                    "hidden": false
                },
                {
                    "_id": "68db671fd2bf1f4b15ec7633",
                    "user": {
                        "_id": "676fcb88f40e1460661f0d9b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U-npvTTY6ub1bjIHbjVjm.png",
                        "isPro": false,
                        "fullname": "Xiao Chen",
                        "user": "Cevaaa",
                        "type": "user"
                    },
                    "name": "Xiao Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:00:14.332Z",
                    "hidden": false
                },
                {
                    "_id": "68db671fd2bf1f4b15ec7634",
                    "name": "Qinting Jiang",
                    "hidden": false
                },
                {
                    "_id": "68db671fd2bf1f4b15ec7635",
                    "name": "Faisal Nadeem Khan",
                    "hidden": false
                },
                {
                    "_id": "68db671fd2bf1f4b15ec7636",
                    "name": "Jingyan Jiang",
                    "hidden": false
                },
                {
                    "_id": "68db671fd2bf1f4b15ec7637",
                    "name": "Zhi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T11:14:58.000Z",
            "submittedOnDailyAt": "2025-09-30T03:45:57.562Z",
            "title": "Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach\n  for LLM Reasoning in RLVR",
            "submittedOnDailyBy": {
                "_id": "6560763e152b659e623865ae",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6560763e152b659e623865ae/cTT2jGnPU_8XMrUTvqZ2h.jpeg",
                "isPro": false,
                "fullname": "Xiao Liang",
                "user": "MasterVito",
                "type": "user"
            },
            "summary": "A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR)\ninterprets recent progress through the lens of an exploration-exploitation\ntrade-off, a perspective largely shaped by token-level metrics. We re-examine\nthis perspective, proposing that this perceived trade-off may not be a\nfundamental constraint but rather an artifact of the measurement level. To\ninvestigate this, we shift the analysis to the semantically rich hidden-state\nspace, adopting Effective Rank (ER) to quantify exploration and proposing its\nnovel first- and second-order derivatives, named Effective Rank Velocity (ERV)\nand Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our\nanalysis reveals that at the hidden-state level, exploration and exploitation\ncould be decoupled (Sec. 4). This finding reveals an opportunity to enhance\nboth capacities simultaneously. This insight motivates our method,\nVelocity-Exploiting Rank-Learning (VERL), the first to operationalize the\nprinciple of synergistic exploration-exploitation enhancement by directly\nshaping the RL advantage function. The key innovation is leveraging the\ntheoretically stable ERA as a predictive meta-controller to create a\nsynergistic, dual-channel incentive structure. Instead of forcing a trade-off,\nVERL prospectively amplifies rewards for exploration to preempt overconfidence\nand reinforces exploitative gains to consolidate reasoning. Experiments across\ndiverse LLMs and reasoning benchmarks show consistent gains, including up to\n21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.",
            "upvotes": 38,
            "discussionId": "68db6720d2bf1f4b15ec7638",
            "projectPage": "https://hf618.github.io/VERL.github.io/",
            "githubRepo": "https://github.com/hf618/VERL",
            "ai_summary": "Re-examining the exploration-exploitation trade-off in Reinforcement Learning for Verifiable Rewards through hidden-state analysis reveals opportunities for simultaneous enhancement using Effective Rank and its derivatives, leading to improved performance in diverse benchmarks.",
            "ai_keywords": [
                "Reinforcement Learning for Verifiable Rewards",
                "exploration-exploitation trade-off",
                "token-level metrics",
                "semantically rich hidden-state space",
                "Effective Rank",
                "Effective Rank Velocity",
                "Effective Rank Acceleration",
                "Velocity-Exploiting Rank-Learning",
                "RL advantage function",
                "predictive meta-controller",
                "dual-channel incentive structure",
                "Gaokao 2024 dataset"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-09-28T07:14:58.000Z",
        "title": "Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach\n  for LLM Reasoning in RLVR",
        "summary": "A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR)\ninterprets recent progress through the lens of an exploration-exploitation\ntrade-off, a perspective largely shaped by token-level metrics. We re-examine\nthis perspective, proposing that this perceived trade-off may not be a\nfundamental constraint but rather an artifact of the measurement level. To\ninvestigate this, we shift the analysis to the semantically rich hidden-state\nspace, adopting Effective Rank (ER) to quantify exploration and proposing its\nnovel first- and second-order derivatives, named Effective Rank Velocity (ERV)\nand Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our\nanalysis reveals that at the hidden-state level, exploration and exploitation\ncould be decoupled (Sec. 4). This finding reveals an opportunity to enhance\nboth capacities simultaneously. This insight motivates our method,\nVelocity-Exploiting Rank-Learning (VERL), the first to operationalize the\nprinciple of synergistic exploration-exploitation enhancement by directly\nshaping the RL advantage function. The key innovation is leveraging the\ntheoretically stable ERA as a predictive meta-controller to create a\nsynergistic, dual-channel incentive structure. Instead of forcing a trade-off,\nVERL prospectively amplifies rewards for exploration to preempt overconfidence\nand reinforces exploitative gains to consolidate reasoning. Experiments across\ndiverse LLMs and reasoning benchmarks show consistent gains, including up to\n21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23808.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6560763e152b659e623865ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6560763e152b659e623865ae/cTT2jGnPU_8XMrUTvqZ2h.jpeg",
            "fullname": "Xiao Liang",
            "name": "MasterVito",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "submitterOrganization": {
            "_id": "628735cbc83a2d6ab8d14a66",
            "name": "Tsinghua",
            "fullname": "Tsinghua University"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.24695",
            "authors": [
                {
                    "_id": "68db4b9ad2bf1f4b15ec7425",
                    "user": {
                        "_id": "645b5b09bc7518912e1f9733",
                        "avatarUrl": "/avatars/4d35f728b41f93881a9b67c337f4d1df.svg",
                        "isPro": false,
                        "fullname": "Chen",
                        "user": "Lawrence-cj",
                        "type": "user"
                    },
                    "name": "Junsong Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:29:43.494Z",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec7426",
                    "name": "Yuyang Zhao",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec7427",
                    "name": "Jincheng Yu",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec7428",
                    "name": "Ruihang Chu",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec7429",
                    "name": "Junyu Chen",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec742a",
                    "name": "Shuai Yang",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec742b",
                    "name": "Xianbang Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec742c",
                    "name": "Yicheng Pan",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec742d",
                    "name": "Daquan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec742e",
                    "name": "Huan Ling",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec742f",
                    "name": "Haozhe Liu",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec7430",
                    "name": "Hongwei Yi",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec7431",
                    "name": "Hao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec7432",
                    "user": {
                        "_id": "63129589bbaa385279d1826e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63129589bbaa385279d1826e/0AUs3q4ngRZ-wXuY1jP9G.jpeg",
                        "isPro": true,
                        "fullname": "Muyang Li",
                        "user": "Lmxyy",
                        "type": "user"
                    },
                    "name": "Muyang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:29:40.674Z",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec7433",
                    "name": "Yukang Chen",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec7434",
                    "name": "Han Cai",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec7435",
                    "name": "Sanja Fidler",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec7436",
                    "name": "Ping Luo",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec7437",
                    "name": "Song Han",
                    "hidden": false
                },
                {
                    "_id": "68db4b9ad2bf1f4b15ec7438",
                    "name": "Enze Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T12:28:09.000Z",
            "submittedOnDailyAt": "2025-09-30T01:49:24.742Z",
            "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
            "submittedOnDailyBy": {
                "_id": "64638bd36c27a7e33b26654b",
                "avatarUrl": "/avatars/2ef5aeb94ef7016082975b4cc201873e.svg",
                "isPro": false,
                "fullname": "Yuyang",
                "user": "Yuyang-z",
                "type": "user"
            },
            "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
            "upvotes": 35,
            "discussionId": "68db4b9bd2bf1f4b15ec7439",
            "projectPage": "https://nvlabs.github.io/Sana/Video",
            "ai_summary": "SANA-Video, a small diffusion model, efficiently generates high-resolution, high-quality videos with strong text-video alignment using linear attention and a constant-memory KV cache, achieving competitive performance at a lower cost and faster speed.",
            "ai_keywords": [
                "diffusion model",
                "Linear DiT",
                "linear attention",
                "constant-memory KV cache",
                "block-wise autoregressive",
                "text-video alignment",
                "MovieGen",
                "NVFP4 precision"
            ]
        },
        "publishedAt": "2025-09-29T08:28:09.000Z",
        "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
        "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24695.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64638bd36c27a7e33b26654b",
            "avatarUrl": "/avatars/2ef5aeb94ef7016082975b4cc201873e.svg",
            "fullname": "Yuyang",
            "name": "Yuyang-z",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "submitterOrganization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25190",
            "authors": [
                {
                    "_id": "68db41e1d2bf1f4b15ec72f5",
                    "name": "Penghao Wu",
                    "hidden": false
                },
                {
                    "_id": "68db41e1d2bf1f4b15ec72f6",
                    "name": "Yushan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db41e1d2bf1f4b15ec72f7",
                    "user": {
                        "_id": "64b4a717aa03b6520839e9b8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/Rt3ERG-6BVEA4hAwOz0_I.jpeg",
                        "isPro": false,
                        "fullname": "Haiwen Diao",
                        "user": "Paranioar",
                        "type": "user"
                    },
                    "name": "Haiwen Diao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:32:48.312Z",
                    "hidden": false
                },
                {
                    "_id": "68db41e1d2bf1f4b15ec72f8",
                    "user": {
                        "_id": "62d3f7d84b0933c48f3cdd9c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d3f7d84b0933c48f3cdd9c/rSoSyH0Td9fD9HVlyK7bh.jpeg",
                        "isPro": true,
                        "fullname": "Bo Li",
                        "user": "luodian",
                        "type": "user"
                    },
                    "name": "Bo Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:32:57.152Z",
                    "hidden": false
                },
                {
                    "_id": "68db41e1d2bf1f4b15ec72f9",
                    "name": "Lewei Lu",
                    "hidden": false
                },
                {
                    "_id": "68db41e1d2bf1f4b15ec72fa",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:59:57.000Z",
            "submittedOnDailyAt": "2025-09-30T01:06:08.539Z",
            "title": "Visual Jigsaw Post-Training Improves MLLMs",
            "submittedOnDailyBy": {
                "_id": "64101f81b27543634e377fc1",
                "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
                "isPro": false,
                "fullname": "Penghao Wu",
                "user": "craigwu",
                "type": "user"
            },
            "summary": "Reinforcement learning based post-training has recently emerged as a powerful\nparadigm for enhancing the alignment and reasoning capabilities of multimodal\nlarge language models (MLLMs). While vision-centric post-training is crucial\nfor enhancing MLLMs' intrinsic understanding of visual signals, current\npost-training paradigms are predominantly text-centric, where dense visual\ninputs are only leveraged to extract sparse cues for text-based reasoning.\nThere exist a few approaches in this direction, however, they often still rely\non text as an intermediate mediator or introduce additional visual generative\ndesigns. In this work, we introduce Visual Jigsaw, a generic self-supervised\npost-training framework designed to strengthen visual understanding in MLLMs.\nVisual Jigsaw is formulated as a general ordering task: visual inputs are\npartitioned, shuffled, and the model must reconstruct the visual information by\nproducing the correct permutation in natural language. This naturally aligns\nwith reinforcement learning from verifiable rewards (RLVR), requires no\nadditional visual generative components, and derives its supervisory signal\nautomatically without any annotations. We instantiate Visual Jigsaw across\nthree visual modalities, including images, videos, and 3D data. Extensive\nexperiments demonstrate substantial improvements in fine-grained perception,\ntemporal reasoning, and 3D spatial understanding. Our findings highlight the\npotential of self-supervised vision-centric tasks in post-training MLLMs and\naim to inspire further research on vision-centric pretext designs. Project\nPage: https://penghao-wu.github.io/visual_jigsaw/",
            "upvotes": 33,
            "discussionId": "68db41e1d2bf1f4b15ec72fb",
            "projectPage": "https://penghao-wu.github.io/visual_jigsaw/",
            "githubRepo": "https://github.com/penghao-wu/visual_jigsaw",
            "ai_summary": "Visual Jigsaw, a self-supervised reinforcement learning framework, enhances multimodal large language models' visual understanding through a permutation task without additional annotations or generative components.",
            "ai_keywords": [
                "reinforcement learning",
                "post-training",
                "multimodal large language models",
                "visual understanding",
                "self-supervised",
                "permutation task",
                "reinforcement learning from verifiable rewards",
                "fine-grained perception",
                "temporal reasoning",
                "3D spatial understanding"
            ],
            "githubStars": 22
        },
        "publishedAt": "2025-09-29T13:59:57.000Z",
        "title": "Visual Jigsaw Post-Training Improves MLLMs",
        "summary": "Reinforcement learning based post-training has recently emerged as a powerful\nparadigm for enhancing the alignment and reasoning capabilities of multimodal\nlarge language models (MLLMs). While vision-centric post-training is crucial\nfor enhancing MLLMs' intrinsic understanding of visual signals, current\npost-training paradigms are predominantly text-centric, where dense visual\ninputs are only leveraged to extract sparse cues for text-based reasoning.\nThere exist a few approaches in this direction, however, they often still rely\non text as an intermediate mediator or introduce additional visual generative\ndesigns. In this work, we introduce Visual Jigsaw, a generic self-supervised\npost-training framework designed to strengthen visual understanding in MLLMs.\nVisual Jigsaw is formulated as a general ordering task: visual inputs are\npartitioned, shuffled, and the model must reconstruct the visual information by\nproducing the correct permutation in natural language. This naturally aligns\nwith reinforcement learning from verifiable rewards (RLVR), requires no\nadditional visual generative components, and derives its supervisory signal\nautomatically without any annotations. We instantiate Visual Jigsaw across\nthree visual modalities, including images, videos, and 3D data. Extensive\nexperiments demonstrate substantial improvements in fine-grained perception,\ntemporal reasoning, and 3D spatial understanding. Our findings highlight the\npotential of self-supervised vision-centric tasks in post-training MLLMs and\naim to inspire further research on vision-centric pretext designs. Project\nPage: https://penghao-wu.github.io/visual_jigsaw/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25190.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64101f81b27543634e377fc1",
            "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
            "fullname": "Penghao Wu",
            "name": "craigwu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 16
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.23426",
            "authors": [
                {
                    "_id": "68db40a4d2bf1f4b15ec72e8",
                    "name": "Shanghua Gao",
                    "hidden": false
                },
                {
                    "_id": "68db40a4d2bf1f4b15ec72e9",
                    "name": "Richard Zhu",
                    "hidden": false
                },
                {
                    "_id": "68db40a4d2bf1f4b15ec72ea",
                    "name": "Pengwei Sui",
                    "hidden": false
                },
                {
                    "_id": "68db40a4d2bf1f4b15ec72eb",
                    "user": {
                        "_id": "5f2c36551ebc8c6ede2f0e53",
                        "avatarUrl": "/avatars/e3ddbd15f50b86958377b5fc2460a57e.svg",
                        "isPro": false,
                        "fullname": "Tony Kong",
                        "user": "TonyK",
                        "type": "user"
                    },
                    "name": "Zhenglun Kong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:34:01.295Z",
                    "hidden": false
                },
                {
                    "_id": "68db40a4d2bf1f4b15ec72ec",
                    "name": "Sufian Aldogom",
                    "hidden": false
                },
                {
                    "_id": "68db40a4d2bf1f4b15ec72ed",
                    "name": "Yepeng Huang",
                    "hidden": false
                },
                {
                    "_id": "68db40a4d2bf1f4b15ec72ee",
                    "name": "Ayush Noori",
                    "hidden": false
                },
                {
                    "_id": "68db40a4d2bf1f4b15ec72ef",
                    "user": {
                        "_id": "686445c64b2dd0745a049817",
                        "avatarUrl": "/avatars/34c4b2ef075f80baf9376cbcfec841d6.svg",
                        "isPro": false,
                        "fullname": "Reza Shamji",
                        "user": "rezashamji",
                        "type": "user"
                    },
                    "name": "Reza Shamji",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:33:00.082Z",
                    "hidden": false
                },
                {
                    "_id": "68db40a4d2bf1f4b15ec72f0",
                    "name": "Krishna Parvataneni",
                    "hidden": false
                },
                {
                    "_id": "68db40a4d2bf1f4b15ec72f1",
                    "name": "Theodoros Tsiligkaridis",
                    "hidden": false
                },
                {
                    "_id": "68db40a4d2bf1f4b15ec72f2",
                    "name": "Marinka Zitnik",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6350fc5ba8822aadf571304f/DrcSsDtZXCxIzFp92C15P.mp4"
            ],
            "publishedAt": "2025-09-27T17:38:53.000Z",
            "submittedOnDailyAt": "2025-09-30T01:11:08.584Z",
            "title": "Democratizing AI scientists using ToolUniverse",
            "submittedOnDailyBy": {
                "_id": "6350fc5ba8822aadf571304f",
                "avatarUrl": "/avatars/19686add3cbdaef5772b913152333f9b.svg",
                "isPro": false,
                "fullname": "gasvn",
                "user": "shgao",
                "type": "user"
            },
            "summary": "AI scientists are emerging computational systems that serve as collaborative\npartners in discovery. These systems remain difficult to build because they are\nbespoke, tied to rigid workflows, and lack shared environments that unify\ntools, data, and analyses into a common ecosystem. In omics, unified ecosystems\nhave transformed research by enabling interoperability, reuse, and\ncommunity-driven development; AI scientists require comparable infrastructure.\nWe present ToolUniverse, an ecosystem for building AI scientists from any\nlanguage or reasoning model, whether open or closed. TOOLUNIVERSE standardizes\nhow AI scientists identify and call tools, integrating more than 600 machine\nlearning models, datasets, APIs, and scientific packages for data analysis,\nknowledge retrieval, and experimental design. It automatically refines tool\ninterfaces for correct use by AI scientists, creates new tools from natural\nlanguage descriptions, iteratively optimizes tool specifications, and composes\ntools into agentic workflows. In a case study of hypercholesterolemia,\nToolUniverse was used to create an AI scientist to identify a potent analog of\na drug with favorable predicted properties. The open-source ToolUniverse is\navailable at https://aiscientist.tools.",
            "upvotes": 33,
            "discussionId": "68db40a4d2bf1f4b15ec72f3",
            "projectPage": "https://aiscientist.tools/",
            "githubRepo": "https://github.com/mims-harvard/ToolUniverse",
            "ai_summary": "ToolUniverse is an ecosystem that standardizes and integrates tools, models, and data for AI scientists, enabling automated refinement, creation, and composition of workflows.",
            "ai_keywords": [
                "AI scientists",
                "ToolUniverse",
                "machine learning models",
                "datasets",
                "APIs",
                "scientific packages",
                "data analysis",
                "knowledge retrieval",
                "experimental design",
                "tool interfaces",
                "natural language descriptions",
                "tool specifications",
                "agentic workflows",
                "hypercholesterolemia",
                "drug analogs"
            ],
            "githubStars": 253
        },
        "publishedAt": "2025-09-27T13:38:53.000Z",
        "title": "Democratizing AI scientists using ToolUniverse",
        "summary": "AI scientists are emerging computational systems that serve as collaborative\npartners in discovery. These systems remain difficult to build because they are\nbespoke, tied to rigid workflows, and lack shared environments that unify\ntools, data, and analyses into a common ecosystem. In omics, unified ecosystems\nhave transformed research by enabling interoperability, reuse, and\ncommunity-driven development; AI scientists require comparable infrastructure.\nWe present ToolUniverse, an ecosystem for building AI scientists from any\nlanguage or reasoning model, whether open or closed. TOOLUNIVERSE standardizes\nhow AI scientists identify and call tools, integrating more than 600 machine\nlearning models, datasets, APIs, and scientific packages for data analysis,\nknowledge retrieval, and experimental design. It automatically refines tool\ninterfaces for correct use by AI scientists, creates new tools from natural\nlanguage descriptions, iteratively optimizes tool specifications, and composes\ntools into agentic workflows. In a case study of hypercholesterolemia,\nToolUniverse was used to create an AI scientist to identify a potent analog of\na drug with favorable predicted properties. The open-source ToolUniverse is\navailable at https://aiscientist.tools.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6350fc5ba8822aadf571304f/DrcSsDtZXCxIzFp92C15P.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23426.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6350fc5ba8822aadf571304f",
            "avatarUrl": "/avatars/19686add3cbdaef5772b913152333f9b.svg",
            "fullname": "gasvn",
            "name": "shgao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "submitterOrganization": {
            "_id": "63663038361a96184dbad334",
            "name": "Harvard",
            "fullname": "Harvard University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1667641389305-6366110f575c93cedafde54e.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22193",
            "authors": [
                {
                    "_id": "68da3f770177a6054b013c5a",
                    "user": {
                        "_id": "62be186a5f59ff2320e6e32b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
                        "isPro": false,
                        "fullname": "Nicolas-BZRD",
                        "user": "Nicolas-BZRD",
                        "type": "user"
                    },
                    "name": "Nicolas Boizard",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:36:23.593Z",
                    "hidden": false
                },
                {
                    "_id": "68da3f770177a6054b013c5b",
                    "name": "Hippolyte Gisserot-Boukhlef",
                    "hidden": false
                },
                {
                    "_id": "68da3f770177a6054b013c5c",
                    "name": "Kevin El-Haddad",
                    "hidden": false
                },
                {
                    "_id": "68da3f770177a6054b013c5d",
                    "name": "Cline Hudelot",
                    "hidden": false
                },
                {
                    "_id": "68da3f770177a6054b013c5e",
                    "name": "Pierre Colombo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62be186a5f59ff2320e6e32b/neb6Nao5nmoy6mXX12Is7.png"
            ],
            "publishedAt": "2025-09-26T10:53:52.000Z",
            "submittedOnDailyAt": "2025-09-30T06:47:09.409Z",
            "title": "When Does Reasoning Matter? A Controlled Study of Reasoning's\n  Contribution to Model Performance",
            "submittedOnDailyBy": {
                "_id": "62be186a5f59ff2320e6e32b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
                "isPro": false,
                "fullname": "Nicolas-BZRD",
                "user": "Nicolas-BZRD",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) with reasoning capabilities have achieved\nstate-of-the-art performance on a wide range of tasks. Despite its empirical\nsuccess, the tasks and model scales at which reasoning becomes effective, as\nwell as its training and inference costs, remain underexplored. In this work,\nwe rely on a synthetic data distillation framework to conduct a large-scale\nsupervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models\nof varying sizes, on a wide range of math-centric and general-purpose tasks,\nevaluating both multiple-choice and open-ended formats. Our analysis reveals\nthat reasoning consistently improves model performance, often matching or\nsurpassing significantly larger IFT systems. Notably, while IFT remains\nPareto-optimal in training and inference costs, reasoning models become\nincreasingly valuable as model size scales, overcoming IFT performance limits\non reasoning-intensive and open-ended tasks.",
            "upvotes": 29,
            "discussionId": "68da3f770177a6054b013c5f",
            "ai_summary": "Reasoning models enhance performance across various tasks, surpassing instruction fine-tuned models in reasoning-intensive and open-ended tasks, despite higher computational costs.",
            "ai_keywords": [
                "Instruction Fine-Tuning",
                "reasoning models",
                "synthetic data distillation",
                "multiple-choice",
                "open-ended formats",
                "reasoning-intensive tasks"
            ]
        },
        "publishedAt": "2025-09-26T06:53:52.000Z",
        "title": "When Does Reasoning Matter? A Controlled Study of Reasoning's\n  Contribution to Model Performance",
        "summary": "Large Language Models (LLMs) with reasoning capabilities have achieved\nstate-of-the-art performance on a wide range of tasks. Despite its empirical\nsuccess, the tasks and model scales at which reasoning becomes effective, as\nwell as its training and inference costs, remain underexplored. In this work,\nwe rely on a synthetic data distillation framework to conduct a large-scale\nsupervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models\nof varying sizes, on a wide range of math-centric and general-purpose tasks,\nevaluating both multiple-choice and open-ended formats. Our analysis reveals\nthat reasoning consistently improves model performance, often matching or\nsurpassing significantly larger IFT systems. Notably, while IFT remains\nPareto-optimal in training and inference costs, reasoning models become\nincreasingly valuable as model size scales, overcoming IFT performance limits\non reasoning-intensive and open-ended tasks.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62be186a5f59ff2320e6e32b/neb6Nao5nmoy6mXX12Is7.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22193.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62be186a5f59ff2320e6e32b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
            "fullname": "Nicolas-BZRD",
            "name": "Nicolas-BZRD",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 32
        },
        "submitterOrganization": {
            "_id": "68d276aa856b85d927d94bce",
            "name": "When-Does-Reasoning-Matter",
            "fullname": "When Does Reasoning Matter ?",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62be186a5f59ff2320e6e32b/GjJ15tY7-F4bqR96FN4pd.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25160",
            "authors": [
                {
                    "_id": "68db4fc5d2bf1f4b15ec74a4",
                    "name": "Fan Yuan",
                    "hidden": false
                },
                {
                    "_id": "68db4fc5d2bf1f4b15ec74a5",
                    "user": {
                        "_id": "64098738342c26884c792c93",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
                        "isPro": false,
                        "fullname": "Yuchen Yan",
                        "user": "yanyc",
                        "type": "user"
                    },
                    "name": "Yuchen Yan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:00:51.986Z",
                    "hidden": false
                },
                {
                    "_id": "68db4fc5d2bf1f4b15ec74a6",
                    "name": "Yifan Jiang",
                    "hidden": false
                },
                {
                    "_id": "68db4fc5d2bf1f4b15ec74a7",
                    "name": "Haoran Zhao",
                    "hidden": false
                },
                {
                    "_id": "68db4fc5d2bf1f4b15ec74a8",
                    "name": "Tao Feng",
                    "hidden": false
                },
                {
                    "_id": "68db4fc5d2bf1f4b15ec74a9",
                    "name": "Jinyan Chen",
                    "hidden": false
                },
                {
                    "_id": "68db4fc5d2bf1f4b15ec74aa",
                    "name": "Yanwei Lou",
                    "hidden": false
                },
                {
                    "_id": "68db4fc5d2bf1f4b15ec74ab",
                    "name": "Wenqi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4fc5d2bf1f4b15ec74ac",
                    "user": {
                        "_id": "5e1058e9fcf41d740b69966d",
                        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
                        "isPro": false,
                        "fullname": "Yongliang Shen",
                        "user": "tricktreat",
                        "type": "user"
                    },
                    "name": "Yongliang Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T12:11:41.604Z",
                    "hidden": false
                },
                {
                    "_id": "68db4fc5d2bf1f4b15ec74ad",
                    "name": "Weiming Lu",
                    "hidden": false
                },
                {
                    "_id": "68db4fc5d2bf1f4b15ec74ae",
                    "name": "Jun Xiao",
                    "hidden": false
                },
                {
                    "_id": "68db4fc5d2bf1f4b15ec74af",
                    "name": "Yueting Zhuang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:57:05.000Z",
            "submittedOnDailyAt": "2025-09-30T02:04:37.669Z",
            "title": "GSM8K-V: Can Vision Language Models Solve Grade School Math Word\n  Problems in Visual Contexts",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Vision language models (VLMs) achieve unified modeling of images and text,\nenabling them to accomplish complex real-world tasks through perception,\nplanning, and reasoning. Among these tasks, reasoning is particularly\nrepresentative, with mathematical reasoning serving as a prominent example. It\nhighlights the high-level capability of VLMs to comprehend mathematical\ninformation in images and to perform sophisticated reasoning. Recently,\nnumerous visual mathematical reasoning benchmarks have been proposed, but they\nare often restricted to geometry, lack coverage of math word problems, and\nrarely assess reasoning across multiple images. To address these gaps, we\nintroduce GSM8K-V, a purely visual multi-image mathematical reasoning\nbenchmark. GSM8K-V is built by systematically mapping each sample from the\nwidely used text-based GSM8K into visual form. Through a carefully designed\nautomated image-generation pipeline combined with meticulous human annotation,\nwe curate 1,319 high-quality samples. We evaluate a wide range of open-source\nand closed-source models on GSM8K-V. Results show that although existing VLMs\nhave nearly saturated performance on text-based GSM8K, there remains\nsubstantial room for improvement on GSM8K-V. For example, the best-performing\nmodel, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on\nGSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the\nlimitations of current models as well as potential directions for improvement.\nGSM8K-V offers a new perspective on visual mathematical reasoning and\nestablishes a benchmark to guide the development of more robust and\ngeneralizable VLMs.",
            "upvotes": 25,
            "discussionId": "68db4fc5d2bf1f4b15ec74b0",
            "projectPage": "https://zju-real.github.io/GSM8K-V/",
            "githubRepo": "https://github.com/ZJU-REAL/GSM8K-V",
            "ai_summary": "GSM8K-V is a new visual multi-image mathematical reasoning benchmark that highlights the limitations of current vision language models in handling visual mathematical problems.",
            "ai_keywords": [
                "vision language models",
                "VLMs",
                "mathematical reasoning",
                "visual mathematical reasoning",
                "GSM8K-V",
                "image-generation pipeline",
                "Gemini-2.5-Pro"
            ],
            "githubStars": 21
        },
        "publishedAt": "2025-09-29T13:57:05.000Z",
        "title": "GSM8K-V: Can Vision Language Models Solve Grade School Math Word\n  Problems in Visual Contexts",
        "summary": "Vision language models (VLMs) achieve unified modeling of images and text,\nenabling them to accomplish complex real-world tasks through perception,\nplanning, and reasoning. Among these tasks, reasoning is particularly\nrepresentative, with mathematical reasoning serving as a prominent example. It\nhighlights the high-level capability of VLMs to comprehend mathematical\ninformation in images and to perform sophisticated reasoning. Recently,\nnumerous visual mathematical reasoning benchmarks have been proposed, but they\nare often restricted to geometry, lack coverage of math word problems, and\nrarely assess reasoning across multiple images. To address these gaps, we\nintroduce GSM8K-V, a purely visual multi-image mathematical reasoning\nbenchmark. GSM8K-V is built by systematically mapping each sample from the\nwidely used text-based GSM8K into visual form. Through a carefully designed\nautomated image-generation pipeline combined with meticulous human annotation,\nwe curate 1,319 high-quality samples. We evaluate a wide range of open-source\nand closed-source models on GSM8K-V. Results show that although existing VLMs\nhave nearly saturated performance on text-based GSM8K, there remains\nsubstantial room for improvement on GSM8K-V. For example, the best-performing\nmodel, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on\nGSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the\nlimitations of current models as well as potential directions for improvement.\nGSM8K-V offers a new perspective on visual mathematical reasoning and\nestablishes a benchmark to guide the development of more robust and\ngeneralizable VLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25160.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 115
        },
        "submitterOrganization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.23909",
            "authors": [
                {
                    "_id": "68db3dd7d2bf1f4b15ec72c4",
                    "name": "Xin Luo",
                    "hidden": false
                },
                {
                    "_id": "68db3dd7d2bf1f4b15ec72c5",
                    "name": "Jiahao Wang",
                    "hidden": false
                },
                {
                    "_id": "68db3dd7d2bf1f4b15ec72c6",
                    "name": "Chenyuan Wu",
                    "hidden": false
                },
                {
                    "_id": "68db3dd7d2bf1f4b15ec72c7",
                    "name": "Shitao Xiao",
                    "hidden": false
                },
                {
                    "_id": "68db3dd7d2bf1f4b15ec72c8",
                    "name": "Xiyan Jiang",
                    "hidden": false
                },
                {
                    "_id": "68db3dd7d2bf1f4b15ec72c9",
                    "name": "Defu Lian",
                    "hidden": false
                },
                {
                    "_id": "68db3dd7d2bf1f4b15ec72ca",
                    "name": "Jiajun Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db3dd7d2bf1f4b15ec72cb",
                    "name": "Dong Liu",
                    "hidden": false
                },
                {
                    "_id": "68db3dd7d2bf1f4b15ec72cc",
                    "name": "Zheng liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T14:28:24.000Z",
            "submittedOnDailyAt": "2025-09-30T00:49:08.471Z",
            "title": "EditScore: Unlocking Online RL for Image Editing via High-Fidelity\n  Reward Modeling",
            "submittedOnDailyBy": {
                "_id": "641bd1737c21ab946bf69aff",
                "avatarUrl": "/avatars/83759075ad893a69a0c2cf5493d7e988.svg",
                "isPro": false,
                "fullname": "xin luo",
                "user": "sienna223",
                "type": "user"
            },
            "summary": "Instruction-guided image editing has achieved remarkable progress, yet\ncurrent models still face challenges with complex instructions and often\nrequire multiple samples to produce a desired result. Reinforcement Learning\n(RL) offers a promising solution, but its adoption in image editing has been\nseverely hindered by the lack of a high-fidelity, efficient reward signal. In\nthis work, we present a comprehensive methodology to overcome this barrier,\ncentered on the development of a state-of-the-art, specialized reward model. We\nfirst introduce EditReward-Bench, a comprehensive benchmark to systematically\nevaluate reward models on editing quality. Building on this benchmark, we\ndevelop EditScore, a series of reward models (7B-72B) for evaluating the\nquality of instruction-guided image editing. Through meticulous data curation\nand filtering, EditScore effectively matches the performance of learning\nproprietary VLMs. Furthermore, coupled with an effective self-ensemble strategy\ntailored for the generative nature of EditScore, our largest variant even\nsurpasses GPT-5 in the benchmark. We then demonstrate that a high-fidelity\nreward model is the key to unlocking online RL for image editing. Our\nexperiments show that, while even the largest open-source VLMs fail to provide\nan effective learning signal, EditScore enables efficient and robust policy\noptimization. Applying our framework to a strong base model, OmniGen2, results\nin a final model that shows a substantial and consistent performance uplift.\nOverall, this work provides the first systematic path from benchmarking to\nreward modeling to RL training in image editing, showing that a high-fidelity,\ndomain-specialized reward model is the key to unlocking the full potential of\nRL in this domain.",
            "upvotes": 25,
            "discussionId": "68db3dd8d2bf1f4b15ec72cd",
            "githubRepo": "https://github.com/VectorSpaceLab/EditScore",
            "ai_summary": "A specialized reward model, EditScore, enables effective reinforcement learning for instruction-guided image editing by providing a high-fidelity reward signal.",
            "ai_keywords": [
                "Reinforcement Learning",
                "reward model",
                "EditReward-Bench",
                "EditScore",
                "OmniGen2",
                "policy optimization"
            ],
            "githubStars": 50
        },
        "publishedAt": "2025-09-28T10:28:24.000Z",
        "title": "EditScore: Unlocking Online RL for Image Editing via High-Fidelity\n  Reward Modeling",
        "summary": "Instruction-guided image editing has achieved remarkable progress, yet\ncurrent models still face challenges with complex instructions and often\nrequire multiple samples to produce a desired result. Reinforcement Learning\n(RL) offers a promising solution, but its adoption in image editing has been\nseverely hindered by the lack of a high-fidelity, efficient reward signal. In\nthis work, we present a comprehensive methodology to overcome this barrier,\ncentered on the development of a state-of-the-art, specialized reward model. We\nfirst introduce EditReward-Bench, a comprehensive benchmark to systematically\nevaluate reward models on editing quality. Building on this benchmark, we\ndevelop EditScore, a series of reward models (7B-72B) for evaluating the\nquality of instruction-guided image editing. Through meticulous data curation\nand filtering, EditScore effectively matches the performance of learning\nproprietary VLMs. Furthermore, coupled with an effective self-ensemble strategy\ntailored for the generative nature of EditScore, our largest variant even\nsurpasses GPT-5 in the benchmark. We then demonstrate that a high-fidelity\nreward model is the key to unlocking online RL for image editing. Our\nexperiments show that, while even the largest open-source VLMs fail to provide\nan effective learning signal, EditScore enables efficient and robust policy\noptimization. Applying our framework to a strong base model, OmniGen2, results\nin a final model that shows a substantial and consistent performance uplift.\nOverall, this work provides the first systematic path from benchmarking to\nreward modeling to RL training in image editing, showing that a high-fidelity,\ndomain-specialized reward model is the key to unlocking the full potential of\nRL in this domain.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23909.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "641bd1737c21ab946bf69aff",
            "avatarUrl": "/avatars/83759075ad893a69a0c2cf5493d7e988.svg",
            "fullname": "xin luo",
            "name": "sienna223",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "submitterOrganization": {
            "_id": "61be9739d2f9358e24ca0a4f",
            "name": "BAAI",
            "fullname": "Beijing Academy of Artificial Intelligence",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25175",
            "authors": [
                {
                    "_id": "68db4b13d2bf1f4b15ec741b",
                    "user": {
                        "_id": "6692aff88db712bad780f02a",
                        "avatarUrl": "/avatars/5dc4b1c27c70f6a64864711dbff4910f.svg",
                        "isPro": false,
                        "fullname": "xhl",
                        "user": "zjuxhl",
                        "type": "user"
                    },
                    "name": "Haolei Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:29:48.835Z",
                    "hidden": false
                },
                {
                    "_id": "68db4b13d2bf1f4b15ec741c",
                    "user": {
                        "_id": "68db69d52d9cd53de0af2129",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/BGCasdq8kKQWaPHzkt6NS.webp",
                        "isPro": false,
                        "fullname": "Xinyu Mei",
                        "user": "xinyumei",
                        "type": "user"
                    },
                    "name": "Xinyu Mei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:29:46.012Z",
                    "hidden": false
                },
                {
                    "_id": "68db4b13d2bf1f4b15ec741d",
                    "user": {
                        "_id": "64098738342c26884c792c93",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
                        "isPro": false,
                        "fullname": "Yuchen Yan",
                        "user": "yanyc",
                        "type": "user"
                    },
                    "name": "Yuchen Yan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:29:51.653Z",
                    "hidden": false
                },
                {
                    "_id": "68db4b13d2bf1f4b15ec741e",
                    "name": "Rui Zhou",
                    "hidden": false
                },
                {
                    "_id": "68db4b13d2bf1f4b15ec741f",
                    "name": "Wenqi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4b13d2bf1f4b15ec7420",
                    "name": "Weiming Lu",
                    "hidden": false
                },
                {
                    "_id": "68db4b13d2bf1f4b15ec7421",
                    "name": "Yueting Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68db4b13d2bf1f4b15ec7422",
                    "user": {
                        "_id": "5e1058e9fcf41d740b69966d",
                        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
                        "isPro": false,
                        "fullname": "Yongliang Shen",
                        "user": "tricktreat",
                        "type": "user"
                    },
                    "name": "Yongliang Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T12:11:39.530Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:59:07.000Z",
            "submittedOnDailyAt": "2025-09-30T01:50:03.295Z",
            "title": "EasySteer: A Unified Framework for High-Performance and Extensible LLM\n  Steering",
            "submittedOnDailyBy": {
                "_id": "6692aff88db712bad780f02a",
                "avatarUrl": "/avatars/5dc4b1c27c70f6a64864711dbff4910f.svg",
                "isPro": false,
                "fullname": "xhl",
                "user": "zjuxhl",
                "type": "user"
            },
            "summary": "Large language model (LLM) steering has emerged as a promising paradigm for\ncontrolling model behavior at inference time through targeted manipulation of\nhidden states, offering a lightweight alternative to expensive retraining.\nHowever, existing steering frameworks suffer from critical limitations:\ncomputational inefficiency, limited extensibility, and restricted functionality\nthat hinder both research progress and practical deployment. We present\nEasySteer, a unified framework for high-performance, extensible LLM steering\nbuilt on vLLM. Our system features modular architecture with pluggable\ninterfaces for both analysis-based and learning-based methods, fine-grained\nparameter control, pre-computed steering vectors for eight application domains,\nand an interactive demonstration system. Through deep integration with vLLM's\noptimized inference engine, EasySteer achieves 5.5-11.4times speedup over\nexisting frameworks. Extensive experiments demonstrate its effectiveness in\noverthinking mitigation, hallucination reduction, and other key applications.\nEasySteer transforms steering from research technique to production-ready\ncapability, establishing critical infrastructure for deployable, controllable\nlanguage models.",
            "upvotes": 24,
            "discussionId": "68db4b13d2bf1f4b15ec7423",
            "githubRepo": "https://github.com/ZJU-REAL/EasySteer",
            "ai_summary": "EasySteer is a unified framework for efficient and extensible steering of large language models, offering significant speedups and improved functionality over existing methods.",
            "ai_keywords": [
                "large language model steering",
                "hidden states",
                "vLLM",
                "modular architecture",
                "pluggable interfaces",
                "analysis-based methods",
                "learning-based methods",
                "fine-grained parameter control",
                "steering vectors",
                "overthinking mitigation",
                "hallucination reduction",
                "deployable",
                "controllable language models"
            ],
            "githubStars": 36
        },
        "publishedAt": "2025-09-29T13:59:07.000Z",
        "title": "EasySteer: A Unified Framework for High-Performance and Extensible LLM\n  Steering",
        "summary": "Large language model (LLM) steering has emerged as a promising paradigm for\ncontrolling model behavior at inference time through targeted manipulation of\nhidden states, offering a lightweight alternative to expensive retraining.\nHowever, existing steering frameworks suffer from critical limitations:\ncomputational inefficiency, limited extensibility, and restricted functionality\nthat hinder both research progress and practical deployment. We present\nEasySteer, a unified framework for high-performance, extensible LLM steering\nbuilt on vLLM. Our system features modular architecture with pluggable\ninterfaces for both analysis-based and learning-based methods, fine-grained\nparameter control, pre-computed steering vectors for eight application domains,\nand an interactive demonstration system. Through deep integration with vLLM's\noptimized inference engine, EasySteer achieves 5.5-11.4times speedup over\nexisting frameworks. Extensive experiments demonstrate its effectiveness in\noverthinking mitigation, hallucination reduction, and other key applications.\nEasySteer transforms steering from research technique to production-ready\ncapability, establishing critical infrastructure for deployable, controllable\nlanguage models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25175.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6692aff88db712bad780f02a",
            "avatarUrl": "/avatars/5dc4b1c27c70f6a64864711dbff4910f.svg",
            "fullname": "xhl",
            "name": "zjuxhl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "submitterOrganization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.24014",
            "authors": [
                {
                    "_id": "68db3a3dd2bf1f4b15ec72ae",
                    "user": {
                        "_id": "668e740f1173ab43d9d9ed5e",
                        "avatarUrl": "/avatars/caa9b47c2a5f6d6d679759b8b234a0ab.svg",
                        "isPro": false,
                        "fullname": "Zeqing Wang",
                        "user": "INV-WZQ",
                        "type": "user"
                    },
                    "name": "Zeqing Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:34:37.327Z",
                    "hidden": false
                },
                {
                    "_id": "68db3a3dd2bf1f4b15ec72af",
                    "name": "Gongfan Fang",
                    "hidden": false
                },
                {
                    "_id": "68db3a3dd2bf1f4b15ec72b0",
                    "name": "Xinyin Ma",
                    "hidden": false
                },
                {
                    "_id": "68db3a3dd2bf1f4b15ec72b1",
                    "user": {
                        "_id": "634cfebc350bcee9bed20a4d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
                        "isPro": false,
                        "fullname": "Xingyi Yang",
                        "user": "adamdad",
                        "type": "user"
                    },
                    "name": "Xingyi Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T07:12:20.257Z",
                    "hidden": false
                },
                {
                    "_id": "68db3a3dd2bf1f4b15ec72b2",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/P1TvbBP2RcB6n7w3yvi5s.mp4"
            ],
            "publishedAt": "2025-09-28T18:10:10.000Z",
            "submittedOnDailyAt": "2025-09-30T00:49:13.077Z",
            "title": "SparseD: Sparse Attention for Diffusion Language Models",
            "submittedOnDailyBy": {
                "_id": "634cfebc350bcee9bed20a4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
                "isPro": false,
                "fullname": "Xingyi Yang",
                "user": "adamdad",
                "type": "user"
            },
            "summary": "While diffusion language models (DLMs) offer a promising alternative to\nautoregressive models (ARs), existing open-source DLMs suffer from high\ninference latency. This bottleneck is mainly due to the attention's quadratic\ncomplexity with respect to context length in computing all query-key pairs.\nIntuitively, to reduce this complexity, a natural strategy is to restrict\nattention to sparse patterns that retain only the most relevant connections.\nSuch approaches are well-established in ARs, where attention follows fixed and\nclearly defined sparse patterns. However, in DLMs, we observe distinct sparsity\nbehaviors: (1) attention patterns vary across heads, (2) attention patterns in\neach head remain highly similar across denoising steps, and (3) early denoising\nsteps are critical for generation. These findings render sparse attention\nmethods designed for ARs largely incompatible with DLMs, as they fail to\ncapture head-specific structures and risk degrading generation when applied in\nearly denoising steps. To address these challenges, we propose SparseD, a novel\nsparse attention method for DLMs. Leveraging the observations, SparseD only\nrequires pre-computing head-specific sparse patterns one time, and reuses them\nacross all steps. This prevents recomputing sparse patterns at each denoising\nstep. Meanwhile, SparseD uses full attention in the early steps, then switches\nto sparse attention later to maintain generation quality. Together, these\nestablish SparseD as a practical and efficient solution for deploying DLMs in\nlong-context applications. Experimental results demonstrate that SparseD\nachieves lossless acceleration, delivering up to 1.50times speedup over\nFlashAttention at a 64k context length with 1,024 denoising steps.",
            "upvotes": 24,
            "discussionId": "68db3a3dd2bf1f4b15ec72b3",
            "githubRepo": "https://github.com/INV-WZQ/SparseD",
            "ai_summary": "SparseD is a novel sparse attention method for diffusion language models that addresses the high inference latency by pre-computing head-specific sparse patterns and switching to sparse attention in later denoising steps.",
            "ai_keywords": [
                "diffusion language models",
                "autoregressive models",
                "attention",
                "quadratic complexity",
                "context length",
                "query-key pairs",
                "sparse attention",
                "denoising steps",
                "SparseD",
                "head-specific structures",
                "generation quality",
                "FlashAttention"
            ],
            "githubStars": 28
        },
        "publishedAt": "2025-09-28T14:10:10.000Z",
        "title": "SparseD: Sparse Attention for Diffusion Language Models",
        "summary": "While diffusion language models (DLMs) offer a promising alternative to\nautoregressive models (ARs), existing open-source DLMs suffer from high\ninference latency. This bottleneck is mainly due to the attention's quadratic\ncomplexity with respect to context length in computing all query-key pairs.\nIntuitively, to reduce this complexity, a natural strategy is to restrict\nattention to sparse patterns that retain only the most relevant connections.\nSuch approaches are well-established in ARs, where attention follows fixed and\nclearly defined sparse patterns. However, in DLMs, we observe distinct sparsity\nbehaviors: (1) attention patterns vary across heads, (2) attention patterns in\neach head remain highly similar across denoising steps, and (3) early denoising\nsteps are critical for generation. These findings render sparse attention\nmethods designed for ARs largely incompatible with DLMs, as they fail to\ncapture head-specific structures and risk degrading generation when applied in\nearly denoising steps. To address these challenges, we propose SparseD, a novel\nsparse attention method for DLMs. Leveraging the observations, SparseD only\nrequires pre-computing head-specific sparse patterns one time, and reuses them\nacross all steps. This prevents recomputing sparse patterns at each denoising\nstep. Meanwhile, SparseD uses full attention in the early steps, then switches\nto sparse attention later to maintain generation quality. Together, these\nestablish SparseD as a practical and efficient solution for deploying DLMs in\nlong-context applications. Experimental results demonstrate that SparseD\nachieves lossless acceleration, delivering up to 1.50times speedup over\nFlashAttention at a 64k context length with 1,024 denoising steps.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/P1TvbBP2RcB6n7w3yvi5s.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24014.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "634cfebc350bcee9bed20a4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
            "fullname": "Xingyi Yang",
            "name": "adamdad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.24007",
            "authors": [
                {
                    "_id": "68db4940d2bf1f4b15ec73f9",
                    "name": "Yangzhou Liu",
                    "hidden": false
                },
                {
                    "_id": "68db4940d2bf1f4b15ec73fa",
                    "user": {
                        "_id": "6571382c7644d1128561cebe",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bnJ-T3k_w1g1Mr7b7LglK.jpeg",
                        "isPro": false,
                        "fullname": "Cao Yue",
                        "user": "yuecao0119",
                        "type": "user"
                    },
                    "name": "Yue Cao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:29:54.229Z",
                    "hidden": false
                },
                {
                    "_id": "68db4940d2bf1f4b15ec73fb",
                    "name": "Hao Li",
                    "hidden": false
                },
                {
                    "_id": "68db4940d2bf1f4b15ec73fc",
                    "name": "Gen Luo",
                    "hidden": false
                },
                {
                    "_id": "68db4940d2bf1f4b15ec73fd",
                    "name": "Zhe Chen",
                    "hidden": false
                },
                {
                    "_id": "68db4940d2bf1f4b15ec73fe",
                    "name": "Weiyun Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4940d2bf1f4b15ec73ff",
                    "name": "Xiaobo Liang",
                    "hidden": false
                },
                {
                    "_id": "68db4940d2bf1f4b15ec7400",
                    "name": "Biqing Qi",
                    "hidden": false
                },
                {
                    "_id": "68db4940d2bf1f4b15ec7401",
                    "name": "Lijun Wu",
                    "hidden": false
                },
                {
                    "_id": "68db4940d2bf1f4b15ec7402",
                    "name": "Changyao Tian",
                    "hidden": false
                },
                {
                    "_id": "68db4940d2bf1f4b15ec7403",
                    "name": "Yanting Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4940d2bf1f4b15ec7404",
                    "name": "Yuqiang Li",
                    "hidden": false
                },
                {
                    "_id": "68db4940d2bf1f4b15ec7405",
                    "name": "Tong Lu",
                    "hidden": false
                },
                {
                    "_id": "68db4940d2bf1f4b15ec7406",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68db4940d2bf1f4b15ec7407",
                    "name": "Jifeng Dai",
                    "hidden": false
                },
                {
                    "_id": "68db4940d2bf1f4b15ec7408",
                    "name": "Wenhai Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T17:59:15.000Z",
            "submittedOnDailyAt": "2025-09-30T01:39:11.662Z",
            "title": "Sequential Diffusion Language Models",
            "submittedOnDailyBy": {
                "_id": "6571382c7644d1128561cebe",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bnJ-T3k_w1g1Mr7b7LglK.jpeg",
                "isPro": false,
                "fullname": "Cao Yue",
                "user": "yuecao0119",
                "type": "user"
            },
            "summary": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM",
            "upvotes": 23,
            "discussionId": "68db4941d2bf1f4b15ec7409",
            "projectPage": "https://internvl.github.io/blog/2025-09-29-SDLM/",
            "githubRepo": "https://github.com/OpenGVLab/SDLM",
            "ai_summary": "Sequential Diffusion Language Model (SDLM) enhances pre-trained autoregressive language models by adaptively determining generation length and maintaining KV-cache compatibility, achieving high efficiency and throughput.",
            "ai_keywords": [
                "diffusion language models",
                "fixed-length decoding",
                "key-value caches",
                "block diffusion",
                "next sequence prediction",
                "next-token prediction",
                "sequential diffusion language model",
                "autoregressive language models",
                "diffusion inference",
                "mask blocks",
                "model confidence",
                "robustness",
                "throughput",
                "Qwen-2.5"
            ],
            "githubStars": 22
        },
        "publishedAt": "2025-09-28T13:59:15.000Z",
        "title": "Sequential Diffusion Language Models",
        "summary": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24007.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6571382c7644d1128561cebe",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bnJ-T3k_w1g1Mr7b7LglK.jpeg",
            "fullname": "Cao Yue",
            "name": "yuecao0119",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "submitterOrganization": {
            "_id": "64006c57a3b8fe3ac0e9af7c",
            "name": "OpenGVLab",
            "fullname": "OpenGVLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64006c09330a45b03605bba3/FvdxiTkTqH8rKDOzGZGUE.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25106",
            "authors": [
                {
                    "_id": "68db7847d2bf1f4b15ec7720",
                    "name": "Yuan Liang",
                    "hidden": false
                },
                {
                    "_id": "68db7847d2bf1f4b15ec7721",
                    "name": "Jiaxian Li",
                    "hidden": false
                },
                {
                    "_id": "68db7847d2bf1f4b15ec7722",
                    "user": {
                        "_id": "68528ed95eb3deddd065648a",
                        "avatarUrl": "/avatars/e55960126fa7a2ceb43dd0f2bc8190cb.svg",
                        "isPro": false,
                        "fullname": "wang",
                        "user": "yuqing666",
                        "type": "user"
                    },
                    "name": "Yuqing Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T07:12:30.169Z",
                    "hidden": false
                },
                {
                    "_id": "68db7847d2bf1f4b15ec7723",
                    "name": "Piaohong Wang",
                    "hidden": false
                },
                {
                    "_id": "68db7847d2bf1f4b15ec7724",
                    "name": "Motong Tian",
                    "hidden": false
                },
                {
                    "_id": "68db7847d2bf1f4b15ec7725",
                    "name": "Pai Liu",
                    "hidden": false
                },
                {
                    "_id": "68db7847d2bf1f4b15ec7726",
                    "name": "Shuofei Qiao",
                    "hidden": false
                },
                {
                    "_id": "68db7847d2bf1f4b15ec7727",
                    "name": "Runnan Fang",
                    "hidden": false
                },
                {
                    "_id": "68db7847d2bf1f4b15ec7728",
                    "name": "He Zhu",
                    "hidden": false
                },
                {
                    "_id": "68db7847d2bf1f4b15ec7729",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db7847d2bf1f4b15ec772a",
                    "name": "Minghao Liu",
                    "hidden": false
                },
                {
                    "_id": "68db7847d2bf1f4b15ec772b",
                    "name": "Yuchen Eleanor Jiang",
                    "hidden": false
                },
                {
                    "_id": "68db7847d2bf1f4b15ec772c",
                    "user": {
                        "_id": "620b3bbb0668e435407c8d0a",
                        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                        "isPro": true,
                        "fullname": "Ningyu Zhang",
                        "user": "Ningyu",
                        "type": "user"
                    },
                    "name": "Ningyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:36:05.963Z",
                    "hidden": false
                },
                {
                    "_id": "68db7847d2bf1f4b15ec772d",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:39:17.000Z",
            "submittedOnDailyAt": "2025-09-30T05:00:28.938Z",
            "title": "Towards Personalized Deep Research: Benchmarks and Evaluations",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": true,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "Deep Research Agents (DRAs) can autonomously conduct complex investigations\nand generate comprehensive reports, demonstrating strong real-world potential.\nHowever, existing evaluations mostly rely on close-ended benchmarks, while\nopen-ended deep research benchmarks remain scarce and typically neglect\npersonalized scenarios. To bridge this gap, we introduce Personalized Deep\nResearch Bench, the first benchmark for evaluating personalization in DRAs. It\npairs 50 diverse research tasks across 10 domains with 25 authentic user\nprofiles that combine structured persona attributes with dynamic real-world\ncontexts, yielding 250 realistic user-task queries. To assess system\nperformance, we propose the PQR Evaluation Framework, which jointly measures\n(P) Personalization Alignment, (Q) Content Quality, and (R) Factual\nReliability. Our experiments on a range of systems highlight current\ncapabilities and limitations in handling personalized deep research. This work\nestablishes a rigorous foundation for developing and evaluating the next\ngeneration of truly personalized AI research assistants.",
            "upvotes": 21,
            "discussionId": "68db7847d2bf1f4b15ec772e",
            "ai_summary": "A new benchmark, Personalized Deep Research Bench, evaluates the personalization capabilities of Deep Research Agents across diverse tasks and user profiles using the PQR Evaluation Framework.",
            "ai_keywords": [
                "Deep Research Agents",
                "Personalized Deep Research Bench",
                "PQR Evaluation Framework",
                "Personalization Alignment",
                "Content Quality",
                "Factual Reliability"
            ]
        },
        "publishedAt": "2025-09-29T13:39:17.000Z",
        "title": "Towards Personalized Deep Research: Benchmarks and Evaluations",
        "summary": "Deep Research Agents (DRAs) can autonomously conduct complex investigations\nand generate comprehensive reports, demonstrating strong real-world potential.\nHowever, existing evaluations mostly rely on close-ended benchmarks, while\nopen-ended deep research benchmarks remain scarce and typically neglect\npersonalized scenarios. To bridge this gap, we introduce Personalized Deep\nResearch Bench, the first benchmark for evaluating personalization in DRAs. It\npairs 50 diverse research tasks across 10 domains with 25 authentic user\nprofiles that combine structured persona attributes with dynamic real-world\ncontexts, yielding 250 realistic user-task queries. To assess system\nperformance, we propose the PQR Evaluation Framework, which jointly measures\n(P) Personalization Alignment, (Q) Content Quality, and (R) Factual\nReliability. Our experiments on a range of systems highlight current\ncapabilities and limitations in handling personalized deep research. This work\nestablishes a rigorous foundation for developing and evaluating the next\ngeneration of truly personalized AI research assistants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25106.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 30
        },
        "submitterOrganization": {
            "_id": "67177eecd0fad5b4ccc09461",
            "name": "OPPOer",
            "fullname": "OPPO",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66e24afddce93c7249b418c0/gQ-XFJehEyAH12zhbeR8Z.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.24981",
            "authors": [
                {
                    "_id": "68db71fbd2bf1f4b15ec76e3",
                    "name": "Haoran He",
                    "hidden": false
                },
                {
                    "_id": "68db71fbd2bf1f4b15ec76e4",
                    "name": "Yuxiao Ye",
                    "hidden": false
                },
                {
                    "_id": "68db71fbd2bf1f4b15ec76e5",
                    "name": "Qingpeng Cai",
                    "hidden": false
                },
                {
                    "_id": "68db71fbd2bf1f4b15ec76e6",
                    "name": "Chen Hu",
                    "hidden": false
                },
                {
                    "_id": "68db71fbd2bf1f4b15ec76e7",
                    "name": "Binxing Jiao",
                    "hidden": false
                },
                {
                    "_id": "68db71fbd2bf1f4b15ec76e8",
                    "name": "Daxin Jiang",
                    "hidden": false
                },
                {
                    "_id": "68db71fbd2bf1f4b15ec76e9",
                    "name": "Ling Pan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T16:09:07.000Z",
            "submittedOnDailyAt": "2025-09-30T04:31:29.787Z",
            "title": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable\n  Rewards",
            "submittedOnDailyBy": {
                "_id": "6672937ceac0fb1b9e516595",
                "avatarUrl": "/avatars/5eea5657016572f60b0ecd0fa9a7dae4.svg",
                "isPro": false,
                "fullname": "haoran he",
                "user": "haoranhe",
                "type": "user"
            },
            "summary": "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for\nimproving the reasoning abilities of large language models (LLMs). Current\nmethods rely primarily on policy optimization frameworks like PPO and GRPO,\nwhich follow generalized policy iteration that alternates between evaluating\nthe current policy's value and improving the policy based on evaluation. While\neffective, they often suffer from training instability and diversity collapse,\nrequiring complex heuristic tricks and careful tuning. We observe that standard\nRLVR in math reasoning can be formalized as a specialized finite-horizon Markov\nDecision Process with deterministic state transitions, tree-structured\ndynamics, and binary terminal rewards. Though large in scale, the underlying\nstructure is simpler than general-purpose control settings for which popular RL\nalgorithms (e.g., PPO) were developed, suggesting that several sophisticated\ntechniques in existing methods may be reduced or even omitted. Based on this\ninsight, we prove a surprising result: the optimal action can be recovered from\nthe Q-function of a fixed uniformly random policy, thereby bypassing the\ngeneralized policy iteration loop and its associated heuristics. We introduce\nRandom Policy Valuation for Diverse Reasoning (ROVER) to translate this\nprinciple into a practical and scalable algorithm for LLM math reasoning, a\nminimalist yet highly effective RL method that samples actions from a softmax\nover these uniform-policy Q-values. ROVER preserves diversity throughout\ntraining, allowing sustained exploration of multiple valid pathways. Across\nmultiple base models and standard math reasoning benchmarks, ROVER demonstrates\nsuperior performance in both quality (+8.2 on pass@1,\n+16.8 on pass@256) and diversity (+17.6\\%), despite\nits radical simplification compared to strong, complicated existing methods.",
            "upvotes": 21,
            "discussionId": "68db71fcd2bf1f4b15ec76ea",
            "githubRepo": "https://github.com/tinnerhrhe/ROVER",
            "ai_summary": "ROVER, a minimalist RL method, achieves superior performance and diversity in LLM math reasoning by leveraging Q-values from a fixed random policy, bypassing complex policy iteration.",
            "ai_keywords": [
                "RL with Verifiable Rewards",
                "RLVR",
                "large language models",
                "LLMs",
                "policy optimization",
                "PPO",
                "GRPO",
                "generalized policy iteration",
                "Markov Decision Process",
                "deterministic state transitions",
                "tree-structured dynamics",
                "binary terminal rewards",
                "Q-function",
                "Random Policy Valuation for Diverse Reasoning",
                "ROVER",
                "softmax",
                "exploration",
                "pass@1",
                "pass@256",
                "diversity"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-09-29T12:09:07.000Z",
        "title": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable\n  Rewards",
        "summary": "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for\nimproving the reasoning abilities of large language models (LLMs). Current\nmethods rely primarily on policy optimization frameworks like PPO and GRPO,\nwhich follow generalized policy iteration that alternates between evaluating\nthe current policy's value and improving the policy based on evaluation. While\neffective, they often suffer from training instability and diversity collapse,\nrequiring complex heuristic tricks and careful tuning. We observe that standard\nRLVR in math reasoning can be formalized as a specialized finite-horizon Markov\nDecision Process with deterministic state transitions, tree-structured\ndynamics, and binary terminal rewards. Though large in scale, the underlying\nstructure is simpler than general-purpose control settings for which popular RL\nalgorithms (e.g., PPO) were developed, suggesting that several sophisticated\ntechniques in existing methods may be reduced or even omitted. Based on this\ninsight, we prove a surprising result: the optimal action can be recovered from\nthe Q-function of a fixed uniformly random policy, thereby bypassing the\ngeneralized policy iteration loop and its associated heuristics. We introduce\nRandom Policy Valuation for Diverse Reasoning (ROVER) to translate this\nprinciple into a practical and scalable algorithm for LLM math reasoning, a\nminimalist yet highly effective RL method that samples actions from a softmax\nover these uniform-policy Q-values. ROVER preserves diversity throughout\ntraining, allowing sustained exploration of multiple valid pathways. Across\nmultiple base models and standard math reasoning benchmarks, ROVER demonstrates\nsuperior performance in both quality (+8.2 on pass@1,\n+16.8 on pass@256) and diversity (+17.6\\%), despite\nits radical simplification compared to strong, complicated existing methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24981.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6672937ceac0fb1b9e516595",
            "avatarUrl": "/avatars/5eea5657016572f60b0ecd0fa9a7dae4.svg",
            "fullname": "haoran he",
            "name": "haoranhe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22799",
            "authors": [
                {
                    "_id": "68db55b9d2bf1f4b15ec7504",
                    "name": "Xuan He",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec7505",
                    "name": "Dongfu Jiang",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec7506",
                    "name": "Ping Nie",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec7507",
                    "name": "Minghao Liu",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec7508",
                    "name": "Zhengxuan Jiang",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec7509",
                    "name": "Mingyi Su",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec750a",
                    "name": "Wentao Ma",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec750b",
                    "name": "Junru Lin",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec750c",
                    "name": "Chun Ye",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec750d",
                    "user": {
                        "_id": "65095591cc02352e1e106d0a",
                        "avatarUrl": "/avatars/55bb1dcc31fca1659bcea57d15da50e5.svg",
                        "isPro": false,
                        "fullname": "Tom Lu",
                        "user": "eigentom",
                        "type": "user"
                    },
                    "name": "Yi Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T12:45:13.883Z",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec750e",
                    "name": "Keming Wu",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec750f",
                    "name": "Benjamin Schneider",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec7510",
                    "name": "Quy Duc Do",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec7511",
                    "user": {
                        "_id": "66349404f2c753240d02952a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66349404f2c753240d02952a/xKBKicwyk7BoOITQPwBJn.png",
                        "isPro": false,
                        "fullname": "ZhuofengLi",
                        "user": "ZhuofengLi",
                        "type": "user"
                    },
                    "name": "Zhuofeng Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T11:42:55.608Z",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec7512",
                    "name": "Yiming Jia",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec7513",
                    "name": "Yuxuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec7514",
                    "name": "Guo Cheng",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec7515",
                    "name": "Haozhe Wang",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec7516",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec7517",
                    "name": "Qunshu Lin",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec7518",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec7519",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec751a",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "68db55b9d2bf1f4b15ec751b",
                    "name": "Wenhu Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T18:09:03.000Z",
            "submittedOnDailyAt": "2025-09-30T02:30:24.889Z",
            "title": "VideoScore2: Think before You Score in Generative Video Evaluation",
            "submittedOnDailyBy": {
                "_id": "6313a86154e6e5d9f0f94e04",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                "isPro": false,
                "fullname": "Wenhu Chen",
                "user": "wenhu",
                "type": "user"
            },
            "summary": "Recent advances in text-to-video generation have produced increasingly\nrealistic and diverse content, yet evaluating such videos remains a fundamental\nchallenge due to their multi-faceted nature encompassing visual quality,\nsemantic alignment, and physical consistency. Existing evaluators and reward\nmodels are limited to single opaque scores, lack interpretability, or provide\nonly coarse analysis, making them insufficient for capturing the comprehensive\nnature of video quality assessment. We present VideoScore2, a\nmulti-dimensional, interpretable, and human-aligned framework that explicitly\nevaluates visual quality, text-to-video alignment, and physical/common-sense\nconsistency while producing detailed chain-of-thought rationales. Our model is\ntrained on a large-scale dataset VideoFeedback2 containing 27,168\nhuman-annotated videos with both scores and reasoning traces across three\ndimensions, using a two-stage pipeline of supervised fine-tuning followed by\nreinforcement learning with Group Relative Policy Optimization (GRPO) to\nenhance analytical robustness. Extensive experiments demonstrate that\nVideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our\nin-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance\nacross four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc),\nwhile providing interpretable assessments that bridge the gap between\nevaluation and controllable generation through effective reward modeling for\nBest-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/",
            "upvotes": 20,
            "discussionId": "68db55b9d2bf1f4b15ec751c",
            "ai_summary": "VideoScore2 is a multi-dimensional, interpretable framework for evaluating text-to-video generation, assessing visual quality, alignment, and consistency with detailed rationales.",
            "ai_keywords": [
                "text-to-video generation",
                "visual quality",
                "semantic alignment",
                "physical consistency",
                "VideoScore2",
                "VideoFeedback2",
                "supervised fine-tuning",
                "reinforcement learning",
                "Group Relative Policy Optimization (GRPO)",
                "VideoScore-Bench-v2",
                "VideoGenReward-Bench",
                "VideoPhy2",
                "Best-of-N sampling"
            ]
        },
        "publishedAt": "2025-09-26T14:09:03.000Z",
        "title": "VideoScore2: Think before You Score in Generative Video Evaluation",
        "summary": "Recent advances in text-to-video generation have produced increasingly\nrealistic and diverse content, yet evaluating such videos remains a fundamental\nchallenge due to their multi-faceted nature encompassing visual quality,\nsemantic alignment, and physical consistency. Existing evaluators and reward\nmodels are limited to single opaque scores, lack interpretability, or provide\nonly coarse analysis, making them insufficient for capturing the comprehensive\nnature of video quality assessment. We present VideoScore2, a\nmulti-dimensional, interpretable, and human-aligned framework that explicitly\nevaluates visual quality, text-to-video alignment, and physical/common-sense\nconsistency while producing detailed chain-of-thought rationales. Our model is\ntrained on a large-scale dataset VideoFeedback2 containing 27,168\nhuman-annotated videos with both scores and reasoning traces across three\ndimensions, using a two-stage pipeline of supervised fine-tuning followed by\nreinforcement learning with Group Relative Policy Optimization (GRPO) to\nenhance analytical robustness. Extensive experiments demonstrate that\nVideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our\nin-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance\nacross four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc),\nwhile providing interpretable assessments that bridge the gap between\nevaluation and controllable generation through effective reward modeling for\nBest-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22799.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "fullname": "Wenhu Chen",
            "name": "wenhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 44
        },
        "submitterOrganization": {
            "_id": "6313a90017838d05194fd282",
            "name": "TIGER-Lab",
            "fullname": "TIGER-Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/Noi3Qq3RYz8Jdq6BaFteq.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22824",
            "authors": [
                {
                    "_id": "68db550cd2bf1f4b15ec74fe",
                    "name": "Chi Ruan",
                    "hidden": false
                },
                {
                    "_id": "68db550cd2bf1f4b15ec74ff",
                    "name": "Dongfu Jiang",
                    "hidden": false
                },
                {
                    "_id": "68db550cd2bf1f4b15ec7500",
                    "name": "Yubo Wang",
                    "hidden": false
                },
                {
                    "_id": "68db550cd2bf1f4b15ec7501",
                    "name": "Wenhu Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T18:30:49.000Z",
            "submittedOnDailyAt": "2025-09-30T02:27:39.733Z",
            "title": "Critique-Coder: Enhancing Coder Models by Critique Reinforcement\n  Learning",
            "submittedOnDailyBy": {
                "_id": "6313a86154e6e5d9f0f94e04",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                "isPro": false,
                "fullname": "Wenhu Chen",
                "user": "wenhu",
                "type": "user"
            },
            "summary": "Reinforcement Learning (RL) has emerged as a popular training paradigm,\nparticularly when paired with reasoning models. While effective, it primarily\nfocuses on generating responses and lacks mechanisms to explicitly foster\ncritique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT)\nand Critique-Guided-Distillation (CGD) have shown the benefits of explicitly\nteaching LLMs how to critique. Motivated by them, we propose Critique\nReinforcement Learning (CRL), where the model is tasked with generating a\ncritique for a given (question, solution) pair. The reward is determined solely\nby whether the final judgment label c in {True, False}\nof the generated critique aligns with the ground-truth judgment c^*. Building\non this point, we introduce Critique-Coder, which is trained on a\nhybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL\ndata. We fine-tune multiple models (Critique-Coder) and evaluate them\non different benchmarks to show their advantages over RL-only models. We show\nthat Critique-Coder consistently outperforms RL-only baselines on all\nthe evaluated benchmarks. Notably, our Critique-Coder-8B can reach\nover 60\\% on LiveCodeBench (v5), outperforming other reasoning models like\nDeepCoder-14B and GPT-o1. Beyond code generation, Critique-Coder also\ndemonstrates enhanced general reasoning abilities, as evidenced by its better\nperformance on logic reasoning tasks from the BBEH dataset. This indicates that\nthe application of CRL on coding datasets enhances general reasoning and\ncritique abilities, which are transferable across a broad range of tasks.\nHence, we believe that CRL works as a great complement to standard RL for LLM\nreasoning.",
            "upvotes": 18,
            "discussionId": "68db550cd2bf1f4b15ec7502",
            "projectPage": "https://tiger-ai-lab.github.io/Critique-Coder/",
            "ai_summary": "Critique Reinforcement Learning (CRL) enhances LLMs by teaching them to generate critiques, leading to improved performance on code generation and logic reasoning tasks compared to standard RL.",
            "ai_keywords": [
                "Reinforcement Learning (RL)",
                "Critique-Fine-Tuning (CFT)",
                "Critique-Guided-Distillation (CGD)",
                "Critique Reinforcement Learning (CRL)",
                "Critique-Coder",
                "LiveCodeBench",
                "DeepCoder-14B",
                "GPT-o1",
                "BBEH dataset"
            ]
        },
        "publishedAt": "2025-09-26T14:30:49.000Z",
        "title": "Critique-Coder: Enhancing Coder Models by Critique Reinforcement\n  Learning",
        "summary": "Reinforcement Learning (RL) has emerged as a popular training paradigm,\nparticularly when paired with reasoning models. While effective, it primarily\nfocuses on generating responses and lacks mechanisms to explicitly foster\ncritique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT)\nand Critique-Guided-Distillation (CGD) have shown the benefits of explicitly\nteaching LLMs how to critique. Motivated by them, we propose Critique\nReinforcement Learning (CRL), where the model is tasked with generating a\ncritique for a given (question, solution) pair. The reward is determined solely\nby whether the final judgment label c in {True, False}\nof the generated critique aligns with the ground-truth judgment c^*. Building\non this point, we introduce Critique-Coder, which is trained on a\nhybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL\ndata. We fine-tune multiple models (Critique-Coder) and evaluate them\non different benchmarks to show their advantages over RL-only models. We show\nthat Critique-Coder consistently outperforms RL-only baselines on all\nthe evaluated benchmarks. Notably, our Critique-Coder-8B can reach\nover 60\\% on LiveCodeBench (v5), outperforming other reasoning models like\nDeepCoder-14B and GPT-o1. Beyond code generation, Critique-Coder also\ndemonstrates enhanced general reasoning abilities, as evidenced by its better\nperformance on logic reasoning tasks from the BBEH dataset. This indicates that\nthe application of CRL on coding datasets enhances general reasoning and\ncritique abilities, which are transferable across a broad range of tasks.\nHence, we believe that CRL works as a great complement to standard RL for LLM\nreasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22824.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "fullname": "Wenhu Chen",
            "name": "wenhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 44
        },
        "submitterOrganization": {
            "_id": "6313a90017838d05194fd282",
            "name": "TIGER-Lab",
            "fullname": "TIGER-Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/Noi3Qq3RYz8Jdq6BaFteq.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25123",
            "authors": [
                {
                    "_id": "68db6278d2bf1f4b15ec75e3",
                    "name": "Lifan Yuan",
                    "hidden": false
                },
                {
                    "_id": "68db6278d2bf1f4b15ec75e4",
                    "name": "Weize Chen",
                    "hidden": false
                },
                {
                    "_id": "68db6278d2bf1f4b15ec75e5",
                    "name": "Yuchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db6278d2bf1f4b15ec75e6",
                    "name": "Ganqu Cui",
                    "hidden": false
                },
                {
                    "_id": "68db6278d2bf1f4b15ec75e7",
                    "name": "Hanbin Wang",
                    "hidden": false
                },
                {
                    "_id": "68db6278d2bf1f4b15ec75e8",
                    "name": "Ziming You",
                    "hidden": false
                },
                {
                    "_id": "68db6278d2bf1f4b15ec75e9",
                    "name": "Ning Ding",
                    "hidden": false
                },
                {
                    "_id": "68db6278d2bf1f4b15ec75ea",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68db6278d2bf1f4b15ec75eb",
                    "name": "Maosong Sun",
                    "hidden": false
                },
                {
                    "_id": "68db6278d2bf1f4b15ec75ec",
                    "name": "Hao Peng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:44:27.000Z",
            "submittedOnDailyAt": "2025-09-30T03:34:35.092Z",
            "title": "From f(x) and g(x) to f(g(x)): LLMs Learn New Skills in RL by\n  Composing Old Ones",
            "submittedOnDailyBy": {
                "_id": "648312243b7fe59c876c0dca",
                "avatarUrl": "/avatars/c26ad76cd213529e4670bb599b8199bb.svg",
                "isPro": false,
                "fullname": "weize",
                "user": "weizechen",
                "type": "user"
            },
            "summary": "Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes the reasoning behaviors of the\nmodels. In contrast, next-token training with the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems.",
            "upvotes": 16,
            "discussionId": "68db6278d2bf1f4b15ec75ed",
            "ai_summary": "Reinforcement learning enables large language models to acquire new compositional skills by combining existing ones, which transfer to different tasks and improve reasoning behaviors.",
            "ai_keywords": [
                "reinforcement learning",
                "large language models",
                "compositional skills",
                "reasoning behaviors",
                "next-token training"
            ]
        },
        "publishedAt": "2025-09-29T13:44:27.000Z",
        "title": "From f(x) and g(x) to f(g(x)): LLMs Learn New Skills in RL by\n  Composing Old Ones",
        "summary": "Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes the reasoning behaviors of the\nmodels. In contrast, next-token training with the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25123.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648312243b7fe59c876c0dca",
            "avatarUrl": "/avatars/c26ad76cd213529e4670bb599b8199bb.svg",
            "fullname": "weize",
            "name": "weizechen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.24473",
            "authors": [
                {
                    "_id": "68db44add2bf1f4b15ec7378",
                    "name": "Shijie Lian",
                    "hidden": false
                },
                {
                    "_id": "68db44add2bf1f4b15ec7379",
                    "user": {
                        "_id": "67b55e66d454cc4d10d21cfd",
                        "avatarUrl": "/avatars/3b18014fa7e603a5940175896f89372a.svg",
                        "isPro": false,
                        "fullname": "Changti Wu",
                        "user": "MaplesWCT",
                        "type": "user"
                    },
                    "name": "Changti Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:30:31.203Z",
                    "hidden": false
                },
                {
                    "_id": "68db44add2bf1f4b15ec737a",
                    "name": "Laurence Tianruo Yang",
                    "hidden": false
                },
                {
                    "_id": "68db44add2bf1f4b15ec737b",
                    "name": "Hang Yuan",
                    "hidden": false
                },
                {
                    "_id": "68db44add2bf1f4b15ec737c",
                    "name": "Bin Yu",
                    "hidden": false
                },
                {
                    "_id": "68db44add2bf1f4b15ec737d",
                    "name": "Lei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db44add2bf1f4b15ec737e",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T08:49:21.000Z",
            "submittedOnDailyAt": "2025-09-30T01:27:36.076Z",
            "title": "Euclid's Gift: Enhancing Spatial Perception and Reasoning in\n  Vision-Language Models via Geometric Surrogate Tasks",
            "submittedOnDailyBy": {
                "_id": "65ec01fd770aa0e25d9374dc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg",
                "isPro": false,
                "fullname": "Shijie Lian",
                "user": "LiamLian0727",
                "type": "user"
            },
            "summary": "Spatial intelligence spans a rich suite of abilities, including visualising\nand transforming shapes, mentally rotating objects, judging relational\npositions and containment, and estimating numerosity. However, it still remains\na critical unresolved challenge for Multimodal Large Language Models (MLLMs).To\nfill this gap, we propose to treat Euclidean geometry problem-solving as a\nsurrogate task. Specifically, we meticulously constructed a curated multimodal\ndataset, called Euclid30K, comprising approximately 30K plane and solid\ngeometry problems. To enable the model to acquire and apply Euclidean\nprinciples from these geometry problems, we employed Group Relative Policy\nOptimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family,\ninspiring the models to identify shapes, count, and relate entities, and\nperform multi-step deductive reasoning using Euclidean principles. Our\nexperiments demonstrate that the resulting models achieve substantial zero-shot\ngains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench,\nVSI-Bench, and MindCube) without any task-specific adaptations. Notably, after\ntraining on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models\nrose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them,\nRoboBrain2.0-Euclid-7B achieves 49.6\\% accuracy, surpassing the previous\nstate-of-the-art model, Spatial-MLLM.To our knowledge, this is the first\nsystematic study showing that geometry-centric fine-tuning can confer\nvision-language models with broadly transferable spatial skills. Code and\nEuclid30K dataset can be found in https://zgca-ai4edu.github.io/Euclids_Gift.",
            "upvotes": 15,
            "discussionId": "68db44add2bf1f4b15ec737f",
            "projectPage": "https://zgca-ai4edu.github.io/Euclids_Gift/",
            "githubRepo": "https://github.com/LiamLian0727/Euclids_Gift",
            "ai_summary": "Geometry-centric fine-tuning using the Euclid30K dataset significantly improves spatial reasoning abilities in multimodal large language models across multiple benchmarks.",
            "ai_keywords": [
                "Euclidean geometry",
                "Group Relative Policy Optimization (GRPO)",
                "Qwen2.5VL",
                "RoboBrain2.0",
                "zero-shot gains",
                "spatial reasoning benchmarks",
                "Super-CLEVR",
                "Omni3DBench",
                "VSI-Bench",
                "MindCube",
                "vision-language models"
            ],
            "githubStars": 12
        },
        "publishedAt": "2025-09-29T04:49:21.000Z",
        "title": "Euclid's Gift: Enhancing Spatial Perception and Reasoning in\n  Vision-Language Models via Geometric Surrogate Tasks",
        "summary": "Spatial intelligence spans a rich suite of abilities, including visualising\nand transforming shapes, mentally rotating objects, judging relational\npositions and containment, and estimating numerosity. However, it still remains\na critical unresolved challenge for Multimodal Large Language Models (MLLMs).To\nfill this gap, we propose to treat Euclidean geometry problem-solving as a\nsurrogate task. Specifically, we meticulously constructed a curated multimodal\ndataset, called Euclid30K, comprising approximately 30K plane and solid\ngeometry problems. To enable the model to acquire and apply Euclidean\nprinciples from these geometry problems, we employed Group Relative Policy\nOptimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family,\ninspiring the models to identify shapes, count, and relate entities, and\nperform multi-step deductive reasoning using Euclidean principles. Our\nexperiments demonstrate that the resulting models achieve substantial zero-shot\ngains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench,\nVSI-Bench, and MindCube) without any task-specific adaptations. Notably, after\ntraining on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models\nrose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them,\nRoboBrain2.0-Euclid-7B achieves 49.6\\% accuracy, surpassing the previous\nstate-of-the-art model, Spatial-MLLM.To our knowledge, this is the first\nsystematic study showing that geometry-centric fine-tuning can confer\nvision-language models with broadly transferable spatial skills. Code and\nEuclid30K dataset can be found in https://zgca-ai4edu.github.io/Euclids_Gift.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24473.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65ec01fd770aa0e25d9374dc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg",
            "fullname": "Shijie Lian",
            "name": "LiamLian0727",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "submitterOrganization": {
            "_id": "68896d3a716ee5bfb1428441",
            "name": "ZGCA",
            "fullname": "Zhongguancun Academy",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22820",
            "authors": [
                {
                    "_id": "68db5230d2bf1f4b15ec74df",
                    "user": {
                        "_id": "67c6c1b3806c89e23ea4b42d",
                        "avatarUrl": "/avatars/29c88684f243b063d221ef29300ed18d.svg",
                        "isPro": false,
                        "fullname": "Kim Jaeik",
                        "user": "jaeikkim",
                        "type": "user"
                    },
                    "name": "Jaeik Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:00:49.584Z",
                    "hidden": false
                },
                {
                    "_id": "68db5230d2bf1f4b15ec74e0",
                    "name": "Woojin Kim",
                    "hidden": false
                },
                {
                    "_id": "68db5230d2bf1f4b15ec74e1",
                    "name": "Woohyeon Park",
                    "hidden": false
                },
                {
                    "_id": "68db5230d2bf1f4b15ec74e2",
                    "name": "Jaeyoung Do",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T18:24:48.000Z",
            "submittedOnDailyAt": "2025-09-30T06:38:47.110Z",
            "title": "MMPB: It's Time for Multi-Modal Personalization",
            "submittedOnDailyBy": {
                "_id": "67c6c1b3806c89e23ea4b42d",
                "avatarUrl": "/avatars/29c88684f243b063d221ef29300ed18d.svg",
                "isPro": false,
                "fullname": "Kim Jaeik",
                "user": "jaeikkim",
                "type": "user"
            },
            "summary": "Visual personalization is essential in user-facing AI systems such as smart\nhomes and healthcare, where aligning model behavior with user-centric concepts\nis critical. However, recent large Vision-Language Models (VLMs), despite their\nbroad applicability, remain underexplored in their ability to adapt to\nindividual users. In this paper, we introduce MMPB, the first extensive\nbenchmark for evaluating VLMs on personalization. MMPB comprises 10k\nimage-query pairs and includes 111 personalizable concepts across four\ncategories: humans, animals, objects, and characters, with the human category\nenriched with preference-grounded queries. We structure personalization into\nthree main task types, each highlighting a different key property of VLMs.\nUsing 23 widely used VLMs including both open- and closed-source models, we\nevaluate personalization performance via a three-stage protocol: concept\ninjection, multi-turn dialogue, and personalized querying. Our findings\nindicate that most VLMs (including some closed-source models) struggle with\npersonalization, particularly in maintaining consistency over dialogue,\nhandling user preferences, and adapting to visual cues. Our analysis reveals\nthat the challenges in VLM personalization (such as refusal behaviors and\nlong-context forgetting) highlight substantial room for improvement. By\nidentifying these limitations and offering a scalable benchmark, MMPB offers\nvaluable insights and a solid foundation for future research toward truly\npersonalized multi-modal AI. Project Page: aidaslab.github.io/MMPB",
            "upvotes": 14,
            "discussionId": "68db5230d2bf1f4b15ec74e3",
            "projectPage": "https://aidaslab.github.io/MMPB/",
            "githubRepo": "https://github.com/AIDASLab/MMPB",
            "ai_summary": "MMPB is a benchmark for evaluating the personalization capabilities of Vision-Language Models across various tasks and concepts, revealing significant challenges in maintaining consistency and adapting to user preferences.",
            "ai_keywords": [
                "Vision-Language Models",
                "VLMs",
                "personalization",
                "benchmark",
                "image-query pairs",
                "preference-grounded queries",
                "concept injection",
                "multi-turn dialogue",
                "personalized querying",
                "refusal behaviors",
                "long-context forgetting"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-09-26T14:24:48.000Z",
        "title": "MMPB: It's Time for Multi-Modal Personalization",
        "summary": "Visual personalization is essential in user-facing AI systems such as smart\nhomes and healthcare, where aligning model behavior with user-centric concepts\nis critical. However, recent large Vision-Language Models (VLMs), despite their\nbroad applicability, remain underexplored in their ability to adapt to\nindividual users. In this paper, we introduce MMPB, the first extensive\nbenchmark for evaluating VLMs on personalization. MMPB comprises 10k\nimage-query pairs and includes 111 personalizable concepts across four\ncategories: humans, animals, objects, and characters, with the human category\nenriched with preference-grounded queries. We structure personalization into\nthree main task types, each highlighting a different key property of VLMs.\nUsing 23 widely used VLMs including both open- and closed-source models, we\nevaluate personalization performance via a three-stage protocol: concept\ninjection, multi-turn dialogue, and personalized querying. Our findings\nindicate that most VLMs (including some closed-source models) struggle with\npersonalization, particularly in maintaining consistency over dialogue,\nhandling user preferences, and adapting to visual cues. Our analysis reveals\nthat the challenges in VLM personalization (such as refusal behaviors and\nlong-context forgetting) highlight substantial room for improvement. By\nidentifying these limitations and offering a scalable benchmark, MMPB offers\nvaluable insights and a solid foundation for future research toward truly\npersonalized multi-modal AI. Project Page: aidaslab.github.io/MMPB",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22820.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67c6c1b3806c89e23ea4b42d",
            "avatarUrl": "/avatars/29c88684f243b063d221ef29300ed18d.svg",
            "fullname": "Kim Jaeik",
            "name": "jaeikkim",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "submitterOrganization": {
            "_id": "6801da61b09d06d3197821e8",
            "name": "snu-aidas",
            "fullname": "AI, Big Data, and System Laboratory",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6801d93268af2860f1711fe5/Q8j58UzuV790FhZZkbitM.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25191",
            "authors": [
                {
                    "_id": "68db55d7d2bf1f4b15ec751e",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "68db55d7d2bf1f4b15ec751f",
                    "name": "Chuanchen Luo",
                    "hidden": false
                },
                {
                    "_id": "68db55d7d2bf1f4b15ec7520",
                    "name": "Zimo Tang",
                    "hidden": false
                },
                {
                    "_id": "68db55d7d2bf1f4b15ec7521",
                    "name": "Junran Peng",
                    "hidden": false
                },
                {
                    "_id": "68db55d7d2bf1f4b15ec7522",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:59:59.000Z",
            "submittedOnDailyAt": "2025-09-30T03:17:54.551Z",
            "title": "VGGT-X: When VGGT Meets Dense Novel View Synthesis",
            "submittedOnDailyBy": {
                "_id": "66ef6fd0ea7d19a2399d6b1f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/rJIMhaB4NB8bvBtmsw1vA.png",
                "isPro": false,
                "fullname": "Yang Liu",
                "user": "TeslaYang123",
                "type": "user"
            },
            "summary": "We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel\nView Synthesis (NVS). Despite significant progress in Novel View Synthesis\npowered by NeRF and 3DGS, current approaches remain reliant on accurate 3D\nattributes (e.g., camera poses and point clouds) acquired from\nStructure-from-Motion (SfM), which is often slow and fragile in low-texture or\nlow-overlap captures. Recent 3DFMs showcase orders of magnitude speedup over\nthe traditional pipeline and great potential for online NVS. But most of the\nvalidation and conclusions are confined to sparse-view settings. Our study\nreveals that naively scaling 3DFMs to dense views encounters two fundamental\nbarriers: dramatically increasing VRAM burden and imperfect outputs that\ndegrade initialization-sensitive 3D training. To address these barriers, we\nintroduce VGGT-X, incorporating a memory-efficient VGGT implementation that\nscales to 1,000+ images, an adaptive global alignment for VGGT output\nenhancement, and robust 3DGS training practices. Extensive experiments show\nthat these measures substantially close the fidelity gap with\nCOLMAP-initialized pipelines, achieving state-of-the-art results in dense\nCOLMAP-free NVS and pose estimation. Additionally, we analyze the causes of\nremaining gaps with COLMAP-initialized rendering, providing insights for the\nfuture development of 3D foundation models and dense NVS. Our project page is\navailable at https://dekuliutesla.github.io/vggt-x.github.io/",
            "upvotes": 13,
            "discussionId": "68db55d7d2bf1f4b15ec7523",
            "projectPage": "https://dekuliutesla.github.io/vggt-x.github.io/",
            "githubRepo": "https://github.com/Linketic/VGGT-X",
            "ai_summary": "VGGT-X addresses VRAM and output quality issues in scaling 3D Foundation Models for dense Novel View Synthesis without relying on COLMAP.",
            "ai_keywords": [
                "3D Foundation Models",
                "Novel View Synthesis",
                "NeRF",
                "3DGS",
                "Structure-from-Motion",
                "VGGT-X",
                "memory-efficient",
                "adaptive global alignment",
                "robust 3DGS training",
                "COLMAP-free NVS",
                "pose estimation"
            ],
            "githubStars": 36
        },
        "publishedAt": "2025-09-29T13:59:59.000Z",
        "title": "VGGT-X: When VGGT Meets Dense Novel View Synthesis",
        "summary": "We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel\nView Synthesis (NVS). Despite significant progress in Novel View Synthesis\npowered by NeRF and 3DGS, current approaches remain reliant on accurate 3D\nattributes (e.g., camera poses and point clouds) acquired from\nStructure-from-Motion (SfM), which is often slow and fragile in low-texture or\nlow-overlap captures. Recent 3DFMs showcase orders of magnitude speedup over\nthe traditional pipeline and great potential for online NVS. But most of the\nvalidation and conclusions are confined to sparse-view settings. Our study\nreveals that naively scaling 3DFMs to dense views encounters two fundamental\nbarriers: dramatically increasing VRAM burden and imperfect outputs that\ndegrade initialization-sensitive 3D training. To address these barriers, we\nintroduce VGGT-X, incorporating a memory-efficient VGGT implementation that\nscales to 1,000+ images, an adaptive global alignment for VGGT output\nenhancement, and robust 3DGS training practices. Extensive experiments show\nthat these measures substantially close the fidelity gap with\nCOLMAP-initialized pipelines, achieving state-of-the-art results in dense\nCOLMAP-free NVS and pose estimation. Additionally, we analyze the causes of\nremaining gaps with COLMAP-initialized rendering, providing insights for the\nfuture development of 3D foundation models and dense NVS. Our project page is\navailable at https://dekuliutesla.github.io/vggt-x.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25191.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66ef6fd0ea7d19a2399d6b1f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/rJIMhaB4NB8bvBtmsw1vA.png",
            "fullname": "Yang Liu",
            "name": "TeslaYang123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25161",
            "authors": [
                {
                    "_id": "68db4f7bd2bf1f4b15ec749d",
                    "user": {
                        "_id": "660b79aa822b119f215c3488",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660b79aa822b119f215c3488/le_EuzAz6mhW0FQ9J0knV.jpeg",
                        "isPro": false,
                        "fullname": "Kunhao Liu",
                        "user": "KunhaoLiu",
                        "type": "user"
                    },
                    "name": "Kunhao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T10:18:01.327Z",
                    "hidden": false
                },
                {
                    "_id": "68db4f7bd2bf1f4b15ec749e",
                    "user": {
                        "_id": "657a7458afbb0117ba15c59f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
                        "isPro": false,
                        "fullname": "Wenbo Hu",
                        "user": "wbhu-tc",
                        "type": "user"
                    },
                    "name": "Wenbo Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:00:54.201Z",
                    "hidden": false
                },
                {
                    "_id": "68db4f7bd2bf1f4b15ec749f",
                    "name": "Jiale Xu",
                    "hidden": false
                },
                {
                    "_id": "68db4f7bd2bf1f4b15ec74a0",
                    "name": "Ying Shan",
                    "hidden": false
                },
                {
                    "_id": "68db4f7bd2bf1f4b15ec74a1",
                    "name": "Shijian Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:57:14.000Z",
            "submittedOnDailyAt": "2025-09-30T02:03:32.019Z",
            "title": "Rolling Forcing: Autoregressive Long Video Diffusion in Real Time",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Streaming video generation, as one fundamental component in interactive world\nmodels and neural game engines, aims to generate high-quality, low-latency, and\ntemporally coherent long video streams. However, most existing work suffers\nfrom severe error accumulation that often significantly degrades the generated\nstream videos over long horizons. We design Rolling Forcing, a novel video\ngeneration technique that enables streaming long videos with minimal error\naccumulation. Rolling Forcing comes with three novel designs. First, instead of\niteratively sampling individual frames, which accelerates error propagation, we\ndesign a joint denoising scheme that simultaneously denoises multiple frames\nwith progressively increasing noise levels. This design relaxes the strict\ncausality across adjacent frames, effectively suppressing error growth. Second,\nwe introduce the attention sink mechanism into the long-horizon stream video\ngeneration task, which allows the model to keep key value states of initial\nframes as a global context anchor and thereby enhances long-term global\nconsistency. Third, we design an efficient training algorithm that enables\nfew-step distillation over largely extended denoising windows. This algorithm\noperates on non-overlapping windows and mitigates exposure bias conditioned on\nself-generated histories. Extensive experiments show that Rolling Forcing\nenables real-time streaming generation of multi-minute videos on a single GPU,\nwith substantially reduced error accumulation.",
            "upvotes": 13,
            "discussionId": "68db4f7cd2bf1f4b15ec74a2",
            "projectPage": "https://kunhao-liu.github.io/Rolling_Forcing_Webpage/",
            "githubRepo": "https://github.com/TencentARC/RollingForcing",
            "ai_summary": "Rolling Forcing is a novel video generation technique that reduces error accumulation in long video streams by using joint denoising, attention sink mechanism, and efficient training with non-overlapping windows.",
            "ai_keywords": [
                "joint denoising",
                "attention sink mechanism",
                "error accumulation",
                "long-horizon stream video generation",
                "global context anchor",
                "long-term global consistency",
                "efficient training algorithm",
                "few-step distillation",
                "denoising windows",
                "exposure bias"
            ],
            "githubStars": 71
        },
        "publishedAt": "2025-09-29T13:57:14.000Z",
        "title": "Rolling Forcing: Autoregressive Long Video Diffusion in Real Time",
        "summary": "Streaming video generation, as one fundamental component in interactive world\nmodels and neural game engines, aims to generate high-quality, low-latency, and\ntemporally coherent long video streams. However, most existing work suffers\nfrom severe error accumulation that often significantly degrades the generated\nstream videos over long horizons. We design Rolling Forcing, a novel video\ngeneration technique that enables streaming long videos with minimal error\naccumulation. Rolling Forcing comes with three novel designs. First, instead of\niteratively sampling individual frames, which accelerates error propagation, we\ndesign a joint denoising scheme that simultaneously denoises multiple frames\nwith progressively increasing noise levels. This design relaxes the strict\ncausality across adjacent frames, effectively suppressing error growth. Second,\nwe introduce the attention sink mechanism into the long-horizon stream video\ngeneration task, which allows the model to keep key value states of initial\nframes as a global context anchor and thereby enhances long-term global\nconsistency. Third, we design an efficient training algorithm that enables\nfew-step distillation over largely extended denoising windows. This algorithm\noperates on non-overlapping windows and mitigates exposure bias conditioned on\nself-generated histories. Extensive experiments show that Rolling Forcing\nenables real-time streaming generation of multi-minute videos on a single GPU,\nwith substantially reduced error accumulation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25161.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 115
        },
        "submitterOrganization": {
            "_id": "60e3f7f641ca131919975fe5",
            "name": "TencentARC",
            "fullname": "ARC Lab, Tencent PCG",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1625552871844-60e272ca6c78a8c122b12127.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25077",
            "authors": [
                {
                    "_id": "68db59b1d2bf1f4b15ec75a2",
                    "user": {
                        "_id": "64898937431b7a5e075d2b7b",
                        "avatarUrl": "/avatars/26d919d4ce925909de660303786e98c3.svg",
                        "isPro": false,
                        "fullname": "L",
                        "user": "Dingning",
                        "type": "user"
                    },
                    "name": "Dingning Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:00:38.759Z",
                    "hidden": false
                },
                {
                    "_id": "68db59b1d2bf1f4b15ec75a3",
                    "user": {
                        "_id": "652ce0d4c543a08aa92e010f",
                        "avatarUrl": "/avatars/7978304e3fe99b0d4d0712441c6a24f3.svg",
                        "isPro": false,
                        "fullname": "Haoyu Guo",
                        "user": "ghy0324",
                        "type": "user"
                    },
                    "name": "Haoyu Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:00:36.341Z",
                    "hidden": false
                },
                {
                    "_id": "68db59b1d2bf1f4b15ec75a4",
                    "name": "Jingyi Zhou",
                    "hidden": false
                },
                {
                    "_id": "68db59b1d2bf1f4b15ec75a5",
                    "name": "Tong He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:19:45.000Z",
            "submittedOnDailyAt": "2025-09-30T02:48:50.666Z",
            "title": "BRIDGE - Building Reinforcement-Learning Depth-to-Image Data Generation\n  Engine for Monocular Depth Estimation",
            "submittedOnDailyBy": {
                "_id": "64898937431b7a5e075d2b7b",
                "avatarUrl": "/avatars/26d919d4ce925909de660303786e98c3.svg",
                "isPro": false,
                "fullname": "L",
                "user": "Dingning",
                "type": "user"
            },
            "summary": "Monocular Depth Estimation (MDE) is a foundational task for computer vision.\nTraditional methods are limited by data scarcity and quality, hindering their\nrobustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image\n(D2I) generation framework that synthesizes over 20M realistic and\ngeometrically accurate RGB images, each intrinsically paired with its ground\ntruth depth, from diverse source depth maps. Then we train our depth estimation\nmodel on this dataset, employing a hybrid supervision strategy that integrates\nteacher pseudo-labels with ground truth depth for comprehensive and robust\ntraining. This innovative data generation and training paradigm enables BRIDGE\nto achieve breakthroughs in scale and domain diversity, consistently\noutperforming existing state-of-the-art approaches quantitatively and in\ncomplex scene detail capture, thereby fostering general and robust depth\nfeatures. Code and models are available at\nhttps://dingning-liu.github.io/bridge.github.io/.",
            "upvotes": 13,
            "discussionId": "68db59b1d2bf1f4b15ec75a6",
            "projectPage": "https://dingning-liu.github.io/bridge.github.io/",
            "githubRepo": "https://github.com/lnbxldn/BRIDGE",
            "ai_summary": "BRIDGE uses RL-optimized depth-to-image generation to create a large, diverse dataset, enhancing monocular depth estimation robustness and performance.",
            "ai_keywords": [
                "monocular depth estimation",
                "RL-optimized",
                "depth-to-image",
                "ground truth depth",
                "hybrid supervision",
                "teacher pseudo-labels",
                "complex scene detail capture"
            ],
            "githubStars": 38
        },
        "publishedAt": "2025-09-29T13:19:45.000Z",
        "title": "BRIDGE - Building Reinforcement-Learning Depth-to-Image Data Generation\n  Engine for Monocular Depth Estimation",
        "summary": "Monocular Depth Estimation (MDE) is a foundational task for computer vision.\nTraditional methods are limited by data scarcity and quality, hindering their\nrobustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image\n(D2I) generation framework that synthesizes over 20M realistic and\ngeometrically accurate RGB images, each intrinsically paired with its ground\ntruth depth, from diverse source depth maps. Then we train our depth estimation\nmodel on this dataset, employing a hybrid supervision strategy that integrates\nteacher pseudo-labels with ground truth depth for comprehensive and robust\ntraining. This innovative data generation and training paradigm enables BRIDGE\nto achieve breakthroughs in scale and domain diversity, consistently\noutperforming existing state-of-the-art approaches quantitatively and in\ncomplex scene detail capture, thereby fostering general and robust depth\nfeatures. Code and models are available at\nhttps://dingning-liu.github.io/bridge.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25077.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64898937431b7a5e075d2b7b",
            "avatarUrl": "/avatars/26d919d4ce925909de660303786e98c3.svg",
            "fullname": "L",
            "name": "Dingning",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "submitterOrganization": {
            "_id": "6747ee5decec679eafb90450",
            "name": "ShanghaiAiLab",
            "fullname": "shanghai ailab "
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.24663",
            "authors": [
                {
                    "_id": "68db57fdd2bf1f4b15ec7593",
                    "user": {
                        "_id": "64b8ff3d95bd42c770878042",
                        "avatarUrl": "/avatars/564a4dccdf9e5b813a99979b0ef58183.svg",
                        "isPro": false,
                        "fullname": "Weilin Zhao",
                        "user": "Achazwl",
                        "type": "user"
                    },
                    "name": "Weilin Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-30T12:45:29.337Z",
                    "hidden": false
                },
                {
                    "_id": "68db57fdd2bf1f4b15ec7594",
                    "name": "Zihan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68db57fdd2bf1f4b15ec7595",
                    "user": {
                        "_id": "65e028993c9276bcc05233cd",
                        "avatarUrl": "/avatars/1bb4666670f3778d412c3b04b4f3e75a.svg",
                        "isPro": false,
                        "fullname": "zhousu",
                        "user": "zhousu",
                        "type": "user"
                    },
                    "name": "Zhou Su",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-30T12:46:17.193Z",
                    "hidden": false
                },
                {
                    "_id": "68db57fdd2bf1f4b15ec7596",
                    "user": {
                        "_id": "608f6d72283d0a8d7be9d1f9",
                        "avatarUrl": "/avatars/7f499a37019359a3c488ba6cc11751fc.svg",
                        "isPro": false,
                        "fullname": "Chaojun XIAO",
                        "user": "xcjthu",
                        "type": "user"
                    },
                    "name": "Chaojun Xiao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-30T12:36:30.210Z",
                    "hidden": false
                },
                {
                    "_id": "68db57fdd2bf1f4b15ec7597",
                    "name": "Yuxuan Li",
                    "hidden": false
                },
                {
                    "_id": "68db57fdd2bf1f4b15ec7598",
                    "user": {
                        "_id": "65dad3870af7e21ba473439f",
                        "avatarUrl": "/avatars/da542e7d68ae937bbdb791f17096bb1c.svg",
                        "isPro": false,
                        "fullname": "Yanghao Li",
                        "user": "FrozzZen",
                        "type": "user"
                    },
                    "name": "Yanghao Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-30T12:46:44.138Z",
                    "hidden": false
                },
                {
                    "_id": "68db57fdd2bf1f4b15ec7599",
                    "name": "Yudi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db57fdd2bf1f4b15ec759a",
                    "name": "Weilun Zhao",
                    "hidden": false
                },
                {
                    "_id": "68db57fdd2bf1f4b15ec759b",
                    "name": "Zhen Li",
                    "hidden": false
                },
                {
                    "_id": "68db57fdd2bf1f4b15ec759c",
                    "name": "Yuxiang Huang",
                    "hidden": false
                },
                {
                    "_id": "68db57fdd2bf1f4b15ec759d",
                    "name": "Ao Sun",
                    "hidden": false
                },
                {
                    "_id": "68db57fdd2bf1f4b15ec759e",
                    "name": "Xu Han",
                    "hidden": false
                },
                {
                    "_id": "68db57fdd2bf1f4b15ec759f",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T12:08:33.000Z",
            "submittedOnDailyAt": "2025-09-30T02:42:18.336Z",
            "title": "InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long\n  Adaptation",
            "submittedOnDailyBy": {
                "_id": "608f6d72283d0a8d7be9d1f9",
                "avatarUrl": "/avatars/7f499a37019359a3c488ba6cc11751fc.svg",
                "isPro": false,
                "fullname": "Chaojun XIAO",
                "user": "xcjthu",
                "type": "user"
            },
            "summary": "Long-sequence processing is a critical capability for modern large language\nmodels. However, the self-attention mechanism in the standard Transformer\narchitecture faces severe computational and memory bottlenecks when processing\nlong sequences. While trainable sparse attention methods offer a promising\nsolution, existing approaches such as NSA introduce excessive extra parameters\nand disrupt the conventional pretrain-on-short, finetune-on-long\nworkflow, resulting in slow convergence and difficulty in acceleration. To\novercome these limitations, we introduce dense-sparse switchable attention\nframework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that\nseamlessly adapts models from short to long sequences. Specifically, InfLLM-V2\nreuses dense attention parameters through parameter-free architecture\nmodification, maintaining consistency between short and long sequence\nprocessing. Additionally, InfLLM-V2 ensures computational efficiency across all\nsequence lengths, by using dense attention for short inputs and smoothly\ntransitioning to sparse attention for long sequences. To achieve practical\nacceleration, we further introduce an efficient implementation of InfLLM-V2\nthat significantly reduces the computational overhead. Our experiments on\nlong-context understanding and chain-of-thought reasoning demonstrate that\nInfLLM-V2 is 4times faster than dense attention while retaining 98.1% and\n99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we\nhave trained and open-sourced MiniCPM4.1\n(https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model,\nproviding a reproducible implementation for the research community.",
            "upvotes": 12,
            "discussionId": "68db57fdd2bf1f4b15ec75a0",
            "ai_summary": "A dense-sparse switchable attention framework, InfLLM-V2, enhances long-sequence processing in large language models by efficiently adapting between dense and sparse attention mechanisms.",
            "ai_keywords": [
                "self-attention mechanism",
                "Transformer architecture",
                "trainable sparse attention",
                "dense-sparse switchable attention",
                "InfLLM-V2",
                "parameter-free architecture modification",
                "dense attention",
                "sparse attention",
                "long-context understanding",
                "chain-of-thought reasoning",
                "MiniCPM4.1"
            ]
        },
        "publishedAt": "2025-09-29T08:08:33.000Z",
        "title": "InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long\n  Adaptation",
        "summary": "Long-sequence processing is a critical capability for modern large language\nmodels. However, the self-attention mechanism in the standard Transformer\narchitecture faces severe computational and memory bottlenecks when processing\nlong sequences. While trainable sparse attention methods offer a promising\nsolution, existing approaches such as NSA introduce excessive extra parameters\nand disrupt the conventional pretrain-on-short, finetune-on-long\nworkflow, resulting in slow convergence and difficulty in acceleration. To\novercome these limitations, we introduce dense-sparse switchable attention\nframework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that\nseamlessly adapts models from short to long sequences. Specifically, InfLLM-V2\nreuses dense attention parameters through parameter-free architecture\nmodification, maintaining consistency between short and long sequence\nprocessing. Additionally, InfLLM-V2 ensures computational efficiency across all\nsequence lengths, by using dense attention for short inputs and smoothly\ntransitioning to sparse attention for long sequences. To achieve practical\nacceleration, we further introduce an efficient implementation of InfLLM-V2\nthat significantly reduces the computational overhead. Our experiments on\nlong-context understanding and chain-of-thought reasoning demonstrate that\nInfLLM-V2 is 4times faster than dense attention while retaining 98.1% and\n99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we\nhave trained and open-sourced MiniCPM4.1\n(https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model,\nproviding a reproducible implementation for the research community.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24663.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "608f6d72283d0a8d7be9d1f9",
            "avatarUrl": "/avatars/7f499a37019359a3c488ba6cc11751fc.svg",
            "fullname": "Chaojun XIAO",
            "name": "xcjthu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.23285",
            "authors": [
                {
                    "_id": "68db3809d2bf1f4b15ec728b",
                    "name": "Yifei Chen",
                    "hidden": false
                },
                {
                    "_id": "68db3809d2bf1f4b15ec728c",
                    "name": "Guanting Dong",
                    "hidden": false
                },
                {
                    "_id": "68db3809d2bf1f4b15ec728d",
                    "name": "Zhicheng Dou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-27T12:53:37.000Z",
            "submittedOnDailyAt": "2025-09-30T00:35:38.566Z",
            "title": "Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference\n  Learning",
            "submittedOnDailyBy": {
                "_id": "6621ec2524eb2673fe0790fc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
                "isPro": false,
                "fullname": "Ania Forge",
                "user": "zhangboguodong",
                "type": "user"
            },
            "summary": "Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to\nimprove their internal reasoning ability by integrating external tools.\nHowever, models employing TIR often display suboptimal behaviors, such as\ninsufficient or excessive tool usage and overthinking after tool calls. The\nchallenge of incentivizing LLMs to perform TIR efficiently and accurately,\nwhile stabilizing the reasoning process, remains an open question. In this\npaper, we start by exploring the impact of tool calls on model reasoning from\nthe perspective of information entropy. Our findings indicate that tool call\nresults lead to a distinct change in the information entropy of subsequent\nreasoning, with the overall entropy of the reasoning chain varying based on the\nnumber of tool calls. Building on these insights, we propose Tool-Light, a\nframework designed to encourage LLMs to perform TIR efficiently and accurately.\nOur framework includes dataset construction and multi-stage fine-tuning. For\ndataset construction, we employ continuous self-evolved sampling using the\nfine-tuned model, integrating both vanilla sampling and entropy-guided\nsampling. Besides, we establish strict criteria for selecting positive-negative\npairs during sampling. The training process involves a two-stage approach,\ncomprising Supervised Fine-Tuning (SFT) and Self-Evolved Direct Preference\nOptimization (DPO). Experimental results on 10 datasets demonstrate the\neffectiveness of Tool-Light, significantly improving the model's efficiency in\nexecuting TIR tasks.",
            "upvotes": 12,
            "discussionId": "68db3809d2bf1f4b15ec728e",
            "githubRepo": "https://github.com/asilverlight/Tool-Light",
            "ai_summary": "Tool-Light framework improves large language models' tool-integrated reasoning efficiency and accuracy by leveraging information entropy and a two-stage fine-tuning process.",
            "ai_keywords": [
                "Tool-Integrated Reasoning",
                "large language models",
                "information entropy",
                "Tool-Light",
                "dataset construction",
                "continuous self-evolved sampling",
                "entropy-guided sampling",
                "positive-negative pairs",
                "Supervised Fine-Tuning",
                "Self-Evolved Direct Preference Optimization"
            ],
            "githubStars": 9
        },
        "publishedAt": "2025-09-27T08:53:37.000Z",
        "title": "Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference\n  Learning",
        "summary": "Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to\nimprove their internal reasoning ability by integrating external tools.\nHowever, models employing TIR often display suboptimal behaviors, such as\ninsufficient or excessive tool usage and overthinking after tool calls. The\nchallenge of incentivizing LLMs to perform TIR efficiently and accurately,\nwhile stabilizing the reasoning process, remains an open question. In this\npaper, we start by exploring the impact of tool calls on model reasoning from\nthe perspective of information entropy. Our findings indicate that tool call\nresults lead to a distinct change in the information entropy of subsequent\nreasoning, with the overall entropy of the reasoning chain varying based on the\nnumber of tool calls. Building on these insights, we propose Tool-Light, a\nframework designed to encourage LLMs to perform TIR efficiently and accurately.\nOur framework includes dataset construction and multi-stage fine-tuning. For\ndataset construction, we employ continuous self-evolved sampling using the\nfine-tuned model, integrating both vanilla sampling and entropy-guided\nsampling. Besides, we establish strict criteria for selecting positive-negative\npairs during sampling. The training process involves a two-stage approach,\ncomprising Supervised Fine-Tuning (SFT) and Self-Evolved Direct Preference\nOptimization (DPO). Experimental results on 10 datasets demonstrate the\neffectiveness of Tool-Light, significantly improving the model's efficiency in\nexecuting TIR tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23285.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6621ec2524eb2673fe0790fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
            "fullname": "Ania Forge",
            "name": "zhangboguodong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "submitterOrganization": {
            "_id": "622177ac43826d6f261f8208",
            "name": "RUC",
            "fullname": "Renmin University of China",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25176",
            "authors": [
                {
                    "_id": "68db421ad2bf1f4b15ec72fd",
                    "name": "Haoming Wen",
                    "hidden": false
                },
                {
                    "_id": "68db421ad2bf1f4b15ec72fe",
                    "name": "Yushi Bai",
                    "hidden": false
                },
                {
                    "_id": "68db421ad2bf1f4b15ec72ff",
                    "name": "Juanzi Li",
                    "hidden": false
                },
                {
                    "_id": "68db421ad2bf1f4b15ec7300",
                    "name": "Jie Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:59:08.000Z",
            "submittedOnDailyAt": "2025-09-30T01:35:51.163Z",
            "title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved\n  Compression",
            "submittedOnDailyBy": {
                "_id": "64ed568ccf6118a9379a61b8",
                "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
                "isPro": false,
                "fullname": "Yushi Bai",
                "user": "bys0318",
                "type": "user"
            },
            "summary": "We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved\nCompression, a simple yet effective RL approach for Large Reasoning Models\n(LRMs) that enables more efficient and accurate reasoning. Existing studies\nhave observed repetitive thinking patterns in LRMs, and attempts to reduce them\noften come at the cost of performance. In this paper, we show that this\ntrade-off can be overcome through a training regime that iteratively alternates\nbetween compressing and expanding the reasoning budget, by dynamically\nadjusting the maximum rollout length during training. The compression phase\ncuts the rollout length, forcing the model to make precise and valuable\ndecisions within a limited context, which effectively reduces redundant tokens\nand increases reasoning density. The expansion phase then relaxes the length\nlimit, providing space for the model to explore and plan in long-horizon\nsettings. Remarkably, we find that after each compression-expansion cycle, the\nmodel's performance improves even as its output length decreases, steadily\npushing it closer to the Pareto frontier in the performance-efficiency\ntrade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves\nperformance on AIME24 by 43.2% while reducing token usage by 46.9% after three\niterations, and SIRI-high achieves the highest accuracy compared to all other\nmethods (Figure 1). Our findings shed light on the potential of periodically\noscillating the LRM's output truncation length during training to dynamically\nbalance exploration and efficiency in reasoning, converging towards an optimal\n\"sweet spot\" between the two. Our models are publicly available.",
            "upvotes": 11,
            "discussionId": "68db421ad2bf1f4b15ec7301",
            "projectPage": "https://huggingface.co/collections/THU-KEG/siri-68d65a4ecf9f20dac7322dfe",
            "ai_summary": "SIRI, a reinforcement learning approach with interleaved compression and expansion, enhances the efficiency and accuracy of large reasoning models by dynamically adjusting the reasoning budget.",
            "ai_keywords": [
                "Scaling Iterative Reinforcement Learning",
                "Interleaved Compression",
                "Large Reasoning Models",
                "RL",
                "rollout length",
                "reasoning density",
                "DeepSeek-R1-Distill-Qwen-1.5B",
                "AIME24",
                "Pareto frontier",
                "performance-efficiency trade-off"
            ]
        },
        "publishedAt": "2025-09-29T13:59:08.000Z",
        "title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved\n  Compression",
        "summary": "We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved\nCompression, a simple yet effective RL approach for Large Reasoning Models\n(LRMs) that enables more efficient and accurate reasoning. Existing studies\nhave observed repetitive thinking patterns in LRMs, and attempts to reduce them\noften come at the cost of performance. In this paper, we show that this\ntrade-off can be overcome through a training regime that iteratively alternates\nbetween compressing and expanding the reasoning budget, by dynamically\nadjusting the maximum rollout length during training. The compression phase\ncuts the rollout length, forcing the model to make precise and valuable\ndecisions within a limited context, which effectively reduces redundant tokens\nand increases reasoning density. The expansion phase then relaxes the length\nlimit, providing space for the model to explore and plan in long-horizon\nsettings. Remarkably, we find that after each compression-expansion cycle, the\nmodel's performance improves even as its output length decreases, steadily\npushing it closer to the Pareto frontier in the performance-efficiency\ntrade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves\nperformance on AIME24 by 43.2% while reducing token usage by 46.9% after three\niterations, and SIRI-high achieves the highest accuracy compared to all other\nmethods (Figure 1). Our findings shed light on the potential of periodically\noscillating the LRM's output truncation length during training to dynamically\nbalance exploration and efficiency in reasoning, converging towards an optimal\n\"sweet spot\" between the two. Our models are publicly available.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25176.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ed568ccf6118a9379a61b8",
            "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
            "fullname": "Yushi Bai",
            "name": "bys0318",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "submitterOrganization": {
            "_id": "62ad27f19096e7f9ecb1853a",
            "name": "zai-org",
            "fullname": "Z.ai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25137",
            "authors": [
                {
                    "_id": "68dbd78dd2bf1f4b15ec7af3",
                    "user": {
                        "_id": "64beb69801f1983a86a05de2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64beb69801f1983a86a05de2/tFyCoqZ6gT8NWkZfuncID.jpeg",
                        "isPro": false,
                        "fullname": "Chuanyang Jin",
                        "user": "Chuanyang-Jin",
                        "type": "user"
                    },
                    "name": "Chuanyang Jin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T13:53:01.085Z",
                    "hidden": false
                },
                {
                    "_id": "68dbd78dd2bf1f4b15ec7af4",
                    "name": "Jing Xu",
                    "hidden": false
                },
                {
                    "_id": "68dbd78dd2bf1f4b15ec7af5",
                    "name": "Bo Liu",
                    "hidden": false
                },
                {
                    "_id": "68dbd78dd2bf1f4b15ec7af6",
                    "name": "Leitian Tao",
                    "hidden": false
                },
                {
                    "_id": "68dbd78dd2bf1f4b15ec7af7",
                    "name": "Olga Golovneva",
                    "hidden": false
                },
                {
                    "_id": "68dbd78dd2bf1f4b15ec7af8",
                    "name": "Tianmin Shu",
                    "hidden": false
                },
                {
                    "_id": "68dbd78dd2bf1f4b15ec7af9",
                    "name": "Wenting Zhao",
                    "hidden": false
                },
                {
                    "_id": "68dbd78dd2bf1f4b15ec7afa",
                    "name": "Xian Li",
                    "hidden": false
                },
                {
                    "_id": "68dbd78dd2bf1f4b15ec7afb",
                    "name": "Jason Weston",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:50:31.000Z",
            "submittedOnDailyAt": "2025-09-30T12:31:08.211Z",
            "title": "The Era of Real-World Human Interaction: RL from User Conversations",
            "submittedOnDailyBy": {
                "_id": "64beb69801f1983a86a05de2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64beb69801f1983a86a05de2/tFyCoqZ6gT8NWkZfuncID.jpeg",
                "isPro": false,
                "fullname": "Chuanyang Jin",
                "user": "Chuanyang-Jin",
                "type": "user"
            },
            "summary": "We posit that to achieve continual model improvement and multifaceted\nalignment, future models must learn from natural human interaction. Current\nconversational models are aligned using pre-annotated, expert-generated human\nfeedback. In this work, we introduce Reinforcement Learning from Human\nInteraction (RLHI), a paradigm that learns directly from in-the-wild user\nconversations. We develop two complementary methods: (1) RLHI with User-Guided\nRewrites, which revises unsatisfactory model outputs based on users'\nnatural-language follow-up responses, (2) RLHI with User-Based Rewards, which\nlearns via a reward model conditioned on knowledge of the user's long-term\ninteraction history (termed persona). Together, these methods link long-term\nuser personas to turn-level preferences via persona-conditioned preference\noptimization. Trained on conversations derived from WildChat, both RLHI\nvariants outperform strong baselines in personalization and\ninstruction-following, and similar feedback enhances performance on reasoning\nbenchmarks. These results suggest organic human interaction offers scalable,\neffective supervision for personalized alignment.",
            "upvotes": 11,
            "discussionId": "68dbd78dd2bf1f4b15ec7afc",
            "ai_summary": "Reinforcement Learning from Human Interaction (RLHI) uses in-the-wild user conversations to improve conversational models, enhancing personalization and instruction-following through user-guided rewrites and persona-conditioned rewards.",
            "ai_keywords": [
                "Reinforcement Learning from Human Interaction (RLHI)",
                "User-Guided Rewrites",
                "User-Based Rewards",
                "persona-conditioned preference optimization",
                "WildChat"
            ]
        },
        "publishedAt": "2025-09-29T13:50:31.000Z",
        "title": "The Era of Real-World Human Interaction: RL from User Conversations",
        "summary": "We posit that to achieve continual model improvement and multifaceted\nalignment, future models must learn from natural human interaction. Current\nconversational models are aligned using pre-annotated, expert-generated human\nfeedback. In this work, we introduce Reinforcement Learning from Human\nInteraction (RLHI), a paradigm that learns directly from in-the-wild user\nconversations. We develop two complementary methods: (1) RLHI with User-Guided\nRewrites, which revises unsatisfactory model outputs based on users'\nnatural-language follow-up responses, (2) RLHI with User-Based Rewards, which\nlearns via a reward model conditioned on knowledge of the user's long-term\ninteraction history (termed persona). Together, these methods link long-term\nuser personas to turn-level preferences via persona-conditioned preference\noptimization. Trained on conversations derived from WildChat, both RLHI\nvariants outperform strong baselines in personalization and\ninstruction-following, and similar feedback enhances performance on reasoning\nbenchmarks. These results suggest organic human interaction offers scalable,\neffective supervision for personalized alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25137.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64beb69801f1983a86a05de2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64beb69801f1983a86a05de2/tFyCoqZ6gT8NWkZfuncID.jpeg",
            "fullname": "Chuanyang Jin",
            "name": "Chuanyang-Jin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "submitterOrganization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25131",
            "authors": [
                {
                    "_id": "68db442ad2bf1f4b15ec7368",
                    "user": {
                        "_id": "6423e35b30b0e4ab36dd1b16",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6423e35b30b0e4ab36dd1b16/pea6LVDS9PQAxvQt9GUiZ.jpeg",
                        "isPro": false,
                        "fullname": "Wang Chengyao",
                        "user": "wcy1122",
                        "type": "user"
                    },
                    "name": "Chengyao Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:30:36.950Z",
                    "hidden": false
                },
                {
                    "_id": "68db442ad2bf1f4b15ec7369",
                    "name": "Zhisheng Zhong",
                    "hidden": false
                },
                {
                    "_id": "68db442ad2bf1f4b15ec736a",
                    "name": "Bohao Peng",
                    "hidden": false
                },
                {
                    "_id": "68db442ad2bf1f4b15ec736b",
                    "name": "Senqiao Yang",
                    "hidden": false
                },
                {
                    "_id": "68db442ad2bf1f4b15ec736c",
                    "user": {
                        "_id": "669cefd6119595d21b55a995",
                        "avatarUrl": "/avatars/bafc2387ee70b263bf45c42159381da8.svg",
                        "isPro": false,
                        "fullname": "Yuqi Liu",
                        "user": "Ricky06662",
                        "type": "user"
                    },
                    "name": "Yuqi Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T11:42:57.855Z",
                    "hidden": false
                },
                {
                    "_id": "68db442ad2bf1f4b15ec736d",
                    "name": "Haokun Gui",
                    "hidden": false
                },
                {
                    "_id": "68db442ad2bf1f4b15ec736e",
                    "name": "Bin Xia",
                    "hidden": false
                },
                {
                    "_id": "68db442ad2bf1f4b15ec736f",
                    "name": "Jingyao Li",
                    "hidden": false
                },
                {
                    "_id": "68db442ad2bf1f4b15ec7370",
                    "name": "Bei Yu",
                    "hidden": false
                },
                {
                    "_id": "68db442ad2bf1f4b15ec7371",
                    "name": "Jiaya Jia",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:48:28.000Z",
            "submittedOnDailyAt": "2025-09-30T01:18:28.196Z",
            "title": "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech",
            "submittedOnDailyBy": {
                "_id": "6423e35b30b0e4ab36dd1b16",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6423e35b30b0e4ab36dd1b16/pea6LVDS9PQAxvQt9GUiZ.jpeg",
                "isPro": false,
                "fullname": "Wang Chengyao",
                "user": "wcy1122",
                "type": "user"
            },
            "summary": "We present MGM-Omni, a unified Omni LLM for omni-modal understanding and\nexpressive, long-horizon speech generation. Unlike cascaded pipelines that\nisolate speech synthesis, MGM-Omni adopts a \"brain-mouth\" design with a\ndual-track, token-based architecture that cleanly decouples multimodal\nreasoning from real-time speech generation. This design enables efficient\ncross-modal interaction and low-latency, streaming speech generation. For\nunderstanding, a unified training strategy coupled with a dual audio encoder\ndesign enables long-form audio perception across diverse acoustic conditions.\nFor generation, a chunk-based parallel decoding scheme narrows the text speech\ntoken-rate gap, accelerating inference and supporting streaming zero-shot voice\ncloning with stable timbre over extended durations. Compared to concurrent\nwork, MGM-Omni achieves these capabilities with markedly data-efficient\ntraining. Extensive experiments demonstrate that MGM-Omni outperforms existing\nopen source models in preserving timbre identity across extended sequences,\nproducing natural and context-aware speech, and achieving superior long-form\naudio and omnimodal understanding. MGM-Omni establishes an efficient,\nend-to-end paradigm for omnimodal understanding and controllable, personalised\nlong-horizon speech generation.",
            "upvotes": 11,
            "discussionId": "68db442bd2bf1f4b15ec7372",
            "projectPage": "https://huggingface.co/spaces/wcy1122/MGM-Omni",
            "githubRepo": "https://github.com/dvlab-research/MGM-Omni",
            "ai_summary": "MGM-Omni is a unified multimodal language model for speech generation and understanding, featuring a dual-track architecture for efficient cross-modal interaction and data-efficient training.",
            "ai_keywords": [
                "Omni LLM",
                "brain-mouth design",
                "dual-track architecture",
                "token-based architecture",
                "multimodal reasoning",
                "real-time speech generation",
                "unified training strategy",
                "dual audio encoder",
                "chunk-based parallel decoding",
                "text-speech token-rate gap",
                "streaming zero-shot voice cloning",
                "timbre identity",
                "natural speech",
                "context-aware speech",
                "long-form audio",
                "omnimodal understanding",
                "end-to-end paradigm",
                "controllable speech generation"
            ],
            "githubStars": 132
        },
        "publishedAt": "2025-09-29T13:48:28.000Z",
        "title": "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech",
        "summary": "We present MGM-Omni, a unified Omni LLM for omni-modal understanding and\nexpressive, long-horizon speech generation. Unlike cascaded pipelines that\nisolate speech synthesis, MGM-Omni adopts a \"brain-mouth\" design with a\ndual-track, token-based architecture that cleanly decouples multimodal\nreasoning from real-time speech generation. This design enables efficient\ncross-modal interaction and low-latency, streaming speech generation. For\nunderstanding, a unified training strategy coupled with a dual audio encoder\ndesign enables long-form audio perception across diverse acoustic conditions.\nFor generation, a chunk-based parallel decoding scheme narrows the text speech\ntoken-rate gap, accelerating inference and supporting streaming zero-shot voice\ncloning with stable timbre over extended durations. Compared to concurrent\nwork, MGM-Omni achieves these capabilities with markedly data-efficient\ntraining. Extensive experiments demonstrate that MGM-Omni outperforms existing\nopen source models in preserving timbre identity across extended sequences,\nproducing natural and context-aware speech, and achieving superior long-form\naudio and omnimodal understanding. MGM-Omni establishes an efficient,\nend-to-end paradigm for omnimodal understanding and controllable, personalised\nlong-horizon speech generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25131.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6423e35b30b0e4ab36dd1b16",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6423e35b30b0e4ab36dd1b16/pea6LVDS9PQAxvQt9GUiZ.jpeg",
            "fullname": "Wang Chengyao",
            "name": "wcy1122",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 23
        },
        "submitterOrganization": {
            "_id": "6390c6fdd00f25601f445cd4",
            "name": "CUHK-CSE",
            "fullname": "The Chinese University of Hong Kong",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/621f2eb36e152b56a7cf0248/o8RRAczRjfNEzq70GzUwQ.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25084",
            "authors": [
                {
                    "_id": "68db796dd2bf1f4b15ec773d",
                    "name": "Shuofei Qiao",
                    "hidden": false
                },
                {
                    "_id": "68db796dd2bf1f4b15ec773e",
                    "name": "Yanqiu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68db796dd2bf1f4b15ec773f",
                    "name": "Zhisong Qiu",
                    "hidden": false
                },
                {
                    "_id": "68db796dd2bf1f4b15ec7740",
                    "name": "Xiaobin Wang",
                    "hidden": false
                },
                {
                    "_id": "68db796dd2bf1f4b15ec7741",
                    "name": "Jintian Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db796dd2bf1f4b15ec7742",
                    "name": "Zhao Bin",
                    "hidden": false
                },
                {
                    "_id": "68db796dd2bf1f4b15ec7743",
                    "user": {
                        "_id": "620b3bbb0668e435407c8d0a",
                        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                        "isPro": true,
                        "fullname": "Ningyu Zhang",
                        "user": "Ningyu",
                        "type": "user"
                    },
                    "name": "Ningyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T07:12:02.808Z",
                    "hidden": false
                },
                {
                    "_id": "68db796dd2bf1f4b15ec7744",
                    "name": "Yong Jiang",
                    "hidden": false
                },
                {
                    "_id": "68db796dd2bf1f4b15ec7745",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "68db796dd2bf1f4b15ec7746",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "68db796dd2bf1f4b15ec7747",
                    "name": "Huajun Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:23:08.000Z",
            "submittedOnDailyAt": "2025-09-30T05:02:40.602Z",
            "title": "Scaling Generalist Data-Analytic Agents",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": true,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "Data-analytic agents are emerging as a key catalyst for automated scientific\ndiscovery and for the vision of Innovating AI. Current approaches, however,\nrely heavily on prompt engineering over proprietary models, while open-source\nmodels struggle to face diverse-format, large-scale data files and\nlong-horizon, multi-step reasoning that real-world analytics demands. This\npaper introduces DataMind, a scalable data synthesis and agent training recipe\ndesigned to build generalist data-analytic agents. DataMind tackles three key\nchallenges in building open-source data-analytic agents, including insufficient\ndata resources, improper training strategy, and unstable code-based multi-turn\nrollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a\nrecursive easy-to-hard task composition mechanism to increase the diversity and\ndifficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling\nstrategy followed by model-based and rule-based filtering; 3) a dynamically\nadjustable training objective combining both SFT and RL losses; 4) a\nmemory-frugal and stable code-based multi-turn rollout framework. Built on\nDataMind, we curate DataMind-12K, a high-quality trajectory set spanning\ndiverse domains, task categories, and data file formats for data-analytic\ntasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with\nan average score of 71.16% on multiple data analysis benchmarks, outperforming\nthe strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B\nalso performs best among all open-source models with a score of 68.10%. We also\nincorporate some empirical insights gained from our exploratory trials into the\nanalysis experiments, aiming to provide actionable insights about agentic\ntraining for the community. We will release DataMind-12K and DataMind-7B,14B\nfor the community's future research.",
            "upvotes": 11,
            "discussionId": "68db796dd2bf1f4b15ec7748",
            "ai_summary": "DataMind addresses challenges in building open-source data-analytic agents through task taxonomy, trajectory sampling, dynamic training objectives, and stable multi-turn rollouts, achieving state-of-the-art performance on data analysis benchmarks.",
            "ai_keywords": [
                "fine-grained task taxonomy",
                "recursive easy-to-hard task composition",
                "knowledge-augmented trajectory sampling",
                "model-based filtering",
                "rule-based filtering",
                "dynamically adjustable training objective",
                "SFT",
                "RL losses",
                "memory-frugal",
                "code-based multi-turn rollout",
                "DataMind-12K",
                "DataMind-14B",
                "DataMind-7B"
            ]
        },
        "publishedAt": "2025-09-29T13:23:08.000Z",
        "title": "Scaling Generalist Data-Analytic Agents",
        "summary": "Data-analytic agents are emerging as a key catalyst for automated scientific\ndiscovery and for the vision of Innovating AI. Current approaches, however,\nrely heavily on prompt engineering over proprietary models, while open-source\nmodels struggle to face diverse-format, large-scale data files and\nlong-horizon, multi-step reasoning that real-world analytics demands. This\npaper introduces DataMind, a scalable data synthesis and agent training recipe\ndesigned to build generalist data-analytic agents. DataMind tackles three key\nchallenges in building open-source data-analytic agents, including insufficient\ndata resources, improper training strategy, and unstable code-based multi-turn\nrollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a\nrecursive easy-to-hard task composition mechanism to increase the diversity and\ndifficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling\nstrategy followed by model-based and rule-based filtering; 3) a dynamically\nadjustable training objective combining both SFT and RL losses; 4) a\nmemory-frugal and stable code-based multi-turn rollout framework. Built on\nDataMind, we curate DataMind-12K, a high-quality trajectory set spanning\ndiverse domains, task categories, and data file formats for data-analytic\ntasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with\nan average score of 71.16% on multiple data analysis benchmarks, outperforming\nthe strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B\nalso performs best among all open-source models with a score of 68.10%. We also\nincorporate some empirical insights gained from our exploratory trials into the\nanalysis experiments, aiming to provide actionable insights about agentic\ntraining for the community. We will release DataMind-12K and DataMind-7B,14B\nfor the community's future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25084.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 30
        },
        "submitterOrganization": {
            "_id": "64c8b5837fe12ecd0a7e92eb",
            "name": "Qwen",
            "fullname": "Qwen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.23951",
            "authors": [
                {
                    "_id": "68db4706d2bf1f4b15ec73a0",
                    "user": {
                        "_id": "61279e8feb77492e36e520bb",
                        "avatarUrl": "/avatars/01171097ff7c7488bc2447c2485da8e2.svg",
                        "isPro": false,
                        "fullname": "Siyu Cao",
                        "user": "scao",
                        "type": "user"
                    },
                    "name": "Siyu Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-30T12:13:38.178Z",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73a1",
                    "user": {
                        "_id": "631b2dbd0f22997ce6728e27",
                        "avatarUrl": "/avatars/84dc35edbda6c071fcecec01918c5c56.svg",
                        "isPro": false,
                        "fullname": "Hangting Chen",
                        "user": "hangtingchen",
                        "type": "user"
                    },
                    "name": "Hangting Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-30T12:13:46.654Z",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73a2",
                    "user": {
                        "_id": "60799b15921db717010c7c8e",
                        "avatarUrl": "/avatars/5e33095aa5343538f09831c1ed2230d2.svg",
                        "isPro": false,
                        "fullname": "Peng Chen",
                        "user": "pengchen",
                        "type": "user"
                    },
                    "name": "Peng Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-30T12:13:56.096Z",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73a3",
                    "user": {
                        "_id": "635e6b5d398ff343c4f4ed11",
                        "avatarUrl": "/avatars/3b7809b12a8980cf948be51831c05317.svg",
                        "isPro": false,
                        "fullname": "yijicheng",
                        "user": "yiji",
                        "type": "user"
                    },
                    "name": "Yiji Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-30T12:14:10.568Z",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73a4",
                    "name": "Yutao Cui",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73a5",
                    "name": "Xinchi Deng",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73a6",
                    "name": "Ying Dong",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73a7",
                    "name": "Kipper Gong",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73a8",
                    "name": "Tianpeng Gu",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73a9",
                    "name": "Xiusen Gu",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73aa",
                    "name": "Tiankai Hang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73ab",
                    "name": "Duojun Huang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73ac",
                    "name": "Jie Jiang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73ad",
                    "user": {
                        "_id": "67593dd0f522f4409e614ba0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67593dd0f522f4409e614ba0/cvb9w_8seu3Kbjg_XAnNj.jpeg",
                        "isPro": false,
                        "fullname": "Jiang Zhengkai",
                        "user": "jzzzzk",
                        "type": "user"
                    },
                    "name": "Zhengkai Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-30T12:15:24.753Z",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73ae",
                    "user": {
                        "_id": "67f7ab7ef241f0b92ce4b087",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vDhBPKqfWjMI65K-kL2tm.jpeg",
                        "isPro": false,
                        "fullname": "Weijie Kong",
                        "user": "Knightbreeze",
                        "type": "user"
                    },
                    "name": "Weijie Kong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-30T12:15:36.040Z",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73af",
                    "user": {
                        "_id": "634bc044fe1bfa967d67d5dc",
                        "avatarUrl": "/avatars/590ed3229d95cdcaa05a96028c586165.svg",
                        "isPro": false,
                        "fullname": "Changlin Li",
                        "user": "changlinli",
                        "type": "user"
                    },
                    "name": "Changlin Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-30T12:15:47.754Z",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73b0",
                    "user": {
                        "_id": "66f33e713c412b900bac6f2e",
                        "avatarUrl": "/avatars/9496d54a5a394d0082cb80b505fa26ec.svg",
                        "isPro": false,
                        "fullname": "Donghao Li",
                        "user": "panda12345pa",
                        "type": "user"
                    },
                    "name": "Donghao Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-30T12:15:58.822Z",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73b1",
                    "user": {
                        "_id": "68dbb339cd627292c5a44d5b",
                        "avatarUrl": "/avatars/f4c360bb85f66bb2243789c43b10ccb5.svg",
                        "isPro": false,
                        "fullname": "Junzhe Li",
                        "user": "GingerLi",
                        "type": "user"
                    },
                    "name": "Junzhe Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-30T12:16:08.530Z",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73b2",
                    "name": "Xin Li",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73b3",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73b4",
                    "name": "Zhenxi Li",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73b5",
                    "name": "Zhimin Li",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73b6",
                    "name": "Jiaxin Lin",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73b7",
                    "name": "Linus",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73b8",
                    "name": "Lucaz Liu",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73b9",
                    "name": "Shu Liu",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73ba",
                    "name": "Songtao Liu",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73bb",
                    "name": "Yu Liu",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73bc",
                    "name": "Yuhong Liu",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73bd",
                    "name": "Yanxin Long",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73be",
                    "name": "Fanbin Lu",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73bf",
                    "name": "Qinglin Lu",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73c0",
                    "name": "Yuyang Peng",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73c1",
                    "name": "Yuanbo Peng",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73c2",
                    "name": "Xiangwei Shen",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73c3",
                    "name": "Yixuan Shi",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73c4",
                    "name": "Jiale Tao",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73c5",
                    "name": "Yangyu Tao",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73c6",
                    "name": "Qi Tian",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73c7",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73c8",
                    "name": "Chunyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73c9",
                    "name": "Kai Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73ca",
                    "name": "Lei Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73cb",
                    "name": "Linqing Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73cc",
                    "name": "Lucas Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73cd",
                    "name": "Qixun Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73ce",
                    "name": "Weiyan Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73cf",
                    "name": "Hao Wen",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73d0",
                    "name": "Bing Wu",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73d1",
                    "name": "Jianbing Wu",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73d2",
                    "name": "Yue Wu",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73d3",
                    "name": "Senhao Xie",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73d4",
                    "name": "Fang Yang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73d5",
                    "name": "Miles Yang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73d6",
                    "name": "Xiaofeng Yang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73d7",
                    "name": "Xuan Yang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73d8",
                    "name": "Zhantao Yang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73d9",
                    "name": "Jingmiao Yu",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73da",
                    "name": "Zheng Yuan",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73db",
                    "name": "Chao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73dc",
                    "name": "Jian-Wei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73dd",
                    "name": "Peizhen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73de",
                    "name": "Shi-Xue Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73df",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73e0",
                    "name": "Weigang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73e1",
                    "name": "Yepeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73e2",
                    "name": "Yingfang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73e3",
                    "name": "Zihao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73e4",
                    "name": "Zijian Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73e5",
                    "name": "Penghao Zhao",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73e6",
                    "name": "Zhiyuan Zhao",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73e7",
                    "name": "Xuefei Zhe",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73e8",
                    "name": "Jianchen Zhu",
                    "hidden": false
                },
                {
                    "_id": "68db4706d2bf1f4b15ec73e9",
                    "name": "Zhao Zhong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T16:14:10.000Z",
            "submittedOnDailyAt": "2025-09-30T02:20:41.661Z",
            "title": "HunyuanImage 3.0 Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present HunyuanImage 3.0, a native multimodal model that unifies\nmultimodal understanding and generation within an autoregressive framework,\nwith its image generation module publicly available. The achievement of\nHunyuanImage 3.0 relies on several key components, including meticulous data\ncuration, advanced architecture design, a native Chain-of-Thoughts schema,\nprogressive model pre-training, aggressive model post-training, and an\nefficient infrastructure that enables large-scale training and inference. With\nthese advancements, we successfully trained a Mixture-of-Experts (MoE) model\ncomprising over 80 billion parameters in total, with 13 billion parameters\nactivated per token during inference, making it the largest and most powerful\nopen-source image generative model to date. We conducted extensive experiments\nand the results of automatic and human evaluation of text-image alignment and\nvisual quality demonstrate that HunyuanImage 3.0 rivals previous\nstate-of-the-art models. By releasing the code and weights of HunyuanImage 3.0,\nwe aim to enable the community to explore new ideas with a state-of-the-art\nfoundation model, fostering a dynamic and vibrant multimodal ecosystem. All\nopen source assets are publicly available at\nhttps://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
            "upvotes": 11,
            "discussionId": "68db4706d2bf1f4b15ec73ea",
            "githubRepo": "https://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
            "ai_summary": "HunyuanImage 3.0, a multimodal model with an autoregressive framework, achieves state-of-the-art performance in image generation and text-image alignment using a Mixture-of-Experts architecture with over 80 billion parameters.",
            "ai_keywords": [
                "multimodal model",
                "autoregressive framework",
                "Chain-of-Thoughts schema",
                "progressive model pre-training",
                "aggressive model post-training",
                "Mixture-of-Experts (MoE)",
                "text-image alignment",
                "visual quality"
            ],
            "githubStars": 1420
        },
        "publishedAt": "2025-09-28T12:14:10.000Z",
        "title": "HunyuanImage 3.0 Technical Report",
        "summary": "We present HunyuanImage 3.0, a native multimodal model that unifies\nmultimodal understanding and generation within an autoregressive framework,\nwith its image generation module publicly available. The achievement of\nHunyuanImage 3.0 relies on several key components, including meticulous data\ncuration, advanced architecture design, a native Chain-of-Thoughts schema,\nprogressive model pre-training, aggressive model post-training, and an\nefficient infrastructure that enables large-scale training and inference. With\nthese advancements, we successfully trained a Mixture-of-Experts (MoE) model\ncomprising over 80 billion parameters in total, with 13 billion parameters\nactivated per token during inference, making it the largest and most powerful\nopen-source image generative model to date. We conducted extensive experiments\nand the results of automatic and human evaluation of text-image alignment and\nvisual quality demonstrate that HunyuanImage 3.0 rivals previous\nstate-of-the-art models. By releasing the code and weights of HunyuanImage 3.0,\nwe aim to enable the community to explore new ideas with a state-of-the-art\nfoundation model, fostering a dynamic and vibrant multimodal ecosystem. All\nopen source assets are publicly available at\nhttps://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23951.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 115
        },
        "submitterOrganization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22572",
            "authors": [
                {
                    "_id": "68db4efed2bf1f4b15ec7497",
                    "user": {
                        "_id": "673d4716cc1ef74a349cd2ad",
                        "avatarUrl": "/avatars/a88f1d461c199a2caa1d5e13b70921fe.svg",
                        "isPro": false,
                        "fullname": "Yixuan Han",
                        "user": "yixuan7878",
                        "type": "user"
                    },
                    "name": "Yixuan Han",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T07:12:25.187Z",
                    "hidden": false
                },
                {
                    "_id": "68db4efed2bf1f4b15ec7498",
                    "name": "Fan Ma",
                    "hidden": false
                },
                {
                    "_id": "68db4efed2bf1f4b15ec7499",
                    "name": "Ruijie Quan",
                    "hidden": false
                },
                {
                    "_id": "68db4efed2bf1f4b15ec749a",
                    "name": "Yi Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T16:49:10.000Z",
            "submittedOnDailyAt": "2025-09-30T02:02:06.748Z",
            "title": "Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs\n  at Test Time",
            "submittedOnDailyBy": {
                "_id": "65eaa1e2b11eeb516a973508",
                "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
                "isPro": false,
                "fullname": "Dewei Zhou",
                "user": "limuloo1999",
                "type": "user"
            },
            "summary": "Test-Time Scaling (TTS) enhances the reasoning ability of large language\nmodels (LLMs) by allocating additional computation during inference. However,\nexisting approaches primarily rely on output-level sampling while overlooking\nthe role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we\nobserve that varying the number of activated experts yields complementary\nsolution sets with stable accuracy, revealing a new and underexplored source of\ndiversity. Motivated by this observation, we propose Dynamic Experts Search\n(DES), a TTS strategy that elevates expert activation into a controllable\ndimension of the search space. DES integrates two key components: (1) Dynamic\nMoE, which enables direct control of expert counts during inference to generate\ndiverse reasoning trajectories without additional cost; and (2) Expert\nConfiguration Inheritance, which preserves consistent expert counts within a\nreasoning path while varying them across runs, thereby balancing stability and\ndiversity throughout the search. Extensive experiments across MoE\narchitectures, verifiers and reasoning benchmarks (i.e., math, code and\nknowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing\naccuracy and stability without additional cost. These results highlight DES as\na practical and scalable form of architecture-aware TTS, illustrating how\nstructural flexibility in modern LLMs can advance reasoning.",
            "upvotes": 11,
            "discussionId": "68db4efed2bf1f4b15ec749b",
            "ai_summary": "Dynamic Experts Search (DES) enhances large language models by controlling expert activation during inference, improving accuracy and stability without additional cost.",
            "ai_keywords": [
                "Mixture-of-Experts (MoE)",
                "Dynamic Experts Search (DES)",
                "Dynamic MoE",
                "Expert Configuration Inheritance",
                "reasoning trajectories",
                "verifiers",
                "reasoning benchmarks",
                "math",
                "code",
                "knowledge"
            ]
        },
        "publishedAt": "2025-09-26T12:49:10.000Z",
        "title": "Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs\n  at Test Time",
        "summary": "Test-Time Scaling (TTS) enhances the reasoning ability of large language\nmodels (LLMs) by allocating additional computation during inference. However,\nexisting approaches primarily rely on output-level sampling while overlooking\nthe role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we\nobserve that varying the number of activated experts yields complementary\nsolution sets with stable accuracy, revealing a new and underexplored source of\ndiversity. Motivated by this observation, we propose Dynamic Experts Search\n(DES), a TTS strategy that elevates expert activation into a controllable\ndimension of the search space. DES integrates two key components: (1) Dynamic\nMoE, which enables direct control of expert counts during inference to generate\ndiverse reasoning trajectories without additional cost; and (2) Expert\nConfiguration Inheritance, which preserves consistent expert counts within a\nreasoning path while varying them across runs, thereby balancing stability and\ndiversity throughout the search. Extensive experiments across MoE\narchitectures, verifiers and reasoning benchmarks (i.e., math, code and\nknowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing\naccuracy and stability without additional cost. These results highlight DES as\na practical and scalable form of architecture-aware TTS, illustrating how\nstructural flexibility in modern LLMs can advance reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22572.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65eaa1e2b11eeb516a973508",
            "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
            "fullname": "Dewei Zhou",
            "name": "limuloo1999",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.23219",
            "authors": [
                {
                    "_id": "68db8c98d2bf1f4b15ec7860",
                    "user": {
                        "_id": "64a948d9723beceb2f13f5eb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a948d9723beceb2f13f5eb/UQMO7LczJpabtbcoNt1zQ.jpeg",
                        "isPro": false,
                        "fullname": "XinLi",
                        "user": "XINLI1997",
                        "type": "user"
                    },
                    "name": "Xin Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T10:17:55.697Z",
                    "hidden": false
                },
                {
                    "_id": "68db8c98d2bf1f4b15ec7861",
                    "name": "Mengbing Liu",
                    "hidden": false
                },
                {
                    "_id": "68db8c98d2bf1f4b15ec7862",
                    "name": "Yiyang Zhu",
                    "hidden": false
                },
                {
                    "_id": "68db8c98d2bf1f4b15ec7863",
                    "name": "Wenhe Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db8c98d2bf1f4b15ec7864",
                    "name": "Li Wei",
                    "hidden": false
                },
                {
                    "_id": "68db8c98d2bf1f4b15ec7865",
                    "name": "Jiancheng An",
                    "hidden": false
                },
                {
                    "_id": "68db8c98d2bf1f4b15ec7866",
                    "name": "Chau Yuen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-27T09:58:03.000Z",
            "submittedOnDailyAt": "2025-09-30T06:26:05.655Z",
            "title": "WirelessMathLM: Teaching Mathematical Reasoning for LLMs in Wireless\n  Communications with Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "64a948d9723beceb2f13f5eb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a948d9723beceb2f13f5eb/UQMO7LczJpabtbcoNt1zQ.jpeg",
                "isPro": false,
                "fullname": "XinLi",
                "user": "XINLI1997",
                "type": "user"
            },
            "summary": "Large language models (LLMs) excel at general mathematical reasoning but fail\ncatastrophically on specialized technical mathematics. In wireless\ncommunications, where problems require precise manipulation of\ninformation-theoretic bounds, optimization constraints, and signal processing\nformulations, even state-of-the-art models struggle to achieve competent\nperformance. We present WirelessMathLM, demonstrating that compact models\n(0.5B-7B parameters) can match or exceed much larger models through\ndomain-specific reinforcement learning with verifiable rewards. Our key insight\nis that wireless mathematics problems possess a unique property--verifiable\ncorrectness--that enables effective reinforcement learning without human\nfeedback. We construct WirelessMathBench-XL, a comprehensive benchmark of 4,027\nproblems from 970 papers. Using Group Relative Policy Optimization (GRPO) with\nbinary verification rewards, we train models directly from base checkpoints\nwithout supervised warm-start. Our 7B model achieves 39.5% accuracy on\nWirelessMathBench-XL, approaching GPT-4o (40.4%) while using about 100 times\nfewer parameters than DeepSeek-R1 (671B, 57.4%). Remarkably, GRPO training\nnearly doubles performance across all model scales (0.5B +11%, 3B +103%, 7B\n+81%), with positive transfer to general mathematics benchmarks--our models\ngain +8.4 points on average across MATH, Minerva-Math, OlympiadBench, AMC, and\nAIME without any training on these tasks.",
            "upvotes": 10,
            "discussionId": "68db8c98d2bf1f4b15ec7867",
            "projectPage": "https://lixin.ai/WirelessMathLM",
            "ai_summary": "WirelessMathLM, a compact model trained with domain-specific reinforcement learning, achieves high accuracy on wireless mathematics problems and transfers well to general mathematics benchmarks.",
            "ai_keywords": [
                "Large language models",
                "wireless communications",
                "information-theoretic bounds",
                "optimization constraints",
                "signal processing",
                "domain-specific reinforcement learning",
                "verifiable rewards",
                "WirelessMathLM",
                "WirelessMathBench-XL",
                "Group Relative Policy Optimization",
                "GRPO",
                "binary verification rewards",
                "DeepSeek-R1",
                "MATH",
                "Minerva-Math",
                "OlympiadBench",
                "AMC",
                "AIME"
            ]
        },
        "publishedAt": "2025-09-27T05:58:03.000Z",
        "title": "WirelessMathLM: Teaching Mathematical Reasoning for LLMs in Wireless\n  Communications with Reinforcement Learning",
        "summary": "Large language models (LLMs) excel at general mathematical reasoning but fail\ncatastrophically on specialized technical mathematics. In wireless\ncommunications, where problems require precise manipulation of\ninformation-theoretic bounds, optimization constraints, and signal processing\nformulations, even state-of-the-art models struggle to achieve competent\nperformance. We present WirelessMathLM, demonstrating that compact models\n(0.5B-7B parameters) can match or exceed much larger models through\ndomain-specific reinforcement learning with verifiable rewards. Our key insight\nis that wireless mathematics problems possess a unique property--verifiable\ncorrectness--that enables effective reinforcement learning without human\nfeedback. We construct WirelessMathBench-XL, a comprehensive benchmark of 4,027\nproblems from 970 papers. Using Group Relative Policy Optimization (GRPO) with\nbinary verification rewards, we train models directly from base checkpoints\nwithout supervised warm-start. Our 7B model achieves 39.5% accuracy on\nWirelessMathBench-XL, approaching GPT-4o (40.4%) while using about 100 times\nfewer parameters than DeepSeek-R1 (671B, 57.4%). Remarkably, GRPO training\nnearly doubles performance across all model scales (0.5B +11%, 3B +103%, 7B\n+81%), with positive transfer to general mathematics benchmarks--our models\ngain +8.4 points on average across MATH, Minerva-Math, OlympiadBench, AMC, and\nAIME without any training on these tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23219.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a948d9723beceb2f13f5eb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a948d9723beceb2f13f5eb/UQMO7LczJpabtbcoNt1zQ.jpeg",
            "fullname": "XinLi",
            "name": "XINLI1997",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.23050",
            "authors": [
                {
                    "_id": "68dc155f4159d1f2418f98d2",
                    "name": "Lin Long",
                    "hidden": false
                },
                {
                    "_id": "68dc155f4159d1f2418f98d3",
                    "name": "Changdae Oh",
                    "hidden": false
                },
                {
                    "_id": "68dc155f4159d1f2418f98d4",
                    "name": "Seongheon Park",
                    "hidden": false
                },
                {
                    "_id": "68dc155f4159d1f2418f98d5",
                    "name": "Yixuan Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-27T02:12:05.000Z",
            "submittedOnDailyAt": "2025-09-30T16:12:48.853Z",
            "title": "Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding",
            "submittedOnDailyBy": {
                "_id": "672fc8ede7c89e44c9757259",
                "avatarUrl": "/avatars/caa0d0de519ea96992c81328c89b3843.svg",
                "isPro": false,
                "fullname": "Changdae Oh",
                "user": "changdae",
                "type": "user"
            },
            "summary": "Large vision-language models (LVLMs) achieve strong performance on multimodal\ntasks, yet they often default to their language prior (LP) -- memorized textual\npatterns from pre-training while under-utilizing visual evidence. Prior\nanalyses of LP mostly rely on input-output probing, which fails to reveal the\ninternal mechanisms governing when and how vision influences model behavior. To\naddress this gap, we present the first systematic analysis of language prior\nthrough the lens of chain-of-embedding, which examines the layer-wise\nrepresentation dynamics within LVLMs. Our analysis reveals a universal\nphenomenon: each model exhibits a Visual Integration Point (VIP), a critical\nlayer at which visual information begins to meaningfully reshape hidden\nrepresentations and influence decoding. Building on this observation, we\nintroduce the Total Visual Integration (TVI) estimator, which aggregates\nrepresentation distance beyond the VIP to quantify how strongly visual query\ninfluences response generation. Across 54 model-dataset combinations spanning 9\ncontemporary LVLMs and 6 benchmarks, we demonstrate that VIP consistently\nemerges, and that TVI reliably predicts the strength of language prior. This\noffers a principled toolkit for diagnosing and understanding language prior in\nLVLMs.",
            "upvotes": 10,
            "discussionId": "68dc155f4159d1f2418f98d6",
            "githubRepo": "https://github.com/deeplearning-wisc/understanding_lp",
            "ai_summary": "A systematic analysis of language prior in large vision-language models reveals a Visual Integration Point and introduces a Total Visual Integration estimator to quantify visual influence on response generation.",
            "ai_keywords": [
                "Large vision-language models",
                "language prior",
                "chain-of-embedding",
                "layer-wise representation dynamics",
                "Visual Integration Point",
                "Total Visual Integration estimator"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-09-26T22:12:05.000Z",
        "title": "Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding",
        "summary": "Large vision-language models (LVLMs) achieve strong performance on multimodal\ntasks, yet they often default to their language prior (LP) -- memorized textual\npatterns from pre-training while under-utilizing visual evidence. Prior\nanalyses of LP mostly rely on input-output probing, which fails to reveal the\ninternal mechanisms governing when and how vision influences model behavior. To\naddress this gap, we present the first systematic analysis of language prior\nthrough the lens of chain-of-embedding, which examines the layer-wise\nrepresentation dynamics within LVLMs. Our analysis reveals a universal\nphenomenon: each model exhibits a Visual Integration Point (VIP), a critical\nlayer at which visual information begins to meaningfully reshape hidden\nrepresentations and influence decoding. Building on this observation, we\nintroduce the Total Visual Integration (TVI) estimator, which aggregates\nrepresentation distance beyond the VIP to quantify how strongly visual query\ninfluences response generation. Across 54 model-dataset combinations spanning 9\ncontemporary LVLMs and 6 benchmarks, we demonstrate that VIP consistently\nemerges, and that TVI reliably predicts the strength of language prior. This\noffers a principled toolkit for diagnosing and understanding language prior in\nLVLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23050.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "672fc8ede7c89e44c9757259",
            "avatarUrl": "/avatars/caa0d0de519ea96992c81328c89b3843.svg",
            "fullname": "Changdae Oh",
            "name": "changdae",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "submitterOrganization": {
            "_id": "648a44d392a80d8c960dba28",
            "name": "huggingface-wisc",
            "fullname": "University of Wisconsin-Madison",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/648a446f28e95179adbf590b/MukbqAulMN9HF4CMwR9gm.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22921",
            "authors": [
                {
                    "_id": "68dba231d2bf1f4b15ec7937",
                    "user": {
                        "_id": "66c5beb29fc647b08321cc20",
                        "avatarUrl": "/avatars/d84e497cbcd44e7e06449092033f5bbd.svg",
                        "isPro": false,
                        "fullname": "Matthieu Zimmer",
                        "user": "MatthieuZ",
                        "type": "user"
                    },
                    "name": "Matthieu Zimmer",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T11:42:52.564Z",
                    "hidden": false
                },
                {
                    "_id": "68dba231d2bf1f4b15ec7938",
                    "name": "Xiaotong Ji",
                    "hidden": false
                },
                {
                    "_id": "68dba231d2bf1f4b15ec7939",
                    "user": {
                        "_id": "64b50bea372d4340778befca",
                        "avatarUrl": "/avatars/b1fa3ca96ce9bd3419c9c87dd57b3b9d.svg",
                        "isPro": false,
                        "fullname": "Tu Nguyen",
                        "user": "tumeteor",
                        "type": "user"
                    },
                    "name": "Tu Nguyen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T11:42:50.275Z",
                    "hidden": false
                },
                {
                    "_id": "68dba231d2bf1f4b15ec793a",
                    "name": "Haitham Bou Ammar",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T20:47:49.000Z",
            "submittedOnDailyAt": "2025-09-30T08:59:43.615Z",
            "title": "Rethinking Large Language Model Distillation: A Constrained Markov\n  Decision Process Perspective",
            "submittedOnDailyBy": {
                "_id": "66c5beb29fc647b08321cc20",
                "avatarUrl": "/avatars/d84e497cbcd44e7e06449092033f5bbd.svg",
                "isPro": false,
                "fullname": "Matthieu Zimmer",
                "user": "MatthieuZ",
                "type": "user"
            },
            "summary": "We introduce a novel approach to large language model (LLM) distillation by\nformulating it as a constrained reinforcement learning problem. While recent\nwork has begun exploring the integration of task-specific rewards into\ndistillation processes, existing methods typically rely on ad-hoc reward\nweighting. We propose a principled optimization framework that maximizes\ntask-specific rewards while constraining the divergence from the teacher model\nto remain below a specified threshold. Our approach adapts constrained state\naugmented reinforcement learning to the distillation setting, introducing a\nmodified reward function that maintains theoretical guarantees of constraint\nsatisfaction without requiring state augmentation or teacher model access\nduring deployment and without the computational overhead of the dual Lagrangian\nmethods. Through extensive experiments on mathematical reasoning tasks, we\ndemonstrate that our method achieves better constraint satisfaction rates and\nbetter reasoning compared to the soft Lagrangian relaxation baselines while\nmaintaining competitive task performance. Our framework provides a\ntheoretically grounded and practically efficient solution for reward-aware\ndistillation in resource-constrained settings.",
            "upvotes": 9,
            "discussionId": "68dba231d2bf1f4b15ec793b",
            "ai_summary": "A novel constrained reinforcement learning framework for LLM distillation maximizes task-specific rewards while maintaining constraint satisfaction without state augmentation or dual Lagrangian methods.",
            "ai_keywords": [
                "large language model (LLM) distillation",
                "constrained reinforcement learning",
                "task-specific rewards",
                "reward weighting",
                "constrained state augmented reinforcement learning",
                "reward function",
                "constraint satisfaction",
                "soft Lagrangian relaxation",
                "mathematical reasoning tasks",
                "resource-constrained settings"
            ]
        },
        "publishedAt": "2025-09-26T16:47:49.000Z",
        "title": "Rethinking Large Language Model Distillation: A Constrained Markov\n  Decision Process Perspective",
        "summary": "We introduce a novel approach to large language model (LLM) distillation by\nformulating it as a constrained reinforcement learning problem. While recent\nwork has begun exploring the integration of task-specific rewards into\ndistillation processes, existing methods typically rely on ad-hoc reward\nweighting. We propose a principled optimization framework that maximizes\ntask-specific rewards while constraining the divergence from the teacher model\nto remain below a specified threshold. Our approach adapts constrained state\naugmented reinforcement learning to the distillation setting, introducing a\nmodified reward function that maintains theoretical guarantees of constraint\nsatisfaction without requiring state augmentation or teacher model access\nduring deployment and without the computational overhead of the dual Lagrangian\nmethods. Through extensive experiments on mathematical reasoning tasks, we\ndemonstrate that our method achieves better constraint satisfaction rates and\nbetter reasoning compared to the soft Lagrangian relaxation baselines while\nmaintaining competitive task performance. Our framework provides a\ntheoretically grounded and practically efficient solution for reward-aware\ndistillation in resource-constrained settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22921.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66c5beb29fc647b08321cc20",
            "avatarUrl": "/avatars/d84e497cbcd44e7e06449092033f5bbd.svg",
            "fullname": "Matthieu Zimmer",
            "name": "MatthieuZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "submitterOrganization": {
            "_id": "5f83c275f0801648bf88454a",
            "name": "huawei-noah",
            "fullname": "HUAWEI Noah's Ark Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.23196",
            "authors": [
                {
                    "_id": "68db481cd2bf1f4b15ec73ec",
                    "name": "Haonan Wang",
                    "hidden": false
                },
                {
                    "_id": "68db481cd2bf1f4b15ec73ed",
                    "name": "Weida Liang",
                    "hidden": false
                },
                {
                    "_id": "68db481cd2bf1f4b15ec73ee",
                    "name": "Zihang Fu",
                    "hidden": false
                },
                {
                    "_id": "68db481cd2bf1f4b15ec73ef",
                    "name": "Nie Zheng",
                    "hidden": false
                },
                {
                    "_id": "68db481cd2bf1f4b15ec73f0",
                    "name": "Yifan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db481cd2bf1f4b15ec73f1",
                    "name": "Yao Tong",
                    "hidden": false
                },
                {
                    "_id": "68db481cd2bf1f4b15ec73f2",
                    "user": {
                        "_id": "647db42711084fb58318b053",
                        "avatarUrl": "/avatars/901a4db8e99f6e0c7e7e596c251b315c.svg",
                        "isPro": true,
                        "fullname": "Tongyao",
                        "user": "tyzhu",
                        "type": "user"
                    },
                    "name": "Tongyao Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:29:58.090Z",
                    "hidden": false
                },
                {
                    "_id": "68db481cd2bf1f4b15ec73f3",
                    "name": "Hao Jiang",
                    "hidden": false
                },
                {
                    "_id": "68db481cd2bf1f4b15ec73f4",
                    "name": "Chuang Li",
                    "hidden": false
                },
                {
                    "_id": "68db481cd2bf1f4b15ec73f5",
                    "name": "Jiaying Wu",
                    "hidden": false
                },
                {
                    "_id": "68db481cd2bf1f4b15ec73f6",
                    "name": "Kenji Kawaguchi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-27T08:59:31.000Z",
            "submittedOnDailyAt": "2025-09-30T01:33:39.149Z",
            "title": "From Harm to Help: Turning Reasoning In-Context Demos into Assets for\n  Reasoning LMs",
            "submittedOnDailyBy": {
                "_id": "6496b06a4a9a7e1fe4253ae2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/144NlRW_ETmmOgSYUs_SM.png",
                "isPro": false,
                "fullname": "Haonan Wang",
                "user": "haonan3",
                "type": "user"
            },
            "summary": "Recent reasoning LLMs (RLMs), especially those trained with verifier-based\nreinforcement learning, often perform worse with few-shot CoT than with direct\nanswering. We revisit this paradox using high-quality reasoning traces from\nDeepSeek-R1 as demonstrations and find that adding more exemplars consistently\ndegrades accuracy, even when demonstrations are optimal. A detailed analysis\nreveals two mechanisms behind this decline: (i) semantic misguidance, where\nhigh textual similarity leads the model to treat the target as the same as the\nexemplar and to copy intermediate steps verbatim; and (ii) strategy transfer\nfailure, where the model struggles to extract useful reasoning strategies and\napply them to target questions. Guided by these, we introduce Insight-to-Solve\n(I2S), a sequential test-time procedure that turns demonstrations into\nexplicit, reusable insights and derives a target-specific reasoning trace;\noptionally, the reasoning is self-refined for coherence and correctness (I2S+).\nExtensive experiments on diverse benchmarks show that I2S and I2S+ consistently\noutperform both direct answering and test-time scaling baselines across open-\nand closed-source models. Even for GPT models, our method helps: on AIME'25,\nGPT-4.1 rises by +14.0%, and o1-mini improves by +2.7% on AIME and +1.7% on\nGPQA, indicating that in-context demonstrations can be harnessed effectively\nvia insight-refine-solve framework.",
            "upvotes": 8,
            "discussionId": "68db481cd2bf1f4b15ec73f7",
            "ai_summary": "Insight-to-Solve (I2S) and its refined version (I2S+) improve few-shot chain-of-thought performance by converting demonstrations into reusable insights, outperforming direct answering and scaling methods across various models.",
            "ai_keywords": [
                "reasoning LLMs",
                "verifier-based reinforcement learning",
                "few-shot CoT",
                "DeepSeek-R1",
                "semantic misguidance",
                "strategy transfer failure",
                "Insight-to-Solve",
                "I2S",
                "I2S+",
                "AIME'25",
                "GPT-4.1",
                "o1-mini",
                "GPQA"
            ]
        },
        "publishedAt": "2025-09-27T04:59:31.000Z",
        "title": "From Harm to Help: Turning Reasoning In-Context Demos into Assets for\n  Reasoning LMs",
        "summary": "Recent reasoning LLMs (RLMs), especially those trained with verifier-based\nreinforcement learning, often perform worse with few-shot CoT than with direct\nanswering. We revisit this paradox using high-quality reasoning traces from\nDeepSeek-R1 as demonstrations and find that adding more exemplars consistently\ndegrades accuracy, even when demonstrations are optimal. A detailed analysis\nreveals two mechanisms behind this decline: (i) semantic misguidance, where\nhigh textual similarity leads the model to treat the target as the same as the\nexemplar and to copy intermediate steps verbatim; and (ii) strategy transfer\nfailure, where the model struggles to extract useful reasoning strategies and\napply them to target questions. Guided by these, we introduce Insight-to-Solve\n(I2S), a sequential test-time procedure that turns demonstrations into\nexplicit, reusable insights and derives a target-specific reasoning trace;\noptionally, the reasoning is self-refined for coherence and correctness (I2S+).\nExtensive experiments on diverse benchmarks show that I2S and I2S+ consistently\noutperform both direct answering and test-time scaling baselines across open-\nand closed-source models. Even for GPT models, our method helps: on AIME'25,\nGPT-4.1 rises by +14.0%, and o1-mini improves by +2.7% on AIME and +1.7% on\nGPQA, indicating that in-context demonstrations can be harnessed effectively\nvia insight-refine-solve framework.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23196.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6496b06a4a9a7e1fe4253ae2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/144NlRW_ETmmOgSYUs_SM.png",
            "fullname": "Haonan Wang",
            "name": "haonan3",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.21875",
            "authors": [
                {
                    "_id": "68dbdcb34159d1f2418f976d",
                    "name": "Min-Hsuan Yeh",
                    "hidden": false
                },
                {
                    "_id": "68dbdcb34159d1f2418f976e",
                    "name": "Yixuan Li",
                    "hidden": false
                },
                {
                    "_id": "68dbdcb34159d1f2418f976f",
                    "name": "Tanwi Mallick",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T04:57:46.000Z",
            "submittedOnDailyAt": "2025-09-30T12:16:40.021Z",
            "title": "LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge\n  Signals",
            "submittedOnDailyBy": {
                "_id": "63c07f198d1175e3399d2161",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673559768829-noauth.jpeg",
                "isPro": false,
                "fullname": "Min-Hsuan Yeh",
                "user": "samuelyeh",
                "type": "user"
            },
            "summary": "Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large\nlanguage models (LLMs) by grounding responses in retrieved documents. Yet,\nRAG-based LLMs still hallucinate even when provided with correct and sufficient\ncontext. A growing line of work suggests that this stems from an imbalance\nbetween how models use external context and their internal knowledge, and\nseveral approaches have attempted to quantify these signals for hallucination\ndetection. However, existing methods require extensive hyperparameter tuning,\nlimiting their generalizability. We propose LUMINA, a novel framework that\ndetects hallucinations in RAG systems through context-knowledge signals:\nexternal context utilization is quantified via distributional distance, while\ninternal knowledge utilization is measured by tracking how predicted tokens\nevolve across transformer layers. We further introduce a framework for\nstatistically validating these measurements. Experiments on common RAG\nhallucination benchmarks and four open-source LLMs show that LUMINA achieves\nconsistently high AUROC and AUPRC scores, outperforming prior utilization-based\nmethods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under\nrelaxed assumptions about retrieval quality and model matching, offering both\neffectiveness and practicality.",
            "upvotes": 8,
            "discussionId": "68dbdcb34159d1f2418f9770",
            "ai_summary": "LUMINA detects hallucinations in RAG systems by quantifying external context utilization and internal knowledge utilization, outperforming existing methods on benchmarks.",
            "ai_keywords": [
                "Retrieval-Augmented Generation",
                "RAG",
                "large language models",
                "LLMs",
                "hallucinations",
                "distributional distance",
                "transformer layers",
                "AUROC",
                "AUPRC",
                "HalluRAG"
            ]
        },
        "publishedAt": "2025-09-26T00:57:46.000Z",
        "title": "LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge\n  Signals",
        "summary": "Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large\nlanguage models (LLMs) by grounding responses in retrieved documents. Yet,\nRAG-based LLMs still hallucinate even when provided with correct and sufficient\ncontext. A growing line of work suggests that this stems from an imbalance\nbetween how models use external context and their internal knowledge, and\nseveral approaches have attempted to quantify these signals for hallucination\ndetection. However, existing methods require extensive hyperparameter tuning,\nlimiting their generalizability. We propose LUMINA, a novel framework that\ndetects hallucinations in RAG systems through context-knowledge signals:\nexternal context utilization is quantified via distributional distance, while\ninternal knowledge utilization is measured by tracking how predicted tokens\nevolve across transformer layers. We further introduce a framework for\nstatistically validating these measurements. Experiments on common RAG\nhallucination benchmarks and four open-source LLMs show that LUMINA achieves\nconsistently high AUROC and AUPRC scores, outperforming prior utilization-based\nmethods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under\nrelaxed assumptions about retrieval quality and model matching, offering both\neffectiveness and practicality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21875.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63c07f198d1175e3399d2161",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673559768829-noauth.jpeg",
            "fullname": "Min-Hsuan Yeh",
            "name": "samuelyeh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25370",
            "authors": [
                {
                    "_id": "68dc7b334159d1f2418f99ce",
                    "name": "Kunlun Zhu",
                    "hidden": false
                },
                {
                    "_id": "68dc7b334159d1f2418f99cf",
                    "name": "Zijia Liu",
                    "hidden": false
                },
                {
                    "_id": "68dc7b334159d1f2418f99d0",
                    "name": "Bingxuan Li",
                    "hidden": false
                },
                {
                    "_id": "68dc7b334159d1f2418f99d1",
                    "name": "Muxin Tian",
                    "hidden": false
                },
                {
                    "_id": "68dc7b334159d1f2418f99d2",
                    "name": "Yingxuan Yang",
                    "hidden": false
                },
                {
                    "_id": "68dc7b334159d1f2418f99d3",
                    "name": "Jiaxun Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dc7b334159d1f2418f99d4",
                    "name": "Pengrui Han",
                    "hidden": false
                },
                {
                    "_id": "68dc7b334159d1f2418f99d5",
                    "name": "Qipeng Xie",
                    "hidden": false
                },
                {
                    "_id": "68dc7b334159d1f2418f99d6",
                    "name": "Fuyang Cui",
                    "hidden": false
                },
                {
                    "_id": "68dc7b334159d1f2418f99d7",
                    "name": "Weijia Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dc7b334159d1f2418f99d8",
                    "name": "Xiaoteng Ma",
                    "hidden": false
                },
                {
                    "_id": "68dc7b334159d1f2418f99d9",
                    "name": "Xiaodong Yu",
                    "hidden": false
                },
                {
                    "_id": "68dc7b334159d1f2418f99da",
                    "name": "Gowtham Ramesh",
                    "hidden": false
                },
                {
                    "_id": "68dc7b334159d1f2418f99db",
                    "name": "Jialian Wu",
                    "hidden": false
                },
                {
                    "_id": "68dc7b334159d1f2418f99dc",
                    "name": "Zicheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68dc7b334159d1f2418f99dd",
                    "name": "Pan Lu",
                    "hidden": false
                },
                {
                    "_id": "68dc7b334159d1f2418f99de",
                    "name": "James Zou",
                    "hidden": false
                },
                {
                    "_id": "68dc7b334159d1f2418f99df",
                    "name": "Jiaxuan You",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T18:20:27.000Z",
            "submittedOnDailyAt": "2025-09-30T23:32:52.471Z",
            "title": "Where LLM Agents Fail and How They can Learn From Failures",
            "submittedOnDailyBy": {
                "_id": "64c090a9f613170e7be93d2f",
                "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
                "isPro": false,
                "fullname": "KunlunZhu",
                "user": "KunlunZhu",
                "type": "user"
            },
            "summary": "Large Language Model (LLM) agents, which integrate planning, memory,\nreflection, and tool-use modules, have shown promise in solving complex,\nmulti-step tasks. Yet their sophisticated architectures amplify vulnerability\nto cascading failures, where a single root-cause error propagates through\nsubsequent decisions, leading to task failure. Current systems lack a framework\nthat can comprehensively understand agent error in a modular and systemic way,\nand therefore fail to detect these errors accordingly. We address this gap with\nthree contributions. First, we introduce the AgentErrorTaxonomy, a modular\nclassification of failure modes spanning memory, reflection, planning, action,\nand system-level operations. Second, we construct AgentErrorBench, the first\ndataset of systematically annotated failure trajectories from ALFWorld, GAIA,\nand WebShop, grounding error analysis in real-world agent rollouts. Third, we\npropose AgentDebug, a debugging framework that isolates root-cause failures and\nprovides corrective feedback, enabling agents to recover and iteratively\nimprove. Experiments on AgentErrorBench show that AgentDebug achieves 24%\nhigher all-correct accuracy and 17% higher step accuracy compared to the\nstrongest baseline. Beyond detection, the targeted feedback generated by\nAgentDebug enables LLM agents to iteratively recover from failures, yielding up\nto 26% relative improvements in task success across ALFWorld, GAIA, and\nWebShop. These results establish principled debugging as a pathway to more\nreliable and adaptive LLM agents. The code and data will be available at\nhttps://github.com/ulab-uiuc/AgentDebug",
            "upvotes": 7,
            "discussionId": "68dc7b344159d1f2418f99e0",
            "githubRepo": "https://github.com/ulab-uiuc/AgentDebug",
            "ai_summary": "A modular taxonomy and debugging framework improve error detection and recovery in large language model agents, enhancing task success.",
            "ai_keywords": [
                "AgentErrorTaxonomy",
                "AgentErrorBench",
                "AgentDebug",
                "failure modes",
                "memory",
                "reflection",
                "planning",
                "action",
                "system-level operations",
                "ALFWorld",
                "GAIA",
                "WebShop",
                "all-correct accuracy",
                "step accuracy",
                "task success"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-09-29T14:20:27.000Z",
        "title": "Where LLM Agents Fail and How They can Learn From Failures",
        "summary": "Large Language Model (LLM) agents, which integrate planning, memory,\nreflection, and tool-use modules, have shown promise in solving complex,\nmulti-step tasks. Yet their sophisticated architectures amplify vulnerability\nto cascading failures, where a single root-cause error propagates through\nsubsequent decisions, leading to task failure. Current systems lack a framework\nthat can comprehensively understand agent error in a modular and systemic way,\nand therefore fail to detect these errors accordingly. We address this gap with\nthree contributions. First, we introduce the AgentErrorTaxonomy, a modular\nclassification of failure modes spanning memory, reflection, planning, action,\nand system-level operations. Second, we construct AgentErrorBench, the first\ndataset of systematically annotated failure trajectories from ALFWorld, GAIA,\nand WebShop, grounding error analysis in real-world agent rollouts. Third, we\npropose AgentDebug, a debugging framework that isolates root-cause failures and\nprovides corrective feedback, enabling agents to recover and iteratively\nimprove. Experiments on AgentErrorBench show that AgentDebug achieves 24%\nhigher all-correct accuracy and 17% higher step accuracy compared to the\nstrongest baseline. Beyond detection, the targeted feedback generated by\nAgentDebug enables LLM agents to iteratively recover from failures, yielding up\nto 26% relative improvements in task success across ALFWorld, GAIA, and\nWebShop. These results establish principled debugging as a pathway to more\nreliable and adaptive LLM agents. The code and data will be available at\nhttps://github.com/ulab-uiuc/AgentDebug",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25370.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64c090a9f613170e7be93d2f",
            "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
            "fullname": "KunlunZhu",
            "name": "KunlunZhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "submitterOrganization": {
            "_id": "65448bef5b5d9185ba3202b9",
            "name": "UIUC-CS",
            "fullname": "University of Illinois at Urbana-Champaign",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.23924",
            "authors": [
                {
                    "_id": "68db800bd2bf1f4b15ec77d0",
                    "user": {
                        "_id": "64f73a44102fbfb26410962e",
                        "avatarUrl": "/avatars/328302a495de6a4418be835456d1d3c6.svg",
                        "isPro": false,
                        "fullname": "jingyi Yang",
                        "user": "JY-Young",
                        "type": "user"
                    },
                    "name": "Jingyi Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T07:12:40.232Z",
                    "hidden": false
                },
                {
                    "_id": "68db800bd2bf1f4b15ec77d1",
                    "name": "Guanxu Chen",
                    "hidden": false
                },
                {
                    "_id": "68db800bd2bf1f4b15ec77d2",
                    "name": "Xuhao Hu",
                    "hidden": false
                },
                {
                    "_id": "68db800bd2bf1f4b15ec77d3",
                    "name": "Jing Shao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T15:01:15.000Z",
            "submittedOnDailyAt": "2025-09-30T05:38:56.124Z",
            "title": "Taming Masked Diffusion Language Models via Consistency Trajectory\n  Reinforcement Learning with Fewer Decoding Step",
            "submittedOnDailyBy": {
                "_id": "64f73a44102fbfb26410962e",
                "avatarUrl": "/avatars/328302a495de6a4418be835456d1d3c6.svg",
                "isPro": false,
                "fullname": "jingyi Yang",
                "user": "JY-Young",
                "type": "user"
            },
            "summary": "Masked diffusion language models (MDLMs) have recently emerged as a promising\nalternative to autoregressive (AR) language models, offering properties such as\nparallel decoding, flexible generation orders, and the potential for fewer\ninference steps. Despite these advantages, decoding strategies and\nreinforcement learning (RL) algorithms tailored for MDLMs remain underexplored.\nA naive approach is to directly transfer techniques well-established for AR\nmodels to MDLMs. However, this raises an immediate question: Is such a naive\ntransfer truly optimal? For example, 1) Block-wise and semi-AR decoding\nstrategies are not employed during the training of MDLMs, so why do they\noutperform full diffusion-style decoding during inference? 2) Applying RL\nalgorithms designed for AR models directly to MDLMs exhibits a\ntraining-inference inconsistency, since MDLM decoding are non-causal\n(parallel). This results in inconsistencies between the rollout trajectory and\nthe optimization trajectory. To address these challenges, we propose EOS Early\nRejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which\nunlock the potential of MDLMs to perform full diffusion-style decoding,\nachieving competitive performance with fewer decoding steps. Additionally, we\nintroduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)\nfor taming MDLMs, which emphasizes the consistency between rollout trajectory\nand optimization trajectory, and reduces the optimization errors caused by\nskip-step optimization. We conduct extensive experiments on reasoning tasks,\nsuch as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The\nresults demonstrate that the proposed EOSER and ASS mechanisms, together with\nCJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs.\nCode: https://github.com/yjyddq/EOSER-ASS-RL.",
            "upvotes": 7,
            "discussionId": "68db800bd2bf1f4b15ec77d4",
            "githubRepo": "https://github.com/yjyddq/EOSER-ASS-RL",
            "ai_summary": "Proposed decoding strategies and reinforcement learning algorithms improve the performance and efficiency of masked diffusion language models during inference.",
            "ai_keywords": [
                "Masked diffusion language models",
                "autoregressive models",
                "parallel decoding",
                "flexible generation orders",
                "block-wise decoding",
                "semi-AR decoding",
                "full diffusion-style decoding",
                "reinforcement learning",
                "non-causal decoding",
                "rollout trajectory",
                "optimization trajectory",
                "EOS Early Rejection",
                "Ascending Step-Size",
                "Consistency Trajectory Group Relative Policy Optimization",
                "LLaDA-8B-Instruct"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-09-28T11:01:15.000Z",
        "title": "Taming Masked Diffusion Language Models via Consistency Trajectory\n  Reinforcement Learning with Fewer Decoding Step",
        "summary": "Masked diffusion language models (MDLMs) have recently emerged as a promising\nalternative to autoregressive (AR) language models, offering properties such as\nparallel decoding, flexible generation orders, and the potential for fewer\ninference steps. Despite these advantages, decoding strategies and\nreinforcement learning (RL) algorithms tailored for MDLMs remain underexplored.\nA naive approach is to directly transfer techniques well-established for AR\nmodels to MDLMs. However, this raises an immediate question: Is such a naive\ntransfer truly optimal? For example, 1) Block-wise and semi-AR decoding\nstrategies are not employed during the training of MDLMs, so why do they\noutperform full diffusion-style decoding during inference? 2) Applying RL\nalgorithms designed for AR models directly to MDLMs exhibits a\ntraining-inference inconsistency, since MDLM decoding are non-causal\n(parallel). This results in inconsistencies between the rollout trajectory and\nthe optimization trajectory. To address these challenges, we propose EOS Early\nRejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which\nunlock the potential of MDLMs to perform full diffusion-style decoding,\nachieving competitive performance with fewer decoding steps. Additionally, we\nintroduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)\nfor taming MDLMs, which emphasizes the consistency between rollout trajectory\nand optimization trajectory, and reduces the optimization errors caused by\nskip-step optimization. We conduct extensive experiments on reasoning tasks,\nsuch as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The\nresults demonstrate that the proposed EOSER and ASS mechanisms, together with\nCJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs.\nCode: https://github.com/yjyddq/EOSER-ASS-RL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23924.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64f73a44102fbfb26410962e",
            "avatarUrl": "/avatars/328302a495de6a4418be835456d1d3c6.svg",
            "fullname": "jingyi Yang",
            "name": "JY-Young",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "submitterOrganization": {
            "_id": "643cb0625fcffe09fb6ca688",
            "name": "Fudan-University",
            "fullname": "Fudan University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.23866",
            "authors": [
                {
                    "_id": "68db503dd2bf1f4b15ec74b2",
                    "name": "Pengxiang Li",
                    "hidden": false
                },
                {
                    "_id": "68db503dd2bf1f4b15ec74b3",
                    "name": "Zechen Hu",
                    "hidden": false
                },
                {
                    "_id": "68db503dd2bf1f4b15ec74b4",
                    "name": "Zirui Shang",
                    "hidden": false
                },
                {
                    "_id": "68db503dd2bf1f4b15ec74b5",
                    "name": "Jingrong Wu",
                    "hidden": false
                },
                {
                    "_id": "68db503dd2bf1f4b15ec74b6",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "68db503dd2bf1f4b15ec74b7",
                    "name": "Hui Liu",
                    "hidden": false
                },
                {
                    "_id": "68db503dd2bf1f4b15ec74b8",
                    "name": "Zhi Gao",
                    "hidden": false
                },
                {
                    "_id": "68db503dd2bf1f4b15ec74b9",
                    "name": "Chenrui Shi",
                    "hidden": false
                },
                {
                    "_id": "68db503dd2bf1f4b15ec74ba",
                    "name": "Bofei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db503dd2bf1f4b15ec74bb",
                    "name": "Zihao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db503dd2bf1f4b15ec74bc",
                    "name": "Xiaochuan Shi",
                    "hidden": false
                },
                {
                    "_id": "68db503dd2bf1f4b15ec74bd",
                    "name": "Zedong YU",
                    "hidden": false
                },
                {
                    "_id": "68db503dd2bf1f4b15ec74be",
                    "name": "Yuwei Wu",
                    "hidden": false
                },
                {
                    "_id": "68db503dd2bf1f4b15ec74bf",
                    "name": "Xinxiao Wu",
                    "hidden": false
                },
                {
                    "_id": "68db503dd2bf1f4b15ec74c0",
                    "name": "Yunde Jia",
                    "hidden": false
                },
                {
                    "_id": "68db503dd2bf1f4b15ec74c1",
                    "name": "Liuyu Xiang",
                    "hidden": false
                },
                {
                    "_id": "68db503dd2bf1f4b15ec74c2",
                    "name": "Zhaofeng He",
                    "hidden": false
                },
                {
                    "_id": "68db503dd2bf1f4b15ec74c3",
                    "name": "Qing Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T13:19:20.000Z",
            "submittedOnDailyAt": "2025-09-30T02:10:09.841Z",
            "title": "Efficient Multi-turn RL for GUI Agents via Decoupled Training and\n  Adaptive Data Curation",
            "submittedOnDailyBy": {
                "_id": "6455f8c2c8f569b995d603c9",
                "avatarUrl": "/avatars/3cb2cad3ab123887c270234bb6a4ca43.svg",
                "isPro": false,
                "fullname": "Qing Li",
                "user": "li-qing",
                "type": "user"
            },
            "summary": "Vision-language model (VLM) based GUI agents show promise for automating\ncomplex desktop and mobile tasks, but face significant challenges in applying\nreinforcement learning (RL): (1) slow multi-turn interactions with GUI\nenvironments for policy rollout, and (2) insufficient high-quality\nagent-environment interactions for policy learning. To address these\nchallenges, we propose DART, a Decoupled Agentic RL Training framework for GUI\nagents, which coordinates heterogeneous modules in a highly decoupled manner.\nDART separates the training system into four asynchronous modules: environment\ncluster, rollout service, data manager, and trainer. This design enables\nnon-blocking communication, asynchronous training, rollout-wise trajectory\nsampling, and per-worker model synchronization, significantly improving the\nsystem efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput,\nand 5.5* environment utilization. To facilitate effective learning from\nabundant samples, we introduce an adaptive data curation scheme: (1)\npre-collecting successful trajectories for challenging tasks to supplement\nsparse success in online sampling; (2) dynamically adjusting rollout numbers\nand trajectory lengths based on task difficulty; (3) training selectively on\nhigh-entropy steps to prioritize critical decisions; (4) stabilizing learning\nvia truncated importance sampling for policy mismatch between policy rollout\nand updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task\nsuccess rate, a 14.61% absolute gain over the base model, and 7.34% higher than\nopen-source SOTA. We will fully open-source our training framework, data, and\nmodel checkpoints via computer-use-agents.github.io/dart-gui, which we believe\nis a timely contribution to the open-source community of agentic RL training.",
            "upvotes": 7,
            "discussionId": "68db503dd2bf1f4b15ec74c4",
            "projectPage": "https://computer-use-agents.github.io/dart-gui/",
            "githubRepo": "https://github.com/computer-use-agents/dart-gui",
            "ai_summary": "DART, a decoupled reinforcement learning framework for GUI agents, improves efficiency and learning effectiveness through asynchronous modules and adaptive data curation, achieving high task success rates on the OSWorld benchmark.",
            "ai_keywords": [
                "reinforcement learning",
                "RL",
                "GUI agents",
                "DART",
                "Decoupled Agentic RL Training",
                "environment cluster",
                "rollout service",
                "data manager",
                "trainer",
                "non-blocking communication",
                "asynchronous training",
                "rollout-wise trajectory sampling",
                "per-worker model synchronization",
                "adaptive data curation",
                "successful trajectories",
                "rollout numbers",
                "trajectory lengths",
                "high-entropy steps",
                "truncated importance sampling",
                "OSWorld benchmark",
                "DART-GUI-7B"
            ],
            "githubStars": 9
        },
        "publishedAt": "2025-09-28T09:19:20.000Z",
        "title": "Efficient Multi-turn RL for GUI Agents via Decoupled Training and\n  Adaptive Data Curation",
        "summary": "Vision-language model (VLM) based GUI agents show promise for automating\ncomplex desktop and mobile tasks, but face significant challenges in applying\nreinforcement learning (RL): (1) slow multi-turn interactions with GUI\nenvironments for policy rollout, and (2) insufficient high-quality\nagent-environment interactions for policy learning. To address these\nchallenges, we propose DART, a Decoupled Agentic RL Training framework for GUI\nagents, which coordinates heterogeneous modules in a highly decoupled manner.\nDART separates the training system into four asynchronous modules: environment\ncluster, rollout service, data manager, and trainer. This design enables\nnon-blocking communication, asynchronous training, rollout-wise trajectory\nsampling, and per-worker model synchronization, significantly improving the\nsystem efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput,\nand 5.5* environment utilization. To facilitate effective learning from\nabundant samples, we introduce an adaptive data curation scheme: (1)\npre-collecting successful trajectories for challenging tasks to supplement\nsparse success in online sampling; (2) dynamically adjusting rollout numbers\nand trajectory lengths based on task difficulty; (3) training selectively on\nhigh-entropy steps to prioritize critical decisions; (4) stabilizing learning\nvia truncated importance sampling for policy mismatch between policy rollout\nand updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task\nsuccess rate, a 14.61% absolute gain over the base model, and 7.34% higher than\nopen-source SOTA. We will fully open-source our training framework, data, and\nmodel checkpoints via computer-use-agents.github.io/dart-gui, which we believe\nis a timely contribution to the open-source community of agentic RL training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23866.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6455f8c2c8f569b995d603c9",
            "avatarUrl": "/avatars/3cb2cad3ab123887c270234bb6a4ca43.svg",
            "fullname": "Qing Li",
            "name": "li-qing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "submitterOrganization": {
            "_id": "63a95ac93453852ef5399a77",
            "name": "bigai",
            "fullname": "Beijing Institute for General Artificial Intelligence",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.23564",
            "authors": [
                {
                    "_id": "68dbdc134159d1f2418f9769",
                    "name": "Min-Hsuan Yeh",
                    "hidden": false
                },
                {
                    "_id": "68dbdc134159d1f2418f976a",
                    "name": "Yixuan Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T01:44:05.000Z",
            "submittedOnDailyAt": "2025-09-30T12:05:12.409Z",
            "title": "Clean First, Align Later: Benchmarking Preference Data Cleaning for\n  Reliable LLM Alignment",
            "submittedOnDailyBy": {
                "_id": "63c07f198d1175e3399d2161",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673559768829-noauth.jpeg",
                "isPro": false,
                "fullname": "Min-Hsuan Yeh",
                "user": "samuelyeh",
                "type": "user"
            },
            "summary": "Human feedback plays a pivotal role in aligning large language models (LLMs)\nwith human preferences. However, such feedback is often noisy or inconsistent,\nwhich can degrade the quality of reward models and hinder alignment. While\nvarious automated data cleaning methods have been proposed to mitigate this\nissue, a systematic evaluation of their effectiveness and generalizability\nremains lacking. To bridge this gap, we introduce the first comprehensive\nbenchmark for evaluating 13 preference data cleaning methods in the context of\nLLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning\nstrategies in terms of alignment performance and generalizability across\ndiverse datasets, model architectures, and optimization algorithms. By unifying\ndisparate methods and rigorously comparing them, we uncover key factors that\ndetermine the success of data cleaning in alignment tasks. This benchmark lays\nthe groundwork for principled and reproducible approaches to improving LLM\nalignment through better data quality-highlighting the crucial but\nunderexplored role of data preprocessing in responsible AI development. We\nrelease modular implementations of all methods to catalyze further research:\nhttps://github.com/deeplearning-wisc/PrefCleanBench.",
            "upvotes": 7,
            "discussionId": "68dbdc144159d1f2418f976b",
            "ai_summary": "PrefCleanBench evaluates 13 preference data cleaning methods for aligning large language models with human preferences, providing a standardized protocol to assess their effectiveness and generalizability.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "human feedback",
                "reward models",
                "alignment",
                "automated data cleaning",
                "benchmark",
                "alignment performance",
                "generalizability",
                "datasets",
                "model architectures",
                "optimization algorithms",
                "data preprocessing",
                "responsible AI development"
            ]
        },
        "publishedAt": "2025-09-27T21:44:05.000Z",
        "title": "Clean First, Align Later: Benchmarking Preference Data Cleaning for\n  Reliable LLM Alignment",
        "summary": "Human feedback plays a pivotal role in aligning large language models (LLMs)\nwith human preferences. However, such feedback is often noisy or inconsistent,\nwhich can degrade the quality of reward models and hinder alignment. While\nvarious automated data cleaning methods have been proposed to mitigate this\nissue, a systematic evaluation of their effectiveness and generalizability\nremains lacking. To bridge this gap, we introduce the first comprehensive\nbenchmark for evaluating 13 preference data cleaning methods in the context of\nLLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning\nstrategies in terms of alignment performance and generalizability across\ndiverse datasets, model architectures, and optimization algorithms. By unifying\ndisparate methods and rigorously comparing them, we uncover key factors that\ndetermine the success of data cleaning in alignment tasks. This benchmark lays\nthe groundwork for principled and reproducible approaches to improving LLM\nalignment through better data quality-highlighting the crucial but\nunderexplored role of data preprocessing in responsible AI development. We\nrelease modular implementations of all methods to catalyze further research:\nhttps://github.com/deeplearning-wisc/PrefCleanBench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23564.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63c07f198d1175e3399d2161",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673559768829-noauth.jpeg",
            "fullname": "Min-Hsuan Yeh",
            "name": "samuelyeh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25149",
            "authors": [
                {
                    "_id": "68db57aed2bf1f4b15ec7538",
                    "name": "NVIDIA",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7539",
                    "name": "Felix Abecassis",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec753a",
                    "name": "Anjulie Agrusa",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec753b",
                    "name": "Dong Ahn",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec753c",
                    "name": "Jonah Alben",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec753d",
                    "name": "Stefania Alborghetti",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec753e",
                    "name": "Michael Andersch",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec753f",
                    "name": "Sivakumar Arayandi",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7540",
                    "name": "Alexis Bjorlin",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7541",
                    "name": "Aaron Blakeman",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7542",
                    "name": "Evan Briones",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7543",
                    "name": "Ian Buck",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7544",
                    "name": "Bryan Catanzaro",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7545",
                    "name": "Jinhang Choi",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7546",
                    "name": "Mike Chrzanowski",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7547",
                    "name": "Eric Chung",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7548",
                    "name": "Victor Cui",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7549",
                    "name": "Steve Dai",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec754a",
                    "name": "Bita Darvish Rouhani",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec754b",
                    "name": "Carlo del Mundo",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec754c",
                    "name": "Deena Donia",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec754d",
                    "name": "Burc Eryilmaz",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec754e",
                    "name": "Henry Estela",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec754f",
                    "name": "Abhinav Goel",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7550",
                    "name": "Oleg Goncharov",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7551",
                    "name": "Yugi Guvvala",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7552",
                    "name": "Robert Hesse",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7553",
                    "name": "Russell Hewett",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7554",
                    "name": "Herbert Hum",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7555",
                    "name": "Ujval Kapasi",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7556",
                    "name": "Brucek Khailany",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7557",
                    "name": "Mikail Khona",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7558",
                    "name": "Nick Knight",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7559",
                    "name": "Alex Kondratenko",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec755a",
                    "name": "Ronny Krashinsky",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec755b",
                    "name": "Ben Lanir",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec755c",
                    "name": "Simon Layton",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec755d",
                    "name": "Michael Lightstone",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec755e",
                    "name": "Daniel Lo",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec755f",
                    "name": "Paulius Micikevicius",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7560",
                    "name": "Asit Mishra",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7561",
                    "name": "Tim Moon",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7562",
                    "name": "Deepak Narayanan",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7563",
                    "name": "Chao Ni",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7564",
                    "name": "Abhijit Paithankar",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7565",
                    "name": "Satish Pasumarthi",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7566",
                    "name": "Ankit Patel",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7567",
                    "name": "Mostofa Patwary",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7568",
                    "name": "Ashwin Poojary",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7569",
                    "name": "Gargi Prasad",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec756a",
                    "name": "Sweta Priyadarshi",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec756b",
                    "name": "Yigong Qin",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec756c",
                    "name": "Xiaowei Ren",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec756d",
                    "name": "Oleg Rybakov",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec756e",
                    "name": "Charbel Sakr",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec756f",
                    "name": "Sanjeev Satheesh",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7570",
                    "name": "Stas Sergienko",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7571",
                    "name": "Pasha Shamis",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7572",
                    "name": "Kirthi Shankar",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7573",
                    "name": "Nishant Sharma",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7574",
                    "name": "Mohammad Shoeybi",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7575",
                    "name": "Michael Siu",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7576",
                    "name": "Misha Smelyanskiy",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7577",
                    "name": "Darko Stosic",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7578",
                    "name": "Dusan Stosic",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7579",
                    "name": "Bor-Yiing Su",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec757a",
                    "name": "Frank Sun",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec757b",
                    "name": "Nima Tajbakhsh",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec757c",
                    "name": "Shelby Thomas",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec757d",
                    "name": "Przemek Tredak",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec757e",
                    "name": "Evgeny Tsykunov",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec757f",
                    "name": "Gandhi Vaithilingam",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7580",
                    "name": "Aditya Vavre",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7581",
                    "name": "Rangharajan Venkatesan",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7582",
                    "name": "Roger Waleffe",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7583",
                    "name": "Qiyu Wan",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7584",
                    "name": "Hexin Wang",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7585",
                    "name": "Mengdi Wang",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7586",
                    "name": "Lizzie Wei",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7587",
                    "name": "Hao Wu",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7588",
                    "name": "Evan Wu",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7589",
                    "name": "Keith Wyss",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec758a",
                    "name": "Ning Xu",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec758b",
                    "name": "Jinze Xue",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec758c",
                    "name": "Charlene Yang",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec758d",
                    "name": "Yujia Zhai",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec758e",
                    "name": "Ruoxi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec758f",
                    "name": "Jingyang Zhu",
                    "hidden": false
                },
                {
                    "_id": "68db57aed2bf1f4b15ec7590",
                    "name": "Zhongbo Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:53:17.000Z",
            "submittedOnDailyAt": "2025-09-30T02:38:26.887Z",
            "title": "Pretraining Large Language Models with NVFP4",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) today are powerful problem solvers across many\ndomains, and they continue to get stronger as they scale in model size,\ntraining set size, and training set quality, as shown by extensive research and\nexperimentation across the industry. Training a frontier model today requires\non the order of tens to hundreds of yottaflops, which is a massive investment\nof time, compute, and energy. Improving pretraining efficiency is therefore\nessential to enable the next generation of even more capable LLMs. While 8-bit\nfloating point (FP8) training is now widely adopted, transitioning to even\nnarrower precision, such as 4-bit floating point (FP4), could unlock additional\nimprovements in computational speed and resource utilization. However,\nquantization at this level poses challenges to training stability, convergence,\nand implementation, notably for large-scale models trained on long token\nhorizons.\n  In this study, we introduce a novel approach for stable and accurate training\nof large language models (LLMs) using the NVFP4 format. Our method integrates\nRandom Hadamard transforms (RHT) to bound block-level outliers, employs a\ntwo-dimensional quantization scheme for consistent representations across both\nthe forward and backward passes, utilizes stochastic rounding for unbiased\ngradient estimation, and incorporates selective high-precision layers. We\nvalidate our approach by training a 12-billion-parameter model on 10 trillion\ntokens -- the longest publicly documented training run in 4-bit precision to\ndate. Our results show that the model trained with our NVFP4-based pretraining\ntechnique achieves training loss and downstream task accuracies comparable to\nan FP8 baseline. These findings highlight that NVFP4, when combined with our\ntraining approach, represents a major step forward in narrow-precision LLM\ntraining algorithms.",
            "upvotes": 6,
            "discussionId": "68db57afd2bf1f4b15ec7591",
            "ai_summary": "A novel training approach using NVFP4 format with Random Hadamard transforms, two-dimensional quantization, stochastic rounding, and selective high-precision layers enables stable and accurate training of large language models in 4-bit precision.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "8-bit floating point (FP8)",
                "4-bit floating point (FP4)",
                "quantization",
                "training stability",
                "convergence",
                "implementation",
                "Random Hadamard transforms (RHT)",
                "two-dimensional quantization",
                "stochastic rounding",
                "selective high-precision layers",
                "NVFP4",
                "training loss",
                "downstream task accuracies"
            ]
        },
        "publishedAt": "2025-09-29T13:53:17.000Z",
        "title": "Pretraining Large Language Models with NVFP4",
        "summary": "Large Language Models (LLMs) today are powerful problem solvers across many\ndomains, and they continue to get stronger as they scale in model size,\ntraining set size, and training set quality, as shown by extensive research and\nexperimentation across the industry. Training a frontier model today requires\non the order of tens to hundreds of yottaflops, which is a massive investment\nof time, compute, and energy. Improving pretraining efficiency is therefore\nessential to enable the next generation of even more capable LLMs. While 8-bit\nfloating point (FP8) training is now widely adopted, transitioning to even\nnarrower precision, such as 4-bit floating point (FP4), could unlock additional\nimprovements in computational speed and resource utilization. However,\nquantization at this level poses challenges to training stability, convergence,\nand implementation, notably for large-scale models trained on long token\nhorizons.\n  In this study, we introduce a novel approach for stable and accurate training\nof large language models (LLMs) using the NVFP4 format. Our method integrates\nRandom Hadamard transforms (RHT) to bound block-level outliers, employs a\ntwo-dimensional quantization scheme for consistent representations across both\nthe forward and backward passes, utilizes stochastic rounding for unbiased\ngradient estimation, and incorporates selective high-precision layers. We\nvalidate our approach by training a 12-billion-parameter model on 10 trillion\ntokens -- the longest publicly documented training run in 4-bit precision to\ndate. Our results show that the model trained with our NVFP4-based pretraining\ntechnique achieves training loss and downstream task accuracies comparable to\nan FP8 baseline. These findings highlight that NVFP4, when combined with our\ntraining approach, represents a major step forward in narrow-precision LLM\ntraining algorithms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25149.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 115
        },
        "submitterOrganization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.24335",
            "authors": [
                {
                    "_id": "68db4439d2bf1f4b15ec7374",
                    "user": {
                        "_id": "6348de0c62c668c7b48d83c9",
                        "avatarUrl": "/avatars/7296ea9bb301e19c10926022959b2023.svg",
                        "isPro": false,
                        "fullname": "Guolin Ke",
                        "user": "guolinke",
                        "type": "user"
                    },
                    "name": "Guolin Ke",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:30:33.722Z",
                    "hidden": false
                },
                {
                    "_id": "68db4439d2bf1f4b15ec7375",
                    "name": "Hui Xue",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T06:34:24.000Z",
            "submittedOnDailyAt": "2025-09-30T01:28:30.854Z",
            "title": "Hyperspherical Latents Improve Continuous-Token Autoregressive\n  Generation",
            "submittedOnDailyBy": {
                "_id": "6348de0c62c668c7b48d83c9",
                "avatarUrl": "/avatars/7296ea9bb301e19c10926022959b2023.svg",
                "isPro": false,
                "fullname": "Guolin Ke",
                "user": "guolinke",
                "type": "user"
            },
            "summary": "Autoregressive (AR) models are promising for image generation, yet\ncontinuous-token AR variants often trail latent diffusion and masked-generation\nmodels. The core issue is heterogeneous variance in VAE latents, which is\namplified during AR decoding, especially under classifier-free guidance (CFG),\nand can cause variance collapse. We propose SphereAR to address this issue. Its\ncore design is to constrain all AR inputs and outputs -- including after CFG --\nto lie on a fixed-radius hypersphere (constant ell_2 norm), leveraging\nhyperspherical VAEs. Our theoretical analysis shows that hyperspherical\nconstraint removes the scale component (the primary cause of variance\ncollapse), thereby stabilizing AR decoding. Empirically, on ImageNet\ngeneration, SphereAR-H (943M) sets a new state of the art for AR models,\nachieving FID 1.34. Even at smaller scales, SphereAR-L (479M) reaches FID 1.54\nand SphereAR-B (208M) reaches 1.92, matching or surpassing much larger\nbaselines such as MAR-H (943M, 1.55) and VAR-d30 (2B, 1.92). To our knowledge,\nthis is the first time a pure next-token AR image generator with raster order\nsurpasses diffusion and masked-generation models at comparable parameter\nscales.",
            "upvotes": 6,
            "discussionId": "68db4439d2bf1f4b15ec7376",
            "githubRepo": "https://github.com/guolinke/SphereAR",
            "ai_summary": "SphereAR, an autoregressive model with hyperspherical constraints, achieves state-of-the-art performance in image generation, surpassing diffusion and masked-generation models at similar parameter scales.",
            "ai_keywords": [
                "autoregressive models",
                "continuous-token AR",
                "latent diffusion",
                "masked-generation models",
                "VAE latents",
                "classifier-free guidance",
                "variance collapse",
                "hypersphere",
                "hyperspherical VAEs",
                "FID",
                "SphereAR-H",
                "SphereAR-L",
                "SphereAR-B",
                "MAR-H",
                "VAR-d30"
            ],
            "githubStars": 18
        },
        "publishedAt": "2025-09-29T02:34:24.000Z",
        "title": "Hyperspherical Latents Improve Continuous-Token Autoregressive\n  Generation",
        "summary": "Autoregressive (AR) models are promising for image generation, yet\ncontinuous-token AR variants often trail latent diffusion and masked-generation\nmodels. The core issue is heterogeneous variance in VAE latents, which is\namplified during AR decoding, especially under classifier-free guidance (CFG),\nand can cause variance collapse. We propose SphereAR to address this issue. Its\ncore design is to constrain all AR inputs and outputs -- including after CFG --\nto lie on a fixed-radius hypersphere (constant ell_2 norm), leveraging\nhyperspherical VAEs. Our theoretical analysis shows that hyperspherical\nconstraint removes the scale component (the primary cause of variance\ncollapse), thereby stabilizing AR decoding. Empirically, on ImageNet\ngeneration, SphereAR-H (943M) sets a new state of the art for AR models,\nachieving FID 1.34. Even at smaller scales, SphereAR-L (479M) reaches FID 1.54\nand SphereAR-B (208M) reaches 1.92, matching or surpassing much larger\nbaselines such as MAR-H (943M, 1.55) and VAR-d30 (2B, 1.92). To our knowledge,\nthis is the first time a pure next-token AR image generator with raster order\nsurpasses diffusion and masked-generation models at comparable parameter\nscales.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24335.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6348de0c62c668c7b48d83c9",
            "avatarUrl": "/avatars/7296ea9bb301e19c10926022959b2023.svg",
            "fullname": "Guolin Ke",
            "name": "guolinke",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.24193",
            "authors": [
                {
                    "_id": "68db4081d2bf1f4b15ec72dc",
                    "name": "Ran Xu",
                    "hidden": false
                },
                {
                    "_id": "68db4081d2bf1f4b15ec72dd",
                    "name": "Yuchen Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68db4081d2bf1f4b15ec72de",
                    "name": "Zihan Dong",
                    "hidden": false
                },
                {
                    "_id": "68db4081d2bf1f4b15ec72df",
                    "name": "Jonathan Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4081d2bf1f4b15ec72e0",
                    "name": "Yue Yu",
                    "hidden": false
                },
                {
                    "_id": "68db4081d2bf1f4b15ec72e1",
                    "name": "Joyce C. Ho",
                    "hidden": false
                },
                {
                    "_id": "68db4081d2bf1f4b15ec72e2",
                    "name": "Linjun Zhang",
                    "hidden": false
                },
                {
                    "_id": "68db4081d2bf1f4b15ec72e3",
                    "name": "Haoyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68db4081d2bf1f4b15ec72e4",
                    "name": "Wenqi Shi",
                    "hidden": false
                },
                {
                    "_id": "68db4081d2bf1f4b15ec72e5",
                    "name": "Carl Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T02:14:30.000Z",
            "submittedOnDailyAt": "2025-09-30T01:00:55.755Z",
            "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced\n  Self-Play",
            "submittedOnDailyBy": {
                "_id": "6471bddd609ae9f56368f132",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6471bddd609ae9f56368f132/G91Q4iCGN2dy3oMaz-LrO.jpeg",
                "isPro": true,
                "fullname": "Yuchen Zhuang",
                "user": "yczhuang",
                "type": "user"
            },
            "summary": "Search-augmented LLMs often struggle with complex reasoning tasks due to\nineffective multi-hop retrieval and limited reasoning ability. We propose\nAceSearcher, a cooperative self-play framework that trains a single large\nlanguage model (LLM) to alternate between two roles: a decomposer that breaks\ndown complex queries and a solver that integrates retrieved contexts for answer\ngeneration. AceSearcher couples supervised fine-tuning on a diverse mixture of\nsearch, reasoning, and decomposition tasks with reinforcement fine-tuning\noptimized for final answer accuracy, eliminating the need for intermediate\nannotations. Extensive experiments on three reasoning-intensive tasks across 10\ndatasets show that AceSearcher outperforms state-of-the-art baselines,\nachieving an average exact match improvement of 7.6%. Remarkably, on\ndocument-level finance reasoning tasks, AceSearcher-32B matches the performance\nof the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller\nscales (1.5B and 8B), AceSearcher often surpasses existing search-augmented\nLLMs with up to 9x more parameters, highlighting its exceptional efficiency and\neffectiveness in tackling complex reasoning tasks. Our code will be published\nat https://github.com/ritaranx/AceSearcher and\nhttps://huggingface.co/AceSearcher.",
            "upvotes": 6,
            "discussionId": "68db4082d2bf1f4b15ec72e6",
            "ai_summary": "AceSearcher, a cooperative self-play framework, enhances a large language model's reasoning ability by alternating between decomposing queries and solving them, outperforming state-of-the-art models with fewer parameters.",
            "ai_keywords": [
                "large language model",
                "decomposer",
                "solver",
                "supervised fine-tuning",
                "reinforcement fine-tuning",
                "exact match improvement",
                "document-level finance reasoning",
                "parameter efficiency"
            ]
        },
        "publishedAt": "2025-09-28T22:14:30.000Z",
        "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced\n  Self-Play",
        "summary": "Search-augmented LLMs often struggle with complex reasoning tasks due to\nineffective multi-hop retrieval and limited reasoning ability. We propose\nAceSearcher, a cooperative self-play framework that trains a single large\nlanguage model (LLM) to alternate between two roles: a decomposer that breaks\ndown complex queries and a solver that integrates retrieved contexts for answer\ngeneration. AceSearcher couples supervised fine-tuning on a diverse mixture of\nsearch, reasoning, and decomposition tasks with reinforcement fine-tuning\noptimized for final answer accuracy, eliminating the need for intermediate\nannotations. Extensive experiments on three reasoning-intensive tasks across 10\ndatasets show that AceSearcher outperforms state-of-the-art baselines,\nachieving an average exact match improvement of 7.6%. Remarkably, on\ndocument-level finance reasoning tasks, AceSearcher-32B matches the performance\nof the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller\nscales (1.5B and 8B), AceSearcher often surpasses existing search-augmented\nLLMs with up to 9x more parameters, highlighting its exceptional efficiency and\neffectiveness in tackling complex reasoning tasks. Our code will be published\nat https://github.com/ritaranx/AceSearcher and\nhttps://huggingface.co/AceSearcher.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24193.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6471bddd609ae9f56368f132",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6471bddd609ae9f56368f132/G91Q4iCGN2dy3oMaz-LrO.jpeg",
            "fullname": "Yuchen Zhuang",
            "name": "yczhuang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.24786",
            "authors": [
                {
                    "_id": "68db43edd2bf1f4b15ec734e",
                    "user": {
                        "_id": "67067633351e0c16a5c27497",
                        "avatarUrl": "/avatars/356aa3431198c8931b820a714bcfb19d.svg",
                        "isPro": false,
                        "fullname": "Shenghao Fu",
                        "user": "fushh7",
                        "type": "user"
                    },
                    "name": "Shenghao Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:30:44.180Z",
                    "hidden": false
                },
                {
                    "_id": "68db43edd2bf1f4b15ec734f",
                    "user": {
                        "_id": "66a097801a26a2350395edc7",
                        "avatarUrl": "/avatars/1e7e127cb7222df7d56e5bfda6bab519.svg",
                        "isPro": false,
                        "fullname": "Qize Yang",
                        "user": "PhilipC",
                        "type": "user"
                    },
                    "name": "Qize Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T10:18:08.655Z",
                    "hidden": false
                },
                {
                    "_id": "68db43edd2bf1f4b15ec7350",
                    "user": {
                        "_id": "644fe6a9e1d7a97f3b66e906",
                        "avatarUrl": "/avatars/ad1a45f0b1c8a4d03ba87f2a3ce5a8f8.svg",
                        "isPro": false,
                        "fullname": "Yuanming-Li",
                        "user": "Lymann",
                        "type": "user"
                    },
                    "name": "Yuan-Ming Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:30:39.978Z",
                    "hidden": false
                },
                {
                    "_id": "68db43edd2bf1f4b15ec7351",
                    "name": "Xihan Wei",
                    "hidden": false
                },
                {
                    "_id": "68db43edd2bf1f4b15ec7352",
                    "name": "Xiaohua Xie",
                    "hidden": false
                },
                {
                    "_id": "68db43edd2bf1f4b15ec7353",
                    "name": "Wei-Shi Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T13:43:55.000Z",
            "submittedOnDailyAt": "2025-09-30T01:18:04.944Z",
            "title": "LOVE-R1: Advancing Long Video Understanding with an Adaptive Zoom-in\n  Mechanism via Multi-Step Reasoning",
            "submittedOnDailyBy": {
                "_id": "67067633351e0c16a5c27497",
                "avatarUrl": "/avatars/356aa3431198c8931b820a714bcfb19d.svg",
                "isPro": false,
                "fullname": "Shenghao Fu",
                "user": "fushh7",
                "type": "user"
            },
            "summary": "Long video understanding is still challenging for recent Large Video-Language\nModels (LVLMs) due to the conflict between long-form temporal understanding and\ndetailed spatial perception. LVLMs with a uniform frame sampling mechanism,\nwhich samples frames with an equal frame size and fixed sampling rate,\ninevitably sacrifice either temporal clues or spatial details, resulting in\nsuboptimal solutions. To mitigate this dilemma, we propose LOVE-R1, a model\nthat can adaptively zoom in on a video clip. The model is first provided with\ndensely sampled frames but in a small resolution. If some spatial details are\nneeded, the model can zoom in on a clip of interest with a large frame\nresolution based on its reasoning until key visual information is obtained. The\nwhole process is implemented as a multi-step reasoning process. To train the\nreasoning ability, we first finetune the model on our collected 38k\nhigh-quality CoT data and enhance it with decoupled reinforcement finetuning.\nAs outcome rewards can not provide fine-grained process supervision, we\ndecouple multi-step reasoning into multiple single-step reasoning and optimize\nthe internal zoom-in ability explicitly. Experiments on long video\nunderstanding benchmarks show that our model with the slow-fast adaptive frame\nsampling mechanism achieves a great trade-off between sampling density and\nframe resolutions, and LOVE-R1 outperforms our baseline Qwen2.5-VL by an\naverage of 3.1% points across 4 common long video understanding benchmarks.",
            "upvotes": 5,
            "discussionId": "68db43edd2bf1f4b15ec7354",
            "ai_summary": "LOVE-R1, a model with adaptive frame sampling, enhances long video understanding by balancing temporal and spatial details through multi-step reasoning and decoupled reinforcement learning.",
            "ai_keywords": [
                "Large Video-Language Models",
                "LVLMs",
                "uniform frame sampling",
                "adaptive zoom",
                "densely sampled frames",
                "large frame resolution",
                "multi-step reasoning",
                "decoupled reinforcement finetuning",
                "slow-fast adaptive frame sampling"
            ]
        },
        "publishedAt": "2025-09-29T09:43:55.000Z",
        "title": "LOVE-R1: Advancing Long Video Understanding with an Adaptive Zoom-in\n  Mechanism via Multi-Step Reasoning",
        "summary": "Long video understanding is still challenging for recent Large Video-Language\nModels (LVLMs) due to the conflict between long-form temporal understanding and\ndetailed spatial perception. LVLMs with a uniform frame sampling mechanism,\nwhich samples frames with an equal frame size and fixed sampling rate,\ninevitably sacrifice either temporal clues or spatial details, resulting in\nsuboptimal solutions. To mitigate this dilemma, we propose LOVE-R1, a model\nthat can adaptively zoom in on a video clip. The model is first provided with\ndensely sampled frames but in a small resolution. If some spatial details are\nneeded, the model can zoom in on a clip of interest with a large frame\nresolution based on its reasoning until key visual information is obtained. The\nwhole process is implemented as a multi-step reasoning process. To train the\nreasoning ability, we first finetune the model on our collected 38k\nhigh-quality CoT data and enhance it with decoupled reinforcement finetuning.\nAs outcome rewards can not provide fine-grained process supervision, we\ndecouple multi-step reasoning into multiple single-step reasoning and optimize\nthe internal zoom-in ability explicitly. Experiments on long video\nunderstanding benchmarks show that our model with the slow-fast adaptive frame\nsampling mechanism achieves a great trade-off between sampling density and\nframe resolutions, and LOVE-R1 outperforms our baseline Qwen2.5-VL by an\naverage of 3.1% points across 4 common long video understanding benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24786.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67067633351e0c16a5c27497",
            "avatarUrl": "/avatars/356aa3431198c8931b820a714bcfb19d.svg",
            "fullname": "Shenghao Fu",
            "name": "fushh7",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "submitterOrganization": {
            "_id": "67d15cca6e2cf0e062dbfb54",
            "name": "AlibabaTongyiLab",
            "fullname": "TongyiLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.23371",
            "authors": [
                {
                    "_id": "68db3818d2bf1f4b15ec7290",
                    "user": {
                        "_id": "653f1d243bd61358055ad51d",
                        "avatarUrl": "/avatars/698c03b9a4bb69659d2ed594626e3895.svg",
                        "isPro": false,
                        "fullname": "junmingyang",
                        "user": "jmyang",
                        "type": "user"
                    },
                    "name": "Junming Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:34:49.878Z",
                    "hidden": false
                },
                {
                    "_id": "68db3818d2bf1f4b15ec7291",
                    "name": "Ning Xu",
                    "hidden": false
                },
                {
                    "_id": "68db3818d2bf1f4b15ec7292",
                    "name": "Biao Liu",
                    "hidden": false
                },
                {
                    "_id": "68db3818d2bf1f4b15ec7293",
                    "name": "Shiqi Qiao",
                    "hidden": false
                },
                {
                    "_id": "68db3818d2bf1f4b15ec7294",
                    "name": "Xin Geng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-27T15:38:24.000Z",
            "submittedOnDailyAt": "2025-09-30T00:28:47.911Z",
            "title": "Alignment through Meta-Weighted Online Sampling: Bridging the Gap\n  between Data Generation and Preference Optimization",
            "submittedOnDailyBy": {
                "_id": "653f1d243bd61358055ad51d",
                "avatarUrl": "/avatars/698c03b9a4bb69659d2ed594626e3895.svg",
                "isPro": false,
                "fullname": "junmingyang",
                "user": "jmyang",
                "type": "user"
            },
            "summary": "Preference optimization is crucial for aligning large language models (LLMs)\nwith human values and intentions. A significant challenge in this process is\nthe distribution mismatch between pre-collected offline preference data and the\nevolving model policy. Existing methods attempt to reduce this gap using static\nheuristics or decoupled online sampling strategies, but they often fail to\nadapt to the model's dynamic learning state. To bridge this gap, we propose\nMeta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework\nthat dynamically couples data generation with model training. MetaAPO employs a\nlightweight meta-learner, as an \"alignment gap estimator\", to evaluate the\npotential benefits of on-policy sampling in relation to offline data. This\nguides targeted online generation and assigns sample-wise meta-weights to the\noptimization objective, dynamically balancing the quality and distribution of\nonline and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench\ndemonstrate that MetaAPO consistently outperforms existing preference\noptimization approaches across various settings, while reducing 42% in online\nannotation costs.",
            "upvotes": 5,
            "discussionId": "68db3819d2bf1f4b15ec7295",
            "ai_summary": "Meta-Weighted Adaptive Preference Optimization (MetaAPO) dynamically balances online and offline data to align large language models with human preferences, outperforming existing methods and reducing annotation costs.",
            "ai_keywords": [
                "Meta-Weighted Adaptive Preference Optimization",
                "MetaAPO",
                "meta-learner",
                "alignment gap estimator",
                "on-policy sampling",
                "AlpacaEval 2",
                "Arena-Hard",
                "MT-Bench"
            ]
        },
        "publishedAt": "2025-09-27T11:38:24.000Z",
        "title": "Alignment through Meta-Weighted Online Sampling: Bridging the Gap\n  between Data Generation and Preference Optimization",
        "summary": "Preference optimization is crucial for aligning large language models (LLMs)\nwith human values and intentions. A significant challenge in this process is\nthe distribution mismatch between pre-collected offline preference data and the\nevolving model policy. Existing methods attempt to reduce this gap using static\nheuristics or decoupled online sampling strategies, but they often fail to\nadapt to the model's dynamic learning state. To bridge this gap, we propose\nMeta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework\nthat dynamically couples data generation with model training. MetaAPO employs a\nlightweight meta-learner, as an \"alignment gap estimator\", to evaluate the\npotential benefits of on-policy sampling in relation to offline data. This\nguides targeted online generation and assigns sample-wise meta-weights to the\noptimization objective, dynamically balancing the quality and distribution of\nonline and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench\ndemonstrate that MetaAPO consistently outperforms existing preference\noptimization approaches across various settings, while reducing 42% in online\nannotation costs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23371.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "653f1d243bd61358055ad51d",
            "avatarUrl": "/avatars/698c03b9a4bb69659d2ed594626e3895.svg",
            "fullname": "junmingyang",
            "name": "jmyang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.21953",
            "authors": [
                {
                    "_id": "68db399cd2bf1f4b15ec7297",
                    "user": {
                        "_id": "676a2ca72d7050defde9b25d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qNWwTpxhHfbRSHdtdDhQl.png",
                        "isPro": false,
                        "fullname": "Suger",
                        "user": "SugerWu",
                        "type": "user"
                    },
                    "name": "Tao Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-30T06:34:46.687Z",
                    "hidden": false
                },
                {
                    "_id": "68db399cd2bf1f4b15ec7298",
                    "name": "Yibo Jiang",
                    "hidden": false
                },
                {
                    "_id": "68db399cd2bf1f4b15ec7299",
                    "name": "Yehao Lu",
                    "hidden": false
                },
                {
                    "_id": "68db399cd2bf1f4b15ec729a",
                    "name": "Zhizhong Wang",
                    "hidden": false
                },
                {
                    "_id": "68db399cd2bf1f4b15ec729b",
                    "name": "Zeyi Huang",
                    "hidden": false
                },
                {
                    "_id": "68db399cd2bf1f4b15ec729c",
                    "name": "Zequn Qin",
                    "hidden": false
                },
                {
                    "_id": "68db399cd2bf1f4b15ec729d",
                    "name": "Xi Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T06:41:43.000Z",
            "submittedOnDailyAt": "2025-09-30T00:30:46.930Z",
            "title": "MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially\n  Disentangled Attention and Identity-Aware Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "676a2ca72d7050defde9b25d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qNWwTpxhHfbRSHdtdDhQl.png",
                "isPro": false,
                "fullname": "Suger",
                "user": "SugerWu",
                "type": "user"
            },
            "summary": "Multi-subject image generation aims to synthesize user-provided subjects in a\nsingle image while preserving subject fidelity, ensuring prompt consistency,\nand aligning with human aesthetic preferences. However, existing methods,\nparticularly those built on the In-Context-Learning paradigm, are limited by\ntheir reliance on simple reconstruction-based objectives, leading to both\nsevere attribute leakage that compromises subject fidelity and failing to align\nwith nuanced human preferences. To address this, we propose MultiCrafter, a\nframework that ensures high-fidelity, preference-aligned generation. First, we\nfind that the root cause of attribute leakage is a significant entanglement of\nattention between different subjects during the generation process. Therefore,\nwe introduce explicit positional supervision to explicitly separate attention\nregions for each subject, effectively mitigating attribute leakage. To enable\nthe model to accurately plan the attention region of different subjects in\ndiverse scenarios, we employ a Mixture-of-Experts architecture to enhance the\nmodel's capacity, allowing different experts to focus on different scenarios.\nFinally, we design a novel online reinforcement learning framework to align the\nmodel with human preferences, featuring a scoring mechanism to accurately\nassess multi-subject fidelity and a more stable training strategy tailored for\nthe MoE architecture. Experiments validate that our framework significantly\nimproves subject fidelity while aligning with human preferences better.",
            "upvotes": 5,
            "discussionId": "68db399dd2bf1f4b15ec729e",
            "projectPage": "https://wutao-cs.github.io/MultiCrafter/",
            "ai_summary": "MultiCrafter framework improves multi-subject image generation by addressing attribute leakage through explicit positional supervision, utilizing a Mixture-of-Experts architecture, and aligning with human preferences via online reinforcement learning.",
            "ai_keywords": [
                "multi-subject image generation",
                "attribute leakage",
                "explicit positional supervision",
                "Mixture-of-Experts architecture",
                "online reinforcement learning",
                "scoring mechanism"
            ]
        },
        "publishedAt": "2025-09-26T02:41:43.000Z",
        "title": "MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially\n  Disentangled Attention and Identity-Aware Reinforcement Learning",
        "summary": "Multi-subject image generation aims to synthesize user-provided subjects in a\nsingle image while preserving subject fidelity, ensuring prompt consistency,\nand aligning with human aesthetic preferences. However, existing methods,\nparticularly those built on the In-Context-Learning paradigm, are limited by\ntheir reliance on simple reconstruction-based objectives, leading to both\nsevere attribute leakage that compromises subject fidelity and failing to align\nwith nuanced human preferences. To address this, we propose MultiCrafter, a\nframework that ensures high-fidelity, preference-aligned generation. First, we\nfind that the root cause of attribute leakage is a significant entanglement of\nattention between different subjects during the generation process. Therefore,\nwe introduce explicit positional supervision to explicitly separate attention\nregions for each subject, effectively mitigating attribute leakage. To enable\nthe model to accurately plan the attention region of different subjects in\ndiverse scenarios, we employ a Mixture-of-Experts architecture to enhance the\nmodel's capacity, allowing different experts to focus on different scenarios.\nFinally, we design a novel online reinforcement learning framework to align the\nmodel with human preferences, featuring a scoring mechanism to accurately\nassess multi-subject fidelity and a more stable training strategy tailored for\nthe MoE architecture. Experiments validate that our framework significantly\nimproves subject fidelity while aligning with human preferences better.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21953.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "676a2ca72d7050defde9b25d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qNWwTpxhHfbRSHdtdDhQl.png",
            "fullname": "Suger",
            "name": "SugerWu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25050",
            "authors": [
                {
                    "_id": "68dc2b934159d1f2418f98f6",
                    "name": "Shuchen Xue",
                    "hidden": false
                },
                {
                    "_id": "68dc2b934159d1f2418f98f7",
                    "name": "Chongjian Ge",
                    "hidden": false
                },
                {
                    "_id": "68dc2b934159d1f2418f98f8",
                    "name": "Shilong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dc2b934159d1f2418f98f9",
                    "name": "Yichen Li",
                    "hidden": false
                },
                {
                    "_id": "68dc2b934159d1f2418f98fa",
                    "name": "Zhi-Ming Ma",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:02:20.000Z",
            "submittedOnDailyAt": "2025-09-30T17:47:08.761Z",
            "title": "Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion\n  Models",
            "submittedOnDailyBy": {
                "_id": "64cf5e81a2e7f9ff61eb3a0c",
                "avatarUrl": "/avatars/cbaa11daec9d4113bf7de93fe9b9ee86.svg",
                "isPro": false,
                "fullname": "scxue",
                "user": "Cauthyyy",
                "type": "user"
            },
            "summary": "Reinforcement Learning (RL) has emerged as a central paradigm for advancing\nLarge Language Models (LLMs), where pre-training and RL post-training share the\nsame log-likelihood formulation. In contrast, recent RL approaches for\ndiffusion models, most notably Denoising Diffusion Policy Optimization (DDPO),\noptimize an objective different from the pretraining objectives--score/flow\nmatching loss. In this work, we establish a novel theoretical analysis: DDPO is\nan implicit form of score/flow matching with noisy targets, which increases\nvariance and slows convergence. Building on this analysis, we introduce\nAdvantage Weighted Matching (AWM), a policy-gradient method for\ndiffusion. It uses the same score/flow-matching loss as pretraining to obtain a\nlower-variance objective and reweights each sample by its advantage. In effect,\nAWM raises the influence of high-reward samples and suppresses low-reward ones\nwhile keeping the modeling objective identical to pretraining. This unifies\npretraining and RL conceptually and practically, is consistent with\npolicy-gradient theory, reduces variance, and yields faster convergence. This\nsimple yet effective design yields substantial benefits: on GenEval, OCR, and\nPickScore benchmarks, AWM delivers up to a 24times speedup over Flow-GRPO\n(which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX,\nwithout compromising generation quality. Code is available at\nhttps://github.com/scxue/advantage_weighted_matching.",
            "upvotes": 4,
            "discussionId": "68dc2b934159d1f2418f98fb",
            "ai_summary": "Advantage Weighted Matching (AWM) is a policy-gradient method for diffusion models that reduces variance and speeds up convergence compared to Denoising Diffusion Policy Optimization (DDPO).",
            "ai_keywords": [
                "Reinforcement Learning",
                "Large Language Models",
                "Denoising Diffusion Policy Optimization",
                "DDPO",
                "score/flow matching",
                "noisy targets",
                "policy-gradient method",
                "Advantage Weighted Matching",
                "AWM",
                "GenEval",
                "OCR",
                "PickScore",
                "Stable Diffusion",
                "FLUX"
            ]
        },
        "publishedAt": "2025-09-29T13:02:20.000Z",
        "title": "Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion\n  Models",
        "summary": "Reinforcement Learning (RL) has emerged as a central paradigm for advancing\nLarge Language Models (LLMs), where pre-training and RL post-training share the\nsame log-likelihood formulation. In contrast, recent RL approaches for\ndiffusion models, most notably Denoising Diffusion Policy Optimization (DDPO),\noptimize an objective different from the pretraining objectives--score/flow\nmatching loss. In this work, we establish a novel theoretical analysis: DDPO is\nan implicit form of score/flow matching with noisy targets, which increases\nvariance and slows convergence. Building on this analysis, we introduce\nAdvantage Weighted Matching (AWM), a policy-gradient method for\ndiffusion. It uses the same score/flow-matching loss as pretraining to obtain a\nlower-variance objective and reweights each sample by its advantage. In effect,\nAWM raises the influence of high-reward samples and suppresses low-reward ones\nwhile keeping the modeling objective identical to pretraining. This unifies\npretraining and RL conceptually and practically, is consistent with\npolicy-gradient theory, reduces variance, and yields faster convergence. This\nsimple yet effective design yields substantial benefits: on GenEval, OCR, and\nPickScore benchmarks, AWM delivers up to a 24times speedup over Flow-GRPO\n(which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX,\nwithout compromising generation quality. Code is available at\nhttps://github.com/scxue/advantage_weighted_matching.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25050.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64cf5e81a2e7f9ff61eb3a0c",
            "avatarUrl": "/avatars/cbaa11daec9d4113bf7de93fe9b9ee86.svg",
            "fullname": "scxue",
            "name": "Cauthyyy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 0
        },
        "submitterOrganization": {
            "_id": "61e5d14f77496de0a6d95c6b",
            "name": "adobe",
            "fullname": "Adobe",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.24317",
            "authors": [
                {
                    "_id": "68dc47b84159d1f2418f9916",
                    "name": "Xianhang Li",
                    "hidden": false
                },
                {
                    "_id": "68dc47b84159d1f2418f9917",
                    "name": "Chen Huang",
                    "hidden": false
                },
                {
                    "_id": "68dc47b84159d1f2418f9918",
                    "name": "Chun-Liang Li",
                    "hidden": false
                },
                {
                    "_id": "68dc47b84159d1f2418f9919",
                    "name": "Eran Malach",
                    "hidden": false
                },
                {
                    "_id": "68dc47b84159d1f2418f991a",
                    "name": "Josh Susskind",
                    "hidden": false
                },
                {
                    "_id": "68dc47b84159d1f2418f991b",
                    "name": "Vimal Thilak",
                    "hidden": false
                },
                {
                    "_id": "68dc47b84159d1f2418f991c",
                    "name": "Etai Littwin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T05:55:17.000Z",
            "submittedOnDailyAt": "2025-09-30T20:19:38.238Z",
            "title": "Rethinking JEPA: Compute-Efficient Video SSL with Frozen Teachers",
            "submittedOnDailyBy": {
                "_id": "645e72d4b5c9a8666d0ddc1c",
                "avatarUrl": "/avatars/f0f1684731eb5ae148eadf841cceb18e.svg",
                "isPro": true,
                "fullname": "xhl",
                "user": "Xianhang",
                "type": "user"
            },
            "summary": "Video Joint Embedding Predictive Architectures (V-JEPA) learn generalizable\noff-the-shelf video representation by predicting masked regions in latent space\nwith an exponential moving average (EMA)-updated teacher. While EMA prevents\nrepresentation collapse, it complicates scalable model selection and couples\nteacher and student architectures. We revisit masked-latent prediction and show\nthat a frozen teacher suffices. Concretely, we (i) train a target encoder with\na simple pixel-reconstruction objective under V-JEPA masking, then (ii) freeze\nit and train a student to predict the teacher's latents on masked regions. This\nleads to a two-stage, unregularized scheme that we refer to as SALT\n(Static-teacher Asymmetric Latent Training). SALT decouples optimization into\npixel reconstruction (teacher) and masked latent prediction (student),\nincreasing transparency, efficiency, and scalability while preserving the\nability of representation to generalize under frozen evaluation. Empirically,\nour student models outperform recently proposed V-JEPA 2 encoders under frozen\nbackbone evaluation across diverse benchmarks. They are also more\ncompute-optimal: at matched pretraining FLOPs, our method achieves higher\nprobing accuracy, and its scaling curves dominate V-JEPA's accuracy-FLOPs\nPareto frontier. Finally, we find that student quality is remarkably robust to\nteacher quality: high-performing students emerge even with small, sub-optimal\nteachers. This points to a compute budget allocation that should overwhelmingly\nfavor the student. These results position SALT as a simple, scalable, and\ncompute-efficient alternative to EMA-based self-distillation for video\nrepresentation learning.",
            "upvotes": 4,
            "discussionId": "68dc47b84159d1f2418f991d",
            "ai_summary": "SALT, a two-stage training method using a frozen teacher, achieves better video representation learning with higher efficiency and scalability compared to EMA-based approaches.",
            "ai_keywords": [
                "Video Joint Embedding Predictive Architectures",
                "V-JEPA",
                "masked regions",
                "latent space",
                "exponential moving average",
                "EMA",
                "representation collapse",
                "pixel-reconstruction objective",
                "SALT",
                "Static-teacher Asymmetric Latent Training",
                "probing accuracy",
                "Pareto frontier"
            ]
        },
        "publishedAt": "2025-09-29T01:55:17.000Z",
        "title": "Rethinking JEPA: Compute-Efficient Video SSL with Frozen Teachers",
        "summary": "Video Joint Embedding Predictive Architectures (V-JEPA) learn generalizable\noff-the-shelf video representation by predicting masked regions in latent space\nwith an exponential moving average (EMA)-updated teacher. While EMA prevents\nrepresentation collapse, it complicates scalable model selection and couples\nteacher and student architectures. We revisit masked-latent prediction and show\nthat a frozen teacher suffices. Concretely, we (i) train a target encoder with\na simple pixel-reconstruction objective under V-JEPA masking, then (ii) freeze\nit and train a student to predict the teacher's latents on masked regions. This\nleads to a two-stage, unregularized scheme that we refer to as SALT\n(Static-teacher Asymmetric Latent Training). SALT decouples optimization into\npixel reconstruction (teacher) and masked latent prediction (student),\nincreasing transparency, efficiency, and scalability while preserving the\nability of representation to generalize under frozen evaluation. Empirically,\nour student models outperform recently proposed V-JEPA 2 encoders under frozen\nbackbone evaluation across diverse benchmarks. They are also more\ncompute-optimal: at matched pretraining FLOPs, our method achieves higher\nprobing accuracy, and its scaling curves dominate V-JEPA's accuracy-FLOPs\nPareto frontier. Finally, we find that student quality is remarkably robust to\nteacher quality: high-performing students emerge even with small, sub-optimal\nteachers. This points to a compute budget allocation that should overwhelmingly\nfavor the student. These results position SALT as a simple, scalable, and\ncompute-efficient alternative to EMA-based self-distillation for video\nrepresentation learning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24317.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "645e72d4b5c9a8666d0ddc1c",
            "avatarUrl": "/avatars/f0f1684731eb5ae148eadf841cceb18e.svg",
            "fullname": "xhl",
            "name": "Xianhang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "submitterOrganization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
    }
]