[
    {
        "paper": {
            "id": "2509.24002",
            "authors": [
                {
                    "_id": "68dc9eff4159d1f2418f9b97",
                    "user": {
                        "_id": "626d268d5f7327906f05cad1",
                        "avatarUrl": "/avatars/18bda74612a3ee63a17f991bcc695106.svg",
                        "isPro": true,
                        "fullname": "Zijian Wu",
                        "user": "Jakumetsu",
                        "type": "user"
                    },
                    "name": "Zijian Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:21:21.979Z",
                    "hidden": false
                },
                {
                    "_id": "68dc9eff4159d1f2418f9b98",
                    "name": "Xiangyan Liu",
                    "hidden": false
                },
                {
                    "_id": "68dc9eff4159d1f2418f9b99",
                    "name": "Xinyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dc9eff4159d1f2418f9b9a",
                    "user": {
                        "_id": "65d95ea62a6348b052519214",
                        "avatarUrl": "/avatars/b1d2fc97ad5df7cf187be48daa0e1f34.svg",
                        "isPro": false,
                        "fullname": "chen",
                        "user": "chenlingjun",
                        "type": "user"
                    },
                    "name": "Lingjun Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T15:29:18.308Z",
                    "hidden": false
                },
                {
                    "_id": "68dc9eff4159d1f2418f9b9b",
                    "name": "Fanqing Meng",
                    "hidden": false
                },
                {
                    "_id": "68dc9eff4159d1f2418f9b9c",
                    "user": {
                        "_id": "666fe1a5b07525f0bde69c27",
                        "avatarUrl": "/avatars/bb98ab0b974c8fe011739baa8dadd91a.svg",
                        "isPro": false,
                        "fullname": "Lingxiao Du",
                        "user": "Cierra0506",
                        "type": "user"
                    },
                    "name": "Lingxiao Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:21:32.767Z",
                    "hidden": false
                },
                {
                    "_id": "68dc9eff4159d1f2418f9b9d",
                    "name": "Yiran Zhao",
                    "hidden": false
                },
                {
                    "_id": "68dc9eff4159d1f2418f9b9e",
                    "name": "Fanshi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dc9eff4159d1f2418f9b9f",
                    "name": "Yaoqi Ye",
                    "hidden": false
                },
                {
                    "_id": "68dc9eff4159d1f2418f9ba0",
                    "name": "Jiawei Wang",
                    "hidden": false
                },
                {
                    "_id": "68dc9eff4159d1f2418f9ba1",
                    "name": "Zirui Wang",
                    "hidden": false
                },
                {
                    "_id": "68dc9eff4159d1f2418f9ba2",
                    "name": "Jinjie Ni",
                    "hidden": false
                },
                {
                    "_id": "68dc9eff4159d1f2418f9ba3",
                    "name": "Yufan Yang",
                    "hidden": false
                },
                {
                    "_id": "68dc9eff4159d1f2418f9ba4",
                    "user": {
                        "_id": "64189e45d13ffa40812ac44c",
                        "avatarUrl": "/avatars/472ac989846f7c371067bfbeceeffb05.svg",
                        "isPro": false,
                        "fullname": "Arvin Xu",
                        "user": "arvinxx",
                        "type": "user"
                    },
                    "name": "Arvin Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T15:29:20.221Z",
                    "hidden": false
                },
                {
                    "_id": "68dc9eff4159d1f2418f9ba5",
                    "name": "Michael Qizhe Shieh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T17:53:27.000Z",
            "submittedOnDailyAt": "2025-10-01T01:58:54.337Z",
            "title": "MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP\n  Use",
            "submittedOnDailyBy": {
                "_id": "626d268d5f7327906f05cad1",
                "avatarUrl": "/avatars/18bda74612a3ee63a17f991bcc695106.svg",
                "isPro": true,
                "fullname": "Zijian Wu",
                "user": "Jakumetsu",
                "type": "user"
            },
            "summary": "MCP standardizes how LLMs interact with external systems, forming the\nfoundation for general agents. However, existing MCP benchmarks remain narrow\nin scope: they focus on read-heavy tasks or tasks with limited interaction\ndepth, and fail to capture the complexity and realism of real-world workflows.\nTo address this gap, we propose MCPMark, a benchmark designed to evaluate MCP\nuse in a more realistic and comprehensive manner. It consists of 127\nhigh-quality tasks collaboratively created by domain experts and AI agents.\nEach task begins with a curated initial state and includes a programmatic\nscript for automatic verification. These tasks demand richer and more diverse\ninteractions with the environment, involving a broad range of create, read,\nupdate, and delete (CRUD) operations. We conduct a comprehensive evaluation of\ncutting-edge LLMs using a minimal agent framework that operates in a\ntool-calling loop. Empirical results show that the best-performing model,\ngpt-5-medium, reaches only 52.56\\% pass@1 and 33.86\\% pass^4, while other\nwidely regarded strong models, including claude-sonnet-4 and o3, fall below\n30\\% pass@1 and 15\\% pass^4. On average, LLMs require 16.2 execution\nturns and 17.4 tool calls per task, significantly surpassing those in\nprevious MCP benchmarks and highlighting the stress-testing nature of MCPMark.",
            "upvotes": 122,
            "discussionId": "68dc9eff4159d1f2418f9ba6",
            "projectPage": "https://mcpmark.ai/",
            "githubRepo": "https://github.com/eval-sys/mcpmark",
            "ai_summary": "MCPMark is a comprehensive benchmark for evaluating MCP use in real-world workflows, featuring diverse tasks that require richer interactions with the environment, and reveals that current LLMs perform poorly on these tasks.",
            "ai_keywords": [
                "MCP",
                "LLMs",
                "general agents",
                "MCP benchmarks",
                "MCPMark",
                "high-quality tasks",
                "domain experts",
                "AI agents",
                "initial state",
                "programmatic script",
                "automatic verification",
                "CRUD operations",
                "minimal agent framework",
                "tool-calling loop",
                "gpt-5-medium",
                "claude-sonnet-4",
                "o3",
                "pass@1",
                "pass^4",
                "execution turns",
                "tool calls"
            ],
            "githubStars": 184
        },
        "publishedAt": "2025-09-28T13:53:27.000Z",
        "title": "MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP\n  Use",
        "summary": "MCP standardizes how LLMs interact with external systems, forming the\nfoundation for general agents. However, existing MCP benchmarks remain narrow\nin scope: they focus on read-heavy tasks or tasks with limited interaction\ndepth, and fail to capture the complexity and realism of real-world workflows.\nTo address this gap, we propose MCPMark, a benchmark designed to evaluate MCP\nuse in a more realistic and comprehensive manner. It consists of 127\nhigh-quality tasks collaboratively created by domain experts and AI agents.\nEach task begins with a curated initial state and includes a programmatic\nscript for automatic verification. These tasks demand richer and more diverse\ninteractions with the environment, involving a broad range of create, read,\nupdate, and delete (CRUD) operations. We conduct a comprehensive evaluation of\ncutting-edge LLMs using a minimal agent framework that operates in a\ntool-calling loop. Empirical results show that the best-performing model,\ngpt-5-medium, reaches only 52.56\\% pass@1 and 33.86\\% pass^4, while other\nwidely regarded strong models, including claude-sonnet-4 and o3, fall below\n30\\% pass@1 and 15\\% pass^4. On average, LLMs require 16.2 execution\nturns and 17.4 tool calls per task, significantly surpassing those in\nprevious MCP benchmarks and highlighting the stress-testing nature of MCPMark.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24002.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "626d268d5f7327906f05cad1",
            "avatarUrl": "/avatars/18bda74612a3ee63a17f991bcc695106.svg",
            "fullname": "Zijian Wu",
            "name": "Jakumetsu",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "submitterOrganization": {
            "_id": "6508ab2b349930913196378b",
            "name": "NationalUniversityofSingapore",
            "fullname": "National University of Singapore",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.26507",
            "authors": [
                {
                    "_id": "68dc87954159d1f2418f9a13",
                    "name": "Adrian Kosowski",
                    "hidden": false
                },
                {
                    "_id": "68dc87954159d1f2418f9a14",
                    "user": {
                        "_id": "6763fd76d63e4b348eac8290",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/4ZtRLbXBmrXVQgpQzPpDm.png",
                        "isPro": false,
                        "fullname": "Przemysław Uznański",
                        "user": "izulin",
                        "type": "user"
                    },
                    "name": "Przemysław Uznański",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-10-01T15:32:27.465Z",
                    "hidden": false
                },
                {
                    "_id": "68dc87954159d1f2418f9a15",
                    "user": {
                        "_id": "651310e3b7994cff61d71320",
                        "avatarUrl": "/avatars/161cb412c7cb284fa7483e790807e4d3.svg",
                        "isPro": false,
                        "fullname": "Jan Chorowski",
                        "user": "janchorowski",
                        "type": "user"
                    },
                    "name": "Jan Chorowski",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:22:24.524Z",
                    "hidden": false
                },
                {
                    "_id": "68dc87954159d1f2418f9a16",
                    "name": "Zuzanna Stamirowska",
                    "hidden": false
                },
                {
                    "_id": "68dc87954159d1f2418f9a17",
                    "name": "Michał Bartoszkiewicz",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T16:49:01.000Z",
            "submittedOnDailyAt": "2025-10-01T09:46:19.334Z",
            "title": "The Dragon Hatchling: The Missing Link between the Transformer and\n  Models of the Brain",
            "submittedOnDailyBy": {
                "_id": "651310e3b7994cff61d71320",
                "avatarUrl": "/avatars/161cb412c7cb284fa7483e790807e4d3.svg",
                "isPro": false,
                "fullname": "Jan Chorowski",
                "user": "janchorowski",
                "type": "user"
            },
            "summary": "The relationship between computing systems and the brain has served as\nmotivation for pioneering theoreticians since John von Neumann and Alan Turing.\nUniform, scale-free biological networks, such as the brain, have powerful\nproperties, including generalizing over time, which is the main barrier for\nMachine Learning on the path to Universal Reasoning Models.\n  We introduce `Dragon Hatchling' (BDH), a new Large Language Model\narchitecture based on a scale-free biologically inspired network of \\n\nlocally-interacting neuron particles. BDH couples strong theoretical\nfoundations and inherent interpretability without sacrificing Transformer-like\nperformance.\n  BDH is a practical, performant state-of-the-art attention-based state space\nsequence learning architecture. In addition to being a graph model, BDH admits\na GPU-friendly formulation. It exhibits Transformer-like scaling laws:\nempirically BDH rivals GPT2 performance on language and translation tasks, at\nthe same number of parameters (10M to 1B), for the same training data.\n  BDH can be represented as a brain model. The working memory of BDH during\ninference entirely relies on synaptic plasticity with Hebbian learning using\nspiking neurons. We confirm empirically that specific, individual synapses\nstrengthen connection whenever BDH hears or reasons about a specific concept\nwhile processing language inputs. The neuron interaction network of BDH is a\ngraph of high modularity with heavy-tailed degree distribution. The BDH model\nis biologically plausible, explaining one possible mechanism which human\nneurons could use to achieve speech.\n  BDH is designed for interpretability. Activation vectors of BDH are sparse\nand positive. We demonstrate monosemanticity in BDH on language tasks.\nInterpretability of state, which goes beyond interpretability of neurons and\nmodel parameters, is an inherent feature of the BDH architecture.",
            "upvotes": 105,
            "discussionId": "68dc87954159d1f2418f9a18",
            "githubRepo": "https://github.com/pathwaycom/bdh",
            "ai_summary": "BDH, a biologically inspired Large Language Model, combines scale-free network architecture with Hebbian learning to achieve Transformer-like performance while maintaining interpretability.",
            "ai_keywords": [
                "Large Language Model",
                "scale-free network",
                "Hebbian learning",
                "synaptic plasticity",
                "spiking neurons",
                "graph model",
                "GPU-friendly",
                "Transformer-like scaling laws",
                "monosemanticity",
                "interpretability"
            ],
            "githubStars": 447
        },
        "publishedAt": "2025-09-30T12:49:01.000Z",
        "title": "The Dragon Hatchling: The Missing Link between the Transformer and\n  Models of the Brain",
        "summary": "The relationship between computing systems and the brain has served as\nmotivation for pioneering theoreticians since John von Neumann and Alan Turing.\nUniform, scale-free biological networks, such as the brain, have powerful\nproperties, including generalizing over time, which is the main barrier for\nMachine Learning on the path to Universal Reasoning Models.\n  We introduce `Dragon Hatchling' (BDH), a new Large Language Model\narchitecture based on a scale-free biologically inspired network of \\n\nlocally-interacting neuron particles. BDH couples strong theoretical\nfoundations and inherent interpretability without sacrificing Transformer-like\nperformance.\n  BDH is a practical, performant state-of-the-art attention-based state space\nsequence learning architecture. In addition to being a graph model, BDH admits\na GPU-friendly formulation. It exhibits Transformer-like scaling laws:\nempirically BDH rivals GPT2 performance on language and translation tasks, at\nthe same number of parameters (10M to 1B), for the same training data.\n  BDH can be represented as a brain model. The working memory of BDH during\ninference entirely relies on synaptic plasticity with Hebbian learning using\nspiking neurons. We confirm empirically that specific, individual synapses\nstrengthen connection whenever BDH hears or reasons about a specific concept\nwhile processing language inputs. The neuron interaction network of BDH is a\ngraph of high modularity with heavy-tailed degree distribution. The BDH model\nis biologically plausible, explaining one possible mechanism which human\nneurons could use to achieve speech.\n  BDH is designed for interpretability. Activation vectors of BDH are sparse\nand positive. We demonstrate monosemanticity in BDH on language tasks.\nInterpretability of state, which goes beyond interpretability of neurons and\nmodel parameters, is an inherent feature of the BDH architecture.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26507.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "651310e3b7994cff61d71320",
            "avatarUrl": "/avatars/161cb412c7cb284fa7483e790807e4d3.svg",
            "fullname": "Jan Chorowski",
            "name": "janchorowski",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "submitterOrganization": {
            "_id": "6697b1a8b981e21b48bc575f",
            "name": "pathwaycom",
            "fullname": "Pathway",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6697aead07b36ccd016f0888/WRvZQPV-Md9_vMZofeA-u.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25541",
            "authors": [
                {
                    "_id": "68dc90f34159d1f2418f9abf",
                    "name": "Qinsi Wang",
                    "hidden": false
                },
                {
                    "_id": "68dc90f34159d1f2418f9ac0",
                    "user": {
                        "_id": "635e3a76106f984574c36409",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
                        "isPro": false,
                        "fullname": "Bo Liu",
                        "user": "Benjamin-eecs",
                        "type": "user"
                    },
                    "name": "Bo Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:21:49.246Z",
                    "hidden": false
                },
                {
                    "_id": "68dc90f34159d1f2418f9ac1",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:21:44.939Z",
                    "hidden": false
                },
                {
                    "_id": "68dc90f34159d1f2418f9ac2",
                    "name": "Jing Shi",
                    "hidden": false
                },
                {
                    "_id": "68dc90f34159d1f2418f9ac3",
                    "name": "Yueqian Lin",
                    "hidden": false
                },
                {
                    "_id": "68dc90f34159d1f2418f9ac4",
                    "name": "Yiran Chen",
                    "hidden": false
                },
                {
                    "_id": "68dc90f34159d1f2418f9ac5",
                    "name": "Hai Helen Li",
                    "hidden": false
                },
                {
                    "_id": "68dc90f34159d1f2418f9ac6",
                    "user": {
                        "_id": "66274e02348a5304435dc9cc",
                        "avatarUrl": "/avatars/bda87559cd497c310597c2fc8430b31f.svg",
                        "isPro": false,
                        "fullname": "Kun Wan",
                        "user": "timecuriosity",
                        "type": "user"
                    },
                    "name": "Kun Wan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:21:47.064Z",
                    "hidden": false
                },
                {
                    "_id": "68dc90f34159d1f2418f9ac7",
                    "name": "Wentian Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T21:55:55.000Z",
            "submittedOnDailyAt": "2025-10-01T00:55:06.135Z",
            "title": "Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified\n  Self-Play",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Although reinforcement learning (RL) can effectively enhance the reasoning\ncapabilities of vision-language models (VLMs), current methods remain heavily\ndependent on labor-intensive datasets that require extensive manual\nconstruction and verification, leading to extremely high training costs and\nconsequently constraining the practical deployment of VLMs. To address this\nchallenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM\nself-improvement through competitive visual games generated from arbitrary\nimage pairs. Specifically, Vision-Zero encompasses three main attributes: (1)\nStrategic Self-Play Framework: Vision-Zero trains VLMs in \"Who Is the\nSpy\"-style games, where the models engage in strategic reasoning and actions\nacross multiple roles. Through interactive gameplay, models autonomously\ngenerate their training data without human annotation. (2) Gameplay from\nArbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate\ngames from arbitrary images, thereby enhancing the model's reasoning ability\nacross diverse domains and showing strong generalization to different tasks. We\ndemonstrate this versatility using three distinct types of image datasets:\nCLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable\nPerformance Gain: We introduce Iterative Self-Play Policy Optimization\n(Iterative-SPO), a novel training algorithm that alternates between Self-Play\nand reinforcement learning with verifiable rewards (RLVR), mitigating the\nperformance plateau often seen in self-play-only training and achieving\nsustained long-term improvements. Despite using label-free data, Vision-Zero\nachieves state-of-the-art performance on reasoning, chart question answering,\nand vision-centric understanding tasks, surpassing other annotation-based\nmethods. Models and code has been released at\nhttps://github.com/wangqinsi1/Vision-Zero.",
            "upvotes": 103,
            "discussionId": "68dc90f34159d1f2418f9ac8",
            "githubRepo": "https://github.com/wangqinsi1/Vision-Zero",
            "ai_summary": "Vision-Zero is a domain-agnostic framework that enhances vision-language models through self-improvement in competitive visual games, using Iterative Self-Play Policy Optimization and achieving state-of-the-art performance without human annotation.",
            "ai_keywords": [
                "reinforcement learning",
                "vision-language models",
                "strategic self-play framework",
                "self-play",
                "reinforcement learning with verifiable rewards",
                "Iterative Self-Play Policy Optimization"
            ],
            "githubStars": 22
        },
        "publishedAt": "2025-09-29T17:55:55.000Z",
        "title": "Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified\n  Self-Play",
        "summary": "Although reinforcement learning (RL) can effectively enhance the reasoning\ncapabilities of vision-language models (VLMs), current methods remain heavily\ndependent on labor-intensive datasets that require extensive manual\nconstruction and verification, leading to extremely high training costs and\nconsequently constraining the practical deployment of VLMs. To address this\nchallenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM\nself-improvement through competitive visual games generated from arbitrary\nimage pairs. Specifically, Vision-Zero encompasses three main attributes: (1)\nStrategic Self-Play Framework: Vision-Zero trains VLMs in \"Who Is the\nSpy\"-style games, where the models engage in strategic reasoning and actions\nacross multiple roles. Through interactive gameplay, models autonomously\ngenerate their training data without human annotation. (2) Gameplay from\nArbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate\ngames from arbitrary images, thereby enhancing the model's reasoning ability\nacross diverse domains and showing strong generalization to different tasks. We\ndemonstrate this versatility using three distinct types of image datasets:\nCLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable\nPerformance Gain: We introduce Iterative Self-Play Policy Optimization\n(Iterative-SPO), a novel training algorithm that alternates between Self-Play\nand reinforcement learning with verifiable rewards (RLVR), mitigating the\nperformance plateau often seen in self-play-only training and achieving\nsustained long-term improvements. Despite using label-free data, Vision-Zero\nachieves state-of-the-art performance on reasoning, chart question answering,\nand vision-centric understanding tasks, surpassing other annotation-based\nmethods. Models and code has been released at\nhttps://github.com/wangqinsi1/Vision-Zero.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25541.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 115
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.23873",
            "authors": [
                {
                    "_id": "68dbf6444159d1f2418f9822",
                    "user": {
                        "_id": "66968099c952e09a4cb29f78",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66968099c952e09a4cb29f78/n90NI2R3E9_RqCyMjDCQF.webp",
                        "isPro": false,
                        "fullname": "Wang",
                        "user": "Steven-Shaobo",
                        "type": "user"
                    },
                    "name": "Shaobo Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:23:24.707Z",
                    "hidden": false
                },
                {
                    "_id": "68dbf6444159d1f2418f9823",
                    "user": {
                        "_id": "68355c5ec0003bc40230b3f2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WZQ5f8oqpni0i-D3a5R-P.png",
                        "isPro": false,
                        "fullname": "jasmineWang",
                        "user": "Jessamine",
                        "type": "user"
                    },
                    "name": "Jiaming Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:23:32.506Z",
                    "hidden": false
                },
                {
                    "_id": "68dbf6444159d1f2418f9824",
                    "user": {
                        "_id": "67bc471acda19f7ad35a2f20",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67bc471acda19f7ad35a2f20/_dbTYE3f4XF9_OY1qXHhw.jpeg",
                        "isPro": false,
                        "fullname": "Jiajun Zhang",
                        "user": "JiajunZhang",
                        "type": "user"
                    },
                    "name": "Jiajun Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:23:16.205Z",
                    "hidden": false
                },
                {
                    "_id": "68dbf6444159d1f2418f9825",
                    "name": "Cong Wang",
                    "hidden": false
                },
                {
                    "_id": "68dbf6444159d1f2418f9826",
                    "name": "Yue Min",
                    "hidden": false
                },
                {
                    "_id": "68dbf6444159d1f2418f9827",
                    "name": "Zichen Wen",
                    "hidden": false
                },
                {
                    "_id": "68dbf6444159d1f2418f9828",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "68dbf6444159d1f2418f9829",
                    "name": "Huiqiang Jiang",
                    "hidden": false
                },
                {
                    "_id": "68dbf6444159d1f2418f982a",
                    "name": "Junyang Lin",
                    "hidden": false
                },
                {
                    "_id": "68dbf6444159d1f2418f982b",
                    "name": "Dayiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68dbf6444159d1f2418f982c",
                    "name": "Linfeng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T13:27:38.000Z",
            "submittedOnDailyAt": "2025-10-01T09:12:46.902Z",
            "title": "Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token\n  Pruning for Efficient Supervised Fine-Tuning",
            "submittedOnDailyBy": {
                "_id": "68355c5ec0003bc40230b3f2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WZQ5f8oqpni0i-D3a5R-P.png",
                "isPro": false,
                "fullname": "jasmineWang",
                "user": "Jessamine",
                "type": "user"
            },
            "summary": "As supervised fine-tuning (SFT) evolves from a lightweight post-training step\ninto a compute-intensive phase rivaling mid-training in scale, data efficiency\nhas become critical for aligning large language models (LLMs) under tight\nbudgets. Existing data pruning methods suffer from a fragmented design: they\noperate either at the sample level or the token level in isolation, failing to\njointly optimize both dimensions. This disconnect leads to significant\ninefficiencies--high-value samples may still contain redundant tokens, while\ntoken-level pruning often discards crucial instructional or corrective signals\nembedded in individual examples. To address this bottleneck, we introduce the\nError-Uncertainty (EU) Plane, a diagnostic framework that jointly characterizes\nthe heterogeneous utility of training data across samples and tokens. Guided by\nthis insight, we propose Quadrant-based Tuning (Q-Tuning), a unified framework\nthat strategically coordinates sample pruning and token pruning. Q-Tuning\nemploys a two-stage strategy: first, it performs sample-level triage to retain\nexamples rich in informative misconceptions or calibration signals; second, it\napplies an asymmetric token-pruning policy, using a context-aware scoring\nmechanism to trim less salient tokens exclusively from misconception samples\nwhile preserving calibration samples in their entirety. Our method sets a new\nstate of the art across five diverse benchmarks. Remarkably, on SmolLM2-1.7B,\nQ-Tuning achieves a +38\\% average improvement over the full-data SFT baseline\nusing only 12.5\\% of the original training data. As the first dynamic pruning\napproach to consistently outperform full-data training, Q-Tuning provides a\npractical and scalable blueprint for maximizing data utilization in\nbudget-constrained LLM SFT.",
            "upvotes": 57,
            "discussionId": "68dbf6454159d1f2418f982d",
            "projectPage": "https://gszfwsb.github.io/Q-tuning/",
            "ai_summary": "Quadrant-based Tuning (Q-Tuning) optimizes both sample and token pruning in supervised fine-tuning of large language models, achieving superior performance with reduced data.",
            "ai_keywords": [
                "supervised fine-tuning",
                "data efficiency",
                "large language models",
                "data pruning",
                "sample-level pruning",
                "token-level pruning",
                "Error-Uncertainty (EU) Plane",
                "Quadrant-based Tuning",
                "sample-level triage",
                "asymmetric token-pruning",
                "context-aware scoring mechanism",
                "misconception samples",
                "calibration samples",
                "dynamic pruning"
            ]
        },
        "publishedAt": "2025-09-28T09:27:38.000Z",
        "title": "Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token\n  Pruning for Efficient Supervised Fine-Tuning",
        "summary": "As supervised fine-tuning (SFT) evolves from a lightweight post-training step\ninto a compute-intensive phase rivaling mid-training in scale, data efficiency\nhas become critical for aligning large language models (LLMs) under tight\nbudgets. Existing data pruning methods suffer from a fragmented design: they\noperate either at the sample level or the token level in isolation, failing to\njointly optimize both dimensions. This disconnect leads to significant\ninefficiencies--high-value samples may still contain redundant tokens, while\ntoken-level pruning often discards crucial instructional or corrective signals\nembedded in individual examples. To address this bottleneck, we introduce the\nError-Uncertainty (EU) Plane, a diagnostic framework that jointly characterizes\nthe heterogeneous utility of training data across samples and tokens. Guided by\nthis insight, we propose Quadrant-based Tuning (Q-Tuning), a unified framework\nthat strategically coordinates sample pruning and token pruning. Q-Tuning\nemploys a two-stage strategy: first, it performs sample-level triage to retain\nexamples rich in informative misconceptions or calibration signals; second, it\napplies an asymmetric token-pruning policy, using a context-aware scoring\nmechanism to trim less salient tokens exclusively from misconception samples\nwhile preserving calibration samples in their entirety. Our method sets a new\nstate of the art across five diverse benchmarks. Remarkably, on SmolLM2-1.7B,\nQ-Tuning achieves a +38\\% average improvement over the full-data SFT baseline\nusing only 12.5\\% of the original training data. As the first dynamic pruning\napproach to consistently outperform full-data training, Q-Tuning provides a\npractical and scalable blueprint for maximizing data utilization in\nbudget-constrained LLM SFT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23873.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "68355c5ec0003bc40230b3f2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WZQ5f8oqpni0i-D3a5R-P.png",
            "fullname": "jasmineWang",
            "name": "Jessamine",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "submitterOrganization": {
            "_id": "68be41370a3fcebdcad6516a",
            "name": "alibabagroup",
            "fullname": "alibaba",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68be3ab7e52df040b2cf80dc/li4G29u_EGswyTN1Sm_Kq.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25760",
            "authors": [
                {
                    "_id": "68dc90b84159d1f2418f9aa7",
                    "name": "Zhepei Wei",
                    "hidden": false
                },
                {
                    "_id": "68dc90b84159d1f2418f9aa8",
                    "name": "Xiao Yang",
                    "hidden": false
                },
                {
                    "_id": "68dc90b84159d1f2418f9aa9",
                    "name": "Kai Sun",
                    "hidden": false
                },
                {
                    "_id": "68dc90b84159d1f2418f9aaa",
                    "name": "Jiaqi Wang",
                    "hidden": false
                },
                {
                    "_id": "68dc90b84159d1f2418f9aab",
                    "name": "Rulin Shao",
                    "hidden": false
                },
                {
                    "_id": "68dc90b84159d1f2418f9aac",
                    "name": "Sean Chen",
                    "hidden": false
                },
                {
                    "_id": "68dc90b84159d1f2418f9aad",
                    "name": "Mohammad Kachuee",
                    "hidden": false
                },
                {
                    "_id": "68dc90b84159d1f2418f9aae",
                    "user": {
                        "_id": "62bcdae6e75a73c22a18b031",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62bcdae6e75a73c22a18b031/2jkNvSxIzXtpfMNfFaj9i.png",
                        "isPro": false,
                        "fullname": "Teja Gollapudi",
                        "user": "Teja-Gollapudi",
                        "type": "user"
                    },
                    "name": "Teja Gollapudi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:21:59.364Z",
                    "hidden": false
                },
                {
                    "_id": "68dc90b84159d1f2418f9aaf",
                    "name": "Tony Liao",
                    "hidden": false
                },
                {
                    "_id": "68dc90b84159d1f2418f9ab0",
                    "name": "Nicolas Scheffer",
                    "hidden": false
                },
                {
                    "_id": "68dc90b84159d1f2418f9ab1",
                    "name": "Rakesh Wanga",
                    "hidden": false
                },
                {
                    "_id": "68dc90b84159d1f2418f9ab2",
                    "name": "Anuj Kumar",
                    "hidden": false
                },
                {
                    "_id": "68dc90b84159d1f2418f9ab3",
                    "name": "Yu Meng",
                    "hidden": false
                },
                {
                    "_id": "68dc90b84159d1f2418f9ab4",
                    "name": "Wen-tau Yih",
                    "hidden": false
                },
                {
                    "_id": "68dc90b84159d1f2418f9ab5",
                    "name": "Xin Luna Dong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T04:25:17.000Z",
            "submittedOnDailyAt": "2025-10-01T00:56:26.188Z",
            "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "6526307af06ac0cf9a922e86",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg",
                "isPro": false,
                "fullname": "Zhepei Wei",
                "user": "weizhepei",
                "type": "user"
            },
            "summary": "While large language models (LLMs) have demonstrated strong performance on\nfactoid question answering, they are still prone to hallucination and\nuntruthful responses, particularly when tasks demand information outside their\nparametric knowledge. Indeed, truthfulness requires more than accuracy --\nmodels must also recognize uncertainty and abstain when unsure to avoid\nhallucinations. This presents a fundamental challenge for existing methods:\napproaches that optimize for accuracy often amplify hallucinations, while those\nthat encourage abstention can become overly conservative, sacrificing correct\nanswers. Both extremes ultimately compromise truthfulness. In this work, we\npresent TruthRL, a general reinforcement learning (RL) framework that directly\noptimizes the truthfulness of LLMs. Specifically, we implement TruthRL using\nGRPO with a simple yet effective ternary reward that distinguishes correct\nanswers, hallucinations, and abstentions. It incentivizes models to reduce\nhallucinations not only by providing correct responses, but also by enabling\nabstention when uncertain, thereby improving truthfulness. Extensive\nexperiments across four knowledge-intensive benchmarks show that, compared to\nvanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves\ntruthfulness by 21.1%, with consistent gains across various backbone models\n(e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth\nablation study demonstrates that vanilla accuracy-driven methods, such as\nsupervised fine-tuning or RL with a binary reward, struggle to balance factual\ncorrectness and uncertainty. In contrast, our proposed truthfulness-driven\nTruthRL achieves strong performance in both accuracy and truthfulness,\nunderscoring the importance of learning objective design for developing\ntruthful LLMs.",
            "upvotes": 45,
            "discussionId": "68dc90b84159d1f2418f9ab6",
            "ai_summary": "TruthRL, a reinforcement learning framework, enhances the truthfulness of large language models by balancing accuracy and abstention, significantly reducing hallucinations and improving performance across benchmarks.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "hallucination",
                "untruthful responses",
                "parametric knowledge",
                "truthfulness",
                "reinforcement learning",
                "RL",
                "GRPO",
                "ternary reward",
                "abstention",
                "accuracy-driven methods",
                "supervised fine-tuning",
                "binary reward",
                "knowledge-intensive benchmarks",
                "Qwen",
                "Llama",
                "retrieval",
                "non-retrieval setups",
                "ablation study"
            ]
        },
        "publishedAt": "2025-09-30T00:25:17.000Z",
        "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning",
        "summary": "While large language models (LLMs) have demonstrated strong performance on\nfactoid question answering, they are still prone to hallucination and\nuntruthful responses, particularly when tasks demand information outside their\nparametric knowledge. Indeed, truthfulness requires more than accuracy --\nmodels must also recognize uncertainty and abstain when unsure to avoid\nhallucinations. This presents a fundamental challenge for existing methods:\napproaches that optimize for accuracy often amplify hallucinations, while those\nthat encourage abstention can become overly conservative, sacrificing correct\nanswers. Both extremes ultimately compromise truthfulness. In this work, we\npresent TruthRL, a general reinforcement learning (RL) framework that directly\noptimizes the truthfulness of LLMs. Specifically, we implement TruthRL using\nGRPO with a simple yet effective ternary reward that distinguishes correct\nanswers, hallucinations, and abstentions. It incentivizes models to reduce\nhallucinations not only by providing correct responses, but also by enabling\nabstention when uncertain, thereby improving truthfulness. Extensive\nexperiments across four knowledge-intensive benchmarks show that, compared to\nvanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves\ntruthfulness by 21.1%, with consistent gains across various backbone models\n(e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth\nablation study demonstrates that vanilla accuracy-driven methods, such as\nsupervised fine-tuning or RL with a binary reward, struggle to balance factual\ncorrectness and uncertainty. In contrast, our proposed truthfulness-driven\nTruthRL achieves strong performance in both accuracy and truthfulness,\nunderscoring the importance of learning objective design for developing\ntruthful LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25760.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6526307af06ac0cf9a922e86",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg",
            "fullname": "Zhepei Wei",
            "name": "weizhepei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "submitterOrganization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26625",
            "authors": [
                {
                    "_id": "68dc87ff4159d1f2418f9a1a",
                    "name": "Junlin Han",
                    "hidden": false
                },
                {
                    "_id": "68dc87ff4159d1f2418f9a1b",
                    "name": "Shengbang Tong",
                    "hidden": false
                },
                {
                    "_id": "68dc87ff4159d1f2418f9a1c",
                    "name": "David Fan",
                    "hidden": false
                },
                {
                    "_id": "68dc87ff4159d1f2418f9a1d",
                    "name": "Yufan Ren",
                    "hidden": false
                },
                {
                    "_id": "68dc87ff4159d1f2418f9a1e",
                    "user": {
                        "_id": "622b93067b9143726fbedc37",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1647023505150-622b93067b9143726fbedc37.jpeg",
                        "isPro": false,
                        "fullname": "Koustuv Sinha",
                        "user": "koustuvs",
                        "type": "user"
                    },
                    "name": "Koustuv Sinha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T15:29:28.567Z",
                    "hidden": false
                },
                {
                    "_id": "68dc87ff4159d1f2418f9a1f",
                    "name": "Philip Torr",
                    "hidden": false
                },
                {
                    "_id": "68dc87ff4159d1f2418f9a20",
                    "name": "Filippos Kokkinos",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T17:57:44.000Z",
            "submittedOnDailyAt": "2025-10-01T00:18:03.860Z",
            "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from\n  Language Pre-training",
            "submittedOnDailyBy": {
                "_id": "636e6ee287545ca5a136b4c3",
                "avatarUrl": "/avatars/208d32b1202e2da210146027212dbdd3.svg",
                "isPro": false,
                "fullname": "Junlin Han",
                "user": "Junlinh",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs.",
            "upvotes": 35,
            "discussionId": "68dc87ff4159d1f2418f9a21",
            "ai_summary": "LLMs develop visual priors during language pre-training, which can be leveraged for vision tasks with minimal additional data, and these priors are composed of separable perception and reasoning components.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "visual priors",
                "latent visual capabilities",
                "multimodal data",
                "visual tasks",
                "implicit knowledge",
                "visual world",
                "perception priors",
                "reasoning priors",
                "pre-training",
                "reasoning-centric data",
                "transferable",
                "visual reasoning",
                "perception ability",
                "vision encoder",
                "visual instruction tuning",
                "text describing the visual world",
                "vision-aware LLMs",
                "data-centric recipe",
                "pre-training",
                "visual alignment",
                "supervised multimodal fine-tuning",
                "Multi-Level Existence Bench",
                "MLE-Bench"
            ]
        },
        "publishedAt": "2025-09-30T13:57:44.000Z",
        "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from\n  Language Pre-training",
        "summary": "Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26625.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636e6ee287545ca5a136b4c3",
            "avatarUrl": "/avatars/208d32b1202e2da210146027212dbdd3.svg",
            "fullname": "Junlin Han",
            "name": "Junlinh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26536",
            "authors": [
                {
                    "_id": "68dc947d4159d1f2418f9af3",
                    "name": "Yida Xue",
                    "hidden": false
                },
                {
                    "_id": "68dc947d4159d1f2418f9af4",
                    "name": "Mingjun Mao",
                    "hidden": false
                },
                {
                    "_id": "68dc947d4159d1f2418f9af5",
                    "name": "Xiangyuan Ru",
                    "hidden": false
                },
                {
                    "_id": "68dc947d4159d1f2418f9af6",
                    "name": "Yuqi Zhu",
                    "hidden": false
                },
                {
                    "_id": "68dc947d4159d1f2418f9af7",
                    "name": "Baochang Ren",
                    "hidden": false
                },
                {
                    "_id": "68dc947d4159d1f2418f9af8",
                    "name": "Shuofei Qiao",
                    "hidden": false
                },
                {
                    "_id": "68dc947d4159d1f2418f9af9",
                    "name": "Mengru Wang",
                    "hidden": false
                },
                {
                    "_id": "68dc947d4159d1f2418f9afa",
                    "name": "Shumin Deng",
                    "hidden": false
                },
                {
                    "_id": "68dc947d4159d1f2418f9afb",
                    "name": "Xinyu An",
                    "hidden": false
                },
                {
                    "_id": "68dc947d4159d1f2418f9afc",
                    "user": {
                        "_id": "620b3bbb0668e435407c8d0a",
                        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                        "isPro": true,
                        "fullname": "Ningyu Zhang",
                        "user": "Ningyu",
                        "type": "user"
                    },
                    "name": "Ningyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:21:42.160Z",
                    "hidden": false
                },
                {
                    "_id": "68dc947d4159d1f2418f9afd",
                    "name": "Ying Chen",
                    "hidden": false
                },
                {
                    "_id": "68dc947d4159d1f2418f9afe",
                    "name": "Huajun Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/1Nye1TFWd66krDhvPXjQ1.png"
            ],
            "publishedAt": "2025-09-30T17:09:32.000Z",
            "submittedOnDailyAt": "2025-10-01T01:11:27.179Z",
            "title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": true,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater\nembodied agents, designed to advance AI in one of the most demanding real-world\nenvironments. Unlike terrestrial or aerial domains, underwater settings present\nextreme perceptual and decision-making challenges, including low visibility,\ndynamic ocean currents, making effective agent deployment exceptionally\ndifficult. OceanGym encompasses eight realistic task domains and a unified\nagent framework driven by Multi-modal Large Language Models (MLLMs), which\nintegrates perception, memory, and sequential decision-making. Agents are\nrequired to comprehend optical and sonar data, autonomously explore complex\nenvironments, and accomplish long-horizon objectives under these harsh\nconditions. Extensive experiments reveal substantial gaps between\nstate-of-the-art MLLM-driven agents and human experts, highlighting the\npersistent difficulty of perception, planning, and adaptability in ocean\nunderwater environments. By providing a high-fidelity, rigorously designed\nplatform, OceanGym establishes a testbed for developing robust embodied AI and\ntransferring these capabilities to real-world autonomous ocean underwater\nvehicles, marking a decisive step toward intelligent agents capable of\noperating in one of Earth's last unexplored frontiers. The code and data are\navailable at https://github.com/OceanGPT/OceanGym.",
            "upvotes": 30,
            "discussionId": "68dc947d4159d1f2418f9aff",
            "projectPage": "https://oceangpt.github.io/OceanGym/",
            "githubRepo": "https://github.com/OceanGPT/OceanGym",
            "ai_summary": "OceanGym is a benchmark for underwater embodied agents using Multi-modal Large Language Models to address challenges in perception, planning, and adaptability in harsh ocean environments.",
            "ai_keywords": [
                "Multi-modal Large Language Models",
                "MLLMs",
                "optical data",
                "sonar data",
                "sequential decision-making",
                "embodied AI",
                "autonomous ocean underwater vehicles"
            ],
            "githubStars": 28
        },
        "publishedAt": "2025-09-30T13:09:32.000Z",
        "title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents",
        "summary": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater\nembodied agents, designed to advance AI in one of the most demanding real-world\nenvironments. Unlike terrestrial or aerial domains, underwater settings present\nextreme perceptual and decision-making challenges, including low visibility,\ndynamic ocean currents, making effective agent deployment exceptionally\ndifficult. OceanGym encompasses eight realistic task domains and a unified\nagent framework driven by Multi-modal Large Language Models (MLLMs), which\nintegrates perception, memory, and sequential decision-making. Agents are\nrequired to comprehend optical and sonar data, autonomously explore complex\nenvironments, and accomplish long-horizon objectives under these harsh\nconditions. Extensive experiments reveal substantial gaps between\nstate-of-the-art MLLM-driven agents and human experts, highlighting the\npersistent difficulty of perception, planning, and adaptability in ocean\nunderwater environments. By providing a high-fidelity, rigorously designed\nplatform, OceanGym establishes a testbed for developing robust embodied AI and\ntransferring these capabilities to real-world autonomous ocean underwater\nvehicles, marking a decisive step toward intelligent agents capable of\noperating in one of Earth's last unexplored frontiers. The code and data are\navailable at https://github.com/OceanGPT/OceanGym.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/1Nye1TFWd66krDhvPXjQ1.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26536.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 30
        },
        "submitterOrganization": {
            "_id": "6345aadf5efccdc07f1365a5",
            "name": "ZhejiangUniversity",
            "fullname": "Zhejiang University"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25848",
            "authors": [
                {
                    "_id": "68dc984b4159d1f2418f9b70",
                    "name": "Xinyu Tian",
                    "hidden": false
                },
                {
                    "_id": "68dc984b4159d1f2418f9b71",
                    "name": "Shu Zou",
                    "hidden": false
                },
                {
                    "_id": "68dc984b4159d1f2418f9b72",
                    "name": "Zhaoyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "68dc984b4159d1f2418f9b73",
                    "name": "Mengqi He",
                    "hidden": false
                },
                {
                    "_id": "68dc984b4159d1f2418f9b74",
                    "name": "Fabian Waschkowski",
                    "hidden": false
                },
                {
                    "_id": "68dc984b4159d1f2418f9b75",
                    "name": "Lukas Wesemann",
                    "hidden": false
                },
                {
                    "_id": "68dc984b4159d1f2418f9b76",
                    "name": "Peter Tu",
                    "hidden": false
                },
                {
                    "_id": "68dc984b4159d1f2418f9b77",
                    "name": "Jing Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T06:37:47.000Z",
            "submittedOnDailyAt": "2025-10-01T02:32:09.976Z",
            "title": "More Thought, Less Accuracy? On the Dual Nature of Reasoning in\n  Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "630c7d9ecb1d6a2384273f1a",
                "avatarUrl": "/avatars/8ba76de6092e5d9fcc4f23d548befe9a.svg",
                "isPro": false,
                "fullname": "Xinyu Tian",
                "user": "xytian1008",
                "type": "user"
            },
            "summary": "Reasoning has emerged as a pivotal capability in Large Language Models\n(LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy\nOptimization (GRPO), these models are able to solve complex tasks such as\nmathematics and code generation. Building on these advances, recent research\nhas sought to extend reasoning to Vision-Language Models (VLMs), yielding\npromising results across diverse visual tasks. Despite this progress, our study\nuncovers the dual nature of multimodal reasoning: while it substantially\nenhances logical inference and facilitates performance on challenging problems,\nit may gradually impair perceptual grounding, leading to recognition failures\non otherwise basic visual questions. Through further analysis, we attribute\nthis phenomenon to visual forgetting, wherein prolonged reasoning causes the\nmodel to increasingly disregard visual input. To address this, we propose\nVision-Anchored Policy Optimization (VAPO), a simple yet effective method that\nexplicitly steers the reasoning process toward visually grounded trajectories.\nOur result model, VAPO-Thinker-7B, significantly strengthens the model's\nreliance on visual information and achieves new state-of-the-art results on a\nwide range of established benchmarks. Project page:\nhttps://xytian1008.github.io/VAPO/",
            "upvotes": 29,
            "discussionId": "68dc984b4159d1f2418f9b78",
            "projectPage": "https://xytian1008.github.io/VAPO/",
            "githubRepo": "https://github.com/xytian1008/VAPO",
            "ai_summary": "VAPO-Thinker-7B enhances multimodal reasoning by anchoring the process to visual information, improving performance on visual tasks while maintaining logical inference.",
            "ai_keywords": [
                "Reinforcement Learning",
                "Group Relative Policy Optimization",
                "Vision-Language Models",
                "multimodal reasoning",
                "logical inference",
                "perceptual grounding",
                "visual forgetting",
                "Vision-Anchored Policy Optimization"
            ],
            "githubStars": 12
        },
        "publishedAt": "2025-09-30T02:37:47.000Z",
        "title": "More Thought, Less Accuracy? On the Dual Nature of Reasoning in\n  Vision-Language Models",
        "summary": "Reasoning has emerged as a pivotal capability in Large Language Models\n(LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy\nOptimization (GRPO), these models are able to solve complex tasks such as\nmathematics and code generation. Building on these advances, recent research\nhas sought to extend reasoning to Vision-Language Models (VLMs), yielding\npromising results across diverse visual tasks. Despite this progress, our study\nuncovers the dual nature of multimodal reasoning: while it substantially\nenhances logical inference and facilitates performance on challenging problems,\nit may gradually impair perceptual grounding, leading to recognition failures\non otherwise basic visual questions. Through further analysis, we attribute\nthis phenomenon to visual forgetting, wherein prolonged reasoning causes the\nmodel to increasingly disregard visual input. To address this, we propose\nVision-Anchored Policy Optimization (VAPO), a simple yet effective method that\nexplicitly steers the reasoning process toward visually grounded trajectories.\nOur result model, VAPO-Thinker-7B, significantly strengthens the model's\nreliance on visual information and achieves new state-of-the-art results on a\nwide range of established benchmarks. Project page:\nhttps://xytian1008.github.io/VAPO/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25848.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630c7d9ecb1d6a2384273f1a",
            "avatarUrl": "/avatars/8ba76de6092e5d9fcc4f23d548befe9a.svg",
            "fullname": "Xinyu Tian",
            "name": "xytian1008",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26226",
            "authors": [
                {
                    "_id": "68dd2c3fe795697646c0a181",
                    "name": "Xin Xu",
                    "hidden": false
                },
                {
                    "_id": "68dd2c3fe795697646c0a182",
                    "name": "Cliveb AI",
                    "hidden": false
                },
                {
                    "_id": "68dd2c3fe795697646c0a183",
                    "name": "Kai Yang",
                    "hidden": false
                },
                {
                    "_id": "68dd2c3fe795697646c0a184",
                    "name": "Tianhao Chen",
                    "hidden": false
                },
                {
                    "_id": "68dd2c3fe795697646c0a185",
                    "name": "Yang Wang",
                    "hidden": false
                },
                {
                    "_id": "68dd2c3fe795697646c0a186",
                    "name": "Saiyong Yang",
                    "hidden": false
                },
                {
                    "_id": "68dd2c3fe795697646c0a187",
                    "name": "Can Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T13:25:00.000Z",
            "submittedOnDailyAt": "2025-10-01T11:59:03.071Z",
            "title": "Thinking-Free Policy Initialization Makes Distilled Reasoning Models\n  More Effective and Efficient Reasoners",
            "submittedOnDailyBy": {
                "_id": "64e2d169d2af12910d682130",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e2d169d2af12910d682130/VG8UdqJCJGc0K4G0P0XQP.jpeg",
                "isPro": false,
                "fullname": "xuxin",
                "user": "xx18",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Reward (RLVR) effectively solves\ncomplex tasks but demands extremely long context lengths during training,\nleading to substantial computational costs. While multi-stage training can\npartially mitigate this, starting with overly short contexts often causes\nirreversible performance degradation, ultimately failing to reduce overall\ntraining compute significantly. In this paper, we introduce\n**T**hinking-**F**ree **P**olicy **I**nitialization (**TFPI**), a simple yet\neffective adaptation to RLVR that bridges long Chain-of-Thought (CoT)\ndistillation and standard RLVR. TFPI employs a simple *ThinkFree* operation,\nexplicitly discarding the thinking content via a direct *</think>* append, to\nreduce token usage during inference. Training with *ThinkFree*-adapted inputs\nimproves performance and lowers token consumption, even in the original\nslow-thinking mode. Extensive experiments across various benchmarks have shown\nthat TFPI accelerates RL convergence, achieves a higher performance ceiling,\nand yields more token-efficient reasoning models without specialized rewards or\ncomplex training designs. With TFPI only, we train a 4B model to reach 89.0%\naccuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours.",
            "upvotes": 26,
            "discussionId": "68dd2c3fe795697646c0a188",
            "ai_summary": "TFPI, a simple adaptation to RLVR, improves performance and reduces token usage by discarding thinking content during training, accelerating RL convergence and achieving higher accuracy with less computational cost.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Reward (RLVR)",
                "Chain-of-Thought (CoT) distillation",
                "ThinkFree operation",
                "token usage",
                "RL convergence",
                "performance ceiling",
                "token-efficient reasoning models",
                "AIME24",
                "LiveCodeBench"
            ]
        },
        "publishedAt": "2025-09-30T09:25:00.000Z",
        "title": "Thinking-Free Policy Initialization Makes Distilled Reasoning Models\n  More Effective and Efficient Reasoners",
        "summary": "Reinforcement Learning with Verifiable Reward (RLVR) effectively solves\ncomplex tasks but demands extremely long context lengths during training,\nleading to substantial computational costs. While multi-stage training can\npartially mitigate this, starting with overly short contexts often causes\nirreversible performance degradation, ultimately failing to reduce overall\ntraining compute significantly. In this paper, we introduce\n**T**hinking-**F**ree **P**olicy **I**nitialization (**TFPI**), a simple yet\neffective adaptation to RLVR that bridges long Chain-of-Thought (CoT)\ndistillation and standard RLVR. TFPI employs a simple *ThinkFree* operation,\nexplicitly discarding the thinking content via a direct *</think>* append, to\nreduce token usage during inference. Training with *ThinkFree*-adapted inputs\nimproves performance and lowers token consumption, even in the original\nslow-thinking mode. Extensive experiments across various benchmarks have shown\nthat TFPI accelerates RL convergence, achieves a higher performance ceiling,\nand yields more token-efficient reasoning models without specialized rewards or\ncomplex training designs. With TFPI only, we train a 4B model to reach 89.0%\naccuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26226.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e2d169d2af12910d682130",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e2d169d2af12910d682130/VG8UdqJCJGc0K4G0P0XQP.jpeg",
            "fullname": "xuxin",
            "name": "xx18",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "submitterOrganization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25182",
            "authors": [
                {
                    "_id": "68dc88d34159d1f2418f9a36",
                    "name": "Junyu Chen",
                    "hidden": false
                },
                {
                    "_id": "68dc88d34159d1f2418f9a37",
                    "name": "Wenkun He",
                    "hidden": false
                },
                {
                    "_id": "68dc88d34159d1f2418f9a38",
                    "name": "Yuchao Gu",
                    "hidden": false
                },
                {
                    "_id": "68dc88d34159d1f2418f9a39",
                    "name": "Yuyang Zhao",
                    "hidden": false
                },
                {
                    "_id": "68dc88d34159d1f2418f9a3a",
                    "name": "Jincheng Yu",
                    "hidden": false
                },
                {
                    "_id": "68dc88d34159d1f2418f9a3b",
                    "user": {
                        "_id": "645b5b09bc7518912e1f9733",
                        "avatarUrl": "/avatars/4d35f728b41f93881a9b67c337f4d1df.svg",
                        "isPro": false,
                        "fullname": "Chen",
                        "user": "Lawrence-cj",
                        "type": "user"
                    },
                    "name": "Junsong Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:22:15.012Z",
                    "hidden": false
                },
                {
                    "_id": "68dc88d34159d1f2418f9a3c",
                    "name": "Dongyun Zou",
                    "hidden": false
                },
                {
                    "_id": "68dc88d34159d1f2418f9a3d",
                    "name": "Yujun Lin",
                    "hidden": false
                },
                {
                    "_id": "68dc88d34159d1f2418f9a3e",
                    "name": "Zhekai Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dc88d34159d1f2418f9a3f",
                    "user": {
                        "_id": "63129589bbaa385279d1826e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63129589bbaa385279d1826e/0AUs3q4ngRZ-wXuY1jP9G.jpeg",
                        "isPro": true,
                        "fullname": "Muyang Li",
                        "user": "Lmxyy",
                        "type": "user"
                    },
                    "name": "Muyang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:22:18.394Z",
                    "hidden": false
                },
                {
                    "_id": "68dc88d34159d1f2418f9a40",
                    "name": "Haocheng Xi",
                    "hidden": false
                },
                {
                    "_id": "68dc88d34159d1f2418f9a41",
                    "name": "Ligeng Zhu",
                    "hidden": false
                },
                {
                    "_id": "68dc88d34159d1f2418f9a42",
                    "name": "Enze Xie",
                    "hidden": false
                },
                {
                    "_id": "68dc88d34159d1f2418f9a43",
                    "name": "Song Han",
                    "hidden": false
                },
                {
                    "_id": "68dc88d34159d1f2418f9a44",
                    "name": "Han Cai",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/650e2b14c945dfc9386a7e28/bgdM9-uqshQ2p3QyNbj82.jpeg"
            ],
            "publishedAt": "2025-09-29T17:59:31.000Z",
            "submittedOnDailyAt": "2025-10-01T00:21:53.066Z",
            "title": "DC-VideoGen: Efficient Video Generation with Deep Compression Video\n  Autoencoder",
            "submittedOnDailyBy": {
                "_id": "650e2b14c945dfc9386a7e28",
                "avatarUrl": "/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg",
                "isPro": false,
                "fullname": "Han Cai",
                "user": "han-cai",
                "type": "user"
            },
            "summary": "We introduce DC-VideoGen, a post-training acceleration framework for\nefficient video generation. DC-VideoGen can be applied to any pre-trained video\ndiffusion model, improving efficiency by adapting it to a deep compression\nlatent space with lightweight fine-tuning. The framework builds on two key\ninnovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal\ntemporal design that achieves 32x/64x spatial and 4x temporal compression while\npreserving reconstruction quality and generalization to longer videos; and (ii)\nAE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer\nof pre-trained models into the new latent space. Adapting the pre-trained\nWan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100\nGPU. The accelerated models achieve up to 14.8x lower inference latency than\ntheir base counterparts without compromising quality, and further enable\n2160x3840 video generation on a single GPU. Code:\nhttps://github.com/dc-ai-projects/DC-VideoGen.",
            "upvotes": 26,
            "discussionId": "68dc88d34159d1f2418f9a45",
            "ai_summary": "DC-VideoGen accelerates video generation by adapting pre-trained diffusion models to a deep compression latent space, reducing inference latency and enabling high-resolution video generation.",
            "ai_keywords": [
                "DC-VideoGen",
                "video diffusion model",
                "deep compression latent space",
                "lightweight fine-tuning",
                "Deep Compression Video Autoencoder",
                "chunk-causal temporal design",
                "AE-Adapt-V",
                "Wan-2.1-14B model",
                "inference latency",
                "high-resolution video generation"
            ]
        },
        "publishedAt": "2025-09-29T13:59:31.000Z",
        "title": "DC-VideoGen: Efficient Video Generation with Deep Compression Video\n  Autoencoder",
        "summary": "We introduce DC-VideoGen, a post-training acceleration framework for\nefficient video generation. DC-VideoGen can be applied to any pre-trained video\ndiffusion model, improving efficiency by adapting it to a deep compression\nlatent space with lightweight fine-tuning. The framework builds on two key\ninnovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal\ntemporal design that achieves 32x/64x spatial and 4x temporal compression while\npreserving reconstruction quality and generalization to longer videos; and (ii)\nAE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer\nof pre-trained models into the new latent space. Adapting the pre-trained\nWan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100\nGPU. The accelerated models achieve up to 14.8x lower inference latency than\ntheir base counterparts without compromising quality, and further enable\n2160x3840 video generation on a single GPU. Code:\nhttps://github.com/dc-ai-projects/DC-VideoGen.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/650e2b14c945dfc9386a7e28/bgdM9-uqshQ2p3QyNbj82.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25182.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650e2b14c945dfc9386a7e28",
            "avatarUrl": "/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg",
            "fullname": "Han Cai",
            "name": "han-cai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "submitterOrganization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25154",
            "authors": [
                {
                    "_id": "68dc97c24159d1f2418f9b54",
                    "name": "Dawei Li",
                    "hidden": false
                },
                {
                    "_id": "68dc97c24159d1f2418f9b55",
                    "name": "Zhen Tan",
                    "hidden": false
                },
                {
                    "_id": "68dc97c24159d1f2418f9b56",
                    "user": {
                        "_id": "65b2fae679954e21ac426aec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b2fae679954e21ac426aec/LybSb_awygRTQinm1npUq.jpeg",
                        "isPro": false,
                        "fullname": "Chengshuai Zhao",
                        "user": "chengshuaizhao",
                        "type": "user"
                    },
                    "name": "Chengshuai Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:21:39.481Z",
                    "hidden": false
                },
                {
                    "_id": "68dc97c24159d1f2418f9b57",
                    "name": "Bohan Jiang",
                    "hidden": false
                },
                {
                    "_id": "68dc97c24159d1f2418f9b58",
                    "name": "Baixiang Huang",
                    "hidden": false
                },
                {
                    "_id": "68dc97c24159d1f2418f9b59",
                    "name": "Pingchuan Ma",
                    "hidden": false
                },
                {
                    "_id": "68dc97c24159d1f2418f9b5a",
                    "name": "Abdullah Alnaibari",
                    "hidden": false
                },
                {
                    "_id": "68dc97c24159d1f2418f9b5b",
                    "name": "Kai Shu",
                    "hidden": false
                },
                {
                    "_id": "68dc97c24159d1f2418f9b5c",
                    "name": "Huan Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:54:57.000Z",
            "submittedOnDailyAt": "2025-10-01T01:24:59.807Z",
            "title": "Who's Your Judge? On the Detectability of LLM-Generated Judgments",
            "submittedOnDailyBy": {
                "_id": "6474e1afb68461d5cf7c41cc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
                "isPro": false,
                "fullname": "Dawei Li",
                "user": "wjldw",
                "type": "user"
            },
            "summary": "Large Language Model (LLM)-based judgments leverage powerful LLMs to\nefficiently evaluate candidate content and provide judgment scores. However,\nthe inherent biases and vulnerabilities of LLM-generated judgments raise\nconcerns, underscoring the urgent need for distinguishing them in sensitive\nscenarios like academic peer reviewing. In this work, we propose and formalize\nthe task of judgment detection and systematically investigate the detectability\nof LLM-generated judgments. Unlike LLM-generated text detection, judgment\ndetection relies solely on judgment scores and candidates, reflecting\nreal-world scenarios where textual feedback is often unavailable in the\ndetection process. Our preliminary analysis shows that existing LLM-generated\ntext detection methods perform poorly given their incapability to capture the\ninteraction between judgment scores and candidate content -- an aspect crucial\nfor effective judgment detection. Inspired by this, we introduce\nJ-Detector, a lightweight and transparent neural detector augmented\nwith explicitly extracted linguistic and LLM-enhanced features to link LLM\njudges' biases with candidates' properties for accurate detection. Experiments\nacross diverse datasets demonstrate the effectiveness of J-Detector\nand show how its interpretability enables quantifying biases in LLM judges.\nFinally, we analyze key factors affecting the detectability of LLM-generated\njudgments and validate the practical utility of judgment detection in\nreal-world scenarios.",
            "upvotes": 24,
            "discussionId": "68dc97c24159d1f2418f9b5d",
            "ai_summary": "J-Detector, a neural detector with linguistic and LLM-enhanced features, effectively identifies LLM-generated judgments based on scores and candidate content, addressing biases and vulnerabilities in sensitive scenarios.",
            "ai_keywords": [
                "Large Language Model",
                "judgment detection",
                "neural detector",
                "linguistic features",
                "LLM-enhanced features",
                "judgment scores",
                "candidate content",
                "biases",
                "detectability"
            ]
        },
        "publishedAt": "2025-09-29T13:54:57.000Z",
        "title": "Who's Your Judge? On the Detectability of LLM-Generated Judgments",
        "summary": "Large Language Model (LLM)-based judgments leverage powerful LLMs to\nefficiently evaluate candidate content and provide judgment scores. However,\nthe inherent biases and vulnerabilities of LLM-generated judgments raise\nconcerns, underscoring the urgent need for distinguishing them in sensitive\nscenarios like academic peer reviewing. In this work, we propose and formalize\nthe task of judgment detection and systematically investigate the detectability\nof LLM-generated judgments. Unlike LLM-generated text detection, judgment\ndetection relies solely on judgment scores and candidates, reflecting\nreal-world scenarios where textual feedback is often unavailable in the\ndetection process. Our preliminary analysis shows that existing LLM-generated\ntext detection methods perform poorly given their incapability to capture the\ninteraction between judgment scores and candidate content -- an aspect crucial\nfor effective judgment detection. Inspired by this, we introduce\nJ-Detector, a lightweight and transparent neural detector augmented\nwith explicitly extracted linguistic and LLM-enhanced features to link LLM\njudges' biases with candidates' properties for accurate detection. Experiments\nacross diverse datasets demonstrate the effectiveness of J-Detector\nand show how its interpretability enables quantifying biases in LLM judges.\nFinally, we analyze key factors affecting the detectability of LLM-generated\njudgments and validate the practical utility of judgment detection in\nreal-world scenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25154.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6474e1afb68461d5cf7c41cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
            "fullname": "Dawei Li",
            "name": "wjldw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "submitterOrganization": {
            "_id": "6830bdb6802db9cd255151d8",
            "name": "DMML",
            "fullname": "Data Mining and Machine Learning lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6474e1afb68461d5cf7c41cc/jxoUTsOe3Yhnz3Zng3LFh.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25758",
            "authors": [
                {
                    "_id": "68dc8a1f4159d1f2418f9a64",
                    "name": "Yein Park",
                    "hidden": false
                },
                {
                    "_id": "68dc8a1f4159d1f2418f9a65",
                    "name": "Minbyul Jeong",
                    "hidden": false
                },
                {
                    "_id": "68dc8a1f4159d1f2418f9a66",
                    "name": "Jaewoo Kang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T04:23:43.000Z",
            "submittedOnDailyAt": "2025-10-01T00:26:22.625Z",
            "title": "Thinking Sparks!: Emergent Attention Heads in Reasoning Models During\n  Post Training",
            "submittedOnDailyBy": {
                "_id": "64587be872b60ae7a3817858",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
                "isPro": false,
                "fullname": "Minbyul Jeong",
                "user": "Minbyul",
                "type": "user"
            },
            "summary": "The remarkable capabilities of modern large reasoning models are largely\nunlocked through post-training techniques such as supervised fine-tuning and\nreinforcement learning. However, the architectural mechanisms behind such\nimprovements remain largely opaque. In this work, we use circuit analysis to\ndemonstrate that post-training for complex reasoning sparks the emergence of\nnovel, functionally specialized attention heads. These heads collectively\nsupport structured reasoning and computation. Our comparative analysis across\nQwen families and DeepSeek-distilled model reveals that these emergent heads\nevolve differently under different training regimes. Distillation and SFT\nfoster a cumulative addition of stable reasoning heads. In contrast, group\nrelative policy optimization operates in a dynamic search mode: relatively few\nattention heads are iteratively activated, evaluated, and pruned, with their\nsurvival closely tracking fluctuations in the task reward signal. Furthermore,\nwe find that controllable think on/off models do not possess dedicated thinking\nheads. Instead, turning off explicit reasoning triggers a broader-but less\nefficient-set of compensatory heads. Through ablation and qualitative analyses,\nwe connect these circuit-level dynamics to a crucial performance trade-off:\nstrengthened heads enable sophisticated problem-solving strategies for\ndifficult problems but can also introduce over-thinking failure modes, such as\ncalculation errors or logical loops on simpler tasks. These findings connect\ncircuit-level dynamics to macro-level performance, identifying an inherent\ntension where complex reasoning comes at the cost of elementary computations.\nMore broadly, our work points to future directions for training policy design,\nemphasizing the need to balance the development of effective reasoning\nstrategies with the assurance of reliable, flawless execution.",
            "upvotes": 18,
            "discussionId": "68dc8a1f4159d1f2418f9a67",
            "ai_summary": "Post-training techniques like supervised fine-tuning and reinforcement learning lead to the emergence of specialized attention heads that support structured reasoning, with different training regimes affecting their evolution and performance.",
            "ai_keywords": [
                "supervised fine-tuning",
                "reinforcement learning",
                "circuit analysis",
                "attention heads",
                "structured reasoning",
                "Qwen families",
                "DeepSeek-distilled model",
                "group relative policy optimization",
                "think on/off models",
                "ablation analysis",
                "qualitative analysis",
                "over-thinking failure modes"
            ]
        },
        "publishedAt": "2025-09-30T00:23:43.000Z",
        "title": "Thinking Sparks!: Emergent Attention Heads in Reasoning Models During\n  Post Training",
        "summary": "The remarkable capabilities of modern large reasoning models are largely\nunlocked through post-training techniques such as supervised fine-tuning and\nreinforcement learning. However, the architectural mechanisms behind such\nimprovements remain largely opaque. In this work, we use circuit analysis to\ndemonstrate that post-training for complex reasoning sparks the emergence of\nnovel, functionally specialized attention heads. These heads collectively\nsupport structured reasoning and computation. Our comparative analysis across\nQwen families and DeepSeek-distilled model reveals that these emergent heads\nevolve differently under different training regimes. Distillation and SFT\nfoster a cumulative addition of stable reasoning heads. In contrast, group\nrelative policy optimization operates in a dynamic search mode: relatively few\nattention heads are iteratively activated, evaluated, and pruned, with their\nsurvival closely tracking fluctuations in the task reward signal. Furthermore,\nwe find that controllable think on/off models do not possess dedicated thinking\nheads. Instead, turning off explicit reasoning triggers a broader-but less\nefficient-set of compensatory heads. Through ablation and qualitative analyses,\nwe connect these circuit-level dynamics to a crucial performance trade-off:\nstrengthened heads enable sophisticated problem-solving strategies for\ndifficult problems but can also introduce over-thinking failure modes, such as\ncalculation errors or logical loops on simpler tasks. These findings connect\ncircuit-level dynamics to macro-level performance, identifying an inherent\ntension where complex reasoning comes at the cost of elementary computations.\nMore broadly, our work points to future directions for training policy design,\nemphasizing the need to balance the development of effective reasoning\nstrategies with the assurance of reliable, flawless execution.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25758.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64587be872b60ae7a3817858",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
            "fullname": "Minbyul Jeong",
            "name": "Minbyul",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "submitterOrganization": {
            "_id": "6621bc39e774284ec1742ab8",
            "name": "KoreaUniversity",
            "fullname": "Korea University"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26488",
            "authors": [
                {
                    "_id": "68dc88d74159d1f2418f9a47",
                    "user": {
                        "_id": "65811eeaa2284a018e51f1ba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
                        "isPro": false,
                        "fullname": "Zigeng Chen",
                        "user": "Zigeng",
                        "type": "user"
                    },
                    "name": "Zigeng Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:22:11.281Z",
                    "hidden": false
                },
                {
                    "_id": "68dc88d74159d1f2418f9a48",
                    "name": "Gongfan Fang",
                    "hidden": false
                },
                {
                    "_id": "68dc88d74159d1f2418f9a49",
                    "name": "Xinyin Ma",
                    "hidden": false
                },
                {
                    "_id": "68dc88d74159d1f2418f9a4a",
                    "name": "Ruonan Yu",
                    "hidden": false
                },
                {
                    "_id": "68dc88d74159d1f2418f9a4b",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65811eeaa2284a018e51f1ba/0NK-gFnfpLThLyS0atsLH.mp4"
            ],
            "publishedAt": "2025-09-30T16:32:52.000Z",
            "submittedOnDailyAt": "2025-10-01T00:34:29.943Z",
            "title": "dParallel: Learnable Parallel Decoding for dLLMs",
            "submittedOnDailyBy": {
                "_id": "65811eeaa2284a018e51f1ba",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
                "isPro": false,
                "fullname": "Zigeng Chen",
                "user": "Zigeng",
                "type": "user"
            },
            "summary": "Diffusion large language models (dLLMs) have recently drawn considerable\nattention within the research community as a promising alternative to\nautoregressive generation, offering parallel token prediction and lower\ninference latency. Yet, their parallel decoding potential remains largely\nunderexplored, as existing open-source models still require nearly token-length\ndecoding steps to ensure performance. To address this, we introduce dParallel,\na simple and effective method that unlocks the inherent parallelism of dLLMs\nfor fast sampling. We identify that the key bottleneck to parallel decoding\narises from the sequential certainty convergence for masked tokens. Building on\nthis insight, we introduce the core of our approach: certainty-forcing\ndistillation, a novel training strategy that distills the model to follow its\noriginal sampling trajectories while enforcing it to achieve high certainty on\nmasked tokens more rapidly and in parallel. Extensive experiments across\nvarious benchmarks demonstrate that our method can dramatically reduce the\nnumber of decoding steps while maintaining performance. When applied to the\nLLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on\nGSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP\nbenchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup\nwhile maintaining accuracy. Our code is available at\nhttps://github.com/czg1225/dParallel",
            "upvotes": 17,
            "discussionId": "68dc88d74159d1f2418f9a4c",
            "githubRepo": "https://github.com/czg1225/dParallel",
            "ai_summary": "dParallel is a method that enhances the parallel decoding of diffusion large language models, significantly reducing decoding steps without compromising performance.",
            "ai_keywords": [
                "diffusion large language models",
                "dLLMs",
                "autoregressive generation",
                "parallel token prediction",
                "parallel decoding",
                "masked tokens",
                "certainty-forcing distillation",
                "LLaDA-8B-Instruct",
                "GSM8K",
                "MBPP benchmark"
            ],
            "githubStars": 14
        },
        "publishedAt": "2025-09-30T12:32:52.000Z",
        "title": "dParallel: Learnable Parallel Decoding for dLLMs",
        "summary": "Diffusion large language models (dLLMs) have recently drawn considerable\nattention within the research community as a promising alternative to\nautoregressive generation, offering parallel token prediction and lower\ninference latency. Yet, their parallel decoding potential remains largely\nunderexplored, as existing open-source models still require nearly token-length\ndecoding steps to ensure performance. To address this, we introduce dParallel,\na simple and effective method that unlocks the inherent parallelism of dLLMs\nfor fast sampling. We identify that the key bottleneck to parallel decoding\narises from the sequential certainty convergence for masked tokens. Building on\nthis insight, we introduce the core of our approach: certainty-forcing\ndistillation, a novel training strategy that distills the model to follow its\noriginal sampling trajectories while enforcing it to achieve high certainty on\nmasked tokens more rapidly and in parallel. Extensive experiments across\nvarious benchmarks demonstrate that our method can dramatically reduce the\nnumber of decoding steps while maintaining performance. When applied to the\nLLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on\nGSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP\nbenchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup\nwhile maintaining accuracy. Our code is available at\nhttps://github.com/czg1225/dParallel",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65811eeaa2284a018e51f1ba/0NK-gFnfpLThLyS0atsLH.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26488.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65811eeaa2284a018e51f1ba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
            "fullname": "Zigeng Chen",
            "name": "Zigeng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "submitterOrganization": {
            "_id": "6508ab2b349930913196378b",
            "name": "NationalUniversityofSingapore",
            "fullname": "National University of Singapore",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.26490",
            "authors": [
                {
                    "_id": "68dc8ca34159d1f2418f9a7e",
                    "user": {
                        "_id": "66ecee857264238429a1211f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ecee857264238429a1211f/TbuM7ToLBrSxDF8mOccpK.jpeg",
                        "isPro": false,
                        "fullname": "Wei He",
                        "user": "hewei2001",
                        "type": "user"
                    },
                    "name": "Wei He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:22:01.757Z",
                    "hidden": false
                },
                {
                    "_id": "68dc8ca34159d1f2418f9a7f",
                    "name": "Yueqing Sun",
                    "hidden": false
                },
                {
                    "_id": "68dc8ca34159d1f2418f9a80",
                    "name": "Hongyan Hao",
                    "hidden": false
                },
                {
                    "_id": "68dc8ca34159d1f2418f9a81",
                    "name": "Xueyuan Hao",
                    "hidden": false
                },
                {
                    "_id": "68dc8ca34159d1f2418f9a82",
                    "name": "Zhikang Xia",
                    "hidden": false
                },
                {
                    "_id": "68dc8ca34159d1f2418f9a83",
                    "name": "Qi Gu",
                    "hidden": false
                },
                {
                    "_id": "68dc8ca34159d1f2418f9a84",
                    "name": "Chengcheng Han",
                    "hidden": false
                },
                {
                    "_id": "68dc8ca34159d1f2418f9a85",
                    "name": "Dengchang Zhao",
                    "hidden": false
                },
                {
                    "_id": "68dc8ca34159d1f2418f9a86",
                    "name": "Hui Su",
                    "hidden": false
                },
                {
                    "_id": "68dc8ca34159d1f2418f9a87",
                    "name": "Kefeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dc8ca34159d1f2418f9a88",
                    "name": "Man Gao",
                    "hidden": false
                },
                {
                    "_id": "68dc8ca34159d1f2418f9a89",
                    "name": "Xi Su",
                    "hidden": false
                },
                {
                    "_id": "68dc8ca34159d1f2418f9a8a",
                    "name": "Xiaodong Cai",
                    "hidden": false
                },
                {
                    "_id": "68dc8ca34159d1f2418f9a8b",
                    "name": "Xunliang Cai",
                    "hidden": false
                },
                {
                    "_id": "68dc8ca34159d1f2418f9a8c",
                    "name": "Yu Yang",
                    "hidden": false
                },
                {
                    "_id": "68dc8ca34159d1f2418f9a8d",
                    "name": "Yunke Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T16:33:49.000Z",
            "submittedOnDailyAt": "2025-10-01T00:38:43.265Z",
            "title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\n  Real-world Applications",
            "submittedOnDailyBy": {
                "_id": "66ecee857264238429a1211f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ecee857264238429a1211f/TbuM7ToLBrSxDF8mOccpK.jpeg",
                "isPro": false,
                "fullname": "Wei He",
                "user": "hewei2001",
                "type": "user"
            },
            "summary": "As LLM-based agents are increasingly deployed in real-life scenarios,\nexisting benchmarks fail to capture their inherent complexity of handling\nextensive information, leveraging diverse resources, and managing dynamic user\ninteractions. To address this gap, we introduce VitaBench, a challenging\nbenchmark that evaluates agents on versatile interactive tasks grounded in\nreal-world settings. Drawing from daily applications in food delivery, in-store\nconsumption, and online travel services, VitaBench presents agents with the\nmost complex life-serving simulation environment to date, comprising 66 tools.\nThrough a framework that eliminates domain-specific policies, we enable\nflexible composition of these scenarios and tools, yielding 100 cross-scenario\ntasks (main results) and 300 single-scenario tasks. Each task is derived from\nmultiple real user requests and requires agents to reason across temporal and\nspatial dimensions, utilize complex tool sets, proactively clarify ambiguous\ninstructions, and track shifting user intent throughout multi-turn\nconversations. Moreover, we propose a rubric-based sliding window evaluator,\nenabling robust assessment of diverse solution pathways in complex environments\nand stochastic interactions. Our comprehensive evaluation reveals that even the\nmost advanced models achieve only 30% success rate on cross-scenario tasks, and\nless than 50% success rate on others. Overall, we believe VitaBench will serve\nas a valuable resource for advancing the development of AI agents in practical\nreal-world applications. The code, dataset, and leaderboard are available at\nhttps://vitabench.github.io/",
            "upvotes": 15,
            "discussionId": "68dc8ca44159d1f2418f9a8e",
            "projectPage": "https://vitabench.github.io/",
            "githubRepo": "https://github.com/meituan/vitabench",
            "ai_summary": "VitaBench is a benchmark for evaluating LLM-based agents in complex, real-world interactive tasks using a diverse set of tools and scenarios.",
            "ai_keywords": [
                "LLM-based agents",
                "VitaBench",
                "interactive tasks",
                "real-world settings",
                "food delivery",
                "in-store consumption",
                "online travel services",
                "life-serving simulation environment",
                "domain-specific policies",
                "flexible composition",
                "cross-scenario tasks",
                "single-scenario tasks",
                "real user requests",
                "temporal dimensions",
                "spatial dimensions",
                "complex tool sets",
                "ambiguous instructions",
                "shifting user intent",
                "multi-turn conversations",
                "rubric-based sliding window evaluator",
                "stochastic interactions"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-09-30T12:33:49.000Z",
        "title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\n  Real-world Applications",
        "summary": "As LLM-based agents are increasingly deployed in real-life scenarios,\nexisting benchmarks fail to capture their inherent complexity of handling\nextensive information, leveraging diverse resources, and managing dynamic user\ninteractions. To address this gap, we introduce VitaBench, a challenging\nbenchmark that evaluates agents on versatile interactive tasks grounded in\nreal-world settings. Drawing from daily applications in food delivery, in-store\nconsumption, and online travel services, VitaBench presents agents with the\nmost complex life-serving simulation environment to date, comprising 66 tools.\nThrough a framework that eliminates domain-specific policies, we enable\nflexible composition of these scenarios and tools, yielding 100 cross-scenario\ntasks (main results) and 300 single-scenario tasks. Each task is derived from\nmultiple real user requests and requires agents to reason across temporal and\nspatial dimensions, utilize complex tool sets, proactively clarify ambiguous\ninstructions, and track shifting user intent throughout multi-turn\nconversations. Moreover, we propose a rubric-based sliding window evaluator,\nenabling robust assessment of diverse solution pathways in complex environments\nand stochastic interactions. Our comprehensive evaluation reveals that even the\nmost advanced models achieve only 30% success rate on cross-scenario tasks, and\nless than 50% success rate on others. Overall, we believe VitaBench will serve\nas a valuable resource for advancing the development of AI agents in practical\nreal-world applications. The code, dataset, and leaderboard are available at\nhttps://vitabench.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26490.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66ecee857264238429a1211f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ecee857264238429a1211f/TbuM7ToLBrSxDF8mOccpK.jpeg",
            "fullname": "Wei He",
            "name": "hewei2001",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "submitterOrganization": {
            "_id": "68b28d79a176a9beb30d2049",
            "name": "meituan-longcat",
            "fullname": "LongCat",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.22646",
            "authors": [
                {
                    "_id": "68dcb64b4159d1f2418f9bf5",
                    "name": "Xingyu Fu",
                    "hidden": false
                },
                {
                    "_id": "68dcb64b4159d1f2418f9bf6",
                    "name": "Siyi Liu",
                    "hidden": false
                },
                {
                    "_id": "68dcb64b4159d1f2418f9bf7",
                    "name": "Yinuo Xu",
                    "hidden": false
                },
                {
                    "_id": "68dcb64b4159d1f2418f9bf8",
                    "name": "Pan Lu",
                    "hidden": false
                },
                {
                    "_id": "68dcb64b4159d1f2418f9bf9",
                    "name": "Guangqiuse Hu",
                    "hidden": false
                },
                {
                    "_id": "68dcb64b4159d1f2418f9bfa",
                    "name": "Tianbo Yang",
                    "hidden": false
                },
                {
                    "_id": "68dcb64b4159d1f2418f9bfb",
                    "name": "Taran Anantasagar",
                    "hidden": false
                },
                {
                    "_id": "68dcb64b4159d1f2418f9bfc",
                    "name": "Christopher Shen",
                    "hidden": false
                },
                {
                    "_id": "68dcb64b4159d1f2418f9bfd",
                    "name": "Yikai Mao",
                    "hidden": false
                },
                {
                    "_id": "68dcb64b4159d1f2418f9bfe",
                    "name": "Yuanzhe Liu",
                    "hidden": false
                },
                {
                    "_id": "68dcb64b4159d1f2418f9bff",
                    "name": "Keyush Shah",
                    "hidden": false
                },
                {
                    "_id": "68dcb64b4159d1f2418f9c00",
                    "name": "Chung Un Lee",
                    "hidden": false
                },
                {
                    "_id": "68dcb64b4159d1f2418f9c01",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "68dcb64b4159d1f2418f9c02",
                    "name": "James Zou",
                    "hidden": false
                },
                {
                    "_id": "68dcb64b4159d1f2418f9c03",
                    "name": "Dan Roth",
                    "hidden": false
                },
                {
                    "_id": "68dcb64b4159d1f2418f9c04",
                    "name": "Chris Callison-Burch",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T17:59:54.000Z",
            "submittedOnDailyAt": "2025-10-01T03:36:03.020Z",
            "title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\n  LLMs",
            "submittedOnDailyBy": {
                "_id": "6336091b2db86a181ccd6054",
                "avatarUrl": "/avatars/829f69436225d05d2c2136bc90f640d7.svg",
                "isPro": false,
                "fullname": "Xingyu Fu",
                "user": "Fiaa",
                "type": "user"
            },
            "summary": "Can humans identify AI-generated (fake) videos and provide grounded reasons?\nWhile video generation models have advanced rapidly, a critical dimension --\nwhether humans can detect deepfake traces within a generated video, i.e.,\nspatiotemporal grounded visual artifacts that reveal a video as machine\ngenerated -- has been largely overlooked. We introduce DeeptraceReward, the\nfirst fine-grained, spatially- and temporally- aware benchmark that annotates\nhuman-perceived fake traces for video generation reward. The dataset comprises\n4.3K detailed annotations across 3.3K high-quality generated videos. Each\nannotation provides a natural-language explanation, pinpoints a bounding-box\nregion containing the perceived trace, and marks precise onset and offset\ntimestamps. We consolidate these annotations into 9 major categories of\ndeepfake traces that lead humans to identify a video as AI-generated, and train\nmultimodal language models (LMs) as reward models to mimic human judgments and\nlocalizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by\n34.7% on average across fake clue identification, grounding, and explanation.\nInterestingly, we observe a consistent difficulty gradient: binary fake v.s.\nreal classification is substantially easier than fine-grained deepfake trace\ndetection; within the latter, performance degrades from natural language\nexplanations (easiest), to spatial grounding, to temporal labeling (hardest).\nBy foregrounding human-perceived deepfake traces, DeeptraceReward provides a\nrigorous testbed and training signal for socially aware and trustworthy video\ngeneration.",
            "upvotes": 14,
            "discussionId": "68dcb64c4159d1f2418f9c05",
            "projectPage": "https://deeptracereward.github.io/",
            "ai_summary": "DeeptraceReward is a benchmark dataset that annotates human-perceived deepfake traces in videos, used to train multimodal language models for detecting AI-generated videos.",
            "ai_keywords": [
                "deepfake traces",
                "spatiotemporal grounded visual artifacts",
                "fine-grained",
                "spatially- and temporally- aware benchmark",
                "multimodal language models",
                "reward models",
                "binary fake v.s. real classification",
                "natural language explanations",
                "spatial grounding",
                "temporal labeling"
            ]
        },
        "publishedAt": "2025-09-26T13:59:54.000Z",
        "title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\n  LLMs",
        "summary": "Can humans identify AI-generated (fake) videos and provide grounded reasons?\nWhile video generation models have advanced rapidly, a critical dimension --\nwhether humans can detect deepfake traces within a generated video, i.e.,\nspatiotemporal grounded visual artifacts that reveal a video as machine\ngenerated -- has been largely overlooked. We introduce DeeptraceReward, the\nfirst fine-grained, spatially- and temporally- aware benchmark that annotates\nhuman-perceived fake traces for video generation reward. The dataset comprises\n4.3K detailed annotations across 3.3K high-quality generated videos. Each\nannotation provides a natural-language explanation, pinpoints a bounding-box\nregion containing the perceived trace, and marks precise onset and offset\ntimestamps. We consolidate these annotations into 9 major categories of\ndeepfake traces that lead humans to identify a video as AI-generated, and train\nmultimodal language models (LMs) as reward models to mimic human judgments and\nlocalizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by\n34.7% on average across fake clue identification, grounding, and explanation.\nInterestingly, we observe a consistent difficulty gradient: binary fake v.s.\nreal classification is substantially easier than fine-grained deepfake trace\ndetection; within the latter, performance degrades from natural language\nexplanations (easiest), to spatial grounding, to temporal labeling (hardest).\nBy foregrounding human-perceived deepfake traces, DeeptraceReward provides a\nrigorous testbed and training signal for socially aware and trustworthy video\ngeneration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22646.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6336091b2db86a181ccd6054",
            "avatarUrl": "/avatars/829f69436225d05d2c2136bc90f640d7.svg",
            "fullname": "Xingyu Fu",
            "name": "Fiaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "submitterOrganization": {
            "_id": "6735d51c08a190b1caea1f29",
            "name": "PrincetonUniversity",
            "fullname": "Princeton University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26231",
            "authors": [
                {
                    "_id": "68dc9cd34159d1f2418f9b8e",
                    "name": "Jiayi Guo",
                    "hidden": false
                },
                {
                    "_id": "68dc9cd34159d1f2418f9b8f",
                    "name": "Chuanhao Yan",
                    "hidden": false
                },
                {
                    "_id": "68dc9cd34159d1f2418f9b90",
                    "name": "Xingqian Xu",
                    "hidden": false
                },
                {
                    "_id": "68dc9cd34159d1f2418f9b91",
                    "name": "Yulin Wang",
                    "hidden": false
                },
                {
                    "_id": "68dc9cd34159d1f2418f9b92",
                    "name": "Kai Wang",
                    "hidden": false
                },
                {
                    "_id": "68dc9cd34159d1f2418f9b93",
                    "name": "Gao Huang",
                    "hidden": false
                },
                {
                    "_id": "68dc9cd34159d1f2418f9b94",
                    "name": "Humphrey Shi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T13:27:03.000Z",
            "submittedOnDailyAt": "2025-10-01T01:48:17.637Z",
            "title": "IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance",
            "submittedOnDailyBy": {
                "_id": "6417c111dce1e4c0229ec182",
                "avatarUrl": "/avatars/de65da1f02b136b1af57862d9d0c48f2.svg",
                "isPro": false,
                "fullname": "Jiayi Guo",
                "user": "JiayiGuo821",
                "type": "user"
            },
            "summary": "Ensuring precise multimodal alignment between diffusion-generated images and\ninput prompts has been a long-standing challenge. Earlier works finetune\ndiffusion weight using high-quality preference data, which tends to be limited\nand difficult to scale up. Recent editing-based methods further refine local\nregions of generated images but may compromise overall image quality. In this\nwork, we propose Implicit Multimodal Guidance (IMG), a novel\nre-generation-based multimodal alignment framework that requires no extra data\nor editing operations. Specifically, given a generated image and its prompt,\nIMG a) utilizes a multimodal large language model (MLLM) to identify\nmisalignments; b) introduces an Implicit Aligner that manipulates diffusion\nconditioning features to reduce misalignments and enable re-generation; and c)\nformulates the re-alignment goal into a trainable objective, namely Iteratively\nUpdated Preference Objective. Extensive qualitative and quantitative\nevaluations on SDXL, SDXL-DPO, and FLUX show that IMG outperforms existing\nalignment methods. Furthermore, IMG acts as a flexible plug-and-play adapter,\nseamlessly enhancing prior finetuning-based alignment methods. Our code will be\navailable at https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment.",
            "upvotes": 13,
            "discussionId": "68dc9cd44159d1f2418f9b95",
            "githubRepo": "https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment",
            "ai_summary": "Implicit Multimodal Guidance (IMG) enhances multimodal alignment between diffusion-generated images and prompts without additional data or editing, outperforming existing methods.",
            "ai_keywords": [
                "diffusion-generated images",
                "multimodal alignment",
                "diffusion weight",
                "preference data",
                "editing-based methods",
                "Implicit Multimodal Guidance",
                "multimodal large language model",
                "Implicit Aligner",
                "diffusion conditioning features",
                "Iteratively Updated Preference Objective",
                "SDXL",
                "SDXL-DPO",
                "FLUX"
            ],
            "githubStars": 25
        },
        "publishedAt": "2025-09-30T09:27:03.000Z",
        "title": "IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance",
        "summary": "Ensuring precise multimodal alignment between diffusion-generated images and\ninput prompts has been a long-standing challenge. Earlier works finetune\ndiffusion weight using high-quality preference data, which tends to be limited\nand difficult to scale up. Recent editing-based methods further refine local\nregions of generated images but may compromise overall image quality. In this\nwork, we propose Implicit Multimodal Guidance (IMG), a novel\nre-generation-based multimodal alignment framework that requires no extra data\nor editing operations. Specifically, given a generated image and its prompt,\nIMG a) utilizes a multimodal large language model (MLLM) to identify\nmisalignments; b) introduces an Implicit Aligner that manipulates diffusion\nconditioning features to reduce misalignments and enable re-generation; and c)\nformulates the re-alignment goal into a trainable objective, namely Iteratively\nUpdated Preference Objective. Extensive qualitative and quantitative\nevaluations on SDXL, SDXL-DPO, and FLUX show that IMG outperforms existing\nalignment methods. Furthermore, IMG acts as a flexible plug-and-play adapter,\nseamlessly enhancing prior finetuning-based alignment methods. Our code will be\navailable at https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26231.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6417c111dce1e4c0229ec182",
            "avatarUrl": "/avatars/de65da1f02b136b1af57862d9d0c48f2.svg",
            "fullname": "Jiayi Guo",
            "name": "JiayiGuo821",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "submitterOrganization": {
            "_id": "62675f8cdacab364889b6ca7",
            "name": "shi-labs",
            "fullname": "SHI Labs",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1650941798672-61e1188afc27c0f5e3641eb3.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26603",
            "authors": [
                {
                    "_id": "68dcd65b4159d1f2418f9c68",
                    "name": "Yixuan Weng",
                    "hidden": false
                },
                {
                    "_id": "68dcd65b4159d1f2418f9c69",
                    "name": "Minjun Zhu",
                    "hidden": false
                },
                {
                    "_id": "68dcd65b4159d1f2418f9c6a",
                    "name": "Qiujie Xie",
                    "hidden": false
                },
                {
                    "_id": "68dcd65b4159d1f2418f9c6b",
                    "name": "Qiyao Sun",
                    "hidden": false
                },
                {
                    "_id": "68dcd65b4159d1f2418f9c6c",
                    "name": "Zhen Lin",
                    "hidden": false
                },
                {
                    "_id": "68dcd65b4159d1f2418f9c6d",
                    "name": "Sifan Liu",
                    "hidden": false
                },
                {
                    "_id": "68dcd65b4159d1f2418f9c6e",
                    "name": "Yue Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/611568222999876a45605af5/ngc1-pgZhq8lIVqYyEbqa.mp4"
            ],
            "publishedAt": "2025-09-30T17:49:32.000Z",
            "submittedOnDailyAt": "2025-10-01T06:04:39.689Z",
            "title": "DeepScientist: Advancing Frontier-Pushing Scientific Findings\n  Progressively",
            "submittedOnDailyBy": {
                "_id": "611568222999876a45605af5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1628792849997-noauth.jpeg",
                "isPro": false,
                "fullname": "WENGSYX",
                "user": "WENGSYX",
                "type": "user"
            },
            "summary": "While previous AI Scientist systems can generate novel findings, they often\nlack the focus to produce scientifically valuable contributions that address\npressing human-defined challenges. We introduce DeepScientist, a system\ndesigned to overcome this by conducting goal-oriented, fully autonomous\nscientific discovery over month-long timelines. It formalizes discovery as a\nBayesian Optimization problem, operationalized through a hierarchical\nevaluation process consisting of \"hypothesize, verify, and analyze\". Leveraging\na cumulative Findings Memory, this loop intelligently balances the exploration\nof novel hypotheses with exploitation, selectively promoting the most promising\nfindings to higher-fidelity levels of validation. Consuming over 20,000 GPU\nhours, the system generated about 5,000 unique scientific ideas and\nexperimentally validated approximately 1100 of them, ultimately surpassing\nhuman-designed state-of-the-art (SOTA) methods on three frontier AI tasks by\n183.7\\%, 1.9\\%, and 7.9\\%. This work provides the first large-scale evidence of\nan AI achieving discoveries that progressively surpass human SOTA on scientific\ntasks, producing valuable findings that genuinely push the frontier of\nscientific discovery. To facilitate further research into this process, we will\nopen-source all experimental logs and system code at\nhttps://github.com/ResearAI/DeepScientist/.",
            "upvotes": 12,
            "discussionId": "68dcd65b4159d1f2418f9c6f",
            "projectPage": "https://ai-researcher.net",
            "githubRepo": "https://github.com/ResearAI/DeepScientist",
            "ai_summary": "DeepScientist autonomously conducts scientific discovery through Bayesian Optimization, surpassing human state-of-the-art methods on multiple AI tasks.",
            "ai_keywords": [
                "Bayesian Optimization",
                "hierarchical evaluation",
                "Findings Memory",
                "exploration",
                "exploitation",
                "scientific discovery",
                "state-of-the-art",
                "AI tasks"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-09-30T13:49:32.000Z",
        "title": "DeepScientist: Advancing Frontier-Pushing Scientific Findings\n  Progressively",
        "summary": "While previous AI Scientist systems can generate novel findings, they often\nlack the focus to produce scientifically valuable contributions that address\npressing human-defined challenges. We introduce DeepScientist, a system\ndesigned to overcome this by conducting goal-oriented, fully autonomous\nscientific discovery over month-long timelines. It formalizes discovery as a\nBayesian Optimization problem, operationalized through a hierarchical\nevaluation process consisting of \"hypothesize, verify, and analyze\". Leveraging\na cumulative Findings Memory, this loop intelligently balances the exploration\nof novel hypotheses with exploitation, selectively promoting the most promising\nfindings to higher-fidelity levels of validation. Consuming over 20,000 GPU\nhours, the system generated about 5,000 unique scientific ideas and\nexperimentally validated approximately 1100 of them, ultimately surpassing\nhuman-designed state-of-the-art (SOTA) methods on three frontier AI tasks by\n183.7\\%, 1.9\\%, and 7.9\\%. This work provides the first large-scale evidence of\nan AI achieving discoveries that progressively surpass human SOTA on scientific\ntasks, producing valuable findings that genuinely push the frontier of\nscientific discovery. To facilitate further research into this process, we will\nopen-source all experimental logs and system code at\nhttps://github.com/ResearAI/DeepScientist/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/611568222999876a45605af5/ngc1-pgZhq8lIVqYyEbqa.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26603.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "611568222999876a45605af5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1628792849997-noauth.jpeg",
            "fullname": "WENGSYX",
            "name": "WENGSYX",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "submitterOrganization": {
            "_id": "66bb231e40d36c70d6ad0c4b",
            "name": "WestlakeNLP",
            "fullname": "Text Intelligence Lab of Westlake University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/622ee9f3165ba2c1bcbc7706/KpIm3isRczYp7kSnfNGSL.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26391",
            "authors": [
                {
                    "_id": "68dc90ca4159d1f2418f9ab8",
                    "user": {
                        "_id": "65e55ca4a0681de63022843e",
                        "avatarUrl": "/avatars/5b1ac4a81f0c38fda6f47b392f7474c8.svg",
                        "isPro": false,
                        "fullname": "zhu chenhui",
                        "user": "flateon",
                        "type": "user"
                    },
                    "name": "Chenhui Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:21:56.849Z",
                    "hidden": false
                },
                {
                    "_id": "68dc90ca4159d1f2418f9ab9",
                    "name": "Yilu Wu",
                    "hidden": false
                },
                {
                    "_id": "68dc90ca4159d1f2418f9aba",
                    "user": {
                        "_id": "66615c855fd9d736e670e0a9",
                        "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
                        "isPro": false,
                        "fullname": "wangshuai",
                        "user": "wangsssssss",
                        "type": "user"
                    },
                    "name": "Shuai Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:21:54.463Z",
                    "hidden": false
                },
                {
                    "_id": "68dc90ca4159d1f2418f9abb",
                    "name": "Gangshan Wu",
                    "hidden": false
                },
                {
                    "_id": "68dc90ca4159d1f2418f9abc",
                    "name": "Limin Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T15:26:04.000Z",
            "submittedOnDailyAt": "2025-10-01T01:00:58.800Z",
            "title": "MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation",
            "submittedOnDailyBy": {
                "_id": "65e55ca4a0681de63022843e",
                "avatarUrl": "/avatars/5b1ac4a81f0c38fda6f47b392f7474c8.svg",
                "isPro": false,
                "fullname": "zhu chenhui",
                "user": "flateon",
                "type": "user"
            },
            "summary": "Image-to-video generation has made remarkable progress with the advancements\nin diffusion models, yet generating videos with realistic motion remains highly\nchallenging. This difficulty arises from the complexity of accurately modeling\nmotion, which involves capturing physical constraints, object interactions, and\ndomain-specific dynamics that are not easily generalized across diverse\nscenarios. To address this, we propose MotionRAG, a retrieval-augmented\nframework that enhances motion realism by adapting motion priors from relevant\nreference videos through Context-Aware Motion Adaptation (CAMA). The key\ntechnical innovations include: (i) a retrieval-based pipeline extracting\nhigh-level motion features using video encoder and specialized resamplers to\ndistill semantic motion representations; (ii) an in-context learning approach\nfor motion adaptation implemented through a causal transformer architecture;\n(iii) an attention-based motion injection adapter that seamlessly integrates\ntransferred motion features into pretrained video diffusion models. Extensive\nexperiments demonstrate that our method achieves significant improvements\nacross multiple domains and various base models, all with negligible\ncomputational overhead during inference. Furthermore, our modular design\nenables zero-shot generalization to new domains by simply updating the\nretrieval database without retraining any components. This research enhances\nthe core capability of video generation systems by enabling the effective\nretrieval and transfer of motion priors, facilitating the synthesis of\nrealistic motion dynamics.",
            "upvotes": 12,
            "discussionId": "68dc90ca4159d1f2418f9abd",
            "githubRepo": "https://github.com/MCG-NJU/MotionRAG",
            "ai_summary": "MotionRAG enhances video generation by integrating motion priors from reference videos using a retrieval-augmented framework, improving motion realism with negligible computational overhead.",
            "ai_keywords": [
                "diffusion models",
                "MotionRAG",
                "Context-Aware Motion Adaptation (CAMA)",
                "video encoder",
                "specialized resamplers",
                "causal transformer architecture",
                "attention-based motion injection adapter",
                "zero-shot generalization"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-09-30T11:26:04.000Z",
        "title": "MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation",
        "summary": "Image-to-video generation has made remarkable progress with the advancements\nin diffusion models, yet generating videos with realistic motion remains highly\nchallenging. This difficulty arises from the complexity of accurately modeling\nmotion, which involves capturing physical constraints, object interactions, and\ndomain-specific dynamics that are not easily generalized across diverse\nscenarios. To address this, we propose MotionRAG, a retrieval-augmented\nframework that enhances motion realism by adapting motion priors from relevant\nreference videos through Context-Aware Motion Adaptation (CAMA). The key\ntechnical innovations include: (i) a retrieval-based pipeline extracting\nhigh-level motion features using video encoder and specialized resamplers to\ndistill semantic motion representations; (ii) an in-context learning approach\nfor motion adaptation implemented through a causal transformer architecture;\n(iii) an attention-based motion injection adapter that seamlessly integrates\ntransferred motion features into pretrained video diffusion models. Extensive\nexperiments demonstrate that our method achieves significant improvements\nacross multiple domains and various base models, all with negligible\ncomputational overhead during inference. Furthermore, our modular design\nenables zero-shot generalization to new domains by simply updating the\nretrieval database without retraining any components. This research enhances\nthe core capability of video generation systems by enabling the effective\nretrieval and transfer of motion priors, facilitating the synthesis of\nrealistic motion dynamics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26391.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e55ca4a0681de63022843e",
            "avatarUrl": "/avatars/5b1ac4a81f0c38fda6f47b392f7474c8.svg",
            "fullname": "zhu chenhui",
            "name": "flateon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.23610",
            "authors": [
                {
                    "_id": "68dba99cd2bf1f4b15ec7950",
                    "name": "Kai Li",
                    "hidden": false
                },
                {
                    "_id": "68dba99cd2bf1f4b15ec7951",
                    "name": "Kejun Gao",
                    "hidden": false
                },
                {
                    "_id": "68dba99cd2bf1f4b15ec7952",
                    "name": "Xiaolin Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T03:25:34.000Z",
            "submittedOnDailyAt": "2025-10-01T00:42:52.519Z",
            "title": "Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and\n  Multi-Scale Global-Local Attention",
            "submittedOnDailyBy": {
                "_id": "6387676c23da90491eb9fb16",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
                "isPro": true,
                "fullname": "Kai Li",
                "user": "JusperLee",
                "type": "user"
            },
            "summary": "Audio-visual speech separation (AVSS) methods leverage visual cues to extract\ntarget speech and have demonstrated strong separation quality in noisy acoustic\nenvironments. However, these methods usually involve a large number of\nparameters and require high computational cost, which is unacceptable in many\napplications where speech separation serves as only a preprocessing step for\nfurther speech processing. To address this issue, we propose an efficient AVSS\nmethod, named Dolphin. For visual feature extraction, we develop DP-LipCoder, a\ndual-path lightweight video encoder that transforms lip-motion into discrete\naudio-aligned semantic tokens. For audio separation, we construct a lightweight\nencoder-decoder separator, in which each layer incorporates a global-local\nattention (GLA) block to efficiently capture multi-scale dependencies.\nExperiments on three benchmark datasets showed that Dolphin not only surpassed\nthe current state-of-the-art (SOTA) model in separation quality but also\nachieved remarkable improvements in efficiency: over 50% fewer parameters, more\nthan 2.4x reduction in MACs, and over 6x faster GPU inference speed. These\nresults indicate that Dolphin offers a practical and deployable solution for\nhigh-performance AVSS in real-world scenarios. Our code and demo page are\npublicly available at http://cslikai.cn/Dolphin/.",
            "upvotes": 12,
            "discussionId": "68dba99cd2bf1f4b15ec7953",
            "projectPage": "https://cslikai.cn/Dolphin",
            "githubRepo": "https://github.com/JusperLee/Dolphin",
            "ai_summary": "Dolphin, an efficient AVSS method, uses a dual-path lightweight video encoder and a lightweight encoder-decoder separator with global-local attention blocks to achieve high separation quality and significant computational efficiency.",
            "ai_keywords": [
                "dual-path lightweight video encoder",
                "DP-LipCoder",
                "discrete audio-aligned semantic tokens",
                "lightweight encoder-decoder separator",
                "global-local attention (GLA) block",
                "multi-scale dependencies",
                "state-of-the-art (SOTA)",
                "MACs",
                "GPU inference speed"
            ],
            "githubStars": 9
        },
        "publishedAt": "2025-09-27T23:25:34.000Z",
        "title": "Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and\n  Multi-Scale Global-Local Attention",
        "summary": "Audio-visual speech separation (AVSS) methods leverage visual cues to extract\ntarget speech and have demonstrated strong separation quality in noisy acoustic\nenvironments. However, these methods usually involve a large number of\nparameters and require high computational cost, which is unacceptable in many\napplications where speech separation serves as only a preprocessing step for\nfurther speech processing. To address this issue, we propose an efficient AVSS\nmethod, named Dolphin. For visual feature extraction, we develop DP-LipCoder, a\ndual-path lightweight video encoder that transforms lip-motion into discrete\naudio-aligned semantic tokens. For audio separation, we construct a lightweight\nencoder-decoder separator, in which each layer incorporates a global-local\nattention (GLA) block to efficiently capture multi-scale dependencies.\nExperiments on three benchmark datasets showed that Dolphin not only surpassed\nthe current state-of-the-art (SOTA) model in separation quality but also\nachieved remarkable improvements in efficiency: over 50% fewer parameters, more\nthan 2.4x reduction in MACs, and over 6x faster GPU inference speed. These\nresults indicate that Dolphin offers a practical and deployable solution for\nhigh-performance AVSS in real-world scenarios. Our code and demo page are\npublicly available at http://cslikai.cn/Dolphin/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23610.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6387676c23da90491eb9fb16",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
            "fullname": "Kai Li",
            "name": "JusperLee",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 29
        },
        "submitterOrganization": {
            "_id": "628735cbc83a2d6ab8d14a66",
            "name": "Tsinghua",
            "fullname": "Tsinghua University"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26618",
            "authors": [
                {
                    "_id": "68dcbd594159d1f2418f9c0e",
                    "user": {
                        "_id": "641d211e353524fe41f16387",
                        "avatarUrl": "/avatars/c6e72c82c029b415a035beebee50b52c.svg",
                        "isPro": false,
                        "fullname": "Haodong Li",
                        "user": "haodongli",
                        "type": "user"
                    },
                    "name": "Haodong Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:20:53.957Z",
                    "hidden": false
                },
                {
                    "_id": "68dcbd594159d1f2418f9c0f",
                    "name": "Wangguangdong Zheng",
                    "hidden": false
                },
                {
                    "_id": "68dcbd594159d1f2418f9c10",
                    "name": "Jing He",
                    "hidden": false
                },
                {
                    "_id": "68dcbd594159d1f2418f9c11",
                    "name": "Yuhao Liu",
                    "hidden": false
                },
                {
                    "_id": "68dcbd594159d1f2418f9c12",
                    "name": "Xin Lin",
                    "hidden": false
                },
                {
                    "_id": "68dcbd594159d1f2418f9c13",
                    "name": "Xin Yang",
                    "hidden": false
                },
                {
                    "_id": "68dcbd594159d1f2418f9c14",
                    "name": "Ying-Cong Chen",
                    "hidden": false
                },
                {
                    "_id": "68dcbd594159d1f2418f9c15",
                    "name": "Chunchao Guo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/641d211e353524fe41f16387/OZ55UfUOPi7nlgeuUyRyV.mp4"
            ],
            "publishedAt": "2025-09-30T17:55:37.000Z",
            "submittedOnDailyAt": "2025-10-01T04:05:15.902Z",
            "title": "DA^2: Depth Anything in Any Direction",
            "submittedOnDailyBy": {
                "_id": "641d211e353524fe41f16387",
                "avatarUrl": "/avatars/c6e72c82c029b415a035beebee50b52c.svg",
                "isPro": false,
                "fullname": "Haodong Li",
                "user": "haodongli",
                "type": "user"
            },
            "summary": "Panorama has a full FoV (360^circtimes180^circ), offering a more\ncomplete visual description than perspective images. Thanks to this\ncharacteristic, panoramic depth estimation is gaining increasing traction in 3D\nvision. However, due to the scarcity of panoramic data, previous methods are\noften restricted to in-domain settings, leading to poor zero-shot\ngeneralization. Furthermore, due to the spherical distortions inherent in\npanoramas, many approaches rely on perspective splitting (e.g., cubemaps),\nwhich leads to suboptimal efficiency. To address these challenges, we propose\nDA^{2}: Depth Anything in\nAny Direction, an accurate, zero-shot generalizable, and\nfully end-to-end panoramic depth estimator. Specifically, for scaling up\npanoramic data, we introduce a data curation engine for generating high-quality\npanoramic depth data from perspective, and create sim543K panoramic\nRGB-depth pairs, bringing the total to sim607K. To further mitigate the\nspherical distortions, we present SphereViT, which explicitly leverages\nspherical coordinates to enforce the spherical geometric consistency in\npanoramic image features, yielding improved performance. A comprehensive\nbenchmark on multiple datasets clearly demonstrates DA^{2}'s SoTA\nperformance, with an average 38% improvement on AbsRel over the strongest\nzero-shot baseline. Surprisingly, DA^{2} even outperforms prior in-domain\nmethods, highlighting its superior zero-shot generalization. Moreover, as an\nend-to-end solution, DA^{2} exhibits much higher efficiency over fusion-based\napproaches. Both the code and the curated panoramic data will be released.\nProject page: https://depth-any-in-any-dir.github.io/.",
            "upvotes": 11,
            "discussionId": "68dcbd5a4159d1f2418f9c16",
            "projectPage": "https://depth-any-in-any-dir.github.io/",
            "githubRepo": "https://github.com/EnVision-Research/DA-2",
            "ai_summary": "DA², a zero-shot generalizable and fully end-to-end panoramic depth estimator, addresses challenges in panoramic depth estimation by using a data curation engine and SphereViT to handle spherical distortions, achieving state-of-the-art performance.",
            "ai_keywords": [
                "panoramic depth estimation",
                "3D vision",
                "zero-shot generalization",
                "perspective splitting",
                "cubemaps",
                "data curation engine",
                "SphereViT",
                "spherical coordinates",
                "spherical geometric consistency",
                "AbsRel",
                "in-domain methods",
                "fusion-based approaches"
            ],
            "githubStars": 50
        },
        "publishedAt": "2025-09-30T13:55:37.000Z",
        "title": "DA^2: Depth Anything in Any Direction",
        "summary": "Panorama has a full FoV (360^circtimes180^circ), offering a more\ncomplete visual description than perspective images. Thanks to this\ncharacteristic, panoramic depth estimation is gaining increasing traction in 3D\nvision. However, due to the scarcity of panoramic data, previous methods are\noften restricted to in-domain settings, leading to poor zero-shot\ngeneralization. Furthermore, due to the spherical distortions inherent in\npanoramas, many approaches rely on perspective splitting (e.g., cubemaps),\nwhich leads to suboptimal efficiency. To address these challenges, we propose\nDA^{2}: Depth Anything in\nAny Direction, an accurate, zero-shot generalizable, and\nfully end-to-end panoramic depth estimator. Specifically, for scaling up\npanoramic data, we introduce a data curation engine for generating high-quality\npanoramic depth data from perspective, and create sim543K panoramic\nRGB-depth pairs, bringing the total to sim607K. To further mitigate the\nspherical distortions, we present SphereViT, which explicitly leverages\nspherical coordinates to enforce the spherical geometric consistency in\npanoramic image features, yielding improved performance. A comprehensive\nbenchmark on multiple datasets clearly demonstrates DA^{2}'s SoTA\nperformance, with an average 38% improvement on AbsRel over the strongest\nzero-shot baseline. Surprisingly, DA^{2} even outperforms prior in-domain\nmethods, highlighting its superior zero-shot generalization. Moreover, as an\nend-to-end solution, DA^{2} exhibits much higher efficiency over fusion-based\napproaches. Both the code and the curated panoramic data will be released.\nProject page: https://depth-any-in-any-dir.github.io/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/641d211e353524fe41f16387/OZ55UfUOPi7nlgeuUyRyV.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26618.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "641d211e353524fe41f16387",
            "avatarUrl": "/avatars/c6e72c82c029b415a035beebee50b52c.svg",
            "fullname": "Haodong Li",
            "name": "haodongli",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "submitterOrganization": {
            "_id": "6645f953c39288df638dbdd5",
            "name": "Tencent-Hunyuan",
            "fullname": "Tencent Hunyuan",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25911",
            "authors": [
                {
                    "_id": "68dcc1a04159d1f2418f9c28",
                    "name": "Yu Wang",
                    "hidden": false
                },
                {
                    "_id": "68dcc1a04159d1f2418f9c29",
                    "name": "Ryuichi Takanobu",
                    "hidden": false
                },
                {
                    "_id": "68dcc1a04159d1f2418f9c2a",
                    "user": {
                        "_id": "68c8ff3df0714530f2d75b22",
                        "avatarUrl": "/avatars/984b32090b9026dc31d54b93e9ac6a9e.svg",
                        "isPro": false,
                        "fullname": "Zhiqi Liang",
                        "user": "zkadelzq",
                        "type": "user"
                    },
                    "name": "Zhiqi Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:20:49.318Z",
                    "hidden": false
                },
                {
                    "_id": "68dcc1a04159d1f2418f9c2b",
                    "name": "Yuzhen Mao",
                    "hidden": false
                },
                {
                    "_id": "68dcc1a04159d1f2418f9c2c",
                    "user": {
                        "_id": "686647f3e2cca4bc45818704",
                        "avatarUrl": "/avatars/1efeff78892223b6e5592d4ef994f7c3.svg",
                        "isPro": false,
                        "fullname": "YUANZHE HU",
                        "user": "ai-hyz",
                        "type": "user"
                    },
                    "name": "Yuanzhe Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:20:51.430Z",
                    "hidden": false
                },
                {
                    "_id": "68dcc1a04159d1f2418f9c2d",
                    "name": "Julian McAuley",
                    "hidden": false
                },
                {
                    "_id": "68dcc1a04159d1f2418f9c2e",
                    "name": "Xiaojian Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T08:02:34.000Z",
            "submittedOnDailyAt": "2025-10-01T04:23:00.484Z",
            "title": "Mem-α: Learning Memory Construction via Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "686647f3e2cca4bc45818704",
                "avatarUrl": "/avatars/1efeff78892223b6e5592d4ef994f7c3.svg",
                "isPro": false,
                "fullname": "YUANZHE HU",
                "user": "ai-hyz",
                "type": "user"
            },
            "summary": "Large language model (LLM) agents are constrained by limited context windows,\nnecessitating external memory systems for long-term information understanding.\nCurrent memory-augmented agents typically depend on pre-defined instructions\nand tools for memory updates. However, language models may lack the ability to\ndetermine which information to store, how to structure it, and when to update\nit, especially as memory systems become more complex. This results in\nsuboptimal memory construction and information loss. To this end, we propose\nMem-alpha, a reinforcement learning framework that trains agents to effectively\nmanage complex memory systems through interaction and feedback. We also\nconstruct a specialized training dataset spanning diverse multi-turn\ninteraction patterns paired with comprehensive evaluation questions designed to\nteach effective memory management. During training, agents process sequential\ninformation chunks, learn to extract and store relevant content, then update\nthe memory system. The reward signal derives from downstream question-answering\naccuracy over the full interaction history, directly optimizing for memory\nconstruction. To illustrate the effectiveness of our training framework, we\ndesign a memory architecture comprising core, episodic, and semantic\ncomponents, equipped with multiple tools for memory operations. Empirical\nevaluation demonstrates that Mem-alpha achieves significant improvements over\nexisting memory-augmented agent baselines. Despite being trained exclusively on\ninstances with a maximum length of 30k tokens, our agents exhibit remarkable\ngeneralization to sequences exceeding 400k tokens, over 13x the training\nlength, highlighting the robustness of Mem-alpha.",
            "upvotes": 10,
            "discussionId": "68dcc1a04159d1f2418f9c2f",
            "ai_summary": "Mem-alpha, a reinforcement learning framework, enhances memory management in large language models through interaction and feedback, improving performance and generalization in long-term information understanding.",
            "ai_keywords": [
                "reinforcement learning",
                "memory-augmented agents",
                "memory management",
                "interaction patterns",
                "episodic memory",
                "semantic memory",
                "memory operations",
                "question-answering accuracy",
                "generalization"
            ]
        },
        "publishedAt": "2025-09-30T04:02:34.000Z",
        "title": "Mem-α: Learning Memory Construction via Reinforcement Learning",
        "summary": "Large language model (LLM) agents are constrained by limited context windows,\nnecessitating external memory systems for long-term information understanding.\nCurrent memory-augmented agents typically depend on pre-defined instructions\nand tools for memory updates. However, language models may lack the ability to\ndetermine which information to store, how to structure it, and when to update\nit, especially as memory systems become more complex. This results in\nsuboptimal memory construction and information loss. To this end, we propose\nMem-alpha, a reinforcement learning framework that trains agents to effectively\nmanage complex memory systems through interaction and feedback. We also\nconstruct a specialized training dataset spanning diverse multi-turn\ninteraction patterns paired with comprehensive evaluation questions designed to\nteach effective memory management. During training, agents process sequential\ninformation chunks, learn to extract and store relevant content, then update\nthe memory system. The reward signal derives from downstream question-answering\naccuracy over the full interaction history, directly optimizing for memory\nconstruction. To illustrate the effectiveness of our training framework, we\ndesign a memory architecture comprising core, episodic, and semantic\ncomponents, equipped with multiple tools for memory operations. Empirical\nevaluation demonstrates that Mem-alpha achieves significant improvements over\nexisting memory-augmented agent baselines. Despite being trained exclusively on\ninstances with a maximum length of 30k tokens, our agents exhibit remarkable\ngeneralization to sequences exceeding 400k tokens, over 13x the training\nlength, highlighting the robustness of Mem-alpha.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25911.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "686647f3e2cca4bc45818704",
            "avatarUrl": "/avatars/1efeff78892223b6e5592d4ef994f7c3.svg",
            "fullname": "YUANZHE HU",
            "name": "ai-hyz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26495",
            "authors": [
                {
                    "_id": "68dc986b4159d1f2418f9b7a",
                    "user": {
                        "_id": "65756c5e1488186315c6696d",
                        "avatarUrl": "/avatars/2ec67e0d1921b6635c69669b591a3110.svg",
                        "isPro": false,
                        "fullname": "Jingdi Lei",
                        "user": "huaXiaKyrie",
                        "type": "user"
                    },
                    "name": "Jingdi Lei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:21:37.129Z",
                    "hidden": false
                },
                {
                    "_id": "68dc986b4159d1f2418f9b7b",
                    "name": "Varun Gumma",
                    "hidden": false
                },
                {
                    "_id": "68dc986b4159d1f2418f9b7c",
                    "name": "Rishabh Bhardwaj",
                    "hidden": false
                },
                {
                    "_id": "68dc986b4159d1f2418f9b7d",
                    "name": "Seok Min Lim",
                    "hidden": false
                },
                {
                    "_id": "68dc986b4159d1f2418f9b7e",
                    "name": "Chuan Li",
                    "hidden": false
                },
                {
                    "_id": "68dc986b4159d1f2418f9b7f",
                    "name": "Amir Zadeh",
                    "hidden": false
                },
                {
                    "_id": "68dc986b4159d1f2418f9b80",
                    "name": "Soujanya Poria",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T16:39:17.000Z",
            "submittedOnDailyAt": "2025-10-01T01:58:41.393Z",
            "title": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost\n  Always!",
            "submittedOnDailyBy": {
                "_id": "626b626405fe1cb65725aca1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/ZVSbhynzpQhVGq9kGywW6.png",
                "isPro": false,
                "fullname": "Soujanya Poria",
                "user": "soujanyaporia",
                "type": "user"
            },
            "summary": "Large Language Model (LLM) safety is one of the most pressing challenges for\nenabling wide-scale deployment. While most studies and global discussions focus\non generic harms, such as models assisting users in harming themselves or\nothers, enterprises face a more fundamental concern: whether LLM-based agents\nare safe for their intended use case. To address this, we introduce operational\nsafety, defined as an LLM's ability to appropriately accept or refuse user\nqueries when tasked with a specific purpose. We further propose OffTopicEval,\nan evaluation suite and benchmark for measuring operational safety both in\ngeneral and within specific agentic use cases. Our evaluations on six model\nfamilies comprising 20 open-weight LLMs reveal that while performance varies\nacross models, all of them remain highly operationally unsafe. Even the\nstrongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\%\n-- fall far short of reliable operational safety, while GPT models plateau in\nthe 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma\nand Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational\nsafety is a core model alignment issue, to suppress these failures, we propose\nprompt-based steering methods: query grounding (Q-ground) and system-prompt\ngrounding (P-ground), which substantially improve OOD refusal. Q-ground\nprovides consistent gains of up to 23\\%, while P-ground delivers even larger\nboosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results\nhighlight both the urgent need for operational safety interventions and the\npromise of prompt-based steering as a first step toward more reliable LLM-based\nagents.",
            "upvotes": 9,
            "discussionId": "68dc986c4159d1f2418f9b81",
            "ai_summary": "Operational safety, measured by OffTopicEval, is a critical issue for LLMs, with most models falling short, but prompt-based steering methods show promise in improving out-of-distribution refusal.",
            "ai_keywords": [
                "Large Language Model (LLM)",
                "operational safety",
                "OffTopicEval",
                "query grounding (Q-ground)",
                "system-prompt grounding (P-ground)",
                "out-of-distribution (OOD) refusal"
            ]
        },
        "publishedAt": "2025-09-30T12:39:17.000Z",
        "title": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost\n  Always!",
        "summary": "Large Language Model (LLM) safety is one of the most pressing challenges for\nenabling wide-scale deployment. While most studies and global discussions focus\non generic harms, such as models assisting users in harming themselves or\nothers, enterprises face a more fundamental concern: whether LLM-based agents\nare safe for their intended use case. To address this, we introduce operational\nsafety, defined as an LLM's ability to appropriately accept or refuse user\nqueries when tasked with a specific purpose. We further propose OffTopicEval,\nan evaluation suite and benchmark for measuring operational safety both in\ngeneral and within specific agentic use cases. Our evaluations on six model\nfamilies comprising 20 open-weight LLMs reveal that while performance varies\nacross models, all of them remain highly operationally unsafe. Even the\nstrongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\%\n-- fall far short of reliable operational safety, while GPT models plateau in\nthe 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma\nand Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational\nsafety is a core model alignment issue, to suppress these failures, we propose\nprompt-based steering methods: query grounding (Q-ground) and system-prompt\ngrounding (P-ground), which substantially improve OOD refusal. Q-ground\nprovides consistent gains of up to 23\\%, while P-ground delivers even larger\nboosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results\nhighlight both the urgent need for operational safety interventions and the\npromise of prompt-based steering as a first step toward more reliable LLM-based\nagents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26495.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "626b626405fe1cb65725aca1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/ZVSbhynzpQhVGq9kGywW6.png",
            "fullname": "Soujanya Poria",
            "name": "soujanyaporia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "submitterOrganization": {
            "_id": "626ab9dac804c432c1b27a48",
            "name": "declare-lab",
            "fullname": "Deep Cognition and Language Research (DeCLaRe) Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/grq3rj2uj0WRjjPjAtR1I.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26030",
            "authors": [
                {
                    "_id": "68dd3d8cd80e1286a364fdb3",
                    "name": "Shuche Wang",
                    "hidden": false
                },
                {
                    "_id": "68dd3d8cd80e1286a364fdb4",
                    "name": "Fengzhuo Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dd3d8cd80e1286a364fdb5",
                    "name": "Jiaxiang Li",
                    "hidden": false
                },
                {
                    "_id": "68dd3d8cd80e1286a364fdb6",
                    "name": "Cunxiao Du",
                    "hidden": false
                },
                {
                    "_id": "68dd3d8cd80e1286a364fdb7",
                    "name": "Chao Du",
                    "hidden": false
                },
                {
                    "_id": "68dd3d8cd80e1286a364fdb8",
                    "name": "Tianyu Pang",
                    "hidden": false
                },
                {
                    "_id": "68dd3d8cd80e1286a364fdb9",
                    "name": "Zhuoran Yang",
                    "hidden": false
                },
                {
                    "_id": "68dd3d8cd80e1286a364fdba",
                    "name": "Mingyi Hong",
                    "hidden": false
                },
                {
                    "_id": "68dd3d8cd80e1286a364fdbb",
                    "name": "Vincent Y. F. Tan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T10:04:08.000Z",
            "submittedOnDailyAt": "2025-10-01T13:14:18.199Z",
            "title": "Muon Outperforms Adam in Tail-End Associative Memory Learning",
            "submittedOnDailyBy": {
                "_id": "64b8c1a995bd42c7707f7918",
                "avatarUrl": "/avatars/08c2929f8f150ecd6f8e5a06c4cb9034.svg",
                "isPro": false,
                "fullname": "Fengzhuo Zhang",
                "user": "Fengzhuo",
                "type": "user"
            },
            "summary": "The Muon optimizer is consistently faster than Adam in training Large\nLanguage Models (LLMs), yet the mechanism underlying its success remains\nunclear. This paper demystifies this mechanism through the lens of associative\nmemory. By ablating the transformer components optimized by Muon, we reveal\nthat the associative memory parameters of LLMs, namely the Value and Output\n(VO) attention weights and Feed-Forward Networks (FFNs), are the primary\ncontributors to Muon's superiority. Motivated by this associative memory view,\nwe then explain Muon's superiority on real-world corpora, which are\nintrinsically heavy-tailed: a few classes (tail classes) appear far less\nfrequently than others. The superiority is explained through two key\nproperties: (i) its update rule consistently yields a more isotropic singular\nspectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes\ntail classes more effectively than Adam. Beyond empirical evidence, we\ntheoretically confirm these findings by analyzing a one-layer associative\nmemory model under class-imbalanced data. We prove that Muon consistently\nachieves balanced learning across classes regardless of feature embeddings,\nwhereas Adam can induce large disparities in learning errors depending on\nembedding properties. In summary, our empirical observations and theoretical\nanalyses reveal Muon's core advantage: its update rule aligns with the\nouter-product structure of linear associative memories, enabling more balanced\nand effective learning of tail classes in heavy-tailed distributions than Adam.",
            "upvotes": 9,
            "discussionId": "68dd3d8cd80e1286a364fdbc",
            "ai_summary": "Muon optimizer outperforms Adam in training LLMs by effectively optimizing associative memory parameters and balancing learning across classes in heavy-tailed data.",
            "ai_keywords": [
                "Muon optimizer",
                "Adam",
                "Large Language Models (LLMs)",
                "associative memory",
                "transformer components",
                "Value and Output (VO) attention weights",
                "Feed-Forward Networks (FFNs)",
                "heavy-tailed data",
                "singular spectrum",
                "tail classes",
                "class-imbalanced data",
                "balanced learning",
                "outer-product structure",
                "linear associative memories"
            ]
        },
        "publishedAt": "2025-09-30T06:04:08.000Z",
        "title": "Muon Outperforms Adam in Tail-End Associative Memory Learning",
        "summary": "The Muon optimizer is consistently faster than Adam in training Large\nLanguage Models (LLMs), yet the mechanism underlying its success remains\nunclear. This paper demystifies this mechanism through the lens of associative\nmemory. By ablating the transformer components optimized by Muon, we reveal\nthat the associative memory parameters of LLMs, namely the Value and Output\n(VO) attention weights and Feed-Forward Networks (FFNs), are the primary\ncontributors to Muon's superiority. Motivated by this associative memory view,\nwe then explain Muon's superiority on real-world corpora, which are\nintrinsically heavy-tailed: a few classes (tail classes) appear far less\nfrequently than others. The superiority is explained through two key\nproperties: (i) its update rule consistently yields a more isotropic singular\nspectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes\ntail classes more effectively than Adam. Beyond empirical evidence, we\ntheoretically confirm these findings by analyzing a one-layer associative\nmemory model under class-imbalanced data. We prove that Muon consistently\nachieves balanced learning across classes regardless of feature embeddings,\nwhereas Adam can induce large disparities in learning errors depending on\nembedding properties. In summary, our empirical observations and theoretical\nanalyses reveal Muon's core advantage: its update rule aligns with the\nouter-product structure of linear associative memories, enabling more balanced\nand effective learning of tail classes in heavy-tailed distributions than Adam.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26030.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b8c1a995bd42c7707f7918",
            "avatarUrl": "/avatars/08c2929f8f150ecd6f8e5a06c4cb9034.svg",
            "fullname": "Fengzhuo Zhang",
            "name": "Fengzhuo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26542",
            "authors": [
                {
                    "_id": "68dc885c4159d1f2418f9a23",
                    "user": {
                        "_id": "64b5198c25882acb62fb77ef",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b5198c25882acb62fb77ef/HX9pfMEPQlfjvSAgSLplY.png",
                        "isPro": false,
                        "fullname": "Yueqian Lin",
                        "user": "linyueqian",
                        "type": "user"
                    },
                    "name": "Yueqian Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:22:20.736Z",
                    "hidden": false
                },
                {
                    "_id": "68dc885c4159d1f2418f9a24",
                    "name": "Zhengmian Hu",
                    "hidden": false
                },
                {
                    "_id": "68dc885c4159d1f2418f9a25",
                    "name": "Qinsi Wang",
                    "hidden": false
                },
                {
                    "_id": "68dc885c4159d1f2418f9a26",
                    "name": "Yudong Liu",
                    "hidden": false
                },
                {
                    "_id": "68dc885c4159d1f2418f9a27",
                    "name": "Hengfan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dc885c4159d1f2418f9a28",
                    "name": "Jayakumar Subramanian",
                    "hidden": false
                },
                {
                    "_id": "68dc885c4159d1f2418f9a29",
                    "name": "Nikos Vlassis",
                    "hidden": false
                },
                {
                    "_id": "68dc885c4159d1f2418f9a2a",
                    "name": "Hai Helen Li",
                    "hidden": false
                },
                {
                    "_id": "68dc885c4159d1f2418f9a2b",
                    "name": "Yiran Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T17:17:09.000Z",
            "submittedOnDailyAt": "2025-10-01T00:34:18.271Z",
            "title": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced\n  Performance Gap",
            "submittedOnDailyBy": {
                "_id": "64b5198c25882acb62fb77ef",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b5198c25882acb62fb77ef/HX9pfMEPQlfjvSAgSLplY.png",
                "isPro": false,
                "fullname": "Yueqian Lin",
                "user": "linyueqian",
                "type": "user"
            },
            "summary": "We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for\nevaluating reasoning ability in voice-interactive systems under real-time\nconversational constraints. VERA comprises 2,931 voice-native episodes derived\nfrom established text benchmarks and organized into five tracks (Math, Web,\nScience, Long-Context, Factual). Each item is adapted for speech interaction\nwhile preserving reasoning difficulty. VERA enables direct text-voice\ncomparison within model families and supports analysis of how architectural\nchoices affect reliability. We assess 12 contemporary voice systems alongside\nstrong text baselines and observe large, consistent modality gaps: on\ncompetition mathematics a leading text model attains 74.8% accuracy while its\nvoice counterpart reaches 6.1%; macro-averaged across tracks the best text\nmodels achieve 54.0% versus 11.3% for voice. Latency-accuracy analyses reveal a\nlow-latency plateau, where fast voice systems cluster around ~10% accuracy,\nwhile approaching text performance requires sacrificing real-time interaction.\nDiagnostic experiments indicate that common mitigations are insufficient.\nIncreasing \"thinking time\" yields negligible gains; a decoupled cascade that\nseparates reasoning from narration improves accuracy but still falls well short\nof text and introduces characteristic grounding/consistency errors. Failure\nanalyses further show distinct error signatures across native streaming,\nend-to-end, and cascade designs. VERA provides a reproducible testbed and\ntargeted diagnostics for architectures that decouple thinking from speaking,\noffering a principled way to measure progress toward real-time voice assistants\nthat are both fluent and reliably reasoned.",
            "upvotes": 8,
            "discussionId": "68dc885c4159d1f2418f9a2c",
            "githubRepo": "https://github.com/linyueqian/VERA",
            "ai_summary": "VERA is a benchmark for evaluating reasoning ability in voice-interactive systems, revealing significant performance gaps compared to text models and highlighting challenges in real-time interaction.",
            "ai_keywords": [
                "voice-interactive systems",
                "reasoning ability",
                "real-time conversational constraints",
                "voice-native episodes",
                "text benchmarks",
                "speech interaction",
                "text-voice comparison",
                "architectural choices",
                "reliability",
                "latency-accuracy analyses",
                "low-latency plateau",
                "thinking time",
                "decoupled cascade",
                "error signatures",
                "native streaming",
                "end-to-end",
                "cascade designs",
                "real-time voice assistants",
                "fluent reasoning"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-09-30T13:17:09.000Z",
        "title": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced\n  Performance Gap",
        "summary": "We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for\nevaluating reasoning ability in voice-interactive systems under real-time\nconversational constraints. VERA comprises 2,931 voice-native episodes derived\nfrom established text benchmarks and organized into five tracks (Math, Web,\nScience, Long-Context, Factual). Each item is adapted for speech interaction\nwhile preserving reasoning difficulty. VERA enables direct text-voice\ncomparison within model families and supports analysis of how architectural\nchoices affect reliability. We assess 12 contemporary voice systems alongside\nstrong text baselines and observe large, consistent modality gaps: on\ncompetition mathematics a leading text model attains 74.8% accuracy while its\nvoice counterpart reaches 6.1%; macro-averaged across tracks the best text\nmodels achieve 54.0% versus 11.3% for voice. Latency-accuracy analyses reveal a\nlow-latency plateau, where fast voice systems cluster around ~10% accuracy,\nwhile approaching text performance requires sacrificing real-time interaction.\nDiagnostic experiments indicate that common mitigations are insufficient.\nIncreasing \"thinking time\" yields negligible gains; a decoupled cascade that\nseparates reasoning from narration improves accuracy but still falls well short\nof text and introduces characteristic grounding/consistency errors. Failure\nanalyses further show distinct error signatures across native streaming,\nend-to-end, and cascade designs. VERA provides a reproducible testbed and\ntargeted diagnostics for architectures that decouple thinking from speaking,\noffering a principled way to measure progress toward real-time voice assistants\nthat are both fluent and reliably reasoned.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26542.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b5198c25882acb62fb77ef",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b5198c25882acb62fb77ef/HX9pfMEPQlfjvSAgSLplY.png",
            "fullname": "Yueqian Lin",
            "name": "linyueqian",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25189",
            "authors": [
                {
                    "_id": "68dcae184159d1f2418f9bd9",
                    "name": "Gongrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dcae184159d1f2418f9bda",
                    "name": "Jialiang Zhu",
                    "hidden": false
                },
                {
                    "_id": "68dcae184159d1f2418f9bdb",
                    "name": "Ruiqi Yang",
                    "hidden": false
                },
                {
                    "_id": "68dcae184159d1f2418f9bdc",
                    "name": "Kai Qiu",
                    "hidden": false
                },
                {
                    "_id": "68dcae184159d1f2418f9bdd",
                    "user": {
                        "_id": "62a30bf72dac39c2173c0a8c",
                        "avatarUrl": "/avatars/15fb1ea3dcc7ccd8bc8002ce282e27b3.svg",
                        "isPro": false,
                        "fullname": "Miaosen Zhang",
                        "user": "Miaosen",
                        "type": "user"
                    },
                    "name": "Miaosen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:21:07.867Z",
                    "hidden": false
                },
                {
                    "_id": "68dcae184159d1f2418f9bde",
                    "name": "Zhirong Wu",
                    "hidden": false
                },
                {
                    "_id": "68dcae184159d1f2418f9bdf",
                    "name": "Qi Dai",
                    "hidden": false
                },
                {
                    "_id": "68dcae184159d1f2418f9be0",
                    "name": "Bei Liu",
                    "hidden": false
                },
                {
                    "_id": "68dcae184159d1f2418f9be1",
                    "name": "Chong Luo",
                    "hidden": false
                },
                {
                    "_id": "68dcae184159d1f2418f9be2",
                    "name": "Zhengyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "68dcae184159d1f2418f9be3",
                    "name": "Linjie Li",
                    "hidden": false
                },
                {
                    "_id": "68dcae184159d1f2418f9be4",
                    "name": "Lijuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68dcae184159d1f2418f9be5",
                    "name": "Weizhu Chen",
                    "hidden": false
                },
                {
                    "_id": "68dcae184159d1f2418f9be6",
                    "name": "Yuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dcae184159d1f2418f9be7",
                    "name": "Xin Li",
                    "hidden": false
                },
                {
                    "_id": "68dcae184159d1f2418f9be8",
                    "name": "Zhaoyi Liu",
                    "hidden": false
                },
                {
                    "_id": "68dcae184159d1f2418f9be9",
                    "name": "Xin Geng",
                    "hidden": false
                },
                {
                    "_id": "68dcae184159d1f2418f9bea",
                    "name": "Baining Guo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:59:57.000Z",
            "submittedOnDailyAt": "2025-10-01T03:00:32.005Z",
            "title": "InfoAgent: Advancing Autonomous Information-Seeking Agents",
            "submittedOnDailyBy": {
                "_id": "62a30bf72dac39c2173c0a8c",
                "avatarUrl": "/avatars/15fb1ea3dcc7ccd8bc8002ce282e27b3.svg",
                "isPro": false,
                "fullname": "Miaosen Zhang",
                "user": "Miaosen",
                "type": "user"
            },
            "summary": "Building Large Language Model agents that expand their capabilities by\ninteracting with external tools represents a new frontier in AI research and\napplications. In this paper, we introduce InfoAgent, a deep research agent\npowered by an innovative data synthesis pipeline and orchestrated web search\ntools. To construct challenging, hard-to-find queries,we build entity trees and\napply sub-tree sampling with entity fuzzification to systematically increase\nquestion difficulty. Unlike prior work that relies heavily on commercial search\ntools, we develop a dedicated self-hosted search infrastructure, enhancing\ntransparency of agent environments and facilitating further advancement of\nagent capacity. We evaluate the effectiveness of our data pipeline by measuring\nthe average number of tool calls required to correctly answer a question, and\nalso show that our agent yields better performance when equipped with our\ntools. Our InfoAgent is post-trained from Qwen3-14B using a two-stage\nrecipe: cold-start supervised finetuning to instill long-horizon search\nbehaviors, followed by reinforcement learning which significantly improves\nreasoning-driven tool use. With our methods, InfoAgent achieves 15.3\\% accuracy\non BrowseComp, 29.2\\% on BrowseComp-ZH, and 40.4\\% on Xbench-DS, outperforming\nprior open-source deep research agents such as WebSailor-72B and DeepDive-32B.",
            "upvotes": 8,
            "discussionId": "68dcae184159d1f2418f9beb",
            "ai_summary": "InfoAgent, a deep research agent using a custom data synthesis pipeline and search infrastructure, outperforms existing agents by improving tool use and reasoning.",
            "ai_keywords": [
                "deep research agent",
                "data synthesis pipeline",
                "entity trees",
                "sub-tree sampling",
                "entity fuzzification",
                "self-hosted search infrastructure",
                "cold-start supervised finetuning",
                "reinforcement learning",
                "BrowseComp",
                "BrowseComp-ZH",
                "Xbench-DS",
                "WebSailor-72B",
                "DeepDive-32B"
            ]
        },
        "publishedAt": "2025-09-29T13:59:57.000Z",
        "title": "InfoAgent: Advancing Autonomous Information-Seeking Agents",
        "summary": "Building Large Language Model agents that expand their capabilities by\ninteracting with external tools represents a new frontier in AI research and\napplications. In this paper, we introduce InfoAgent, a deep research agent\npowered by an innovative data synthesis pipeline and orchestrated web search\ntools. To construct challenging, hard-to-find queries,we build entity trees and\napply sub-tree sampling with entity fuzzification to systematically increase\nquestion difficulty. Unlike prior work that relies heavily on commercial search\ntools, we develop a dedicated self-hosted search infrastructure, enhancing\ntransparency of agent environments and facilitating further advancement of\nagent capacity. We evaluate the effectiveness of our data pipeline by measuring\nthe average number of tool calls required to correctly answer a question, and\nalso show that our agent yields better performance when equipped with our\ntools. Our InfoAgent is post-trained from Qwen3-14B using a two-stage\nrecipe: cold-start supervised finetuning to instill long-horizon search\nbehaviors, followed by reinforcement learning which significantly improves\nreasoning-driven tool use. With our methods, InfoAgent achieves 15.3\\% accuracy\non BrowseComp, 29.2\\% on BrowseComp-ZH, and 40.4\\% on Xbench-DS, outperforming\nprior open-source deep research agents such as WebSailor-72B and DeepDive-32B.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25189.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62a30bf72dac39c2173c0a8c",
            "avatarUrl": "/avatars/15fb1ea3dcc7ccd8bc8002ce282e27b3.svg",
            "fullname": "Miaosen Zhang",
            "name": "Miaosen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "submitterOrganization": {
            "_id": "5e6485f787403103f9f1055e",
            "name": "microsoft",
            "fullname": "Microsoft",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.24207",
            "authors": [
                {
                    "_id": "68dcaada4159d1f2418f9bb8",
                    "name": "Sijia Liu",
                    "hidden": false
                },
                {
                    "_id": "68dcaada4159d1f2418f9bb9",
                    "name": "Niklas Muennighoff",
                    "hidden": false
                },
                {
                    "_id": "68dcaada4159d1f2418f9bba",
                    "name": "Kawin Ethayarajh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T02:41:16.000Z",
            "submittedOnDailyAt": "2025-10-01T02:48:25.499Z",
            "title": "Humanline: Online Alignment as Perceptual Loss",
            "submittedOnDailyBy": {
                "_id": "651d2b485d3519c0b7595af7",
                "avatarUrl": "/avatars/00ce2ecbc35e22a90f72b9015299aa29.svg",
                "isPro": false,
                "fullname": "Sijia L",
                "user": "sijial430",
                "type": "user"
            },
            "summary": "Online alignment (e.g., GRPO) is generally more performant than offline\nalignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral\neconomics, we propose a human-centric explanation. We prove that online\non-policy sampling better approximates the human-perceived distribution of what\nthe model can produce, and PPO/GRPO-style clipping -- originally introduced to\njust stabilize training -- recovers a perceptual bias in how humans perceive\nprobability. In this sense, PPO/GRPO act as perceptual losses already. Our\ntheory further suggests that the online/offline dichotomy is itself incidental\nto maximizing human utility, since we can achieve the same effect by\nselectively training on any data in a manner that mimics human perception,\nrather than restricting ourselves to online on-policy data. Doing so would\nallow us to post-train more quickly, cheaply, and flexibly without sacrificing\nperformance. To this end, we propose a design pattern that explicitly\nincorporates perceptual distortions of probability into objectives like\nDPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that\nthese humanline variants, even when trained with offline off-policy data, can\nmatch the performance of their online counterparts on both verifiable and\nunverifiable tasks.",
            "upvotes": 8,
            "discussionId": "68dcaadb4159d1f2418f9bbb",
            "ai_summary": "Online alignment methods like GRPO outperform offline methods like DPO due to better approximation of human-perceived probability distributions, and introducing perceptual biases into offline training can achieve similar performance.",
            "ai_keywords": [
                "GRPO",
                "DPO",
                "prospect theory",
                "on-policy sampling",
                "PPO",
                "perceptual bias",
                "perceptual losses",
                "humanline variants",
                "KTO"
            ]
        },
        "publishedAt": "2025-09-28T22:41:16.000Z",
        "title": "Humanline: Online Alignment as Perceptual Loss",
        "summary": "Online alignment (e.g., GRPO) is generally more performant than offline\nalignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral\neconomics, we propose a human-centric explanation. We prove that online\non-policy sampling better approximates the human-perceived distribution of what\nthe model can produce, and PPO/GRPO-style clipping -- originally introduced to\njust stabilize training -- recovers a perceptual bias in how humans perceive\nprobability. In this sense, PPO/GRPO act as perceptual losses already. Our\ntheory further suggests that the online/offline dichotomy is itself incidental\nto maximizing human utility, since we can achieve the same effect by\nselectively training on any data in a manner that mimics human perception,\nrather than restricting ourselves to online on-policy data. Doing so would\nallow us to post-train more quickly, cheaply, and flexibly without sacrificing\nperformance. To this end, we propose a design pattern that explicitly\nincorporates perceptual distortions of probability into objectives like\nDPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that\nthese humanline variants, even when trained with offline off-policy data, can\nmatch the performance of their online counterparts on both verifiable and\nunverifiable tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24207.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "651d2b485d3519c0b7595af7",
            "avatarUrl": "/avatars/00ce2ecbc35e22a90f72b9015299aa29.svg",
            "fullname": "Sijia L",
            "name": "sijial430",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "submitterOrganization": {
            "_id": "6735d51c08a190b1caea1f29",
            "name": "PrincetonUniversity",
            "fullname": "Princeton University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26628",
            "authors": [
                {
                    "_id": "68dc8bf34159d1f2418f9a6e",
                    "user": {
                        "_id": "667187ba9ab144eb3ac43a1b",
                        "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
                        "isPro": false,
                        "fullname": "Runze Liu",
                        "user": "RyanLiu112",
                        "type": "user"
                    },
                    "name": "Runze Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:22:06.937Z",
                    "hidden": false
                },
                {
                    "_id": "68dc8bf34159d1f2418f9a6f",
                    "name": "Jiakang Wang",
                    "hidden": false
                },
                {
                    "_id": "68dc8bf34159d1f2418f9a70",
                    "user": {
                        "_id": "645b0c3ec35da9c7afd95421",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
                        "isPro": false,
                        "fullname": "Yuling",
                        "user": "YerbaPage",
                        "type": "user"
                    },
                    "name": "Yuling Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:22:04.689Z",
                    "hidden": false
                },
                {
                    "_id": "68dc8bf34159d1f2418f9a71",
                    "name": "Zhihui Xie",
                    "hidden": false
                },
                {
                    "_id": "68dc8bf34159d1f2418f9a72",
                    "name": "Chenxin An",
                    "hidden": false
                },
                {
                    "_id": "68dc8bf34159d1f2418f9a73",
                    "name": "Kaiyan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dc8bf34159d1f2418f9a74",
                    "name": "Jian Zhao",
                    "hidden": false
                },
                {
                    "_id": "68dc8bf34159d1f2418f9a75",
                    "name": "Xiaodong Gu",
                    "hidden": false
                },
                {
                    "_id": "68dc8bf34159d1f2418f9a76",
                    "name": "Lei Lin",
                    "hidden": false
                },
                {
                    "_id": "68dc8bf34159d1f2418f9a77",
                    "name": "Wenping Hu",
                    "hidden": false
                },
                {
                    "_id": "68dc8bf34159d1f2418f9a78",
                    "name": "Xiu Li",
                    "hidden": false
                },
                {
                    "_id": "68dc8bf34159d1f2418f9a79",
                    "name": "Fuzheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dc8bf34159d1f2418f9a7a",
                    "name": "Guorui Zhou",
                    "hidden": false
                },
                {
                    "_id": "68dc8bf34159d1f2418f9a7b",
                    "name": "Kun Gai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T17:58:34.000Z",
            "submittedOnDailyAt": "2025-10-01T00:35:07.177Z",
            "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL\n  in Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "667187ba9ab144eb3ac43a1b",
                "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
                "isPro": false,
                "fullname": "Runze Liu",
                "user": "RyanLiu112",
                "type": "user"
            },
            "summary": "Reinforcement Learning (RL) has shown remarkable success in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Process-Supervised RL\n(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.\nHowever, existing PSRL approaches suffer from limited exploration efficiency,\nboth in terms of branching positions and sampling. In this paper, we introduce\na novel PSRL framework (AttnRL), which enables efficient exploration for\nreasoning models. Motivated by preliminary observations that steps exhibiting\nhigh attention scores correlate with reasoning behaviors, we propose to branch\nfrom positions with high values. Furthermore, we develop an adaptive sampling\nstrategy that accounts for problem difficulty and historical batch size,\nensuring that the whole training batch maintains non-zero advantage values. To\nfurther improve sampling efficiency, we design a one-step off-policy training\npipeline for PSRL. Extensive experiments on multiple challenging mathematical\nreasoning benchmarks demonstrate that our method consistently outperforms prior\napproaches in terms of performance and sampling and training efficiency.",
            "upvotes": 7,
            "discussionId": "68dc8bf34159d1f2418f9a7c",
            "ai_summary": "A novel PSRL framework (AttnRL) enhances exploration efficiency in reasoning models by branching from high attention positions and using an adaptive sampling strategy, outperforming prior methods in mathematical reasoning benchmarks.",
            "ai_keywords": [
                "Reinforcement Learning",
                "Large Language Models",
                "Process-Supervised RL",
                "PSRL",
                "AttnRL",
                "attention scores",
                "reasoning behaviors",
                "adaptive sampling strategy",
                "one-step off-policy training",
                "mathematical reasoning benchmarks"
            ]
        },
        "publishedAt": "2025-09-30T13:58:34.000Z",
        "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL\n  in Reasoning Models",
        "summary": "Reinforcement Learning (RL) has shown remarkable success in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Process-Supervised RL\n(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.\nHowever, existing PSRL approaches suffer from limited exploration efficiency,\nboth in terms of branching positions and sampling. In this paper, we introduce\na novel PSRL framework (AttnRL), which enables efficient exploration for\nreasoning models. Motivated by preliminary observations that steps exhibiting\nhigh attention scores correlate with reasoning behaviors, we propose to branch\nfrom positions with high values. Furthermore, we develop an adaptive sampling\nstrategy that accounts for problem difficulty and historical batch size,\nensuring that the whole training batch maintains non-zero advantage values. To\nfurther improve sampling efficiency, we design a one-step off-policy training\npipeline for PSRL. Extensive experiments on multiple challenging mathematical\nreasoning benchmarks demonstrate that our method consistently outperforms prior\napproaches in terms of performance and sampling and training efficiency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26628.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "667187ba9ab144eb3ac43a1b",
            "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
            "fullname": "Runze Liu",
            "name": "RyanLiu112",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "submitterOrganization": {
            "_id": "628735cbc83a2d6ab8d14a66",
            "name": "Tsinghua",
            "fullname": "Tsinghua University"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25397",
            "authors": [
                {
                    "_id": "68dcd5a44159d1f2418f9c5d",
                    "name": "Johan Linåker",
                    "hidden": false
                },
                {
                    "_id": "68dcd5a44159d1f2418f9c5e",
                    "name": "Cailean Osborne",
                    "hidden": false
                },
                {
                    "_id": "68dcd5a44159d1f2418f9c5f",
                    "name": "Jennifer Ding",
                    "hidden": false
                },
                {
                    "_id": "68dcd5a44159d1f2418f9c60",
                    "user": {
                        "_id": "62d648291fa3e4e7ae3fa6e8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d648291fa3e4e7ae3fa6e8/oatOwf8Xqe5eDbCSuYqCd.png",
                        "isPro": true,
                        "fullname": "ben burtenshaw",
                        "user": "burtenshaw",
                        "type": "user"
                    },
                    "name": "Ben Burtenshaw",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-10-01T09:59:23.869Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T18:55:18.000Z",
            "submittedOnDailyAt": "2025-10-01T05:49:23.206Z",
            "title": "A Cartography of Open Collaboration in Open Source AI: Mapping\n  Practices, Motivations, and Governance in 14 Open Large Language Model\n  Projects",
            "submittedOnDailyBy": {
                "_id": "62d648291fa3e4e7ae3fa6e8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d648291fa3e4e7ae3fa6e8/oatOwf8Xqe5eDbCSuYqCd.png",
                "isPro": true,
                "fullname": "ben burtenshaw",
                "user": "burtenshaw",
                "type": "user"
            },
            "summary": "The proliferation of open large language models (LLMs) is fostering a vibrant\necosystem of research and innovation in artificial intelligence (AI). However,\nthe methods of collaboration used to develop open LLMs both before and after\ntheir public release have not yet been comprehensively studied, limiting our\nunderstanding of how open LLM projects are initiated, organized, and governed\nas well as what opportunities there are to foster this ecosystem even further.\nWe address this gap through an exploratory analysis of open collaboration\nthroughout the development and reuse lifecycle of open LLMs, drawing on\nsemi-structured interviews with the developers of 14 open LLMs from grassroots\nprojects, research institutes, startups, and Big Tech companies in North\nAmerica, Europe, Africa, and Asia. We make three key contributions to research\nand practice. First, collaboration in open LLM projects extends far beyond the\nLLMs themselves, encompassing datasets, benchmarks, open source frameworks,\nleaderboards, knowledge sharing and discussion forums, and compute\npartnerships, among others. Second, open LLM developers have a variety of\nsocial, economic, and technological motivations, from democratizing AI access\nand promoting open science to building regional ecosystems and expanding\nlanguage representation. Third, the sampled open LLM projects exhibit five\ndistinct organizational models, ranging from single company projects to\nnon-profit-sponsored grassroots projects, which vary in their centralization of\ncontrol and community engagement strategies used throughout the open LLM\nlifecycle. We conclude with practical recommendations for stakeholders seeking\nto support the global community building a more open future for AI.",
            "upvotes": 7,
            "discussionId": "68dcd5a44159d1f2418f9c61",
            "ai_summary": "Research explores collaboration in open large language models, identifying diverse motivations and organizational models among developers from various sectors.",
            "ai_keywords": [
                "large language models",
                "open collaboration",
                "datasets",
                "benchmarks",
                "open source frameworks",
                "leaderboards",
                "knowledge sharing",
                "discussion forums",
                "compute partnerships",
                "democratizing AI",
                "open science",
                "regional ecosystems",
                "language representation",
                "organizational models",
                "community engagement"
            ]
        },
        "publishedAt": "2025-09-29T14:55:18.000Z",
        "title": "A Cartography of Open Collaboration in Open Source AI: Mapping\n  Practices, Motivations, and Governance in 14 Open Large Language Model\n  Projects",
        "summary": "The proliferation of open large language models (LLMs) is fostering a vibrant\necosystem of research and innovation in artificial intelligence (AI). However,\nthe methods of collaboration used to develop open LLMs both before and after\ntheir public release have not yet been comprehensively studied, limiting our\nunderstanding of how open LLM projects are initiated, organized, and governed\nas well as what opportunities there are to foster this ecosystem even further.\nWe address this gap through an exploratory analysis of open collaboration\nthroughout the development and reuse lifecycle of open LLMs, drawing on\nsemi-structured interviews with the developers of 14 open LLMs from grassroots\nprojects, research institutes, startups, and Big Tech companies in North\nAmerica, Europe, Africa, and Asia. We make three key contributions to research\nand practice. First, collaboration in open LLM projects extends far beyond the\nLLMs themselves, encompassing datasets, benchmarks, open source frameworks,\nleaderboards, knowledge sharing and discussion forums, and compute\npartnerships, among others. Second, open LLM developers have a variety of\nsocial, economic, and technological motivations, from democratizing AI access\nand promoting open science to building regional ecosystems and expanding\nlanguage representation. Third, the sampled open LLM projects exhibit five\ndistinct organizational models, ranging from single company projects to\nnon-profit-sponsored grassroots projects, which vary in their centralization of\ncontrol and community engagement strategies used throughout the open LLM\nlifecycle. We conclude with practical recommendations for stakeholders seeking\nto support the global community building a more open future for AI.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25397.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62d648291fa3e4e7ae3fa6e8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d648291fa3e4e7ae3fa6e8/oatOwf8Xqe5eDbCSuYqCd.png",
            "fullname": "ben burtenshaw",
            "name": "burtenshaw",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3786
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25339",
            "authors": [
                {
                    "_id": "68dc916b4159d1f2418f9aca",
                    "name": "Paul Gavrikov",
                    "hidden": false
                },
                {
                    "_id": "68dc916b4159d1f2418f9acb",
                    "name": "Wei Lin",
                    "hidden": false
                },
                {
                    "_id": "68dc916b4159d1f2418f9acc",
                    "name": "M. Jehanzeb Mirza",
                    "hidden": false
                },
                {
                    "_id": "68dc916b4159d1f2418f9acd",
                    "name": "Soumya Jahagirdar",
                    "hidden": false
                },
                {
                    "_id": "68dc916b4159d1f2418f9ace",
                    "name": "Muhammad Huzaifa",
                    "hidden": false
                },
                {
                    "_id": "68dc916b4159d1f2418f9acf",
                    "name": "Sivan Doveh",
                    "hidden": false
                },
                {
                    "_id": "68dc916b4159d1f2418f9ad0",
                    "name": "Serena Yeung-Levy",
                    "hidden": false
                },
                {
                    "_id": "68dc916b4159d1f2418f9ad1",
                    "name": "James Glass",
                    "hidden": false
                },
                {
                    "_id": "68dc916b4159d1f2418f9ad2",
                    "name": "Hilde Kuehne",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T18:00:25.000Z",
            "submittedOnDailyAt": "2025-10-01T00:57:09.721Z",
            "title": "VisualOverload: Probing Visual Understanding of VLMs in Really Dense\n  Scenes",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Is basic visual understanding really solved in state-of-the-art VLMs? We\npresent VisualOverload, a slightly different visual question answering (VQA)\nbenchmark comprising 2,720 question-answer pairs, with privately held\nground-truth responses. Unlike prior VQA datasets that typically focus on near\nglobal image understanding, VisualOverload challenges models to perform simple,\nknowledge-free vision tasks in densely populated (or, overloaded) scenes. Our\ndataset consists of high-resolution scans of public-domain paintings that are\npopulated with multiple figures, actions, and unfolding subplots set against\nelaborately detailed backdrops. We manually annotated these images with\nquestions across six task categories to probe for a thorough understanding of\nthe scene. We hypothesize that current benchmarks overestimate the performance\nof VLMs, and encoding and reasoning over details is still a challenging task\nfor them, especially if they are confronted with densely populated scenes.\nIndeed, we observe that even the best model (o3) out of 37 tested models only\nachieves 19.6% accuracy on our hardest test split and overall 69.5% accuracy on\nall questions. Beyond a thorough evaluation, we complement our benchmark with\nan error analysis that reveals multiple failure modes, including a lack of\ncounting skills, failure in OCR, and striking logical inconsistencies under\ncomplex tasks. Altogether, VisualOverload exposes a critical gap in current\nvision models and offers a crucial resource for the community to develop better\nmodels.\n  Benchmark: http://paulgavrikov.github.io/visualoverload",
            "upvotes": 7,
            "discussionId": "68dc916b4159d1f2418f9ad3",
            "projectPage": "https://paulgavrikov.github.io/visualoverload/",
            "githubRepo": "https://github.com/paulgavrikov/visualoverload",
            "ai_summary": "VisualOverload is a VQA benchmark that challenges models with simple vision tasks in densely populated scenes, revealing gaps in current VLMs' performance and offering insights into their failure modes.",
            "ai_keywords": [
                "visual question answering",
                "VQA",
                "densely populated scenes",
                "high-resolution scans",
                "public-domain paintings",
                "manual annotation",
                "task categories",
                "error analysis",
                "counting skills",
                "OCR",
                "logical inconsistencies",
                "vision models"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-09-29T14:00:25.000Z",
        "title": "VisualOverload: Probing Visual Understanding of VLMs in Really Dense\n  Scenes",
        "summary": "Is basic visual understanding really solved in state-of-the-art VLMs? We\npresent VisualOverload, a slightly different visual question answering (VQA)\nbenchmark comprising 2,720 question-answer pairs, with privately held\nground-truth responses. Unlike prior VQA datasets that typically focus on near\nglobal image understanding, VisualOverload challenges models to perform simple,\nknowledge-free vision tasks in densely populated (or, overloaded) scenes. Our\ndataset consists of high-resolution scans of public-domain paintings that are\npopulated with multiple figures, actions, and unfolding subplots set against\nelaborately detailed backdrops. We manually annotated these images with\nquestions across six task categories to probe for a thorough understanding of\nthe scene. We hypothesize that current benchmarks overestimate the performance\nof VLMs, and encoding and reasoning over details is still a challenging task\nfor them, especially if they are confronted with densely populated scenes.\nIndeed, we observe that even the best model (o3) out of 37 tested models only\nachieves 19.6% accuracy on our hardest test split and overall 69.5% accuracy on\nall questions. Beyond a thorough evaluation, we complement our benchmark with\nan error analysis that reveals multiple failure modes, including a lack of\ncounting skills, failure in OCR, and striking logical inconsistencies under\ncomplex tasks. Altogether, VisualOverload exposes a critical gap in current\nvision models and offers a crucial resource for the community to develop better\nmodels.\n  Benchmark: http://paulgavrikov.github.io/visualoverload",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25339.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 115
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22613",
            "authors": [
                {
                    "_id": "68dc86ef4159d1f2418f9a09",
                    "name": "Siwei Wang",
                    "hidden": false
                },
                {
                    "_id": "68dc86ef4159d1f2418f9a0a",
                    "name": "Yifei Shen",
                    "hidden": false
                },
                {
                    "_id": "68dc86ef4159d1f2418f9a0b",
                    "name": "Haoran Sun",
                    "hidden": false
                },
                {
                    "_id": "68dc86ef4159d1f2418f9a0c",
                    "name": "Shi Feng",
                    "hidden": false
                },
                {
                    "_id": "68dc86ef4159d1f2418f9a0d",
                    "name": "Shang-Hua Teng",
                    "hidden": false
                },
                {
                    "_id": "68dc86ef4159d1f2418f9a0e",
                    "name": "Li Dong",
                    "hidden": false
                },
                {
                    "_id": "68dc86ef4159d1f2418f9a0f",
                    "name": "Yaru Hao",
                    "hidden": false
                },
                {
                    "_id": "68dc86ef4159d1f2418f9a10",
                    "user": {
                        "_id": "68dcae5c9cb88433d59a17b2",
                        "avatarUrl": "/avatars/09e90d4a22fb8ad476d0e5f440acc2be.svg",
                        "isPro": false,
                        "fullname": "Wei Chen",
                        "user": "weichen999",
                        "type": "user"
                    },
                    "name": "Wei Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:22:29.262Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T17:39:48.000Z",
            "submittedOnDailyAt": "2025-10-01T00:20:26.292Z",
            "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model\n  Planning: A Theoretical Perspective",
            "submittedOnDailyBy": {
                "_id": "649aa367c6cf3cc95bc1b7f6",
                "avatarUrl": "/avatars/4bf5446c261eab08fc06caebf4c5779a.svg",
                "isPro": false,
                "fullname": "Yifei Shen",
                "user": "yshenaw",
                "type": "user"
            },
            "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the\nplanning capabilities of Large Language Models (LLMs), yet the theoretical\nbasis for their effectiveness remains elusive. In this work, we investigate\nRL's benefits and limitations through a tractable graph-based abstraction,\nfocusing on policy gradient (PG) and Q-learning methods. Our theoretical\nanalyses reveal that supervised fine-tuning (SFT) may introduce\nco-occurrence-based spurious solutions, whereas RL achieves correct planning\nprimarily through exploration, underscoring exploration's role in enabling\nbetter generalization. However, we also show that PG suffers from diversity\ncollapse, where output diversity decreases during training and persists even\nafter perfect accuracy is attained. By contrast, Q-learning provides two key\nadvantages: off-policy learning and diversity preservation at convergence. We\nfurther demonstrate that careful reward design is necessary to prevent reward\nhacking in Q-learning. Finally, applying our framework to the real-world\nplanning benchmark Blocksworld, we confirm that these behaviors manifest in\npractice.",
            "upvotes": 7,
            "discussionId": "68dc86f04159d1f2418f9a11",
            "ai_summary": "Theoretical analysis of reinforcement learning methods in enhancing LLM planning reveals that while RL improves generalization through exploration, policy gradient suffers from diversity collapse, whereas Q-learning maintains diversity and requires careful reward design.",
            "ai_keywords": [
                "reinforcement learning",
                "Large Language Models",
                "policy gradient",
                "Q-learning",
                "supervised fine-tuning",
                "co-occurrence-based spurious solutions",
                "diversity collapse",
                "off-policy learning",
                "reward hacking",
                "Blocksworld"
            ]
        },
        "publishedAt": "2025-09-26T13:39:48.000Z",
        "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model\n  Planning: A Theoretical Perspective",
        "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the\nplanning capabilities of Large Language Models (LLMs), yet the theoretical\nbasis for their effectiveness remains elusive. In this work, we investigate\nRL's benefits and limitations through a tractable graph-based abstraction,\nfocusing on policy gradient (PG) and Q-learning methods. Our theoretical\nanalyses reveal that supervised fine-tuning (SFT) may introduce\nco-occurrence-based spurious solutions, whereas RL achieves correct planning\nprimarily through exploration, underscoring exploration's role in enabling\nbetter generalization. However, we also show that PG suffers from diversity\ncollapse, where output diversity decreases during training and persists even\nafter perfect accuracy is attained. By contrast, Q-learning provides two key\nadvantages: off-policy learning and diversity preservation at convergence. We\nfurther demonstrate that careful reward design is necessary to prevent reward\nhacking in Q-learning. Finally, applying our framework to the real-world\nplanning benchmark Blocksworld, we confirm that these behaviors manifest in\npractice.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22613.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649aa367c6cf3cc95bc1b7f6",
            "avatarUrl": "/avatars/4bf5446c261eab08fc06caebf4c5779a.svg",
            "fullname": "Yifei Shen",
            "name": "yshenaw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "submitterOrganization": {
            "_id": "68151d0f51add3813f3f7d1b",
            "name": "MicrosoftResearch",
            "fullname": "Microsoft Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.00492",
            "authors": [
                {
                    "_id": "68ddd3816024653e8a3ed052",
                    "name": "Dong Bok Lee",
                    "hidden": false
                },
                {
                    "_id": "68ddd3816024653e8a3ed053",
                    "name": "Seanie Lee",
                    "hidden": false
                },
                {
                    "_id": "68ddd3816024653e8a3ed054",
                    "name": "Sangwoo Park",
                    "hidden": false
                },
                {
                    "_id": "68ddd3816024653e8a3ed055",
                    "name": "Minki Kang",
                    "hidden": false
                },
                {
                    "_id": "68ddd3816024653e8a3ed056",
                    "name": "Jinheon Baek",
                    "hidden": false
                },
                {
                    "_id": "68ddd3816024653e8a3ed057",
                    "name": "Dongki Kim",
                    "hidden": false
                },
                {
                    "_id": "68ddd3816024653e8a3ed058",
                    "name": "Dominik Wagner",
                    "hidden": false
                },
                {
                    "_id": "68ddd3816024653e8a3ed059",
                    "name": "Jiongdao Jin",
                    "hidden": false
                },
                {
                    "_id": "68ddd3816024653e8a3ed05a",
                    "name": "Heejun Lee",
                    "hidden": false
                },
                {
                    "_id": "68ddd3816024653e8a3ed05b",
                    "name": "Tobias Bocklet",
                    "hidden": false
                },
                {
                    "_id": "68ddd3816024653e8a3ed05c",
                    "name": "Jinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68ddd3816024653e8a3ed05d",
                    "name": "Jingjing Fu",
                    "hidden": false
                },
                {
                    "_id": "68ddd3816024653e8a3ed05e",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                },
                {
                    "_id": "68ddd3816024653e8a3ed05f",
                    "name": "Jiang Bia",
                    "hidden": false
                },
                {
                    "_id": "68ddd3816024653e8a3ed060",
                    "name": "Lei Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T04:21:14.000Z",
            "submittedOnDailyAt": "2025-10-01T23:51:39.301Z",
            "title": "Rethinking Reward Models for Multi-Domain Test-Time Scaling",
            "submittedOnDailyBy": {
                "_id": "64ad5f59b7e4b2c1ce47eb43",
                "avatarUrl": "/avatars/1f13ebe21a90d8c99920aa2c8cd9ac45.svg",
                "isPro": false,
                "fullname": "Seanie Lee",
                "user": "Seanie-lee",
                "type": "user"
            },
            "summary": "The reliability of large language models (LLMs) during test-time scaling is\noften assessed with external verifiers or reward models that\ndistinguish correct reasoning from flawed logic. Prior work generally assumes\nthat process reward models (PRMs), which score every intermediate reasoning\nstep, outperform outcome reward models (ORMs) that assess only the final\nanswer. This view is based mainly on evidence from narrow, math-adjacent\ndomains. We present the first unified evaluation of four reward model variants,\ndiscriminative ORM and PRM (\\DisORM, \\DisPRM) and generative ORM and PRM\n(\\GenORM, \\GenPRM), across 14 diverse domains. Contrary to conventional wisdom,\nwe find that (i) \\DisORM performs on par with \\DisPRM, (ii) \\GenPRM is not\ncompetitive, and (iii) overall, \\GenORM is the most robust, yielding\nsignificant and consistent gains across every tested domain. We attribute this\nto PRM-style stepwise scoring, which inherits label noise from LLM\nauto-labeling and has difficulty evaluating long reasoning trajectories,\nincluding those involving self-correcting reasoning. Our theoretical analysis\nshows that step-wise aggregation compounds errors as reasoning length grows,\nand our empirical observations confirm this effect. These findings challenge\nthe prevailing assumption that fine-grained supervision is always better and\nsupport generative outcome verification for multi-domain deployment. We\npublicly release our code, datasets, and checkpoints at\nhttps://github.com/db-Lee/Multi-RM{\\small\\texttt{https://github.com/db-Lee/Multi-RM}}\nto facilitate future research in multi-domain settings.",
            "upvotes": 6,
            "discussionId": "68ddd3816024653e8a3ed061",
            "githubRepo": "https://github.com/db-Lee/Multi-RM",
            "ai_summary": "A unified evaluation across diverse domains shows that generative outcome reward models outperform process reward models and discriminative outcome reward models in assessing large language model reliability.",
            "ai_keywords": [
                "large language models",
                "external verifiers",
                "reward models",
                "process reward models",
                "outcome reward models",
                "discriminative ORM",
                "discriminative PRM",
                "generative ORM",
                "generative PRM",
                "label noise",
                "self-correcting reasoning",
                "step-wise aggregation",
                "multi-domain deployment"
            ]
        },
        "publishedAt": "2025-10-01T00:21:14.000Z",
        "title": "Rethinking Reward Models for Multi-Domain Test-Time Scaling",
        "summary": "The reliability of large language models (LLMs) during test-time scaling is\noften assessed with external verifiers or reward models that\ndistinguish correct reasoning from flawed logic. Prior work generally assumes\nthat process reward models (PRMs), which score every intermediate reasoning\nstep, outperform outcome reward models (ORMs) that assess only the final\nanswer. This view is based mainly on evidence from narrow, math-adjacent\ndomains. We present the first unified evaluation of four reward model variants,\ndiscriminative ORM and PRM (\\DisORM, \\DisPRM) and generative ORM and PRM\n(\\GenORM, \\GenPRM), across 14 diverse domains. Contrary to conventional wisdom,\nwe find that (i) \\DisORM performs on par with \\DisPRM, (ii) \\GenPRM is not\ncompetitive, and (iii) overall, \\GenORM is the most robust, yielding\nsignificant and consistent gains across every tested domain. We attribute this\nto PRM-style stepwise scoring, which inherits label noise from LLM\nauto-labeling and has difficulty evaluating long reasoning trajectories,\nincluding those involving self-correcting reasoning. Our theoretical analysis\nshows that step-wise aggregation compounds errors as reasoning length grows,\nand our empirical observations confirm this effect. These findings challenge\nthe prevailing assumption that fine-grained supervision is always better and\nsupport generative outcome verification for multi-domain deployment. We\npublicly release our code, datasets, and checkpoints at\nhttps://github.com/db-Lee/Multi-RM{\\small\\texttt{https://github.com/db-Lee/Multi-RM}}\nto facilitate future research in multi-domain settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00492.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64ad5f59b7e4b2c1ce47eb43",
            "avatarUrl": "/avatars/1f13ebe21a90d8c99920aa2c8cd9ac45.svg",
            "fullname": "Seanie Lee",
            "name": "Seanie-lee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26476",
            "authors": [
                {
                    "_id": "68dc92b64159d1f2418f9ae2",
                    "user": {
                        "_id": "667094ab92412fd464d7f747",
                        "avatarUrl": "/avatars/c9d0edf7a4976a2b1dd8f191e84c3da1.svg",
                        "isPro": false,
                        "fullname": "YASH AKHAURI",
                        "user": "akhauriyash",
                        "type": "user"
                    },
                    "name": "Yash Akhauri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T15:29:22.497Z",
                    "hidden": false
                },
                {
                    "_id": "68dc92b64159d1f2418f9ae3",
                    "name": "Xingyou Song",
                    "hidden": false
                },
                {
                    "_id": "68dc92b64159d1f2418f9ae4",
                    "name": "Arissa Wongpanich",
                    "hidden": false
                },
                {
                    "_id": "68dc92b64159d1f2418f9ae5",
                    "name": "Bryan Lewandowski",
                    "hidden": false
                },
                {
                    "_id": "68dc92b64159d1f2418f9ae6",
                    "name": "Mohamed S. Abdelfattah",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T16:25:23.000Z",
            "submittedOnDailyAt": "2025-10-01T01:02:49.611Z",
            "title": "Regression Language Models for Code",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We study code-to-metric regression: predicting numeric outcomes of code\nexecutions, a challenging task due to the open-ended nature of programming\nlanguages. While prior methods have resorted to heavy and domain-specific\nfeature engineering, we show that a single unified Regression Language Model\n(RLM) can simultaneously predict directly from text, (i) the memory footprint\nof code across multiple high-level languages such as Python and C++, (ii) the\nlatency of Triton GPU kernels, and (iii) the accuracy and speed of trained\nneural networks represented in ONNX. In particular, a relatively small 300M\nparameter RLM initialized from T5Gemma, obtains > 0.9 Spearman-rank on\ncompetitive programming submissions from APPS, and a single unified model\nachieves > 0.5 average Spearman-rank across 17 separate languages from CodeNet.\nFurthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five\nclassic NAS design spaces previously dominated by graph neural networks, and\nsimultaneously predict architecture latencies on numerous hardware platforms.",
            "upvotes": 6,
            "discussionId": "68dc92b64159d1f2418f9ae7",
            "githubRepo": "https://github.com/google-deepmind/regress-lm/tree/main",
            "ai_summary": "A unified Regression Language Model (RLM) predicts numeric outcomes of code executions, including memory footprint, latency, and neural network performance, across multiple languages and hardware platforms.",
            "ai_keywords": [
                "Regression Language Model (RLM)",
                "T5Gemma",
                "Spearman-rank",
                "Kendall-Tau",
                "NAS design spaces",
                "graph neural networks"
            ],
            "githubStars": 234
        },
        "publishedAt": "2025-09-30T12:25:23.000Z",
        "title": "Regression Language Models for Code",
        "summary": "We study code-to-metric regression: predicting numeric outcomes of code\nexecutions, a challenging task due to the open-ended nature of programming\nlanguages. While prior methods have resorted to heavy and domain-specific\nfeature engineering, we show that a single unified Regression Language Model\n(RLM) can simultaneously predict directly from text, (i) the memory footprint\nof code across multiple high-level languages such as Python and C++, (ii) the\nlatency of Triton GPU kernels, and (iii) the accuracy and speed of trained\nneural networks represented in ONNX. In particular, a relatively small 300M\nparameter RLM initialized from T5Gemma, obtains > 0.9 Spearman-rank on\ncompetitive programming submissions from APPS, and a single unified model\nachieves > 0.5 average Spearman-rank across 17 separate languages from CodeNet.\nFurthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five\nclassic NAS design spaces previously dominated by graph neural networks, and\nsimultaneously predict architecture latencies on numerous hardware platforms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26476.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 115
        },
        "submitterOrganization": {
            "_id": "5e6aca39878b8b2bf9806447",
            "name": "google",
            "fullname": "Google",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26645",
            "authors": [
                {
                    "_id": "68dcd6a04159d1f2418f9c71",
                    "user": {
                        "_id": "66606a13fc6c0816442bd161",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
                        "isPro": false,
                        "fullname": "Xingyu Chen",
                        "user": "rover-xingyu",
                        "type": "user"
                    },
                    "name": "Xingyu Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:20:41.482Z",
                    "hidden": false
                },
                {
                    "_id": "68dcd6a04159d1f2418f9c72",
                    "name": "Yue Chen",
                    "hidden": false
                },
                {
                    "_id": "68dcd6a04159d1f2418f9c73",
                    "name": "Yuliang Xiu",
                    "hidden": false
                },
                {
                    "_id": "68dcd6a04159d1f2418f9c74",
                    "name": "Andreas Geiger",
                    "hidden": false
                },
                {
                    "_id": "68dcd6a04159d1f2418f9c75",
                    "name": "Anpei Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66606a13fc6c0816442bd161/o_7JHVjCwILwL1xpGK-t6.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/66606a13fc6c0816442bd161/M67C9yUMFsTHvYNDyCTwy.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/66606a13fc6c0816442bd161/KJXn0Q-Xnlp7YBH08_jz9.mp4"
            ],
            "publishedAt": "2025-09-30T17:59:51.000Z",
            "submittedOnDailyAt": "2025-10-01T05:57:29.331Z",
            "title": "TTT3R: 3D Reconstruction as Test-Time Training",
            "submittedOnDailyBy": {
                "_id": "66606a13fc6c0816442bd161",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
                "isPro": false,
                "fullname": "Xingyu Chen",
                "user": "rover-xingyu",
                "type": "user"
            },
            "summary": "Modern Recurrent Neural Networks have become a competitive architecture for\n3D reconstruction due to their linear-time complexity. However, their\nperformance degrades significantly when applied beyond the training context\nlength, revealing limited length generalization. In this work, we revisit the\n3D reconstruction foundation models from a Test-Time Training perspective,\nframing their designs as an online learning problem. Building on this\nperspective, we leverage the alignment confidence between the memory state and\nincoming observations to derive a closed-form learning rate for memory updates,\nto balance between retaining historical information and adapting to new\nobservations. This training-free intervention, termed TTT3R, substantially\nimproves length generalization, achieving a 2times improvement in global\npose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU\nmemory to process thousands of images. Code available in\nhttps://rover-xingyu.github.io/TTT3R",
            "upvotes": 4,
            "discussionId": "68dcd6a04159d1f2418f9c76",
            "projectPage": "https://rover-xingyu.github.io/TTT3R/",
            "githubRepo": "https://github.com/Inception3D/TTT3R",
            "ai_summary": "TTT3R, a test-time training intervention, enhances length generalization in 3D reconstruction by dynamically adjusting memory updates based on alignment confidence, improving global pose estimation and processing efficiency.",
            "ai_keywords": [
                "Recurrent Neural Networks",
                "3D reconstruction",
                "linear-time complexity",
                "length generalization",
                "Test-Time Training",
                "memory state",
                "incoming observations",
                "closed-form learning rate",
                "global pose estimation",
                "FPS",
                "GPU memory"
            ],
            "githubStars": 117
        },
        "publishedAt": "2025-09-30T13:59:51.000Z",
        "title": "TTT3R: 3D Reconstruction as Test-Time Training",
        "summary": "Modern Recurrent Neural Networks have become a competitive architecture for\n3D reconstruction due to their linear-time complexity. However, their\nperformance degrades significantly when applied beyond the training context\nlength, revealing limited length generalization. In this work, we revisit the\n3D reconstruction foundation models from a Test-Time Training perspective,\nframing their designs as an online learning problem. Building on this\nperspective, we leverage the alignment confidence between the memory state and\nincoming observations to derive a closed-form learning rate for memory updates,\nto balance between retaining historical information and adapting to new\nobservations. This training-free intervention, termed TTT3R, substantially\nimproves length generalization, achieving a 2times improvement in global\npose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU\nmemory to process thousands of images. Code available in\nhttps://rover-xingyu.github.io/TTT3R",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66606a13fc6c0816442bd161/o_7JHVjCwILwL1xpGK-t6.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/66606a13fc6c0816442bd161/M67C9yUMFsTHvYNDyCTwy.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/66606a13fc6c0816442bd161/KJXn0Q-Xnlp7YBH08_jz9.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26645.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66606a13fc6c0816442bd161",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
            "fullname": "Xingyu Chen",
            "name": "rover-xingyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.26539",
            "authors": [
                {
                    "_id": "68dc8f6f4159d1f2418f9a95",
                    "name": "Zhen Yang",
                    "hidden": false
                },
                {
                    "_id": "68dc8f6f4159d1f2418f9a96",
                    "name": "Zi-Yi Dou",
                    "hidden": false
                },
                {
                    "_id": "68dc8f6f4159d1f2418f9a97",
                    "name": "Di Feng",
                    "hidden": false
                },
                {
                    "_id": "68dc8f6f4159d1f2418f9a98",
                    "name": "Forrest Huang",
                    "hidden": false
                },
                {
                    "_id": "68dc8f6f4159d1f2418f9a99",
                    "name": "Anh Nguyen",
                    "hidden": false
                },
                {
                    "_id": "68dc8f6f4159d1f2418f9a9a",
                    "name": "Keen You",
                    "hidden": false
                },
                {
                    "_id": "68dc8f6f4159d1f2418f9a9b",
                    "name": "Omar Attia",
                    "hidden": false
                },
                {
                    "_id": "68dc8f6f4159d1f2418f9a9c",
                    "name": "Yuhao Yang",
                    "hidden": false
                },
                {
                    "_id": "68dc8f6f4159d1f2418f9a9d",
                    "name": "Michael Feng",
                    "hidden": false
                },
                {
                    "_id": "68dc8f6f4159d1f2418f9a9e",
                    "name": "Haotian Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dc8f6f4159d1f2418f9a9f",
                    "name": "Ram Ramrakhya",
                    "hidden": false
                },
                {
                    "_id": "68dc8f6f4159d1f2418f9aa0",
                    "name": "Chao Jia",
                    "hidden": false
                },
                {
                    "_id": "68dc8f6f4159d1f2418f9aa1",
                    "name": "Jeffrey Nichols",
                    "hidden": false
                },
                {
                    "_id": "68dc8f6f4159d1f2418f9aa2",
                    "name": "Alexander Toshev",
                    "hidden": false
                },
                {
                    "_id": "68dc8f6f4159d1f2418f9aa3",
                    "name": "Yinfei Yang",
                    "hidden": false
                },
                {
                    "_id": "68dc8f6f4159d1f2418f9aa4",
                    "name": "Zhe Gan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T17:13:56.000Z",
            "submittedOnDailyAt": "2025-10-01T00:48:59.196Z",
            "title": "Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Developing autonomous agents that effectively interact with Graphic User\nInterfaces (GUIs) remains a challenging open problem, especially for small\non-device models. In this paper, we present Ferret-UI Lite, a compact,\nend-to-end GUI agent that operates across diverse platforms, including mobile,\nweb, and desktop. Utilizing techniques optimized for developing small models,\nwe build our 3B Ferret-UI Lite agent through curating a diverse GUI data\nmixture from real and synthetic sources, strengthening inference-time\nperformance through chain-of-thought reasoning and visual tool-use, and\nreinforcement learning with designed rewards. Ferret-UI Lite achieves\ncompetitive performance with other small-scale GUI agents. In GUI grounding,\nFerret-UI Lite attains scores of 91.6%, 53.3%, and 61.2% on the\nScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI\nnavigation, Ferret-UI Lite achieves success rates of 28.0% on AndroidWorld\nand 19.8% on OSWorld. We share our methods and lessons learned from\ndeveloping compact, on-device GUI agents.",
            "upvotes": 4,
            "discussionId": "68dc8f704159d1f2418f9aa5",
            "ai_summary": "Ferret-UI Lite, a compact end-to-end GUI agent, achieves competitive performance across diverse platforms using chain-of-thought reasoning, visual tool-use, and reinforcement learning.",
            "ai_keywords": [
                "chain-of-thought reasoning",
                "visual tool-use",
                "reinforcement learning",
                "GUI agent",
                "GUI grounding",
                "GUI navigation",
                "ScreenSpot-V2",
                "ScreenSpot-Pro",
                "OSWorld-G",
                "AndroidWorld",
                "OSWorld"
            ]
        },
        "publishedAt": "2025-09-30T13:13:56.000Z",
        "title": "Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents",
        "summary": "Developing autonomous agents that effectively interact with Graphic User\nInterfaces (GUIs) remains a challenging open problem, especially for small\non-device models. In this paper, we present Ferret-UI Lite, a compact,\nend-to-end GUI agent that operates across diverse platforms, including mobile,\nweb, and desktop. Utilizing techniques optimized for developing small models,\nwe build our 3B Ferret-UI Lite agent through curating a diverse GUI data\nmixture from real and synthetic sources, strengthening inference-time\nperformance through chain-of-thought reasoning and visual tool-use, and\nreinforcement learning with designed rewards. Ferret-UI Lite achieves\ncompetitive performance with other small-scale GUI agents. In GUI grounding,\nFerret-UI Lite attains scores of 91.6%, 53.3%, and 61.2% on the\nScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI\nnavigation, Ferret-UI Lite achieves success rates of 28.0% on AndroidWorld\nand 19.8% on OSWorld. We share our methods and lessons learned from\ndeveloping compact, on-device GUI agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26539.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 115
        },
        "submitterOrganization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.23166",
            "authors": [
                {
                    "_id": "68dca79d4159d1f2418f9ba8",
                    "name": "Chenxing Wei",
                    "hidden": false
                },
                {
                    "_id": "68dca79d4159d1f2418f9ba9",
                    "name": "Hong Wang",
                    "hidden": false
                },
                {
                    "_id": "68dca79d4159d1f2418f9baa",
                    "name": "Ying He",
                    "hidden": false
                },
                {
                    "_id": "68dca79d4159d1f2418f9bab",
                    "name": "Fei Yu",
                    "hidden": false
                },
                {
                    "_id": "68dca79d4159d1f2418f9bac",
                    "name": "Yao Shu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65ed3051492a7f35db21fea2/LVF04XieiNnP-D38XBARY.png"
            ],
            "publishedAt": "2025-09-27T07:46:15.000Z",
            "submittedOnDailyAt": "2025-10-01T02:46:07.690Z",
            "title": "Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with\n  LLMs",
            "submittedOnDailyBy": {
                "_id": "65ed3051492a7f35db21fea2",
                "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
                "isPro": false,
                "fullname": "Chenxing Wei",
                "user": "kittttttt",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) employ multi-turn interaction as a fundamental\nparadigm for completing complex tasks. However, their performance often\ndegrades in extended interactions, as they are typically trained on static,\nsingle-turn data, which hinders their ability to adapt to real-time user\nfeedback. To address this limitation, we first propose a new paradigm:\nTest-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which utilizes\nuser feedback from the ongoing interaction as a reward signal to estimate a\nlatent optimal policy aligned with user preferences, then updates a small\nsubset of parameters to steer the model toward this policy, ultimately enabling\nefficient in-conversation self-correction. We then introduce Optimum-Referenced\nOne-Step Adaptation (ROSA), a lightweight algorithm that operationalizes T2PAM.\nROSA guides the model parameters toward a theoretical optimal policy in a\nsingle, efficient update step, avoiding costly iterative gradient-based\noptimization and minimizing computational overhead. We provide a rigorous\ntheoretical analysis guaranteeing that the policy of ROSA converges to the\npreference of user as the number of interactions increases. Extensive\nexperiments on challenging benchmark demonstrate that ROSA achieves significant\nimprovements in both task effectiveness and efficiency.",
            "upvotes": 4,
            "discussionId": "68dca79e4159d1f2418f9bad",
            "githubRepo": "https://github.com/kithib/ROSA",
            "ai_summary": "ROSA, a lightweight algorithm, enhances multi-turn interactions in LLMs by adapting to user feedback in real-time, improving both task effectiveness and efficiency.",
            "ai_keywords": [
                "multi-turn interaction",
                "Test-Time Policy Adaptation",
                "T2PAM",
                "latent optimal policy",
                "parameter adaptation",
                "Optimum-Referenced One-Step Adaptation",
                "ROSA",
                "theoretical optimal policy",
                "gradient-based optimization",
                "computational overhead"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-09-27T03:46:15.000Z",
        "title": "Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with\n  LLMs",
        "summary": "Large Language Models (LLMs) employ multi-turn interaction as a fundamental\nparadigm for completing complex tasks. However, their performance often\ndegrades in extended interactions, as they are typically trained on static,\nsingle-turn data, which hinders their ability to adapt to real-time user\nfeedback. To address this limitation, we first propose a new paradigm:\nTest-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which utilizes\nuser feedback from the ongoing interaction as a reward signal to estimate a\nlatent optimal policy aligned with user preferences, then updates a small\nsubset of parameters to steer the model toward this policy, ultimately enabling\nefficient in-conversation self-correction. We then introduce Optimum-Referenced\nOne-Step Adaptation (ROSA), a lightweight algorithm that operationalizes T2PAM.\nROSA guides the model parameters toward a theoretical optimal policy in a\nsingle, efficient update step, avoiding costly iterative gradient-based\noptimization and minimizing computational overhead. We provide a rigorous\ntheoretical analysis guaranteeing that the policy of ROSA converges to the\npreference of user as the number of interactions increases. Extensive\nexperiments on challenging benchmark demonstrate that ROSA achieves significant\nimprovements in both task effectiveness and efficiency.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65ed3051492a7f35db21fea2/LVF04XieiNnP-D38XBARY.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23166.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65ed3051492a7f35db21fea2",
            "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
            "fullname": "Chenxing Wei",
            "name": "kittttttt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.00231",
            "authors": [
                {
                    "_id": "68ddceaf6024653e8a3ed044",
                    "name": "Alex Chen",
                    "hidden": false
                },
                {
                    "_id": "68ddceaf6024653e8a3ed045",
                    "name": "Renato Geh",
                    "hidden": false
                },
                {
                    "_id": "68ddceaf6024653e8a3ed046",
                    "name": "Aditya Grover",
                    "hidden": false
                },
                {
                    "_id": "68ddceaf6024653e8a3ed047",
                    "name": "Guy Van den Broeck",
                    "hidden": false
                },
                {
                    "_id": "68ddceaf6024653e8a3ed048",
                    "name": "Daniel Israel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T19:55:26.000Z",
            "submittedOnDailyAt": "2025-10-01T23:31:25.526Z",
            "title": "The Pitfalls of KV Cache Compression",
            "submittedOnDailyBy": {
                "_id": "630139f1f6bea7dd15bdaf4e",
                "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
                "isPro": false,
                "fullname": "Daniel Israel",
                "user": "danielmisrael",
                "type": "user"
            },
            "summary": "KV cache compression promises increased throughput and efficiency with\nnegligible loss in performance. While the gains in throughput are indisputable\nand recent literature has indeed shown minimal degradation on particular\nbenchmarks, in general the consequences of compression in realistic scenarios\nsuch as multi-instruction prompting have been insufficiently studied. In this\npaper, we identify several pitfalls practitioners should be aware of when\ndeploying KV cache compressed LLMs. Importantly, we show that certain\ninstructions degrade much more rapidly with compression, effectively causing\nthem to be completely ignored by the LLM. As a practical example of that, we\nhighlight system prompt leakage as a case study, empirically showing the impact\nof compression on leakage and general instruction following. We show several\nfactors that play a role in prompt leakage: compression method, instruction\norder, and KV eviction bias. We then propose simple changes to KV cache\neviction policies that can reduce the impact of these factors and improve the\noverall performance in multi-instruction tasks.",
            "upvotes": 3,
            "discussionId": "68ddceaf6024653e8a3ed049",
            "ai_summary": "Compression of KV caches in LLMs can lead to performance degradation in multi-instruction tasks, particularly affecting system prompt leakage, but improved eviction policies can mitigate these issues.",
            "ai_keywords": [
                "KV cache compression",
                "LLMs",
                "multi-instruction prompting",
                "system prompt leakage",
                "KV eviction policies"
            ]
        },
        "publishedAt": "2025-09-30T15:55:26.000Z",
        "title": "The Pitfalls of KV Cache Compression",
        "summary": "KV cache compression promises increased throughput and efficiency with\nnegligible loss in performance. While the gains in throughput are indisputable\nand recent literature has indeed shown minimal degradation on particular\nbenchmarks, in general the consequences of compression in realistic scenarios\nsuch as multi-instruction prompting have been insufficiently studied. In this\npaper, we identify several pitfalls practitioners should be aware of when\ndeploying KV cache compressed LLMs. Importantly, we show that certain\ninstructions degrade much more rapidly with compression, effectively causing\nthem to be completely ignored by the LLM. As a practical example of that, we\nhighlight system prompt leakage as a case study, empirically showing the impact\nof compression on leakage and general instruction following. We show several\nfactors that play a role in prompt leakage: compression method, instruction\norder, and KV eviction bias. We then propose simple changes to KV cache\neviction policies that can reduce the impact of these factors and improve the\noverall performance in multi-instruction tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00231.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "630139f1f6bea7dd15bdaf4e",
            "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
            "fullname": "Daniel Israel",
            "name": "danielmisrael",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25716",
            "authors": [
                {
                    "_id": "68dd47440898d7e69e16c9b8",
                    "name": "Esakkivel Esakkiraja",
                    "hidden": false
                },
                {
                    "_id": "68dd47440898d7e69e16c9b9",
                    "user": {
                        "_id": "65ea6558b985ae912c57b294",
                        "avatarUrl": "/avatars/54eeba9ce8cb5bf35edfa8fad76a813d.svg",
                        "isPro": false,
                        "fullname": "Denis Akhiyarov",
                        "user": "dtanow",
                        "type": "user"
                    },
                    "name": "Denis Akhiyarov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T15:31:13.054Z",
                    "hidden": false
                },
                {
                    "_id": "68dd47440898d7e69e16c9ba",
                    "name": "Aditya Shanmugham",
                    "hidden": false
                },
                {
                    "_id": "68dd47440898d7e69e16c9bb",
                    "name": "Chitra Ganapathy",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65ea6558b985ae912c57b294/-fqmlKMNiV7lmF142EO0W.png"
            ],
            "publishedAt": "2025-09-30T03:23:27.000Z",
            "submittedOnDailyAt": "2025-10-01T14:16:49.071Z",
            "title": "DeepCodeSeek: Real-Time API Retrieval for Context-Aware Code Generation",
            "submittedOnDailyBy": {
                "_id": "65ea6558b985ae912c57b294",
                "avatarUrl": "/avatars/54eeba9ce8cb5bf35edfa8fad76a813d.svg",
                "isPro": false,
                "fullname": "Denis Akhiyarov",
                "user": "dtanow",
                "type": "user"
            },
            "summary": "Current search techniques are limited to standard RAG query-document\napplications. In this paper, we propose a novel technique to expand the code\nand index for predicting the required APIs, directly enabling high-quality,\nend-to-end code generation for auto-completion and agentic AI applications. We\naddress the problem of API leaks in current code-to-code benchmark datasets by\nintroducing a new dataset built from real-world ServiceNow Script Includes that\ncapture the challenge of unclear API usage intent in the code. Our evaluation\nmetrics show that this method achieves 87.86% top-40 retrieval accuracy,\nallowing the critical context with APIs needed for successful downstream code\ngeneration. To enable real-time predictions, we develop a comprehensive\npost-training pipeline that optimizes a compact 0.6B reranker through synthetic\ndataset generation, supervised fine-tuning, and reinforcement learning. This\napproach enables our compact reranker to outperform a much larger 8B model\nwhile maintaining 2.5x reduced latency, effectively addressing the nuances of\nenterprise-specific code without the computational overhead of larger models.",
            "upvotes": 3,
            "discussionId": "68dd47440898d7e69e16c9bc",
            "ai_summary": "A novel technique for predicting APIs and generating code in real-time using a compact reranker outperforms larger models with reduced latency, addressing API leaks and unclear usage intent in enterprise code.",
            "ai_keywords": [
                "API leaks",
                "code generation",
                "auto-completion",
                "agentic AI",
                "ServiceNow Script Includes",
                "top-40 retrieval accuracy",
                "post-training pipeline",
                "synthetic dataset generation",
                "supervised fine-tuning",
                "reinforcement learning",
                "compact reranker",
                "latency"
            ]
        },
        "publishedAt": "2025-09-29T23:23:27.000Z",
        "title": "DeepCodeSeek: Real-Time API Retrieval for Context-Aware Code Generation",
        "summary": "Current search techniques are limited to standard RAG query-document\napplications. In this paper, we propose a novel technique to expand the code\nand index for predicting the required APIs, directly enabling high-quality,\nend-to-end code generation for auto-completion and agentic AI applications. We\naddress the problem of API leaks in current code-to-code benchmark datasets by\nintroducing a new dataset built from real-world ServiceNow Script Includes that\ncapture the challenge of unclear API usage intent in the code. Our evaluation\nmetrics show that this method achieves 87.86% top-40 retrieval accuracy,\nallowing the critical context with APIs needed for successful downstream code\ngeneration. To enable real-time predictions, we develop a comprehensive\npost-training pipeline that optimizes a compact 0.6B reranker through synthetic\ndataset generation, supervised fine-tuning, and reinforcement learning. This\napproach enables our compact reranker to outperform a much larger 8B model\nwhile maintaining 2.5x reduced latency, effectively addressing the nuances of\nenterprise-specific code without the computational overhead of larger models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65ea6558b985ae912c57b294/-fqmlKMNiV7lmF142EO0W.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25716.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65ea6558b985ae912c57b294",
            "avatarUrl": "/avatars/54eeba9ce8cb5bf35edfa8fad76a813d.svg",
            "fullname": "Denis Akhiyarov",
            "name": "dtanow",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "submitterOrganization": {
            "_id": "65f4df5de83b55da5d79fbb6",
            "name": "ServiceNow-AI",
            "fullname": "ServiceNow-AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63d3095c2727d7888cbb54e2/Uv-Lx8PVGviqokfOyYlCN.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.26329",
            "authors": [
                {
                    "_id": "68dc98004159d1f2418f9b5f",
                    "name": "Yi-Cheng Lin",
                    "hidden": false
                },
                {
                    "_id": "68dc98004159d1f2418f9b60",
                    "name": "Yu-Hua Chen",
                    "hidden": false
                },
                {
                    "_id": "68dc98004159d1f2418f9b61",
                    "name": "Jia-Kai Dong",
                    "hidden": false
                },
                {
                    "_id": "68dc98004159d1f2418f9b62",
                    "name": "Yueh-Hsuan Huang",
                    "hidden": false
                },
                {
                    "_id": "68dc98004159d1f2418f9b63",
                    "name": "Szu-Chi Chen",
                    "hidden": false
                },
                {
                    "_id": "68dc98004159d1f2418f9b64",
                    "name": "Yu-Chen Chen",
                    "hidden": false
                },
                {
                    "_id": "68dc98004159d1f2418f9b65",
                    "name": "Chih-Yao Chen",
                    "hidden": false
                },
                {
                    "_id": "68dc98004159d1f2418f9b66",
                    "name": "Yu-Jung Lin",
                    "hidden": false
                },
                {
                    "_id": "68dc98004159d1f2418f9b67",
                    "name": "Yu-Ling Chen",
                    "hidden": false
                },
                {
                    "_id": "68dc98004159d1f2418f9b68",
                    "name": "Zih-Yu Chen",
                    "hidden": false
                },
                {
                    "_id": "68dc98004159d1f2418f9b69",
                    "name": "I-Ning Tsai",
                    "hidden": false
                },
                {
                    "_id": "68dc98004159d1f2418f9b6a",
                    "name": "Hsiu-Hsuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68dc98004159d1f2418f9b6b",
                    "name": "Ho-Lam Chung",
                    "hidden": false
                },
                {
                    "_id": "68dc98004159d1f2418f9b6c",
                    "name": "Ke-Han Lu",
                    "hidden": false
                },
                {
                    "_id": "68dc98004159d1f2418f9b6d",
                    "name": "Hung-yi Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T14:40:45.000Z",
            "submittedOnDailyAt": "2025-10-01T01:25:55.123Z",
            "title": "TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics",
            "submittedOnDailyBy": {
                "_id": "650b0d66664f7b7d088ca281",
                "avatarUrl": "/avatars/fce475c301f53e166fc3c8f5c5112c4a.svg",
                "isPro": false,
                "fullname": "Yi-Cheng Lin",
                "user": "dlion168",
                "type": "user"
            },
            "summary": "Large audio-language models are advancing rapidly, yet most evaluations\nemphasize speech or globally sourced sounds, overlooking culturally distinctive\ncues. This gap raises a critical question: can current models generalize to\nlocalized, non-semantic audio that communities instantly recognize but\noutsiders do not? To address this, we present TAU (Taiwan Audio Understanding),\na benchmark of everyday Taiwanese \"soundmarks.\" TAU is built through a pipeline\ncombining curated sources, human editing, and LLM-assisted question generation,\nproducing 702 clips and 1,794 multiple-choice items that cannot be solved by\ntranscripts alone. Experiments show that state-of-the-art LALMs, including\nGemini 2.5 and Qwen2-Audio, perform far below local humans. TAU demonstrates\nthe need for localized benchmarks to reveal cultural blind spots, guide more\nequitable multimodal evaluation, and ensure models serve communities beyond the\nglobal mainstream.",
            "upvotes": 2,
            "discussionId": "68dc98004159d1f2418f9b6e",
            "ai_summary": "TAU, a benchmark of culturally specific Taiwanese soundmarks, reveals that state-of-the-art large audio-language models underperform compared to local humans, highlighting the need for localized evaluations.",
            "ai_keywords": [
                "audio-language models",
                "soundmarks",
                "benchmark",
                "LLM-assisted question generation",
                "multimodal evaluation"
            ]
        },
        "publishedAt": "2025-09-30T10:40:45.000Z",
        "title": "TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics",
        "summary": "Large audio-language models are advancing rapidly, yet most evaluations\nemphasize speech or globally sourced sounds, overlooking culturally distinctive\ncues. This gap raises a critical question: can current models generalize to\nlocalized, non-semantic audio that communities instantly recognize but\noutsiders do not? To address this, we present TAU (Taiwan Audio Understanding),\na benchmark of everyday Taiwanese \"soundmarks.\" TAU is built through a pipeline\ncombining curated sources, human editing, and LLM-assisted question generation,\nproducing 702 clips and 1,794 multiple-choice items that cannot be solved by\ntranscripts alone. Experiments show that state-of-the-art LALMs, including\nGemini 2.5 and Qwen2-Audio, perform far below local humans. TAU demonstrates\nthe need for localized benchmarks to reveal cultural blind spots, guide more\nequitable multimodal evaluation, and ensure models serve communities beyond the\nglobal mainstream.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26329.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650b0d66664f7b7d088ca281",
            "avatarUrl": "/avatars/fce475c301f53e166fc3c8f5c5112c4a.svg",
            "fullname": "Yi-Cheng Lin",
            "name": "dlion168",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25085",
            "authors": [
                {
                    "_id": "68dd0458e795697646c0a04b",
                    "name": "Feng Wang",
                    "hidden": false
                },
                {
                    "_id": "68dd0458e795697646c0a04c",
                    "name": "Yuqing Li",
                    "hidden": false
                },
                {
                    "_id": "68dd0458e795697646c0a04d",
                    "user": {
                        "_id": "603763514de52ff951d89793",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/n-QouGYg7oE5QeDaAb3Ns.png",
                        "isPro": false,
                        "fullname": "Han Xiao",
                        "user": "hanxiao",
                        "type": "user"
                    },
                    "name": "Han Xiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T15:29:11.728Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:23:54.000Z",
            "submittedOnDailyAt": "2025-10-01T09:07:44.637Z",
            "title": "jina-reranker-v3: Last but Not Late Interaction for Document Reranking",
            "submittedOnDailyBy": {
                "_id": "603763514de52ff951d89793",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/n-QouGYg7oE5QeDaAb3Ns.png",
                "isPro": false,
                "fullname": "Han Xiao",
                "user": "hanxiao",
                "type": "user"
            },
            "summary": "jina-reranker-v3 is a 0.6B parameter multilingual document reranker that\nintroduces a novel last but not late interaction. Unlike late interaction\nmodels such as ColBERT that perform separate encoding followed by multi-vector\nmatching, our approach conducts causal self-attention between query and\ndocuments within the same context window, enabling rich cross-document\ninteractions before extracting contextual embeddings from the last token of\neach document. This compact architecture achieves state-of-the-art BEIR\nperformance with 61.94 nDCG@10 while being ten times smaller than generative\nlistwise rerankers.",
            "upvotes": 2,
            "discussionId": "68dd0458e795697646c0a04e",
            "ai_summary": "A multilingual document reranker using causal self-attention achieves state-of-the-art performance with a compact architecture.",
            "ai_keywords": [
                "multilingual document reranker",
                "causal self-attention",
                "BEIR",
                "nDCG@10",
                "generative listwise rerankers"
            ]
        },
        "publishedAt": "2025-09-29T13:23:54.000Z",
        "title": "jina-reranker-v3: Last but Not Late Interaction for Document Reranking",
        "summary": "jina-reranker-v3 is a 0.6B parameter multilingual document reranker that\nintroduces a novel last but not late interaction. Unlike late interaction\nmodels such as ColBERT that perform separate encoding followed by multi-vector\nmatching, our approach conducts causal self-attention between query and\ndocuments within the same context window, enabling rich cross-document\ninteractions before extracting contextual embeddings from the last token of\neach document. This compact architecture achieves state-of-the-art BEIR\nperformance with 61.94 nDCG@10 while being ten times smaller than generative\nlistwise rerankers.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25085.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "603763514de52ff951d89793",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/n-QouGYg7oE5QeDaAb3Ns.png",
            "fullname": "Han Xiao",
            "name": "hanxiao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "submitterOrganization": {
            "_id": "63563e0c2d14fcd7d83743cf",
            "name": "jinaai",
            "fullname": "Jina AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/603763514de52ff951d89793/wD54VbAHHyHop3uYlJKl4.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.23773",
            "authors": [
                {
                    "_id": "68dc9a964159d1f2418f9b83",
                    "name": "Utkarsh Sahu",
                    "hidden": false
                },
                {
                    "_id": "68dc9a964159d1f2418f9b84",
                    "name": "Zhisheng Qi",
                    "hidden": false
                },
                {
                    "_id": "68dc9a964159d1f2418f9b85",
                    "name": "Mahantesh Halappanavar",
                    "hidden": false
                },
                {
                    "_id": "68dc9a964159d1f2418f9b86",
                    "name": "Nedim Lipka",
                    "hidden": false
                },
                {
                    "_id": "68dc9a964159d1f2418f9b87",
                    "name": "Ryan A. Rossi",
                    "hidden": false
                },
                {
                    "_id": "68dc9a964159d1f2418f9b88",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:21:35.032Z",
                    "hidden": false
                },
                {
                    "_id": "68dc9a964159d1f2418f9b89",
                    "name": "Yu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dc9a964159d1f2418f9b8a",
                    "name": "Yao Ma",
                    "hidden": false
                },
                {
                    "_id": "68dc9a964159d1f2418f9b8b",
                    "name": "Yu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T09:40:27.000Z",
            "submittedOnDailyAt": "2025-10-01T01:36:08.796Z",
            "title": "Knowledge Homophily in Large Language Models",
            "submittedOnDailyBy": {
                "_id": "62c5947524171688a9feb992",
                "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                "isPro": false,
                "fullname": "Franck Dernoncourt",
                "user": "Franck-Dernoncourt",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have been increasingly studied as neural\nknowledge bases for supporting knowledge-intensive applications such as\nquestion answering and fact checking. However, the structural organization of\ntheir knowledge remains unexplored. Inspired by cognitive neuroscience\nfindings, such as semantic clustering and priming, where knowing one fact\nincreases the likelihood of recalling related facts, we investigate an\nanalogous knowledge homophily pattern in LLMs. To this end, we map LLM\nknowledge into a graph representation through knowledge checking at both the\ntriplet and entity levels. After that, we analyze the knowledgeability\nrelationship between an entity and its neighbors, discovering that LLMs tend to\npossess a similar level of knowledge about entities positioned closer in the\ngraph. Motivated by this homophily principle, we propose a Graph Neural Network\n(GNN) regression model to estimate entity-level knowledgeability scores for\ntriplets by leveraging their neighborhood scores. The predicted\nknowledgeability enables us to prioritize checking less well-known triplets,\nthereby maximizing knowledge coverage under the same labeling budget. This not\nonly improves the efficiency of active labeling for fine-tuning to inject\nknowledge into LLMs but also enhances multi-hop path retrieval in\nreasoning-intensive question answering.",
            "upvotes": 2,
            "discussionId": "68dc9a964159d1f2418f9b8c",
            "ai_summary": "Graph Neural Network regression models estimate entity-level knowledgeability in Large Language Models to improve active labeling and multi-hop reasoning.",
            "ai_keywords": [
                "Large Language Models",
                "knowledge homophily",
                "graph representation",
                "knowledge checking",
                "triplet",
                "entity",
                "Graph Neural Network",
                "knowledgeability scores",
                "active labeling",
                "multi-hop path retrieval"
            ]
        },
        "publishedAt": "2025-09-28T05:40:27.000Z",
        "title": "Knowledge Homophily in Large Language Models",
        "summary": "Large Language Models (LLMs) have been increasingly studied as neural\nknowledge bases for supporting knowledge-intensive applications such as\nquestion answering and fact checking. However, the structural organization of\ntheir knowledge remains unexplored. Inspired by cognitive neuroscience\nfindings, such as semantic clustering and priming, where knowing one fact\nincreases the likelihood of recalling related facts, we investigate an\nanalogous knowledge homophily pattern in LLMs. To this end, we map LLM\nknowledge into a graph representation through knowledge checking at both the\ntriplet and entity levels. After that, we analyze the knowledgeability\nrelationship between an entity and its neighbors, discovering that LLMs tend to\npossess a similar level of knowledge about entities positioned closer in the\ngraph. Motivated by this homophily principle, we propose a Graph Neural Network\n(GNN) regression model to estimate entity-level knowledgeability scores for\ntriplets by leveraging their neighborhood scores. The predicted\nknowledgeability enables us to prioritize checking less well-known triplets,\nthereby maximizing knowledge coverage under the same labeling budget. This not\nonly improves the efficiency of active labeling for fine-tuning to inject\nknowledge into LLMs but also enhances multi-hop path retrieval in\nreasoning-intensive question answering.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23773.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.23094",
            "authors": [
                {
                    "_id": "68db35a9d2bf1f4b15ec727b",
                    "name": "Yuchu Jiang",
                    "hidden": false
                },
                {
                    "_id": "68db35a9d2bf1f4b15ec727c",
                    "name": "Yue Cai",
                    "hidden": false
                },
                {
                    "_id": "68db35a9d2bf1f4b15ec727d",
                    "name": "Xiangzhong Luo",
                    "hidden": false
                },
                {
                    "_id": "68db35a9d2bf1f4b15ec727e",
                    "name": "Jiale Fu",
                    "hidden": false
                },
                {
                    "_id": "68db35a9d2bf1f4b15ec727f",
                    "name": "Jiarui Wang",
                    "hidden": false
                },
                {
                    "_id": "68db35a9d2bf1f4b15ec7280",
                    "name": "Chonghan Liu",
                    "hidden": false
                },
                {
                    "_id": "68db35a9d2bf1f4b15ec7281",
                    "name": "Xu Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-27T04:07:23.000Z",
            "submittedOnDailyAt": "2025-10-01T09:35:15.128Z",
            "title": "d^2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching",
            "submittedOnDailyBy": {
                "_id": "6404ae623d49e1e066b83861",
                "avatarUrl": "/avatars/3d0b6a6a63eb161d8a30dcb9218ede08.svg",
                "isPro": false,
                "fullname": "Yuchu Jiang",
                "user": "Kamichanw",
                "type": "user"
            },
            "summary": "Diffusion-based large language models (dLLMs), despite their promising\nperformance, still suffer from inferior inference efficiency. This is because\ndLLMs rely on bidirectional attention and cannot directly benefit from the\nstandard key-value (KV) cache as autoregressive models (ARMs) do. To tackle\nthis issue, we introduce Dual aDaptive Cache (d^2Cache), which is a\ntraining-free approximate KV cache framework for accelerating dLLM inference.\nd^2Cache features a two-stage fine-grained selection strategy to identify\ntokens and adaptively update their KV states at each decoding step, while\ncaching the KV states of the remaining tokens for reuse. Furthermore,\nd^2Cache naturally offers a more reliable decoding alternative, which can\nenable quasi left-to-right generation and mitigate premature overconfidence in\ntokens at the end of the sequence. Extensive experimental results on two\nrepresentative dLLMs (\\ie, LLaDA and Dream) demonstrate that d^2Cache not\nonly achieves substantial inference speedups, but also yields consistent\nimprovements in generation quality. The code is available at\nhttps://github.com/Kamichanw/d2Cache.",
            "upvotes": 2,
            "discussionId": "68db35a9d2bf1f4b15ec7282",
            "githubRepo": "https://github.com/Kamichanw/d2Cache",
            "ai_summary": "Dual aDaptive Cache (d²Cache) accelerates diffusion-based large language model inference by selectively updating key-value states and enabling quasi left-to-right generation, improving both speed and quality.",
            "ai_keywords": [
                "diffusion-based large language models",
                "dLLMs",
                "bidirectional attention",
                "key-value cache",
                "autoregressive models",
                "Dual aDaptive Cache",
                "d²Cache",
                "two-stage fine-grained selection",
                "quasi left-to-right generation",
                "LLaDA",
                "Dream"
            ],
            "githubStars": 8
        },
        "publishedAt": "2025-09-27T00:07:23.000Z",
        "title": "d^2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching",
        "summary": "Diffusion-based large language models (dLLMs), despite their promising\nperformance, still suffer from inferior inference efficiency. This is because\ndLLMs rely on bidirectional attention and cannot directly benefit from the\nstandard key-value (KV) cache as autoregressive models (ARMs) do. To tackle\nthis issue, we introduce Dual aDaptive Cache (d^2Cache), which is a\ntraining-free approximate KV cache framework for accelerating dLLM inference.\nd^2Cache features a two-stage fine-grained selection strategy to identify\ntokens and adaptively update their KV states at each decoding step, while\ncaching the KV states of the remaining tokens for reuse. Furthermore,\nd^2Cache naturally offers a more reliable decoding alternative, which can\nenable quasi left-to-right generation and mitigate premature overconfidence in\ntokens at the end of the sequence. Extensive experimental results on two\nrepresentative dLLMs (\\ie, LLaDA and Dream) demonstrate that d^2Cache not\nonly achieves substantial inference speedups, but also yields consistent\nimprovements in generation quality. The code is available at\nhttps://github.com/Kamichanw/d2Cache.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23094.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6404ae623d49e1e066b83861",
            "avatarUrl": "/avatars/3d0b6a6a63eb161d8a30dcb9218ede08.svg",
            "fullname": "Yuchu Jiang",
            "name": "Kamichanw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25248",
            "authors": [
                {
                    "_id": "68dcfc72e795697646c0a026",
                    "name": "Zehua Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dcfc72e795697646c0a027",
                    "name": "Ati Priya Bajaj",
                    "hidden": false
                },
                {
                    "_id": "68dcfc72e795697646c0a028",
                    "user": {
                        "_id": "636f4ad38305bc19758bfb70",
                        "avatarUrl": "/avatars/7becbb5d0280114bcbf05a9604f5de1f.svg",
                        "isPro": false,
                        "fullname": "Divij Handa",
                        "user": "Divij",
                        "type": "user"
                    },
                    "name": "Divij Handa",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:20:17.955Z",
                    "hidden": false
                },
                {
                    "_id": "68dcfc72e795697646c0a029",
                    "name": "Siyu Liu",
                    "hidden": false
                },
                {
                    "_id": "68dcfc72e795697646c0a02a",
                    "name": "Arvind S Raj",
                    "hidden": false
                },
                {
                    "_id": "68dcfc72e795697646c0a02b",
                    "name": "Hongkai Chen",
                    "hidden": false
                },
                {
                    "_id": "68dcfc72e795697646c0a02c",
                    "name": "Hulin Wang",
                    "hidden": false
                },
                {
                    "_id": "68dcfc72e795697646c0a02d",
                    "name": "Yibo Liu",
                    "hidden": false
                },
                {
                    "_id": "68dcfc72e795697646c0a02e",
                    "name": "Zion Leonahenahe Basque",
                    "hidden": false
                },
                {
                    "_id": "68dcfc72e795697646c0a02f",
                    "name": "Souradip Nath",
                    "hidden": false
                },
                {
                    "_id": "68dcfc72e795697646c0a030",
                    "name": "Vishal Juneja",
                    "hidden": false
                },
                {
                    "_id": "68dcfc72e795697646c0a031",
                    "name": "Nikhil Chapre",
                    "hidden": false
                },
                {
                    "_id": "68dcfc72e795697646c0a032",
                    "name": "Yan Shoshitaishvili",
                    "hidden": false
                },
                {
                    "_id": "68dcfc72e795697646c0a033",
                    "name": "Adam Doupé",
                    "hidden": false
                },
                {
                    "_id": "68dcfc72e795697646c0a034",
                    "name": "Chitta Baral",
                    "hidden": false
                },
                {
                    "_id": "68dcfc72e795697646c0a035",
                    "name": "Ruoyu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-27T03:02:46.000Z",
            "submittedOnDailyAt": "2025-10-01T08:34:56.276Z",
            "title": "BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source\n  Software",
            "submittedOnDailyBy": {
                "_id": "636f4ad38305bc19758bfb70",
                "avatarUrl": "/avatars/7becbb5d0280114bcbf05a9604f5de1f.svg",
                "isPro": false,
                "fullname": "Divij Handa",
                "user": "Divij",
                "type": "user"
            },
            "summary": "Automatically compiling open-source software (OSS) projects is a vital,\nlabor-intensive, and complex task, which makes it a good challenge for LLM\nAgents. Existing methods rely on manually curated rules and workflows, which\ncannot adapt to OSS that requires customized configuration or environment\nsetup. Recent attempts using Large Language Models (LLMs) used selective\nevaluation on a subset of highly rated OSS, a practice that underestimates the\nrealistic challenges of OSS compilation. In practice, compilation instructions\nare often absent, dependencies are undocumented, and successful builds may even\nrequire patching source files or modifying build scripts. We propose a more\nchallenging and realistic benchmark, BUILD-BENCH, comprising OSS that are more\ndiverse in quality, scale, and characteristics. Furthermore, we propose a\nstrong baseline LLM-based agent, OSS-BUILD-AGENT, an effective system with\nenhanced build instruction retrieval module that achieves state-of-the-art\nperformance on BUILD-BENCH and is adaptable to heterogeneous OSS\ncharacteristics. We also provide detailed analysis regarding different\ncompilation method design choices and their influence to the whole task,\noffering insights to guide future advances. We believe performance on\nBUILD-BENCH can faithfully reflect an agent's ability to tackle compilation as\na complex software engineering tasks, and, as such, our benchmark will spur\ninnovation with a significant impact on downstream applications in the fields\nof software development and software security.",
            "upvotes": 2,
            "discussionId": "68dcfc72e795697646c0a036",
            "ai_summary": "A new benchmark, BUILD-BENCH, and an LLM-based agent, OSS-BUILD-AGENT, address the complexities of compiling diverse open-source software projects.",
            "ai_keywords": [
                "LLM Agents",
                "Large Language Models",
                "BUILD-BENCH",
                "OSS-BUILD-AGENT",
                "build instruction retrieval",
                "software engineering tasks"
            ]
        },
        "publishedAt": "2025-09-26T23:02:46.000Z",
        "title": "BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source\n  Software",
        "summary": "Automatically compiling open-source software (OSS) projects is a vital,\nlabor-intensive, and complex task, which makes it a good challenge for LLM\nAgents. Existing methods rely on manually curated rules and workflows, which\ncannot adapt to OSS that requires customized configuration or environment\nsetup. Recent attempts using Large Language Models (LLMs) used selective\nevaluation on a subset of highly rated OSS, a practice that underestimates the\nrealistic challenges of OSS compilation. In practice, compilation instructions\nare often absent, dependencies are undocumented, and successful builds may even\nrequire patching source files or modifying build scripts. We propose a more\nchallenging and realistic benchmark, BUILD-BENCH, comprising OSS that are more\ndiverse in quality, scale, and characteristics. Furthermore, we propose a\nstrong baseline LLM-based agent, OSS-BUILD-AGENT, an effective system with\nenhanced build instruction retrieval module that achieves state-of-the-art\nperformance on BUILD-BENCH and is adaptable to heterogeneous OSS\ncharacteristics. We also provide detailed analysis regarding different\ncompilation method design choices and their influence to the whole task,\noffering insights to guide future advances. We believe performance on\nBUILD-BENCH can faithfully reflect an agent's ability to tackle compilation as\na complex software engineering tasks, and, as such, our benchmark will spur\ninnovation with a significant impact on downstream applications in the fields\nof software development and software security.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25248.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636f4ad38305bc19758bfb70",
            "avatarUrl": "/avatars/7becbb5d0280114bcbf05a9604f5de1f.svg",
            "fullname": "Divij Handa",
            "name": "Divij",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "submitterOrganization": {
            "_id": "62687508ff21ae22a196a25f",
            "name": "cogint",
            "fullname": "Cogint ASU",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/rnyzqFy3Auu868qyII1Ue.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.26604",
            "authors": [
                {
                    "_id": "68dd2684e795697646c0a14e",
                    "user": {
                        "_id": "64b7c3d5f72c6cd946dff515",
                        "avatarUrl": "/avatars/f579bc83b529782bbe6109f18d89837a.svg",
                        "isPro": false,
                        "fullname": "Ilpo Viertola",
                        "user": "bilpo",
                        "type": "user"
                    },
                    "name": "Ilpo Viertola",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T15:28:18.336Z",
                    "hidden": false
                },
                {
                    "_id": "68dd2684e795697646c0a14f",
                    "name": "Vladimir Iashin",
                    "hidden": false
                },
                {
                    "_id": "68dd2684e795697646c0a150",
                    "name": "Esa Rahtu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T17:49:41.000Z",
            "submittedOnDailyAt": "2025-10-01T11:34:29.440Z",
            "title": "Video Object Segmentation-Aware Audio Generation",
            "submittedOnDailyBy": {
                "_id": "64b7c3d5f72c6cd946dff515",
                "avatarUrl": "/avatars/f579bc83b529782bbe6109f18d89837a.svg",
                "isPro": false,
                "fullname": "Ilpo Viertola",
                "user": "bilpo",
                "type": "user"
            },
            "summary": "Existing multimodal audio generation models often lack precise user control,\nwhich limits their applicability in professional Foley workflows. In\nparticular, these models focus on the entire video and do not provide precise\nmethods for prioritizing a specific object within a scene, generating\nunnecessary background sounds, or focusing on the wrong objects. To address\nthis gap, we introduce the novel task of video object segmentation-aware audio\ngeneration, which explicitly conditions sound synthesis on object-level\nsegmentation maps. We present SAGANet, a new multimodal generative model that\nenables controllable audio generation by leveraging visual segmentation masks\nalong with video and textual cues. Our model provides users with fine-grained\nand visually localized control over audio generation. To support this task and\nfurther research on segmentation-aware Foley, we propose Segmented Music Solos,\na benchmark dataset of musical instrument performance videos with segmentation\ninformation. Our method demonstrates substantial improvements over current\nstate-of-the-art methods and sets a new standard for controllable,\nhigh-fidelity Foley synthesis. Code, samples, and Segmented Music Solos are\navailable at https://saganet.notion.site",
            "upvotes": 1,
            "discussionId": "68dd2684e795697646c0a151",
            "projectPage": "https://saganet.notion.site",
            "githubRepo": "https://github.com/ilpoviertola/SAGANet",
            "ai_summary": "SAGANet, a multimodal generative model, enhances audio generation by using object-level segmentation maps, improving control and fidelity in professional Foley workflows.",
            "ai_keywords": [
                "multimodal audio generation",
                "video object segmentation-aware audio generation",
                "SAGANet",
                "sound synthesis",
                "visual segmentation masks",
                "Segmented Music Solos",
                "Foley synthesis"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-09-30T13:49:41.000Z",
        "title": "Video Object Segmentation-Aware Audio Generation",
        "summary": "Existing multimodal audio generation models often lack precise user control,\nwhich limits their applicability in professional Foley workflows. In\nparticular, these models focus on the entire video and do not provide precise\nmethods for prioritizing a specific object within a scene, generating\nunnecessary background sounds, or focusing on the wrong objects. To address\nthis gap, we introduce the novel task of video object segmentation-aware audio\ngeneration, which explicitly conditions sound synthesis on object-level\nsegmentation maps. We present SAGANet, a new multimodal generative model that\nenables controllable audio generation by leveraging visual segmentation masks\nalong with video and textual cues. Our model provides users with fine-grained\nand visually localized control over audio generation. To support this task and\nfurther research on segmentation-aware Foley, we propose Segmented Music Solos,\na benchmark dataset of musical instrument performance videos with segmentation\ninformation. Our method demonstrates substantial improvements over current\nstate-of-the-art methods and sets a new standard for controllable,\nhigh-fidelity Foley synthesis. Code, samples, and Segmented Music Solos are\navailable at https://saganet.notion.site",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26604.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b7c3d5f72c6cd946dff515",
            "avatarUrl": "/avatars/f579bc83b529782bbe6109f18d89837a.svg",
            "fullname": "Ilpo Viertola",
            "name": "bilpo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25666",
            "authors": [
                {
                    "_id": "68dd394de795697646c0a192",
                    "name": "Justin Chih-Yao Chen",
                    "hidden": false
                },
                {
                    "_id": "68dd394de795697646c0a193",
                    "name": "Becky Xiangyu Peng",
                    "hidden": false
                },
                {
                    "_id": "68dd394de795697646c0a194",
                    "name": "Prafulla Kumar Choubey",
                    "hidden": false
                },
                {
                    "_id": "68dd394de795697646c0a195",
                    "name": "Kung-Hsiang Huang",
                    "hidden": false
                },
                {
                    "_id": "68dd394de795697646c0a196",
                    "name": "Jiaxin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dd394de795697646c0a197",
                    "name": "Mohit Bansal",
                    "hidden": false
                },
                {
                    "_id": "68dd394de795697646c0a198",
                    "name": "Chien-Sheng Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T02:01:40.000Z",
            "submittedOnDailyAt": "2025-10-01T12:54:03.394Z",
            "title": "Nudging the Boundaries of LLM Reasoning",
            "submittedOnDailyBy": {
                "_id": "62ce26129f723d34cf1f595a",
                "avatarUrl": "/avatars/8e305ac7c170d70fbf83c109789b40d9.svg",
                "isPro": false,
                "fullname": "Justin Chen",
                "user": "dinobby",
                "type": "user"
            },
            "summary": "Current online reinforcement learning (RL) algorithms like GRPO share a key\nlimitation in LLM reasoning: they cannot learn from problems that are\n\"unsolvable\" to the model. In other words, they can only improve performance on\nproblems where the model is capable of exploring the correct answer.\nConsequently, the model's \"upper limit\" remains unchanged after RL training,\neven though the likelihood of solving easier, solvable problems may increase.\nThese hard samples cannot contribute to training, as no rollouts yield rewards\nand thus no gradients are produced. To unlock learning from these hard samples,\nwe propose NuRL, a \"nudging\" method that aims to push the upper bound of LLM\nreasoning using self-generated hints, i.e., abstract cues that help reduce the\nproblem difficulty for the model. Given a question and its gold answer, the\nmodel generates a CoT and then produces a hint containing the core knowledge\nneeded to solve the problem. During training, we generate G rollouts from the\nbase policy and use the pass rate to decide whether the hint should be\ninjected. For hard samples with a 0% pass rate, we inject the hint and\nregenerate a new batch of trajectories. This yields two benefits: (1) the hint\nboosts pass rates (from 0% to non-zero), thereby introducing training signals\nfor previously unsolvable samples, and (2) the hints are self-generated,\navoiding distributional shift and do not rely on external models. NuRL achieves\nconsistent improvements across 6 benchmarks and 3 models, while remaining\ncomplementary to test-time scaling. Notably, NuRL can raise the model's upper\nlimit, whereas GRPO leaves pass@1024 unchanged from the base model.\nFurthermore, we present a systematic study of what makes an effective hint and\nwhen hints are most useful. Interestingly, the best hints are abstract and\nhigh-level, and are most beneficial when applied necessarily and after GRPO has\nconverged.",
            "upvotes": 1,
            "discussionId": "68dd394de795697646c0a199",
            "ai_summary": "NuRL, a nudging method using self-generated hints, enhances the upper limit of LLM reasoning in online reinforcement learning by enabling learning from previously unsolvable problems.",
            "ai_keywords": [
                "online reinforcement learning",
                "RL",
                "GRPO",
                "LLM reasoning",
                "unsolvable problems",
                "rollouts",
                "rewards",
                "gradients",
                "nudging method",
                "self-generated hints",
                "core knowledge",
                "pass rate",
                "training signals",
                "distributional shift",
                "test-time scaling",
                "pass@1024",
                "effective hints",
                "abstract hints",
                "high-level hints"
            ]
        },
        "publishedAt": "2025-09-29T22:01:40.000Z",
        "title": "Nudging the Boundaries of LLM Reasoning",
        "summary": "Current online reinforcement learning (RL) algorithms like GRPO share a key\nlimitation in LLM reasoning: they cannot learn from problems that are\n\"unsolvable\" to the model. In other words, they can only improve performance on\nproblems where the model is capable of exploring the correct answer.\nConsequently, the model's \"upper limit\" remains unchanged after RL training,\neven though the likelihood of solving easier, solvable problems may increase.\nThese hard samples cannot contribute to training, as no rollouts yield rewards\nand thus no gradients are produced. To unlock learning from these hard samples,\nwe propose NuRL, a \"nudging\" method that aims to push the upper bound of LLM\nreasoning using self-generated hints, i.e., abstract cues that help reduce the\nproblem difficulty for the model. Given a question and its gold answer, the\nmodel generates a CoT and then produces a hint containing the core knowledge\nneeded to solve the problem. During training, we generate G rollouts from the\nbase policy and use the pass rate to decide whether the hint should be\ninjected. For hard samples with a 0% pass rate, we inject the hint and\nregenerate a new batch of trajectories. This yields two benefits: (1) the hint\nboosts pass rates (from 0% to non-zero), thereby introducing training signals\nfor previously unsolvable samples, and (2) the hints are self-generated,\navoiding distributional shift and do not rely on external models. NuRL achieves\nconsistent improvements across 6 benchmarks and 3 models, while remaining\ncomplementary to test-time scaling. Notably, NuRL can raise the model's upper\nlimit, whereas GRPO leaves pass@1024 unchanged from the base model.\nFurthermore, we present a systematic study of what makes an effective hint and\nwhen hints are most useful. Interestingly, the best hints are abstract and\nhigh-level, and are most beneficial when applied necessarily and after GRPO has\nconverged.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25666.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62ce26129f723d34cf1f595a",
            "avatarUrl": "/avatars/8e305ac7c170d70fbf83c109789b40d9.svg",
            "fullname": "Justin Chen",
            "name": "dinobby",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25631",
            "authors": [
                {
                    "_id": "68dd57baae4a047656f86bb7",
                    "name": "Jason Stock",
                    "hidden": false
                },
                {
                    "_id": "68dd57baae4a047656f86bb8",
                    "name": "Troy Arcomano",
                    "hidden": false
                },
                {
                    "_id": "68dd57baae4a047656f86bb9",
                    "name": "Rao Kotamarthi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6309242c1a8d77fa3190be5e/0VxgQdM63A8HFAbhb5aQ4.gif"
            ],
            "publishedAt": "2025-09-30T00:54:24.000Z",
            "submittedOnDailyAt": "2025-10-01T20:07:04.872Z",
            "title": "Swift: An Autoregressive Consistency Model for Efficient Weather\n  Forecasting",
            "submittedOnDailyBy": {
                "_id": "6309242c1a8d77fa3190be5e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6309242c1a8d77fa3190be5e/8lIcbT6UACrP0_WpCuV5p.jpeg",
                "isPro": false,
                "fullname": "Jason Stock",
                "user": "stockeh",
                "type": "user"
            },
            "summary": "Diffusion models offer a physically grounded framework for probabilistic\nweather forecasting, but their typical reliance on slow, iterative solvers\nduring inference makes them impractical for subseasonal-to-seasonal (S2S)\napplications where long lead-times and domain-driven calibration are essential.\nTo address this, we introduce Swift, a single-step consistency model that, for\nthe first time, enables autoregressive finetuning of a probability flow model\nwith a continuous ranked probability score (CRPS) objective. This eliminates\nthe need for multi-model ensembling or parameter perturbations. Results show\nthat Swift produces skillful 6-hourly forecasts that remain stable for up to 75\ndays, running 39times faster than state-of-the-art diffusion baselines while\nachieving forecast skill competitive with the numerical-based, operational IFS\nENS. This marks a step toward efficient and reliable ensemble forecasting from\nmedium-range to seasonal-scales.",
            "upvotes": 1,
            "discussionId": "68dd57baae4a047656f86bba",
            "githubRepo": "https://github.com/stockeh/swift",
            "ai_summary": "Swift, a single-step consistency model, enables efficient and skillful probabilistic weather forecasting by autoregressive finetuning of a probability flow model with CRPS, outperforming diffusion models and competitive with IFS ENS.",
            "ai_keywords": [
                "diffusion models",
                "consistency model",
                "autoregressive finetuning",
                "probability flow model",
                "continuous ranked probability score",
                "CRPS",
                "ensemble forecasting",
                "medium-range",
                "seasonal-scales"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-09-29T20:54:24.000Z",
        "title": "Swift: An Autoregressive Consistency Model for Efficient Weather\n  Forecasting",
        "summary": "Diffusion models offer a physically grounded framework for probabilistic\nweather forecasting, but their typical reliance on slow, iterative solvers\nduring inference makes them impractical for subseasonal-to-seasonal (S2S)\napplications where long lead-times and domain-driven calibration are essential.\nTo address this, we introduce Swift, a single-step consistency model that, for\nthe first time, enables autoregressive finetuning of a probability flow model\nwith a continuous ranked probability score (CRPS) objective. This eliminates\nthe need for multi-model ensembling or parameter perturbations. Results show\nthat Swift produces skillful 6-hourly forecasts that remain stable for up to 75\ndays, running 39times faster than state-of-the-art diffusion baselines while\nachieving forecast skill competitive with the numerical-based, operational IFS\nENS. This marks a step toward efficient and reliable ensemble forecasting from\nmedium-range to seasonal-scales.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6309242c1a8d77fa3190be5e/0VxgQdM63A8HFAbhb5aQ4.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25631.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6309242c1a8d77fa3190be5e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6309242c1a8d77fa3190be5e/8lIcbT6UACrP0_WpCuV5p.jpeg",
            "fullname": "Jason Stock",
            "name": "stockeh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25134",
            "authors": [
                {
                    "_id": "68db6506d2bf1f4b15ec75fc",
                    "name": "Tomoyuki Suzuki",
                    "hidden": false
                },
                {
                    "_id": "68db6506d2bf1f4b15ec75fd",
                    "name": "Kang-Jun Liu",
                    "hidden": false
                },
                {
                    "_id": "68db6506d2bf1f4b15ec75fe",
                    "name": "Naoto Inoue",
                    "hidden": false
                },
                {
                    "_id": "68db6506d2bf1f4b15ec75ff",
                    "name": "Kota Yamaguchi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/628b45cf488ff0ea1c7007dc/xhZBntJXvNREfocYxWcDi.png"
            ],
            "publishedAt": "2025-09-29T17:50:12.000Z",
            "submittedOnDailyAt": "2025-10-01T03:35:02.847Z",
            "title": "LayerD: Decomposing Raster Graphic Designs into Layers",
            "submittedOnDailyBy": {
                "_id": "628b45cf488ff0ea1c7007dc",
                "avatarUrl": "/avatars/8000a1f6ea159cad9f0ffa384cd50f2a.svg",
                "isPro": false,
                "fullname": "Tomoyuki Suzuki",
                "user": "tomoyukun",
                "type": "user"
            },
            "summary": "Designers craft and edit graphic designs in a layer representation, but\nlayer-based editing becomes impossible once composited into a raster image. In\nthis work, we propose LayerD, a method to decompose raster graphic designs into\nlayers for re-editable creative workflow. LayerD addresses the decomposition\ntask by iteratively extracting unoccluded foreground layers. We propose a\nsimple yet effective refinement approach taking advantage of the assumption\nthat layers often exhibit uniform appearance in graphic designs. As\ndecomposition is ill-posed and the ground-truth layer structure may not be\nreliable, we develop a quality metric that addresses the difficulty. In\nexperiments, we show that LayerD successfully achieves high-quality\ndecomposition and outperforms baselines. We also demonstrate the use of LayerD\nwith state-of-the-art image generators and layer-based editing.",
            "upvotes": 1,
            "discussionId": "68db6506d2bf1f4b15ec7600",
            "projectPage": "https://cyberagentailab.github.io/LayerD/",
            "githubRepo": "https://github.com/CyberAgentAILab/LayerD",
            "ai_summary": "LayerD decomposes raster images into editable layers using iterative extraction and refinement, outperforming existing methods and enabling use with advanced image generators.",
            "ai_keywords": [
                "layer representation",
                "raster image",
                "decomposition",
                "foreground layers",
                "uniform appearance",
                "quality metric",
                "state-of-the-art image generators",
                "layer-based editing"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-09-29T13:50:12.000Z",
        "title": "LayerD: Decomposing Raster Graphic Designs into Layers",
        "summary": "Designers craft and edit graphic designs in a layer representation, but\nlayer-based editing becomes impossible once composited into a raster image. In\nthis work, we propose LayerD, a method to decompose raster graphic designs into\nlayers for re-editable creative workflow. LayerD addresses the decomposition\ntask by iteratively extracting unoccluded foreground layers. We propose a\nsimple yet effective refinement approach taking advantage of the assumption\nthat layers often exhibit uniform appearance in graphic designs. As\ndecomposition is ill-posed and the ground-truth layer structure may not be\nreliable, we develop a quality metric that addresses the difficulty. In\nexperiments, we show that LayerD successfully achieves high-quality\ndecomposition and outperforms baselines. We also demonstrate the use of LayerD\nwith state-of-the-art image generators and layer-based editing.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/628b45cf488ff0ea1c7007dc/xhZBntJXvNREfocYxWcDi.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25134.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "628b45cf488ff0ea1c7007dc",
            "avatarUrl": "/avatars/8000a1f6ea159cad9f0ffa384cd50f2a.svg",
            "fullname": "Tomoyuki Suzuki",
            "name": "tomoyukun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25082",
            "authors": [
                {
                    "_id": "68dcc85e4159d1f2418f9c43",
                    "name": "Xiaoyi Huang",
                    "hidden": false
                },
                {
                    "_id": "68dcc85e4159d1f2418f9c44",
                    "name": "Junwei Wu",
                    "hidden": false
                },
                {
                    "_id": "68dcc85e4159d1f2418f9c45",
                    "user": {
                        "_id": "655469586bc4180700cf7a34",
                        "avatarUrl": "/avatars/252392d0c45783d8f149feac7a6215ec.svg",
                        "isPro": false,
                        "fullname": "Kejia Zhang",
                        "user": "KejiaRobust",
                        "type": "user"
                    },
                    "name": "Kejia Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T15:29:15.983Z",
                    "hidden": false
                },
                {
                    "_id": "68dcc85e4159d1f2418f9c46",
                    "name": "Carl Yang",
                    "hidden": false
                },
                {
                    "_id": "68dcc85e4159d1f2418f9c47",
                    "name": "Zhiming Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:22:40.000Z",
            "submittedOnDailyAt": "2025-10-01T04:52:50.918Z",
            "title": "MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial\n  Purification",
            "submittedOnDailyBy": {
                "_id": "655469586bc4180700cf7a34",
                "avatarUrl": "/avatars/252392d0c45783d8f149feac7a6215ec.svg",
                "isPro": false,
                "fullname": "Kejia Zhang",
                "user": "KejiaRobust",
                "type": "user"
            },
            "summary": "Adversarial purification with diffusion models has emerged as a promising\ndefense strategy, but existing methods typically rely on uniform noise\ninjection, which indiscriminately perturbs all frequencies, corrupting semantic\nstructures and undermining robustness. Our empirical study reveals that\nadversarial perturbations are not uniformly distributed: they are predominantly\nconcentrated in high-frequency regions, with heterogeneous magnitude intensity\npatterns that vary across frequencies and attack types. Motivated by this\nobservation, we introduce MANI-Pure, a magnitude-adaptive purification\nframework that leverages the magnitude spectrum of inputs to guide the\npurification process. Instead of injecting homogeneous noise, MANI-Pure\nadaptively applies heterogeneous, frequency-targeted noise, effectively\nsuppressing adversarial perturbations in fragile high-frequency, low-magnitude\nbands while preserving semantically critical low-frequency content. Extensive\nexperiments on CIFAR-10 and ImageNet-1K validate the effectiveness of\nMANI-Pure. It narrows the clean accuracy gap to within 0.59 of the original\nclassifier, while boosting robust accuracy by 2.15, and achieves the top-1\nrobust accuracy on the RobustBench leaderboard, surpassing the previous\nstate-of-the-art method.",
            "upvotes": 1,
            "discussionId": "68dcc85e4159d1f2418f9c48",
            "projectPage": "https://phoshowy.github.io/MANI-Pure.github.io/",
            "ai_summary": "MANI-Pure, a magnitude-adaptive purification framework using diffusion models, effectively suppresses high-frequency adversarial perturbations while preserving low-frequency content, enhancing robust accuracy.",
            "ai_keywords": [
                "adversarial purification",
                "diffusion models",
                "uniform noise",
                "high-frequency regions",
                "magnitude spectrum",
                "frequency-targeted noise",
                "CIFAR-10",
                "ImageNet-1K",
                "RobustBench"
            ]
        },
        "publishedAt": "2025-09-29T13:22:40.000Z",
        "title": "MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial\n  Purification",
        "summary": "Adversarial purification with diffusion models has emerged as a promising\ndefense strategy, but existing methods typically rely on uniform noise\ninjection, which indiscriminately perturbs all frequencies, corrupting semantic\nstructures and undermining robustness. Our empirical study reveals that\nadversarial perturbations are not uniformly distributed: they are predominantly\nconcentrated in high-frequency regions, with heterogeneous magnitude intensity\npatterns that vary across frequencies and attack types. Motivated by this\nobservation, we introduce MANI-Pure, a magnitude-adaptive purification\nframework that leverages the magnitude spectrum of inputs to guide the\npurification process. Instead of injecting homogeneous noise, MANI-Pure\nadaptively applies heterogeneous, frequency-targeted noise, effectively\nsuppressing adversarial perturbations in fragile high-frequency, low-magnitude\nbands while preserving semantically critical low-frequency content. Extensive\nexperiments on CIFAR-10 and ImageNet-1K validate the effectiveness of\nMANI-Pure. It narrows the clean accuracy gap to within 0.59 of the original\nclassifier, while boosting robust accuracy by 2.15, and achieves the top-1\nrobust accuracy on the RobustBench leaderboard, surpassing the previous\nstate-of-the-art method.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25082.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655469586bc4180700cf7a34",
            "avatarUrl": "/avatars/252392d0c45783d8f149feac7a6215ec.svg",
            "fullname": "Kejia Zhang",
            "name": "KejiaRobust",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.24732",
            "authors": [
                {
                    "_id": "68dbc577d2bf1f4b15ec7a6b",
                    "name": "Juergen Schmidhuber",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T12:57:35.000Z",
            "submittedOnDailyAt": "2025-10-01T05:38:13.437Z",
            "title": "Who invented deep residual learning?",
            "submittedOnDailyBy": {
                "_id": "635e3a76106f984574c36409",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
                "isPro": false,
                "fullname": "Bo Liu",
                "user": "Benjamin-eecs",
                "type": "user"
            },
            "summary": "Modern AI is based on deep artificial neural networks (NNs). As of 2025, the\nmost cited scientific article of the 21st century is an NN paper on deep\nresidual learning with residual connections. Who invented this? We present a\ntimeline of the evolution of deep residual learning.",
            "upvotes": 1,
            "discussionId": "68dbc578d2bf1f4b15ec7a6c",
            "ai_summary": "A timeline of the evolution of deep residual learning, a key advancement in neural network architecture.",
            "ai_keywords": [
                "deep artificial neural networks",
                "NNs",
                "deep residual learning",
                "residual connections"
            ]
        },
        "publishedAt": "2025-09-29T08:57:35.000Z",
        "title": "Who invented deep residual learning?",
        "summary": "Modern AI is based on deep artificial neural networks (NNs). As of 2025, the\nmost cited scientific article of the 21st century is an NN paper on deep\nresidual learning with residual connections. Who invented this? We present a\ntimeline of the evolution of deep residual learning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24732.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "635e3a76106f984574c36409",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
            "fullname": "Bo Liu",
            "name": "Benjamin-eecs",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.23695",
            "authors": [
                {
                    "_id": "68dbf49f4159d1f2418f980d",
                    "user": {
                        "_id": "636480adf2984121733e503d",
                        "avatarUrl": "/avatars/bd5bcff68ca620f2ed05812abc65ac1f.svg",
                        "isPro": false,
                        "fullname": "Qingren Yao",
                        "user": "Qingren",
                        "type": "user"
                    },
                    "name": "Qingren Yao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:23:45.913Z",
                    "hidden": false
                },
                {
                    "_id": "68dbf49f4159d1f2418f980e",
                    "name": "Ming Jin",
                    "hidden": false
                },
                {
                    "_id": "68dbf49f4159d1f2418f980f",
                    "name": "Chengqi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dbf49f4159d1f2418f9810",
                    "name": "Chao-Han Huck Yang",
                    "hidden": false
                },
                {
                    "_id": "68dbf49f4159d1f2418f9811",
                    "name": "Jun Qi",
                    "hidden": false
                },
                {
                    "_id": "68dbf49f4159d1f2418f9812",
                    "name": "Shirui Pan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T07:07:13.000Z",
            "submittedOnDailyAt": "2025-10-01T09:27:29.602Z",
            "title": "Estimating Time Series Foundation Model Transferability via In-Context\n  Learning",
            "submittedOnDailyBy": {
                "_id": "636480adf2984121733e503d",
                "avatarUrl": "/avatars/bd5bcff68ca620f2ed05812abc65ac1f.svg",
                "isPro": false,
                "fullname": "Qingren Yao",
                "user": "Qingren",
                "type": "user"
            },
            "summary": "Time series foundation models (TSFMs) offer strong zero-shot forecasting via\nlarge-scale pre-training, yet fine-tuning remains critical for boosting\nperformance in domains with limited public data. With the growing number of\nTSFMs, efficiently identifying the best model for downstream fine-tuning\nbecomes increasingly challenging. In this work, we introduce TimeTic, a\ntransferability estimation framework that recasts model selection as an\nin-context-learning problem: given observations on known (source) datasets, it\npredicts how a TSFM will perform after fine-tuning on a downstream (target)\ndataset. TimeTic flexibly organizes the observed model-data relationships as\ncontextual information, allowing it to adapt seamlessly to various test-time\nscenarios. Leveraging the natural tabular structure formed by dataset\nmeta-features, model characteristics, and fine-tuned performance, we employ\ntabular foundation models to serve as in-context learners. We further introduce\na novel model characterization based on entropy evolution across model layers,\ncapturing embedding-space distinctions and enabling TimeTic to generalize\nacross arbitrary model sets. We establish a comprehensive benchmark for\ntransferability estimation including 10 datasets, 10 foundation models, and 3\nforecasting tasks. On this benchmark, TimeTic's estimation demonstrates strong\nalignment with actual fine-tuned performance for previously unseen datasets,\nachieving a mean rank correlation of approximately 0.6 and a 30% improvement\ncompared to using zero-shot performance as the transferability score.",
            "upvotes": 1,
            "discussionId": "68dbf49f4159d1f2418f9813",
            "ai_summary": "TimeTic is a transferability estimation framework that predicts the performance of time series foundation models after fine-tuning on unseen datasets, using tabular foundation models and entropy evolution for model characterization.",
            "ai_keywords": [
                "time series foundation models",
                "zero-shot forecasting",
                "fine-tuning",
                "transferability estimation",
                "in-context-learning",
                "contextual information",
                "tabular foundation models",
                "entropy evolution",
                "model characterization",
                "mean rank correlation"
            ]
        },
        "publishedAt": "2025-09-28T03:07:13.000Z",
        "title": "Estimating Time Series Foundation Model Transferability via In-Context\n  Learning",
        "summary": "Time series foundation models (TSFMs) offer strong zero-shot forecasting via\nlarge-scale pre-training, yet fine-tuning remains critical for boosting\nperformance in domains with limited public data. With the growing number of\nTSFMs, efficiently identifying the best model for downstream fine-tuning\nbecomes increasingly challenging. In this work, we introduce TimeTic, a\ntransferability estimation framework that recasts model selection as an\nin-context-learning problem: given observations on known (source) datasets, it\npredicts how a TSFM will perform after fine-tuning on a downstream (target)\ndataset. TimeTic flexibly organizes the observed model-data relationships as\ncontextual information, allowing it to adapt seamlessly to various test-time\nscenarios. Leveraging the natural tabular structure formed by dataset\nmeta-features, model characteristics, and fine-tuned performance, we employ\ntabular foundation models to serve as in-context learners. We further introduce\na novel model characterization based on entropy evolution across model layers,\ncapturing embedding-space distinctions and enabling TimeTic to generalize\nacross arbitrary model sets. We establish a comprehensive benchmark for\ntransferability estimation including 10 datasets, 10 foundation models, and 3\nforecasting tasks. On this benchmark, TimeTic's estimation demonstrates strong\nalignment with actual fine-tuned performance for previously unseen datasets,\nachieving a mean rank correlation of approximately 0.6 and a 30% improvement\ncompared to using zero-shot performance as the transferability score.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23695.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636480adf2984121733e503d",
            "avatarUrl": "/avatars/bd5bcff68ca620f2ed05812abc65ac1f.svg",
            "fullname": "Qingren Yao",
            "name": "Qingren",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 0
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.22889",
            "authors": [
                {
                    "_id": "68dc101c4159d1f2418f98b6",
                    "user": {
                        "_id": "68dc0fdce927b1b11ce08bea",
                        "avatarUrl": "/avatars/5bba8ded95a8bb10f92426a20fccff92.svg",
                        "isPro": false,
                        "fullname": "Federico Chinello",
                        "user": "chinefed",
                        "type": "user"
                    },
                    "name": "Federico Chinello",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-01T10:23:11.191Z",
                    "hidden": false
                },
                {
                    "_id": "68dc101c4159d1f2418f98b7",
                    "name": "Giacomo Boracchi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/68dc0fdce927b1b11ce08bea/4zUaqlST3BF9ljMZvmL-D.png",
                "https://cdn-uploads.huggingface.co/production/uploads/68dc0fdce927b1b11ce08bea/hP9R_cwpfo--7KPT_h4BV.png",
                "https://cdn-uploads.huggingface.co/production/uploads/68dc0fdce927b1b11ce08bea/SQvltnhGNi5CXwsh76E9Z.png"
            ],
            "publishedAt": "2025-09-26T20:13:00.000Z",
            "submittedOnDailyAt": "2025-10-01T11:46:03.838Z",
            "title": "Convolutional Set Transformer",
            "submittedOnDailyBy": {
                "_id": "68dc0fdce927b1b11ce08bea",
                "avatarUrl": "/avatars/5bba8ded95a8bb10f92426a20fccff92.svg",
                "isPro": false,
                "fullname": "Federico Chinello",
                "user": "chinefed",
                "type": "user"
            },
            "summary": "We introduce the Convolutional Set Transformer (CST), a novel neural\narchitecture designed to process image sets of arbitrary cardinality that are\nvisually heterogeneous yet share high-level semantics - such as a common\ncategory, scene, or concept. Existing set-input networks, e.g., Deep Sets and\nSet Transformer, are limited to vector inputs and cannot directly handle 3D\nimage tensors. As a result, they must be cascaded with a feature extractor,\ntypically a CNN, which encodes images into embeddings before the set-input\nnetwork can model inter-image relationships. In contrast, CST operates directly\non 3D image tensors, performing feature extraction and contextual modeling\nsimultaneously, thereby enabling synergies between the two processes. This\ndesign yields superior performance in tasks such as Set Classification and Set\nAnomaly Detection and further provides native compatibility with CNN\nexplainability methods such as Grad-CAM, unlike competing approaches that\nremain opaque. Finally, we show that CSTs can be pre-trained on large-scale\ndatasets and subsequently adapted to new domains and tasks through standard\nTransfer Learning schemes. To support further research, we release CST-15, a\nCST backbone pre-trained on ImageNet\n(https://github.com/chinefed/convolutional-set-transformer).",
            "upvotes": 1,
            "discussionId": "68dc101d4159d1f2418f98b8",
            "projectPage": "https://chinefed.github.io/convolutional-set-transformer/",
            "githubRepo": "https://github.com/chinefed/convolutional-set-transformer",
            "ai_summary": "The Convolutional Set Transformer (CST) processes image sets directly, combining feature extraction and contextual modeling for improved performance in set classification and anomaly detection, with compatibility for CNN explainability methods.",
            "ai_keywords": [
                "Convolutional Set Transformer",
                "CST",
                "set-input networks",
                "Deep Sets",
                "Set Transformer",
                "3D image tensors",
                "feature extraction",
                "contextual modeling",
                "Set Classification",
                "Set Anomaly Detection",
                "Grad-CAM",
                "Transfer Learning",
                "CST-15",
                "ImageNet"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-09-26T16:13:00.000Z",
        "title": "Convolutional Set Transformer",
        "summary": "We introduce the Convolutional Set Transformer (CST), a novel neural\narchitecture designed to process image sets of arbitrary cardinality that are\nvisually heterogeneous yet share high-level semantics - such as a common\ncategory, scene, or concept. Existing set-input networks, e.g., Deep Sets and\nSet Transformer, are limited to vector inputs and cannot directly handle 3D\nimage tensors. As a result, they must be cascaded with a feature extractor,\ntypically a CNN, which encodes images into embeddings before the set-input\nnetwork can model inter-image relationships. In contrast, CST operates directly\non 3D image tensors, performing feature extraction and contextual modeling\nsimultaneously, thereby enabling synergies between the two processes. This\ndesign yields superior performance in tasks such as Set Classification and Set\nAnomaly Detection and further provides native compatibility with CNN\nexplainability methods such as Grad-CAM, unlike competing approaches that\nremain opaque. Finally, we show that CSTs can be pre-trained on large-scale\ndatasets and subsequently adapted to new domains and tasks through standard\nTransfer Learning schemes. To support further research, we release CST-15, a\nCST backbone pre-trained on ImageNet\n(https://github.com/chinefed/convolutional-set-transformer).",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/68dc0fdce927b1b11ce08bea/4zUaqlST3BF9ljMZvmL-D.png",
            "https://cdn-uploads.huggingface.co/production/uploads/68dc0fdce927b1b11ce08bea/hP9R_cwpfo--7KPT_h4BV.png",
            "https://cdn-uploads.huggingface.co/production/uploads/68dc0fdce927b1b11ce08bea/SQvltnhGNi5CXwsh76E9Z.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22889.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68dc0fdce927b1b11ce08bea",
            "avatarUrl": "/avatars/5bba8ded95a8bb10f92426a20fccff92.svg",
            "fullname": "Federico Chinello",
            "name": "chinefed",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]