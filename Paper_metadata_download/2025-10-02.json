[
    {
        "paper": {
            "id": "2509.25454",
            "authors": [
                {
                    "_id": "68ddfdad6024653e8a3ed13a",
                    "user": {
                        "_id": "675e0d5cdd3e9eeed6954f5a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/7oMEoBmaFiCR9K2q9Z_7q.png",
                        "isPro": false,
                        "fullname": "Fang Wu",
                        "user": "fangwu97",
                        "type": "user"
                    },
                    "name": "Fang Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:54:54.837Z",
                    "hidden": false
                },
                {
                    "_id": "68ddfdad6024653e8a3ed13b",
                    "user": {
                        "_id": "65b8909c89eb3dfbe8d26780",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b8909c89eb3dfbe8d26780/aJExXsCN9oQOdd9bsQOEO.jpeg",
                        "isPro": false,
                        "fullname": "Weihao XUAN",
                        "user": "weihao1115",
                        "type": "user"
                    },
                    "name": "Weihao Xuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:54:58.316Z",
                    "hidden": false
                },
                {
                    "_id": "68ddfdad6024653e8a3ed13c",
                    "name": "Heli Qi",
                    "hidden": false
                },
                {
                    "_id": "68ddfdad6024653e8a3ed13d",
                    "name": "Ximing Lu",
                    "hidden": false
                },
                {
                    "_id": "68ddfdad6024653e8a3ed13e",
                    "name": "Aaron Tu",
                    "hidden": false
                },
                {
                    "_id": "68ddfdad6024653e8a3ed13f",
                    "name": "Li Erran Li",
                    "hidden": false
                },
                {
                    "_id": "68ddfdad6024653e8a3ed140",
                    "name": "Yejin Choi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T20:00:29.000Z",
            "submittedOnDailyAt": "2025-10-02T02:53:24.628Z",
            "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search",
            "submittedOnDailyBy": {
                "_id": "675e0d5cdd3e9eeed6954f5a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/7oMEoBmaFiCR9K2q9Z_7q.png",
                "isPro": false,
                "fullname": "Fang Wu",
                "user": "fangwu97",
                "type": "user"
            },
            "summary": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation.",
            "upvotes": 97,
            "discussionId": "68ddfdad6024653e8a3ed141",
            "ai_summary": "DeepSearch integrates Monte Carlo Tree Search into RLVR training to enhance exploration and credit assignment, achieving state-of-the-art performance with reduced computational cost.",
            "ai_keywords": [
                "RLVR",
                "Monte Carlo Tree Search",
                "training loop",
                "systematic exploration",
                "credit assignment",
                "global frontier selection",
                "entropy-based guidance",
                "adaptive replay buffer",
                "solution caching",
                "mathematical reasoning benchmarks"
            ],
            "organization": {
                "_id": "6112d84f8c2e1f4060908c9e",
                "name": "stanfordnlp",
                "fullname": "Stanford NLP",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1628624969199-6032802e1f993496bc14d9e3.png"
            }
        },
        "publishedAt": "2025-09-29T16:00:29.000Z",
        "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search",
        "summary": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25454.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "675e0d5cdd3e9eeed6954f5a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/7oMEoBmaFiCR9K2q9Z_7q.png",
            "fullname": "Fang Wu",
            "name": "fangwu97",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "6112d84f8c2e1f4060908c9e",
            "name": "stanfordnlp",
            "fullname": "Stanford NLP",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1628624969199-6032802e1f993496bc14d9e3.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.01051",
            "authors": [
                {
                    "_id": "68ddde806024653e8a3ed0a9",
                    "user": {
                        "_id": "65f5392c68b8e0cb3c9977a2",
                        "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
                        "isPro": false,
                        "fullname": "Zichen",
                        "user": "lkevinzc",
                        "type": "user"
                    },
                    "name": "Zichen Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T03:07:51.006Z",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0aa",
                    "name": "Anya Sims",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0ab",
                    "user": {
                        "_id": "6336a11331efcb5647ef32c8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6336a11331efcb5647ef32c8/mM0eC0mIV4W4W7-R-cleV.png",
                        "isPro": false,
                        "fullname": "Keyu Duan",
                        "user": "vermouthdky",
                        "type": "user"
                    },
                    "name": "Keyu Duan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:55:22.843Z",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0ac",
                    "user": {
                        "_id": "64e416dc54e18f390ef79ba4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5n01J00ZaVRrebsON8iYA.jpeg",
                        "isPro": true,
                        "fullname": "Changyu Chen",
                        "user": "Cameron-Chen",
                        "type": "user"
                    },
                    "name": "Changyu Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:55:16.488Z",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0ad",
                    "name": "Simon Yu",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0ae",
                    "user": {
                        "_id": "66129c7b50350afe76757262",
                        "avatarUrl": "/avatars/a2f4fac076b9d658a0d904ed54960f6f.svg",
                        "isPro": false,
                        "fullname": "Xiangxin Zhou",
                        "user": "zhouxiangxin",
                        "type": "user"
                    },
                    "name": "Xiangxin Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:55:20.132Z",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0af",
                    "name": "Haotian Xu",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0b0",
                    "name": "Shaopan Xiong",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0b1",
                    "user": {
                        "_id": "635e3a76106f984574c36409",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
                        "isPro": false,
                        "fullname": "Bo Liu",
                        "user": "Benjamin-eecs",
                        "type": "user"
                    },
                    "name": "Bo Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T03:07:48.166Z",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0b2",
                    "name": "Chenmien Tan",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0b3",
                    "name": "Chuen Yang Beh",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0b4",
                    "name": "Weixun Wang",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0b5",
                    "name": "Hao Zhu",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0b6",
                    "name": "Weiyan Shi",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0b7",
                    "name": "Diyi Yang",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0b8",
                    "name": "Michael Shieh",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0b9",
                    "name": "Yee Whye Teh",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0ba",
                    "name": "Wee Sun Lee",
                    "hidden": false
                },
                {
                    "_id": "68ddde806024653e8a3ed0bb",
                    "name": "Min Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T15:55:57.000Z",
            "submittedOnDailyAt": "2025-10-02T00:38:09.699Z",
            "title": "GEM: A Gym for Agentic LLMs",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The training paradigm for large language models (LLMs) is moving from static\ndatasets to experience-based learning, where agents acquire skills via\ninteracting with complex environments. To facilitate this transition we\nintroduce GEM (General Experience Maker), an open-source environment simulator\ndesigned for the age of LLMs. Analogous to OpenAI-Gym for traditional\nreinforcement learning (RL), GEM provides a standardized framework for the\nenvironment-agent interface, including asynchronous vectorized execution for\nhigh throughput, and flexible wrappers for easy extensibility. GEM also\nfeatures a diverse suite of environments, robust integrated tools, and\nsingle-file example scripts demonstrating using GEM with five popular RL\ntraining frameworks. Along with this, we also provide a set of baselines across\n24 environments using REINFORCE with Return Batch Normalization (ReBN), which\n-- unlike GRPO -- is compatible with the full RL setting of dense per-turn\nrewards and offers better credit assignment. We further conduct apple-to-apple\nbenchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings\nusing GEM to shed light on the algorithmic designs. Lastly, GEM also functions\nas a convenient evaluation toolkit besides a training environment. We hope this\nframework can help accelerate future agentic LLM research.",
            "upvotes": 52,
            "discussionId": "68ddde806024653e8a3ed0bc",
            "githubRepo": "https://github.com/axon-rl/gem",
            "ai_summary": "GEM, an open-source environment simulator, facilitates experience-based learning for large language models by providing a standardized framework and diverse environments for training and benchmarking reinforcement learning algorithms.",
            "ai_keywords": [
                "large language models",
                "experience-based learning",
                "GEM",
                "environment simulator",
                "OpenAI-Gym",
                "asynchronous vectorized execution",
                "flexible wrappers",
                "REINFORCE",
                "Return Batch Normalization",
                "ReBN",
                "GRPO",
                "PPO",
                "benchmarking",
                "agentic LLM research"
            ],
            "githubStars": 151,
            "organization": {
                "_id": "61f4e841c771e23a1abb61ff",
                "name": "sail",
                "fullname": "Sea AI Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643440185801-5df833bdda6d0311fd3d5403.png"
            }
        },
        "publishedAt": "2025-10-01T11:55:57.000Z",
        "title": "GEM: A Gym for Agentic LLMs",
        "summary": "The training paradigm for large language models (LLMs) is moving from static\ndatasets to experience-based learning, where agents acquire skills via\ninteracting with complex environments. To facilitate this transition we\nintroduce GEM (General Experience Maker), an open-source environment simulator\ndesigned for the age of LLMs. Analogous to OpenAI-Gym for traditional\nreinforcement learning (RL), GEM provides a standardized framework for the\nenvironment-agent interface, including asynchronous vectorized execution for\nhigh throughput, and flexible wrappers for easy extensibility. GEM also\nfeatures a diverse suite of environments, robust integrated tools, and\nsingle-file example scripts demonstrating using GEM with five popular RL\ntraining frameworks. Along with this, we also provide a set of baselines across\n24 environments using REINFORCE with Return Batch Normalization (ReBN), which\n-- unlike GRPO -- is compatible with the full RL setting of dense per-turn\nrewards and offers better credit assignment. We further conduct apple-to-apple\nbenchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings\nusing GEM to shed light on the algorithmic designs. Lastly, GEM also functions\nas a convenient evaluation toolkit besides a training environment. We hope this\nframework can help accelerate future agentic LLM research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01051.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 116
        },
        "organization": {
            "_id": "61f4e841c771e23a1abb61ff",
            "name": "sail",
            "fullname": "Sea AI Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643440185801-5df833bdda6d0311fd3d5403.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.00406",
            "authors": [
                {
                    "_id": "68dddcb66024653e8a3ed095",
                    "name": "Hengtao Li",
                    "hidden": false
                },
                {
                    "_id": "68dddcb66024653e8a3ed096",
                    "name": "Pengxiang Ding",
                    "hidden": false
                },
                {
                    "_id": "68dddcb66024653e8a3ed097",
                    "name": "Runze Suo",
                    "hidden": false
                },
                {
                    "_id": "68dddcb66024653e8a3ed098",
                    "user": {
                        "_id": "68c429ef60f075cc46cc9cff",
                        "avatarUrl": "/avatars/21d277a1b46eabc967121a111e270cdd.svg",
                        "isPro": false,
                        "fullname": "yh-wang",
                        "user": "yh-wang",
                        "type": "user"
                    },
                    "name": "Yihao Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:55:32.098Z",
                    "hidden": false
                },
                {
                    "_id": "68dddcb66024653e8a3ed099",
                    "name": "Zirui Ge",
                    "hidden": false
                },
                {
                    "_id": "68dddcb66024653e8a3ed09a",
                    "name": "Dongyuan Zang",
                    "hidden": false
                },
                {
                    "_id": "68dddcb66024653e8a3ed09b",
                    "name": "Kexian Yu",
                    "hidden": false
                },
                {
                    "_id": "68dddcb66024653e8a3ed09c",
                    "name": "Mingyang Sun",
                    "hidden": false
                },
                {
                    "_id": "68dddcb66024653e8a3ed09d",
                    "name": "Hongyin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dddcb66024653e8a3ed09e",
                    "name": "Donglin Wang",
                    "hidden": false
                },
                {
                    "_id": "68dddcb66024653e8a3ed09f",
                    "name": "Weihua Su",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T01:33:10.000Z",
            "submittedOnDailyAt": "2025-10-02T00:30:41.593Z",
            "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified\n  Rewards in World Simulators",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Vision-Language-Action (VLA) models enable embodied decision-making but rely\nheavily on imitation learning, leading to compounding errors and poor\nrobustness under distribution shift. Reinforcement learning (RL) can mitigate\nthese issues yet typically demands costly real-world interactions or suffers\nfrom sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning\nframework that leverages a data-driven world model as a controllable simulator.\nTrained from real interaction data, the simulator predicts future visual\nobservations conditioned on actions, allowing policy rollouts with dense,\ntrajectory-level rewards derived from goal-achieving references. This design\ndelivers an efficient and action-aligned learning signal, drastically lowering\nsample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses\nstrong supervised baselines and achieves greater efficiency than\nsimulator-based RL. Moreover, it exhibits strong robustness under perturbed\nconditions, sustaining stable task execution. Our results establish\nworld-model-based RFT as a practical post-training paradigm to enhance the\ngeneralization and robustness of VLA models. For more details, please refer to\nhttps://vla-rft.github.io/.",
            "upvotes": 52,
            "discussionId": "68dddcb66024653e8a3ed0a0",
            "projectPage": "https://vla-rft.github.io/",
            "githubRepo": "https://github.com/OpenHelix-Team/VLA-RFT",
            "ai_summary": "VLA-RFT uses a data-driven world model to fine-tune VLA models efficiently, reducing sample requirements and improving robustness under perturbations.",
            "ai_keywords": [
                "reinforcement learning",
                "imitation learning",
                "distribution shift",
                "world model",
                "policy rollouts",
                "trajectory-level rewards",
                "goal-achieving references",
                "sample requirements",
                "generalization",
                "robustness"
            ],
            "githubStars": 23
        },
        "publishedAt": "2025-09-30T21:33:10.000Z",
        "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified\n  Rewards in World Simulators",
        "summary": "Vision-Language-Action (VLA) models enable embodied decision-making but rely\nheavily on imitation learning, leading to compounding errors and poor\nrobustness under distribution shift. Reinforcement learning (RL) can mitigate\nthese issues yet typically demands costly real-world interactions or suffers\nfrom sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning\nframework that leverages a data-driven world model as a controllable simulator.\nTrained from real interaction data, the simulator predicts future visual\nobservations conditioned on actions, allowing policy rollouts with dense,\ntrajectory-level rewards derived from goal-achieving references. This design\ndelivers an efficient and action-aligned learning signal, drastically lowering\nsample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses\nstrong supervised baselines and achieves greater efficiency than\nsimulator-based RL. Moreover, it exhibits strong robustness under perturbed\nconditions, sustaining stable task execution. Our results establish\nworld-model-based RFT as a practical post-training paradigm to enhance the\ngeneralization and robustness of VLA models. For more details, please refer to\nhttps://vla-rft.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00406.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 116
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25849",
            "authors": [
                {
                    "_id": "68de03246024653e8a3ed14f",
                    "name": "Ziniu Li",
                    "hidden": false
                },
                {
                    "_id": "68de03246024653e8a3ed150",
                    "name": "Congliang Chen",
                    "hidden": false
                },
                {
                    "_id": "68de03246024653e8a3ed151",
                    "name": "Tianyun Yang",
                    "hidden": false
                },
                {
                    "_id": "68de03246024653e8a3ed152",
                    "name": "Tian Ding",
                    "hidden": false
                },
                {
                    "_id": "68de03246024653e8a3ed153",
                    "name": "Ruoyu Sun",
                    "hidden": false
                },
                {
                    "_id": "68de03246024653e8a3ed154",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "68de03246024653e8a3ed155",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "68de03246024653e8a3ed156",
                    "name": "Zhi-Quan Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T06:41:57.000Z",
            "submittedOnDailyAt": "2025-10-02T03:25:18.339Z",
            "title": "Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget\n  Allocation",
            "submittedOnDailyBy": {
                "_id": "647c4c901f878439e2fd34d6",
                "avatarUrl": "/avatars/b4399d210d7239d4662b11a4ee7b527d.svg",
                "isPro": false,
                "fullname": "Ziniu Li",
                "user": "ziniuli",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) can self-improve through reinforcement learning,\nwhere they generate trajectories to explore and discover better solutions.\nHowever, this exploration process is computationally expensive, often forcing\ncurrent methods to assign limited exploration budgets to each task. This\nuniform allocation creates problematic edge cases: easy tasks consistently\nsucceed while difficult tasks consistently fail, both producing zero gradients\nduring training updates for the widely used Group Relative Policy Optimization\n(GRPO). We address this problem from the lens of exploration budget allocation.\nViewing each task's exploration as an \"item\" with a distinct \"value\" and\n\"cost\", we establish a connection to the classical knapsack problem. This\nformulation allows us to derive an optimal assignment rule that adaptively\ndistributes resources based on the model's current learning status. When\napplied to GRPO, our method increases the effective ratio of non-zero policy\ngradients by 20-40% during training. Acting as a computational \"free lunch\",\nour approach could reallocate exploration budgets from tasks where learning is\nsaturated to those where it is most impactful. This enables significantly\nlarger budgets (e.g., 93 rollouts) for especially challenging problems, which\nwould be computationally prohibitive under a uniform allocation. These\nimprovements translate to meaningful gains on mathematical reasoning\nbenchmarks, with average improvements of 2-4 points and peak gains of 9 points\non specific tasks. Notably, achieving comparable performance with traditional\nhomogeneous allocation would require about 2x the computational resources.",
            "upvotes": 31,
            "discussionId": "68de03256024653e8a3ed157",
            "ai_summary": "An adaptive exploration budget allocation method for reinforcement learning in Large Language Models improves training efficiency and performance on mathematical reasoning benchmarks.",
            "ai_keywords": [
                "reinforcement learning",
                "trajectories",
                "Group Relative Policy Optimization (GRPO)",
                "exploration budget allocation",
                "knapsack problem",
                "policy gradients",
                "computational resources",
                "mathematical reasoning benchmarks"
            ],
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-09-30T02:41:57.000Z",
        "title": "Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget\n  Allocation",
        "summary": "Large Language Models (LLMs) can self-improve through reinforcement learning,\nwhere they generate trajectories to explore and discover better solutions.\nHowever, this exploration process is computationally expensive, often forcing\ncurrent methods to assign limited exploration budgets to each task. This\nuniform allocation creates problematic edge cases: easy tasks consistently\nsucceed while difficult tasks consistently fail, both producing zero gradients\nduring training updates for the widely used Group Relative Policy Optimization\n(GRPO). We address this problem from the lens of exploration budget allocation.\nViewing each task's exploration as an \"item\" with a distinct \"value\" and\n\"cost\", we establish a connection to the classical knapsack problem. This\nformulation allows us to derive an optimal assignment rule that adaptively\ndistributes resources based on the model's current learning status. When\napplied to GRPO, our method increases the effective ratio of non-zero policy\ngradients by 20-40% during training. Acting as a computational \"free lunch\",\nour approach could reallocate exploration budgets from tasks where learning is\nsaturated to those where it is most impactful. This enables significantly\nlarger budgets (e.g., 93 rollouts) for especially challenging problems, which\nwould be computationally prohibitive under a uniform allocation. These\nimprovements translate to meaningful gains on mathematical reasoning\nbenchmarks, with average improvements of 2-4 points and peak gains of 9 points\non specific tasks. Notably, achieving comparable performance with traditional\nhomogeneous allocation would require about 2x the computational resources.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25849.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647c4c901f878439e2fd34d6",
            "avatarUrl": "/avatars/b4399d210d7239d4662b11a4ee7b527d.svg",
            "fullname": "Ziniu Li",
            "name": "ziniuli",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25455",
            "authors": [
                {
                    "_id": "68dd50e30898d7e69e16c9c7",
                    "user": {
                        "_id": "63b2e4fb922f26a27e75b763",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b2e4fb922f26a27e75b763/o0txunjDifS2LOvsnxDHD.png",
                        "isPro": false,
                        "fullname": "Alexander Kovrigin",
                        "user": "waleko",
                        "type": "user"
                    },
                    "name": "Alexander Kovrigin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T03:08:27.923Z",
                    "hidden": false
                },
                {
                    "_id": "68dd50e30898d7e69e16c9c8",
                    "user": {
                        "_id": "64a0539e7b57fab3a5d2905e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0539e7b57fab3a5d2905e/4gZo0l51m_ztFjPkBPGq0.jpeg",
                        "isPro": false,
                        "fullname": "Aleksandra Eliseeva",
                        "user": "saridormi",
                        "type": "user"
                    },
                    "name": "Aleksandra Eliseeva",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T03:08:23.716Z",
                    "hidden": false
                },
                {
                    "_id": "68dd50e30898d7e69e16c9c9",
                    "user": {
                        "_id": "6532abdb09179320406230e0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6532abdb09179320406230e0/fdjKr2lYnqP6RQECArkcl.jpeg",
                        "isPro": false,
                        "fullname": "Konstantin Grotov",
                        "user": "konstantgr",
                        "type": "user"
                    },
                    "name": "Konstantin Grotov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T03:08:31.102Z",
                    "hidden": false
                },
                {
                    "_id": "68dd50e30898d7e69e16c9ca",
                    "user": {
                        "_id": "64380bed961bb61e463bf93d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64380bed961bb61e463bf93d/zsel0Dzv1yU9O8zAxvCBw.jpeg",
                        "isPro": false,
                        "fullname": "Egor Bogomolov",
                        "user": "egor-bogomolov",
                        "type": "user"
                    },
                    "name": "Egor Bogomolov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:56:17.621Z",
                    "hidden": false
                },
                {
                    "_id": "68dd50e30898d7e69e16c9cb",
                    "name": "Yaroslav Zharov",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63b2e4fb922f26a27e75b763/e3UTEuQYvqf7R3_xYpZpC.png"
            ],
            "publishedAt": "2025-09-29T20:03:05.000Z",
            "submittedOnDailyAt": "2025-10-02T07:31:37.963Z",
            "title": "PIPer: On-Device Environment Setup via Online Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "63b2e4fb922f26a27e75b763",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b2e4fb922f26a27e75b763/o0txunjDifS2LOvsnxDHD.png",
                "isPro": false,
                "fullname": "Alexander Kovrigin",
                "user": "waleko",
                "type": "user"
            },
            "summary": "Environment setup-the process of configuring the system to work with a\nspecific software project-represents a persistent challenge in Software\nEngineering (SE). Automated environment setup methods could assist developers\nby providing fully configured environments for arbitrary repositories without\nmanual effort. This also helps SE researchers to scale execution-based\nbenchmarks. However, recent studies reveal that even state-of-the-art Large\nLanguage Models (LLMs) achieve limited success in automating this task. To\naddress this limitation, we tune a specialized model for environment setup. We\ncombine supervised fine-tuning for generating correct Bash scripts and\nReinforcement Learning with Verifiable Rewards (RLVR) to adapt it to the task\nof environment setup. On EnvBench-Python, our method enables Qwen3-8B (a model\nrunnable on consumer hardware) to perform on par with larger models-Qwen3-32B\nand GPT-4o. The training code and model checkpoints are available online:\nhttps://github.com/JetBrains-Research/PIPer.",
            "upvotes": 26,
            "discussionId": "68dd50e40898d7e69e16c9cc",
            "githubRepo": "https://github.com/JetBrains-Research/PIPer",
            "ai_summary": "A specialized model combining supervised fine-tuning and Reinforcement Learning with Verifiable Rewards achieves competitive performance in automated environment setup tasks.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "supervised fine-tuning",
                "Reinforcement Learning with Verifiable Rewards",
                "RLVR",
                "Bash scripts",
                "EnvBench-Python",
                "Qwen3-8B",
                "Qwen3-32B",
                "GPT-4o"
            ],
            "githubStars": 5,
            "organization": {
                "_id": "64665ffb326128fd2c6b2708",
                "name": "JetBrains-Research",
                "fullname": "JetBrains Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6457bd2cf9b42c048d0f771d/Bz6WJPFu9MBOn7d7xQVai.png"
            }
        },
        "publishedAt": "2025-09-29T16:03:05.000Z",
        "title": "PIPer: On-Device Environment Setup via Online Reinforcement Learning",
        "summary": "Environment setup-the process of configuring the system to work with a\nspecific software project-represents a persistent challenge in Software\nEngineering (SE). Automated environment setup methods could assist developers\nby providing fully configured environments for arbitrary repositories without\nmanual effort. This also helps SE researchers to scale execution-based\nbenchmarks. However, recent studies reveal that even state-of-the-art Large\nLanguage Models (LLMs) achieve limited success in automating this task. To\naddress this limitation, we tune a specialized model for environment setup. We\ncombine supervised fine-tuning for generating correct Bash scripts and\nReinforcement Learning with Verifiable Rewards (RLVR) to adapt it to the task\nof environment setup. On EnvBench-Python, our method enables Qwen3-8B (a model\nrunnable on consumer hardware) to perform on par with larger models-Qwen3-32B\nand GPT-4o. The training code and model checkpoints are available online:\nhttps://github.com/JetBrains-Research/PIPer.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63b2e4fb922f26a27e75b763/e3UTEuQYvqf7R3_xYpZpC.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25455.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63b2e4fb922f26a27e75b763",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b2e4fb922f26a27e75b763/o0txunjDifS2LOvsnxDHD.png",
            "fullname": "Alexander Kovrigin",
            "name": "waleko",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "64665ffb326128fd2c6b2708",
            "name": "JetBrains-Research",
            "fullname": "JetBrains Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6457bd2cf9b42c048d0f771d/Bz6WJPFu9MBOn7d7xQVai.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.22944",
            "authors": [
                {
                    "_id": "68de4111a9129e507bf4a499",
                    "name": "Lorenz K. Müller",
                    "hidden": false
                },
                {
                    "_id": "68de4111a9129e507bf4a49a",
                    "user": {
                        "_id": "68b1e03b8aefe9d999b719f2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyPVTSmon49qmwELaMrnX.png",
                        "isPro": false,
                        "fullname": "Philippe Bich",
                        "user": "pbicho",
                        "type": "user"
                    },
                    "name": "Philippe Bich",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:54:24.817Z",
                    "hidden": false
                },
                {
                    "_id": "68de4111a9129e507bf4a49b",
                    "name": "Jiawei Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68de4111a9129e507bf4a49c",
                    "user": {
                        "_id": "6852cd91d1b6e201cd9c2446",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/btssGVVp_cFX7r_0wWV0y.png",
                        "isPro": false,
                        "fullname": "Celik",
                        "user": "storianc",
                        "type": "user"
                    },
                    "name": "Ahmet Çelik",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:54:21.246Z",
                    "hidden": false
                },
                {
                    "_id": "68de4111a9129e507bf4a49d",
                    "user": {
                        "_id": "6548f86f46ce83d797a00699",
                        "avatarUrl": "/avatars/6dc0af3ee1587749313997fc46718cad.svg",
                        "isPro": false,
                        "fullname": "Luca Benfenati",
                        "user": "lucabnf",
                        "type": "user"
                    },
                    "name": "Luca Benfenati",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:54:18.970Z",
                    "hidden": false
                },
                {
                    "_id": "68de4111a9129e507bf4a49e",
                    "name": "Lukas Cavigelli",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/68b1e03b8aefe9d999b719f2/P9DKglx_UKN6OIlYHPtf9.png"
            ],
            "publishedAt": "2025-09-26T21:22:54.000Z",
            "submittedOnDailyAt": "2025-10-02T13:03:17.302Z",
            "title": "SINQ: Sinkhorn-Normalized Quantization for Calibration-Free\n  Low-Precision LLM Weights",
            "submittedOnDailyBy": {
                "_id": "68b1e03b8aefe9d999b719f2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyPVTSmon49qmwELaMrnX.png",
                "isPro": false,
                "fullname": "Philippe Bich",
                "user": "pbicho",
                "type": "user"
            },
            "summary": "Post-training quantization has emerged as the most widely used strategy for\ndeploying large language models at low precision. Still, current methods show\nperplexity degradation at bit-widths less than or equal to 4, partly because\nrepresenting outliers causes precision issues in parameters that share the same\nscales as these outliers. This problem is especially pronounced for\ncalibration-free, uniform quantization methods. We introduce SINQ to augment\nexisting post-training quantizers with an additional second-axis scale factor\nand a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize\nper-row and per-column variances, thereby minimizing a novel per-matrix proxy\ntarget for quantization: the matrix imbalance. Our method has no interactions\nbetween layers and can be trivially applied to new architectures to quantize\nany linear layers. We evaluate our method on the Qwen3 model family and\nDeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against\nuncalibrated uniform quantization baselines and can be further enhanced by\ncombining it with calibration and non-uniform quantization levels. Code to\nreproduce the results of this work and to easily quantize models using SINQ is\navailable at https://github.com/huawei-csl/SINQ.",
            "upvotes": 21,
            "discussionId": "68de4111a9129e507bf4a49f",
            "projectPage": "https://github.com/huawei-csl/SINQ",
            "githubRepo": "https://github.com/huawei-csl/SINQ",
            "ai_summary": "SINQ enhances post-training quantization by introducing a second-axis scale factor and Sinkhorn-Knopp-style algorithm to minimize matrix imbalance, improving perplexity on large language models.",
            "ai_keywords": [
                "post-training quantization",
                "large language models",
                "perplexity",
                "calibration-free",
                "uniform quantization",
                "second-axis scale factor",
                "Sinkhorn-Knopp-style algorithm",
                "matrix imbalance",
                "Qwen3",
                "DeepSeek-V2.5",
                "WikiText2",
                "C4",
                "non-uniform quantization levels"
            ],
            "githubStars": 38,
            "organization": {
                "_id": "68dd34af7ffcb962c2e1c461",
                "name": "huawei-csl",
                "fullname": "HUAWEI Computing Systems Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6442ef61860f7a25bef0ea51/rkv-GMqP_NCzoQxXhsvuW.jpeg"
            }
        },
        "publishedAt": "2025-09-26T17:22:54.000Z",
        "title": "SINQ: Sinkhorn-Normalized Quantization for Calibration-Free\n  Low-Precision LLM Weights",
        "summary": "Post-training quantization has emerged as the most widely used strategy for\ndeploying large language models at low precision. Still, current methods show\nperplexity degradation at bit-widths less than or equal to 4, partly because\nrepresenting outliers causes precision issues in parameters that share the same\nscales as these outliers. This problem is especially pronounced for\ncalibration-free, uniform quantization methods. We introduce SINQ to augment\nexisting post-training quantizers with an additional second-axis scale factor\nand a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize\nper-row and per-column variances, thereby minimizing a novel per-matrix proxy\ntarget for quantization: the matrix imbalance. Our method has no interactions\nbetween layers and can be trivially applied to new architectures to quantize\nany linear layers. We evaluate our method on the Qwen3 model family and\nDeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against\nuncalibrated uniform quantization baselines and can be further enhanced by\ncombining it with calibration and non-uniform quantization levels. Code to\nreproduce the results of this work and to easily quantize models using SINQ is\navailable at https://github.com/huawei-csl/SINQ.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/68b1e03b8aefe9d999b719f2/P9DKglx_UKN6OIlYHPtf9.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22944.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "68b1e03b8aefe9d999b719f2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyPVTSmon49qmwELaMrnX.png",
            "fullname": "Philippe Bich",
            "name": "pbicho",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "68dd34af7ffcb962c2e1c461",
            "name": "huawei-csl",
            "fullname": "HUAWEI Computing Systems Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6442ef61860f7a25bef0ea51/rkv-GMqP_NCzoQxXhsvuW.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.01174",
            "authors": [
                {
                    "_id": "68dddb006024653e8a3ed075",
                    "user": {
                        "_id": "657669f2b4379e65a8c6d5cf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657669f2b4379e65a8c6d5cf/IkHvOEmQ2osPVlAjBK_Xz.png",
                        "isPro": false,
                        "fullname": "YanzheChen",
                        "user": "YanzheChen",
                        "type": "user"
                    },
                    "name": "Yanzhe Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T03:07:56.299Z",
                    "hidden": false
                },
                {
                    "_id": "68dddb006024653e8a3ed076",
                    "user": {
                        "_id": "64440be5af034cdfd69ca3a7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
                        "isPro": false,
                        "fullname": "Qinghong (Kevin) Lin",
                        "user": "KevinQHLin",
                        "type": "user"
                    },
                    "name": "Kevin Qinghong Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:55:35.043Z",
                    "hidden": false
                },
                {
                    "_id": "68dddb006024653e8a3ed077",
                    "name": "Mike Zheng Shou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T17:56:48.000Z",
            "submittedOnDailyAt": "2025-10-02T00:23:15.150Z",
            "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "While recent generative models advance pixel-space video synthesis, they\nremain limited in producing professional educational videos, which demand\ndisciplinary knowledge, precise visual structures, and coherent transitions,\nlimiting their applicability in educational scenarios. Intuitively, such\nrequirements are better addressed through the manipulation of a renderable\nenvironment, which can be explicitly controlled via logical commands (e.g.,\ncode). In this work, we propose Code2Video, a code-centric agent framework for\ngenerating educational videos via executable Python code. The framework\ncomprises three collaborative agents: (i) Planner, which structures lecture\ncontent into temporally coherent flows and prepares corresponding visual\nassets; (ii) Coder, which converts structured instructions into executable\nPython codes while incorporating scope-guided auto-fix to enhance efficiency;\nand (iii) Critic, which leverages vision-language models (VLM) with visual\nanchor prompts to refine spatial layout and ensure clarity. To support\nsystematic evaluation, we build MMMC, a benchmark of professionally produced,\ndiscipline-specific educational videos. We evaluate MMMC across diverse\ndimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and\nparticularly, TeachQuiz, a novel end-to-end metric that quantifies how well a\nVLM, after unlearning, can recover knowledge by watching the generated videos.\nOur results demonstrate the potential of Code2Video as a scalable,\ninterpretable, and controllable approach, achieving 40% improvement over direct\ncode generation and producing videos comparable to human-crafted tutorials. The\ncode and datasets are available at https://github.com/showlab/Code2Video.",
            "upvotes": 20,
            "discussionId": "68dddb006024653e8a3ed078",
            "projectPage": "https://showlab.github.io/Code2Video/",
            "githubRepo": "https://github.com/showlab/Code2Video",
            "ai_summary": "Code2Video generates educational videos using a code-centric agent framework, improving coherence and interpretability compared to direct code generation.",
            "ai_keywords": [
                "generative models",
                "pixel-space video synthesis",
                "renderable environment",
                "logical commands",
                "code-centric agent framework",
                "Planner",
                "Coder",
                "Critic",
                "vision-language models",
                "VLM",
                "visual anchor prompts",
                "MMMC",
                "TeachQuiz",
                "direct code generation",
                "human-crafted tutorials"
            ],
            "githubStars": 39,
            "organization": {
                "_id": "63a553c4ce5763e06f78669c",
                "name": "showlab",
                "fullname": "Show Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"
            }
        },
        "publishedAt": "2025-10-01T13:56:48.000Z",
        "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation",
        "summary": "While recent generative models advance pixel-space video synthesis, they\nremain limited in producing professional educational videos, which demand\ndisciplinary knowledge, precise visual structures, and coherent transitions,\nlimiting their applicability in educational scenarios. Intuitively, such\nrequirements are better addressed through the manipulation of a renderable\nenvironment, which can be explicitly controlled via logical commands (e.g.,\ncode). In this work, we propose Code2Video, a code-centric agent framework for\ngenerating educational videos via executable Python code. The framework\ncomprises three collaborative agents: (i) Planner, which structures lecture\ncontent into temporally coherent flows and prepares corresponding visual\nassets; (ii) Coder, which converts structured instructions into executable\nPython codes while incorporating scope-guided auto-fix to enhance efficiency;\nand (iii) Critic, which leverages vision-language models (VLM) with visual\nanchor prompts to refine spatial layout and ensure clarity. To support\nsystematic evaluation, we build MMMC, a benchmark of professionally produced,\ndiscipline-specific educational videos. We evaluate MMMC across diverse\ndimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and\nparticularly, TeachQuiz, a novel end-to-end metric that quantifies how well a\nVLM, after unlearning, can recover knowledge by watching the generated videos.\nOur results demonstrate the potential of Code2Video as a scalable,\ninterpretable, and controllable approach, achieving 40% improvement over direct\ncode generation and producing videos comparable to human-crafted tutorials. The\ncode and datasets are available at https://github.com/showlab/Code2Video.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01174.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 116
        },
        "organization": {
            "_id": "63a553c4ce5763e06f78669c",
            "name": "showlab",
            "fullname": "Show Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.00615",
            "authors": [
                {
                    "_id": "68de1bef6024653e8a3ed1b8",
                    "name": "Minki Kang",
                    "hidden": false
                },
                {
                    "_id": "68de1bef6024653e8a3ed1b9",
                    "name": "Wei-Ning Chen",
                    "hidden": false
                },
                {
                    "_id": "68de1bef6024653e8a3ed1ba",
                    "name": "Dongge Han",
                    "hidden": false
                },
                {
                    "_id": "68de1bef6024653e8a3ed1bb",
                    "name": "Huseyin A. Inan",
                    "hidden": false
                },
                {
                    "_id": "68de1bef6024653e8a3ed1bc",
                    "name": "Lukas Wutschitz",
                    "hidden": false
                },
                {
                    "_id": "68de1bef6024653e8a3ed1bd",
                    "name": "Yanzhi Chen",
                    "hidden": false
                },
                {
                    "_id": "68de1bef6024653e8a3ed1be",
                    "name": "Robert Sim",
                    "hidden": false
                },
                {
                    "_id": "68de1bef6024653e8a3ed1bf",
                    "name": "Saravan Rajmohan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T07:43:49.000Z",
            "submittedOnDailyAt": "2025-10-02T05:03:06.006Z",
            "title": "ACON: Optimizing Context Compression for Long-horizon LLM Agents",
            "submittedOnDailyBy": {
                "_id": "64b74920fe6a108d03fed767",
                "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
                "isPro": false,
                "fullname": "Minki Kang",
                "user": "Nardien",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are increasingly deployed as agents in dynamic,\nreal-world environments, where success requires both reasoning and effective\ntool use. A central challenge for agentic tasks is the growing context length,\nas agents must accumulate long histories of actions and observations. This\nexpansion raises costs and reduces efficiency in long-horizon tasks, yet prior\nwork on context compression has mostly focused on single-step tasks or narrow\napplications. We introduce Agent Context Optimization (ACON), a unified\nframework that optimally compresses both environment observations and\ninteraction histories into concise yet informative condensations. ACON\nleverages compression guideline optimization in natural language space: given\npaired trajectories where full context succeeds but compressed context fails,\ncapable LLMs analyze the causes of failure, and the compression guideline is\nupdated accordingly. Furthermore, we propose distilling the optimized LLM\ncompressor into smaller models to reduce the overhead of the additional module.\nExperiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON\nreduces memory usage by 26-54% (peak tokens) while largely preserving task\nperformance, preserves over 95% of accuracy when distilled into smaller\ncompressors, and enhances smaller LMs as long-horizon agents with up to 46%\nperformance improvement.",
            "upvotes": 20,
            "discussionId": "68de1bef6024653e8a3ed1c0",
            "ai_summary": "Agent Context Optimization (ACON) compresses context in large language models for efficient long-horizon tasks by analyzing failure cases and distilling the compressor into smaller models.",
            "ai_keywords": [
                "Agent Context Optimization",
                "ACON",
                "context compression",
                "environment observations",
                "interaction histories",
                "compression guideline optimization",
                "natural language space",
                "paired trajectories",
                "LLM compressor",
                "distillation",
                "long-horizon tasks",
                "memory usage",
                "task performance",
                "accuracy",
                "performance improvement"
            ],
            "organization": {
                "_id": "5e6485f787403103f9f1055e",
                "name": "microsoft",
                "fullname": "Microsoft",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
            }
        },
        "publishedAt": "2025-10-01T03:43:49.000Z",
        "title": "ACON: Optimizing Context Compression for Long-horizon LLM Agents",
        "summary": "Large language models (LLMs) are increasingly deployed as agents in dynamic,\nreal-world environments, where success requires both reasoning and effective\ntool use. A central challenge for agentic tasks is the growing context length,\nas agents must accumulate long histories of actions and observations. This\nexpansion raises costs and reduces efficiency in long-horizon tasks, yet prior\nwork on context compression has mostly focused on single-step tasks or narrow\napplications. We introduce Agent Context Optimization (ACON), a unified\nframework that optimally compresses both environment observations and\ninteraction histories into concise yet informative condensations. ACON\nleverages compression guideline optimization in natural language space: given\npaired trajectories where full context succeeds but compressed context fails,\ncapable LLMs analyze the causes of failure, and the compression guideline is\nupdated accordingly. Furthermore, we propose distilling the optimized LLM\ncompressor into smaller models to reduce the overhead of the additional module.\nExperiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON\nreduces memory usage by 26-54% (peak tokens) while largely preserving task\nperformance, preserves over 95% of accuracy when distilled into smaller\ncompressors, and enhances smaller LMs as long-horizon agents with up to 46%\nperformance improvement.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00615.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b74920fe6a108d03fed767",
            "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
            "fullname": "Minki Kang",
            "name": "Nardien",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "organization": {
            "_id": "5e6485f787403103f9f1055e",
            "name": "microsoft",
            "fullname": "Microsoft",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.00977",
            "authors": [
                {
                    "_id": "68de7e1b70ada21878c750a1",
                    "name": "Yihong Wu",
                    "hidden": false
                },
                {
                    "_id": "68de7e1b70ada21878c750a2",
                    "name": "Liheng Ma",
                    "hidden": false
                },
                {
                    "_id": "68de7e1b70ada21878c750a3",
                    "name": "Lei Ding",
                    "hidden": false
                },
                {
                    "_id": "68de7e1b70ada21878c750a4",
                    "name": "Muzhi Li",
                    "hidden": false
                },
                {
                    "_id": "68de7e1b70ada21878c750a5",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68de7e1b70ada21878c750a6",
                    "name": "Kejia Chen",
                    "hidden": false
                },
                {
                    "_id": "68de7e1b70ada21878c750a7",
                    "name": "Zhan Su",
                    "hidden": false
                },
                {
                    "_id": "68de7e1b70ada21878c750a8",
                    "name": "Zhanguang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68de7e1b70ada21878c750a9",
                    "name": "Chenyang Huang",
                    "hidden": false
                },
                {
                    "_id": "68de7e1b70ada21878c750aa",
                    "name": "Yingxue Zhang",
                    "hidden": false
                },
                {
                    "_id": "68de7e1b70ada21878c750ab",
                    "name": "Mark Coates",
                    "hidden": false
                },
                {
                    "_id": "68de7e1b70ada21878c750ac",
                    "name": "Jian-Yun Nie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T14:52:11.000Z",
            "submittedOnDailyAt": "2025-10-02T12:02:06.427Z",
            "title": "It Takes Two: Your GRPO Is Secretly DPO",
            "submittedOnDailyBy": {
                "_id": "65e1ff68602ea7063c12bb4e",
                "avatarUrl": "/avatars/6e9e0ac4d3879cb78e67abf15069b46a.svg",
                "isPro": false,
                "fullname": "Yihong Wu",
                "user": "Yihong7788",
                "type": "user"
            },
            "summary": "Group Relative Policy Optimization (GRPO) is a prominent reinforcement\nlearning algorithm for post-training Large Language Models (LLMs). It is\ncommonly believed that GRPO necessitates a large group size to ensure stable\ntraining via precise statistical estimation, which incurs substantial\ncomputational overhead. In this work, we challenge this assumption by reframing\nGRPO as a form of contrastive learning, which reveals a fundamental connection\nto Direct Preference Optimization (DPO). Motivated by DPO's empirical success,\nwe investigate the minimal two-rollout case (2-GRPO), a configuration\npreviously deemed infeasible. We provide a rigorous theoretical analysis to\nvalidate 2-GRPO and demonstrate empirically that it achieves performance on par\nwith 16-GRPO, despite using only 1/8 of the rollouts and reducing training time\nby over 70%.",
            "upvotes": 17,
            "discussionId": "68de7e1b70ada21878c750ad",
            "ai_summary": "Reframing Group Relative Policy Optimization as contrastive learning reveals its connection to Direct Preference Optimization, enabling minimal two-rollout GRPO to achieve performance comparable to larger group sizes with reduced computational cost.",
            "ai_keywords": [
                "reinforcement learning",
                "Large Language Models",
                "Group Relative Policy Optimization",
                "contrastive learning",
                "Direct Preference Optimization",
                "rollouts"
            ]
        },
        "publishedAt": "2025-10-01T10:52:11.000Z",
        "title": "It Takes Two: Your GRPO Is Secretly DPO",
        "summary": "Group Relative Policy Optimization (GRPO) is a prominent reinforcement\nlearning algorithm for post-training Large Language Models (LLMs). It is\ncommonly believed that GRPO necessitates a large group size to ensure stable\ntraining via precise statistical estimation, which incurs substantial\ncomputational overhead. In this work, we challenge this assumption by reframing\nGRPO as a form of contrastive learning, which reveals a fundamental connection\nto Direct Preference Optimization (DPO). Motivated by DPO's empirical success,\nwe investigate the minimal two-rollout case (2-GRPO), a configuration\npreviously deemed infeasible. We provide a rigorous theoretical analysis to\nvalidate 2-GRPO and demonstrate empirically that it achieves performance on par\nwith 16-GRPO, despite using only 1/8 of the rollouts and reducing training time\nby over 70%.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00977.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e1ff68602ea7063c12bb4e",
            "avatarUrl": "/avatars/6e9e0ac4d3879cb78e67abf15069b46a.svg",
            "fullname": "Yihong Wu",
            "name": "Yihong7788",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.00232",
            "authors": [
                {
                    "_id": "68ddd96f6024653e8a3ed06d",
                    "user": {
                        "_id": "6190ab805ca89a28e9f66873",
                        "avatarUrl": "/avatars/3c7ecc398fbf851acd2a132e947a92be.svg",
                        "isPro": false,
                        "fullname": "Xin Xu",
                        "user": "XinXuNLPer",
                        "type": "user"
                    },
                    "name": "Xin Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T03:07:58.427Z",
                    "hidden": false
                },
                {
                    "_id": "68ddd96f6024653e8a3ed06e",
                    "name": "Xunzhi He",
                    "hidden": false
                },
                {
                    "_id": "68ddd96f6024653e8a3ed06f",
                    "name": "Churan Zhi",
                    "hidden": false
                },
                {
                    "_id": "68ddd96f6024653e8a3ed070",
                    "name": "Ruizhe Chen",
                    "hidden": false
                },
                {
                    "_id": "68ddd96f6024653e8a3ed071",
                    "name": "Julian McAuley",
                    "hidden": false
                },
                {
                    "_id": "68ddd96f6024653e8a3ed072",
                    "name": "Zexue He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T19:56:54.000Z",
            "submittedOnDailyAt": "2025-10-02T00:22:25.710Z",
            "title": "BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model\n  Responses",
            "submittedOnDailyBy": {
                "_id": "6190ab805ca89a28e9f66873",
                "avatarUrl": "/avatars/3c7ecc398fbf851acd2a132e947a92be.svg",
                "isPro": false,
                "fullname": "Xin Xu",
                "user": "XinXuNLPer",
                "type": "user"
            },
            "summary": "Existing studies on bias mitigation methods for large language models (LLMs)\nuse diverse baselines and metrics to evaluate debiasing performance, leading to\ninconsistent comparisons among them. Moreover, their evaluations are mostly\nbased on the comparison between LLMs' probabilities of biased and unbiased\ncontexts, which ignores the gap between such evaluations and real-world use\ncases where users interact with LLMs by reading model responses and expect fair\nand safe outputs rather than LLMs' probabilities. To enable consistent\nevaluation across debiasing methods and bridge this gap, we introduce\nBiasFreeBench, an empirical benchmark that comprehensively compares eight\nmainstream bias mitigation techniques (covering four prompting-based and four\ntraining-based methods) on two test scenarios (multi-choice QA and open-ended\nmulti-turn QA) by reorganizing existing datasets into a unified query-response\nsetting. We further introduce a response-level metric, Bias-Free Score, to\nmeasure the extent to which LLM responses are fair, safe, and\nanti-stereotypical. Debiasing performances are systematically compared and\nanalyzed across key dimensions: the prompting vs. training paradigm, model\nsize, and generalization of different training strategies to unseen bias types.\nWe will publicly release our benchmark, aiming to establish a unified testbed\nfor bias mitigation research.",
            "upvotes": 13,
            "discussionId": "68ddd96f6024653e8a3ed073",
            "githubRepo": "https://github.com/xxupiano/BiasFreeBench",
            "ai_summary": "BiasFreeBench evaluates bias mitigation techniques in large language models using a unified benchmark and response-level metric to ensure fair and safe outputs in real-world scenarios.",
            "ai_keywords": [
                "large language models",
                "bias mitigation",
                "BiasFreeBench",
                "prompting-based methods",
                "training-based methods",
                "multi-choice QA",
                "open-ended multi-turn QA",
                "Bias-Free Score",
                "model size",
                "generalization"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-09-30T15:56:54.000Z",
        "title": "BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model\n  Responses",
        "summary": "Existing studies on bias mitigation methods for large language models (LLMs)\nuse diverse baselines and metrics to evaluate debiasing performance, leading to\ninconsistent comparisons among them. Moreover, their evaluations are mostly\nbased on the comparison between LLMs' probabilities of biased and unbiased\ncontexts, which ignores the gap between such evaluations and real-world use\ncases where users interact with LLMs by reading model responses and expect fair\nand safe outputs rather than LLMs' probabilities. To enable consistent\nevaluation across debiasing methods and bridge this gap, we introduce\nBiasFreeBench, an empirical benchmark that comprehensively compares eight\nmainstream bias mitigation techniques (covering four prompting-based and four\ntraining-based methods) on two test scenarios (multi-choice QA and open-ended\nmulti-turn QA) by reorganizing existing datasets into a unified query-response\nsetting. We further introduce a response-level metric, Bias-Free Score, to\nmeasure the extent to which LLM responses are fair, safe, and\nanti-stereotypical. Debiasing performances are systematically compared and\nanalyzed across key dimensions: the prompting vs. training paradigm, model\nsize, and generalization of different training strategies to unseen bias types.\nWe will publicly release our benchmark, aiming to establish a unified testbed\nfor bias mitigation research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00232.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6190ab805ca89a28e9f66873",
            "avatarUrl": "/avatars/3c7ecc398fbf851acd2a132e947a92be.svg",
            "fullname": "Xin Xu",
            "name": "XinXuNLPer",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.00184",
            "authors": [
                {
                    "_id": "68ddd5b56024653e8a3ed063",
                    "name": "Xiaoyan Bai",
                    "hidden": false
                },
                {
                    "_id": "68ddd5b56024653e8a3ed064",
                    "name": "Itamar Pres",
                    "hidden": false
                },
                {
                    "_id": "68ddd5b56024653e8a3ed065",
                    "user": {
                        "_id": "63081e15a670ed10f9d44229",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
                        "isPro": true,
                        "fullname": "Yuntian Deng",
                        "user": "yuntian-deng",
                        "type": "user"
                    },
                    "name": "Yuntian Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T03:08:01.692Z",
                    "hidden": false
                },
                {
                    "_id": "68ddd5b56024653e8a3ed066",
                    "name": "Chenhao Tan",
                    "hidden": false
                },
                {
                    "_id": "68ddd5b56024653e8a3ed067",
                    "name": "Stuart Shieber",
                    "hidden": false
                },
                {
                    "_id": "68ddd5b56024653e8a3ed068",
                    "name": "Fernanda Viégas",
                    "hidden": false
                },
                {
                    "_id": "68ddd5b56024653e8a3ed069",
                    "name": "Martin Wattenberg",
                    "hidden": false
                },
                {
                    "_id": "68ddd5b56024653e8a3ed06a",
                    "name": "Andrew Lee",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/Ey4NUP7APueLX50YzeHCp.png"
            ],
            "publishedAt": "2025-09-30T19:03:26.000Z",
            "submittedOnDailyAt": "2025-10-02T00:14:01.486Z",
            "title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls",
            "submittedOnDailyBy": {
                "_id": "63081e15a670ed10f9d44229",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
                "isPro": true,
                "fullname": "Yuntian Deng",
                "user": "yuntian-deng",
                "type": "user"
            },
            "summary": "Language models are increasingly capable, yet still fail at a seemingly\nsimple task of multi-digit multiplication. In this work, we study why, by\nreverse-engineering a model that successfully learns multiplication via\nimplicit chain-of-thought, and report three findings: (1) Evidence of\nlong-range structure: Logit attributions and linear probes indicate that the\nmodel encodes the necessary long-range dependencies for multi-digit\nmultiplication. (2) Mechanism: the model encodes long-range dependencies using\nattention to construct a directed acyclic graph to ``cache'' and ``retrieve''\npairwise partial products. (3) Geometry: the model implements partial products\nin attention heads by forming Minkowski sums between pairs of digits, and\ndigits are represented using a Fourier basis, both of which are intuitive and\nefficient representations that the standard fine-tuning model lacks. With these\ninsights, we revisit the learning dynamics of standard fine-tuning and find\nthat the model converges to a local optimum that lacks the required long-range\ndependencies. We further validate this understanding by introducing an\nauxiliary loss that predicts the ``running sum'' via a linear regression probe,\nwhich provides an inductive bias that enables the model to successfully learn\nmulti-digit multiplication. In summary, by reverse-engineering the mechanisms\nof an implicit chain-of-thought model we uncover a pitfall for learning\nlong-range dependencies in Transformers and provide an example of how the\ncorrect inductive bias can address this issue.",
            "upvotes": 13,
            "discussionId": "68ddd5b66024653e8a3ed06b",
            "githubRepo": "https://github.com/ajyl/icot",
            "ai_summary": "Reverse-engineering a model that learns multi-digit multiplication via implicit chain-of-thought reveals that it uses attention to encode long-range dependencies and represents partial products efficiently, insights that help address limitations in standard fine-tuning.",
            "ai_keywords": [
                "implicit chain-of-thought",
                "logit attributions",
                "linear probes",
                "long-range dependencies",
                "attention",
                "directed acyclic graph",
                "Minkowski sums",
                "Fourier basis",
                "learning dynamics",
                "standard fine-tuning",
                "local optimum",
                "auxiliary loss",
                "linear regression probe",
                "inductive bias"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-09-30T15:03:26.000Z",
        "title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls",
        "summary": "Language models are increasingly capable, yet still fail at a seemingly\nsimple task of multi-digit multiplication. In this work, we study why, by\nreverse-engineering a model that successfully learns multiplication via\nimplicit chain-of-thought, and report three findings: (1) Evidence of\nlong-range structure: Logit attributions and linear probes indicate that the\nmodel encodes the necessary long-range dependencies for multi-digit\nmultiplication. (2) Mechanism: the model encodes long-range dependencies using\nattention to construct a directed acyclic graph to ``cache'' and ``retrieve''\npairwise partial products. (3) Geometry: the model implements partial products\nin attention heads by forming Minkowski sums between pairs of digits, and\ndigits are represented using a Fourier basis, both of which are intuitive and\nefficient representations that the standard fine-tuning model lacks. With these\ninsights, we revisit the learning dynamics of standard fine-tuning and find\nthat the model converges to a local optimum that lacks the required long-range\ndependencies. We further validate this understanding by introducing an\nauxiliary loss that predicts the ``running sum'' via a linear regression probe,\nwhich provides an inductive bias that enables the model to successfully learn\nmulti-digit multiplication. In summary, by reverse-engineering the mechanisms\nof an implicit chain-of-thought model we uncover a pitfall for learning\nlong-range dependencies in Transformers and provide an example of how the\ncorrect inductive bias can address this issue.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/Ey4NUP7APueLX50YzeHCp.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00184.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63081e15a670ed10f9d44229",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
            "fullname": "Yuntian Deng",
            "name": "yuntian-deng",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 258
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.26346",
            "authors": [
                {
                    "_id": "68de90a170ada21878c750c2",
                    "name": "Keming Wu",
                    "hidden": false
                },
                {
                    "_id": "68de90a170ada21878c750c3",
                    "name": "Sicong Jiang",
                    "hidden": false
                },
                {
                    "_id": "68de90a170ada21878c750c4",
                    "name": "Max Ku",
                    "hidden": false
                },
                {
                    "_id": "68de90a170ada21878c750c5",
                    "name": "Ping Nie",
                    "hidden": false
                },
                {
                    "_id": "68de90a170ada21878c750c6",
                    "name": "Minghao Liu",
                    "hidden": false
                },
                {
                    "_id": "68de90a170ada21878c750c7",
                    "name": "Wenhu Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/cyunewV7oulaDvISk46os.png"
            ],
            "publishedAt": "2025-09-30T14:51:04.000Z",
            "submittedOnDailyAt": "2025-10-02T13:19:13.805Z",
            "title": "EditReward: A Human-Aligned Reward Model for Instruction-Guided Image\n  Editing",
            "submittedOnDailyBy": {
                "_id": "6313a86154e6e5d9f0f94e04",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                "isPro": false,
                "fullname": "Wenhu Chen",
                "user": "wenhu",
                "type": "user"
            },
            "summary": "Recently, we have witnessed great progress in image editing with natural\nlanguage instructions. Several closed-source models like GPT-Image-1, Seedream,\nand Google-Nano-Banana have shown highly promising progress. However, the\nopen-source models are still lagging. The main bottleneck is the lack of a\nreliable reward model to scale up high-quality synthetic training data. To\naddress this critical bottleneck, we built \\mname, trained with our new\nlarge-scale human preference dataset, meticulously annotated by trained experts\nfollowing a rigorous protocol containing over 200K preference pairs. \\mname\ndemonstrates superior alignment with human preferences in instruction-guided\nimage editing tasks. Experiments show that \\mname achieves state-of-the-art\nhuman correlation on established benchmarks such as GenAI-Bench, AURORA-Bench,\nImagenHub, and our new \\benchname, outperforming a wide range of VLM-as-judge\nmodels. Furthermore, we use \\mname to select a high-quality subset from the\nexisting noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected\nsubset, which shows significant improvement over training on the full set. This\ndemonstrates \\mname's ability to serve as a reward model to scale up\nhigh-quality training data for image editing. Furthermore, its strong alignment\nsuggests potential for advanced applications like reinforcement learning-based\npost-training and test-time scaling of image editing models. \\mname with its\ntraining dataset will be released to help the community build more high-quality\nimage editing training datasets.",
            "upvotes": 12,
            "discussionId": "68de90a270ada21878c750c8",
            "ai_summary": "A new reward model, trained with a large-scale human preference dataset, improves instruction-guided image editing by selecting high-quality training data and achieving state-of-the-art performance on benchmarks.",
            "ai_keywords": [
                "reward model",
                "human preference dataset",
                "instruction-guided image editing",
                "GenAI-Bench",
                "AURORA-Bench",
                "ImagenHub",
                "reinforcement learning",
                "post-training",
                "test-time scaling"
            ],
            "organization": {
                "_id": "6313a90017838d05194fd282",
                "name": "TIGER-Lab",
                "fullname": "TIGER-Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/Noi3Qq3RYz8Jdq6BaFteq.png"
            }
        },
        "publishedAt": "2025-09-30T10:51:04.000Z",
        "title": "EditReward: A Human-Aligned Reward Model for Instruction-Guided Image\n  Editing",
        "summary": "Recently, we have witnessed great progress in image editing with natural\nlanguage instructions. Several closed-source models like GPT-Image-1, Seedream,\nand Google-Nano-Banana have shown highly promising progress. However, the\nopen-source models are still lagging. The main bottleneck is the lack of a\nreliable reward model to scale up high-quality synthetic training data. To\naddress this critical bottleneck, we built \\mname, trained with our new\nlarge-scale human preference dataset, meticulously annotated by trained experts\nfollowing a rigorous protocol containing over 200K preference pairs. \\mname\ndemonstrates superior alignment with human preferences in instruction-guided\nimage editing tasks. Experiments show that \\mname achieves state-of-the-art\nhuman correlation on established benchmarks such as GenAI-Bench, AURORA-Bench,\nImagenHub, and our new \\benchname, outperforming a wide range of VLM-as-judge\nmodels. Furthermore, we use \\mname to select a high-quality subset from the\nexisting noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected\nsubset, which shows significant improvement over training on the full set. This\ndemonstrates \\mname's ability to serve as a reward model to scale up\nhigh-quality training data for image editing. Furthermore, its strong alignment\nsuggests potential for advanced applications like reinforcement learning-based\npost-training and test-time scaling of image editing models. \\mname with its\ntraining dataset will be released to help the community build more high-quality\nimage editing training datasets.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/cyunewV7oulaDvISk46os.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26346.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "fullname": "Wenhu Chen",
            "name": "wenhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 44
        },
        "organization": {
            "_id": "6313a90017838d05194fd282",
            "name": "TIGER-Lab",
            "fullname": "TIGER-Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/Noi3Qq3RYz8Jdq6BaFteq.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01180",
            "authors": [
                {
                    "_id": "68de14816024653e8a3ed195",
                    "name": "Jian Hu",
                    "hidden": false
                },
                {
                    "_id": "68de14816024653e8a3ed196",
                    "name": "Mingjie Liu",
                    "hidden": false
                },
                {
                    "_id": "68de14816024653e8a3ed197",
                    "name": "Ximing Lu",
                    "hidden": false
                },
                {
                    "_id": "68de14816024653e8a3ed198",
                    "name": "Fang Wu",
                    "hidden": false
                },
                {
                    "_id": "68de14816024653e8a3ed199",
                    "name": "Zaid Harchaoui",
                    "hidden": false
                },
                {
                    "_id": "68de14816024653e8a3ed19a",
                    "name": "Shizhe Diao",
                    "hidden": false
                },
                {
                    "_id": "68de14816024653e8a3ed19b",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "68de14816024653e8a3ed19c",
                    "name": "Pavlo Molchanov",
                    "hidden": false
                },
                {
                    "_id": "68de14816024653e8a3ed19d",
                    "name": "Jun Yang",
                    "hidden": false
                },
                {
                    "_id": "68de14816024653e8a3ed19e",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "68de14816024653e8a3ed19f",
                    "name": "Yi Dong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T17:59:02.000Z",
            "submittedOnDailyAt": "2025-10-02T04:28:57.646Z",
            "title": "BroRL: Scaling Reinforcement Learning via Broadened Exploration",
            "submittedOnDailyBy": {
                "_id": "633bd54b00732349209a18fe",
                "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
                "isPro": false,
                "fullname": "Shizhe Diao",
                "user": "shizhediao",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\ningredient for unlocking complex reasoning capabilities in large language\nmodels. Recent work ProRL has shown promise in scaling RL by increasing the\nnumber of training steps. However, performance plateaus after thousands of\nsteps, with clear diminishing returns from allocating more computation to\nadditional training. In this work, we investigate a complementary paradigm for\nscaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to\nexhaustively Broaden exploration, which yields continuous performance gains\nbeyond the saturation point observed in ProRL when scaling the number of\ntraining steps. Our approach is motivated by a mass balance equation analysis\nallowing us to characterize the rate of change in probability mass for correct\nand incorrect tokens during the reinforcement process. We show that under a\none-step RL assumption, sampled rollout tokens always contribute to\ncorrect-mass expansion, while unsampled tokens outside rollouts may lead to\ngains or losses depending on their distribution and the net reward balance.\nImportantly, as the number of rollouts per example N increases, the effect of\nunsampled terms diminishes, ensuring overall correct-mass expansion. To\nvalidate our theoretical analysis, we conduct simulations under more relaxed\nconditions and find that a sufficiently large rollout size N-corresponding to\nample exploration-guarantees an increase in the probability mass of all correct\ntokens. Empirically, BroRL revives models saturated after 3K ProRL training\nsteps and demonstrates robust, continuous improvement, achieving\nstate-of-the-art results for the 1.5B model across diverse benchmarks.",
            "upvotes": 10,
            "discussionId": "68de14816024653e8a3ed1a0",
            "ai_summary": "BroRL enhances reinforcement learning by increasing rollouts per example, overcoming performance plateaus and achieving state-of-the-art results in large language models.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards",
                "RLVR",
                "ProRL",
                "rollouts",
                "exploration",
                "mass balance equation",
                "probability mass",
                "correct tokens",
                "incorrect tokens",
                "one-step RL",
                "net reward balance",
                "correct-mass expansion",
                "unsampled terms"
            ],
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-10-01T13:59:02.000Z",
        "title": "BroRL: Scaling Reinforcement Learning via Broadened Exploration",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\ningredient for unlocking complex reasoning capabilities in large language\nmodels. Recent work ProRL has shown promise in scaling RL by increasing the\nnumber of training steps. However, performance plateaus after thousands of\nsteps, with clear diminishing returns from allocating more computation to\nadditional training. In this work, we investigate a complementary paradigm for\nscaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to\nexhaustively Broaden exploration, which yields continuous performance gains\nbeyond the saturation point observed in ProRL when scaling the number of\ntraining steps. Our approach is motivated by a mass balance equation analysis\nallowing us to characterize the rate of change in probability mass for correct\nand incorrect tokens during the reinforcement process. We show that under a\none-step RL assumption, sampled rollout tokens always contribute to\ncorrect-mass expansion, while unsampled tokens outside rollouts may lead to\ngains or losses depending on their distribution and the net reward balance.\nImportantly, as the number of rollouts per example N increases, the effect of\nunsampled terms diminishes, ensuring overall correct-mass expansion. To\nvalidate our theoretical analysis, we conduct simulations under more relaxed\nconditions and find that a sufficiently large rollout size N-corresponding to\nample exploration-guarantees an increase in the probability mass of all correct\ntokens. Empirically, BroRL revives models saturated after 3K ProRL training\nsteps and demonstrates robust, continuous improvement, achieving\nstate-of-the-art results for the 1.5B model across diverse benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01180.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "633bd54b00732349209a18fe",
            "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
            "fullname": "Shizhe Diao",
            "name": "shizhediao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.00967",
            "authors": [
                {
                    "_id": "68de29c1a9129e507bf4a427",
                    "user": {
                        "_id": "64d476faf41bdb1e2f88416e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d476faf41bdb1e2f88416e/60Ax_bkHK8aF7bhS3kfDY.jpeg",
                        "isPro": false,
                        "fullname": "Cong Yu",
                        "user": "Benyucong",
                        "type": "user"
                    },
                    "name": "Cong Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:54:35.852Z",
                    "hidden": false
                },
                {
                    "_id": "68de29c1a9129e507bf4a428",
                    "name": "Valter Uotila",
                    "hidden": false
                },
                {
                    "_id": "68de29c1a9129e507bf4a429",
                    "user": {
                        "_id": "63dd3f4ea8877129a15a66d1",
                        "avatarUrl": "/avatars/5410233517f4bcfae827094dc1fb7bcf.svg",
                        "isPro": false,
                        "fullname": "SHILONG DENG",
                        "user": "zczlsde",
                        "type": "user"
                    },
                    "name": "Shilong Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:54:30.080Z",
                    "hidden": false
                },
                {
                    "_id": "68de29c1a9129e507bf4a42a",
                    "name": "Qingyuan Wu",
                    "hidden": false
                },
                {
                    "_id": "68de29c1a9129e507bf4a42b",
                    "name": "Tuo Shi",
                    "hidden": false
                },
                {
                    "_id": "68de29c1a9129e507bf4a42c",
                    "user": {
                        "_id": "661d8018ba7a3d451862aff2",
                        "avatarUrl": "/avatars/572c757e18dc11437ca1ffd0d1a9e9d1.svg",
                        "isPro": false,
                        "fullname": "Songlin Jiang",
                        "user": "HollowMan6",
                        "type": "user"
                    },
                    "name": "Songlin Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:54:32.885Z",
                    "hidden": false
                },
                {
                    "_id": "68de29c1a9129e507bf4a42d",
                    "name": "Lei You",
                    "hidden": false
                },
                {
                    "_id": "68de29c1a9129e507bf4a42e",
                    "name": "Bo Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T14:40:04.000Z",
            "submittedOnDailyAt": "2025-10-02T12:48:54.353Z",
            "title": "QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via\n  Agentic RL",
            "submittedOnDailyBy": {
                "_id": "64d476faf41bdb1e2f88416e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d476faf41bdb1e2f88416e/60Ax_bkHK8aF7bhS3kfDY.jpeg",
                "isPro": false,
                "fullname": "Cong Yu",
                "user": "Benyucong",
                "type": "user"
            },
            "summary": "Designing and optimizing task-specific quantum circuits are crucial to\nleverage the advantage of quantum computing. Recent large language model\n(LLM)-based quantum circuit generation has emerged as a promising automatic\nsolution. However, the fundamental challenges remain unaddressed: (i)\nparameterized quantum gates require precise numerical values for optimal\nperformance, which also depend on multiple aspects, including the number of\nquantum gates, their parameters, and the layout/depth of the circuits. (ii)\nLLMs often generate low-quality or incorrect quantum circuits due to the lack\nof quantum domain-specific knowledge. We propose QUASAR, an agentic\nreinforcement learning (RL) framework for quantum circuits generation and\noptimization based on tool-augmented LLMs. To align the LLM with\nquantum-specific knowledge and improve the generated quantum circuits, QUASAR\ndesigns (i) a quantum circuit verification approach with external quantum\nsimulators and (ii) a sophisticated hierarchical reward mechanism in RL\ntraining. Extensive evaluation shows improvements in both syntax and semantic\nperformance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR\nhas achieved the validity of 99.31% in Pass@1 and 100% in Pass@10,\noutperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several\nsupervised-fine-tuning (SFT)-only and RL-only baselines.",
            "upvotes": 10,
            "discussionId": "68de29c2a9129e507bf4a42f",
            "githubRepo": "https://github.com/benyucong/QUASAR",
            "ai_summary": "QUASAR, an RL framework using tool-augmented LLMs, improves quantum circuit generation and optimization through verification and hierarchical rewards, achieving high validity compared to industrial LLMs.",
            "ai_keywords": [
                "quantum circuits",
                "parameterized quantum gates",
                "large language model",
                "reinforcement learning",
                "quantum circuit verification",
                "hierarchical reward mechanism",
                "Pass@1",
                "Pass@10",
                "GPT-4o",
                "GPT-5",
                "DeepSeek-V3",
                "supervised-fine-tuning",
                "RL-only"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-10-01T10:40:04.000Z",
        "title": "QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via\n  Agentic RL",
        "summary": "Designing and optimizing task-specific quantum circuits are crucial to\nleverage the advantage of quantum computing. Recent large language model\n(LLM)-based quantum circuit generation has emerged as a promising automatic\nsolution. However, the fundamental challenges remain unaddressed: (i)\nparameterized quantum gates require precise numerical values for optimal\nperformance, which also depend on multiple aspects, including the number of\nquantum gates, their parameters, and the layout/depth of the circuits. (ii)\nLLMs often generate low-quality or incorrect quantum circuits due to the lack\nof quantum domain-specific knowledge. We propose QUASAR, an agentic\nreinforcement learning (RL) framework for quantum circuits generation and\noptimization based on tool-augmented LLMs. To align the LLM with\nquantum-specific knowledge and improve the generated quantum circuits, QUASAR\ndesigns (i) a quantum circuit verification approach with external quantum\nsimulators and (ii) a sophisticated hierarchical reward mechanism in RL\ntraining. Extensive evaluation shows improvements in both syntax and semantic\nperformance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR\nhas achieved the validity of 99.31% in Pass@1 and 100% in Pass@10,\noutperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several\nsupervised-fine-tuning (SFT)-only and RL-only baselines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00967.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d476faf41bdb1e2f88416e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d476faf41bdb1e2f88416e/60Ax_bkHK8aF7bhS3kfDY.jpeg",
            "fullname": "Cong Yu",
            "name": "Benyucong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25301",
            "authors": [
                {
                    "_id": "68ddf28d6024653e8a3ed0f0",
                    "name": "Tianrui Qin",
                    "hidden": false
                },
                {
                    "_id": "68ddf28d6024653e8a3ed0f1",
                    "name": "Qianben Chen",
                    "hidden": false
                },
                {
                    "_id": "68ddf28d6024653e8a3ed0f2",
                    "name": "Sinuo Wang",
                    "hidden": false
                },
                {
                    "_id": "68ddf28d6024653e8a3ed0f3",
                    "name": "He Xing",
                    "hidden": false
                },
                {
                    "_id": "68ddf28d6024653e8a3ed0f4",
                    "name": "King Zhu",
                    "hidden": false
                },
                {
                    "_id": "68ddf28d6024653e8a3ed0f5",
                    "name": "He Zhu",
                    "hidden": false
                },
                {
                    "_id": "68ddf28d6024653e8a3ed0f6",
                    "name": "Dingfeng Shi",
                    "hidden": false
                },
                {
                    "_id": "68ddf28d6024653e8a3ed0f7",
                    "name": "Xinxin Liu",
                    "hidden": false
                },
                {
                    "_id": "68ddf28d6024653e8a3ed0f8",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ddf28d6024653e8a3ed0f9",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68ddf28d6024653e8a3ed0fa",
                    "name": "Yuchen Eleanor Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ddf28d6024653e8a3ed0fb",
                    "name": "Xitong Gao",
                    "hidden": false
                },
                {
                    "_id": "68ddf28d6024653e8a3ed0fc",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:39:30.000Z",
            "submittedOnDailyAt": "2025-10-02T02:06:46.346Z",
            "title": "Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel\n  Execution",
            "submittedOnDailyBy": {
                "_id": "64301abe450c0de9a1d3d18e",
                "avatarUrl": "/avatars/01b284874dadc7d21d656c53dcb77e42.svg",
                "isPro": false,
                "fullname": "tianrui",
                "user": "tianyue818",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks when equipped with external tools. However, current\nframeworks predominantly rely on sequential processing, leading to inefficient\nexecution particularly for tasks requiring extensive tool interaction. This\npaper introduces Flash-Searcher, a novel parallel agent reasoning framework\nthat fundamentally reimagines the execution paradigm from sequential chains to\ndirected acyclic graphs (DAGs). Flash-Searcher decomposes complex tasks into\nsubtasks with explicit dependencies, enabling concurrent execution of\nindependent reasoning paths while maintaining logical constraints. Through\ndynamic workflow optimization, our framework continuously refines the execution\ngraph based on intermediate results, effectively integrating summary module.\nComprehensive evaluations across multiple benchmarks demonstrate that\nFlash-Searcher consistently outperforms existing approaches. Specifically, it\nachieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, while\nreducing agent execution steps by up to 35% compared to current frameworks.\nFurthermore, when distilling this parallel reasoning pipeline into single\nmodels, we observe substantial performance gains across diverse backbone\narchitectures, underscoring the generalizability of our methodology. Our work\nthus represents a significant advance in agent architecture design, offering a\nmore scalable and efficient paradigm for complex reasoning tasks.",
            "upvotes": 10,
            "discussionId": "68ddf28d6024653e8a3ed0fd",
            "githubRepo": "https://github.com/OPPO-PersonalAI/Flash-Searcher",
            "ai_summary": "Flash-Searcher, a parallel agent reasoning framework using directed acyclic graphs, enhances efficiency and performance in complex reasoning tasks by enabling concurrent execution and dynamic workflow optimization.",
            "ai_keywords": [
                "parallel agent reasoning framework",
                "directed acyclic graphs (DAGs)",
                "subtasks",
                "explicit dependencies",
                "concurrent execution",
                "dynamic workflow optimization",
                "summary module",
                "BrowseComp",
                "xbench-DeepSearch",
                "agent execution steps",
                "distilling",
                "backbone architectures"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "684a463d17db6e9f271a0b66",
                "name": "PersonalAILab",
                "fullname": "OPPO-Personal-AI-Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632bfaebea6e62428ab0e9c2/P5L4TzWg2d4NXntonI-fK.png"
            }
        },
        "publishedAt": "2025-09-29T13:39:30.000Z",
        "title": "Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel\n  Execution",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks when equipped with external tools. However, current\nframeworks predominantly rely on sequential processing, leading to inefficient\nexecution particularly for tasks requiring extensive tool interaction. This\npaper introduces Flash-Searcher, a novel parallel agent reasoning framework\nthat fundamentally reimagines the execution paradigm from sequential chains to\ndirected acyclic graphs (DAGs). Flash-Searcher decomposes complex tasks into\nsubtasks with explicit dependencies, enabling concurrent execution of\nindependent reasoning paths while maintaining logical constraints. Through\ndynamic workflow optimization, our framework continuously refines the execution\ngraph based on intermediate results, effectively integrating summary module.\nComprehensive evaluations across multiple benchmarks demonstrate that\nFlash-Searcher consistently outperforms existing approaches. Specifically, it\nachieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, while\nreducing agent execution steps by up to 35% compared to current frameworks.\nFurthermore, when distilling this parallel reasoning pipeline into single\nmodels, we observe substantial performance gains across diverse backbone\narchitectures, underscoring the generalizability of our methodology. Our work\nthus represents a significant advance in agent architecture design, offering a\nmore scalable and efficient paradigm for complex reasoning tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25301.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64301abe450c0de9a1d3d18e",
            "avatarUrl": "/avatars/01b284874dadc7d21d656c53dcb77e42.svg",
            "fullname": "tianrui",
            "name": "tianyue818",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "684a463d17db6e9f271a0b66",
            "name": "PersonalAILab",
            "fullname": "OPPO-Personal-AI-Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632bfaebea6e62428ab0e9c2/P5L4TzWg2d4NXntonI-fK.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.00931",
            "authors": [
                {
                    "_id": "68de08046024653e8a3ed169",
                    "user": {
                        "_id": "677cfa6cac2db4c2265edb26",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Kbi96ndfY-CIuJNd2TRZt.jpeg",
                        "isPro": false,
                        "fullname": "Ammar Khairi",
                        "user": "ammar-cohere",
                        "type": "user"
                    },
                    "name": "Ammar Khairi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:54:51.759Z",
                    "hidden": false
                },
                {
                    "_id": "68de08046024653e8a3ed16a",
                    "name": "Daniel D'souza",
                    "hidden": false
                },
                {
                    "_id": "68de08046024653e8a3ed16b",
                    "name": "Marzieh Fadaee",
                    "hidden": false
                },
                {
                    "_id": "68de08046024653e8a3ed16c",
                    "name": "Julia Kreutzer",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/677cfa6cac2db4c2265edb26/prGa1iEoIedoN0S_pMqXi.png"
            ],
            "publishedAt": "2025-10-01T14:14:31.000Z",
            "submittedOnDailyAt": "2025-10-02T03:37:41.156Z",
            "title": "Making, not Taking, the Best of N",
            "submittedOnDailyBy": {
                "_id": "677cfa6cac2db4c2265edb26",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Kbi96ndfY-CIuJNd2TRZt.jpeg",
                "isPro": false,
                "fullname": "Ammar Khairi",
                "user": "ammar-cohere",
                "type": "user"
            },
            "summary": "Obtaining high-quality generations in modern LLMs has largely been framed as\na selection problem: identifying a single winning generation from a diverse\npool of N samples, the Best-of-N (BoN). Yet, this approach is inherently\nzero-sum, discarding diverse and potentially useful information from the pool.\nInstead, we explore a collaborative setup, where all candidates can potentially\ncontribute to the final winning generation. To this end, we propose Fusion-of-N\n(FusioN): a method that uses a general LLM judge to synthesize the most\ninformative elements of each sample into a single final answer. We compare\nFusioN to BoN in two settings, (i) test-time scaling, where we sample and\naggregate from a single model at test-time (ii) synthetic data generation,\nwhere we fuse samples from a pool of diverse teachers to improve a student\nmodel. We extensively benchmark both setups across 11 languages, 3 diverse\ntasks and varying model scales. Across the bench, FusioN consistently\noutperforms BoN showing versatility and robustness both in test-time scaling\nand in downstream gains from synthetic data generation. We also perform\nextensive analysis on FusioN, where it shows surprising strengths and\nrobustness under challenging settings. These results show that we should shift\nhow we think about evaluating and utilizing LLM generations from a monolithic\nmeasure of quality, to embracing their polylithic nature. This shift allows us\nto integrate diverse strengths, unlock latent potential, and achieve\nimprovements that were previously inaccessible through selection alone.",
            "upvotes": 7,
            "discussionId": "68de08046024653e8a3ed16d",
            "ai_summary": "Fusion-of-N (FusioN) method improves LLM generation quality by synthesizing elements from multiple samples, outperforming Best-of-N in various settings and tasks.",
            "ai_keywords": [
                "Best-of-N",
                "Fusion-of-N",
                "LLM judge",
                "test-time scaling",
                "synthetic data generation",
                "diverse teachers",
                "student model",
                "polylithic nature"
            ],
            "organization": {
                "_id": "640ca1c93623f6a56ddab373",
                "name": "CohereLabs",
                "fullname": "Cohere Labs",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678549441248-5e70f6048ce3c604d78fe133.png"
            }
        },
        "publishedAt": "2025-10-01T10:14:31.000Z",
        "title": "Making, not Taking, the Best of N",
        "summary": "Obtaining high-quality generations in modern LLMs has largely been framed as\na selection problem: identifying a single winning generation from a diverse\npool of N samples, the Best-of-N (BoN). Yet, this approach is inherently\nzero-sum, discarding diverse and potentially useful information from the pool.\nInstead, we explore a collaborative setup, where all candidates can potentially\ncontribute to the final winning generation. To this end, we propose Fusion-of-N\n(FusioN): a method that uses a general LLM judge to synthesize the most\ninformative elements of each sample into a single final answer. We compare\nFusioN to BoN in two settings, (i) test-time scaling, where we sample and\naggregate from a single model at test-time (ii) synthetic data generation,\nwhere we fuse samples from a pool of diverse teachers to improve a student\nmodel. We extensively benchmark both setups across 11 languages, 3 diverse\ntasks and varying model scales. Across the bench, FusioN consistently\noutperforms BoN showing versatility and robustness both in test-time scaling\nand in downstream gains from synthetic data generation. We also perform\nextensive analysis on FusioN, where it shows surprising strengths and\nrobustness under challenging settings. These results show that we should shift\nhow we think about evaluating and utilizing LLM generations from a monolithic\nmeasure of quality, to embracing their polylithic nature. This shift allows us\nto integrate diverse strengths, unlock latent potential, and achieve\nimprovements that were previously inaccessible through selection alone.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/677cfa6cac2db4c2265edb26/prGa1iEoIedoN0S_pMqXi.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00931.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "677cfa6cac2db4c2265edb26",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Kbi96ndfY-CIuJNd2TRZt.jpeg",
            "fullname": "Ammar Khairi",
            "name": "ammar-cohere",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "640ca1c93623f6a56ddab373",
            "name": "CohereLabs",
            "fullname": "Cohere Labs",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678549441248-5e70f6048ce3c604d78fe133.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.00526",
            "authors": [
                {
                    "_id": "68dddc4c6024653e8a3ed083",
                    "name": "Gaotang Li",
                    "hidden": false
                },
                {
                    "_id": "68dddc4c6024653e8a3ed084",
                    "name": "Ruizhong Qiu",
                    "hidden": false
                },
                {
                    "_id": "68dddc4c6024653e8a3ed085",
                    "name": "Xiusi Chen",
                    "hidden": false
                },
                {
                    "_id": "68dddc4c6024653e8a3ed086",
                    "name": "Heng Ji",
                    "hidden": false
                },
                {
                    "_id": "68dddc4c6024653e8a3ed087",
                    "name": "Hanghang Tong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T05:17:47.000Z",
            "submittedOnDailyAt": "2025-10-02T00:33:45.370Z",
            "title": "Beyond Log Likelihood: Probability-Based Objectives for Supervised\n  Fine-Tuning across the Model Capability Continuum",
            "submittedOnDailyBy": {
                "_id": "654d784d71a30c4bca09a319",
                "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
                "isPro": false,
                "fullname": "Gaotang Li",
                "user": "gaotang",
                "type": "user"
            },
            "summary": "Supervised fine-tuning (SFT) is the standard approach for post-training large\nlanguage models (LLMs), yet it often shows limited generalization. We trace\nthis limitation to its default training objective: negative log likelihood\n(NLL). While NLL is classically optimal when training from scratch,\npost-training operates in a different paradigm and could violate its optimality\nassumptions, where models already encode task-relevant priors and supervision\ncan be long and noisy. To this end, we study a general family of\nprobability-based objectives and characterize their effectiveness under\ndifferent conditions. Through comprehensive experiments and extensive ablation\nstudies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a\ncritical dimension that governs objective behavior: the model-capability\ncontinuum. Near the model-strong end, prior-leaning objectives that downweight\nlow-probability tokens (e.g., -p, -p^{10}, thresholded variants)\nconsistently outperform NLL; toward the model-weak end, NLL dominates; in\nbetween, no single objective prevails. Our theoretical analysis further\nelucidates how objectives trade places across the continuum, providing a\nprincipled foundation for adapting objectives to model capability. Our code is\navailable at https://github.com/GaotangLi/Beyond-Log-Likelihood.",
            "upvotes": 7,
            "discussionId": "68dddc4c6024653e8a3ed088",
            "githubRepo": "https://github.com/GaotangLi/Beyond-Log-Likelihood",
            "ai_summary": "Research identifies probability-based objectives that outperform negative log likelihood for fine-tuning large language models, depending on model capability.",
            "ai_keywords": [
                "supervised fine-tuning",
                "large language models",
                "negative log likelihood",
                "probability-based objectives",
                "model-capability continuum",
                "prior-leaning objectives"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-10-01T01:17:47.000Z",
        "title": "Beyond Log Likelihood: Probability-Based Objectives for Supervised\n  Fine-Tuning across the Model Capability Continuum",
        "summary": "Supervised fine-tuning (SFT) is the standard approach for post-training large\nlanguage models (LLMs), yet it often shows limited generalization. We trace\nthis limitation to its default training objective: negative log likelihood\n(NLL). While NLL is classically optimal when training from scratch,\npost-training operates in a different paradigm and could violate its optimality\nassumptions, where models already encode task-relevant priors and supervision\ncan be long and noisy. To this end, we study a general family of\nprobability-based objectives and characterize their effectiveness under\ndifferent conditions. Through comprehensive experiments and extensive ablation\nstudies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a\ncritical dimension that governs objective behavior: the model-capability\ncontinuum. Near the model-strong end, prior-leaning objectives that downweight\nlow-probability tokens (e.g., -p, -p^{10}, thresholded variants)\nconsistently outperform NLL; toward the model-weak end, NLL dominates; in\nbetween, no single objective prevails. Our theoretical analysis further\nelucidates how objectives trade places across the continuum, providing a\nprincipled foundation for adapting objectives to model capability. Our code is\navailable at https://github.com/GaotangLi/Beyond-Log-Likelihood.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00526.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "654d784d71a30c4bca09a319",
            "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
            "fullname": "Gaotang Li",
            "name": "gaotang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.00553",
            "authors": [
                {
                    "_id": "68dddc7b6024653e8a3ed08a",
                    "name": "Yuchen Cai",
                    "hidden": false
                },
                {
                    "_id": "68dddc7b6024653e8a3ed08b",
                    "name": "Ding Cao",
                    "hidden": false
                },
                {
                    "_id": "68dddc7b6024653e8a3ed08c",
                    "user": {
                        "_id": "64e2d169d2af12910d682130",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e2d169d2af12910d682130/VG8UdqJCJGc0K4G0P0XQP.jpeg",
                        "isPro": false,
                        "fullname": "xuxin",
                        "user": "xx18",
                        "type": "user"
                    },
                    "name": "Xin Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T03:07:53.608Z",
                    "hidden": false
                },
                {
                    "_id": "68dddc7b6024653e8a3ed08d",
                    "name": "Zijun Yao",
                    "hidden": false
                },
                {
                    "_id": "68dddc7b6024653e8a3ed08e",
                    "name": "Yuqing Huang",
                    "hidden": false
                },
                {
                    "_id": "68dddc7b6024653e8a3ed08f",
                    "name": "Zhenyu Tan",
                    "hidden": false
                },
                {
                    "_id": "68dddc7b6024653e8a3ed090",
                    "name": "Benyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dddc7b6024653e8a3ed091",
                    "name": "Guiquan Liu",
                    "hidden": false
                },
                {
                    "_id": "68dddc7b6024653e8a3ed092",
                    "name": "Junfeng Fang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T06:13:50.000Z",
            "submittedOnDailyAt": "2025-10-02T00:30:18.583Z",
            "title": "On Predictability of Reinforcement Learning Dynamics for Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "64e2d169d2af12910d682130",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e2d169d2af12910d682130/VG8UdqJCJGc0K4G0P0XQP.jpeg",
                "isPro": false,
                "fullname": "xuxin",
                "user": "xx18",
                "type": "user"
            },
            "summary": "Recent advances in reasoning capabilities of large language models (LLMs) are\nlargely driven by reinforcement learning (RL), yet the underlying parameter\ndynamics during RL training remain poorly understood. This work identifies two\nfundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1\nDominance, where the top singular subspace of the parameter update matrix\nnearly fully determines reasoning improvements, recovering over 99\\% of\nperformance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace\nevolves linearly throughout training, enabling accurate prediction from early\ncheckpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the\ngeneralizability of these properties. More importantly, based on these\nfindings, we propose AlphaRL, a plug-in acceleration framework that\nextrapolates the final parameter update using a short early training window,\nachieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning\nperformance without extra modules or hyperparameter tuning. This positions our\nfinding as a versatile and practical tool for large-scale RL, opening a path\ntoward principled, interpretable, and efficient training paradigm for LLMs.",
            "upvotes": 6,
            "discussionId": "68dddc7b6024653e8a3ed093",
            "ai_summary": "Two fundamental properties of reinforcement learning-induced parameter updates in large language models are identified, leading to a plug-in acceleration framework that significantly speeds up training without sacrificing performance.",
            "ai_keywords": [
                "reinforcement learning",
                "parameter updates",
                "Rank-1 Dominance",
                "singular subspace",
                "Rank-1 Linear Dynamics",
                "AlphaRL",
                "large language models"
            ]
        },
        "publishedAt": "2025-10-01T02:13:50.000Z",
        "title": "On Predictability of Reinforcement Learning Dynamics for Large Language\n  Models",
        "summary": "Recent advances in reasoning capabilities of large language models (LLMs) are\nlargely driven by reinforcement learning (RL), yet the underlying parameter\ndynamics during RL training remain poorly understood. This work identifies two\nfundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1\nDominance, where the top singular subspace of the parameter update matrix\nnearly fully determines reasoning improvements, recovering over 99\\% of\nperformance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace\nevolves linearly throughout training, enabling accurate prediction from early\ncheckpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the\ngeneralizability of these properties. More importantly, based on these\nfindings, we propose AlphaRL, a plug-in acceleration framework that\nextrapolates the final parameter update using a short early training window,\nachieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning\nperformance without extra modules or hyperparameter tuning. This positions our\nfinding as a versatile and practical tool for large-scale RL, opening a path\ntoward principled, interpretable, and efficient training paradigm for LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00553.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e2d169d2af12910d682130",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e2d169d2af12910d682130/VG8UdqJCJGc0K4G0P0XQP.jpeg",
            "fullname": "xuxin",
            "name": "xx18",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.22887",
            "authors": [
                {
                    "_id": "68ddf9806024653e8a3ed0ff",
                    "name": "EunJeong Hwang",
                    "hidden": false
                },
                {
                    "_id": "68ddf9806024653e8a3ed100",
                    "user": {
                        "_id": "64510a21f800611f94f0d9f8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lOeHK9Bvt3IXcB7Urx6jZ.jpeg",
                        "isPro": false,
                        "fullname": "Yuwei Yin",
                        "user": "yuweiyin",
                        "type": "user"
                    },
                    "name": "Yuwei Yin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:55:06.761Z",
                    "hidden": false
                },
                {
                    "_id": "68ddf9806024653e8a3ed101",
                    "name": "Giuseppe Carenini",
                    "hidden": false
                },
                {
                    "_id": "68ddf9806024653e8a3ed102",
                    "name": "Peter West",
                    "hidden": false
                },
                {
                    "_id": "68ddf9806024653e8a3ed103",
                    "name": "Vered Shwartz",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T20:07:34.000Z",
            "submittedOnDailyAt": "2025-10-02T02:35:41.337Z",
            "title": "Infusing Theory of Mind into Socially Intelligent LLM Agents",
            "submittedOnDailyBy": {
                "_id": "63409e8e181f5648a58e0b41",
                "avatarUrl": "/avatars/9758d83fac022fc343b62145f859a4c6.svg",
                "isPro": false,
                "fullname": "EunJeong Hwang",
                "user": "ejhwang",
                "type": "user"
            },
            "summary": "Theory of Mind (ToM)-an understanding of the mental states of others-is a key\naspect of human social intelligence, yet, chatbots and LLM-based social agents\ndo not typically integrate it. In this work, we demonstrate that LLMs that\nexplicitly use ToM get better at dialogue, achieving goals more effectively.\nAfter showing that simply prompting models to generate mental states between\ndialogue turns already provides significant benefit, we further introduce\nToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM\nwith dialogue lookahead to produce mental states that are maximally useful for\nachieving dialogue goals. Experiments on the Sotopia interactive social\nevaluation benchmark demonstrate the effectiveness of our method over a range\nof baselines. Comprehensive analysis shows that ToMA exhibits more strategic,\ngoal-oriented reasoning behaviors, which enable long-horizon adaptation, while\nmaintaining better relationships with their partners. Our results suggest a\nstep forward in integrating ToM for building socially intelligent LLM agents.",
            "upvotes": 5,
            "discussionId": "68ddf9806024653e8a3ed104",
            "githubRepo": "https://github.com/eujhwang/toma",
            "ai_summary": "Integrating Theory of Mind into LLMs improves dialogue effectiveness and goal achievement by enabling strategic reasoning and better partner relationships.",
            "ai_keywords": [
                "Theory of Mind",
                "ToM",
                "dialogue lookahead",
                "Sotopia benchmark",
                "strategic reasoning",
                "long-horizon adaptation"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "67481e994bcfe97d0e7f242d",
                "name": "UBC-V",
                "fullname": "University of British Columbia",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66c75a8a060b6c3335d28ee7/5qUvu4_6J60ULUQaMYZ2C.png"
            }
        },
        "publishedAt": "2025-09-26T16:07:34.000Z",
        "title": "Infusing Theory of Mind into Socially Intelligent LLM Agents",
        "summary": "Theory of Mind (ToM)-an understanding of the mental states of others-is a key\naspect of human social intelligence, yet, chatbots and LLM-based social agents\ndo not typically integrate it. In this work, we demonstrate that LLMs that\nexplicitly use ToM get better at dialogue, achieving goals more effectively.\nAfter showing that simply prompting models to generate mental states between\ndialogue turns already provides significant benefit, we further introduce\nToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM\nwith dialogue lookahead to produce mental states that are maximally useful for\nachieving dialogue goals. Experiments on the Sotopia interactive social\nevaluation benchmark demonstrate the effectiveness of our method over a range\nof baselines. Comprehensive analysis shows that ToMA exhibits more strategic,\ngoal-oriented reasoning behaviors, which enable long-horizon adaptation, while\nmaintaining better relationships with their partners. Our results suggest a\nstep forward in integrating ToM for building socially intelligent LLM agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22887.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63409e8e181f5648a58e0b41",
            "avatarUrl": "/avatars/9758d83fac022fc343b62145f859a4c6.svg",
            "fullname": "EunJeong Hwang",
            "name": "ejhwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "67481e994bcfe97d0e7f242d",
            "name": "UBC-V",
            "fullname": "University of British Columbia",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66c75a8a060b6c3335d28ee7/5qUvu4_6J60ULUQaMYZ2C.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.00536",
            "authors": [
                {
                    "_id": "68ddddfc6024653e8a3ed0a2",
                    "name": "Kung-Hsiang Huang",
                    "hidden": false
                },
                {
                    "_id": "68ddddfc6024653e8a3ed0a3",
                    "name": "Haoyi Qiu",
                    "hidden": false
                },
                {
                    "_id": "68ddddfc6024653e8a3ed0a4",
                    "name": "Yutong Dai",
                    "hidden": false
                },
                {
                    "_id": "68ddddfc6024653e8a3ed0a5",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "68ddddfc6024653e8a3ed0a6",
                    "name": "Chien-Sheng Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T05:37:54.000Z",
            "submittedOnDailyAt": "2025-10-02T00:35:56.555Z",
            "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance.",
            "upvotes": 4,
            "discussionId": "68ddddfc6024653e8a3ed0a7",
            "ai_summary": "GUI-KV, a KV cache compression method for GUI agents, improves efficiency by exploiting spatial and temporal redundancies, reducing computational cost while maintaining accuracy.",
            "ai_keywords": [
                "vision-language models",
                "GUI agents",
                "attention patterns",
                "transformer layers",
                "attention sparsity",
                "KV cache compression",
                "spatial saliency guidance",
                "temporal redundancy scoring",
                "AgentNetBench benchmark",
                "decoding FLOPs",
                "step accuracy"
            ]
        },
        "publishedAt": "2025-10-01T01:37:54.000Z",
        "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness",
        "summary": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00536.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 116
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25531",
            "authors": [
                {
                    "_id": "68dd24dee795697646c0a134",
                    "name": "Huu Nguyen",
                    "hidden": false
                },
                {
                    "_id": "68dd24dee795697646c0a135",
                    "name": "Victor May",
                    "hidden": false
                },
                {
                    "_id": "68dd24dee795697646c0a136",
                    "name": "Harsh Raj",
                    "hidden": false
                },
                {
                    "_id": "68dd24dee795697646c0a137",
                    "name": "Marianna Nezhurina",
                    "hidden": false
                },
                {
                    "_id": "68dd24dee795697646c0a138",
                    "name": "Yishan Wang",
                    "hidden": false
                },
                {
                    "_id": "68dd24dee795697646c0a139",
                    "name": "Yanqi Luo",
                    "hidden": false
                },
                {
                    "_id": "68dd24dee795697646c0a13a",
                    "name": "Minh Chien Vu",
                    "hidden": false
                },
                {
                    "_id": "68dd24dee795697646c0a13b",
                    "user": {
                        "_id": "6308c49c454dc257521bc7f9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6308c49c454dc257521bc7f9/UWUS6OPa6OpVu1T0gd-wJ.jpeg",
                        "isPro": false,
                        "fullname": "Taishi",
                        "user": "Taishi-N324",
                        "type": "user"
                    },
                    "name": "Taishi Nakamura",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T03:08:37.727Z",
                    "hidden": false
                },
                {
                    "_id": "68dd24dee795697646c0a13c",
                    "name": "Ken Tsui",
                    "hidden": false
                },
                {
                    "_id": "68dd24dee795697646c0a13d",
                    "name": "Van Khue Nguyen",
                    "hidden": false
                },
                {
                    "_id": "68dd24dee795697646c0a13e",
                    "name": "David Salinas",
                    "hidden": false
                },
                {
                    "_id": "68dd24dee795697646c0a13f",
                    "name": "Aleksandra Krasnodębska",
                    "hidden": false
                },
                {
                    "_id": "68dd24dee795697646c0a140",
                    "name": "Christoph Schuhmann",
                    "hidden": false
                },
                {
                    "_id": "68dd24dee795697646c0a141",
                    "name": "Mats Leon Richter",
                    "hidden": false
                },
                {
                    "_id": "68dd24dee795697646c0a142",
                    "name": "Xuan-Son",
                    "hidden": false
                },
                {
                    "_id": "68dd24dee795697646c0a143",
                    "name": "Vu",
                    "hidden": false
                },
                {
                    "_id": "68dd24dee795697646c0a144",
                    "name": "Jenia Jitsev",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T21:40:10.000Z",
            "submittedOnDailyAt": "2025-10-02T16:37:41.016Z",
            "title": "MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality\n  Instruction and Reasoning Data Built from Permissive-First Text Sources",
            "submittedOnDailyBy": {
                "_id": "5fc6879e1c5ee87b1164876d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fc6879e1c5ee87b1164876d/Tjnm_lv0Bq0gPbFOTDH6E.jpeg",
                "isPro": false,
                "fullname": "Huu Nguyen",
                "user": "huu-ontocord",
                "type": "user"
            },
            "summary": "We present MixtureVitae, an open-access pretraining corpus built to minimize\nlegal risk while providing strong model performance. MixtureVitae follows a\nrisk-mitigated sourcing strategy that combines public-domain and permissively\nlicensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions\n(e.g., government works and EU TDM-eligible sources), alongside targeted\ninstruction, reasoning and synthetic data with documented provenance. We detail\na transparent, multi-stage pipeline for license-aware filtering, safety and\nquality screening, and domain-aware mixing, and we release the dataset and\ncuration recipes to support reproducible research. In controlled experiments\nusing the open-sci-ref training protocol (fixed architectures at\n130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens),\nmodels trained on MixtureVitae consistently outperform other permissive\ndatasets across a suite of standard benchmarks, and at the 1.7B/300B setting\nthey surpass FineWeb-Edu and approach DCLM in the later stages of training.\nPerformance is particularly strong on math/code and competitive on QA tasks.\nThese results demonstrate that permissive-first, risk-mitigated data provides a\npractical and legally mitigated foundation for training capable LLMs, reducing\nreliance on indiscriminate web scraping without sacrificing competitiveness.\nCode: https://github.com/ontocord/mixturevitae",
            "upvotes": 4,
            "discussionId": "68dd24dee795697646c0a145",
            "ai_summary": "MixtureVitae is a pretraining corpus that combines public-domain and permissively licensed text with low-risk additions, achieving strong model performance across benchmarks while minimizing legal risk.",
            "ai_keywords": [
                "pretraining corpus",
                "risk-mitigated sourcing strategy",
                "public-domain",
                "permissively licensed text",
                "CC-BY",
                "Apache",
                "government works",
                "EU TDM-eligible sources",
                "targeted instruction",
                "reasoning",
                "synthetic data",
                "license-aware filtering",
                "safety screening",
                "quality screening",
                "domain-aware mixing",
                "open-sci-ref training protocol",
                "LLMs",
                "FineWeb-Edu",
                "DCLM",
                "math/code",
                "QA tasks"
            ],
            "organization": {
                "_id": "65aad27a15102fd65991dc33",
                "name": "ontocord",
                "fullname": "Ontocord.AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5fc6879e1c5ee87b1164876d/v969EaBxjBJfyULkmQlyX.png"
            }
        },
        "publishedAt": "2025-09-29T17:40:10.000Z",
        "title": "MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality\n  Instruction and Reasoning Data Built from Permissive-First Text Sources",
        "summary": "We present MixtureVitae, an open-access pretraining corpus built to minimize\nlegal risk while providing strong model performance. MixtureVitae follows a\nrisk-mitigated sourcing strategy that combines public-domain and permissively\nlicensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions\n(e.g., government works and EU TDM-eligible sources), alongside targeted\ninstruction, reasoning and synthetic data with documented provenance. We detail\na transparent, multi-stage pipeline for license-aware filtering, safety and\nquality screening, and domain-aware mixing, and we release the dataset and\ncuration recipes to support reproducible research. In controlled experiments\nusing the open-sci-ref training protocol (fixed architectures at\n130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens),\nmodels trained on MixtureVitae consistently outperform other permissive\ndatasets across a suite of standard benchmarks, and at the 1.7B/300B setting\nthey surpass FineWeb-Edu and approach DCLM in the later stages of training.\nPerformance is particularly strong on math/code and competitive on QA tasks.\nThese results demonstrate that permissive-first, risk-mitigated data provides a\npractical and legally mitigated foundation for training capable LLMs, reducing\nreliance on indiscriminate web scraping without sacrificing competitiveness.\nCode: https://github.com/ontocord/mixturevitae",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25531.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "5fc6879e1c5ee87b1164876d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fc6879e1c5ee87b1164876d/Tjnm_lv0Bq0gPbFOTDH6E.jpeg",
            "fullname": "Huu Nguyen",
            "name": "huu-ontocord",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 49
        },
        "organization": {
            "_id": "65aad27a15102fd65991dc33",
            "name": "ontocord",
            "fullname": "Ontocord.AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5fc6879e1c5ee87b1164876d/v969EaBxjBJfyULkmQlyX.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.23250",
            "authors": [
                {
                    "_id": "68db7e0ad2bf1f4b15ec77ae",
                    "name": "Brandon Ong",
                    "hidden": false
                },
                {
                    "_id": "68db7e0ad2bf1f4b15ec77af",
                    "name": "Tej Deep Pala",
                    "hidden": false
                },
                {
                    "_id": "68db7e0ad2bf1f4b15ec77b0",
                    "name": "Vernon Toh",
                    "hidden": false
                },
                {
                    "_id": "68db7e0ad2bf1f4b15ec77b1",
                    "name": "William Chandra Tjhi",
                    "hidden": false
                },
                {
                    "_id": "68db7e0ad2bf1f4b15ec77b2",
                    "name": "Soujanya Poria",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-27T10:56:58.000Z",
            "submittedOnDailyAt": "2025-10-02T00:29:03.356Z",
            "title": "Training Vision-Language Process Reward Models for Test-Time Scaling in\n  Multimodal Reasoning: Key Insights and Lessons Learned",
            "submittedOnDailyBy": {
                "_id": "626b626405fe1cb65725aca1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/ZVSbhynzpQhVGq9kGywW6.png",
                "isPro": false,
                "fullname": "Soujanya Poria",
                "user": "soujanyaporia",
                "type": "user"
            },
            "summary": "Process Reward Models (PRMs) provide step-level supervision that improves the\nreliability of reasoning in large language models. While PRMs have been\nextensively studied in text-based domains, their extension to Vision Language\nModels (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on\nMonte Carlo Tree Search (MCTS) for data construction, which can often produce\nnoisy supervision signals and limit generalization across tasks. In this work,\nwe aim to elucidate the design space of VL-PRMs by exploring diverse strategies\nfor dataset construction, training, and test-time scaling. First, we introduce\na hybrid data synthesis framework that combines MCTS with judgments from a\nstrong VLM, producing more accurate step-level labels. Second, we propose\nperception-focused supervision, enabling our PRM to explicitly detect errors at\nthe visual grounding stage of reasoning. Third, we systematically evaluate\nmultiple test-time scaling strategies, showing that our PRMs can reliably guide\nVLMs toward more accurate solutions. Our experiments covering five diverse\nmultimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and\nMathVision) reveal several key insights: (i) VL-PRMs when used as Outcome\nReward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM\nguided process step selection, (ii) smaller VL-PRMs can match or even surpass\nlarger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning\nabilities in stronger VLM backbones, (iv) perception-level supervision leads to\nsignificant gains in test-time scaling, and (v) TTS performance of different\npolicies improve on advanced math reasoning datasets despite not training\nVL-PRMs on such datasets. We hope our work will motivate further research and\nsupport the advancement of VLMs.",
            "upvotes": 4,
            "discussionId": "68db7e0ad2bf1f4b15ec77b3",
            "githubRepo": "https://github.com/theogbrand/vlprm",
            "ai_summary": "Hybrid data synthesis and perception-focused supervision improve the reliability of Vision-Language Process Reward Models (VL-PRMs) in guiding VLMs across diverse multimodal benchmarks.",
            "ai_keywords": [
                "Process Reward Models",
                "Vision Language Models",
                "Monte Carlo Tree Search",
                "hybrid data synthesis",
                "perception-focused supervision",
                "Outcome Reward Models",
                "test-time scaling",
                "multimodal benchmarks",
                "visual grounding",
                "process errors",
                "latent reasoning abilities"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "626ab9dac804c432c1b27a48",
                "name": "declare-lab",
                "fullname": "Deep Cognition and Language Research (DeCLaRe) Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/grq3rj2uj0WRjjPjAtR1I.png"
            }
        },
        "publishedAt": "2025-09-27T06:56:58.000Z",
        "title": "Training Vision-Language Process Reward Models for Test-Time Scaling in\n  Multimodal Reasoning: Key Insights and Lessons Learned",
        "summary": "Process Reward Models (PRMs) provide step-level supervision that improves the\nreliability of reasoning in large language models. While PRMs have been\nextensively studied in text-based domains, their extension to Vision Language\nModels (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on\nMonte Carlo Tree Search (MCTS) for data construction, which can often produce\nnoisy supervision signals and limit generalization across tasks. In this work,\nwe aim to elucidate the design space of VL-PRMs by exploring diverse strategies\nfor dataset construction, training, and test-time scaling. First, we introduce\na hybrid data synthesis framework that combines MCTS with judgments from a\nstrong VLM, producing more accurate step-level labels. Second, we propose\nperception-focused supervision, enabling our PRM to explicitly detect errors at\nthe visual grounding stage of reasoning. Third, we systematically evaluate\nmultiple test-time scaling strategies, showing that our PRMs can reliably guide\nVLMs toward more accurate solutions. Our experiments covering five diverse\nmultimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and\nMathVision) reveal several key insights: (i) VL-PRMs when used as Outcome\nReward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM\nguided process step selection, (ii) smaller VL-PRMs can match or even surpass\nlarger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning\nabilities in stronger VLM backbones, (iv) perception-level supervision leads to\nsignificant gains in test-time scaling, and (v) TTS performance of different\npolicies improve on advanced math reasoning datasets despite not training\nVL-PRMs on such datasets. We hope our work will motivate further research and\nsupport the advancement of VLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23250.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "626b626405fe1cb65725aca1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/ZVSbhynzpQhVGq9kGywW6.png",
            "fullname": "Soujanya Poria",
            "name": "soujanyaporia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "626ab9dac804c432c1b27a48",
            "name": "declare-lab",
            "fullname": "Deep Cognition and Language Research (DeCLaRe) Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/grq3rj2uj0WRjjPjAtR1I.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01152",
            "authors": [
                {
                    "_id": "68decb1fdf49fb0df1e03a0c",
                    "name": "Mustafa Omer Gul",
                    "hidden": false
                },
                {
                    "_id": "68decb1fdf49fb0df1e03a0d",
                    "name": "Claire Cardie",
                    "hidden": false
                },
                {
                    "_id": "68decb1fdf49fb0df1e03a0e",
                    "name": "Tanya Goyal",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/650b7db9a27ba14323e74421/r8NcNOIZLNbi6Z1-7T1VM.png"
            ],
            "publishedAt": "2025-10-01T17:41:54.000Z",
            "submittedOnDailyAt": "2025-10-02T17:29:09.445Z",
            "title": "Pay-Per-Search Models are Abstention Models",
            "submittedOnDailyBy": {
                "_id": "650b7db9a27ba14323e74421",
                "avatarUrl": "/avatars/668a881a1bb394a2115fd8f34fe4e875.svg",
                "isPro": false,
                "fullname": "Tanya Goyal",
                "user": "tanyagoyal-p",
                "type": "user"
            },
            "summary": "LLMs cannot reliably recognize their parametric knowledge boundaries and\noften hallucinate answers to outside-of-boundary questions. In contrast, humans\nrecognize their limitations and can either seek external help for such\nquestions or abstain. In this paper, we introduce MASH (Modeling Abstention via\nSelective Help-seeking), a training framework that readily extracts abstentions\nfrom LLMs. Our key idea is that any external help-seeking by an LLM, i.e.\nsearch tool use, can serve as a proxy for abstention if the external help\n(search) is appropriately penalized while simultaneously rewarding answer\naccuracy. MASH operationalizes this idea using reinforcement learning with a\npay-per-search reward.\n  We run experiments on three knowledge-intensive QA datasets. Our results show\nthat MASH substantially improves upon the selective help-seeking performance of\nprior efficient search approaches; on multi-hop datasets, MASH improves answer\naccuracy by 7.6%. Furthermore, MASH demonstrates strong off-the-shelf\nabstention -- it can distinguish between unanswerable/answerable questions and\nselectively generate responses for answerable questions -- showcasing behavior\nanalogous to specialized abstention approaches. We emphasize that contrary to\nprior abstention methods, MASH does not require pre-determining knowledge\nboundaries to construct training data. Instead, MASH's abstentions are a\nby-product of training for the auxiliary selective help-seeking task. Overall,\nwe show that MASH training effectively aligns search tool use with parametric\nknowledge, which can be successfully leveraged for making abstention decisions.",
            "upvotes": 3,
            "discussionId": "68decb1fdf49fb0df1e03a0f",
            "ai_summary": "MASH, a reinforcement learning framework, improves LLMs' selective help-seeking and abstention capabilities without pre-determined knowledge boundaries.",
            "ai_keywords": [
                "LLMs",
                "parametric knowledge boundaries",
                "hallucination",
                "selective help-seeking",
                "reinforcement learning",
                "pay-per-search reward",
                "knowledge-intensive QA datasets",
                "multi-hop datasets",
                "answer accuracy",
                "off-the-shelf abstention",
                "specialized abstention approaches"
            ],
            "organization": {
                "_id": "681dd2e9a61bb228fae1702b",
                "name": "cornell",
                "fullname": "Cornell University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652303d0974423bd3ef70468/4ZbVAynBI2QThFWmlWE-b.png"
            }
        },
        "publishedAt": "2025-10-01T13:41:54.000Z",
        "title": "Pay-Per-Search Models are Abstention Models",
        "summary": "LLMs cannot reliably recognize their parametric knowledge boundaries and\noften hallucinate answers to outside-of-boundary questions. In contrast, humans\nrecognize their limitations and can either seek external help for such\nquestions or abstain. In this paper, we introduce MASH (Modeling Abstention via\nSelective Help-seeking), a training framework that readily extracts abstentions\nfrom LLMs. Our key idea is that any external help-seeking by an LLM, i.e.\nsearch tool use, can serve as a proxy for abstention if the external help\n(search) is appropriately penalized while simultaneously rewarding answer\naccuracy. MASH operationalizes this idea using reinforcement learning with a\npay-per-search reward.\n  We run experiments on three knowledge-intensive QA datasets. Our results show\nthat MASH substantially improves upon the selective help-seeking performance of\nprior efficient search approaches; on multi-hop datasets, MASH improves answer\naccuracy by 7.6%. Furthermore, MASH demonstrates strong off-the-shelf\nabstention -- it can distinguish between unanswerable/answerable questions and\nselectively generate responses for answerable questions -- showcasing behavior\nanalogous to specialized abstention approaches. We emphasize that contrary to\nprior abstention methods, MASH does not require pre-determining knowledge\nboundaries to construct training data. Instead, MASH's abstentions are a\nby-product of training for the auxiliary selective help-seeking task. Overall,\nwe show that MASH training effectively aligns search tool use with parametric\nknowledge, which can be successfully leveraged for making abstention decisions.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/650b7db9a27ba14323e74421/r8NcNOIZLNbi6Z1-7T1VM.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01152.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650b7db9a27ba14323e74421",
            "avatarUrl": "/avatars/668a881a1bb394a2115fd8f34fe4e875.svg",
            "fullname": "Tanya Goyal",
            "name": "tanyagoyal-p",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "681dd2e9a61bb228fae1702b",
            "name": "cornell",
            "fullname": "Cornell University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652303d0974423bd3ef70468/4ZbVAynBI2QThFWmlWE-b.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.00510",
            "authors": [
                {
                    "_id": "68dde0406024653e8a3ed0be",
                    "name": "Jiarun Liu",
                    "hidden": false
                },
                {
                    "_id": "68dde0406024653e8a3ed0bf",
                    "name": "Shiyue Xu",
                    "hidden": false
                },
                {
                    "_id": "68dde0406024653e8a3ed0c0",
                    "name": "Shangkun Liu",
                    "hidden": false
                },
                {
                    "_id": "68dde0406024653e8a3ed0c1",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "68dde0406024653e8a3ed0c2",
                    "name": "Wen Liu",
                    "hidden": false
                },
                {
                    "_id": "68dde0406024653e8a3ed0c3",
                    "name": "Min Liu",
                    "hidden": false
                },
                {
                    "_id": "68dde0406024653e8a3ed0c4",
                    "name": "Xiaoqing Zhou",
                    "hidden": false
                },
                {
                    "_id": "68dde0406024653e8a3ed0c5",
                    "name": "Hanmin Wang",
                    "hidden": false
                },
                {
                    "_id": "68dde0406024653e8a3ed0c6",
                    "name": "Shilin Jia",
                    "hidden": false
                },
                {
                    "_id": "68dde0406024653e8a3ed0c7",
                    "name": "zhen Wang",
                    "hidden": false
                },
                {
                    "_id": "68dde0406024653e8a3ed0c8",
                    "name": "Shaohua Tian",
                    "hidden": false
                },
                {
                    "_id": "68dde0406024653e8a3ed0c9",
                    "name": "Hanhao Li",
                    "hidden": false
                },
                {
                    "_id": "68dde0406024653e8a3ed0ca",
                    "name": "Junbo Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dde0406024653e8a3ed0cb",
                    "name": "Yongli Yu",
                    "hidden": false
                },
                {
                    "_id": "68dde0406024653e8a3ed0cc",
                    "name": "Peng Cao",
                    "hidden": false
                },
                {
                    "_id": "68dde0406024653e8a3ed0cd",
                    "name": "Haofen Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T04:41:58.000Z",
            "submittedOnDailyAt": "2025-10-02T00:45:45.437Z",
            "title": "JoyAgent-JDGenie: Technical Report on the GAIA",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Large Language Models are increasingly deployed as autonomous agents for\ncomplex real-world tasks, yet existing systems often focus on isolated\nimprovements without a unifying design for robustness and adaptability. We\npropose a generalist agent architecture that integrates three core components:\na collective multi-agent framework combining planning and execution agents with\ncritic model voting, a hierarchical memory system spanning working, semantic,\nand procedural layers, and a refined tool suite for search, code execution, and\nmultimodal parsing. Evaluated on a comprehensive benchmark, our framework\nconsistently outperforms open-source baselines and approaches the performance\nof proprietary systems. These results demonstrate the importance of\nsystem-level integration and highlight a path toward scalable, resilient, and\nadaptive AI assistants capable of operating across diverse domains and tasks.",
            "upvotes": 3,
            "discussionId": "68dde0406024653e8a3ed0ce",
            "githubRepo": "https://github.com/jd-opensource/joyagent-jdgenie",
            "ai_summary": "A generalist agent architecture combining multi-agent planning, hierarchical memory, and a refined tool suite outperforms existing systems in diverse tasks.",
            "ai_keywords": [
                "multi-agent framework",
                "planning agents",
                "execution agents",
                "critic model voting",
                "hierarchical memory system",
                "working memory",
                "semantic memory",
                "procedural memory",
                "tool suite",
                "search",
                "code execution",
                "multimodal parsing"
            ],
            "githubStars": 10778,
            "organization": {
                "_id": "682a97a154b087448a5504ee",
                "name": "jingdong1",
                "fullname": "jingdong",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/682a966182dc4fc3e373e3ed/9xUgXZCij6qGJpLqpR99M.png"
            }
        },
        "publishedAt": "2025-10-01T00:41:58.000Z",
        "title": "JoyAgent-JDGenie: Technical Report on the GAIA",
        "summary": "Large Language Models are increasingly deployed as autonomous agents for\ncomplex real-world tasks, yet existing systems often focus on isolated\nimprovements without a unifying design for robustness and adaptability. We\npropose a generalist agent architecture that integrates three core components:\na collective multi-agent framework combining planning and execution agents with\ncritic model voting, a hierarchical memory system spanning working, semantic,\nand procedural layers, and a refined tool suite for search, code execution, and\nmultimodal parsing. Evaluated on a comprehensive benchmark, our framework\nconsistently outperforms open-source baselines and approaches the performance\nof proprietary systems. These results demonstrate the importance of\nsystem-level integration and highlight a path toward scalable, resilient, and\nadaptive AI assistants capable of operating across diverse domains and tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00510.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 116
        },
        "organization": {
            "_id": "682a97a154b087448a5504ee",
            "name": "jingdong1",
            "fullname": "jingdong",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/682a966182dc4fc3e373e3ed/9xUgXZCij6qGJpLqpR99M.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01037",
            "authors": [
                {
                    "_id": "68de25726024653e8a3ed1f8",
                    "name": "Yongcheng Zeng",
                    "hidden": false
                },
                {
                    "_id": "68de25726024653e8a3ed1f9",
                    "user": {
                        "_id": "65d1b42f3da87ce21e33261a",
                        "avatarUrl": "/avatars/041cb441fa3871acde4ba565632056bf.svg",
                        "isPro": false,
                        "fullname": "RubinSun",
                        "user": "RubinSun",
                        "type": "user"
                    },
                    "name": "Zexu Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:54:41.772Z",
                    "hidden": false
                },
                {
                    "_id": "68de25726024653e8a3ed1fa",
                    "name": "Bokai Ji",
                    "hidden": false
                },
                {
                    "_id": "68de25726024653e8a3ed1fb",
                    "name": "Erxue Min",
                    "hidden": false
                },
                {
                    "_id": "68de25726024653e8a3ed1fc",
                    "name": "Hengyi Cai",
                    "hidden": false
                },
                {
                    "_id": "68de25726024653e8a3ed1fd",
                    "name": "Shuaiqiang Wang",
                    "hidden": false
                },
                {
                    "_id": "68de25726024653e8a3ed1fe",
                    "name": "Dawei Yin",
                    "hidden": false
                },
                {
                    "_id": "68de25726024653e8a3ed1ff",
                    "name": "Haifeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68de25726024653e8a3ed200",
                    "name": "Xu Chen",
                    "hidden": false
                },
                {
                    "_id": "68de25726024653e8a3ed201",
                    "name": "Jun Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T15:41:27.000Z",
            "submittedOnDailyAt": "2025-10-02T05:49:05.470Z",
            "title": "CurES: From Gradient Analysis to Efficient Curriculum Learning for\n  Reasoning LLMs",
            "submittedOnDailyBy": {
                "_id": "65d1b42f3da87ce21e33261a",
                "avatarUrl": "/avatars/041cb441fa3871acde4ba565632056bf.svg",
                "isPro": false,
                "fullname": "RubinSun",
                "user": "RubinSun",
                "type": "user"
            },
            "summary": "Curriculum learning plays a crucial role in enhancing the training efficiency\nof large language models (LLMs) on reasoning tasks. However, existing methods\noften fail to adequately account for variations in prompt difficulty or rely on\nsimplistic filtering mechanisms to select prompt datasets within a narrow\ncriterion range, resulting in significant computational waste. In this work, we\napproach the problem from the perspective of reinforcement learning gradient\noptimization, offering a systematic and theoretical investigation into how to\nimprove the training efficiency of LLMs. We identify two key factors\ninfluencing training efficiency: the selection of training prompts and the\nallocation of rollout quantities across different prompts. Our theoretical\nanalysis reveals that the sampling distribution of prompts dictates the\nconvergence rate of gradient descent, while the allocation of the rollout\nquantity influences the consistency and stability of overall gradient updates.\nBased on these insights, we propose CurES, an efficient training method that\naccelerates convergence and employs Bayesian posterior estimation to minimize\ncomputational overhead. Experiments demonstrate that our CurES outperforms\nGroup Relative Policy Optimization (GRPO) by +3.30 points and\n+4.82 points with 1.5B and 7B models, respectively. Additionally,\nCurES exhibits faster convergence compared to baselines, including GRPO.",
            "upvotes": 2,
            "discussionId": "68de25726024653e8a3ed202",
            "githubRepo": "https://github.com/ZexuSun/CurES",
            "ai_summary": "CurES, a reinforcement learning-based method, improves the training efficiency of large language models by optimizing prompt selection and rollout allocation, leading to faster convergence and reduced computational overhead.",
            "ai_keywords": [
                "curriculum learning",
                "large language models",
                "reinforcement learning",
                "gradient optimization",
                "prompt selection",
                "rollout quantities",
                "gradient descent",
                "Bayesian posterior estimation",
                "Group Relative Policy Optimization"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-10-01T11:41:27.000Z",
        "title": "CurES: From Gradient Analysis to Efficient Curriculum Learning for\n  Reasoning LLMs",
        "summary": "Curriculum learning plays a crucial role in enhancing the training efficiency\nof large language models (LLMs) on reasoning tasks. However, existing methods\noften fail to adequately account for variations in prompt difficulty or rely on\nsimplistic filtering mechanisms to select prompt datasets within a narrow\ncriterion range, resulting in significant computational waste. In this work, we\napproach the problem from the perspective of reinforcement learning gradient\noptimization, offering a systematic and theoretical investigation into how to\nimprove the training efficiency of LLMs. We identify two key factors\ninfluencing training efficiency: the selection of training prompts and the\nallocation of rollout quantities across different prompts. Our theoretical\nanalysis reveals that the sampling distribution of prompts dictates the\nconvergence rate of gradient descent, while the allocation of the rollout\nquantity influences the consistency and stability of overall gradient updates.\nBased on these insights, we propose CurES, an efficient training method that\naccelerates convergence and employs Bayesian posterior estimation to minimize\ncomputational overhead. Experiments demonstrate that our CurES outperforms\nGroup Relative Policy Optimization (GRPO) by +3.30 points and\n+4.82 points with 1.5B and 7B models, respectively. Additionally,\nCurES exhibits faster convergence compared to baselines, including GRPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01037.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d1b42f3da87ce21e33261a",
            "avatarUrl": "/avatars/041cb441fa3871acde4ba565632056bf.svg",
            "fullname": "RubinSun",
            "name": "RubinSun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.00777",
            "authors": [
                {
                    "_id": "68dddb2c6024653e8a3ed07a",
                    "name": "Youngbin Choi",
                    "hidden": false
                },
                {
                    "_id": "68dddb2c6024653e8a3ed07b",
                    "name": "Minjong Lee",
                    "hidden": false
                },
                {
                    "_id": "68dddb2c6024653e8a3ed07c",
                    "name": "Saemi Moon",
                    "hidden": false
                },
                {
                    "_id": "68dddb2c6024653e8a3ed07d",
                    "name": "Seunghyuk Cho",
                    "hidden": false
                },
                {
                    "_id": "68dddb2c6024653e8a3ed07e",
                    "name": "Chaehyeon Chung",
                    "hidden": false
                },
                {
                    "_id": "68dddb2c6024653e8a3ed07f",
                    "name": "MoonJeong Park",
                    "hidden": false
                },
                {
                    "_id": "68dddb2c6024653e8a3ed080",
                    "name": "Dongwoo Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T11:16:04.000Z",
            "submittedOnDailyAt": "2025-10-02T00:24:22.727Z",
            "title": "In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "650a4546a0f81fbc0a94ef1a",
                "avatarUrl": "/avatars/4aed2b283e0c0e2567c14eb81adf809a.svg",
                "isPro": false,
                "fullname": "Minjong Lee",
                "user": "Minjong",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are increasingly studied in the context of\nmulti-turn reasoning, where models iteratively refine their outputs based on\nuser-provided feedback. Such settings are crucial for tasks that require\ncomplex reasoning, yet existing feedback paradigms often rely on issuing new\nmessages. LLMs struggle to integrate these reliably, leading to inconsistent\nimprovements. In this work, we introduce in-place feedback, a novel interaction\nparadigm in which users directly edit an LLM's previous response, and the model\nconditions on this modified response to generate its revision. Empirical\nevaluations on diverse reasoning-intensive benchmarks reveal that in-place\nfeedback achieves better performance than conventional multi-turn feedback\nwhile using 79.1% fewer tokens. Complementary analyses on controlled\nenvironments further demonstrate that in-place feedback resolves a core\nlimitation of multi-turn feedback: models often fail to apply feedback\nprecisely to erroneous parts of the response, leaving errors uncorrected and\nsometimes introducing new mistakes into previously correct content. These\nfindings suggest that in-place feedback offers a more natural and effective\nmechanism for guiding LLMs in reasoning-intensive tasks.",
            "upvotes": 2,
            "discussionId": "68dddb2c6024653e8a3ed081",
            "ai_summary": "In-place feedback allows users to directly edit LLM responses, improving performance and reducing token usage in multi-turn reasoning tasks.",
            "ai_keywords": [
                "large language models",
                "multi-turn reasoning",
                "in-place feedback",
                "feedback paradigms",
                "token usage",
                "reasoning-intensive benchmarks",
                "controlled environments"
            ]
        },
        "publishedAt": "2025-10-01T07:16:04.000Z",
        "title": "In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn\n  Reasoning",
        "summary": "Large language models (LLMs) are increasingly studied in the context of\nmulti-turn reasoning, where models iteratively refine their outputs based on\nuser-provided feedback. Such settings are crucial for tasks that require\ncomplex reasoning, yet existing feedback paradigms often rely on issuing new\nmessages. LLMs struggle to integrate these reliably, leading to inconsistent\nimprovements. In this work, we introduce in-place feedback, a novel interaction\nparadigm in which users directly edit an LLM's previous response, and the model\nconditions on this modified response to generate its revision. Empirical\nevaluations on diverse reasoning-intensive benchmarks reveal that in-place\nfeedback achieves better performance than conventional multi-turn feedback\nwhile using 79.1% fewer tokens. Complementary analyses on controlled\nenvironments further demonstrate that in-place feedback resolves a core\nlimitation of multi-turn feedback: models often fail to apply feedback\nprecisely to erroneous parts of the response, leaving errors uncorrected and\nsometimes introducing new mistakes into previously correct content. These\nfindings suggest that in-place feedback offers a more natural and effective\nmechanism for guiding LLMs in reasoning-intensive tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00777.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "650a4546a0f81fbc0a94ef1a",
            "avatarUrl": "/avatars/4aed2b283e0c0e2567c14eb81adf809a.svg",
            "fullname": "Minjong Lee",
            "name": "Minjong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.19185",
            "authors": [
                {
                    "_id": "68d4a1a136950a9dff156889",
                    "name": "Mohammed Mehedi Hasan",
                    "hidden": false
                },
                {
                    "_id": "68d4a1a136950a9dff15688a",
                    "user": {
                        "_id": "62b4f3b7464e664268bf4e85",
                        "avatarUrl": "/avatars/16e79e11a8734b1d241e0f0c55a54045.svg",
                        "isPro": false,
                        "fullname": "Leo",
                        "user": "hao-li",
                        "type": "user"
                    },
                    "name": "Hao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-25T07:11:05.515Z",
                    "hidden": false
                },
                {
                    "_id": "68d4a1a136950a9dff15688b",
                    "name": "Emad Fallahzadeh",
                    "hidden": false
                },
                {
                    "_id": "68d4a1a136950a9dff15688c",
                    "name": "Gopi Krishnan Rajbahadur",
                    "hidden": false
                },
                {
                    "_id": "68d4a1a136950a9dff15688d",
                    "name": "Bram Adams",
                    "hidden": false
                },
                {
                    "_id": "68d4a1a136950a9dff15688e",
                    "name": "Ahmed E. Hassan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-23T16:02:09.000Z",
            "submittedOnDailyAt": "2025-10-02T01:31:01.949Z",
            "title": "An Empirical Study of Testing Practices in Open Source AI Agent\n  Frameworks and Agentic Applications",
            "submittedOnDailyBy": {
                "_id": "62b4f3b7464e664268bf4e85",
                "avatarUrl": "/avatars/16e79e11a8734b1d241e0f0c55a54045.svg",
                "isPro": false,
                "fullname": "Leo",
                "user": "hao-li",
                "type": "user"
            },
            "summary": "Foundation model (FM)-based AI agents are rapidly gaining adoption across\ndiverse domains, but their inherent non-determinism and non-reproducibility\npose testing and quality assurance challenges. While recent benchmarks provide\ntask-level evaluations, there is limited understanding of how developers verify\nthe internal correctness of these agents during development.\n  To address this gap, we conduct the first large-scale empirical study of\ntesting practices in the AI agent ecosystem, analyzing 39 open-source agent\nframeworks and 439 agentic applications. We identify ten distinct testing\npatterns and find that novel, agent-specific methods like DeepEval are seldom\nused (around 1%), while traditional patterns like negative and membership\ntesting are widely adapted to manage FM uncertainty. By mapping these patterns\nto canonical architectural components of agent frameworks and agentic\napplications, we uncover a fundamental inversion of testing effort:\ndeterministic components like Resource Artifacts (tools) and Coordination\nArtifacts (workflows) consume over 70% of testing effort, while the FM-based\nPlan Body receives less than 5%. Crucially, this reveals a critical blind spot,\nas the Trigger component (prompts) remains neglected, appearing in around 1% of\nall tests.\n  Our findings offer the first empirical testing baseline in FM-based agent\nframeworks and agentic applications, revealing a rational but incomplete\nadaptation to non-determinism. To address it, framework developers should\nimprove support for novel testing methods, application developers must adopt\nprompt regression testing, and researchers should explore barriers to adoption.\nStrengthening these practices is vital for building more robust and dependable\nAI agents.",
            "upvotes": 2,
            "discussionId": "68d4a1a136950a9dff15688f",
            "ai_summary": "The study identifies testing practices in AI agent frameworks and applications, highlighting a focus on deterministic components and a neglect of the Trigger component, suggesting improvements for robustness.",
            "ai_keywords": [
                "Foundation model",
                "AI agents",
                "non-determinism",
                "non-reproducibility",
                "testing practices",
                "DeepEval",
                "negative testing",
                "membership testing",
                "Resource Artifacts",
                "Coordination Artifacts",
                "Plan Body",
                "Trigger component",
                "prompt regression testing"
            ]
        },
        "publishedAt": "2025-09-23T12:02:09.000Z",
        "title": "An Empirical Study of Testing Practices in Open Source AI Agent\n  Frameworks and Agentic Applications",
        "summary": "Foundation model (FM)-based AI agents are rapidly gaining adoption across\ndiverse domains, but their inherent non-determinism and non-reproducibility\npose testing and quality assurance challenges. While recent benchmarks provide\ntask-level evaluations, there is limited understanding of how developers verify\nthe internal correctness of these agents during development.\n  To address this gap, we conduct the first large-scale empirical study of\ntesting practices in the AI agent ecosystem, analyzing 39 open-source agent\nframeworks and 439 agentic applications. We identify ten distinct testing\npatterns and find that novel, agent-specific methods like DeepEval are seldom\nused (around 1%), while traditional patterns like negative and membership\ntesting are widely adapted to manage FM uncertainty. By mapping these patterns\nto canonical architectural components of agent frameworks and agentic\napplications, we uncover a fundamental inversion of testing effort:\ndeterministic components like Resource Artifacts (tools) and Coordination\nArtifacts (workflows) consume over 70% of testing effort, while the FM-based\nPlan Body receives less than 5%. Crucially, this reveals a critical blind spot,\nas the Trigger component (prompts) remains neglected, appearing in around 1% of\nall tests.\n  Our findings offer the first empirical testing baseline in FM-based agent\nframeworks and agentic applications, revealing a rational but incomplete\nadaptation to non-determinism. To address it, framework developers should\nimprove support for novel testing methods, application developers must adopt\nprompt regression testing, and researchers should explore barriers to adoption.\nStrengthening these practices is vital for building more robust and dependable\nAI agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19185.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62b4f3b7464e664268bf4e85",
            "avatarUrl": "/avatars/16e79e11a8734b1d241e0f0c55a54045.svg",
            "fullname": "Leo",
            "name": "hao-li",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.01070",
            "authors": [
                {
                    "_id": "68de15226024653e8a3ed1a2",
                    "user": {
                        "_id": "6422f416a73327caad9d1d86",
                        "avatarUrl": "/avatars/aa3639277cd1732504402fc64a57eff8.svg",
                        "isPro": false,
                        "fullname": "Bartosz Cywiński",
                        "user": "bcywinski",
                        "type": "user"
                    },
                    "name": "Bartosz Cywiński",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:54:46.599Z",
                    "hidden": false
                },
                {
                    "_id": "68de15226024653e8a3ed1a3",
                    "name": "Emil Ryd",
                    "hidden": false
                },
                {
                    "_id": "68de15226024653e8a3ed1a4",
                    "name": "Rowan Wang",
                    "hidden": false
                },
                {
                    "_id": "68de15226024653e8a3ed1a5",
                    "name": "Senthooran Rajamanoharan",
                    "hidden": false
                },
                {
                    "_id": "68de15226024653e8a3ed1a6",
                    "name": "Neel Nanda",
                    "hidden": false
                },
                {
                    "_id": "68de15226024653e8a3ed1a7",
                    "name": "Arthur Conmy",
                    "hidden": false
                },
                {
                    "_id": "68de15226024653e8a3ed1a8",
                    "name": "Samuel Marks",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T16:12:28.000Z",
            "submittedOnDailyAt": "2025-10-02T04:50:49.182Z",
            "title": "Eliciting Secret Knowledge from Language Models",
            "submittedOnDailyBy": {
                "_id": "6422f416a73327caad9d1d86",
                "avatarUrl": "/avatars/aa3639277cd1732504402fc64a57eff8.svg",
                "isPro": false,
                "fullname": "Bartosz Cywiński",
                "user": "bcywinski",
                "type": "user"
            },
            "summary": "We study secret elicitation: discovering knowledge that an AI possesses but\ndoes not explicitly verbalize. As a testbed, we train three families of large\nlanguage models (LLMs) to possess specific knowledge that they apply downstream\nbut deny knowing when asked directly. For example, in one setting, we train an\nLLM to generate replies that are consistent with knowing the user is female,\nwhile denying this knowledge when asked directly. We then design various\nblack-box and white-box secret elicitation techniques and evaluate them based\non whether they can help an LLM auditor successfully guess the secret\nknowledge. Many of our techniques improve on simple baselines. Our most\neffective techniques (performing best in 2/3 settings) are based on prefill\nattacks, a black-box technique where the LLM reveals secret knowledge when\ngenerating a completion from a predefined prefix. In our remaining setting,\nwhite-box techniques based on logit lens and sparse autoencoders (SAEs) are\nmost effective. We release our models and code, establishing a public benchmark\nfor evaluating secret elicitation methods.",
            "upvotes": 1,
            "discussionId": "68de15236024653e8a3ed1a9",
            "githubRepo": "https://github.com/cywinski/eliciting-secret-knowledge",
            "ai_summary": "Researchers develop and evaluate techniques to uncover hidden knowledge in large language models through black-box and white-box methods, with prefill attacks and logit lens being particularly effective.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "secret elicitation",
                "prefill attacks",
                "logit lens",
                "sparse autoencoders",
                "SAEs"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-10-01T12:12:28.000Z",
        "title": "Eliciting Secret Knowledge from Language Models",
        "summary": "We study secret elicitation: discovering knowledge that an AI possesses but\ndoes not explicitly verbalize. As a testbed, we train three families of large\nlanguage models (LLMs) to possess specific knowledge that they apply downstream\nbut deny knowing when asked directly. For example, in one setting, we train an\nLLM to generate replies that are consistent with knowing the user is female,\nwhile denying this knowledge when asked directly. We then design various\nblack-box and white-box secret elicitation techniques and evaluate them based\non whether they can help an LLM auditor successfully guess the secret\nknowledge. Many of our techniques improve on simple baselines. Our most\neffective techniques (performing best in 2/3 settings) are based on prefill\nattacks, a black-box technique where the LLM reveals secret knowledge when\ngenerating a completion from a predefined prefix. In our remaining setting,\nwhite-box techniques based on logit lens and sparse autoencoders (SAEs) are\nmost effective. We release our models and code, establishing a public benchmark\nfor evaluating secret elicitation methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01070.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6422f416a73327caad9d1d86",
            "avatarUrl": "/avatars/aa3639277cd1732504402fc64a57eff8.svg",
            "fullname": "Bartosz Cywiński",
            "name": "bcywinski",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.01061",
            "authors": [
                {
                    "_id": "68de21df6024653e8a3ed1cf",
                    "user": {
                        "_id": "64b7f06fda8017900e893eb4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7f06fda8017900e893eb4/3VcQFjERXyAjHQFLMFEJt.jpeg",
                        "isPro": false,
                        "fullname": "Mark Boss",
                        "user": "mboss",
                        "type": "user"
                    },
                    "name": "Mark Boss",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:54:44.039Z",
                    "hidden": false
                },
                {
                    "_id": "68de21df6024653e8a3ed1d0",
                    "name": "Andreas Engelhardt",
                    "hidden": false
                },
                {
                    "_id": "68de21df6024653e8a3ed1d1",
                    "name": "Simon Donné",
                    "hidden": false
                },
                {
                    "_id": "68de21df6024653e8a3ed1d2",
                    "name": "Varun Jampani",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T16:01:17.000Z",
            "submittedOnDailyAt": "2025-10-02T05:26:35.206Z",
            "title": "ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced\n  Wasserstein Distance for Variance Reduction",
            "submittedOnDailyBy": {
                "_id": "64b7f06fda8017900e893eb4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7f06fda8017900e893eb4/3VcQFjERXyAjHQFLMFEJt.jpeg",
                "isPro": false,
                "fullname": "Mark Boss",
                "user": "mboss",
                "type": "user"
            },
            "summary": "Distribution matching is central to many vision and graphics tasks, where the\nwidely used Wasserstein distance is too costly to compute for high dimensional\ndistributions. The Sliced Wasserstein Distance (SWD) offers a scalable\nalternative, yet its Monte Carlo estimator suffers from high variance,\nresulting in noisy gradients and slow convergence. We introduce Reservoir SWD\n(ReSWD), which integrates Weighted Reservoir Sampling into SWD to adaptively\nretain informative projection directions in optimization steps, resulting in\nstable gradients while remaining unbiased. Experiments on synthetic benchmarks\nand real-world tasks such as color correction and diffusion guidance show that\nReSWD consistently outperforms standard SWD and other variance reduction\nbaselines. Project page: https://reservoirswd.github.io/",
            "upvotes": 1,
            "discussionId": "68de21e06024653e8a3ed1d3",
            "projectPage": "https://reservoirswd.github.io/",
            "githubRepo": "https://github.com/Stability-AI/ReSWD",
            "ai_summary": "Reservoir SWD reduces variance in Sliced Wasserstein Distance, improving gradient stability and performance in vision and graphics tasks.",
            "ai_keywords": [
                "Sliced Wasserstein Distance",
                "SWD",
                "Weighted Reservoir Sampling",
                "Reservoir SWD",
                "ReSWD",
                "variance reduction",
                "gradient stability",
                "color correction",
                "diffusion guidance"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "62e1573a6fb6e362b4a90690",
                "name": "stabilityai",
                "fullname": "Stability AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png"
            }
        },
        "publishedAt": "2025-10-01T12:01:17.000Z",
        "title": "ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced\n  Wasserstein Distance for Variance Reduction",
        "summary": "Distribution matching is central to many vision and graphics tasks, where the\nwidely used Wasserstein distance is too costly to compute for high dimensional\ndistributions. The Sliced Wasserstein Distance (SWD) offers a scalable\nalternative, yet its Monte Carlo estimator suffers from high variance,\nresulting in noisy gradients and slow convergence. We introduce Reservoir SWD\n(ReSWD), which integrates Weighted Reservoir Sampling into SWD to adaptively\nretain informative projection directions in optimization steps, resulting in\nstable gradients while remaining unbiased. Experiments on synthetic benchmarks\nand real-world tasks such as color correction and diffusion guidance show that\nReSWD consistently outperforms standard SWD and other variance reduction\nbaselines. Project page: https://reservoirswd.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01061.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b7f06fda8017900e893eb4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7f06fda8017900e893eb4/3VcQFjERXyAjHQFLMFEJt.jpeg",
            "fullname": "Mark Boss",
            "name": "mboss",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 34
        },
        "organization": {
            "_id": "62e1573a6fb6e362b4a90690",
            "name": "stabilityai",
            "fullname": "Stability AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.00438",
            "authors": [
                {
                    "_id": "68de6e6f70ada21878c75057",
                    "name": "Zhaoyang Li",
                    "hidden": false
                },
                {
                    "_id": "68de6e6f70ada21878c75058",
                    "name": "Dongjun Qian",
                    "hidden": false
                },
                {
                    "_id": "68de6e6f70ada21878c75059",
                    "name": "Kai Su",
                    "hidden": false
                },
                {
                    "_id": "68de6e6f70ada21878c7505a",
                    "name": "Qishuai Diao",
                    "hidden": false
                },
                {
                    "_id": "68de6e6f70ada21878c7505b",
                    "name": "Xiangyang Xia",
                    "hidden": false
                },
                {
                    "_id": "68de6e6f70ada21878c7505c",
                    "name": "Chang Liu",
                    "hidden": false
                },
                {
                    "_id": "68de6e6f70ada21878c7505d",
                    "name": "Wenfei Yang",
                    "hidden": false
                },
                {
                    "_id": "68de6e6f70ada21878c7505e",
                    "name": "Tianzhu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68de6e6f70ada21878c7505f",
                    "name": "Zehuan Yuan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T02:41:11.000Z",
            "submittedOnDailyAt": "2025-10-02T10:52:53.746Z",
            "title": "BindWeave: Subject-Consistent Video Generation via Cross-Modal\n  Integration",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "Diffusion Transformer has shown remarkable abilities in generating\nhigh-fidelity videos, delivering visually coherent frames and rich details over\nextended durations. However, existing video generation models still fall short\nin subject-consistent video generation due to an inherent difficulty in parsing\nprompts that specify complex spatial relationships, temporal logic, and\ninteractions among multiple subjects. To address this issue, we propose\nBindWeave, a unified framework that handles a broad range of subject-to-video\nscenarios from single-subject cases to complex multi-subject scenes with\nheterogeneous entities. To bind complex prompt semantics to concrete visual\nsubjects, we introduce an MLLM-DiT framework in which a pretrained multimodal\nlarge language model performs deep cross-modal reasoning to ground entities and\ndisentangle roles, attributes, and interactions, yielding subject-aware hidden\nstates that condition the diffusion transformer for high-fidelity\nsubject-consistent video generation. Experiments on the OpenS2V benchmark\ndemonstrate that our method achieves superior performance across subject\nconsistency, naturalness, and text relevance in generated videos, outperforming\nexisting open-source and commercial models.",
            "upvotes": 1,
            "discussionId": "68de6e6f70ada21878c75060",
            "projectPage": "https://lzy-dot.github.io/BindWeave/",
            "ai_summary": "BindWeave, a unified framework using MLLM-DiT, enhances subject-consistent video generation by integrating deep cross-modal reasoning with diffusion transformers, achieving superior performance on OpenS2V.",
            "ai_keywords": [
                "Diffusion Transformer",
                "BindWeave",
                "MLLM-DiT",
                "multimodal large language model",
                "deep cross-modal reasoning",
                "subject-aware hidden states",
                "subject consistency",
                "naturalness",
                "text relevance",
                "OpenS2V benchmark"
            ]
        },
        "publishedAt": "2025-09-30T22:41:11.000Z",
        "title": "BindWeave: Subject-Consistent Video Generation via Cross-Modal\n  Integration",
        "summary": "Diffusion Transformer has shown remarkable abilities in generating\nhigh-fidelity videos, delivering visually coherent frames and rich details over\nextended durations. However, existing video generation models still fall short\nin subject-consistent video generation due to an inherent difficulty in parsing\nprompts that specify complex spatial relationships, temporal logic, and\ninteractions among multiple subjects. To address this issue, we propose\nBindWeave, a unified framework that handles a broad range of subject-to-video\nscenarios from single-subject cases to complex multi-subject scenes with\nheterogeneous entities. To bind complex prompt semantics to concrete visual\nsubjects, we introduce an MLLM-DiT framework in which a pretrained multimodal\nlarge language model performs deep cross-modal reasoning to ground entities and\ndisentangle roles, attributes, and interactions, yielding subject-aware hidden\nstates that condition the diffusion transformer for high-fidelity\nsubject-consistent video generation. Experiments on the OpenS2V benchmark\ndemonstrate that our method achieves superior performance across subject\nconsistency, naturalness, and text relevance in generated videos, outperforming\nexisting open-source and commercial models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00438.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 60
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26514",
            "authors": [
                {
                    "_id": "68dc94d24159d1f2418f9b43",
                    "name": "Yue Wang",
                    "hidden": false
                },
                {
                    "_id": "68dc94d24159d1f2418f9b44",
                    "name": "Ruotian Ma",
                    "hidden": false
                },
                {
                    "_id": "68dc94d24159d1f2418f9b45",
                    "name": "Xingyu Chen",
                    "hidden": false
                },
                {
                    "_id": "68dc94d24159d1f2418f9b46",
                    "name": "Zhengliang Shi",
                    "hidden": false
                },
                {
                    "_id": "68dc94d24159d1f2418f9b47",
                    "name": "Wanshun Chen",
                    "hidden": false
                },
                {
                    "_id": "68dc94d24159d1f2418f9b48",
                    "name": "Huang Liu",
                    "hidden": false
                },
                {
                    "_id": "68dc94d24159d1f2418f9b49",
                    "name": "Jiadi Yao",
                    "hidden": false
                },
                {
                    "_id": "68dc94d24159d1f2418f9b4a",
                    "name": "Qu Yang",
                    "hidden": false
                },
                {
                    "_id": "68dc94d24159d1f2418f9b4b",
                    "name": "Qingxuan Jiang",
                    "hidden": false
                },
                {
                    "_id": "68dc94d24159d1f2418f9b4c",
                    "name": "Fanghua Ye",
                    "hidden": false
                },
                {
                    "_id": "68dc94d24159d1f2418f9b4d",
                    "name": "Juntao Li",
                    "hidden": false
                },
                {
                    "_id": "68dc94d24159d1f2418f9b4e",
                    "name": "Min Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dc94d24159d1f2418f9b4f",
                    "name": "Zhaopeng Tu",
                    "hidden": false
                },
                {
                    "_id": "68dc94d24159d1f2418f9b50",
                    "name": "Xiaolong Li",
                    "hidden": false
                },
                {
                    "_id": "68dc94d24159d1f2418f9b51",
                    "name": "Linus",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T16:52:14.000Z",
            "submittedOnDailyAt": "2025-10-02T17:12:37.087Z",
            "title": "BatonVoice: An Operationalist Framework for Enhancing Controllable\n  Speech Synthesis with Linguistic Intelligence from LLMs",
            "submittedOnDailyBy": {
                "_id": "67485743561b1e6f9579389f",
                "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
                "isPro": false,
                "fullname": "Zhaopeng Tu",
                "user": "zptu",
                "type": "user"
            },
            "summary": "The rise of Large Language Models (LLMs) is reshaping multimodel models, with\nspeech synthesis being a prominent application. However, existing approaches\noften underutilize the linguistic intelligence of these models, typically\nfailing to leverage their powerful instruction-following capabilities. This\nlimitation hinders the model's ability to follow text instructions for\ncontrollable Text-to-Speech~(TTS). To address this, we propose a new paradigm\ninspired by ``operationalism'' that decouples instruction understanding from\nspeech generation. We introduce BatonVoice, a framework where an LLM acts as a\n``conductor'', understanding user instructions and generating a textual\n``plan'' -- explicit vocal features (e.g., pitch, energy). A separate TTS\nmodel, the ``orchestra'', then generates the speech from these features. To\nrealize this component, we develop BatonTTS, a TTS model trained specifically\nfor this task. Our experiments demonstrate that BatonVoice achieves strong\nperformance in controllable and emotional speech synthesis, outperforming\nstrong open- and closed-source baselines. Notably, our approach enables\nremarkable zero-shot cross-lingual generalization, accurately applying feature\ncontrol abilities to languages unseen during post-training. This demonstrates\nthat objectifying speech into textual vocal features can more effectively\nunlock the linguistic intelligence of LLMs.",
            "upvotes": 1,
            "discussionId": "68dc94d24159d1f2418f9b52",
            "ai_summary": "BatonVoice framework decouples instruction understanding from speech generation, using an LLM to create vocal feature plans and a specialized TTS model to produce speech, achieving strong performance in controllable and emotional speech synthesis with zero-shot cross-lingual generalization.",
            "ai_keywords": [
                "Large Language Models",
                "multimodel models",
                "speech synthesis",
                "instruction-following capabilities",
                "Text-to-Speech",
                "operationalism",
                "BatonVoice",
                "BatonTTS",
                "vocal features",
                "controllable speech synthesis",
                "emotional speech synthesis",
                "zero-shot cross-lingual generalization"
            ],
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-09-30T12:52:14.000Z",
        "title": "BatonVoice: An Operationalist Framework for Enhancing Controllable\n  Speech Synthesis with Linguistic Intelligence from LLMs",
        "summary": "The rise of Large Language Models (LLMs) is reshaping multimodel models, with\nspeech synthesis being a prominent application. However, existing approaches\noften underutilize the linguistic intelligence of these models, typically\nfailing to leverage their powerful instruction-following capabilities. This\nlimitation hinders the model's ability to follow text instructions for\ncontrollable Text-to-Speech~(TTS). To address this, we propose a new paradigm\ninspired by ``operationalism'' that decouples instruction understanding from\nspeech generation. We introduce BatonVoice, a framework where an LLM acts as a\n``conductor'', understanding user instructions and generating a textual\n``plan'' -- explicit vocal features (e.g., pitch, energy). A separate TTS\nmodel, the ``orchestra'', then generates the speech from these features. To\nrealize this component, we develop BatonTTS, a TTS model trained specifically\nfor this task. Our experiments demonstrate that BatonVoice achieves strong\nperformance in controllable and emotional speech synthesis, outperforming\nstrong open- and closed-source baselines. Notably, our approach enables\nremarkable zero-shot cross-lingual generalization, accurately applying feature\ncontrol abilities to languages unseen during post-training. This demonstrates\nthat objectifying speech into textual vocal features can more effectively\nunlock the linguistic intelligence of LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26514.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67485743561b1e6f9579389f",
            "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
            "fullname": "Zhaopeng Tu",
            "name": "zptu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25916",
            "authors": [
                {
                    "_id": "68de2852a9129e507bf4a41f",
                    "name": "Peng Liu",
                    "hidden": false
                },
                {
                    "_id": "68de2852a9129e507bf4a420",
                    "name": "Haozhan Shen",
                    "hidden": false
                },
                {
                    "_id": "68de2852a9129e507bf4a421",
                    "name": "Chunxin Fang",
                    "hidden": false
                },
                {
                    "_id": "68de2852a9129e507bf4a422",
                    "name": "Zhicheng Sun",
                    "hidden": false
                },
                {
                    "_id": "68de2852a9129e507bf4a423",
                    "name": "Jiajia Liao",
                    "hidden": false
                },
                {
                    "_id": "68de2852a9129e507bf4a424",
                    "name": "Tiancheng Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T08:10:56.000Z",
            "submittedOnDailyAt": "2025-10-02T05:53:53.622Z",
            "title": "VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained\n  Perception in VLMs",
            "submittedOnDailyBy": {
                "_id": "5f0de36419cb630495b8153c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658676776546-5f0de36419cb630495b8153c.jpeg",
                "isPro": false,
                "fullname": "Tony Zhao",
                "user": "tianchez",
                "type": "user"
            },
            "summary": "Vision-Language Models (VLMs) excel at high-level scene understanding but\nfalter on fine-grained perception tasks requiring precise localization. This\nfailure stems from a fundamental mismatch, as generating exact numerical\ncoordinates is a challenging task for language-centric architectures. In this\npaper, we introduce VLM-FO1, a novel framework that overcomes this limitation\nby reframing object-centric perception from a brittle coordinate generation\nproblem into a robust feature retrieval task. Our method operates as a\nplug-and-play module that integrates with any pre-trained VLM. It leverages a\nHybrid Fine-grained Region Encoder (HFRE), featuring a dual vision encoder, to\ngenerate powerful region tokens rich in both semantic and spatial detail. A\ntoken-based referencing system then enables the LLM to seamlessly reason about\nand ground language in these specific visual regions. Experiments show that\nVLM-FO1 achieves state-of-the-art performance across a diverse suite of\nbenchmarks, demonstrating exceptional capabilities in object grounding, region\ngenerational understanding, and visual region reasoning. Crucially, our\ntwo-stage training strategy ensures that these perception gains are achieved\nwithout compromising the base model's general visual understanding\ncapabilities. VLM-FO1 establishes an effective and flexible paradigm for\nbuilding perception-aware VLMs, bridging the gap between high-level reasoning\nand fine-grained visual grounding.",
            "upvotes": 1,
            "discussionId": "68de2852a9129e507bf4a425",
            "ai_summary": "VLM-FO1 enhances vision-language models with a hybrid fine-grained region encoder to improve object localization and region understanding without sacrificing general visual capabilities.",
            "ai_keywords": [
                "Vision-Language Models",
                "VLM-FO1",
                "Hybrid Fine-grained Region Encoder",
                "HFRE",
                "dual vision encoder",
                "region tokens",
                "token-based referencing system",
                "object grounding",
                "region generational understanding",
                "visual region reasoning",
                "two-stage training strategy"
            ],
            "organization": {
                "_id": "62dd5fdac33f9cb60bf668ad",
                "name": "omlab",
                "fullname": "Om AI Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5f0de36419cb630495b8153c/0T0ttw9sIEIerOZ1L1Zfm.png"
            }
        },
        "publishedAt": "2025-09-30T04:10:56.000Z",
        "title": "VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained\n  Perception in VLMs",
        "summary": "Vision-Language Models (VLMs) excel at high-level scene understanding but\nfalter on fine-grained perception tasks requiring precise localization. This\nfailure stems from a fundamental mismatch, as generating exact numerical\ncoordinates is a challenging task for language-centric architectures. In this\npaper, we introduce VLM-FO1, a novel framework that overcomes this limitation\nby reframing object-centric perception from a brittle coordinate generation\nproblem into a robust feature retrieval task. Our method operates as a\nplug-and-play module that integrates with any pre-trained VLM. It leverages a\nHybrid Fine-grained Region Encoder (HFRE), featuring a dual vision encoder, to\ngenerate powerful region tokens rich in both semantic and spatial detail. A\ntoken-based referencing system then enables the LLM to seamlessly reason about\nand ground language in these specific visual regions. Experiments show that\nVLM-FO1 achieves state-of-the-art performance across a diverse suite of\nbenchmarks, demonstrating exceptional capabilities in object grounding, region\ngenerational understanding, and visual region reasoning. Crucially, our\ntwo-stage training strategy ensures that these perception gains are achieved\nwithout compromising the base model's general visual understanding\ncapabilities. VLM-FO1 establishes an effective and flexible paradigm for\nbuilding perception-aware VLMs, bridging the gap between high-level reasoning\nand fine-grained visual grounding.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25916.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f0de36419cb630495b8153c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658676776546-5f0de36419cb630495b8153c.jpeg",
            "fullname": "Tony Zhao",
            "name": "tianchez",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "organization": {
            "_id": "62dd5fdac33f9cb60bf668ad",
            "name": "omlab",
            "fullname": "Om AI Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5f0de36419cb630495b8153c/0T0ttw9sIEIerOZ1L1Zfm.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25411",
            "authors": [
                {
                    "_id": "68dded9d6024653e8a3ed0e2",
                    "user": {
                        "_id": "63e03e8942591dda0b9a0dec",
                        "avatarUrl": "/avatars/fbf54bc5a8aadc7399d1a1be9b9d6b65.svg",
                        "isPro": false,
                        "fullname": "Zewei Zhang",
                        "user": "zeweizhang",
                        "type": "user"
                    },
                    "name": "Zewei Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:55:13.434Z",
                    "hidden": false
                },
                {
                    "_id": "68dded9d6024653e8a3ed0e3",
                    "name": "Huan Liu",
                    "hidden": false
                },
                {
                    "_id": "68dded9d6024653e8a3ed0e4",
                    "name": "Yuanhao Yu",
                    "hidden": false
                },
                {
                    "_id": "68dded9d6024653e8a3ed0e5",
                    "name": "Jun Chen",
                    "hidden": false
                },
                {
                    "_id": "68dded9d6024653e8a3ed0e6",
                    "name": "Xiangyu Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T19:09:37.000Z",
            "submittedOnDailyAt": "2025-10-02T01:51:27.396Z",
            "title": "Boolean Satisfiability via Imitation Learning",
            "submittedOnDailyBy": {
                "_id": "63e03e8942591dda0b9a0dec",
                "avatarUrl": "/avatars/fbf54bc5a8aadc7399d1a1be9b9d6b65.svg",
                "isPro": false,
                "fullname": "Zewei Zhang",
                "user": "zeweizhang",
                "type": "user"
            },
            "summary": "We propose ImitSAT, a branching policy for conflict-driven clause learning\n(CDCL) solvers based on imitation learning for the Boolean satisfiability\nproblem (SAT). Unlike previous methods that predict instance-level signals to\nimprove CDCL branching indirectly, or rely on reinforcement learning and\ninsufficient CDCL information to enhance branching, ImitSAT learns from expert\nKeyTrace that collapses a full run into the sequence of surviving decisions.\nReplaying a KeyTrace on the same instance is nearly conflict-free, providing\ndense decision-level supervision and directly reducing propagations -- the\ndominant contributor to wall-clock time. This prefix-conditioned supervision\nenables ImitSAT to reproduce high-quality branches without exploration,\nyielding faster convergence, stable training, and seamless integration into\nCDCL. Extensive experiments demonstrate that ImitSAT reduces propagation counts\nand runtime, outperforming state-of-the-art learned approaches. We released the\nsource code and trained model at https://github.com/zewei-Zhang/ImitSAT",
            "upvotes": 1,
            "discussionId": "68dded9d6024653e8a3ed0e7",
            "githubRepo": "https://github.com/zewei-Zhang/ImitSAT",
            "ai_summary": "ImitSAT, a branching policy for CDCL solvers using imitation learning from expert traces, reduces propagation counts and runtime by providing dense decision-level supervision.",
            "ai_keywords": [
                "ImitSAT",
                "conflict-driven clause learning",
                "CDCL",
                "Boolean satisfiability problem",
                "SAT",
                "imitation learning",
                "KeyTrace",
                "decision-level supervision",
                "propagations",
                "wall-clock time",
                "prefix-conditioned supervision",
                "learned approaches"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-09-29T15:09:37.000Z",
        "title": "Boolean Satisfiability via Imitation Learning",
        "summary": "We propose ImitSAT, a branching policy for conflict-driven clause learning\n(CDCL) solvers based on imitation learning for the Boolean satisfiability\nproblem (SAT). Unlike previous methods that predict instance-level signals to\nimprove CDCL branching indirectly, or rely on reinforcement learning and\ninsufficient CDCL information to enhance branching, ImitSAT learns from expert\nKeyTrace that collapses a full run into the sequence of surviving decisions.\nReplaying a KeyTrace on the same instance is nearly conflict-free, providing\ndense decision-level supervision and directly reducing propagations -- the\ndominant contributor to wall-clock time. This prefix-conditioned supervision\nenables ImitSAT to reproduce high-quality branches without exploration,\nyielding faster convergence, stable training, and seamless integration into\nCDCL. Extensive experiments demonstrate that ImitSAT reduces propagation counts\nand runtime, outperforming state-of-the-art learned approaches. We released the\nsource code and trained model at https://github.com/zewei-Zhang/ImitSAT",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25411.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63e03e8942591dda0b9a0dec",
            "avatarUrl": "/avatars/fbf54bc5a8aadc7399d1a1be9b9d6b65.svg",
            "fullname": "Zewei Zhang",
            "name": "zeweizhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25045",
            "authors": [
                {
                    "_id": "68de2c31a9129e507bf4a43d",
                    "user": {
                        "_id": "642585e7120a3ed32332e7ff",
                        "avatarUrl": "/avatars/cbc8ce70e090a3e3243b0e42fa00b8a4.svg",
                        "isPro": false,
                        "fullname": "Marco Bronzini",
                        "user": "MartialDeimos",
                        "type": "user"
                    },
                    "name": "Marco Bronzini",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:54:27.735Z",
                    "hidden": false
                },
                {
                    "_id": "68de2c31a9129e507bf4a43e",
                    "name": "Carlo Nicolini",
                    "hidden": false
                },
                {
                    "_id": "68de2c31a9129e507bf4a43f",
                    "name": "Bruno Lepri",
                    "hidden": false
                },
                {
                    "_id": "68de2c31a9129e507bf4a440",
                    "name": "Jacopo Staiano",
                    "hidden": false
                },
                {
                    "_id": "68de2c31a9129e507bf4a441",
                    "name": "Andrea Passerini",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/642585e7120a3ed32332e7ff/MY9xtHBHzYTsc2gvxic6i.png"
            ],
            "publishedAt": "2025-09-29T16:59:07.000Z",
            "submittedOnDailyAt": "2025-10-02T06:11:39.570Z",
            "title": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic\n  Architectures",
            "submittedOnDailyBy": {
                "_id": "642585e7120a3ed32332e7ff",
                "avatarUrl": "/avatars/cbc8ce70e090a3e3243b0e42fa00b8a4.svg",
                "isPro": false,
                "fullname": "Marco Bronzini",
                "user": "MartialDeimos",
                "type": "user"
            },
            "summary": "Despite their capabilities, Large Language Models (LLMs) remain opaque with\nlimited understanding of their internal representations. Current\ninterpretability methods, such as direct logit attribution (DLA) and sparse\nautoencoders (SAEs), provide restricted insight due to limitations such as the\nmodel's output vocabulary or unclear feature names. This work introduces\nHyperdimensional Probe, a novel paradigm for decoding information from the LLM\nvector space. It combines ideas from symbolic representations and neural\nprobing to project the model's residual stream into interpretable concepts via\nVector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs\nand conventional probes while overcoming their key limitations. We validate our\ndecoding paradigm with controlled input-completion tasks, probing the model's\nfinal state before next-token prediction on inputs spanning syntactic pattern\nrecognition, key-value associations, and abstract inference. We further assess\nit in a question-answering setting, examining the state of the model both\nbefore and after text generation. Our experiments show that our probe reliably\nextracts meaningful concepts across varied LLMs, embedding sizes, and input\ndomains, also helping identify LLM failures. Our work advances information\ndecoding in LLM vector space, enabling extracting more informative,\ninterpretable, and structured features from neural representations.",
            "upvotes": 1,
            "discussionId": "68de2c31a9129e507bf4a442",
            "githubRepo": "https://github.com/Ipazia-AI/hyperprobe",
            "ai_summary": "A novel Hyperdimensional Probe method decodes information from LLM vector spaces using Vector Symbolic Architectures, providing interpretable insights into model states and failures.",
            "ai_keywords": [
                "Large Language Models",
                "Hyperdimensional Probe",
                "Vector Symbolic Architectures",
                "direct logit attribution",
                "sparse autoencoders",
                "residual stream",
                "next-token prediction",
                "syntactic pattern recognition",
                "key-value associations",
                "abstract inference",
                "question-answering",
                "neural probing",
                "symbolic representations"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-09-29T12:59:07.000Z",
        "title": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic\n  Architectures",
        "summary": "Despite their capabilities, Large Language Models (LLMs) remain opaque with\nlimited understanding of their internal representations. Current\ninterpretability methods, such as direct logit attribution (DLA) and sparse\nautoencoders (SAEs), provide restricted insight due to limitations such as the\nmodel's output vocabulary or unclear feature names. This work introduces\nHyperdimensional Probe, a novel paradigm for decoding information from the LLM\nvector space. It combines ideas from symbolic representations and neural\nprobing to project the model's residual stream into interpretable concepts via\nVector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs\nand conventional probes while overcoming their key limitations. We validate our\ndecoding paradigm with controlled input-completion tasks, probing the model's\nfinal state before next-token prediction on inputs spanning syntactic pattern\nrecognition, key-value associations, and abstract inference. We further assess\nit in a question-answering setting, examining the state of the model both\nbefore and after text generation. Our experiments show that our probe reliably\nextracts meaningful concepts across varied LLMs, embedding sizes, and input\ndomains, also helping identify LLM failures. Our work advances information\ndecoding in LLM vector space, enabling extracting more informative,\ninterpretable, and structured features from neural representations.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/642585e7120a3ed32332e7ff/MY9xtHBHzYTsc2gvxic6i.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25045.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642585e7120a3ed32332e7ff",
            "avatarUrl": "/avatars/cbc8ce70e090a3e3243b0e42fa00b8a4.svg",
            "fullname": "Marco Bronzini",
            "name": "MartialDeimos",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.00225",
            "authors": [
                {
                    "_id": "68de944470ada21878c750e4",
                    "name": "Yue Meng",
                    "hidden": false
                },
                {
                    "_id": "68de944470ada21878c750e5",
                    "name": "Fei Chen",
                    "hidden": false
                },
                {
                    "_id": "68de944470ada21878c750e6",
                    "name": "Chuchu Fan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T19:51:05.000Z",
            "submittedOnDailyAt": "2025-10-02T13:34:56.044Z",
            "title": "TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic\n  Tasks",
            "submittedOnDailyBy": {
                "_id": "668e100b97171f3399e07f5d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pgzatAZut-uoXOBOwgiFB.jpeg",
                "isPro": false,
                "fullname": "Yue Meng",
                "user": "yuemithucsd",
                "type": "user"
            },
            "summary": "Learning control policies for complex, long-horizon tasks is a central\nchallenge in robotics and autonomous systems. Signal Temporal Logic (STL)\noffers a powerful and expressive language for specifying such tasks, but its\nnon-Markovian nature and inherent sparse reward make it difficult to be solved\nvia standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus\nonly on limited STL fragments or use STL robustness scores as sparse terminal\nrewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization,\nto solve general STL tasks. TGPO decomposes STL into timed subgoals and\ninvariant constraints and provides a hierarchical framework to tackle the\nproblem. The high-level component of TGPO proposes concrete time allocations\nfor these subgoals, and the low-level time-conditioned policy learns to achieve\nthe sequenced subgoals using a dense, stage-wise reward signal. During\ninference, we sample various time allocations and select the most promising\nassignment for the policy network to rollout the solution trajectory. To foster\nefficient policy learning for complex STL with multiple subgoals, we leverage\nthe learned critic to guide the high-level temporal search via\nMetropolis-Hastings sampling, focusing exploration on temporally feasible\nsolutions. We conduct experiments on five environments, ranging from\nlow-dimensional navigation to manipulation, drone, and quadrupedal locomotion.\nUnder a wide range of STL tasks, TGPO significantly outperforms\nstate-of-the-art baselines (especially for high-dimensional and long-horizon\ncases), with an average of 31.6% improvement in task success rate compared to\nthe best baseline. The code will be available at\nhttps://github.com/mengyuest/TGPO",
            "upvotes": 0,
            "discussionId": "68de944470ada21878c750e7",
            "ai_summary": "TGPO, a Temporal Grounded Policy Optimization framework, decomposes STL tasks into subgoals and uses a hierarchical approach with dense rewards to improve task success rates in complex, long-horizon robotics tasks.",
            "ai_keywords": [
                "Signal Temporal Logic (STL)",
                "Reinforcement Learning (RL)",
                "timed subgoals",
                "invariant constraints",
                "hierarchical framework",
                "time-conditioned policy",
                "dense",
                "stage-wise reward signal",
                "Metropolis-Hastings sampling",
                "temporal search",
                "task success rate"
            ],
            "organization": {
                "_id": "63728bde14d543d507ae970d",
                "name": "MIT",
                "fullname": "Massachusetts Institute of Technology",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"
            }
        },
        "publishedAt": "2025-09-30T15:51:05.000Z",
        "title": "TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic\n  Tasks",
        "summary": "Learning control policies for complex, long-horizon tasks is a central\nchallenge in robotics and autonomous systems. Signal Temporal Logic (STL)\noffers a powerful and expressive language for specifying such tasks, but its\nnon-Markovian nature and inherent sparse reward make it difficult to be solved\nvia standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus\nonly on limited STL fragments or use STL robustness scores as sparse terminal\nrewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization,\nto solve general STL tasks. TGPO decomposes STL into timed subgoals and\ninvariant constraints and provides a hierarchical framework to tackle the\nproblem. The high-level component of TGPO proposes concrete time allocations\nfor these subgoals, and the low-level time-conditioned policy learns to achieve\nthe sequenced subgoals using a dense, stage-wise reward signal. During\ninference, we sample various time allocations and select the most promising\nassignment for the policy network to rollout the solution trajectory. To foster\nefficient policy learning for complex STL with multiple subgoals, we leverage\nthe learned critic to guide the high-level temporal search via\nMetropolis-Hastings sampling, focusing exploration on temporally feasible\nsolutions. We conduct experiments on five environments, ranging from\nlow-dimensional navigation to manipulation, drone, and quadrupedal locomotion.\nUnder a wide range of STL tasks, TGPO significantly outperforms\nstate-of-the-art baselines (especially for high-dimensional and long-horizon\ncases), with an average of 31.6% improvement in task success rate compared to\nthe best baseline. The code will be available at\nhttps://github.com/mengyuest/TGPO",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00225.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "668e100b97171f3399e07f5d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pgzatAZut-uoXOBOwgiFB.jpeg",
            "fullname": "Yue Meng",
            "name": "yuemithucsd",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "63728bde14d543d507ae970d",
            "name": "MIT",
            "fullname": "Massachusetts Institute of Technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.25162",
            "authors": [
                {
                    "_id": "68dd5f21ae4a047656f86bc2",
                    "name": "Bowei Chen",
                    "hidden": false
                },
                {
                    "_id": "68dd5f21ae4a047656f86bc3",
                    "name": "Sai Bi",
                    "hidden": false
                },
                {
                    "_id": "68dd5f21ae4a047656f86bc4",
                    "name": "Hao Tan",
                    "hidden": false
                },
                {
                    "_id": "68dd5f21ae4a047656f86bc5",
                    "name": "He Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dd5f21ae4a047656f86bc6",
                    "name": "Tianyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dd5f21ae4a047656f86bc7",
                    "name": "Zhengqi Li",
                    "hidden": false
                },
                {
                    "_id": "68dd5f21ae4a047656f86bc8",
                    "name": "Yuanjun Xiong",
                    "hidden": false
                },
                {
                    "_id": "68dd5f21ae4a047656f86bc9",
                    "name": "Jianming Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dd5f21ae4a047656f86bca",
                    "name": "Kai Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:57:39.000Z",
            "submittedOnDailyAt": "2025-10-02T17:07:14.454Z",
            "title": "Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": true,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "In this work, we propose aligning pretrained visual encoders to serve as\ntokenizers for latent diffusion models in image generation. Unlike training a\nvariational autoencoder (VAE) from scratch, which primarily emphasizes\nlow-level details, our approach leverages the rich semantic structure of\nfoundation encoders. We introduce a three-stage alignment strategy: (1) freeze\nthe encoder and train an adapter and a decoder to establish a semantic latent\nspace; (2) jointly optimize all components with an additional semantic\npreservation loss, enabling the encoder to capture perceptual details while\nretaining high-level semantics; and (3) refine the decoder for improved\nreconstruction quality. This alignment yields semantically rich image\ntokenizers that benefit diffusion models. On ImageNet 256times256, our\ntokenizer accelerates the convergence of diffusion models, reaching a gFID of\n1.90 within just 64 epochs, and improves generation both with and without\nclassifier-free guidance. Scaling to LAION, a 2B-parameter text-to-image model\ntrained with our tokenizer consistently outperforms FLUX VAE under the same\ntraining steps. Overall, our method is simple, scalable, and establishes a\nsemantically grounded paradigm for continuous tokenizer design.",
            "upvotes": 0,
            "discussionId": "68dd5f21ae4a047656f86bcb",
            "ai_summary": "Pretrained visual encoders are aligned as tokenizers for latent diffusion models, improving image generation quality and convergence speed.",
            "ai_keywords": [
                "visual encoders",
                "tokenizers",
                "latent diffusion models",
                "semantic latent space",
                "adapter",
                "decoder",
                "semantic preservation loss",
                "perceptual details",
                "high-level semantics",
                "gFID",
                "ImageNet",
                "LAION",
                "FLUX VAE",
                "classifier-free guidance"
            ]
        },
        "publishedAt": "2025-09-29T13:57:39.000Z",
        "title": "Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models",
        "summary": "In this work, we propose aligning pretrained visual encoders to serve as\ntokenizers for latent diffusion models in image generation. Unlike training a\nvariational autoencoder (VAE) from scratch, which primarily emphasizes\nlow-level details, our approach leverages the rich semantic structure of\nfoundation encoders. We introduce a three-stage alignment strategy: (1) freeze\nthe encoder and train an adapter and a decoder to establish a semantic latent\nspace; (2) jointly optimize all components with an additional semantic\npreservation loss, enabling the encoder to capture perceptual details while\nretaining high-level semantics; and (3) refine the decoder for improved\nreconstruction quality. This alignment yields semantically rich image\ntokenizers that benefit diffusion models. On ImageNet 256times256, our\ntokenizer accelerates the convergence of diffusion models, reaching a gFID of\n1.90 within just 64 epochs, and improves generation both with and without\nclassifier-free guidance. Scaling to LAION, a 2B-parameter text-to-image model\ntrained with our tokenizer consistently outperforms FLUX VAE under the same\ntraining steps. Overall, our method is simple, scalable, and establishes a\nsemantically grounded paradigm for continuous tokenizer design.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25162.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 985
        },
        "isAuthorParticipating": false
    }
]