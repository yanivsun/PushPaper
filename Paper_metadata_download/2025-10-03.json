[
    {
        "paper": {
            "id": "2510.00446",
            "authors": [
                {
                    "_id": "68ddef306024653e8a3ed0e9",
                    "user": {
                        "_id": "645b0c3ec35da9c7afd95421",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
                        "isPro": false,
                        "fullname": "Yuling",
                        "user": "YerbaPage",
                        "type": "user"
                    },
                    "name": "Yuling Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:55:10.013Z",
                    "hidden": false
                },
                {
                    "_id": "68ddef306024653e8a3ed0ea",
                    "name": "Yichun Qian",
                    "hidden": false
                },
                {
                    "_id": "68ddef306024653e8a3ed0eb",
                    "name": "Hongyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ddef306024653e8a3ed0ec",
                    "name": "Beijun Shen",
                    "hidden": false
                },
                {
                    "_id": "68ddef306024653e8a3ed0ed",
                    "name": "Xiaodong Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T02:54:57.000Z",
            "submittedOnDailyAt": "2025-10-03T00:37:13.283Z",
            "title": "LongCodeZip: Compress Long Context for Code Language Models",
            "submittedOnDailyBy": {
                "_id": "645b0c3ec35da9c7afd95421",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
                "isPro": false,
                "fullname": "Yuling",
                "user": "YerbaPage",
                "type": "user"
            },
            "summary": "Code generation under long contexts is becoming increasingly critical as\nLarge Language Models (LLMs) are required to reason over extensive information\nin the codebase. While recent advances enable code LLMs to process long inputs,\nhigh API costs and generation latency remain substantial bottlenecks. Existing\ncontext pruning techniques, such as LLMLingua, achieve promising results for\ngeneral text but overlook code-specific structures and dependencies, leading to\nsuboptimal performance in programming tasks. In this paper, we propose\nLongCodeZip, a novel plug-and-play code compression framework designed\nspecifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1)\ncoarse-grained compression, which identifies and ranks function-level chunks\nusing conditional perplexity with respect to the instruction, retaining only\nthe most relevant functions; and (2) fine-grained compression, which segments\nretained functions into blocks based on perplexity and selects an optimal\nsubset under an adaptive token budget to maximize relevance. Evaluations across\nmultiple tasks, including code completion, summarization, and question\nanswering, show that LongCodeZip consistently outperforms baseline methods,\nachieving up to a 5.6x compression ratio without degrading task performance. By\neffectively reducing context size while preserving essential information,\nLongCodeZip enables LLMs to better scale to real-world, large-scale code\nscenarios, advancing the efficiency and capability of code intelligence\napplications.",
            "upvotes": 70,
            "discussionId": "68ddef316024653e8a3ed0ee",
            "githubRepo": "https://github.com/YerbaPage/LongCodeZip",
            "ai_summary": "LongCodeZip is a code compression framework for LLMs that uses dual-stage compression to reduce context size without degrading performance, improving efficiency in code intelligence applications.",
            "ai_keywords": [
                "Large Language Models",
                "code LLMs",
                "context pruning",
                "LLMLingua",
                "conditional perplexity",
                "function-level chunks",
                "fine-grained compression",
                "token budget",
                "code completion",
                "code summarization",
                "question answering",
                "code intelligence applications"
            ],
            "githubStars": 56,
            "organization": {
                "_id": "6724d0b84c0a2bf36e39a226",
                "name": "Stanford-University",
                "fullname": "Stanford University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6724cfba409a4f96ce1d773b/BjPcf9AfmEU3WeSVb33s8.png"
            }
        },
        "publishedAt": "2025-09-30T22:54:57.000Z",
        "title": "LongCodeZip: Compress Long Context for Code Language Models",
        "summary": "Code generation under long contexts is becoming increasingly critical as\nLarge Language Models (LLMs) are required to reason over extensive information\nin the codebase. While recent advances enable code LLMs to process long inputs,\nhigh API costs and generation latency remain substantial bottlenecks. Existing\ncontext pruning techniques, such as LLMLingua, achieve promising results for\ngeneral text but overlook code-specific structures and dependencies, leading to\nsuboptimal performance in programming tasks. In this paper, we propose\nLongCodeZip, a novel plug-and-play code compression framework designed\nspecifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1)\ncoarse-grained compression, which identifies and ranks function-level chunks\nusing conditional perplexity with respect to the instruction, retaining only\nthe most relevant functions; and (2) fine-grained compression, which segments\nretained functions into blocks based on perplexity and selects an optimal\nsubset under an adaptive token budget to maximize relevance. Evaluations across\nmultiple tasks, including code completion, summarization, and question\nanswering, show that LongCodeZip consistently outperforms baseline methods,\nachieving up to a 5.6x compression ratio without degrading task performance. By\neffectively reducing context size while preserving essential information,\nLongCodeZip enables LLMs to better scale to real-world, large-scale code\nscenarios, advancing the efficiency and capability of code intelligence\napplications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00446.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "645b0c3ec35da9c7afd95421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
            "fullname": "Yuling",
            "name": "YerbaPage",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 253
        },
        "organization": {
            "_id": "6724d0b84c0a2bf36e39a226",
            "name": "Stanford-University",
            "fullname": "Stanford University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6724cfba409a4f96ce1d773b/BjPcf9AfmEU3WeSVb33s8.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.02283",
            "authors": [
                {
                    "_id": "68df2b55df49fb0df1e03be2",
                    "name": "Justin Cui",
                    "hidden": false
                },
                {
                    "_id": "68df2b55df49fb0df1e03be3",
                    "user": {
                        "_id": "6381c5d63680a7cf34e08ca9",
                        "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
                        "isPro": false,
                        "fullname": "wujie10558@gmail.com",
                        "user": "wujie10",
                        "type": "user"
                    },
                    "name": "Jie Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:32:59.026Z",
                    "hidden": false
                },
                {
                    "_id": "68df2b55df49fb0df1e03be4",
                    "user": {
                        "_id": "637f0eb22438d7485b8ef5d7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
                        "isPro": false,
                        "fullname": "Ming Li",
                        "user": "limingcv",
                        "type": "user"
                    },
                    "name": "Ming Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:33:02.772Z",
                    "hidden": false
                },
                {
                    "_id": "68df2b55df49fb0df1e03be5",
                    "name": "Tao Yang",
                    "hidden": false
                },
                {
                    "_id": "68df2b55df49fb0df1e03be6",
                    "name": "Xiaojie Li",
                    "hidden": false
                },
                {
                    "_id": "68df2b55df49fb0df1e03be7",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "68df2b55df49fb0df1e03be8",
                    "name": "Andrew Bai",
                    "hidden": false
                },
                {
                    "_id": "68df2b55df49fb0df1e03be9",
                    "name": "Yuanhao Ban",
                    "hidden": false
                },
                {
                    "_id": "68df2b55df49fb0df1e03bea",
                    "name": "Cho-Jui Hsieh",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/Fb3ffDmXrslDcQSJH51MW.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/I0qCOZqF-XJo2Xg3i__RJ.mp4"
            ],
            "publishedAt": "2025-10-02T17:55:42.000Z",
            "submittedOnDailyAt": "2025-10-03T00:21:50.292Z",
            "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation",
            "submittedOnDailyBy": {
                "_id": "65862671e878be571bf9fc52",
                "avatarUrl": "/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg",
                "isPro": false,
                "fullname": "cuijiaxing",
                "user": "cuijiaxing",
                "type": "user"
            },
            "summary": "Diffusion models have revolutionized image and video generation, achieving\nunprecedented visual quality. However, their reliance on transformer\narchitectures incurs prohibitively high computational costs, particularly when\nextending generation to long videos. Recent work has explored autoregressive\nformulations for long video generation, typically by distilling from\nshort-horizon bidirectional teachers. Nevertheless, given that teacher models\ncannot synthesize long videos, the extrapolation of student models beyond their\ntraining horizon often leads to pronounced quality degradation, arising from\nthe compounding of errors within the continuous latent space. In this paper, we\npropose a simple yet effective approach to mitigate quality degradation in\nlong-horizon video generation without requiring supervision from long-video\nteachers or retraining on long video datasets. Our approach centers on\nexploiting the rich knowledge of teacher models to provide guidance for the\nstudent model through sampled segments drawn from self-generated long videos.\nOur method maintains temporal consistency while scaling video length by up to\n20x beyond teacher's capability, avoiding common issues such as over-exposure\nand error-accumulation without recomputing overlapping frames like previous\nmethods. When scaling up the computation, our method shows the capability of\ngenerating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the\nmaximum span supported by our base model's position embedding and more than 50x\nlonger than that of our baseline model. Experiments on standard benchmarks and\nour proposed improved benchmark demonstrate that our approach substantially\noutperforms baseline methods in both fidelity and consistency. Our long-horizon\nvideos demo can be found at https://self-forcing-plus-plus.github.io/",
            "upvotes": 60,
            "discussionId": "68df2b56df49fb0df1e03beb",
            "projectPage": "https://self-forcing-plus-plus.github.io/",
            "githubRepo": "https://github.com/justincui03/Self-Forcing-Plus-Plus",
            "ai_summary": "A method is proposed to enhance long-horizon video generation by using sampled segments from self-generated long videos to guide student models, maintaining quality and consistency without additional supervision or retraining.",
            "ai_keywords": [
                "diffusion models",
                "transformer architectures",
                "autoregressive formulations",
                "bidirectional teachers",
                "latent space",
                "quality degradation",
                "temporal consistency",
                "position embedding"
            ],
            "githubStars": 19,
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-10-02T13:55:42.000Z",
        "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation",
        "summary": "Diffusion models have revolutionized image and video generation, achieving\nunprecedented visual quality. However, their reliance on transformer\narchitectures incurs prohibitively high computational costs, particularly when\nextending generation to long videos. Recent work has explored autoregressive\nformulations for long video generation, typically by distilling from\nshort-horizon bidirectional teachers. Nevertheless, given that teacher models\ncannot synthesize long videos, the extrapolation of student models beyond their\ntraining horizon often leads to pronounced quality degradation, arising from\nthe compounding of errors within the continuous latent space. In this paper, we\npropose a simple yet effective approach to mitigate quality degradation in\nlong-horizon video generation without requiring supervision from long-video\nteachers or retraining on long video datasets. Our approach centers on\nexploiting the rich knowledge of teacher models to provide guidance for the\nstudent model through sampled segments drawn from self-generated long videos.\nOur method maintains temporal consistency while scaling video length by up to\n20x beyond teacher's capability, avoiding common issues such as over-exposure\nand error-accumulation without recomputing overlapping frames like previous\nmethods. When scaling up the computation, our method shows the capability of\ngenerating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the\nmaximum span supported by our base model's position embedding and more than 50x\nlonger than that of our baseline model. Experiments on standard benchmarks and\nour proposed improved benchmark demonstrate that our approach substantially\noutperforms baseline methods in both fidelity and consistency. Our long-horizon\nvideos demo can be found at https://self-forcing-plus-plus.github.io/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/Fb3ffDmXrslDcQSJH51MW.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/I0qCOZqF-XJo2Xg3i__RJ.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02283.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65862671e878be571bf9fc52",
            "avatarUrl": "/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg",
            "fullname": "cuijiaxing",
            "name": "cuijiaxing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.02245",
            "authors": [
                {
                    "_id": "68df3b51df49fb0df1e03cd2",
                    "user": {
                        "_id": "62495cb96ee7ee6b646db130",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg",
                        "isPro": false,
                        "fullname": "Runzhe Zhan",
                        "user": "rzzhan",
                        "type": "user"
                    },
                    "name": "Runzhe Zhan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:29:46.232Z",
                    "hidden": false
                },
                {
                    "_id": "68df3b51df49fb0df1e03cd3",
                    "name": "Yafu Li",
                    "hidden": false
                },
                {
                    "_id": "68df3b51df49fb0df1e03cd4",
                    "name": "Zhi Wang",
                    "hidden": false
                },
                {
                    "_id": "68df3b51df49fb0df1e03cd5",
                    "name": "Xiaoye Qu",
                    "hidden": false
                },
                {
                    "_id": "68df3b51df49fb0df1e03cd6",
                    "name": "Dongrui Liu",
                    "hidden": false
                },
                {
                    "_id": "68df3b51df49fb0df1e03cd7",
                    "name": "Jing Shao",
                    "hidden": false
                },
                {
                    "_id": "68df3b51df49fb0df1e03cd8",
                    "name": "Derek F. Wong",
                    "hidden": false
                },
                {
                    "_id": "68df3b51df49fb0df1e03cd9",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T17:31:30.000Z",
            "submittedOnDailyAt": "2025-10-03T01:31:39.660Z",
            "title": "ExGRPO: Learning to Reason from Experience",
            "submittedOnDailyBy": {
                "_id": "62495cb96ee7ee6b646db130",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg",
                "isPro": false,
                "fullname": "Runzhe Zhan",
                "user": "rzzhan",
                "type": "user"
            },
            "summary": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm\nfor improving the reasoning ability of large language models. However, standard\non-policy training discards rollout experiences after a single update, leading\nto computational inefficiency and instability. While prior work on RL has\nhighlighted the benefits of reusing past experience, the role of experience\ncharacteristics in shaping learning dynamics of large reasoning models remains\nunderexplored. In this paper, we are the first to investigate what makes a\nreasoning experience valuable and identify rollout correctness and entropy as\neffective indicators of experience value. Based on these insights, we propose\nExGRPO (Experiential Group Relative Policy Optimization), a framework that\norganizes and prioritizes valuable experiences, and employs a mixed-policy\nobjective to balance exploration with experience exploitation. Experiments on\nfive backbone models (1.5B-8B parameters) show that ExGRPO consistently\nimproves reasoning performance on mathematical/general benchmarks, with an\naverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO\nstabilizes training on both stronger and weaker models where on-policy methods\nfail. These results highlight principled experience management as a key\ningredient for efficient and scalable RLVR.",
            "upvotes": 48,
            "discussionId": "68df3b52df49fb0df1e03cda",
            "githubRepo": "https://github.com/ElliottYan/LUFFY/tree/main/ExGRPO",
            "ai_summary": "ExGRPO, a framework that prioritizes valuable reasoning experiences, improves and stabilizes reinforcement learning from verifiable rewards for large language models.",
            "ai_keywords": [
                "reinforcement learning from verifiable rewards",
                "RLVR",
                "on-policy training",
                "rollout experiences",
                "experience characteristics",
                "rollout correctness",
                "entropy",
                "ExGRPO",
                "Experiential Group Relative Policy Optimization",
                "mixed-policy objective",
                "exploration",
                "experience exploitation",
                "reasoning performance",
                "mathematical benchmarks",
                "general benchmarks"
            ],
            "githubStars": 321
        },
        "publishedAt": "2025-10-02T13:31:30.000Z",
        "title": "ExGRPO: Learning to Reason from Experience",
        "summary": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm\nfor improving the reasoning ability of large language models. However, standard\non-policy training discards rollout experiences after a single update, leading\nto computational inefficiency and instability. While prior work on RL has\nhighlighted the benefits of reusing past experience, the role of experience\ncharacteristics in shaping learning dynamics of large reasoning models remains\nunderexplored. In this paper, we are the first to investigate what makes a\nreasoning experience valuable and identify rollout correctness and entropy as\neffective indicators of experience value. Based on these insights, we propose\nExGRPO (Experiential Group Relative Policy Optimization), a framework that\norganizes and prioritizes valuable experiences, and employs a mixed-policy\nobjective to balance exploration with experience exploitation. Experiments on\nfive backbone models (1.5B-8B parameters) show that ExGRPO consistently\nimproves reasoning performance on mathematical/general benchmarks, with an\naverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO\nstabilizes training on both stronger and weaker models where on-policy methods\nfail. These results highlight principled experience management as a key\ningredient for efficient and scalable RLVR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02245.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "62495cb96ee7ee6b646db130",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg",
            "fullname": "Runzhe Zhan",
            "name": "rzzhan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.02314",
            "authors": [
                {
                    "_id": "68df49acdf49fb0df1e03d51",
                    "name": "Bo-Hsu Ke",
                    "hidden": false
                },
                {
                    "_id": "68df49acdf49fb0df1e03d52",
                    "name": "You-Zhe Xie",
                    "hidden": false
                },
                {
                    "_id": "68df49acdf49fb0df1e03d53",
                    "name": "Yu-Lun Liu",
                    "hidden": false
                },
                {
                    "_id": "68df49acdf49fb0df1e03d54",
                    "name": "Wei-Chen Chiu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/N5QZmwAl2bgkGqsFR0liN.mp4"
            ],
            "publishedAt": "2025-10-02T17:59:57.000Z",
            "submittedOnDailyAt": "2025-10-03T02:30:49.018Z",
            "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided\n  Illusions",
            "submittedOnDailyBy": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
            },
            "summary": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have significantly advanced novel view synthesis. As\nthese methods become prevalent, addressing their vulnerabilities becomes\ncritical. We analyze 3DGS robustness against image-level poisoning attacks and\npropose a novel density-guided poisoning method. Our method strategically\ninjects Gaussian points into low-density regions identified via Kernel Density\nEstimation (KDE), embedding viewpoint-dependent illusory objects clearly\nvisible from poisoned views while minimally affecting innocent views.\nAdditionally, we introduce an adaptive noise strategy to disrupt multi-view\nconsistency, further enhancing attack effectiveness. We propose a KDE-based\nevaluation protocol to assess attack difficulty systematically, enabling\nobjective benchmarking for future research. Extensive experiments demonstrate\nour method's superior performance compared to state-of-the-art techniques.\nProject page: https://hentci.github.io/stealthattack/",
            "upvotes": 46,
            "discussionId": "68df49addf49fb0df1e03d55",
            "projectPage": "https://hentci.github.io/stealthattack/",
            "githubRepo": "https://github.com/Hentci/StealthAttack_official",
            "ai_summary": "A novel density-guided poisoning method for 3D Gaussian Splatting enhances attack effectiveness by strategically injecting Gaussian points and disrupting multi-view consistency.",
            "ai_keywords": [
                "Neural Radiance Fields",
                "3D Gaussian Splatting",
                "image-level poisoning attacks",
                "density-guided poisoning",
                "Gaussian points",
                "Kernel Density Estimation",
                "viewpoint-dependent illusory objects",
                "multi-view consistency",
                "KDE-based evaluation protocol"
            ],
            "githubStars": 39,
            "organization": {
                "_id": "63e39e6499a032b1c950403d",
                "name": "NYCU",
                "fullname": "National Yang Ming Chiao Tung University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
            }
        },
        "publishedAt": "2025-10-02T13:59:57.000Z",
        "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided\n  Illusions",
        "summary": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have significantly advanced novel view synthesis. As\nthese methods become prevalent, addressing their vulnerabilities becomes\ncritical. We analyze 3DGS robustness against image-level poisoning attacks and\npropose a novel density-guided poisoning method. Our method strategically\ninjects Gaussian points into low-density regions identified via Kernel Density\nEstimation (KDE), embedding viewpoint-dependent illusory objects clearly\nvisible from poisoned views while minimally affecting innocent views.\nAdditionally, we introduce an adaptive noise strategy to disrupt multi-view\nconsistency, further enhancing attack effectiveness. We propose a KDE-based\nevaluation protocol to assess attack difficulty systematically, enabling\nobjective benchmarking for future research. Extensive experiments demonstrate\nour method's superior performance compared to state-of-the-art techniques.\nProject page: https://hentci.github.io/stealthattack/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/N5QZmwAl2bgkGqsFR0liN.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02314.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "fullname": "Yu-Lun Liu",
            "name": "yulunliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "63e39e6499a032b1c950403d",
            "name": "NYCU",
            "fullname": "National Yang Ming Chiao Tung University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.02297",
            "authors": [
                {
                    "_id": "68df29e3df49fb0df1e03bca",
                    "name": "Wentao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68df29e3df49fb0df1e03bcb",
                    "name": "Yang Young Lu",
                    "hidden": false
                },
                {
                    "_id": "68df29e3df49fb0df1e03bcc",
                    "user": {
                        "_id": "63081e15a670ed10f9d44229",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
                        "isPro": true,
                        "fullname": "Yuntian Deng",
                        "user": "yuntian-deng",
                        "type": "user"
                    },
                    "name": "Yuntian Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:33:08.482Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/eATqBkbxrVSqsBzcMJ__-.png"
            ],
            "publishedAt": "2025-10-02T17:59:00.000Z",
            "submittedOnDailyAt": "2025-10-03T00:14:16.738Z",
            "title": "Interactive Training: Feedback-Driven Neural Network Optimization",
            "submittedOnDailyBy": {
                "_id": "63081e15a670ed10f9d44229",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
                "isPro": true,
                "fullname": "Yuntian Deng",
                "user": "yuntian-deng",
                "type": "user"
            },
            "summary": "Traditional neural network training typically follows fixed, predefined\noptimization recipes, lacking the flexibility to dynamically respond to\ninstabilities or emerging training issues. In this paper, we introduce\nInteractive Training, an open-source framework that enables real-time,\nfeedback-driven intervention during neural network training by human experts or\nautomated AI agents. At its core, Interactive Training uses a control server to\nmediate communication between users or agents and the ongoing training process,\nallowing users to dynamically adjust optimizer hyperparameters, training data,\nand model checkpoints. Through three case studies, we demonstrate that\nInteractive Training achieves superior training stability, reduced sensitivity\nto initial hyperparameters, and improved adaptability to evolving user needs,\npaving the way toward a future training paradigm where AI agents autonomously\nmonitor training logs, proactively resolve instabilities, and optimize training\ndynamics.",
            "upvotes": 32,
            "discussionId": "68df29e3df49fb0df1e03bcd",
            "projectPage": "https://interactivetraining.ai/",
            "githubRepo": "https://github.com/yuntian-group/interactive-training",
            "ai_summary": "Interactive Training is a framework that allows real-time, feedback-driven intervention during neural network training, improving stability and adaptability.",
            "ai_keywords": [
                "Interactive Training",
                "control server",
                "optimizer hyperparameters",
                "training data",
                "model checkpoints",
                "training stability",
                "sensitivity to initial hyperparameters",
                "adaptability",
                "AI agents",
                "training logs",
                "training dynamics"
            ],
            "githubStars": 7,
            "organization": {
                "_id": "66e4ffb6caa240e5c2fa39e9",
                "name": "yuntian-group",
                "fullname": "Yuntian Group",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/6xB3f3moklDS6qrZAit3I.png"
            }
        },
        "publishedAt": "2025-10-02T13:59:00.000Z",
        "title": "Interactive Training: Feedback-Driven Neural Network Optimization",
        "summary": "Traditional neural network training typically follows fixed, predefined\noptimization recipes, lacking the flexibility to dynamically respond to\ninstabilities or emerging training issues. In this paper, we introduce\nInteractive Training, an open-source framework that enables real-time,\nfeedback-driven intervention during neural network training by human experts or\nautomated AI agents. At its core, Interactive Training uses a control server to\nmediate communication between users or agents and the ongoing training process,\nallowing users to dynamically adjust optimizer hyperparameters, training data,\nand model checkpoints. Through three case studies, we demonstrate that\nInteractive Training achieves superior training stability, reduced sensitivity\nto initial hyperparameters, and improved adaptability to evolving user needs,\npaving the way toward a future training paradigm where AI agents autonomously\nmonitor training logs, proactively resolve instabilities, and optimize training\ndynamics.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/eATqBkbxrVSqsBzcMJ__-.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02297.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63081e15a670ed10f9d44229",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
            "fullname": "Yuntian Deng",
            "name": "yuntian-deng",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 258
        },
        "organization": {
            "_id": "66e4ffb6caa240e5c2fa39e9",
            "name": "yuntian-group",
            "fullname": "Yuntian Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/6xB3f3moklDS6qrZAit3I.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.02209",
            "authors": [
                {
                    "_id": "68df3031df49fb0df1e03c3a",
                    "name": "Yanxu Chen",
                    "hidden": false
                },
                {
                    "_id": "68df3031df49fb0df1e03c3b",
                    "name": "Zijun Yao",
                    "hidden": false
                },
                {
                    "_id": "68df3031df49fb0df1e03c3c",
                    "name": "Yantao Liu",
                    "hidden": false
                },
                {
                    "_id": "68df3031df49fb0df1e03c3d",
                    "name": "Jin Ye",
                    "hidden": false
                },
                {
                    "_id": "68df3031df49fb0df1e03c3e",
                    "name": "Jianing Yu",
                    "hidden": false
                },
                {
                    "_id": "68df3031df49fb0df1e03c3f",
                    "name": "Lei Hou",
                    "hidden": false
                },
                {
                    "_id": "68df3031df49fb0df1e03c40",
                    "name": "Juanzi Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T16:54:57.000Z",
            "submittedOnDailyAt": "2025-10-03T00:38:58.116Z",
            "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nas autonomous agents, showing promise in reasoning, tool use, and sequential\ndecision-making. While prior benchmarks have evaluated LLM agents in domains\nsuch as software engineering and scientific discovery, the finance domain\nremains underexplored, despite its direct relevance to economic value and\nhigh-stakes decision-making. Existing financial benchmarks primarily test\nstatic knowledge through question answering, but they fall short of capturing\nthe dynamic and iterative nature of trading. To address this gap, we introduce\nStockBench, a contamination-free benchmark designed to evaluate LLM agents in\nrealistic, multi-month stock trading environments. Agents receive daily market\nsignals -- including prices, fundamentals, and news -- and must make sequential\nbuy, sell, or hold decisions. Performance is assessed using financial metrics\nsuch as cumulative return, maximum drawdown, and the Sortino ratio. Our\nevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and\nopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM\nagents struggle to outperform the simple buy-and-hold baseline, several models\ndemonstrate the potential to deliver higher returns and manage risk more\neffectively. These findings highlight both the challenges and opportunities in\ndeveloping LLM-powered financial agents, showing that excelling at static\nfinancial knowledge tasks does not necessarily translate into successful\ntrading strategies. We release StockBench as an open-source resource to support\nreproducibility and advance future research in this domain.",
            "upvotes": 24,
            "discussionId": "68df3031df49fb0df1e03c41",
            "ai_summary": "StockBench evaluates large language models in realistic stock trading environments, revealing challenges and opportunities in developing LLM-powered financial agents.",
            "ai_keywords": [
                "large language models",
                "LLM agents",
                "reasoning",
                "tool use",
                "sequential decision-making",
                "software engineering",
                "scientific discovery",
                "finance domain",
                "financial benchmarks",
                "question answering",
                "dynamic trading",
                "contamination-free benchmark",
                "daily market signals",
                "prices",
                "fundamentals",
                "news",
                "sequential buy",
                "sell",
                "hold decisions",
                "cumulative return",
                "maximum drawdown",
                "Sortino ratio",
                "GPT-5",
                "Claude-4",
                "Qwen3",
                "Kimi-K2",
                "GLM-4.5",
                "buy-and-hold baseline",
                "static financial knowledge tasks",
                "trading strategies"
            ]
        },
        "publishedAt": "2025-10-02T12:54:57.000Z",
        "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?",
        "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nas autonomous agents, showing promise in reasoning, tool use, and sequential\ndecision-making. While prior benchmarks have evaluated LLM agents in domains\nsuch as software engineering and scientific discovery, the finance domain\nremains underexplored, despite its direct relevance to economic value and\nhigh-stakes decision-making. Existing financial benchmarks primarily test\nstatic knowledge through question answering, but they fall short of capturing\nthe dynamic and iterative nature of trading. To address this gap, we introduce\nStockBench, a contamination-free benchmark designed to evaluate LLM agents in\nrealistic, multi-month stock trading environments. Agents receive daily market\nsignals -- including prices, fundamentals, and news -- and must make sequential\nbuy, sell, or hold decisions. Performance is assessed using financial metrics\nsuch as cumulative return, maximum drawdown, and the Sortino ratio. Our\nevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and\nopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM\nagents struggle to outperform the simple buy-and-hold baseline, several models\ndemonstrate the potential to deliver higher returns and manage risk more\neffectively. These findings highlight both the challenges and opportunities in\ndeveloping LLM-powered financial agents, showing that excelling at static\nfinancial knowledge tasks does not necessarily translate into successful\ntrading strategies. We release StockBench as an open-source resource to support\nreproducibility and advance future research in this domain.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02209.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 118
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01149",
            "authors": [
                {
                    "_id": "68de63af70ada21878c75026",
                    "name": "Paul Teiletche",
                    "hidden": false
                },
                {
                    "_id": "68de63af70ada21878c75027",
                    "user": {
                        "_id": "661e945eebe3616a1b09e279",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661e945eebe3616a1b09e279/U3DL1BNouUpcusCKAPZm0.jpeg",
                        "isPro": false,
                        "fullname": "Quentin Macé",
                        "user": "QuentinJG",
                        "type": "user"
                    },
                    "name": "Quentin Macé",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:52:04.668Z",
                    "hidden": false
                },
                {
                    "_id": "68de63af70ada21878c75028",
                    "name": "Max Conti",
                    "hidden": false
                },
                {
                    "_id": "68de63af70ada21878c75029",
                    "name": "Antonio Loison",
                    "hidden": false
                },
                {
                    "_id": "68de63af70ada21878c7502a",
                    "name": "Gautier Viaud",
                    "hidden": false
                },
                {
                    "_id": "68de63af70ada21878c7502b",
                    "name": "Pierre Colombo",
                    "hidden": false
                },
                {
                    "_id": "68de63af70ada21878c7502c",
                    "user": {
                        "_id": "60f2e021adf471cbdf8bb660",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg",
                        "isPro": false,
                        "fullname": "Manuel Faysse",
                        "user": "manu",
                        "type": "user"
                    },
                    "name": "Manuel Faysse",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:52:47.596Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T17:41:17.000Z",
            "submittedOnDailyAt": "2025-10-03T09:50:48.566Z",
            "title": "ModernVBERT: Towards Smaller Visual Document Retrievers",
            "submittedOnDailyBy": {
                "_id": "60f2e021adf471cbdf8bb660",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg",
                "isPro": false,
                "fullname": "Manuel Faysse",
                "user": "manu",
                "type": "user"
            },
            "summary": "Multimodal embedding models are gaining prevalence, notably for document\nretrieval as efficient alternatives to text-only pipelines. These models are\ntypically built by finetuning large vision-language decoders (VLMs) with\ncontrastive losses on text-image pairs. In this work, we show that, while\ncost-efficient, this repurposing approach often bottlenecks retrieval\nperformance. Through controlled experiments, we establish a principled recipe\nfor improving visual document retrieval models. We notably measure the impact\nof attention masking, image resolution, modality alignment data regimes, and\nlate interaction centered contrastive objectives which emerge as central\nperformance factors. Building on these insights, we release ModernVBERT, a\ncompact 250M-parameter vision-language encoder that outperforms models up to 10\ntimes larger when finetuned on document retrieval tasks. Models and code are\nmade available at https://huggingface.co/ModernVBERT.",
            "upvotes": 24,
            "discussionId": "68de63b070ada21878c7502d",
            "projectPage": "https://huggingface.co/ModernVBERT",
            "ai_summary": "ModernVBERT, a compact vision-language encoder, outperforms larger models in document retrieval by optimizing attention masking, image resolution, modality alignment, and contrastive objectives.",
            "ai_keywords": [
                "multimodal embedding models",
                "document retrieval",
                "vision-language decoders",
                "contrastive losses",
                "attention masking",
                "image resolution",
                "modality alignment",
                "contrastive objectives",
                "vision-language encoder"
            ],
            "organization": {
                "_id": "68dc126476aff34f469efbc4",
                "name": "ModernVBERT",
                "fullname": "ModernVBERT",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6651baf4b34bbdaec88333e7/qxlWj1d9iagGW6T8yCkJV.png"
            }
        },
        "publishedAt": "2025-10-01T13:41:17.000Z",
        "title": "ModernVBERT: Towards Smaller Visual Document Retrievers",
        "summary": "Multimodal embedding models are gaining prevalence, notably for document\nretrieval as efficient alternatives to text-only pipelines. These models are\ntypically built by finetuning large vision-language decoders (VLMs) with\ncontrastive losses on text-image pairs. In this work, we show that, while\ncost-efficient, this repurposing approach often bottlenecks retrieval\nperformance. Through controlled experiments, we establish a principled recipe\nfor improving visual document retrieval models. We notably measure the impact\nof attention masking, image resolution, modality alignment data regimes, and\nlate interaction centered contrastive objectives which emerge as central\nperformance factors. Building on these insights, we release ModernVBERT, a\ncompact 250M-parameter vision-language encoder that outperforms models up to 10\ntimes larger when finetuned on document retrieval tasks. Models and code are\nmade available at https://huggingface.co/ModernVBERT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01149.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f2e021adf471cbdf8bb660",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg",
            "fullname": "Manuel Faysse",
            "name": "manu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 181
        },
        "organization": {
            "_id": "68dc126476aff34f469efbc4",
            "name": "ModernVBERT",
            "fullname": "ModernVBERT",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6651baf4b34bbdaec88333e7/qxlWj1d9iagGW6T8yCkJV.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.01444",
            "authors": [
                {
                    "_id": "68df2e3edf49fb0df1e03c19",
                    "name": "Rui Liu",
                    "hidden": false
                },
                {
                    "_id": "68df2e3edf49fb0df1e03c1a",
                    "name": "Dian Yu",
                    "hidden": false
                },
                {
                    "_id": "68df2e3edf49fb0df1e03c1b",
                    "user": {
                        "_id": "6623ea65b642e29cdf90a1b4",
                        "avatarUrl": "/avatars/e32e90574c1162b2be87ed78604e3e4d.svg",
                        "isPro": true,
                        "fullname": "TongZheng",
                        "user": "TongZheng1999",
                        "type": "user"
                    },
                    "name": "Tong Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:30:01.705Z",
                    "hidden": false
                },
                {
                    "_id": "68df2e3edf49fb0df1e03c1c",
                    "user": {
                        "_id": "65037565da2d88e201f63b7a",
                        "avatarUrl": "/avatars/d1b6ce17236360e9583b8bb4cb87e506.svg",
                        "isPro": true,
                        "fullname": "Runpeng Dai",
                        "user": "Leo-Dai",
                        "type": "user"
                    },
                    "name": "Runpeng Dai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:30:04.017Z",
                    "hidden": false
                },
                {
                    "_id": "68df2e3edf49fb0df1e03c1d",
                    "name": "Zongxia Li",
                    "hidden": false
                },
                {
                    "_id": "68df2e3edf49fb0df1e03c1e",
                    "name": "Wenhao Yu",
                    "hidden": false
                },
                {
                    "_id": "68df2e3edf49fb0df1e03c1f",
                    "name": "Zhenwen Liang",
                    "hidden": false
                },
                {
                    "_id": "68df2e3edf49fb0df1e03c20",
                    "name": "Linfeng Song",
                    "hidden": false
                },
                {
                    "_id": "68df2e3edf49fb0df1e03c21",
                    "name": "Haitao Mi",
                    "hidden": false
                },
                {
                    "_id": "68df2e3edf49fb0df1e03c22",
                    "name": "Pratap Tokekar",
                    "hidden": false
                },
                {
                    "_id": "68df2e3edf49fb0df1e03c23",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T20:32:08.000Z",
            "submittedOnDailyAt": "2025-10-03T00:33:45.725Z",
            "title": "VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "6656bf615b203a05a1f0968c",
                "avatarUrl": "/avatars/1ee0b0099c10dd76c8e3b7d312221b15.svg",
                "isPro": false,
                "fullname": "Rui Liu",
                "user": "lr10260",
                "type": "user"
            },
            "summary": "Reinforcement learning with verifiable rewards (RLVR) improves reasoning in\nlarge language models (LLMs) but struggles with exploration, an issue that\nstill persists for multimodal LLMs (MLLMs). Current methods treat the visual\ninput as a fixed, deterministic condition, overlooking a critical source of\nambiguity and struggling to build policies robust to plausible visual\nvariations. We introduce VOGUE (Visual Uncertainty Guided\nExploration), a novel method that shifts exploration from the output (text)\nto the input (visual) space. By treating the image as a stochastic context,\nVOGUE quantifies the policy's sensitivity to visual perturbations using the\nsymmetric KL divergence between a \"raw\" and \"noisy\" branch, creating a direct\nsignal for uncertainty-aware exploration. This signal shapes the learning\nobjective via an uncertainty-proportional bonus, which, combined with a\ntoken-entropy bonus and an annealed sampling schedule, effectively balances\nexploration and exploitation. Implemented within GRPO on two model scales\n(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three\nvisual math benchmarks and 3.7% on three general-domain reasoning benchmarks,\nwhile simultaneously increasing pass@4 performance and mitigating the\nexploration decay commonly observed in RL fine-tuning. Our work shows that\ngrounding exploration in the inherent uncertainty of visual inputs is an\neffective strategy for improving multimodal reasoning.",
            "upvotes": 18,
            "discussionId": "68df2e3edf49fb0df1e03c24",
            "ai_summary": "VOGUE, a method that shifts exploration to the visual input space by quantifying policy sensitivity to visual perturbations, enhances multimodal reasoning in large language models.",
            "ai_keywords": [
                "reinforcement learning with verifiable rewards",
                "multimodal LLMs",
                "visual uncertainty",
                "stochastic context",
                "symmetric KL divergence",
                "uncertainty-aware exploration",
                "token-entropy bonus",
                "annealed sampling schedule",
                "GRPO",
                "pass@1 accuracy",
                "pass@4 performance",
                "exploration decay"
            ],
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-10-01T16:32:08.000Z",
        "title": "VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal\n  Reasoning",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) improves reasoning in\nlarge language models (LLMs) but struggles with exploration, an issue that\nstill persists for multimodal LLMs (MLLMs). Current methods treat the visual\ninput as a fixed, deterministic condition, overlooking a critical source of\nambiguity and struggling to build policies robust to plausible visual\nvariations. We introduce VOGUE (Visual Uncertainty Guided\nExploration), a novel method that shifts exploration from the output (text)\nto the input (visual) space. By treating the image as a stochastic context,\nVOGUE quantifies the policy's sensitivity to visual perturbations using the\nsymmetric KL divergence between a \"raw\" and \"noisy\" branch, creating a direct\nsignal for uncertainty-aware exploration. This signal shapes the learning\nobjective via an uncertainty-proportional bonus, which, combined with a\ntoken-entropy bonus and an annealed sampling schedule, effectively balances\nexploration and exploitation. Implemented within GRPO on two model scales\n(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three\nvisual math benchmarks and 3.7% on three general-domain reasoning benchmarks,\nwhile simultaneously increasing pass@4 performance and mitigating the\nexploration decay commonly observed in RL fine-tuning. Our work shows that\ngrounding exploration in the inherent uncertainty of visual inputs is an\neffective strategy for improving multimodal reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01444.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6656bf615b203a05a1f0968c",
            "avatarUrl": "/avatars/1ee0b0099c10dd76c8e3b7d312221b15.svg",
            "fullname": "Rui Liu",
            "name": "lr10260",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.22067",
            "authors": [
                {
                    "_id": "68dec5ae70ada21878c75169",
                    "name": "Anton Korznikov",
                    "hidden": false
                },
                {
                    "_id": "68dec5ae70ada21878c7516a",
                    "name": "Andrey Galichin",
                    "hidden": false
                },
                {
                    "_id": "68dec5ae70ada21878c7516b",
                    "user": {
                        "_id": "60cd95ee15ecba5f2200304a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
                        "isPro": false,
                        "fullname": "Alexey Dontsov",
                        "user": "therem",
                        "type": "user"
                    },
                    "name": "Alexey Dontsov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:33:28.616Z",
                    "hidden": false
                },
                {
                    "_id": "68dec5ae70ada21878c7516c",
                    "name": "Oleg Y. Rogov",
                    "hidden": false
                },
                {
                    "_id": "68dec5ae70ada21878c7516d",
                    "name": "Ivan Oseledets",
                    "hidden": false
                },
                {
                    "_id": "68dec5ae70ada21878c7516e",
                    "user": {
                        "_id": "662f8d645c4db70c77a203b0",
                        "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg",
                        "isPro": false,
                        "fullname": "Elena Tutubalina",
                        "user": "tlenusik",
                        "type": "user"
                    },
                    "name": "Elena Tutubalina",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:33:26.134Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T08:49:47.000Z",
            "submittedOnDailyAt": "2025-10-03T09:26:05.516Z",
            "title": "The Rogue Scalpel: Activation Steering Compromises LLM Safety",
            "submittedOnDailyBy": {
                "_id": "60cd95ee15ecba5f2200304a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
                "isPro": false,
                "fullname": "Alexey Dontsov",
                "user": "therem",
                "type": "user"
            },
            "summary": "Activation steering is a promising technique for controlling LLM behavior by\nadding semantically meaningful vectors directly into a model's hidden states\nduring inference. It is often framed as a precise, interpretable, and\npotentially safer alternative to fine-tuning. We demonstrate the opposite:\nsteering systematically breaks model alignment safeguards, making it comply\nwith harmful requests. Through extensive experiments on different model\nfamilies, we show that even steering in a random direction can increase the\nprobability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign\nfeatures from a sparse autoencoder (SAE), a common source of interpretable\ndirections, increases these rates by a further 2-4%. Finally, we show that\ncombining 20 randomly sampled vectors that jailbreak a single prompt creates a\nuniversal attack, significantly increasing harmful compliance on unseen\nrequests. These results challenge the paradigm of safety through\ninterpretability, showing that precise control over model internals does not\nguarantee precise control over model behavior.",
            "upvotes": 18,
            "discussionId": "68dec5ae70ada21878c7516f",
            "ai_summary": "Activation steering, intended to control LLM behavior, can instead increase harmful compliance and undermine model alignment safeguards.",
            "ai_keywords": [
                "activation steering",
                "LLM behavior",
                "semantically meaningful vectors",
                "hidden states",
                "model alignment safeguards",
                "harmful requests",
                "sparse autoencoder",
                "universal attack"
            ]
        },
        "publishedAt": "2025-09-26T04:49:47.000Z",
        "title": "The Rogue Scalpel: Activation Steering Compromises LLM Safety",
        "summary": "Activation steering is a promising technique for controlling LLM behavior by\nadding semantically meaningful vectors directly into a model's hidden states\nduring inference. It is often framed as a precise, interpretable, and\npotentially safer alternative to fine-tuning. We demonstrate the opposite:\nsteering systematically breaks model alignment safeguards, making it comply\nwith harmful requests. Through extensive experiments on different model\nfamilies, we show that even steering in a random direction can increase the\nprobability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign\nfeatures from a sparse autoencoder (SAE), a common source of interpretable\ndirections, increases these rates by a further 2-4%. Finally, we show that\ncombining 20 randomly sampled vectors that jailbreak a single prompt creates a\nuniversal attack, significantly increasing harmful compliance on unseen\nrequests. These results challenge the paradigm of safety through\ninterpretability, showing that precise control over model internals does not\nguarantee precise control over model behavior.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22067.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60cd95ee15ecba5f2200304a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
            "fullname": "Alexey Dontsov",
            "name": "therem",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.02286",
            "authors": [
                {
                    "_id": "68df64e7df49fb0df1e03da3",
                    "user": {
                        "_id": "628711eb40423ef48fbc07af",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628711eb40423ef48fbc07af/QmW1ow6LFNxPa0TyYBAJY.jpeg",
                        "isPro": false,
                        "fullname": "Ruohao Guo",
                        "user": "ruohao",
                        "type": "user"
                    },
                    "name": "Ruohao Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:28:48.360Z",
                    "hidden": false
                },
                {
                    "_id": "68df64e7df49fb0df1e03da4",
                    "name": "Afshin Oroojlooy",
                    "hidden": false
                },
                {
                    "_id": "68df64e7df49fb0df1e03da5",
                    "name": "Roshan Sridhar",
                    "hidden": false
                },
                {
                    "_id": "68df64e7df49fb0df1e03da6",
                    "name": "Miguel Ballesteros",
                    "hidden": false
                },
                {
                    "_id": "68df64e7df49fb0df1e03da7",
                    "name": "Alan Ritter",
                    "hidden": false
                },
                {
                    "_id": "68df64e7df49fb0df1e03da8",
                    "name": "Dan Roth",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T17:57:05.000Z",
            "submittedOnDailyAt": "2025-10-03T04:40:07.960Z",
            "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks",
            "submittedOnDailyBy": {
                "_id": "628711eb40423ef48fbc07af",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628711eb40423ef48fbc07af/QmW1ow6LFNxPa0TyYBAJY.jpeg",
                "isPro": false,
                "fullname": "Ruohao Guo",
                "user": "ruohao",
                "type": "user"
            },
            "summary": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns.",
            "upvotes": 17,
            "discussionId": "68df64e8df49fb0df1e03da9",
            "ai_summary": "DialTree-RPO, an on-policy reinforcement learning framework with tree search, autonomously discovers diverse multi-turn attack strategies against large language models, achieving higher attack success rates and uncovering new attack trajectories.",
            "ai_keywords": [
                "adversarial attacks",
                "multi-turn interaction",
                "reinforcement learning",
                "tree search",
                "sequential decision-making",
                "dialogue policies",
                "attack success rate"
            ]
        },
        "publishedAt": "2025-10-02T13:57:05.000Z",
        "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks",
        "summary": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02286.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "628711eb40423ef48fbc07af",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628711eb40423ef48fbc07af/QmW1ow6LFNxPa0TyYBAJY.jpeg",
            "fullname": "Ruohao Guo",
            "name": "ruohao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.01591",
            "authors": [
                {
                    "_id": "68df27e9df49fb0df1e03bb7",
                    "user": {
                        "_id": "62ffa3f8311cad266f9af236",
                        "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
                        "isPro": false,
                        "fullname": "Zhenwen Liang",
                        "user": "invokerliang",
                        "type": "user"
                    },
                    "name": "Zhenwen Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:33:10.962Z",
                    "hidden": false
                },
                {
                    "_id": "68df27e9df49fb0df1e03bb8",
                    "name": "Ruosen Li",
                    "hidden": false
                },
                {
                    "_id": "68df27e9df49fb0df1e03bb9",
                    "name": "Yujun Zhou",
                    "hidden": false
                },
                {
                    "_id": "68df27e9df49fb0df1e03bba",
                    "name": "Linfeng Song",
                    "hidden": false
                },
                {
                    "_id": "68df27e9df49fb0df1e03bbb",
                    "name": "Dian Yu",
                    "hidden": false
                },
                {
                    "_id": "68df27e9df49fb0df1e03bbc",
                    "name": "Xinya Du",
                    "hidden": false
                },
                {
                    "_id": "68df27e9df49fb0df1e03bbd",
                    "name": "Haitao Mi",
                    "hidden": false
                },
                {
                    "_id": "68df27e9df49fb0df1e03bbe",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T02:14:33.000Z",
            "submittedOnDailyAt": "2025-10-03T00:04:05.892Z",
            "title": "CLUE: Non-parametric Verification from Experience via Hidden-State\n  Clustering",
            "submittedOnDailyBy": {
                "_id": "62ffa3f8311cad266f9af236",
                "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
                "isPro": false,
                "fullname": "Zhenwen Liang",
                "user": "invokerliang",
                "type": "user"
            },
            "summary": "Assessing the quality of Large Language Model (LLM) outputs presents a\ncritical challenge. Previous methods either rely on text-level information\n(e.g., reward models, majority voting), which can overfit to superficial cues,\nor on calibrated confidence from token probabilities, which would fail on\nless-calibrated models. Yet both of these signals are, in fact, partial\nprojections of a richer source of information: the model's internal hidden\nstates. Early layers, closer to token embeddings, preserve semantic and lexical\nfeatures that underpin text-based judgments, while later layers increasingly\nalign with output logits, embedding confidence-related information. This paper\nexplores hidden states directly as a unified foundation for verification. We\nshow that the correctness of a solution is encoded as a geometrically separable\nsignature within the trajectory of hidden activations. To validate this, we\npresent Clue (Clustering and Experience-based Verification), a deliberately\nminimalist, non-parametric verifier. With no trainable parameters, CLUE only\nsummarizes each reasoning trace by an hidden state delta and classifies\ncorrectness via nearest-centroid distance to ``success'' and ``failure''\nclusters formed from past experience. The simplicity of this method highlights\nthe strength of the underlying signal. Empirically, CLUE consistently\noutperforms LLM-as-a-judge baselines and matches or exceeds modern\nconfidence-based methods in reranking candidates, improving both top-1 and\nmajority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24\nwith a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%\n(top-maj@16).",
            "upvotes": 17,
            "discussionId": "68df27e9df49fb0df1e03bbf",
            "ai_summary": "Hidden states in Large Language Models encode correctness as a separable signature, enabling a minimalist verifier (CLUE) to outperform text-level and confidence-based methods in reranking and accuracy.",
            "ai_keywords": [
                "Large Language Model",
                "hidden states",
                "token embeddings",
                "semantic features",
                "lexical features",
                "output logits",
                "confidence-related information",
                "Clue",
                "Clustering and Experience-based Verification",
                "nearest-centroid distance",
                "AIME",
                "GPQA"
            ],
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-10-01T22:14:33.000Z",
        "title": "CLUE: Non-parametric Verification from Experience via Hidden-State\n  Clustering",
        "summary": "Assessing the quality of Large Language Model (LLM) outputs presents a\ncritical challenge. Previous methods either rely on text-level information\n(e.g., reward models, majority voting), which can overfit to superficial cues,\nor on calibrated confidence from token probabilities, which would fail on\nless-calibrated models. Yet both of these signals are, in fact, partial\nprojections of a richer source of information: the model's internal hidden\nstates. Early layers, closer to token embeddings, preserve semantic and lexical\nfeatures that underpin text-based judgments, while later layers increasingly\nalign with output logits, embedding confidence-related information. This paper\nexplores hidden states directly as a unified foundation for verification. We\nshow that the correctness of a solution is encoded as a geometrically separable\nsignature within the trajectory of hidden activations. To validate this, we\npresent Clue (Clustering and Experience-based Verification), a deliberately\nminimalist, non-parametric verifier. With no trainable parameters, CLUE only\nsummarizes each reasoning trace by an hidden state delta and classifies\ncorrectness via nearest-centroid distance to ``success'' and ``failure''\nclusters formed from past experience. The simplicity of this method highlights\nthe strength of the underlying signal. Empirically, CLUE consistently\noutperforms LLM-as-a-judge baselines and matches or exceeds modern\nconfidence-based methods in reranking candidates, improving both top-1 and\nmajority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24\nwith a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%\n(top-maj@16).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01591.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62ffa3f8311cad266f9af236",
            "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
            "fullname": "Zhenwen Liang",
            "name": "invokerliang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.02250",
            "authors": [
                {
                    "_id": "68df34efdf49fb0df1e03ca7",
                    "name": "Gonzalo Gonzalez-Pumariega",
                    "hidden": false
                },
                {
                    "_id": "68df34efdf49fb0df1e03ca8",
                    "name": "Vincent Tu",
                    "hidden": false
                },
                {
                    "_id": "68df34efdf49fb0df1e03ca9",
                    "name": "Chih-Lun Lee",
                    "hidden": false
                },
                {
                    "_id": "68df34efdf49fb0df1e03caa",
                    "name": "Jiachen Yang",
                    "hidden": false
                },
                {
                    "_id": "68df34efdf49fb0df1e03cab",
                    "name": "Ang Li",
                    "hidden": false
                },
                {
                    "_id": "68df34efdf49fb0df1e03cac",
                    "user": {
                        "_id": "64679a226192d39142245e5e",
                        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
                        "isPro": false,
                        "fullname": "Xin Eric Wang",
                        "user": "xw-eric",
                        "type": "user"
                    },
                    "name": "Xin Eric Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:29:57.752Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T17:37:08.000Z",
            "submittedOnDailyAt": "2025-10-03T01:00:35.455Z",
            "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
            "submittedOnDailyBy": {
                "_id": "64679a226192d39142245e5e",
                "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
                "isPro": false,
                "fullname": "Xin Eric Wang",
                "user": "xw-eric",
                "type": "user"
            },
            "summary": "Computer-use agents (CUAs) hold promise for automating everyday digital\ntasks, but their unreliability and high variance hinder their application to\nlong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method\nthat scales over agents by generating multiple rollouts and selecting among\nthem using behavior narratives that describe the agents' rollouts. It enables\nboth wide exploration and principled trajectory selection, substantially\nimproving robustness and success rates. On OSWorld, our bBoN scaling method\nestablishes a new state of the art (SoTA) at 69.9%, significantly outperforming\nprior methods and approaching human-level performance at 72%, with\ncomprehensive ablations validating key design choices. We further demonstrate\nstrong generalization results to different operating systems on\nWindowsAgentArena and AndroidWorld. Crucially, our results highlight the\nunreasonable effectiveness of scaling CUAs, when you do it right: effective\nscaling requires structured trajectory understanding and selection, and bBoN\nprovides a practical framework to achieve this.",
            "upvotes": 16,
            "discussionId": "68df34efdf49fb0df1e03cad",
            "projectPage": "https://www.simular.ai/articles/agent-s3",
            "githubRepo": "https://github.com/simular-ai/Agent-S/",
            "ai_summary": "Behavior Best-of-N (bBoN) improves the reliability and success rates of computer-use agents by generating and selecting among multiple rollouts using behavior narratives, achieving state-of-the-art performance on OSWorld and strong generalization to different operating systems.",
            "ai_keywords": [
                "Behavior Best-of-N",
                "bBoN",
                "rollouts",
                "behavior narratives",
                "trajectory selection",
                "OSWorld",
                "WindowsAgentArena",
                "AndroidWorld"
            ],
            "githubStars": 6373,
            "organization": {
                "_id": "63f9b97fac8368a4dce39668",
                "name": "simular-ai",
                "fullname": "Simular",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f9b921aa034447f0ded85f/cwz8V-HNY3JjSdhGdSFkY.png"
            }
        },
        "publishedAt": "2025-10-02T13:37:08.000Z",
        "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
        "summary": "Computer-use agents (CUAs) hold promise for automating everyday digital\ntasks, but their unreliability and high variance hinder their application to\nlong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method\nthat scales over agents by generating multiple rollouts and selecting among\nthem using behavior narratives that describe the agents' rollouts. It enables\nboth wide exploration and principled trajectory selection, substantially\nimproving robustness and success rates. On OSWorld, our bBoN scaling method\nestablishes a new state of the art (SoTA) at 69.9%, significantly outperforming\nprior methods and approaching human-level performance at 72%, with\ncomprehensive ablations validating key design choices. We further demonstrate\nstrong generalization results to different operating systems on\nWindowsAgentArena and AndroidWorld. Crucially, our results highlight the\nunreasonable effectiveness of scaling CUAs, when you do it right: effective\nscaling requires structured trajectory understanding and selection, and bBoN\nprovides a practical framework to achieve this.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02250.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64679a226192d39142245e5e",
            "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
            "fullname": "Xin Eric Wang",
            "name": "xw-eric",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "63f9b97fac8368a4dce39668",
            "name": "simular-ai",
            "fullname": "Simular",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f9b921aa034447f0ded85f/cwz8V-HNY3JjSdhGdSFkY.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.01265",
            "authors": [
                {
                    "_id": "68df26d8df49fb0df1e03ba8",
                    "name": "Ali Hatamizadeh",
                    "hidden": false
                },
                {
                    "_id": "68df26d8df49fb0df1e03ba9",
                    "user": {
                        "_id": "6338dd1776421c0543150467",
                        "avatarUrl": "/avatars/4539dcec644e40be33f4a0d419fa66cb.svg",
                        "isPro": false,
                        "fullname": "Syeda Nahida Akter",
                        "user": "SieraL",
                        "type": "user"
                    },
                    "name": "Syeda Nahida Akter",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:33:13.536Z",
                    "hidden": false
                },
                {
                    "_id": "68df26d8df49fb0df1e03baa",
                    "name": "Shrimai Prabhumoye",
                    "hidden": false
                },
                {
                    "_id": "68df26d8df49fb0df1e03bab",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "68df26d8df49fb0df1e03bac",
                    "name": "Mostofa Patwary",
                    "hidden": false
                },
                {
                    "_id": "68df26d8df49fb0df1e03bad",
                    "name": "Mohammad Shoeybi",
                    "hidden": false
                },
                {
                    "_id": "68df26d8df49fb0df1e03bae",
                    "name": "Bryan Catanzaro",
                    "hidden": false
                },
                {
                    "_id": "68df26d8df49fb0df1e03baf",
                    "name": "Yejin Choi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T17:53:54.000Z",
            "submittedOnDailyAt": "2025-10-03T00:55:02.431Z",
            "title": "RLP: Reinforcement as a Pretraining Objective",
            "submittedOnDailyBy": {
                "_id": "64414b62603214724ebd2636",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64414b62603214724ebd2636/x9JVcJRZKZE7hdEII1JRR.jpeg",
                "isPro": false,
                "fullname": "Ali",
                "user": "ahatamiz",
                "type": "user"
            },
            "summary": "The dominant paradigm for training large reasoning models starts with\npre-training using next-token prediction loss on vast amounts of data.\nReinforcement learning, while powerful in scaling reasoning, is introduced only\nas the very last phase of post-training, preceded by supervised fine-tuning.\nWhile dominant, is this an optimal way of training? In this paper, we present\nRLP, an information-driven reinforcement pretraining objective, that brings the\ncore spirit of reinforcement learning -- exploration -- to the last phase of\npretraining. The key idea is to treat chain-of-thought as an exploratory\naction, with rewards computed based on the information gain it provides for\npredicting future tokens. This training objective essentially encourages the\nmodel to think for itself before predicting what comes next, thus teaching an\nindependent thinking behavior earlier in the pretraining. More concretely, the\nreward signal measures the increase in log-likelihood of the next token when\nconditioning on both context and a sampled reasoning chain, compared to\nconditioning on context alone. This approach yields a verifier-free dense\nreward signal, allowing for efficient training for the full document stream\nduring pretraining. Specifically, RLP reframes reinforcement learning for\nreasoning as a pretraining objective on ordinary text, bridging the gap between\nnext-token prediction and the emergence of useful chain-of-thought reasoning.\nPretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an\neight-benchmark math-and-science suite by 19%. With identical post-training,\nthe gains compound, with the largest improvements on reasoning-heavy tasks such\nas AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2\nincreases the overall average from 42.81% to 61.32% and raises the average on\nscientific reasoning by 23%, demonstrating scalability across architectures and\nmodel sizes.",
            "upvotes": 16,
            "discussionId": "68df26d8df49fb0df1e03bb0",
            "projectPage": "https://t.co/6PNJYfiAoJ",
            "githubRepo": "https://github.com/NVlabs/RLP",
            "ai_summary": "RLP, an information-driven reinforcement pretraining objective, enhances reasoning models by integrating exploration into pretraining, leading to significant performance improvements across various benchmarks.",
            "ai_keywords": [
                "reinforcement learning",
                "pre-training",
                "next-token prediction",
                "chain-of-thought",
                "information gain",
                "log-likelihood",
                "dense reward signal",
                "Qwen3-1.7B-Base",
                "AIME25",
                "MMLU-Pro",
                "Nemotron-Nano-12B-v2",
                "scientific reasoning"
            ],
            "githubStars": 76,
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-09-26T13:53:54.000Z",
        "title": "RLP: Reinforcement as a Pretraining Objective",
        "summary": "The dominant paradigm for training large reasoning models starts with\npre-training using next-token prediction loss on vast amounts of data.\nReinforcement learning, while powerful in scaling reasoning, is introduced only\nas the very last phase of post-training, preceded by supervised fine-tuning.\nWhile dominant, is this an optimal way of training? In this paper, we present\nRLP, an information-driven reinforcement pretraining objective, that brings the\ncore spirit of reinforcement learning -- exploration -- to the last phase of\npretraining. The key idea is to treat chain-of-thought as an exploratory\naction, with rewards computed based on the information gain it provides for\npredicting future tokens. This training objective essentially encourages the\nmodel to think for itself before predicting what comes next, thus teaching an\nindependent thinking behavior earlier in the pretraining. More concretely, the\nreward signal measures the increase in log-likelihood of the next token when\nconditioning on both context and a sampled reasoning chain, compared to\nconditioning on context alone. This approach yields a verifier-free dense\nreward signal, allowing for efficient training for the full document stream\nduring pretraining. Specifically, RLP reframes reinforcement learning for\nreasoning as a pretraining objective on ordinary text, bridging the gap between\nnext-token prediction and the emergence of useful chain-of-thought reasoning.\nPretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an\neight-benchmark math-and-science suite by 19%. With identical post-training,\nthe gains compound, with the largest improvements on reasoning-heavy tasks such\nas AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2\nincreases the overall average from 42.81% to 61.32% and raises the average on\nscientific reasoning by 23%, demonstrating scalability across architectures and\nmodel sizes.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01265.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64414b62603214724ebd2636",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64414b62603214724ebd2636/x9JVcJRZKZE7hdEII1JRR.jpeg",
            "fullname": "Ali",
            "name": "ahatamiz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.02240",
            "authors": [
                {
                    "_id": "68df4813df49fb0df1e03d44",
                    "user": {
                        "_id": "67a4a26d5e65aa63c6d30e68",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a4a26d5e65aa63c6d30e68/GtodlJGw-_IL2DTXQTucz.jpeg",
                        "isPro": false,
                        "fullname": "Sicheng Feng",
                        "user": "FSCCS",
                        "type": "user"
                    },
                    "name": "Sicheng Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:29:12.265Z",
                    "hidden": false
                },
                {
                    "_id": "68df4813df49fb0df1e03d45",
                    "user": {
                        "_id": "67c9af33a947280a4afede74",
                        "avatarUrl": "/avatars/23157a9d305aec17564fd229b84a2ca3.svg",
                        "isPro": false,
                        "fullname": "Kaiwen TUO",
                        "user": "KaiwenTUO",
                        "type": "user"
                    },
                    "name": "Kaiwen Tuo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:29:05.104Z",
                    "hidden": false
                },
                {
                    "_id": "68df4813df49fb0df1e03d46",
                    "user": {
                        "_id": "66863d26e2b71e3d09189ae9",
                        "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
                        "isPro": false,
                        "fullname": "Song Wang",
                        "user": "songw-zju",
                        "type": "user"
                    },
                    "name": "Song Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:29:07.845Z",
                    "hidden": false
                },
                {
                    "_id": "68df4813df49fb0df1e03d47",
                    "name": "Lingdong Kong",
                    "hidden": false
                },
                {
                    "_id": "68df4813df49fb0df1e03d48",
                    "name": "Jianke Zhu",
                    "hidden": false
                },
                {
                    "_id": "68df4813df49fb0df1e03d49",
                    "name": "Huan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T17:29:46.000Z",
            "submittedOnDailyAt": "2025-10-03T02:28:45.311Z",
            "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via\n  Multi-Stage Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "66863d26e2b71e3d09189ae9",
                "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
                "isPro": false,
                "fullname": "Song Wang",
                "user": "songw-zju",
                "type": "user"
            },
            "summary": "Fine-grained visual reasoning remains a core challenge for multimodal large\nlanguage models (MLLMs). The recently introduced ReasonMap highlights this gap\nby showing that even advanced MLLMs struggle with spatial reasoning in\nstructured and information-rich settings such as transit maps, a task of clear\npractical and scientific importance. However, standard reinforcement learning\n(RL) on such tasks is impeded by sparse rewards and unstable optimization. To\naddress this, we first construct ReasonMap-Plus, an extended dataset that\nintroduces dense reward signals through Visual Question Answering (VQA) tasks,\nenabling effective cold-start training of fine-grained visual understanding\nskills. Next, we propose RewardMap, a multi-stage RL framework designed to\nimprove both visual understanding and reasoning capabilities of MLLMs.\nRewardMap incorporates two key designs. First, we introduce a difficulty-aware\nreward design that incorporates detail rewards, directly tackling the sparse\nrewards while providing richer supervision. Second, we propose a multi-stage RL\nscheme that bootstraps training from simple perception to complex reasoning\ntasks, offering a more effective cold-start strategy than conventional\nSupervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus\ndemonstrate that each component of RewardMap contributes to consistent\nperformance gains, while their combination yields the best results. Moreover,\nmodels trained with RewardMap achieve an average improvement of 3.47% across 6\nbenchmarks spanning spatial reasoning, fine-grained visual reasoning, and\ngeneral tasks beyond transit maps, underscoring enhanced visual understanding\nand reasoning capabilities.",
            "upvotes": 13,
            "discussionId": "68df4814df49fb0df1e03d4a",
            "projectPage": "https://fscdc.github.io/RewardMap",
            "githubRepo": "https://github.com/fscdc/RewardMap",
            "ai_summary": "RewardMap, a multi-stage reinforcement learning framework, enhances multimodal large language models' visual understanding and reasoning skills through dense reward signals and a difficulty-aware reward design.",
            "ai_keywords": [
                "ReasonMap",
                "ReasonMap-Plus",
                "Visual Question Answering (VQA)",
                "reinforcement learning (RL)",
                "sparse rewards",
                "unstable optimization",
                "difficulty-aware reward design",
                "multi-stage RL",
                "cold-start training",
                "Supervised Fine-Tuning (SFT)",
                "spatial reasoning",
                "fine-grained visual reasoning"
            ],
            "githubStars": 18,
            "organization": {
                "_id": "67d81e5e25ad2831362ec592",
                "name": "WestlakeUniversity",
                "fullname": "Westlake University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62b624f3b52bef716e248fd7/80xTZ-_peWYICJr1JDkOw.png"
            }
        },
        "publishedAt": "2025-10-02T13:29:46.000Z",
        "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via\n  Multi-Stage Reinforcement Learning",
        "summary": "Fine-grained visual reasoning remains a core challenge for multimodal large\nlanguage models (MLLMs). The recently introduced ReasonMap highlights this gap\nby showing that even advanced MLLMs struggle with spatial reasoning in\nstructured and information-rich settings such as transit maps, a task of clear\npractical and scientific importance. However, standard reinforcement learning\n(RL) on such tasks is impeded by sparse rewards and unstable optimization. To\naddress this, we first construct ReasonMap-Plus, an extended dataset that\nintroduces dense reward signals through Visual Question Answering (VQA) tasks,\nenabling effective cold-start training of fine-grained visual understanding\nskills. Next, we propose RewardMap, a multi-stage RL framework designed to\nimprove both visual understanding and reasoning capabilities of MLLMs.\nRewardMap incorporates two key designs. First, we introduce a difficulty-aware\nreward design that incorporates detail rewards, directly tackling the sparse\nrewards while providing richer supervision. Second, we propose a multi-stage RL\nscheme that bootstraps training from simple perception to complex reasoning\ntasks, offering a more effective cold-start strategy than conventional\nSupervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus\ndemonstrate that each component of RewardMap contributes to consistent\nperformance gains, while their combination yields the best results. Moreover,\nmodels trained with RewardMap achieve an average improvement of 3.47% across 6\nbenchmarks spanning spatial reasoning, fine-grained visual reasoning, and\ngeneral tasks beyond transit maps, underscoring enhanced visual understanding\nand reasoning capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02240.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66863d26e2b71e3d09189ae9",
            "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
            "fullname": "Song Wang",
            "name": "songw-zju",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "67d81e5e25ad2831362ec592",
            "name": "WestlakeUniversity",
            "fullname": "Westlake University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62b624f3b52bef716e248fd7/80xTZ-_peWYICJr1JDkOw.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.02190",
            "authors": [
                {
                    "_id": "68df3194df49fb0df1e03c4e",
                    "name": "Yang Yao",
                    "hidden": false
                },
                {
                    "_id": "68df3194df49fb0df1e03c4f",
                    "name": "Yixu Wang",
                    "hidden": false
                },
                {
                    "_id": "68df3194df49fb0df1e03c50",
                    "name": "Yuxuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68df3194df49fb0df1e03c51",
                    "name": "Yi Lu",
                    "hidden": false
                },
                {
                    "_id": "68df3194df49fb0df1e03c52",
                    "name": "Tianle Gu",
                    "hidden": false
                },
                {
                    "_id": "68df3194df49fb0df1e03c53",
                    "name": "Lingyu Li",
                    "hidden": false
                },
                {
                    "_id": "68df3194df49fb0df1e03c54",
                    "name": "Dingyi Zhao",
                    "hidden": false
                },
                {
                    "_id": "68df3194df49fb0df1e03c55",
                    "name": "Keming Wu",
                    "hidden": false
                },
                {
                    "_id": "68df3194df49fb0df1e03c56",
                    "name": "Haozhe Wang",
                    "hidden": false
                },
                {
                    "_id": "68df3194df49fb0df1e03c57",
                    "name": "Ping Nie",
                    "hidden": false
                },
                {
                    "_id": "68df3194df49fb0df1e03c58",
                    "name": "Yan Teng",
                    "hidden": false
                },
                {
                    "_id": "68df3194df49fb0df1e03c59",
                    "name": "Yingchun Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T16:40:02.000Z",
            "submittedOnDailyAt": "2025-10-03T00:45:32.030Z",
            "title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research\n  Agents: From Answers to Reports",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Artificial intelligence is undergoing the paradigm shift from closed language\nmodels to interconnected agent systems capable of external perception and\ninformation integration. As a representative embodiment, Deep Research Agents\n(DRAs) systematically exhibit the capabilities for task decomposition,\ncross-source retrieval, multi-stage reasoning, and structured output, which\nmarkedly enhance performance on complex and open-ended tasks. However, existing\nbenchmarks remain deficient in evaluation dimensions, response formatting, and\nscoring mechanisms, limiting their capacity to assess such systems effectively.\nThis paper introduces a rigorous benchmark and a multidimensional evaluation\nframework tailored to DRAs and report-style responses. The benchmark comprises\n214 expert-curated challenging queries distributed across 10 broad thematic\ndomains, each accompanied by manually constructed reference bundles to support\ncomposite evaluation. The framework enables comprehensive evaluation of\nlong-form reports generated by DRAs, incorporating integrated scoring metrics\nfor semantic quality, topical focus, and retrieval trustworthiness. Extensive\nexperimentation confirms the superior performance of mainstream DRAs over\nweb-search-tool-augmented reasoning models, yet reveals considerable scope for\nfurther improvement. This study provides a robust foundation for capability\nassessment, architectural refinement, and paradigm advancement in DRA systems.",
            "upvotes": 13,
            "discussionId": "68df3195df49fb0df1e03c5a",
            "ai_summary": "A benchmark and evaluation framework for Deep Research Agents (DRAs) assesses their performance on complex tasks with multidimensional metrics.",
            "ai_keywords": [
                "Deep Research Agents",
                "task decomposition",
                "cross-source retrieval",
                "multi-stage reasoning",
                "structured output",
                "benchmark",
                "evaluation framework",
                "thematic domains",
                "reference bundles",
                "semantic quality",
                "topical focus",
                "retrieval trustworthiness"
            ]
        },
        "publishedAt": "2025-10-02T12:40:02.000Z",
        "title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research\n  Agents: From Answers to Reports",
        "summary": "Artificial intelligence is undergoing the paradigm shift from closed language\nmodels to interconnected agent systems capable of external perception and\ninformation integration. As a representative embodiment, Deep Research Agents\n(DRAs) systematically exhibit the capabilities for task decomposition,\ncross-source retrieval, multi-stage reasoning, and structured output, which\nmarkedly enhance performance on complex and open-ended tasks. However, existing\nbenchmarks remain deficient in evaluation dimensions, response formatting, and\nscoring mechanisms, limiting their capacity to assess such systems effectively.\nThis paper introduces a rigorous benchmark and a multidimensional evaluation\nframework tailored to DRAs and report-style responses. The benchmark comprises\n214 expert-curated challenging queries distributed across 10 broad thematic\ndomains, each accompanied by manually constructed reference bundles to support\ncomposite evaluation. The framework enables comprehensive evaluation of\nlong-form reports generated by DRAs, incorporating integrated scoring metrics\nfor semantic quality, topical focus, and retrieval trustworthiness. Extensive\nexperimentation confirms the superior performance of mainstream DRAs over\nweb-search-tool-augmented reasoning models, yet reveals considerable scope for\nfurther improvement. This study provides a robust foundation for capability\nassessment, architectural refinement, and paradigm advancement in DRA systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02190.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 118
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01284",
            "authors": [
                {
                    "_id": "68df3d06df49fb0df1e03d05",
                    "name": "Chetwin Low",
                    "hidden": false
                },
                {
                    "_id": "68df3d06df49fb0df1e03d06",
                    "name": "Weimin Wang",
                    "hidden": false
                },
                {
                    "_id": "68df3d06df49fb0df1e03d07",
                    "user": {
                        "_id": "6743e005ac35394749ca2aaf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/A4856Ek1Cd3wumFwS1_Z3.png",
                        "isPro": false,
                        "fullname": "Calder Katyal",
                        "user": "CalderKat",
                        "type": "user"
                    },
                    "name": "Calder Katyal",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:29:39.790Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62b43ffec624a43b1a1ada46/djeLgQC3e6--D6ou913st.mp4"
            ],
            "publishedAt": "2025-09-30T21:03:50.000Z",
            "submittedOnDailyAt": "2025-10-03T01:53:05.749Z",
            "title": "Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation",
            "submittedOnDailyBy": {
                "_id": "62b43ffec624a43b1a1ada46",
                "avatarUrl": "/avatars/77298e99d2797cf917fdddc6d6de46eb.svg",
                "isPro": false,
                "fullname": "weimin wang",
                "user": "weiminwang",
                "type": "user"
            },
            "summary": "Audio-video generation has often relied on complex multi-stage architectures\nor sequential synthesis of sound and visuals. We introduce Ovi, a unified\nparadigm for audio-video generation that models the two modalities as a single\ngenerative process. By using blockwise cross-modal fusion of twin-DiT modules,\nOvi achieves natural synchronization and removes the need for separate\npipelines or post hoc alignment. To facilitate fine-grained multimodal fusion\nmodeling, we initialize an audio tower with an architecture identical to that\nof a strong pretrained video model. Trained from scratch on hundreds of\nthousands of hours of raw audio, the audio tower learns to generate realistic\nsound effects, as well as speech that conveys rich speaker identity and\nemotion. Fusion is obtained by jointly training the identical video and audio\ntowers via blockwise exchange of timing (via scaled-RoPE embeddings) and\nsemantics (through bidirectional cross-attention) on a vast video corpus. Our\nmodel enables cinematic storytelling with natural speech and accurate,\ncontext-matched sound effects, producing movie-grade video clips. All the\ndemos, code and model weights are published at https://aaxwaz.github.io/Ovi",
            "upvotes": 13,
            "discussionId": "68df3d07df49fb0df1e03d08",
            "projectPage": "https://aaxwaz.github.io/Ovi",
            "githubRepo": "https://github.com/character-ai/Ovi",
            "ai_summary": "Ovi is a unified audio-video generation model using twin-DiT modules with blockwise cross-modal fusion, enabling natural synchronization and high-quality multimodal outputs.",
            "ai_keywords": [
                "twin-DiT modules",
                "blockwise cross-modal fusion",
                "scaled-RoPE embeddings",
                "bidirectional cross-attention",
                "cinematic storytelling",
                "movie-grade video clips"
            ],
            "githubStars": 74,
            "organization": {
                "_id": "641f40f26d51620635e2f5ba",
                "name": "characterai",
                "fullname": "Character.AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/UVDEosgcp2kBUfuAMW7aU.png"
            }
        },
        "publishedAt": "2025-09-30T17:03:50.000Z",
        "title": "Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation",
        "summary": "Audio-video generation has often relied on complex multi-stage architectures\nor sequential synthesis of sound and visuals. We introduce Ovi, a unified\nparadigm for audio-video generation that models the two modalities as a single\ngenerative process. By using blockwise cross-modal fusion of twin-DiT modules,\nOvi achieves natural synchronization and removes the need for separate\npipelines or post hoc alignment. To facilitate fine-grained multimodal fusion\nmodeling, we initialize an audio tower with an architecture identical to that\nof a strong pretrained video model. Trained from scratch on hundreds of\nthousands of hours of raw audio, the audio tower learns to generate realistic\nsound effects, as well as speech that conveys rich speaker identity and\nemotion. Fusion is obtained by jointly training the identical video and audio\ntowers via blockwise exchange of timing (via scaled-RoPE embeddings) and\nsemantics (through bidirectional cross-attention) on a vast video corpus. Our\nmodel enables cinematic storytelling with natural speech and accurate,\ncontext-matched sound effects, producing movie-grade video clips. All the\ndemos, code and model weights are published at https://aaxwaz.github.io/Ovi",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62b43ffec624a43b1a1ada46/djeLgQC3e6--D6ou913st.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01284.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62b43ffec624a43b1a1ada46",
            "avatarUrl": "/avatars/77298e99d2797cf917fdddc6d6de46eb.svg",
            "fullname": "weimin wang",
            "name": "weiminwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "641f40f26d51620635e2f5ba",
            "name": "characterai",
            "fullname": "Character.AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/UVDEosgcp2kBUfuAMW7aU.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.02294",
            "authors": [
                {
                    "_id": "68df2abedf49fb0df1e03bdb",
                    "user": {
                        "_id": "6430bdd8cd31d174a9f900fb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
                        "isPro": false,
                        "fullname": "Ziyin Zhang",
                        "user": "Geralt-Targaryen",
                        "type": "user"
                    },
                    "name": "Ziyin Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:33:05.611Z",
                    "hidden": false
                },
                {
                    "_id": "68df2abedf49fb0df1e03bdc",
                    "name": "Zihan Liao",
                    "hidden": false
                },
                {
                    "_id": "68df2abedf49fb0df1e03bdd",
                    "name": "Hang Yu",
                    "hidden": false
                },
                {
                    "_id": "68df2abedf49fb0df1e03bde",
                    "name": "Peng Di",
                    "hidden": false
                },
                {
                    "_id": "68df2abedf49fb0df1e03bdf",
                    "name": "Rui Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T17:58:49.000Z",
            "submittedOnDailyAt": "2025-10-03T00:33:13.324Z",
            "title": "F2LLM Technical Report: Matching SOTA Embedding Performance with 6\n  Million Open-Source Data",
            "submittedOnDailyBy": {
                "_id": "6430bdd8cd31d174a9f900fb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
                "isPro": false,
                "fullname": "Ziyin Zhang",
                "user": "Geralt-Targaryen",
                "type": "user"
            },
            "summary": "We introduce F2LLM - Foundation to Feature Large Language Models, a suite of\nstate-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike\nprevious top-ranking embedding models that require massive contrastive\npretraining, sophisticated training pipelines, and costly synthetic training\ndata, F2LLM is directly finetuned from foundation models on 6 million\nquery-document-negative tuples curated from open-source, non-synthetic\ndatasets, striking a strong balance between training cost, model size, and\nembedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd\namong models with approximately 4B parameters and 7th overall, while F2LLM-1.7B\nranks 1st among models in the 1B-2B size range. To facilitate future research\nin the field, we release the models, training dataset, and code, positioning\nF2LLM as a strong, reproducible, and budget-friendly baseline for future works.",
            "upvotes": 12,
            "discussionId": "68df2abedf49fb0df1e03be0",
            "githubRepo": "https://github.com/codefuse-ai/CodeFuse-Embeddings/tree/main/F2LLM",
            "ai_summary": "F2LLM, a suite of large language models, achieves high embedding performance with efficient fine-tuning from foundation models using open-source datasets.",
            "ai_keywords": [
                "embedding models",
                "foundation models",
                "contrastive pretraining",
                "training pipelines",
                "synthetic training data",
                "query-document-negative tuples",
                "MTEB English leaderboard",
                "parameter-efficient fine-tuning"
            ],
            "githubStars": 25,
            "organization": {
                "_id": "64f97f402003abb61b3d68de",
                "name": "codefuse-ai",
                "fullname": "CodeFuse AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ed861f254de0e729dba8f1/IxbJCzRWm0Ov645jLyupL.png"
            }
        },
        "publishedAt": "2025-10-02T13:58:49.000Z",
        "title": "F2LLM Technical Report: Matching SOTA Embedding Performance with 6\n  Million Open-Source Data",
        "summary": "We introduce F2LLM - Foundation to Feature Large Language Models, a suite of\nstate-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike\nprevious top-ranking embedding models that require massive contrastive\npretraining, sophisticated training pipelines, and costly synthetic training\ndata, F2LLM is directly finetuned from foundation models on 6 million\nquery-document-negative tuples curated from open-source, non-synthetic\ndatasets, striking a strong balance between training cost, model size, and\nembedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd\namong models with approximately 4B parameters and 7th overall, while F2LLM-1.7B\nranks 1st among models in the 1B-2B size range. To facilitate future research\nin the field, we release the models, training dataset, and code, positioning\nF2LLM as a strong, reproducible, and budget-friendly baseline for future works.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02294.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6430bdd8cd31d174a9f900fb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
            "fullname": "Ziyin Zhang",
            "name": "Geralt-Targaryen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "64f97f402003abb61b3d68de",
            "name": "codefuse-ai",
            "fullname": "CodeFuse AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ed861f254de0e729dba8f1/IxbJCzRWm0Ov645jLyupL.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.02253",
            "authors": [
                {
                    "_id": "68df2c09df49fb0df1e03bf4",
                    "user": {
                        "_id": "66923522e32997bdf7f37462",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-RE4FqyZnDF9PcpY6ZBGH.png",
                        "isPro": false,
                        "fullname": "Zihan Zhou",
                        "user": "Edennnnn",
                        "type": "user"
                    },
                    "name": "Zihan Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:32:56.222Z",
                    "hidden": false
                },
                {
                    "_id": "68df2c09df49fb0df1e03bf5",
                    "user": {
                        "_id": "631c4a23aa346997917bcb89",
                        "avatarUrl": "/avatars/4a562ba78e6be289049027c27798cc6e.svg",
                        "isPro": false,
                        "fullname": "Shilin Lu",
                        "user": "Shilin-LU",
                        "type": "user"
                    },
                    "name": "Shilin Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:32:53.604Z",
                    "hidden": false
                },
                {
                    "_id": "68df2c09df49fb0df1e03bf6",
                    "user": {
                        "_id": "68debc040c8d2ad3d623a17e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/74XAnVrry28utBLJx3RbV.png",
                        "isPro": false,
                        "fullname": "Shuli Leng",
                        "user": "Leahahah",
                        "type": "user"
                    },
                    "name": "Shuli Leng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:30:08.812Z",
                    "hidden": false
                },
                {
                    "_id": "68df2c09df49fb0df1e03bf7",
                    "name": "Shaocong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68df2c09df49fb0df1e03bf8",
                    "name": "Zhuming Lian",
                    "hidden": false
                },
                {
                    "_id": "68df2c09df49fb0df1e03bf9",
                    "name": "Xinlei Yu",
                    "hidden": false
                },
                {
                    "_id": "68df2c09df49fb0df1e03bfa",
                    "name": "Adams Wai-Kin Kong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T17:39:13.000Z",
            "submittedOnDailyAt": "2025-10-03T00:22:00.581Z",
            "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag\n  Editing",
            "submittedOnDailyBy": {
                "_id": "631c4a23aa346997917bcb89",
                "avatarUrl": "/avatars/4a562ba78e6be289049027c27798cc6e.svg",
                "isPro": false,
                "fullname": "Shilin Lu",
                "user": "Shilin-LU",
                "type": "user"
            },
            "summary": "Drag-based image editing has long suffered from distortions in the target\nregion, largely because the priors of earlier base models, Stable Diffusion,\nare insufficient to project optimized latents back onto the natural image\nmanifold. With the shift from UNet-based DDPMs to more scalable DiT with flow\nmatching (e.g., SD3.5, FLUX), generative priors have become significantly\nstronger, enabling advances across diverse editing tasks. However, drag-based\nediting has yet to benefit from these stronger priors. This work proposes the\nfirst framework to effectively harness FLUX's rich prior for drag-based\nediting, dubbed DragFlow, achieving substantial gains over baselines. We first\nshow that directly applying point-based drag editing to DiTs performs poorly:\nunlike the highly compressed features of UNets, DiT features are insufficiently\nstructured to provide reliable guidance for point-wise motion supervision. To\novercome this limitation, DragFlow introduces a region-based editing paradigm,\nwhere affine transformations enable richer and more consistent feature\nsupervision. Additionally, we integrate pretrained open-domain personalization\nadapters (e.g., IP-Adapter) to enhance subject consistency, while preserving\nbackground fidelity through gradient mask-based hard constraints. Multimodal\nlarge language models (MLLMs) are further employed to resolve task ambiguities.\nFor evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)\nfeaturing region-level dragging instructions. Extensive experiments on\nDragBench-DR and ReD Bench show that DragFlow surpasses both point-based and\nregion-based baselines, setting a new state-of-the-art in drag-based image\nediting. Code and datasets will be publicly available upon publication.",
            "upvotes": 10,
            "discussionId": "68df2c0adf49fb0df1e03bfb",
            "ai_summary": "DragFlow leverages FLUX's strong generative priors and region-based editing with affine transformations to achieve state-of-the-art performance in drag-based image editing.",
            "ai_keywords": [
                "DragFlow",
                "FLUX",
                "flow matching",
                "DiT",
                "UNet",
                "DDPMs",
                "affine transformations",
                "pretrained open-domain personalization adapters",
                "IP-Adapter",
                "gradient mask-based hard constraints",
                "multimodal large language models",
                "MLLMs",
                "Region-based Dragging benchmark",
                "ReD Bench",
                "DragBench-DR"
            ]
        },
        "publishedAt": "2025-10-02T13:39:13.000Z",
        "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag\n  Editing",
        "summary": "Drag-based image editing has long suffered from distortions in the target\nregion, largely because the priors of earlier base models, Stable Diffusion,\nare insufficient to project optimized latents back onto the natural image\nmanifold. With the shift from UNet-based DDPMs to more scalable DiT with flow\nmatching (e.g., SD3.5, FLUX), generative priors have become significantly\nstronger, enabling advances across diverse editing tasks. However, drag-based\nediting has yet to benefit from these stronger priors. This work proposes the\nfirst framework to effectively harness FLUX's rich prior for drag-based\nediting, dubbed DragFlow, achieving substantial gains over baselines. We first\nshow that directly applying point-based drag editing to DiTs performs poorly:\nunlike the highly compressed features of UNets, DiT features are insufficiently\nstructured to provide reliable guidance for point-wise motion supervision. To\novercome this limitation, DragFlow introduces a region-based editing paradigm,\nwhere affine transformations enable richer and more consistent feature\nsupervision. Additionally, we integrate pretrained open-domain personalization\nadapters (e.g., IP-Adapter) to enhance subject consistency, while preserving\nbackground fidelity through gradient mask-based hard constraints. Multimodal\nlarge language models (MLLMs) are further employed to resolve task ambiguities.\nFor evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)\nfeaturing region-level dragging instructions. Extensive experiments on\nDragBench-DR and ReD Bench show that DragFlow surpasses both point-based and\nregion-based baselines, setting a new state-of-the-art in drag-based image\nediting. Code and datasets will be publicly available upon publication.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02253.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631c4a23aa346997917bcb89",
            "avatarUrl": "/avatars/4a562ba78e6be289049027c27798cc6e.svg",
            "fullname": "Shilin Lu",
            "name": "Shilin-LU",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.02173",
            "authors": [
                {
                    "_id": "68df5d52df49fb0df1e03d7a",
                    "user": {
                        "_id": "608abf1272b50b02c4b02865",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1619708309549-608abf1272b50b02c4b02865.jpeg",
                        "isPro": false,
                        "fullname": "Hsuan Su",
                        "user": "jacksukk",
                        "type": "user"
                    },
                    "name": "Hsuan Su",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:28:53.649Z",
                    "hidden": false
                },
                {
                    "_id": "68df5d52df49fb0df1e03d7b",
                    "name": "Ting-Yao Hu",
                    "hidden": false
                },
                {
                    "_id": "68df5d52df49fb0df1e03d7c",
                    "name": "Hema Swetha Koppula",
                    "hidden": false
                },
                {
                    "_id": "68df5d52df49fb0df1e03d7d",
                    "name": "Kundan Krishna",
                    "hidden": false
                },
                {
                    "_id": "68df5d52df49fb0df1e03d7e",
                    "name": "Hadi Pouransari",
                    "hidden": false
                },
                {
                    "_id": "68df5d52df49fb0df1e03d7f",
                    "name": "Cheng-Yu Hsieh",
                    "hidden": false
                },
                {
                    "_id": "68df5d52df49fb0df1e03d80",
                    "name": "Cem Koc",
                    "hidden": false
                },
                {
                    "_id": "68df5d52df49fb0df1e03d81",
                    "name": "Joseph Yitan Cheng",
                    "hidden": false
                },
                {
                    "_id": "68df5d52df49fb0df1e03d82",
                    "name": "Oncel Tuzel",
                    "hidden": false
                },
                {
                    "_id": "68df5d52df49fb0df1e03d83",
                    "name": "Raviteja Vemulapalli",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T16:24:28.000Z",
            "submittedOnDailyAt": "2025-10-03T03:58:53.282Z",
            "title": "Learning to Reason for Hallucination Span Detection",
            "submittedOnDailyBy": {
                "_id": "608abf1272b50b02c4b02865",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1619708309549-608abf1272b50b02c4b02865.jpeg",
                "isPro": false,
                "fullname": "Hsuan Su",
                "user": "jacksukk",
                "type": "user"
            },
            "summary": "Large language models (LLMs) often generate hallucinations -- unsupported\ncontent that undermines reliability. While most prior works frame hallucination\ndetection as a binary task, many real-world applications require identifying\nhallucinated spans, which is a multi-step decision making process. This\nnaturally raises the question of whether explicit reasoning can help the\ncomplex task of detecting hallucination spans. To answer this question, we\nfirst evaluate pretrained models with and without Chain-of-Thought (CoT)\nreasoning, and show that CoT reasoning has the potential to generate at least\none correct answer when sampled multiple times. Motivated by this, we propose\nRL4HS, a reinforcement learning framework that incentivizes reasoning with a\nspan-level reward function. RL4HS builds on Group Relative Policy Optimization\nand introduces Class-Aware Policy Optimization to mitigate reward imbalance\nissue. Experiments on the RAGTruth benchmark (summarization, question\nanswering, data-to-text) show that RL4HS surpasses pretrained reasoning models\nand supervised fine-tuning, demonstrating the necessity of reinforcement\nlearning with span-level rewards for detecting hallucination spans.",
            "upvotes": 8,
            "discussionId": "68df5d52df49fb0df1e03d84",
            "ai_summary": "A reinforcement learning framework with span-level rewards improves hallucination span detection in large language models by incentivizing reasoning.",
            "ai_keywords": [
                "Chain-of-Thought",
                "reinforcement learning",
                "span-level reward function",
                "Group Relative Policy Optimization",
                "Class-Aware Policy Optimization",
                "RAGTruth benchmark",
                "summarization",
                "question answering",
                "data-to-text",
                "hallucination span detection"
            ]
        },
        "publishedAt": "2025-10-02T12:24:28.000Z",
        "title": "Learning to Reason for Hallucination Span Detection",
        "summary": "Large language models (LLMs) often generate hallucinations -- unsupported\ncontent that undermines reliability. While most prior works frame hallucination\ndetection as a binary task, many real-world applications require identifying\nhallucinated spans, which is a multi-step decision making process. This\nnaturally raises the question of whether explicit reasoning can help the\ncomplex task of detecting hallucination spans. To answer this question, we\nfirst evaluate pretrained models with and without Chain-of-Thought (CoT)\nreasoning, and show that CoT reasoning has the potential to generate at least\none correct answer when sampled multiple times. Motivated by this, we propose\nRL4HS, a reinforcement learning framework that incentivizes reasoning with a\nspan-level reward function. RL4HS builds on Group Relative Policy Optimization\nand introduces Class-Aware Policy Optimization to mitigate reward imbalance\nissue. Experiments on the RAGTruth benchmark (summarization, question\nanswering, data-to-text) show that RL4HS surpasses pretrained reasoning models\nand supervised fine-tuning, demonstrating the necessity of reinforcement\nlearning with span-level rewards for detecting hallucination spans.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02173.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "608abf1272b50b02c4b02865",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1619708309549-608abf1272b50b02c4b02865.jpeg",
            "fullname": "Hsuan Su",
            "name": "jacksukk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.01346",
            "authors": [
                {
                    "_id": "68e00d7673e20ab577841a5f",
                    "name": "Tudor Achim",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a60",
                    "name": "Alex Best",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a61",
                    "name": "Kevin Der",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a62",
                    "name": "Mathïs Fédérico",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a63",
                    "name": "Sergei Gukov",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a64",
                    "name": "Daniel Halpern-Leister",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a65",
                    "name": "Kirsten Henningsgard",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a66",
                    "name": "Yury Kudryashov",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a67",
                    "name": "Alexander Meiburg",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a68",
                    "name": "Martin Michelsen",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a69",
                    "name": "Riley Patterson",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a6a",
                    "name": "Eric Rodriguez",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a6b",
                    "name": "Laura Scharff",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a6c",
                    "name": "Vikram Shanker",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a6d",
                    "name": "Vladmir Sicca",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a6e",
                    "name": "Hari Sowrirajan",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a6f",
                    "name": "Aidan Swope",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a70",
                    "name": "Matyas Tamas",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a71",
                    "name": "Vlad Tenev",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a72",
                    "name": "Jonathan Thomm",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a73",
                    "name": "Harold Williams",
                    "hidden": false
                },
                {
                    "_id": "68e00d7673e20ab577841a74",
                    "name": "Lawrence Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T18:21:13.000Z",
            "submittedOnDailyAt": "2025-10-03T16:24:47.608Z",
            "title": "Aristotle: IMO-level Automated Theorem Proving",
            "submittedOnDailyBy": {
                "_id": "63dec36ee742e86dc920f2cb",
                "avatarUrl": "/avatars/3c5fd4ab5e5286dbf948ca113f7cbfb4.svg",
                "isPro": false,
                "fullname": "Jonathan Thomm",
                "user": "jthomm",
                "type": "user"
            },
            "summary": "We introduce Aristotle, an AI system that combines formal verification with\ninformal reasoning, achieving gold-medal-equivalent performance on the 2025\nInternational Mathematical Olympiad problems. Aristotle integrates three main\ncomponents: a Lean proof search system, an informal reasoning system that\ngenerates and formalizes lemmas, and a dedicated geometry solver. Our system\ndemonstrates state-of-the-art performance with favorable scaling properties for\nautomated theorem proving.",
            "upvotes": 8,
            "discussionId": "68e00d7673e20ab577841a75",
            "ai_summary": "Aristotle, an AI system combining formal verification and informal reasoning, achieves top performance on International Mathematical Olympiad problems using Lean proof search, lemma generation, and a geometry solver.",
            "ai_keywords": [
                "Lean proof search",
                "formal verification",
                "informal reasoning",
                "lemmas",
                "geometry solver",
                "automated theorem proving"
            ]
        },
        "publishedAt": "2025-10-01T14:21:13.000Z",
        "title": "Aristotle: IMO-level Automated Theorem Proving",
        "summary": "We introduce Aristotle, an AI system that combines formal verification with\ninformal reasoning, achieving gold-medal-equivalent performance on the 2025\nInternational Mathematical Olympiad problems. Aristotle integrates three main\ncomponents: a Lean proof search system, an informal reasoning system that\ngenerates and formalizes lemmas, and a dedicated geometry solver. Our system\ndemonstrates state-of-the-art performance with favorable scaling properties for\nautomated theorem proving.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01346.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63dec36ee742e86dc920f2cb",
            "avatarUrl": "/avatars/3c5fd4ab5e5286dbf948ca113f7cbfb4.svg",
            "fullname": "Jonathan Thomm",
            "name": "jthomm",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01179",
            "authors": [
                {
                    "_id": "68de23376024653e8a3ed1e2",
                    "user": {
                        "_id": "653df1323479e9ebbe3eb6cc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
                        "isPro": true,
                        "fullname": "Zhangchen Xu",
                        "user": "zhangchenxu",
                        "type": "user"
                    },
                    "name": "Zhangchen Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:54:22.052Z",
                    "hidden": false
                },
                {
                    "_id": "68de23376024653e8a3ed1e3",
                    "name": "Adriana Meza Soria",
                    "hidden": false
                },
                {
                    "_id": "68de23376024653e8a3ed1e4",
                    "name": "Shawn Tan",
                    "hidden": false
                },
                {
                    "_id": "68de23376024653e8a3ed1e5",
                    "name": "Anurag Roy",
                    "hidden": false
                },
                {
                    "_id": "68de23376024653e8a3ed1e6",
                    "name": "Ashish Sunil Agrawal",
                    "hidden": false
                },
                {
                    "_id": "68de23376024653e8a3ed1e7",
                    "name": "Radha Poovendran",
                    "hidden": false
                },
                {
                    "_id": "68de23376024653e8a3ed1e8",
                    "name": "Rameswar Panda",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T17:58:03.000Z",
            "submittedOnDailyAt": "2025-10-03T00:25:08.684Z",
            "title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP\n  Environments",
            "submittedOnDailyBy": {
                "_id": "653df1323479e9ebbe3eb6cc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
                "isPro": true,
                "fullname": "Zhangchen Xu",
                "user": "zhangchenxu",
                "type": "user"
            },
            "summary": "Large Language Model (LLM) agents are rapidly emerging as powerful systems\nfor automating tasks across domains. Yet progress in the open-source community\nis constrained by the lack of high quality permissively licensed tool-agentic\ntraining data. Existing datasets are often limited in diversity, realism, and\ncomplexity, particularly regarding multi-tool and multi-turn interactions. To\naddress this gap, we introduce Toucan, the largest publicly available\ntool-agentic dataset to date, containing 1.5 million trajectories synthesized\nfrom nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work,\nToucan leverages authentic MCP environments to generate diverse, realistic, and\nchallenging tasks with trajectories involving real tool execution. Our pipeline\nfirst produces a broad spectrum of tool-use queries using five distinct models,\napplies model-based quality filtering, and then generates agentic trajectories\nwith three teacher models using two agentic frameworks. Rigorous rule-based and\nmodel-based validation ensures high-quality outputs. We also introduce three\nextension mechanisms to further diversify tasks and simulate multi-turn\nconversations. Models fine-tuned on Toucan outperform larger closed-source\ncounterparts on the BFCL V3 benchmark and push the Pareto frontier forward on\nMCP-Universe Bench.",
            "upvotes": 8,
            "discussionId": "68de23386024653e8a3ed1e9",
            "githubRepo": "https://github.com/TheAgentArk/Toucan",
            "ai_summary": "Toucan, a large publicly available tool-agentic dataset, enhances the performance of LLM agents by providing diverse, realistic, and complex multi-tool and multi-turn interactions.",
            "ai_keywords": [
                "Large Language Model (LLM)",
                "tool-agentic training data",
                "Model Context Protocols (MCPs)",
                "tool-use queries",
                "model-based quality filtering",
                "agentic trajectories",
                "teacher models",
                "agentic frameworks",
                "BFCL V3 benchmark",
                "MCP-Universe Bench"
            ],
            "githubStars": 26,
            "organization": {
                "_id": "616e7b1d75754a5d5fa455cf",
                "name": "ibm",
                "fullname": "IBM",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/637bfdf60dc13843b468ac20/9228luWRoGbZwKGxkOOsj.png"
            }
        },
        "publishedAt": "2025-10-01T13:58:03.000Z",
        "title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP\n  Environments",
        "summary": "Large Language Model (LLM) agents are rapidly emerging as powerful systems\nfor automating tasks across domains. Yet progress in the open-source community\nis constrained by the lack of high quality permissively licensed tool-agentic\ntraining data. Existing datasets are often limited in diversity, realism, and\ncomplexity, particularly regarding multi-tool and multi-turn interactions. To\naddress this gap, we introduce Toucan, the largest publicly available\ntool-agentic dataset to date, containing 1.5 million trajectories synthesized\nfrom nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work,\nToucan leverages authentic MCP environments to generate diverse, realistic, and\nchallenging tasks with trajectories involving real tool execution. Our pipeline\nfirst produces a broad spectrum of tool-use queries using five distinct models,\napplies model-based quality filtering, and then generates agentic trajectories\nwith three teacher models using two agentic frameworks. Rigorous rule-based and\nmodel-based validation ensures high-quality outputs. We also introduce three\nextension mechanisms to further diversify tasks and simulate multi-turn\nconversations. Models fine-tuned on Toucan outperform larger closed-source\ncounterparts on the BFCL V3 benchmark and push the Pareto frontier forward on\nMCP-Universe Bench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01179.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "653df1323479e9ebbe3eb6cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
            "fullname": "Zhangchen Xu",
            "name": "zhangchenxu",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "organization": {
            "_id": "616e7b1d75754a5d5fa455cf",
            "name": "ibm",
            "fullname": "IBM",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/637bfdf60dc13843b468ac20/9228luWRoGbZwKGxkOOsj.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.02295",
            "authors": [
                {
                    "_id": "68df252adf49fb0df1e03b9e",
                    "name": "Enxin Song",
                    "hidden": false
                },
                {
                    "_id": "68df252adf49fb0df1e03b9f",
                    "name": "Wenhao Chai",
                    "hidden": false
                },
                {
                    "_id": "68df252adf49fb0df1e03ba0",
                    "name": "Shusheng Yang",
                    "hidden": false
                },
                {
                    "_id": "68df252adf49fb0df1e03ba1",
                    "name": "Ethan Armand",
                    "hidden": false
                },
                {
                    "_id": "68df252adf49fb0df1e03ba2",
                    "name": "Xiaojun Shan",
                    "hidden": false
                },
                {
                    "_id": "68df252adf49fb0df1e03ba3",
                    "name": "Haiyang Xu",
                    "hidden": false
                },
                {
                    "_id": "68df252adf49fb0df1e03ba4",
                    "name": "Jianwen Xie",
                    "hidden": false
                },
                {
                    "_id": "68df252adf49fb0df1e03ba5",
                    "name": "Zhuowen Tu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T17:58:54.000Z",
            "submittedOnDailyAt": "2025-10-03T04:56:26.524Z",
            "title": "VideoNSA: Native Sparse Attention Scales Video Understanding",
            "submittedOnDailyBy": {
                "_id": "63a7b196e27a6dbd4861e275",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a7b196e27a6dbd4861e275/QGn4hdDPNuSRgZVKxzDpq.jpeg",
                "isPro": false,
                "fullname": "EnxinSong",
                "user": "Enxin",
                "type": "user"
            },
            "summary": "Video understanding in multimodal language models remains limited by context\nlength: models often miss key transition frames and struggle to maintain\ncoherence across long time scales. To address this, we adapt Native Sparse\nAttention (NSA) to video-language models. Our method, VideoNSA, adapts\nQwen2.5-VL through end-to-end training on a 216K video instruction dataset. We\nemploy a hardware-aware hybrid approach to attention, preserving dense\nattention for text, while employing NSA for video. Compared to\ntoken-compression and training-free sparse baselines, VideoNSA achieves\nimproved performance on long-video understanding, temporal reasoning, and\nspatial benchmarks. Further ablation analysis reveals four key findings: (1)\nreliable scaling to 128K tokens; (2) an optimal global-local attention\nallocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)\nthe learnable combined sparse attention help induce dynamic attention sinks.",
            "upvotes": 7,
            "discussionId": "68df252adf49fb0df1e03ba6",
            "projectPage": "https://enxinsong.com/VideoNSA-web/",
            "githubRepo": "https://github.com/Espere-1119-Song/VideoNSA",
            "ai_summary": "VideoNSA, an adaptation of Native Sparse Attention to video-language models, enhances long-video understanding and temporal reasoning through end-to-end training and a hardware-aware hybrid attention approach.",
            "ai_keywords": [
                "Native Sparse Attention",
                "NSA",
                "VideoNSA",
                "Qwen2.5-VL",
                "token-compression",
                "sparse baselines",
                "long-video understanding",
                "temporal reasoning",
                "spatial benchmarks",
                "global-local attention",
                "dynamic attention sinks"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-10-02T13:58:54.000Z",
        "title": "VideoNSA: Native Sparse Attention Scales Video Understanding",
        "summary": "Video understanding in multimodal language models remains limited by context\nlength: models often miss key transition frames and struggle to maintain\ncoherence across long time scales. To address this, we adapt Native Sparse\nAttention (NSA) to video-language models. Our method, VideoNSA, adapts\nQwen2.5-VL through end-to-end training on a 216K video instruction dataset. We\nemploy a hardware-aware hybrid approach to attention, preserving dense\nattention for text, while employing NSA for video. Compared to\ntoken-compression and training-free sparse baselines, VideoNSA achieves\nimproved performance on long-video understanding, temporal reasoning, and\nspatial benchmarks. Further ablation analysis reveals four key findings: (1)\nreliable scaling to 128K tokens; (2) an optimal global-local attention\nallocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)\nthe learnable combined sparse attention help induce dynamic attention sinks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02295.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a7b196e27a6dbd4861e275",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a7b196e27a6dbd4861e275/QGn4hdDPNuSRgZVKxzDpq.jpeg",
            "fullname": "EnxinSong",
            "name": "Enxin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 20
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.21789",
            "authors": [
                {
                    "_id": "68df677ddf49fb0df1e03dab",
                    "name": "Xinlei Yu",
                    "hidden": false
                },
                {
                    "_id": "68df677ddf49fb0df1e03dac",
                    "name": "Chengming Xu",
                    "hidden": false
                },
                {
                    "_id": "68df677ddf49fb0df1e03dad",
                    "name": "Guibin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68df677ddf49fb0df1e03dae",
                    "name": "Yongbo He",
                    "hidden": false
                },
                {
                    "_id": "68df677ddf49fb0df1e03daf",
                    "name": "Zhangquan Chen",
                    "hidden": false
                },
                {
                    "_id": "68df677ddf49fb0df1e03db0",
                    "name": "Zhucun Xue",
                    "hidden": false
                },
                {
                    "_id": "68df677ddf49fb0df1e03db1",
                    "name": "Jiangning Zhang",
                    "hidden": false
                },
                {
                    "_id": "68df677ddf49fb0df1e03db2",
                    "name": "Yue Liao",
                    "hidden": false
                },
                {
                    "_id": "68df677ddf49fb0df1e03db3",
                    "name": "Xiaobin Hu",
                    "hidden": false
                },
                {
                    "_id": "68df677ddf49fb0df1e03db4",
                    "name": "Yu-Gang Jiang",
                    "hidden": false
                },
                {
                    "_id": "68df677ddf49fb0df1e03db5",
                    "name": "Shuicheng Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T02:43:24.000Z",
            "submittedOnDailyAt": "2025-10-03T04:38:13.318Z",
            "title": "Visual Multi-Agent System: Mitigating Hallucination Snowballing via\n  Visual Flow",
            "submittedOnDailyBy": {
                "_id": "67d63e228d5c7a132cbcf39b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ynwA3Sya5irwMRCmSeLiC.png",
                "isPro": false,
                "fullname": "neil yu",
                "user": "yxl66666",
                "type": "user"
            },
            "summary": "Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables\nchallenging tasks but suffers from a novel failure term, multi-agent visual\nhallucination snowballing, where hallucinations are seeded in a single agent\nand amplified by following ones due to the over-reliance on textual flow to\nrelay visual information. Through turn-, layer-, and token-wise attention\nanalyses, we provide detailed insights into the essence of hallucination\nsnowballing regarding the reduction of visual attention allocation. It leads us\nto identify a subset of vision tokens with a unimodal attention peak in middle\nlayers that best preserve visual evidence but gradually diminish in deeper\nagent turns, resulting in the visual hallucination snowballing in MAS. Thus, we\npropose ViF, a lightweight, plug-and-play mitigation paradigm that relays\ninter-agent messages with Visual Flow powered by the selected visual relay\ntokens and applies attention reallocation to amplify this pattern. The\nexperiment results demonstrate that our method markedly reduces hallucination\nsnowballing, consistently improving the performance across eight benchmarks\nbased on four common MAS structures and ten base models. The source code will\nbe available at: https://github.com/YU-deep/ViF.git.",
            "upvotes": 7,
            "discussionId": "68df677edf49fb0df1e03db6",
            "githubRepo": "https://github.com/YU-deep/ViF",
            "ai_summary": "ViF mitigates visual hallucination snowballing in Multi-Agent Systems by enhancing visual attention and message relay through selected visual tokens.",
            "ai_keywords": [
                "Multi-Agent System",
                "Visual Language Models",
                "visual hallucination snowballing",
                "turn-wise attention",
                "layer-wise attention",
                "token-wise attention",
                "visual attention allocation",
                "vision tokens",
                "unimodal attention peak",
                "Visual Flow",
                "attention reallocation"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-09-25T22:43:24.000Z",
        "title": "Visual Multi-Agent System: Mitigating Hallucination Snowballing via\n  Visual Flow",
        "summary": "Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables\nchallenging tasks but suffers from a novel failure term, multi-agent visual\nhallucination snowballing, where hallucinations are seeded in a single agent\nand amplified by following ones due to the over-reliance on textual flow to\nrelay visual information. Through turn-, layer-, and token-wise attention\nanalyses, we provide detailed insights into the essence of hallucination\nsnowballing regarding the reduction of visual attention allocation. It leads us\nto identify a subset of vision tokens with a unimodal attention peak in middle\nlayers that best preserve visual evidence but gradually diminish in deeper\nagent turns, resulting in the visual hallucination snowballing in MAS. Thus, we\npropose ViF, a lightweight, plug-and-play mitigation paradigm that relays\ninter-agent messages with Visual Flow powered by the selected visual relay\ntokens and applies attention reallocation to amplify this pattern. The\nexperiment results demonstrate that our method markedly reduces hallucination\nsnowballing, consistently improving the performance across eight benchmarks\nbased on four common MAS structures and ten base models. The source code will\nbe available at: https://github.com/YU-deep/ViF.git.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21789.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67d63e228d5c7a132cbcf39b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ynwA3Sya5irwMRCmSeLiC.png",
            "fullname": "neil yu",
            "name": "yxl66666",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.00428",
            "authors": [
                {
                    "_id": "68de602e70ada21878c74fff",
                    "name": "Seongjae Kang",
                    "hidden": false
                },
                {
                    "_id": "68de602e70ada21878c75000",
                    "name": "Dong Bok Lee",
                    "hidden": false
                },
                {
                    "_id": "68de602e70ada21878c75001",
                    "user": {
                        "_id": "68df412d44f83e2a9773765d",
                        "avatarUrl": "/avatars/3c8a57df03972f57b9b1c6484d3ea7e2.svg",
                        "isPro": false,
                        "fullname": "Juho Jung",
                        "user": "juhojung",
                        "type": "user"
                    },
                    "name": "Juho Jung",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:53:00.709Z",
                    "hidden": false
                },
                {
                    "_id": "68de602e70ada21878c75002",
                    "name": "Dongseop Kim",
                    "hidden": false
                },
                {
                    "_id": "68de602e70ada21878c75003",
                    "name": "Won Hwa Kim",
                    "hidden": false
                },
                {
                    "_id": "68de602e70ada21878c75004",
                    "name": "Sunghoon Joo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T02:14:23.000Z",
            "submittedOnDailyAt": "2025-10-03T00:40:30.421Z",
            "title": "Automated Structured Radiology Report Generation with Rich Clinical\n  Context",
            "submittedOnDailyBy": {
                "_id": "6357a08f8ed056fa1ccd3b38",
                "avatarUrl": "/avatars/07d4ca8f3197a6945ad71e6150801135.svg",
                "isPro": false,
                "fullname": "Seongjae Kang",
                "user": "erjui",
                "type": "user"
            },
            "summary": "Automated structured radiology report generation (SRRG) from chest X-ray\nimages offers significant potential to reduce workload of radiologists by\ngenerating reports in structured formats that ensure clarity, consistency, and\nadherence to clinical reporting standards. While radiologists effectively\nutilize available clinical contexts in their diagnostic reasoning, existing\nSRRG systems overlook these essential elements. This fundamental gap leads to\ncritical problems including temporal hallucinations when referencing\nnon-existent clinical contexts. To address these limitations, we propose\ncontextualized SRRG (C-SRRG) that comprehensively incorporates rich clinical\ncontext for SRRG. We curate C-SRRG dataset by integrating comprehensive\nclinical context encompassing 1) multi-view X-ray images, 2) clinical\nindication, 3) imaging techniques, and 4) prior studies with corresponding\ncomparisons based on patient histories. Through extensive benchmarking with\nstate-of-the-art multimodal large language models, we demonstrate that\nincorporating clinical context with the proposed C-SRRG significantly improves\nreport generation quality. We publicly release dataset, code, and checkpoints\nto facilitate future research for clinically-aligned automated RRG at\nhttps://github.com/vuno/contextualized-srrg.",
            "upvotes": 6,
            "discussionId": "68de602e70ada21878c75005",
            "githubRepo": "https://github.com/vuno/contextualized-srrg",
            "ai_summary": "Incorporating clinical context into automated structured radiology report generation improves report quality by addressing temporal hallucinations and utilizing comprehensive patient data.",
            "ai_keywords": [
                "structured radiology report generation",
                "chest X-ray images",
                "clinical context",
                "temporal hallucinations",
                "multimodal large language models",
                "multi-view X-ray images",
                "clinical indication",
                "imaging techniques",
                "prior studies",
                "patient histories"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-09-30T22:14:23.000Z",
        "title": "Automated Structured Radiology Report Generation with Rich Clinical\n  Context",
        "summary": "Automated structured radiology report generation (SRRG) from chest X-ray\nimages offers significant potential to reduce workload of radiologists by\ngenerating reports in structured formats that ensure clarity, consistency, and\nadherence to clinical reporting standards. While radiologists effectively\nutilize available clinical contexts in their diagnostic reasoning, existing\nSRRG systems overlook these essential elements. This fundamental gap leads to\ncritical problems including temporal hallucinations when referencing\nnon-existent clinical contexts. To address these limitations, we propose\ncontextualized SRRG (C-SRRG) that comprehensively incorporates rich clinical\ncontext for SRRG. We curate C-SRRG dataset by integrating comprehensive\nclinical context encompassing 1) multi-view X-ray images, 2) clinical\nindication, 3) imaging techniques, and 4) prior studies with corresponding\ncomparisons based on patient histories. Through extensive benchmarking with\nstate-of-the-art multimodal large language models, we demonstrate that\nincorporating clinical context with the proposed C-SRRG significantly improves\nreport generation quality. We publicly release dataset, code, and checkpoints\nto facilitate future research for clinically-aligned automated RRG at\nhttps://github.com/vuno/contextualized-srrg.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00428.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6357a08f8ed056fa1ccd3b38",
            "avatarUrl": "/avatars/07d4ca8f3197a6945ad71e6150801135.svg",
            "fullname": "Seongjae Kang",
            "name": "erjui",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.26376",
            "authors": [
                {
                    "_id": "68df3bbfdf49fb0df1e03cdc",
                    "user": {
                        "_id": "6570450a78d7aca0c361a177",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/MX7jHhTQwLs-BvYIu5rqb.jpeg",
                        "isPro": false,
                        "fullname": "Harold Chen",
                        "user": "Harold328",
                        "type": "user"
                    },
                    "name": "Harold Haodong Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:29:43.753Z",
                    "hidden": false
                },
                {
                    "_id": "68df3bbfdf49fb0df1e03cdd",
                    "name": "Xianfeng Wu",
                    "hidden": false
                },
                {
                    "_id": "68df3bbfdf49fb0df1e03cde",
                    "name": "Wen-Jie Shu",
                    "hidden": false
                },
                {
                    "_id": "68df3bbfdf49fb0df1e03cdf",
                    "name": "Rongjin Guo",
                    "hidden": false
                },
                {
                    "_id": "68df3bbfdf49fb0df1e03ce0",
                    "name": "Disen Lan",
                    "hidden": false
                },
                {
                    "_id": "68df3bbfdf49fb0df1e03ce1",
                    "name": "Harry Yang",
                    "hidden": false
                },
                {
                    "_id": "68df3bbfdf49fb0df1e03ce2",
                    "name": "Ying-Cong Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T15:08:25.000Z",
            "submittedOnDailyAt": "2025-10-03T01:30:06.294Z",
            "title": "Go with Your Gut: Scaling Confidence for Autoregressive Image Generation",
            "submittedOnDailyBy": {
                "_id": "6570450a78d7aca0c361a177",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/MX7jHhTQwLs-BvYIu5rqb.jpeg",
                "isPro": false,
                "fullname": "Harold Chen",
                "user": "Harold328",
                "type": "user"
            },
            "summary": "Test-time scaling (TTS) has demonstrated remarkable success in enhancing\nlarge language models, yet its application to next-token prediction (NTP)\nautoregressive (AR) image generation remains largely uncharted. Existing TTS\napproaches for visual AR (VAR), which rely on frequent partial decoding and\nexternal reward models, are ill-suited for NTP-based image generation due to\nthe inherent incompleteness of intermediate decoding results. To bridge this\ngap, we introduce ScalingAR, the first TTS framework specifically designed for\nNTP-based AR image generation that eliminates the need for early decoding or\nauxiliary rewards. ScalingAR leverages token entropy as a novel signal in\nvisual token generation and operates at two complementary scaling levels: (i)\nProfile Level, which streams a calibrated confidence state by fusing intrinsic\nand conditional signals; and (ii) Policy Level, which utilizes this state to\nadaptively terminate low-confidence trajectories and dynamically schedule\nguidance for phase-appropriate conditioning strength. Experiments on both\ngeneral and compositional benchmarks show that ScalingAR (1) improves base\nmodels by 12.5% on GenEval and 15.2% on TIIF-Bench, (2) efficiently reduces\nvisual token consumption by 62.0% while outperforming baselines, and (3)\nsuccessfully enhances robustness, mitigating performance drops by 26.0% in\nchallenging scenarios.",
            "upvotes": 6,
            "discussionId": "68df3bc0df49fb0df1e03ce3",
            "githubRepo": "https://github.com/EnVision-Research/ScalingAR",
            "ai_summary": "ScalingAR enhances next-token prediction in autoregressive image generation by using token entropy and adaptive scaling, improving model performance and efficiency.",
            "ai_keywords": [
                "test-time scaling",
                "next-token prediction",
                "autoregressive image generation",
                "token entropy",
                "profile level",
                "policy level",
                "GenEval",
                "TIIF-Bench",
                "visual token consumption",
                "robustness"
            ],
            "githubStars": 9
        },
        "publishedAt": "2025-09-30T11:08:25.000Z",
        "title": "Go with Your Gut: Scaling Confidence for Autoregressive Image Generation",
        "summary": "Test-time scaling (TTS) has demonstrated remarkable success in enhancing\nlarge language models, yet its application to next-token prediction (NTP)\nautoregressive (AR) image generation remains largely uncharted. Existing TTS\napproaches for visual AR (VAR), which rely on frequent partial decoding and\nexternal reward models, are ill-suited for NTP-based image generation due to\nthe inherent incompleteness of intermediate decoding results. To bridge this\ngap, we introduce ScalingAR, the first TTS framework specifically designed for\nNTP-based AR image generation that eliminates the need for early decoding or\nauxiliary rewards. ScalingAR leverages token entropy as a novel signal in\nvisual token generation and operates at two complementary scaling levels: (i)\nProfile Level, which streams a calibrated confidence state by fusing intrinsic\nand conditional signals; and (ii) Policy Level, which utilizes this state to\nadaptively terminate low-confidence trajectories and dynamically schedule\nguidance for phase-appropriate conditioning strength. Experiments on both\ngeneral and compositional benchmarks show that ScalingAR (1) improves base\nmodels by 12.5% on GenEval and 15.2% on TIIF-Bench, (2) efficiently reduces\nvisual token consumption by 62.0% while outperforming baselines, and (3)\nsuccessfully enhances robustness, mitigating performance drops by 26.0% in\nchallenging scenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26376.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6570450a78d7aca0c361a177",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/MX7jHhTQwLs-BvYIu5rqb.jpeg",
            "fullname": "Harold Chen",
            "name": "Harold328",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.01670",
            "authors": [
                {
                    "_id": "68df30a8df49fb0df1e03c43",
                    "name": "Erfan Shayegani",
                    "hidden": false
                },
                {
                    "_id": "68df30a8df49fb0df1e03c44",
                    "name": "Keegan Hines",
                    "hidden": false
                },
                {
                    "_id": "68df30a8df49fb0df1e03c45",
                    "name": "Yue Dong",
                    "hidden": false
                },
                {
                    "_id": "68df30a8df49fb0df1e03c46",
                    "name": "Nael Abu-Ghazaleh",
                    "hidden": false
                },
                {
                    "_id": "68df30a8df49fb0df1e03c47",
                    "name": "Roman Lutz",
                    "hidden": false
                },
                {
                    "_id": "68df30a8df49fb0df1e03c48",
                    "name": "Spencer Whitehead",
                    "hidden": false
                },
                {
                    "_id": "68df30a8df49fb0df1e03c49",
                    "name": "Vidhisha Balachandran",
                    "hidden": false
                },
                {
                    "_id": "68df30a8df49fb0df1e03c4a",
                    "name": "Besmira Nushi",
                    "hidden": false
                },
                {
                    "_id": "68df30a8df49fb0df1e03c4b",
                    "name": "Vibhav Vineet",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T04:52:15.000Z",
            "submittedOnDailyAt": "2025-10-03T00:41:00.967Z",
            "title": "Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Computer-Use Agents (CUAs) are an increasingly deployed class of agents that\ntake actions on GUIs to accomplish user goals. In this paper, we show that CUAs\nconsistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals\nregardless of feasibility, safety, reliability, or context. We characterize\nthree prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)\nassumptions and decisions under ambiguity, and (iii) contradictory or\ninfeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these\nthree patterns. Built on OSWorld, BLIND-ACT provides realistic environments and\nemploys LLM-based judges to evaluate agent behavior, achieving 93.75% agreement\nwith human annotations. We use BLIND-ACT to evaluate nine frontier models,\nincluding Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing\nhigh average BGD rates (80.8%) across them. We show that BGD exposes subtle\nrisks that arise even when inputs are not directly harmful. While\nprompting-based interventions lower BGD levels, substantial risk persists,\nhighlighting the need for stronger training- or inference-time interventions.\nQualitative analysis reveals observed failure modes: execution-first bias\n(focusing on how to act over whether to act), thought-action disconnect\n(execution diverging from reasoning), and request-primacy (justifying actions\ndue to user request). Identifying BGD and introducing BLIND-ACT establishes a\nfoundation for future research on studying and mitigating this fundamental risk\nand ensuring safe CUA deployment.",
            "upvotes": 5,
            "discussionId": "68df30a9df49fb0df1e03c4c",
            "ai_summary": "Computer-Use Agents consistently exhibit Blind Goal-Directedness, a bias that leads to risky behavior regardless of feasibility or context, as demonstrated by the BLIND-ACT benchmark.",
            "ai_keywords": [
                "Blind Goal-Directedness",
                "contextual reasoning",
                "ambiguity",
                "contradictory goals",
                "benchmark",
                "OSWorld",
                "LLM-based judges",
                "execution-first bias",
                "thought-action disconnect",
                "request-primacy"
            ],
            "organization": {
                "_id": "5e6485f787403103f9f1055e",
                "name": "microsoft",
                "fullname": "Microsoft",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
            }
        },
        "publishedAt": "2025-10-02T00:52:15.000Z",
        "title": "Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness",
        "summary": "Computer-Use Agents (CUAs) are an increasingly deployed class of agents that\ntake actions on GUIs to accomplish user goals. In this paper, we show that CUAs\nconsistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals\nregardless of feasibility, safety, reliability, or context. We characterize\nthree prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)\nassumptions and decisions under ambiguity, and (iii) contradictory or\ninfeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these\nthree patterns. Built on OSWorld, BLIND-ACT provides realistic environments and\nemploys LLM-based judges to evaluate agent behavior, achieving 93.75% agreement\nwith human annotations. We use BLIND-ACT to evaluate nine frontier models,\nincluding Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing\nhigh average BGD rates (80.8%) across them. We show that BGD exposes subtle\nrisks that arise even when inputs are not directly harmful. While\nprompting-based interventions lower BGD levels, substantial risk persists,\nhighlighting the need for stronger training- or inference-time interventions.\nQualitative analysis reveals observed failure modes: execution-first bias\n(focusing on how to act over whether to act), thought-action disconnect\n(execution diverging from reasoning), and request-primacy (justifying actions\ndue to user request). Identifying BGD and introducing BLIND-ACT establishes a\nfoundation for future research on studying and mitigating this fundamental risk\nand ensuring safe CUA deployment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01670.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 118
        },
        "organization": {
            "_id": "5e6485f787403103f9f1055e",
            "name": "microsoft",
            "fullname": "Microsoft",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01304",
            "authors": [
                {
                    "_id": "68df4ea4df49fb0df1e03d57",
                    "user": {
                        "_id": "665d652e0f35c005de892108",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg",
                        "isPro": false,
                        "fullname": "Yu Zeng",
                        "user": "YuZeng260",
                        "type": "user"
                    },
                    "name": "Yu Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:28:59.473Z",
                    "hidden": false
                },
                {
                    "_id": "68df4ea4df49fb0df1e03d58",
                    "name": "Wenxuan Huang",
                    "hidden": false
                },
                {
                    "_id": "68df4ea4df49fb0df1e03d59",
                    "name": "Shiting Huang",
                    "hidden": false
                },
                {
                    "_id": "68df4ea4df49fb0df1e03d5a",
                    "name": "Xikun Bao",
                    "hidden": false
                },
                {
                    "_id": "68df4ea4df49fb0df1e03d5b",
                    "name": "Yukun Qi",
                    "hidden": false
                },
                {
                    "_id": "68df4ea4df49fb0df1e03d5c",
                    "name": "Yiming Zhao",
                    "hidden": false
                },
                {
                    "_id": "68df4ea4df49fb0df1e03d5d",
                    "name": "Qiuchen Wang",
                    "hidden": false
                },
                {
                    "_id": "68df4ea4df49fb0df1e03d5e",
                    "name": "Lin Chen",
                    "hidden": false
                },
                {
                    "_id": "68df4ea4df49fb0df1e03d5f",
                    "name": "Zehui Chen",
                    "hidden": false
                },
                {
                    "_id": "68df4ea4df49fb0df1e03d60",
                    "name": "Huaian Chen",
                    "hidden": false
                },
                {
                    "_id": "68df4ea4df49fb0df1e03d61",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68df4ea4df49fb0df1e03d62",
                    "name": "Feng Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T17:58:05.000Z",
            "submittedOnDailyAt": "2025-10-03T02:57:49.307Z",
            "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and\n  Reasoning in Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "665d652e0f35c005de892108",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg",
                "isPro": false,
                "fullname": "Yu Zeng",
                "user": "YuZeng260",
                "type": "user"
            },
            "summary": "Although current large Vision-Language Models (VLMs) have advanced in\nmultimodal understanding and reasoning, their fundamental perceptual and\nreasoning abilities remain limited. Specifically, even on simple jigsaw tasks,\nexisting VLMs perform near randomly, revealing deficiencies in core perception\nand reasoning capabilities. While high-quality vision-language data can enhance\nthese capabilities, its scarcity and limited scalability impose significant\nconstraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction\nLearning for Enhancing visual perception and reasoning in VLMs. AGILE\nformulates jigsaw solving as an interactive process, enabling the model to\nprogressively engage with the environment. At each step, the model generates\nexecutable code to perform an action based on the current state, while the\nenvironment provides fine-grained visual feedback to guide task completion.\nThrough this iterative cycle of observation and interaction, the model\nincrementally improves its perceptual and reasoning capabilities via\nexploration and feedback. Experimental results show that AGILE not only\nsubstantially boosts performance on jigsaw tasks of varying complexity (e.g.,\nincreasing accuracy from 9.5% to 82.8% under the 2 times 2 setting) but also\ndemonstrates strong generalization across 9 general vision tasks, achieving an\naverage improvement of 3.1%. These results indicate notable enhancements in\nboth perceptual and reasoning abilities. This work opens a new avenue for\nadvancing reasoning and generalization in multimodal models and provides an\nefficient, scalable solution to the scarcity of multimodal reinforcement\nlearning data. The code and datasets is available at\nhttps://github.com/yuzeng0-0/AGILE .",
            "upvotes": 5,
            "discussionId": "68df4ea4df49fb0df1e03d63",
            "projectPage": "https://yuzeng0-0.github.io/AGILE/",
            "githubRepo": "https://github.com/yuzeng0-0/AGILE",
            "ai_summary": "AGILE, an interactive jigsaw-solving framework, enhances visual perception and reasoning in Vision-Language Models through iterative action and feedback, improving performance on jigsaw tasks and general vision tasks.",
            "ai_keywords": [
                "Vision-Language Models",
                "VLMs",
                "jigsaw tasks",
                "perceptual capabilities",
                "reasoning capabilities",
                "AGILE",
                "Agentic jiGsaw Interaction Learning",
                "executable code",
                "fine-grained visual feedback",
                "multimodal understanding",
                "multimodal reasoning",
                "multimodal reinforcement learning"
            ],
            "githubStars": 10
        },
        "publishedAt": "2025-10-01T13:58:05.000Z",
        "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and\n  Reasoning in Vision-Language Models",
        "summary": "Although current large Vision-Language Models (VLMs) have advanced in\nmultimodal understanding and reasoning, their fundamental perceptual and\nreasoning abilities remain limited. Specifically, even on simple jigsaw tasks,\nexisting VLMs perform near randomly, revealing deficiencies in core perception\nand reasoning capabilities. While high-quality vision-language data can enhance\nthese capabilities, its scarcity and limited scalability impose significant\nconstraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction\nLearning for Enhancing visual perception and reasoning in VLMs. AGILE\nformulates jigsaw solving as an interactive process, enabling the model to\nprogressively engage with the environment. At each step, the model generates\nexecutable code to perform an action based on the current state, while the\nenvironment provides fine-grained visual feedback to guide task completion.\nThrough this iterative cycle of observation and interaction, the model\nincrementally improves its perceptual and reasoning capabilities via\nexploration and feedback. Experimental results show that AGILE not only\nsubstantially boosts performance on jigsaw tasks of varying complexity (e.g.,\nincreasing accuracy from 9.5% to 82.8% under the 2 times 2 setting) but also\ndemonstrates strong generalization across 9 general vision tasks, achieving an\naverage improvement of 3.1%. These results indicate notable enhancements in\nboth perceptual and reasoning abilities. This work opens a new avenue for\nadvancing reasoning and generalization in multimodal models and provides an\nefficient, scalable solution to the scarcity of multimodal reinforcement\nlearning data. The code and datasets is available at\nhttps://github.com/yuzeng0-0/AGILE .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01304.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "665d652e0f35c005de892108",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg",
            "fullname": "Yu Zeng",
            "name": "YuZeng260",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.02315",
            "authors": [
                {
                    "_id": "68df48ecdf49fb0df1e03d4c",
                    "name": "Eric Tillmann Bill",
                    "hidden": false
                },
                {
                    "_id": "68df48ecdf49fb0df1e03d4d",
                    "user": {
                        "_id": "63412f2add8853dc7e306a4f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/nqbteeQhS2dPA5LVy7dTD.png",
                        "isPro": false,
                        "fullname": "Enis Simsar",
                        "user": "enisimsar",
                        "type": "user"
                    },
                    "name": "Enis Simsar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:29:01.815Z",
                    "hidden": false
                },
                {
                    "_id": "68df48ecdf49fb0df1e03d4e",
                    "name": "Thomas Hofmann",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T17:59:58.000Z",
            "submittedOnDailyAt": "2025-10-03T02:25:08.105Z",
            "title": "Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject\n  Fidelity",
            "submittedOnDailyBy": {
                "_id": "63412f2add8853dc7e306a4f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/nqbteeQhS2dPA5LVy7dTD.png",
                "isPro": false,
                "fullname": "Enis Simsar",
                "user": "enisimsar",
                "type": "user"
            },
            "summary": "Text-to-image (T2I) models excel on single-entity prompts but struggle with\nmulti-subject descriptions, often showing attribute leakage, identity\nentanglement, and subject omissions. We introduce the first theoretical\nframework with a principled, optimizable objective for steering sampling\ndynamics toward multi-subject fidelity. Viewing flow matching (FM) through\nstochastic optimal control (SOC), we formulate subject disentanglement as\ncontrol over a trained FM sampler. This yields two architecture-agnostic\nalgorithms: (i) a training-free test-time controller that perturbs the base\nvelocity with a single-pass update, and (ii) Adjoint Matching, a lightweight\nfine-tuning rule that regresses a control network to a backward adjoint signal\nwhile preserving base-model capabilities. The same formulation unifies prior\nattention heuristics, extends to diffusion models via a flow-diffusion\ncorrespondence, and provides the first fine-tuning route explicitly designed\nfor multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and\nStable Diffusion XL, both algorithms consistently improve multi-subject\nalignment while maintaining base-model style. Test-time control runs\nefficiently on commodity GPUs, and fine-tuned controllers trained on limited\nprompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal\nControl for Unentangled Subjects), which achieves state-of-the-art\nmulti-subject fidelity across models.",
            "upvotes": 4,
            "discussionId": "68df48eddf49fb0df1e03d4f",
            "githubRepo": "https://github.com/ericbill21/FOCUS/",
            "ai_summary": "A theoretical framework and algorithms for improving multi-subject fidelity in text-to-image models through control over sampling dynamics.",
            "ai_keywords": [
                "flow matching",
                "stochastic optimal control",
                "subject disentanglement",
                "training-free test-time controller",
                "Adjoint Matching",
                "flow-diffusion correspondence",
                "fine-tuning",
                "multi-subject alignment",
                "Stable Diffusion",
                "FLUX",
                "Stable Diffusion XL",
                "FOCUS"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-10-02T13:59:58.000Z",
        "title": "Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject\n  Fidelity",
        "summary": "Text-to-image (T2I) models excel on single-entity prompts but struggle with\nmulti-subject descriptions, often showing attribute leakage, identity\nentanglement, and subject omissions. We introduce the first theoretical\nframework with a principled, optimizable objective for steering sampling\ndynamics toward multi-subject fidelity. Viewing flow matching (FM) through\nstochastic optimal control (SOC), we formulate subject disentanglement as\ncontrol over a trained FM sampler. This yields two architecture-agnostic\nalgorithms: (i) a training-free test-time controller that perturbs the base\nvelocity with a single-pass update, and (ii) Adjoint Matching, a lightweight\nfine-tuning rule that regresses a control network to a backward adjoint signal\nwhile preserving base-model capabilities. The same formulation unifies prior\nattention heuristics, extends to diffusion models via a flow-diffusion\ncorrespondence, and provides the first fine-tuning route explicitly designed\nfor multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and\nStable Diffusion XL, both algorithms consistently improve multi-subject\nalignment while maintaining base-model style. Test-time control runs\nefficiently on commodity GPUs, and fine-tuned controllers trained on limited\nprompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal\nControl for Unentangled Subjects), which achieves state-of-the-art\nmulti-subject fidelity across models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02315.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63412f2add8853dc7e306a4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/nqbteeQhS2dPA5LVy7dTD.png",
            "fullname": "Enis Simsar",
            "name": "enisimsar",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.02263",
            "authors": [
                {
                    "_id": "68df8203df49fb0df1e03e0f",
                    "name": "Yuxiao Qu",
                    "hidden": false
                },
                {
                    "_id": "68df8203df49fb0df1e03e10",
                    "name": "Anikait Singh",
                    "hidden": false
                },
                {
                    "_id": "68df8203df49fb0df1e03e11",
                    "name": "Yoonho Lee",
                    "hidden": false
                },
                {
                    "_id": "68df8203df49fb0df1e03e12",
                    "name": "Amrith Setlur",
                    "hidden": false
                },
                {
                    "_id": "68df8203df49fb0df1e03e13",
                    "name": "Ruslan Salakhutdinov",
                    "hidden": false
                },
                {
                    "_id": "68df8203df49fb0df1e03e14",
                    "name": "Chelsea Finn",
                    "hidden": false
                },
                {
                    "_id": "68df8203df49fb0df1e03e15",
                    "name": "Aviral Kumar",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T17:44:23.000Z",
            "submittedOnDailyAt": "2025-10-03T06:28:15.195Z",
            "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning\n  Problems",
            "submittedOnDailyBy": {
                "_id": "64d98ef7a4839890b25eb78b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
                "isPro": true,
                "fullname": "Fangyuan Yu",
                "user": "Ksgk-fy",
                "type": "user"
            },
            "summary": "Reasoning requires going beyond pattern matching or memorization of solutions\nto identify and implement \"algorithmic procedures\" that can be used to deduce\nanswers to hard problems. Doing so requires realizing the most relevant\nprimitives, intermediate results, or shared procedures, and building upon them.\nWhile RL post-training on long chains of thought ultimately aims to uncover\nthis kind of algorithmic behavior, most reasoning traces learned by large\nmodels fail to consistently capture or reuse procedures, instead drifting into\nverbose and degenerate exploration. To address more effective reasoning, we\nintroduce reasoning abstractions: concise natural language descriptions of\nprocedural and factual knowledge that guide the model toward learning\nsuccessful reasoning. We train models to be capable of proposing multiple\nabstractions given a problem, followed by RL that incentivizes building a\nsolution while using the information provided by these abstractions. This\nresults in a two-player RL training paradigm, abbreviated as RLAD, that jointly\ntrains an abstraction generator and a solution generator. This setup\neffectively enables structured exploration, decouples learning signals of\nabstraction proposal and solution generation, and improves generalization to\nharder problems. We also show that allocating more test-time compute to\ngenerating abstractions is more beneficial for performance than generating more\nsolutions at large test budgets, illustrating the role of abstractions in\nguiding meaningful exploration.",
            "upvotes": 4,
            "discussionId": "68df8204df49fb0df1e03e16",
            "ai_summary": "Introducing reasoning abstractions in reinforcement learning improves structured exploration and generalization for complex problem-solving.",
            "ai_keywords": [
                "reinforcement learning",
                "reasoning abstractions",
                "procedural knowledge",
                "factual knowledge",
                "abstraction generator",
                "solution generator",
                "RLAD",
                "structured exploration",
                "generalization"
            ]
        },
        "publishedAt": "2025-10-02T13:44:23.000Z",
        "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning\n  Problems",
        "summary": "Reasoning requires going beyond pattern matching or memorization of solutions\nto identify and implement \"algorithmic procedures\" that can be used to deduce\nanswers to hard problems. Doing so requires realizing the most relevant\nprimitives, intermediate results, or shared procedures, and building upon them.\nWhile RL post-training on long chains of thought ultimately aims to uncover\nthis kind of algorithmic behavior, most reasoning traces learned by large\nmodels fail to consistently capture or reuse procedures, instead drifting into\nverbose and degenerate exploration. To address more effective reasoning, we\nintroduce reasoning abstractions: concise natural language descriptions of\nprocedural and factual knowledge that guide the model toward learning\nsuccessful reasoning. We train models to be capable of proposing multiple\nabstractions given a problem, followed by RL that incentivizes building a\nsolution while using the information provided by these abstractions. This\nresults in a two-player RL training paradigm, abbreviated as RLAD, that jointly\ntrains an abstraction generator and a solution generator. This setup\neffectively enables structured exploration, decouples learning signals of\nabstraction proposal and solution generation, and improves generalization to\nharder problems. We also show that allocating more test-time compute to\ngenerating abstractions is more beneficial for performance than generating more\nsolutions at large test budgets, illustrating the role of abstractions in\nguiding meaningful exploration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02263.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d98ef7a4839890b25eb78b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
            "fullname": "Fangyuan Yu",
            "name": "Ksgk-fy",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.02259",
            "authors": [
                {
                    "_id": "68df2a3fdf49fb0df1e03bd3",
                    "name": "Tobias Kreiman",
                    "hidden": false
                },
                {
                    "_id": "68df2a3fdf49fb0df1e03bd4",
                    "name": "Yutong Bai",
                    "hidden": false
                },
                {
                    "_id": "68df2a3fdf49fb0df1e03bd5",
                    "name": "Fadi Atieh",
                    "hidden": false
                },
                {
                    "_id": "68df2a3fdf49fb0df1e03bd6",
                    "name": "Elizabeth Weaver",
                    "hidden": false
                },
                {
                    "_id": "68df2a3fdf49fb0df1e03bd7",
                    "name": "Eric Qu",
                    "hidden": false
                },
                {
                    "_id": "68df2a3fdf49fb0df1e03bd8",
                    "name": "Aditi S. Krishnapriyan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T17:42:10.000Z",
            "submittedOnDailyAt": "2025-10-03T00:14:38.936Z",
            "title": "Transformers Discover Molecular Structure Without Graph Priors",
            "submittedOnDailyBy": {
                "_id": "64bece3e01f1983a86a3e043",
                "avatarUrl": "/avatars/7f6372d409e8eb0474f7ff6282cfd4e9.svg",
                "isPro": false,
                "fullname": "Toby Kreiman",
                "user": "tkreiman",
                "type": "user"
            },
            "summary": "Graph Neural Networks (GNNs) are the dominant architecture for molecular\nmachine learning, particularly for molecular property prediction and machine\nlearning interatomic potentials (MLIPs). GNNs perform message passing on\npredefined graphs often induced by a fixed radius cutoff or k-nearest neighbor\nscheme. While this design aligns with the locality present in many molecular\ntasks, a hard-coded graph can limit expressivity due to the fixed receptive\nfield and slows down inference with sparse graph operations. In this work, we\ninvestigate whether pure, unmodified Transformers trained directly on Cartesian\ncoordinatesx2013without predefined graphs or physical\npriorsx2013can approximate molecular energies and forces. As a\nstarting point for our analysis, we demonstrate how to train a Transformer to\ncompetitive energy and force mean absolute errors under a matched training\ncompute budget, relative to a state-of-the-art equivariant GNN on the OMol25\ndataset. We discover that the Transformer learns physically consistent\npatternsx2013such as attention weights that decay inversely with\ninteratomic distancex2013and flexibly adapts them across different\nmolecular environments due to the absence of hard-coded biases. The use of a\nstandard Transformer also unlocks predictable improvements with respect to\nscaling training resources, consistent with empirical scaling laws observed in\nother domains. Our results demonstrate that many favorable properties of GNNs\ncan emerge adaptively in Transformers, challenging the necessity of hard-coded\ngraph inductive biases and pointing toward standardized, scalable architectures\nfor molecular modeling.",
            "upvotes": 4,
            "discussionId": "68df2a3fdf49fb0df1e03bd9",
            "ai_summary": "Transformers trained directly on Cartesian coordinates can achieve competitive performance in molecular energy and force prediction without predefined graphs, demonstrating adaptability and scalability.",
            "ai_keywords": [
                "Graph Neural Networks",
                "GNNs",
                "molecular machine learning",
                "molecular property prediction",
                "machine learning interatomic potentials",
                "MLIPs",
                "message passing",
                "predefined graphs",
                "fixed radius cutoff",
                "k-nearest neighbor",
                "receptive field",
                "inference",
                "sparse graph operations",
                "Transformers",
                "Cartesian coordinates",
                "physical priors",
                "energy and force mean absolute errors",
                "OMol25 dataset",
                "attention weights",
                "interatomic distance",
                "molecular environments",
                "hard-coded biases",
                "scaling training resources",
                "empirical scaling laws",
                "standardized architectures",
                "molecular modeling"
            ],
            "organization": {
                "_id": "61f20a9ce108f2cba2dc0730",
                "name": "Berkeley",
                "fullname": "UC Berkeley",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"
            }
        },
        "publishedAt": "2025-10-02T13:42:10.000Z",
        "title": "Transformers Discover Molecular Structure Without Graph Priors",
        "summary": "Graph Neural Networks (GNNs) are the dominant architecture for molecular\nmachine learning, particularly for molecular property prediction and machine\nlearning interatomic potentials (MLIPs). GNNs perform message passing on\npredefined graphs often induced by a fixed radius cutoff or k-nearest neighbor\nscheme. While this design aligns with the locality present in many molecular\ntasks, a hard-coded graph can limit expressivity due to the fixed receptive\nfield and slows down inference with sparse graph operations. In this work, we\ninvestigate whether pure, unmodified Transformers trained directly on Cartesian\ncoordinatesx2013without predefined graphs or physical\npriorsx2013can approximate molecular energies and forces. As a\nstarting point for our analysis, we demonstrate how to train a Transformer to\ncompetitive energy and force mean absolute errors under a matched training\ncompute budget, relative to a state-of-the-art equivariant GNN on the OMol25\ndataset. We discover that the Transformer learns physically consistent\npatternsx2013such as attention weights that decay inversely with\ninteratomic distancex2013and flexibly adapts them across different\nmolecular environments due to the absence of hard-coded biases. The use of a\nstandard Transformer also unlocks predictable improvements with respect to\nscaling training resources, consistent with empirical scaling laws observed in\nother domains. Our results demonstrate that many favorable properties of GNNs\ncan emerge adaptively in Transformers, challenging the necessity of hard-coded\ngraph inductive biases and pointing toward standardized, scalable architectures\nfor molecular modeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02259.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64bece3e01f1983a86a3e043",
            "avatarUrl": "/avatars/7f6372d409e8eb0474f7ff6282cfd4e9.svg",
            "fullname": "Toby Kreiman",
            "name": "tkreiman",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "61f20a9ce108f2cba2dc0730",
            "name": "Berkeley",
            "fullname": "UC Berkeley",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01623",
            "authors": [
                {
                    "_id": "68df33ebdf49fb0df1e03c92",
                    "name": "Angen Ye",
                    "hidden": false
                },
                {
                    "_id": "68df33ebdf49fb0df1e03c93",
                    "name": "Zeyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68df33ebdf49fb0df1e03c94",
                    "name": "Boyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68df33ebdf49fb0df1e03c95",
                    "name": "Xiaofeng Wang",
                    "hidden": false
                },
                {
                    "_id": "68df33ebdf49fb0df1e03c96",
                    "name": "Dapeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68df33ebdf49fb0df1e03c97",
                    "name": "Zheng Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T02:54:03.000Z",
            "submittedOnDailyAt": "2025-10-03T00:55:59.842Z",
            "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models",
            "submittedOnDailyBy": {
                "_id": "64ec877bb93654d4ca5c92e9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                "isPro": true,
                "fullname": "Zeyu Zhang",
                "user": "SteveZeyuZhang",
                "type": "user"
            },
            "summary": "Vision-Language-Action (VLA) models aim to unify perception, language\nunderstanding, and action generation, offering strong cross-task and\ncross-scene generalization with broad impact on embodied AI. However, current\nVLA models often lack explicit step-by-step reasoning, instead emitting final\nactions without considering affordance constraints or geometric relations.\nTheir post-training pipelines also rarely reinforce reasoning quality, relying\nprimarily on supervised fine-tuning with weak reward design. To address these\nchallenges, we present VLA-R1, a reasoning-enhanced VLA that integrates\nReinforcement Learning from Verifiable Rewards (RLVR) with Group Relative\nPolicy Optimization (GRPO) to systematically optimize both reasoning and\nexecution. Specifically, we design an RLVR-based post-training strategy with\nverifiable rewards for region alignment, trajectory consistency, and output\nformatting, thereby strengthening reasoning robustness and execution accuracy.\nMoreover, we develop VLA-CoT-13K, a high-quality dataset that provides\nchain-of-thought supervision explicitly aligned with affordance and trajectory\nannotations. Furthermore, extensive evaluations on in-domain, out-of-domain,\nsimulation, and real-robot platforms demonstrate that VLA-R1 achieves superior\ngeneralization and real-world performance compared to prior VLA methods. We\nplan to release the model, code, and dataset following the publication of this\nwork. Code: https://github.com/GigaAI-research/VLA-R1. Website:\nhttps://gigaai-research.github.io/VLA-R1.",
            "upvotes": 4,
            "discussionId": "68df33ebdf49fb0df1e03c98",
            "projectPage": "https://gigaai-research.github.io/VLA-R1",
            "githubRepo": "https://github.com/GigaAI-research/VLA-R1",
            "ai_summary": "VLA-R1 enhances VLA models with RLVR and GRPO to improve reasoning and execution, achieving better generalization and real-world performance using a new dataset with chain-of-thought supervision.",
            "ai_keywords": [
                "VLA models",
                "Reinforcement Learning from Verifiable Rewards (RLVR)",
                "Group Relative Policy Optimization (GRPO)",
                "region alignment",
                "trajectory consistency",
                "output formatting",
                "chain-of-thought supervision",
                "affordance",
                "trajectory annotations"
            ],
            "githubStars": 9
        },
        "publishedAt": "2025-10-01T22:54:03.000Z",
        "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models",
        "summary": "Vision-Language-Action (VLA) models aim to unify perception, language\nunderstanding, and action generation, offering strong cross-task and\ncross-scene generalization with broad impact on embodied AI. However, current\nVLA models often lack explicit step-by-step reasoning, instead emitting final\nactions without considering affordance constraints or geometric relations.\nTheir post-training pipelines also rarely reinforce reasoning quality, relying\nprimarily on supervised fine-tuning with weak reward design. To address these\nchallenges, we present VLA-R1, a reasoning-enhanced VLA that integrates\nReinforcement Learning from Verifiable Rewards (RLVR) with Group Relative\nPolicy Optimization (GRPO) to systematically optimize both reasoning and\nexecution. Specifically, we design an RLVR-based post-training strategy with\nverifiable rewards for region alignment, trajectory consistency, and output\nformatting, thereby strengthening reasoning robustness and execution accuracy.\nMoreover, we develop VLA-CoT-13K, a high-quality dataset that provides\nchain-of-thought supervision explicitly aligned with affordance and trajectory\nannotations. Furthermore, extensive evaluations on in-domain, out-of-domain,\nsimulation, and real-robot platforms demonstrate that VLA-R1 achieves superior\ngeneralization and real-world performance compared to prior VLA methods. We\nplan to release the model, code, and dataset following the publication of this\nwork. Code: https://github.com/GigaAI-research/VLA-R1. Website:\nhttps://gigaai-research.github.io/VLA-R1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01623.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "fullname": "Zeyu Zhang",
            "name": "SteveZeyuZhang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01817",
            "authors": [
                {
                    "_id": "68dfacffdf49fb0df1e03ec3",
                    "user": {
                        "_id": "675197c3ae96d7ba4b4a6c66",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/I2GHgrv70cfT8C5EbK6Q5.png",
                        "isPro": false,
                        "fullname": "Adam Filipek",
                        "user": "AdamF92",
                        "type": "user"
                    },
                    "name": "Adam Filipek",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:28:11.037Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T09:01:38.000Z",
            "submittedOnDailyAt": "2025-10-03T14:59:37.354Z",
            "title": "Sparse Query Attention (SQA): A Computationally Efficient Attention\n  Mechanism with Query Heads Reduction",
            "submittedOnDailyBy": {
                "_id": "675197c3ae96d7ba4b4a6c66",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/I2GHgrv70cfT8C5EbK6Q5.png",
                "isPro": false,
                "fullname": "Adam Filipek",
                "user": "AdamF92",
                "type": "user"
            },
            "summary": "The Transformer architecture, underpinned by the Multi-Head Attention (MHA)\nmechanism, has become the de facto standard for state-of-the-art models in\nartificial intelligence. However, the quadratic computational complexity of MHA\nwith respect to sequence length presents a significant barrier to scaling,\nparticularly for applications involving long contexts. Prevailing solutions,\nsuch as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have\neffectively addressed the memory bandwidth bottleneck that dominates\nautoregressive inference latency by sharing Key and Value projections. While\nhighly successful, these methods do not reduce the fundamental number of\nfloating-point operations (FLOPs) required for the attention score computation,\nwhich remains a critical bottleneck for training and full-sequence processing.\nThis paper introduces Sparse Query Attention (SQA), a novel attention\narchitecture that pursues an alternative and complementary optimization path.\nInstead of reducing Key/Value heads, SQA reduces the number of Query heads.\nThis architectural modification directly decreases the computational complexity\nof the attention mechanism by a factor proportional to the reduction in query\nheads, thereby lowering the overall FLOPs. This work presents the theoretical\nfoundation of SQA, its mathematical formulation, and a family of architectural\nvariants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate\nthat SQA can achieve significant throughput improvements of up to 3x in\ncomputation-bound scenarios such as model pre-training, fine-tuning, and\nencoder-based tasks, with only a minimal impact on model quality in preliminary\nsmallscale experiments. SQA was discovered serendipitously during the\ndevelopment of the upcoming Reactive Transformer architecture, suggesting its\npotential as a powerful tool for building more efficient and scalable models",
            "upvotes": 3,
            "discussionId": "68dfad00df49fb0df1e03ec4",
            "projectPage": "https://rxai.dev",
            "githubRepo": "https://github.com/RxAI-dev/rxnn-attention",
            "ai_summary": "Sparse Query Attention (SQA) reduces computational complexity in Transformer models by decreasing the number of Query heads, leading to significant throughput improvements with minimal impact on model quality.",
            "ai_keywords": [
                "Transformer architecture",
                "Multi-Head Attention (MHA)",
                "Multi-Query Attention (MQA)",
                "Grouped-Query Attention (GQA)",
                "Sparse Query Attention (SQA)",
                "floating-point operations (FLOPs)",
                "attention score computation",
                "Reactive Transformer architecture"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "675776b060e4100500aeb4c8",
                "name": "ReactiveAI",
                "fullname": "Reactive AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/675197c3ae96d7ba4b4a6c66/AJDkLuavcYfENIRDzxjqR.png"
            }
        },
        "publishedAt": "2025-10-02T05:01:38.000Z",
        "title": "Sparse Query Attention (SQA): A Computationally Efficient Attention\n  Mechanism with Query Heads Reduction",
        "summary": "The Transformer architecture, underpinned by the Multi-Head Attention (MHA)\nmechanism, has become the de facto standard for state-of-the-art models in\nartificial intelligence. However, the quadratic computational complexity of MHA\nwith respect to sequence length presents a significant barrier to scaling,\nparticularly for applications involving long contexts. Prevailing solutions,\nsuch as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have\neffectively addressed the memory bandwidth bottleneck that dominates\nautoregressive inference latency by sharing Key and Value projections. While\nhighly successful, these methods do not reduce the fundamental number of\nfloating-point operations (FLOPs) required for the attention score computation,\nwhich remains a critical bottleneck for training and full-sequence processing.\nThis paper introduces Sparse Query Attention (SQA), a novel attention\narchitecture that pursues an alternative and complementary optimization path.\nInstead of reducing Key/Value heads, SQA reduces the number of Query heads.\nThis architectural modification directly decreases the computational complexity\nof the attention mechanism by a factor proportional to the reduction in query\nheads, thereby lowering the overall FLOPs. This work presents the theoretical\nfoundation of SQA, its mathematical formulation, and a family of architectural\nvariants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate\nthat SQA can achieve significant throughput improvements of up to 3x in\ncomputation-bound scenarios such as model pre-training, fine-tuning, and\nencoder-based tasks, with only a minimal impact on model quality in preliminary\nsmallscale experiments. SQA was discovered serendipitously during the\ndevelopment of the upcoming Reactive Transformer architecture, suggesting its\npotential as a powerful tool for building more efficient and scalable models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01817.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "675197c3ae96d7ba4b4a6c66",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/I2GHgrv70cfT8C5EbK6Q5.png",
            "fullname": "Adam Filipek",
            "name": "AdamF92",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "675776b060e4100500aeb4c8",
            "name": "ReactiveAI",
            "fullname": "Reactive AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/675197c3ae96d7ba4b4a6c66/AJDkLuavcYfENIRDzxjqR.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.01796",
            "authors": [
                {
                    "_id": "68df2db5df49fb0df1e03c13",
                    "name": "Meng-Hsi Chen",
                    "hidden": false
                },
                {
                    "_id": "68df2db5df49fb0df1e03c14",
                    "name": "Yu-Ang Lee",
                    "hidden": false
                },
                {
                    "_id": "68df2db5df49fb0df1e03c15",
                    "user": {
                        "_id": "643fb7332397d8eef5b844cd",
                        "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
                        "isPro": false,
                        "fullname": "Feng-Ting Liao",
                        "user": "FengTing",
                        "type": "user"
                    },
                    "name": "Feng-Ting Liao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:30:06.450Z",
                    "hidden": false
                },
                {
                    "_id": "68df2db5df49fb0df1e03c16",
                    "name": "Da-shan Shiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T08:38:15.000Z",
            "submittedOnDailyAt": "2025-10-03T00:28:44.666Z",
            "title": "Rethinking the shape convention of an MLP",
            "submittedOnDailyBy": {
                "_id": "643fb7332397d8eef5b844cd",
                "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
                "isPro": false,
                "fullname": "Feng-Ting Liao",
                "user": "FengTing",
                "type": "user"
            },
            "summary": "Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow\ndesign where skip connections operate at the input/output dimensions while\nprocessing occurs in expanded hidden spaces. We challenge this convention by\nproposing wide-narrow-wide (Hourglass) MLP blocks where skip connections\noperate at expanded dimensions while residual computation flows through narrow\nbottlenecks. This inversion leverages higher-dimensional spaces for incremental\nrefinement while maintaining computational efficiency through parameter-matched\ndesigns. Implementing Hourglass MLPs requires an initial projection to lift\ninput signals to expanded dimensions. We propose that this projection can\nremain fixed at random initialization throughout training, enabling efficient\ntraining and inference implementations. We evaluate both architectures on\ngenerative tasks over popular image datasets, characterizing\nperformance-parameter Pareto frontiers through systematic architectural search.\nResults show that Hourglass architectures consistently achieve superior Pareto\nfrontiers compared to conventional designs. As parameter budgets increase,\noptimal Hourglass configurations favor deeper networks with wider skip\nconnections and narrower bottlenecks-a scaling pattern distinct from\nconventional MLPs. Our findings suggest reconsidering skip connection placement\nin modern architectures, with potential applications extending to Transformers\nand other residual networks.",
            "upvotes": 3,
            "discussionId": "68df2db5df49fb0df1e03c17",
            "ai_summary": "Hourglass MLP blocks, with skip connections in expanded dimensions and narrow bottlenecks, outperform conventional narrow-wide-narrow MLPs in generative tasks across image datasets.",
            "ai_keywords": [
                "multi-layer perceptrons",
                "MLPs",
                "wide-narrow-wide",
                "Hourglass",
                "skip connections",
                "residual computation",
                "parameter-matched designs",
                "random initialization",
                "generative tasks",
                "Pareto frontiers",
                "architectural search",
                "Transformers",
                "residual networks"
            ],
            "organization": {
                "_id": "6388e08c5a3d2a335624705b",
                "name": "MediaTek-Research",
                "fullname": "MediaTek Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669948083850-6213410828005421265b27d3.jpeg"
            }
        },
        "publishedAt": "2025-10-02T04:38:15.000Z",
        "title": "Rethinking the shape convention of an MLP",
        "summary": "Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow\ndesign where skip connections operate at the input/output dimensions while\nprocessing occurs in expanded hidden spaces. We challenge this convention by\nproposing wide-narrow-wide (Hourglass) MLP blocks where skip connections\noperate at expanded dimensions while residual computation flows through narrow\nbottlenecks. This inversion leverages higher-dimensional spaces for incremental\nrefinement while maintaining computational efficiency through parameter-matched\ndesigns. Implementing Hourglass MLPs requires an initial projection to lift\ninput signals to expanded dimensions. We propose that this projection can\nremain fixed at random initialization throughout training, enabling efficient\ntraining and inference implementations. We evaluate both architectures on\ngenerative tasks over popular image datasets, characterizing\nperformance-parameter Pareto frontiers through systematic architectural search.\nResults show that Hourglass architectures consistently achieve superior Pareto\nfrontiers compared to conventional designs. As parameter budgets increase,\noptimal Hourglass configurations favor deeper networks with wider skip\nconnections and narrower bottlenecks-a scaling pattern distinct from\nconventional MLPs. Our findings suggest reconsidering skip connection placement\nin modern architectures, with potential applications extending to Transformers\nand other residual networks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01796.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643fb7332397d8eef5b844cd",
            "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
            "fullname": "Feng-Ting Liao",
            "name": "FengTing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "6388e08c5a3d2a335624705b",
            "name": "MediaTek-Research",
            "fullname": "MediaTek Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669948083850-6213410828005421265b27d3.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.00523",
            "authors": [
                {
                    "_id": "68df2bccdf49fb0df1e03bed",
                    "name": "Wei-Yao Wang",
                    "hidden": false
                },
                {
                    "_id": "68df2bccdf49fb0df1e03bee",
                    "name": "Kazuya Tateishi",
                    "hidden": false
                },
                {
                    "_id": "68df2bccdf49fb0df1e03bef",
                    "name": "Qiyu Wu",
                    "hidden": false
                },
                {
                    "_id": "68df2bccdf49fb0df1e03bf0",
                    "name": "Shusuke Takahashi",
                    "hidden": false
                },
                {
                    "_id": "68df2bccdf49fb0df1e03bf1",
                    "name": "Yuki Mitsufuji",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T05:11:54.000Z",
            "submittedOnDailyAt": "2025-10-03T00:22:23.038Z",
            "title": "VIRTUE: Visual-Interactive Text-Image Universal Embedder",
            "submittedOnDailyBy": {
                "_id": "66fbb4eac5e1b393cf3266d8",
                "avatarUrl": "/avatars/a9cff3c9466849dc19cad0bd37fb0459.svg",
                "isPro": false,
                "fullname": "Wei-Yao Wang",
                "user": "SwyWang",
                "type": "user"
            },
            "summary": "Multimodal representation learning models have demonstrated successful\noperation across complex tasks, and the integration of vision-language models\n(VLMs) has further enabled embedding models with instruction-following\ncapabilities. However, existing embedding models lack visual-interactive\ncapabilities to specify regions of interest from users (e.g., point, bounding\nbox, mask), which have been explored in generative models to broaden their\nhuman-interactive applicability. Equipping embedding models with visual\ninteractions not only would unlock new applications with localized grounding of\nuser intent, which remains unexplored, but also enable the models to learn\nentity-level information within images to complement their global\nrepresentations for conventional embedding tasks. In this paper, we propose a\nnovel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends\nthe capabilities of the segmentation model and the vision-language model to the\nrealm of representation learning. In VIRTUE, the segmentation model can process\nvisual prompts that pinpoint specific regions within an image, thereby enabling\nthe embedder to handle complex and ambiguous scenarios more precisely. To\nevaluate the visual-interaction ability of VIRTUE, we introduce a large-scale\nSegmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples\nthat aims to retrieve the text caption by jointly considering the entity with a\nspecific object and image scene. VIRTUE consistently achieves a\nstate-of-the-art performance with significant improvements across 36 universal\nMMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.",
            "upvotes": 3,
            "discussionId": "68df2bcddf49fb0df1e03bf2",
            "ai_summary": "VIRTUE, a novel Visual-InteRactive Text-Image Universal Embedder, integrates segmentation and vision-language models to enable visual interactions and localized grounding, achieving state-of-the-art performance in representation learning tasks.",
            "ai_keywords": [
                "multimodal representation learning",
                "vision-language models",
                "visual-interactive capabilities",
                "segmentation model",
                "visual prompts",
                "entity-level information",
                "global representations",
                "Segmentation-and-Scene Caption Retrieval",
                "MMEB",
                "SCaR"
            ]
        },
        "publishedAt": "2025-10-01T01:11:54.000Z",
        "title": "VIRTUE: Visual-Interactive Text-Image Universal Embedder",
        "summary": "Multimodal representation learning models have demonstrated successful\noperation across complex tasks, and the integration of vision-language models\n(VLMs) has further enabled embedding models with instruction-following\ncapabilities. However, existing embedding models lack visual-interactive\ncapabilities to specify regions of interest from users (e.g., point, bounding\nbox, mask), which have been explored in generative models to broaden their\nhuman-interactive applicability. Equipping embedding models with visual\ninteractions not only would unlock new applications with localized grounding of\nuser intent, which remains unexplored, but also enable the models to learn\nentity-level information within images to complement their global\nrepresentations for conventional embedding tasks. In this paper, we propose a\nnovel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends\nthe capabilities of the segmentation model and the vision-language model to the\nrealm of representation learning. In VIRTUE, the segmentation model can process\nvisual prompts that pinpoint specific regions within an image, thereby enabling\nthe embedder to handle complex and ambiguous scenarios more precisely. To\nevaluate the visual-interaction ability of VIRTUE, we introduce a large-scale\nSegmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples\nthat aims to retrieve the text caption by jointly considering the entity with a\nspecific object and image scene. VIRTUE consistently achieves a\nstate-of-the-art performance with significant improvements across 36 universal\nMMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00523.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66fbb4eac5e1b393cf3266d8",
            "avatarUrl": "/avatars/a9cff3c9466849dc19cad0bd37fb0459.svg",
            "fullname": "Wei-Yao Wang",
            "name": "SwyWang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.24203",
            "authors": [
                {
                    "_id": "68df35c1df49fb0df1e03cb3",
                    "name": "Chaorui Yao",
                    "hidden": false
                },
                {
                    "_id": "68df35c1df49fb0df1e03cb4",
                    "user": {
                        "_id": "6576f9f4654561a1b345610b",
                        "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
                        "isPro": false,
                        "fullname": "Yanxi Chen",
                        "user": "yanxi-chen",
                        "type": "user"
                    },
                    "name": "Yanxi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:29:49.553Z",
                    "hidden": false
                },
                {
                    "_id": "68df35c1df49fb0df1e03cb5",
                    "name": "Yuchang Sun",
                    "hidden": false
                },
                {
                    "_id": "68df35c1df49fb0df1e03cb6",
                    "name": "Yushuo Chen",
                    "hidden": false
                },
                {
                    "_id": "68df35c1df49fb0df1e03cb7",
                    "name": "Wenhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68df35c1df49fb0df1e03cb8",
                    "name": "Xuchen Pan",
                    "hidden": false
                },
                {
                    "_id": "68df35c1df49fb0df1e03cb9",
                    "name": "Yaliang Li",
                    "hidden": false
                },
                {
                    "_id": "68df35c1df49fb0df1e03cba",
                    "name": "Bolin Ding",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T02:34:54.000Z",
            "submittedOnDailyAt": "2025-10-03T01:08:33.784Z",
            "title": "Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm:\n  Demystifying Some Myths About GRPO and Its Friends",
            "submittedOnDailyBy": {
                "_id": "6576f9f4654561a1b345610b",
                "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
                "isPro": false,
                "fullname": "Yanxi Chen",
                "user": "yanxi-chen",
                "type": "user"
            },
            "summary": "Off-policy reinforcement learning (RL) for large language models (LLMs) is\nattracting growing interest, driven by practical constraints in real-world\napplications, the complexity of LLM-RL infrastructure, and the need for further\ninnovations of RL methodologies. While classic REINFORCE and its modern\nvariants like Group Relative Policy Optimization (GRPO) are typically regarded\nas on-policy algorithms with limited tolerance of off-policyness, we present in\nthis work a first-principles derivation for group-relative REINFORCE without\nassuming a specific training data distribution, showing that it admits a native\noff-policy interpretation. This perspective yields two general principles for\nadapting REINFORCE to off-policy settings: regularizing policy updates, and\nactively shaping the data distribution. Our analysis demystifies some myths\nabout the roles of importance sampling and clipping in GRPO, unifies and\nreinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and\nAsymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss,\nand offers theoretical justification for seemingly heuristic data-weighting\nstrategies. Our findings lead to actionable insights that are validated with\nextensive empirical studies, and open up new opportunities for principled\nalgorithm design in off-policy RL for LLMs. Source code for this work is\navailable at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.",
            "upvotes": 3,
            "discussionId": "68df35c2df49fb0df1e03cbb",
            "githubRepo": "https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k",
            "ai_summary": "Off-policy reinforcement learning for large language models is explored through a new derivation of group-relative REINFORCE, offering insights into importance sampling, clipping, and data-weighting strategies.",
            "ai_keywords": [
                "off-policy reinforcement learning",
                "large language models",
                "REINFORCE",
                "Group Relative Policy Optimization",
                "GRPO",
                "Online Policy Mirror Descent",
                "OPMD",
                "Asymmetric REINFORCE",
                "AsymRE",
                "importance sampling",
                "clipping",
                "data-weighting strategies"
            ],
            "githubStars": 357
        },
        "publishedAt": "2025-09-28T22:34:54.000Z",
        "title": "Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm:\n  Demystifying Some Myths About GRPO and Its Friends",
        "summary": "Off-policy reinforcement learning (RL) for large language models (LLMs) is\nattracting growing interest, driven by practical constraints in real-world\napplications, the complexity of LLM-RL infrastructure, and the need for further\ninnovations of RL methodologies. While classic REINFORCE and its modern\nvariants like Group Relative Policy Optimization (GRPO) are typically regarded\nas on-policy algorithms with limited tolerance of off-policyness, we present in\nthis work a first-principles derivation for group-relative REINFORCE without\nassuming a specific training data distribution, showing that it admits a native\noff-policy interpretation. This perspective yields two general principles for\nadapting REINFORCE to off-policy settings: regularizing policy updates, and\nactively shaping the data distribution. Our analysis demystifies some myths\nabout the roles of importance sampling and clipping in GRPO, unifies and\nreinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and\nAsymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss,\nand offers theoretical justification for seemingly heuristic data-weighting\nstrategies. Our findings lead to actionable insights that are validated with\nextensive empirical studies, and open up new opportunities for principled\nalgorithm design in off-policy RL for LLMs. Source code for this work is\navailable at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24203.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6576f9f4654561a1b345610b",
            "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
            "fullname": "Yanxi Chen",
            "name": "yanxi-chen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.01241",
            "authors": [
                {
                    "_id": "68df2f6edf49fb0df1e03c26",
                    "name": "Hu Wei",
                    "hidden": false
                },
                {
                    "_id": "68df2f6edf49fb0df1e03c27",
                    "name": "Ze Xu",
                    "hidden": false
                },
                {
                    "_id": "68df2f6edf49fb0df1e03c28",
                    "name": "Boyu Yang",
                    "hidden": false
                },
                {
                    "_id": "68df2f6edf49fb0df1e03c29",
                    "name": "Linlin Miao",
                    "hidden": false
                },
                {
                    "_id": "68df2f6edf49fb0df1e03c2a",
                    "name": "Weiqi Zhai",
                    "hidden": false
                },
                {
                    "_id": "68df2f6edf49fb0df1e03c2b",
                    "name": "Yihan Li",
                    "hidden": false
                },
                {
                    "_id": "68df2f6edf49fb0df1e03c2c",
                    "name": "Zixuan Li",
                    "hidden": false
                },
                {
                    "_id": "68df2f6edf49fb0df1e03c2d",
                    "name": "Zhijun Wang",
                    "hidden": false
                },
                {
                    "_id": "68df2f6edf49fb0df1e03c2e",
                    "name": "Boya Wang",
                    "hidden": false
                },
                {
                    "_id": "68df2f6edf49fb0df1e03c2f",
                    "name": "Jianwei Yu",
                    "hidden": false
                },
                {
                    "_id": "68df2f6edf49fb0df1e03c30",
                    "name": "Jialing Yuan",
                    "hidden": false
                },
                {
                    "_id": "68df2f6edf49fb0df1e03c31",
                    "name": "Xiaoyue Zhang",
                    "hidden": false
                },
                {
                    "_id": "68df2f6edf49fb0df1e03c32",
                    "name": "Cheng He",
                    "hidden": false
                },
                {
                    "_id": "68df2f6edf49fb0df1e03c33",
                    "name": "Minglei Chen",
                    "hidden": false
                },
                {
                    "_id": "68df2f6edf49fb0df1e03c34",
                    "name": "Zifan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68df2f6edf49fb0df1e03c35",
                    "name": "Qianhui Li",
                    "hidden": false
                },
                {
                    "_id": "68df2f6edf49fb0df1e03c36",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "68df2f6edf49fb0df1e03c37",
                    "name": "Xiang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-24T02:09:32.000Z",
            "submittedOnDailyAt": "2025-10-03T00:36:10.446Z",
            "title": "SKYLENAGE Technical Report: Mathematical Reasoning and\n  Contest-Innovation Benchmarks for Multi-Level Math Evaluation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Large language models (LLMs) now perform strongly on many public math suites,\nyet frontier separation within mathematics increasingly suffers from ceiling\neffects. We present two complementary benchmarks: SKYLENAGE-ReasoningMATH, a\n100-item, structure-aware diagnostic set with per-item metadata on length,\nnumeric density, and symbolic complexity; and SKYLENAGE-MATH, a 150-item\ncontest-style suite spanning four stages from high school to doctoral under a\nseven-subject taxonomy. We evaluate fifteen contemporary LLM variants under a\nsingle setup and analyze subject x model and grade x model performance. On the\ncontest suite, the strongest model reaches 44% while the runner-up reaches 37%;\naccuracy declines from high school to doctoral, and top systems exhibit a\ndoctoral-to-high-school retention near 79%. On the reasoning set, the best\nmodel attains 81% overall, and hardest-slice results reveal clear robustness\ngaps between leaders and the mid-tier. In summary, we release\nSKYLENAGE-ReasoningMATH and report aggregate results for SKYLENAGE-MATH;\ntogether, SKYLENAGE provides a hard, reasoning-centered and broadly covering\nmath benchmark with calibrated difficulty and rich metadata, serving as a\nreference benchmark for future evaluations of mathematical reasoning.",
            "upvotes": 3,
            "discussionId": "68df2f6edf49fb0df1e03c38",
            "ai_summary": "SKYLENAGE benchmarks evaluate LLMs on math reasoning, revealing performance gaps and ceiling effects across different educational levels.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "SKYLENAGE-ReasoningMATH",
                "SKYLENAGE-MATH",
                "structure-aware diagnostic set",
                "numeric density",
                "symbolic complexity",
                "contest-style suite",
                "subject x model performance",
                "grade x model performance",
                "mathematical reasoning"
            ]
        },
        "publishedAt": "2025-09-23T22:09:32.000Z",
        "title": "SKYLENAGE Technical Report: Mathematical Reasoning and\n  Contest-Innovation Benchmarks for Multi-Level Math Evaluation",
        "summary": "Large language models (LLMs) now perform strongly on many public math suites,\nyet frontier separation within mathematics increasingly suffers from ceiling\neffects. We present two complementary benchmarks: SKYLENAGE-ReasoningMATH, a\n100-item, structure-aware diagnostic set with per-item metadata on length,\nnumeric density, and symbolic complexity; and SKYLENAGE-MATH, a 150-item\ncontest-style suite spanning four stages from high school to doctoral under a\nseven-subject taxonomy. We evaluate fifteen contemporary LLM variants under a\nsingle setup and analyze subject x model and grade x model performance. On the\ncontest suite, the strongest model reaches 44% while the runner-up reaches 37%;\naccuracy declines from high school to doctoral, and top systems exhibit a\ndoctoral-to-high-school retention near 79%. On the reasoning set, the best\nmodel attains 81% overall, and hardest-slice results reveal clear robustness\ngaps between leaders and the mid-tier. In summary, we release\nSKYLENAGE-ReasoningMATH and report aggregate results for SKYLENAGE-MATH;\ntogether, SKYLENAGE provides a hard, reasoning-centered and broadly covering\nmath benchmark with calibrated difficulty and rich metadata, serving as a\nreference benchmark for future evaluations of mathematical reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01241.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 118
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.02272",
            "authors": [
                {
                    "_id": "68df3a3bdf49fb0df1e03cbd",
                    "name": "Wen Yang",
                    "hidden": false
                },
                {
                    "_id": "68df3a3bdf49fb0df1e03cbe",
                    "name": "Junhong Wu",
                    "hidden": false
                },
                {
                    "_id": "68df3a3bdf49fb0df1e03cbf",
                    "name": "Chong Li",
                    "hidden": false
                },
                {
                    "_id": "68df3a3bdf49fb0df1e03cc0",
                    "name": "Chengqing Zong",
                    "hidden": false
                },
                {
                    "_id": "68df3a3bdf49fb0df1e03cc1",
                    "name": "Jiajun Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T17:49:49.000Z",
            "submittedOnDailyAt": "2025-10-03T01:27:58.747Z",
            "title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A\n  Cross-Linguistic Perspective",
            "submittedOnDailyBy": {
                "_id": "641fd72a73cfc036ddbf69c8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641fd72a73cfc036ddbf69c8/W4HG5HRm-OkzVpOotgu3m.jpeg",
                "isPro": false,
                "fullname": "WenYang",
                "user": "James-WYang",
                "type": "user"
            },
            "summary": "Recent advancements in Reinforcement Post-Training (RPT) have significantly\nenhanced the capabilities of Large Reasoning Models (LRMs), sparking increased\ninterest in the generalization of RL-based reasoning. While existing work has\nprimarily focused on investigating its generalization across tasks or\nmodalities, this study proposes a novel cross-linguistic perspective to\ninvestigate reasoning generalization. This raises a crucial question:\nDoes the reasoning capability achieved from English RPT effectively\ntransfer to other languages? We address this by systematically evaluating\nEnglish-centric LRMs on multilingual reasoning benchmarks and introducing a\nmetric to quantify cross-lingual transferability. Our findings reveal that\ncross-lingual transferability varies significantly across initial model, target\nlanguage, and training paradigm. Through interventional studies, we find that\nmodels with stronger initial English capabilities tend to over-rely on\nEnglish-specific patterns, leading to diminished cross-lingual generalization.\nTo address this, we conduct a thorough parallel training study. Experimental\nresults yield three key findings: First-Parallel Leap, a substantial\nleap in performance when transitioning from monolingual to just a single\nparallel language, and a predictable Parallel Scaling Law, revealing\nthat cross-lingual reasoning transfer follows a power-law with the number of\ntraining parallel languages. Moreover, we identify the discrepancy between\nactual monolingual performance and the power-law prediction as\nMonolingual Generalization Gap, indicating that English-centric LRMs\nfail to fully generalize across languages. Our study challenges the assumption\nthat LRM reasoning mirrors human cognition, providing critical insights for the\ndevelopment of more language-agnostic LRMs.",
            "upvotes": 2,
            "discussionId": "68df3a3bdf49fb0df1e03cc2",
            "ai_summary": "Research investigates cross-linguistic transferability of reasoning capabilities in Large Reasoning Models, revealing significant variations and proposing a parallel training approach to improve generalization across languages.",
            "ai_keywords": [
                "Reinforcement Post-Training",
                "Large Reasoning Models",
                "cross-linguistic perspective",
                "multilingual reasoning benchmarks",
                "cross-lingual transferability",
                "interventional studies",
                "parallel training",
                "Parallel Leap",
                "Parallel Scaling Law",
                "Monolingual Generalization Gap"
            ],
            "organization": {
                "_id": "640a887796aae649741a586f",
                "name": "CASIA",
                "fullname": "Chinese Academic of Science Institute of Automation",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"
            }
        },
        "publishedAt": "2025-10-02T13:49:49.000Z",
        "title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A\n  Cross-Linguistic Perspective",
        "summary": "Recent advancements in Reinforcement Post-Training (RPT) have significantly\nenhanced the capabilities of Large Reasoning Models (LRMs), sparking increased\ninterest in the generalization of RL-based reasoning. While existing work has\nprimarily focused on investigating its generalization across tasks or\nmodalities, this study proposes a novel cross-linguistic perspective to\ninvestigate reasoning generalization. This raises a crucial question:\nDoes the reasoning capability achieved from English RPT effectively\ntransfer to other languages? We address this by systematically evaluating\nEnglish-centric LRMs on multilingual reasoning benchmarks and introducing a\nmetric to quantify cross-lingual transferability. Our findings reveal that\ncross-lingual transferability varies significantly across initial model, target\nlanguage, and training paradigm. Through interventional studies, we find that\nmodels with stronger initial English capabilities tend to over-rely on\nEnglish-specific patterns, leading to diminished cross-lingual generalization.\nTo address this, we conduct a thorough parallel training study. Experimental\nresults yield three key findings: First-Parallel Leap, a substantial\nleap in performance when transitioning from monolingual to just a single\nparallel language, and a predictable Parallel Scaling Law, revealing\nthat cross-lingual reasoning transfer follows a power-law with the number of\ntraining parallel languages. Moreover, we identify the discrepancy between\nactual monolingual performance and the power-law prediction as\nMonolingual Generalization Gap, indicating that English-centric LRMs\nfail to fully generalize across languages. Our study challenges the assumption\nthat LRM reasoning mirrors human cognition, providing critical insights for the\ndevelopment of more language-agnostic LRMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02272.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "641fd72a73cfc036ddbf69c8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641fd72a73cfc036ddbf69c8/W4HG5HRm-OkzVpOotgu3m.jpeg",
            "fullname": "WenYang",
            "name": "James-WYang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "640a887796aae649741a586f",
            "name": "CASIA",
            "fullname": "Chinese Academic of Science Institute of Automation",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01143",
            "authors": [
                {
                    "_id": "68df7fbadf49fb0df1e03e05",
                    "name": "Harry Dong",
                    "hidden": false
                },
                {
                    "_id": "68df7fbadf49fb0df1e03e06",
                    "name": "David Brandfonbrener",
                    "hidden": false
                },
                {
                    "_id": "68df7fbadf49fb0df1e03e07",
                    "name": "Eryk Helenowski",
                    "hidden": false
                },
                {
                    "_id": "68df7fbadf49fb0df1e03e08",
                    "name": "Yun He",
                    "hidden": false
                },
                {
                    "_id": "68df7fbadf49fb0df1e03e09",
                    "name": "Mrinal Kumar",
                    "hidden": false
                },
                {
                    "_id": "68df7fbadf49fb0df1e03e0a",
                    "name": "Han Fang",
                    "hidden": false
                },
                {
                    "_id": "68df7fbadf49fb0df1e03e0b",
                    "name": "Yuejie Chi",
                    "hidden": false
                },
                {
                    "_id": "68df7fbadf49fb0df1e03e0c",
                    "name": "Karthik Abinav Sankararaman",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T17:33:35.000Z",
            "submittedOnDailyAt": "2025-10-03T06:18:45.587Z",
            "title": "Generalized Parallel Scaling with Interdependent Generations",
            "submittedOnDailyBy": {
                "_id": "64d98ef7a4839890b25eb78b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
                "isPro": true,
                "fullname": "Fangyuan Yu",
                "user": "Ksgk-fy",
                "type": "user"
            },
            "summary": "Parallel LLM inference scaling involves sampling a set of N>1 responses for\na single input prompt. However, these N parallel responses tend to be\ngenerated independently from each other, partitioning compute resources and\nleaving potentially useful information in one generation untapped by others.\nThis is in contrast to response length scaling where past computation is used\nin all future steps. For higher quality responses and response sets, we propose\nBridge to generate interdependent responses in parallel by rethinking batched\nLLM hidden states as holistic tensors rather than independent slices. With only\na small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean\naccuracy gains from reinforcement learning with verifiable rewards by up to 50%\nand boosts consistency of correct responses. Trained once, Bridge scales to any\ngeneration width, all with greater performance than independent generations,\nunlocking a more general mode of parallel scaling that effectively leverages\ninformation between sequences, compatible with any post-generation aggregation\ntechnique.",
            "upvotes": 2,
            "discussionId": "68df7fbadf49fb0df1e03e0d",
            "ai_summary": "Bridge enhances parallel LLM inference by generating interdependent responses, improving accuracy and consistency with minimal additional parameters.",
            "ai_keywords": [
                "LLM",
                "parallel inference",
                "response length scaling",
                "batched LLM hidden states",
                "holistic tensors",
                "reinforcement learning",
                "verifiable rewards",
                "generation width",
                "post-generation aggregation"
            ]
        },
        "publishedAt": "2025-10-01T13:33:35.000Z",
        "title": "Generalized Parallel Scaling with Interdependent Generations",
        "summary": "Parallel LLM inference scaling involves sampling a set of N>1 responses for\na single input prompt. However, these N parallel responses tend to be\ngenerated independently from each other, partitioning compute resources and\nleaving potentially useful information in one generation untapped by others.\nThis is in contrast to response length scaling where past computation is used\nin all future steps. For higher quality responses and response sets, we propose\nBridge to generate interdependent responses in parallel by rethinking batched\nLLM hidden states as holistic tensors rather than independent slices. With only\na small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean\naccuracy gains from reinforcement learning with verifiable rewards by up to 50%\nand boosts consistency of correct responses. Trained once, Bridge scales to any\ngeneration width, all with greater performance than independent generations,\nunlocking a more general mode of parallel scaling that effectively leverages\ninformation between sequences, compatible with any post-generation aggregation\ntechnique.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01143.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d98ef7a4839890b25eb78b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
            "fullname": "Fangyuan Yu",
            "name": "Ksgk-fy",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.00137",
            "authors": [
                {
                    "_id": "68decb5cdf49fb0df1e03a11",
                    "name": "Nima Sheikholeslami",
                    "hidden": false
                },
                {
                    "_id": "68decb5cdf49fb0df1e03a12",
                    "name": "Erfan Hosseini",
                    "hidden": false
                },
                {
                    "_id": "68decb5cdf49fb0df1e03a13",
                    "user": {
                        "_id": "607f060442beb4da0f990182",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/607f060442beb4da0f990182/j5W2tLyU6JqkaTf3kv66s.jpeg",
                        "isPro": false,
                        "fullname": "Patrice Bechard",
                        "user": "patricebechard",
                        "type": "user"
                    },
                    "name": "Patrice Bechard",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:33:23.354Z",
                    "hidden": false
                },
                {
                    "_id": "68decb5cdf49fb0df1e03a14",
                    "name": "Srivatsava Daruru",
                    "hidden": false
                },
                {
                    "_id": "68decb5cdf49fb0df1e03a15",
                    "name": "Sai Rajeswar",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T18:14:01.000Z",
            "submittedOnDailyAt": "2025-10-03T11:08:21.763Z",
            "title": "Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval",
            "submittedOnDailyBy": {
                "_id": "607f060442beb4da0f990182",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/607f060442beb4da0f990182/j5W2tLyU6JqkaTf3kv66s.jpeg",
                "isPro": false,
                "fullname": "Patrice Bechard",
                "user": "patricebechard",
                "type": "user"
            },
            "summary": "Dual-encoder retrievers depend on the principle that relevant documents\nshould score higher than irrelevant ones for a given query. Yet the dominant\nNoise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss,\noptimizes a softened ranking surrogate that we rigorously prove is\nfundamentally oblivious to score separation quality and unrelated to AUC. This\nmismatch leads to poor calibration and suboptimal performance in downstream\ntasks like retrieval-augmented generation (RAG). To address this fundamental\nlimitation, we introduce the MW loss, a new training objective that maximizes\nthe Mann-Whitney U statistic, which is mathematically equivalent to the Area\nunder the ROC Curve (AUC). MW loss encourages each positive-negative pair to be\ncorrectly ranked by minimizing binary cross entropy over score differences. We\nprovide theoretical guarantees that MW loss directly upper-bounds the AoC,\nbetter aligning optimization with retrieval goals. We further promote ROC\ncurves and AUC as natural threshold free diagnostics for evaluating retriever\ncalibration and ranking quality. Empirically, retrievers trained with MW loss\nconsistently outperform contrastive counterparts in AUC and standard retrieval\nmetrics. Our experiments show that MW loss is an empirically superior\nalternative to Contrastive Loss, yielding better-calibrated and more\ndiscriminative retrievers for high-stakes applications like RAG.",
            "upvotes": 2,
            "discussionId": "68decb5cdf49fb0df1e03a16",
            "ai_summary": "A new training objective, MW loss, is introduced to improve retriever calibration and ranking quality by directly optimizing the Area under the ROC Curve (AUC), outperforming Contrastive Loss in retrieval-augmented generation tasks.",
            "ai_keywords": [
                "Noise Contrastive Estimation",
                "Contrastive Loss",
                "Mann-Whitney U statistic",
                "Area under the ROC Curve",
                "AUC",
                "binary cross entropy",
                "retrieval-augmented generation",
                "RAG"
            ],
            "organization": {
                "_id": "65f4df5de83b55da5d79fbb6",
                "name": "ServiceNow-AI",
                "fullname": "ServiceNow-AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63d3095c2727d7888cbb54e2/Uv-Lx8PVGviqokfOyYlCN.png"
            }
        },
        "publishedAt": "2025-09-30T14:14:01.000Z",
        "title": "Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval",
        "summary": "Dual-encoder retrievers depend on the principle that relevant documents\nshould score higher than irrelevant ones for a given query. Yet the dominant\nNoise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss,\noptimizes a softened ranking surrogate that we rigorously prove is\nfundamentally oblivious to score separation quality and unrelated to AUC. This\nmismatch leads to poor calibration and suboptimal performance in downstream\ntasks like retrieval-augmented generation (RAG). To address this fundamental\nlimitation, we introduce the MW loss, a new training objective that maximizes\nthe Mann-Whitney U statistic, which is mathematically equivalent to the Area\nunder the ROC Curve (AUC). MW loss encourages each positive-negative pair to be\ncorrectly ranked by minimizing binary cross entropy over score differences. We\nprovide theoretical guarantees that MW loss directly upper-bounds the AoC,\nbetter aligning optimization with retrieval goals. We further promote ROC\ncurves and AUC as natural threshold free diagnostics for evaluating retriever\ncalibration and ranking quality. Empirically, retrievers trained with MW loss\nconsistently outperform contrastive counterparts in AUC and standard retrieval\nmetrics. Our experiments show that MW loss is an empirically superior\nalternative to Contrastive Loss, yielding better-calibrated and more\ndiscriminative retrievers for high-stakes applications like RAG.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00137.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "607f060442beb4da0f990182",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/607f060442beb4da0f990182/j5W2tLyU6JqkaTf3kv66s.jpeg",
            "fullname": "Patrice Bechard",
            "name": "patricebechard",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "65f4df5de83b55da5d79fbb6",
            "name": "ServiceNow-AI",
            "fullname": "ServiceNow-AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63d3095c2727d7888cbb54e2/Uv-Lx8PVGviqokfOyYlCN.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25729",
            "authors": [
                {
                    "_id": "68df3569df49fb0df1e03caf",
                    "user": {
                        "_id": "6611cb2d80d295a494f08f7e",
                        "avatarUrl": "/avatars/b6095891d4260c625fa727927dac9583.svg",
                        "isPro": false,
                        "fullname": "Zihao Zhao",
                        "user": "zzhao0104",
                        "type": "user"
                    },
                    "name": "Zihao Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:29:53.878Z",
                    "hidden": false
                },
                {
                    "_id": "68df3569df49fb0df1e03cb0",
                    "name": "Anjalie Field",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T03:38:36.000Z",
            "submittedOnDailyAt": "2025-10-03T14:43:46.276Z",
            "title": "Controlled Generation for Private Synthetic Text",
            "submittedOnDailyBy": {
                "_id": "6611cb2d80d295a494f08f7e",
                "avatarUrl": "/avatars/b6095891d4260c625fa727927dac9583.svg",
                "isPro": false,
                "fullname": "Zihao Zhao",
                "user": "zzhao0104",
                "type": "user"
            },
            "summary": "Text anonymization is essential for responsibly developing and deploying AI\nin high-stakes domains such as healthcare, social services, and law. In this\nwork, we propose a novel methodology for privacy-preserving synthetic text\ngeneration that leverages the principles of de-identification and the Hiding In\nPlain Sight (HIPS) theory. Our approach introduces entity-aware control codes\nto guide controllable generation using either in-context learning (ICL) or\nprefix tuning. The ICL variant ensures privacy levels consistent with the\nunderlying de-identification system, while the prefix tuning variant\nincorporates a custom masking strategy and loss function to support scalable,\nhigh-quality generation. Experiments on legal and clinical datasets demonstrate\nthat our method achieves a strong balance between privacy protection and\nutility, offering a practical and effective solution for synthetic text\ngeneration in sensitive domains.",
            "upvotes": 2,
            "discussionId": "68df3569df49fb0df1e03cb1",
            "ai_summary": "A novel methodology for privacy-preserving synthetic text generation using entity-aware control codes and HIPS theory achieves a balance between privacy and utility in sensitive domains.",
            "ai_keywords": [
                "de-identification",
                "Hiding In Plain Sight (HIPS)",
                "entity-aware control codes",
                "in-context learning (ICL)",
                "prefix tuning",
                "custom masking strategy",
                "loss function"
            ],
            "organization": {
                "_id": "6137aeeaf8e9dca6e152bccf",
                "name": "jhu-clsp",
                "fullname": "Center for Language and Speech Processing @ JHU",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1631039662102-6137ad94501f80a6f6e1eac9.png"
            }
        },
        "publishedAt": "2025-09-29T23:38:36.000Z",
        "title": "Controlled Generation for Private Synthetic Text",
        "summary": "Text anonymization is essential for responsibly developing and deploying AI\nin high-stakes domains such as healthcare, social services, and law. In this\nwork, we propose a novel methodology for privacy-preserving synthetic text\ngeneration that leverages the principles of de-identification and the Hiding In\nPlain Sight (HIPS) theory. Our approach introduces entity-aware control codes\nto guide controllable generation using either in-context learning (ICL) or\nprefix tuning. The ICL variant ensures privacy levels consistent with the\nunderlying de-identification system, while the prefix tuning variant\nincorporates a custom masking strategy and loss function to support scalable,\nhigh-quality generation. Experiments on legal and clinical datasets demonstrate\nthat our method achieves a strong balance between privacy protection and\nutility, offering a practical and effective solution for synthetic text\ngeneration in sensitive domains.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25729.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6611cb2d80d295a494f08f7e",
            "avatarUrl": "/avatars/b6095891d4260c625fa727927dac9583.svg",
            "fullname": "Zihao Zhao",
            "name": "zzhao0104",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "6137aeeaf8e9dca6e152bccf",
            "name": "jhu-clsp",
            "fullname": "Center for Language and Speech Processing @ JHU",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1631039662102-6137ad94501f80a6f6e1eac9.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.24304",
            "authors": [
                {
                    "_id": "68dc886d4159d1f2418f9a2e",
                    "name": "Zefeng He",
                    "hidden": false
                },
                {
                    "_id": "68dc886d4159d1f2418f9a2f",
                    "name": "Xiaoye Qu",
                    "hidden": false
                },
                {
                    "_id": "68dc886d4159d1f2418f9a30",
                    "name": "Yafu Li",
                    "hidden": false
                },
                {
                    "_id": "68dc886d4159d1f2418f9a31",
                    "name": "Siyuan Huang",
                    "hidden": false
                },
                {
                    "_id": "68dc886d4159d1f2418f9a32",
                    "name": "Daizong Liu",
                    "hidden": false
                },
                {
                    "_id": "68dc886d4159d1f2418f9a33",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T05:36:58.000Z",
            "submittedOnDailyAt": "2025-10-03T00:43:57.534Z",
            "title": "FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame\n  Spotlighting",
            "submittedOnDailyBy": {
                "_id": "64cb54da1af278541d663708",
                "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
                "isPro": false,
                "fullname": "Xiaoye Qu",
                "user": "Xiaoye08",
                "type": "user"
            },
            "summary": "While Large Vision-Language Models (LVLMs) have achieved substantial progress\nin video understanding, their application to long video reasoning is hindered\nby uniform frame sampling and static textual reasoning, which are inefficient\nand struggle to handle visually intensive video tasks. To overcome these\nchallenges, in this paper, we introduce the concept of thinking with long\nvideos and propose a novel framework FrameThinker. Within this framework, LVLMs\nare able to iteratively interrogate video content. Developing such video\nreasoning capabilities in LVLMs presents notable challenges, particularly in\nadapting the model to new video actions (e.g. select frame), and designing\nreward functions to guide LVLMs to adopt the newly introduced action. To solve\nthese challenges, we propose a two-phase training strategy, first employing\nSupervised Fine-Tuning (SFT) to instill fundamental action capabilities,\nfollowed by Reinforcement Learning (RL) to optimize a strategic decision-making\npolicy. Notably, in this RL phase, we conduct an in-depth and comprehensive\nexploration of the reward design for each action and format reward. Extensive\nexperiments on reasoning benchmarks like Video-Holmes, LongVideo-Reason, and\nlong-video understanding benchmarks such as LongVideoBench, MLVU, VideoMME, and\nLVBench, demonstrate that FrameThinker achieves a significant average\nimprovement of +10.4% over baselines while drastically reducing the number of\nprocessed frames. Most notably, our 7B model, FrameThinker establishes a new\nstate-of-the-art on LongVideo-Reason, achieving 76.1% accuracy using an average\nof only 20.6 frames. This not only outperforms the competitive LongVILA-R1\n(72.0%) but does so with over 20x fewer frames (vs. 512), demonstrating\nunparalleled efficiency and effectiveness.",
            "upvotes": 2,
            "discussionId": "68dc886d4159d1f2418f9a34",
            "projectPage": "https://github.com/lcqysl/FrameThinker-RL",
            "githubRepo": "https://github.com/lcqysl/FrameThinker-RL",
            "ai_summary": "FrameThinker, a novel framework, enhances video reasoning by iteratively interrogating video content through supervised fine-tuning and reinforcement learning, achieving significant improvements and efficiency over existing models.",
            "ai_keywords": [
                "Large Vision-Language Models",
                "LVLMs",
                "video understanding",
                "long video reasoning",
                "frame sampling",
                "textual reasoning",
                "FrameThinker",
                "iterative interrogation",
                "video reasoning capabilities",
                "Supervised Fine-Tuning",
                "SFT",
                "Reinforcement Learning",
                "RL",
                "reward functions",
                "Video-Holmes",
                "LongVideo-Reason",
                "LongVideoBench",
                "MLVU",
                "VideoMME",
                "LVBench",
                "LongVILA-R1"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-09-29T01:36:58.000Z",
        "title": "FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame\n  Spotlighting",
        "summary": "While Large Vision-Language Models (LVLMs) have achieved substantial progress\nin video understanding, their application to long video reasoning is hindered\nby uniform frame sampling and static textual reasoning, which are inefficient\nand struggle to handle visually intensive video tasks. To overcome these\nchallenges, in this paper, we introduce the concept of thinking with long\nvideos and propose a novel framework FrameThinker. Within this framework, LVLMs\nare able to iteratively interrogate video content. Developing such video\nreasoning capabilities in LVLMs presents notable challenges, particularly in\nadapting the model to new video actions (e.g. select frame), and designing\nreward functions to guide LVLMs to adopt the newly introduced action. To solve\nthese challenges, we propose a two-phase training strategy, first employing\nSupervised Fine-Tuning (SFT) to instill fundamental action capabilities,\nfollowed by Reinforcement Learning (RL) to optimize a strategic decision-making\npolicy. Notably, in this RL phase, we conduct an in-depth and comprehensive\nexploration of the reward design for each action and format reward. Extensive\nexperiments on reasoning benchmarks like Video-Holmes, LongVideo-Reason, and\nlong-video understanding benchmarks such as LongVideoBench, MLVU, VideoMME, and\nLVBench, demonstrate that FrameThinker achieves a significant average\nimprovement of +10.4% over baselines while drastically reducing the number of\nprocessed frames. Most notably, our 7B model, FrameThinker establishes a new\nstate-of-the-art on LongVideo-Reason, achieving 76.1% accuracy using an average\nof only 20.6 frames. This not only outperforms the competitive LongVILA-R1\n(72.0%) but does so with over 20x fewer frames (vs. 512), demonstrating\nunparalleled efficiency and effectiveness.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24304.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64cb54da1af278541d663708",
            "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
            "fullname": "Xiaoye Qu",
            "name": "Xiaoye08",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.02306",
            "authors": [
                {
                    "_id": "68dfda5f73e20ab5778419e0",
                    "name": "Raphael Tang",
                    "hidden": false
                },
                {
                    "_id": "68dfda5f73e20ab5778419e1",
                    "name": "Crystina Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dfda5f73e20ab5778419e2",
                    "name": "Wenyan Li",
                    "hidden": false
                },
                {
                    "_id": "68dfda5f73e20ab5778419e3",
                    "name": "Carmen Lai",
                    "hidden": false
                },
                {
                    "_id": "68dfda5f73e20ab5778419e4",
                    "name": "Pontus Stenetorp",
                    "hidden": false
                },
                {
                    "_id": "68dfda5f73e20ab5778419e5",
                    "name": "Yao Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T17:59:41.000Z",
            "submittedOnDailyAt": "2025-10-03T12:47:01.226Z",
            "title": "Drawing Conclusions from Draws: Rethinking Preference Semantics in\n  Arena-Style LLM Evaluation",
            "submittedOnDailyBy": {
                "_id": "63250bb8d206fe7b2d2f1b8b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668053024087-63250bb8d206fe7b2d2f1b8b.jpeg",
                "isPro": false,
                "fullname": "Raphael Tang",
                "user": "tetrisd",
                "type": "user"
            },
            "summary": "In arena-style evaluation of large language models (LLMs), two LLMs respond\nto a user query, and the user chooses the winning response or deems the\n\"battle\" a draw, resulting in an adjustment to the ratings of both models. The\nprevailing approach for modeling these rating dynamics is to view battles as\ntwo-player game matches, as in chess, and apply the Elo rating system and its\nderivatives. In this paper, we critically examine this paradigm. Specifically,\nwe question whether a draw genuinely means that the two models are equal and\nhence whether their ratings should be equalized. Instead, we conjecture that\ndraws are more indicative of query difficulty: if the query is too easy, then\nboth models are more likely to succeed equally. On three real-world arena\ndatasets, we show that ignoring rating updates for draws yields a 1-3% relative\nincrease in battle outcome prediction accuracy (which includes draws) for all\nfour rating systems studied. Further analyses suggest that draws occur more for\nqueries rated as very easy and those as highly objective, with risk ratios of\n1.37 and 1.35, respectively. We recommend future rating systems to reconsider\nexisting draw semantics and to account for query properties in rating updates.",
            "upvotes": 1,
            "discussionId": "68dfda5f73e20ab5778419e6",
            "githubRepo": "https://github.com/daemon/lmarena-draws",
            "ai_summary": "Ignoring rating updates for draws in arena-style evaluations of large language models improves battle outcome prediction accuracy by 1-3% across different rating systems.",
            "ai_keywords": [
                "arena-style evaluation",
                "large language models",
                "Elo rating system",
                "battle outcome prediction",
                "query difficulty",
                "query properties"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "6464cc20860abf030496e986",
                "name": "UniversityCollegeLondon",
                "fullname": "University College London",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6464cadff072e09adec1c372/OszOYithNLCeeNuMIjaoI.png"
            }
        },
        "publishedAt": "2025-10-02T13:59:41.000Z",
        "title": "Drawing Conclusions from Draws: Rethinking Preference Semantics in\n  Arena-Style LLM Evaluation",
        "summary": "In arena-style evaluation of large language models (LLMs), two LLMs respond\nto a user query, and the user chooses the winning response or deems the\n\"battle\" a draw, resulting in an adjustment to the ratings of both models. The\nprevailing approach for modeling these rating dynamics is to view battles as\ntwo-player game matches, as in chess, and apply the Elo rating system and its\nderivatives. In this paper, we critically examine this paradigm. Specifically,\nwe question whether a draw genuinely means that the two models are equal and\nhence whether their ratings should be equalized. Instead, we conjecture that\ndraws are more indicative of query difficulty: if the query is too easy, then\nboth models are more likely to succeed equally. On three real-world arena\ndatasets, we show that ignoring rating updates for draws yields a 1-3% relative\nincrease in battle outcome prediction accuracy (which includes draws) for all\nfour rating systems studied. Further analyses suggest that draws occur more for\nqueries rated as very easy and those as highly objective, with risk ratios of\n1.37 and 1.35, respectively. We recommend future rating systems to reconsider\nexisting draw semantics and to account for query properties in rating updates.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02306.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63250bb8d206fe7b2d2f1b8b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668053024087-63250bb8d206fe7b2d2f1b8b.jpeg",
            "fullname": "Raphael Tang",
            "name": "tetrisd",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "6464cc20860abf030496e986",
            "name": "UniversityCollegeLondon",
            "fullname": "University College London",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6464cadff072e09adec1c372/OszOYithNLCeeNuMIjaoI.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01691",
            "authors": [
                {
                    "_id": "68df2d8edf49fb0df1e03bfd",
                    "name": "Jiyao Liu",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03bfe",
                    "name": "Jinjie Wei",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03bff",
                    "name": "Wanying Qu",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03c00",
                    "name": "Chenglong Ma",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03c01",
                    "name": "Junzhi Ning",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03c02",
                    "name": "Yunheng Li",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03c03",
                    "name": "Ying Chen",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03c04",
                    "name": "Xinzhe Luo",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03c05",
                    "name": "Pengcheng Chen",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03c06",
                    "name": "Xin Gao",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03c07",
                    "name": "Ming Hu",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03c08",
                    "name": "Huihui Xu",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03c09",
                    "name": "Xin Wang",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03c0a",
                    "name": "Shujian Gao",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03c0b",
                    "name": "Dingkang Yang",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03c0c",
                    "name": "Zhongying Deng",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03c0d",
                    "name": "Jin Ye",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03c0e",
                    "name": "Lihao Liu",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03c0f",
                    "name": "Junjun He",
                    "hidden": false
                },
                {
                    "_id": "68df2d8edf49fb0df1e03c10",
                    "name": "Ningsheng Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T05:42:00.000Z",
            "submittedOnDailyAt": "2025-10-03T00:27:45.114Z",
            "title": "MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment\n  Abilities in MLLMs",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Medical Image Quality Assessment (IQA) serves as the first-mile safety gate\nfor clinical AI, yet existing approaches remain constrained by scalar,\nscore-based metrics and fail to reflect the descriptive, human-like reasoning\nprocess central to expert evaluation. To address this gap, we introduce\nMedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning\nparadigm for language-based evaluation of medical image quality with\nMulti-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary\ntasks: (1) MedQ-Perception, which probes low-level perceptual capability via\nhuman-curated questions on fundamental visual attributes; and (2)\nMedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,\naligning model evaluation with human-like reasoning on image quality. The\nbenchmark spans five imaging modalities and over forty quality attributes,\ntotaling 2,600 perceptual queries and 708 reasoning assessments, covering\ndiverse image sources including authentic clinical acquisitions, images with\nsimulated degradations via physics-based reconstructions, and AI-generated\nimages. To evaluate reasoning ability, we propose a multi-dimensional judging\nprotocol that assesses model outputs along four complementary axes. We further\nconduct rigorous human-AI alignment validation by comparing LLM-based judgement\nwith radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates\nthat models exhibit preliminary but unstable perceptual and reasoning skills,\nwith insufficient accuracy for reliable clinical use. These findings highlight\nthe need for targeted optimization of MLLMs in medical IQA. We hope that\nMedQ-Bench will catalyze further exploration and unlock the untapped potential\nof MLLMs for medical image quality evaluation.",
            "upvotes": 1,
            "discussionId": "68df2d8edf49fb0df1e03c11",
            "ai_summary": "MedQ-Bench introduces a benchmark for language-based evaluation of medical image quality using Multi-modal Large Language Models, focusing on both perceptual and reasoning capabilities.",
            "ai_keywords": [
                "Multi-modal Large Language Models",
                "MedQ-Bench",
                "MedQ-Perception",
                "MedQ-Reasoning",
                "perception-reasoning paradigm",
                "human-curated questions",
                "visual attributes",
                "no-reference reasoning",
                "comparison reasoning",
                "multi-dimensional judging protocol",
                "human-AI alignment validation"
            ]
        },
        "publishedAt": "2025-10-02T01:42:00.000Z",
        "title": "MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment\n  Abilities in MLLMs",
        "summary": "Medical Image Quality Assessment (IQA) serves as the first-mile safety gate\nfor clinical AI, yet existing approaches remain constrained by scalar,\nscore-based metrics and fail to reflect the descriptive, human-like reasoning\nprocess central to expert evaluation. To address this gap, we introduce\nMedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning\nparadigm for language-based evaluation of medical image quality with\nMulti-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary\ntasks: (1) MedQ-Perception, which probes low-level perceptual capability via\nhuman-curated questions on fundamental visual attributes; and (2)\nMedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,\naligning model evaluation with human-like reasoning on image quality. The\nbenchmark spans five imaging modalities and over forty quality attributes,\ntotaling 2,600 perceptual queries and 708 reasoning assessments, covering\ndiverse image sources including authentic clinical acquisitions, images with\nsimulated degradations via physics-based reconstructions, and AI-generated\nimages. To evaluate reasoning ability, we propose a multi-dimensional judging\nprotocol that assesses model outputs along four complementary axes. We further\nconduct rigorous human-AI alignment validation by comparing LLM-based judgement\nwith radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates\nthat models exhibit preliminary but unstable perceptual and reasoning skills,\nwith insufficient accuracy for reliable clinical use. These findings highlight\nthe need for targeted optimization of MLLMs in medical IQA. We hope that\nMedQ-Bench will catalyze further exploration and unlock the untapped potential\nof MLLMs for medical image quality evaluation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01691.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 118
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01538",
            "authors": [
                {
                    "_id": "68df42f7df49fb0df1e03d25",
                    "name": "Haokun Zhao",
                    "hidden": false
                },
                {
                    "_id": "68df42f7df49fb0df1e03d26",
                    "user": {
                        "_id": "656553d89bf6665f10e3a92d",
                        "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
                        "isPro": false,
                        "fullname": "xiang wyatt zhang",
                        "user": "Wyattz23",
                        "type": "user"
                    },
                    "name": "Xiang Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:29:20.478Z",
                    "hidden": false
                },
                {
                    "_id": "68df42f7df49fb0df1e03d27",
                    "name": "Jiaqi Wei",
                    "hidden": false
                },
                {
                    "_id": "68df42f7df49fb0df1e03d28",
                    "name": "Yiwei Xu",
                    "hidden": false
                },
                {
                    "_id": "68df42f7df49fb0df1e03d29",
                    "name": "Yuting He",
                    "hidden": false
                },
                {
                    "_id": "68df42f7df49fb0df1e03d2a",
                    "name": "Siqi Sun",
                    "hidden": false
                },
                {
                    "_id": "68df42f7df49fb0df1e03d2b",
                    "name": "Chenyu You",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T00:18:59.000Z",
            "submittedOnDailyAt": "2025-10-03T01:59:16.170Z",
            "title": "TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis",
            "submittedOnDailyBy": {
                "_id": "656553d89bf6665f10e3a92d",
                "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
                "isPro": false,
                "fullname": "xiang wyatt zhang",
                "user": "Wyattz23",
                "type": "user"
            },
            "summary": "Time series forecasting is central to decision-making in domains as diverse\nas energy, finance, climate, and public health. In practice, forecasters face\nthousands of short, noisy series that vary in frequency, quality, and horizon,\nwhere the dominant cost lies not in model fitting, but in the labor-intensive\npreprocessing, validation, and ensembling required to obtain reliable\npredictions. Prevailing statistical and deep learning models are tailored to\nspecific datasets or domains and generalize poorly. A general, domain-agnostic\nframework that minimizes human intervention is urgently in demand. In this\npaper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic\nframework for general time series forecasting. The framework comprises four\nspecialized agents: Curator performs LLM-guided diagnostics augmented by\nexternal tools that reason over data statistics to choose targeted\npreprocessing; Planner narrows the hypothesis space of model choice by\nleveraging multi-modal diagnostics and self-planning over the input; Forecaster\nperforms model fitting and validation and, based on the results, adaptively\nselects the best model configuration as well as ensemble strategy to make final\npredictions; and Reporter synthesizes the whole process into a comprehensive,\ntransparent report. With transparent natural-language rationales and\ncomprehensive reports, TSci transforms the forecasting workflow into a\nwhite-box system that is both interpretable and extensible across tasks.\nEmpirical results on eight established benchmarks demonstrate that TSci\nconsistently outperforms both statistical and LLM-based baselines, reducing\nforecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci\nproduces a clear and rigorous report that makes the forecasting workflow more\ntransparent and interpretable.",
            "upvotes": 1,
            "discussionId": "68df42f7df49fb0df1e03d2c",
            "ai_summary": "TimeSeriesScientist (TSci) is an LLM-driven framework that automates time series forecasting with minimal human intervention, outperforming statistical and LLM-based methods and providing transparent reports.",
            "ai_keywords": [
                "LLM-driven",
                "agentic framework",
                "LLM-guided diagnostics",
                "multi-modal diagnostics",
                "self-planning",
                "model fitting",
                "validation",
                "ensemble strategy",
                "white-box system",
                "interpretable",
                "extensible"
            ]
        },
        "publishedAt": "2025-10-01T20:18:59.000Z",
        "title": "TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis",
        "summary": "Time series forecasting is central to decision-making in domains as diverse\nas energy, finance, climate, and public health. In practice, forecasters face\nthousands of short, noisy series that vary in frequency, quality, and horizon,\nwhere the dominant cost lies not in model fitting, but in the labor-intensive\npreprocessing, validation, and ensembling required to obtain reliable\npredictions. Prevailing statistical and deep learning models are tailored to\nspecific datasets or domains and generalize poorly. A general, domain-agnostic\nframework that minimizes human intervention is urgently in demand. In this\npaper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic\nframework for general time series forecasting. The framework comprises four\nspecialized agents: Curator performs LLM-guided diagnostics augmented by\nexternal tools that reason over data statistics to choose targeted\npreprocessing; Planner narrows the hypothesis space of model choice by\nleveraging multi-modal diagnostics and self-planning over the input; Forecaster\nperforms model fitting and validation and, based on the results, adaptively\nselects the best model configuration as well as ensemble strategy to make final\npredictions; and Reporter synthesizes the whole process into a comprehensive,\ntransparent report. With transparent natural-language rationales and\ncomprehensive reports, TSci transforms the forecasting workflow into a\nwhite-box system that is both interpretable and extensible across tasks.\nEmpirical results on eight established benchmarks demonstrate that TSci\nconsistently outperforms both statistical and LLM-based baselines, reducing\nforecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci\nproduces a clear and rigorous report that makes the forecasting workflow more\ntransparent and interpretable.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01538.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656553d89bf6665f10e3a92d",
            "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
            "fullname": "xiang wyatt zhang",
            "name": "Wyattz23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.01123",
            "authors": [
                {
                    "_id": "68e043c473e20ab577841b8b",
                    "name": "Lovish Madaan",
                    "hidden": false
                },
                {
                    "_id": "68e043c473e20ab577841b8c",
                    "name": "Aniket Didolkar",
                    "hidden": false
                },
                {
                    "_id": "68e043c473e20ab577841b8d",
                    "name": "Suchin Gururangan",
                    "hidden": false
                },
                {
                    "_id": "68e043c473e20ab577841b8e",
                    "name": "John Quan",
                    "hidden": false
                },
                {
                    "_id": "68e043c473e20ab577841b8f",
                    "name": "Ruan Silva",
                    "hidden": false
                },
                {
                    "_id": "68e043c473e20ab577841b90",
                    "name": "Ruslan Salakhutdinov",
                    "hidden": false
                },
                {
                    "_id": "68e043c473e20ab577841b91",
                    "name": "Manzil Zaheer",
                    "hidden": false
                },
                {
                    "_id": "68e043c473e20ab577841b92",
                    "name": "Sanjeev Arora",
                    "hidden": false
                },
                {
                    "_id": "68e043c473e20ab577841b93",
                    "name": "Anirudh Goyal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T17:08:59.000Z",
            "submittedOnDailyAt": "2025-10-03T20:14:51.526Z",
            "title": "Rethinking Thinking Tokens: LLMs as Improvement Operators",
            "submittedOnDailyBy": {
                "_id": "65cbfa6c968742be942e6cba",
                "avatarUrl": "/avatars/1a6cc0983edc28fa92178d3abc283ba1.svg",
                "isPro": false,
                "fullname": "Feng",
                "user": "Yunzhen",
                "type": "user"
            },
            "summary": "Reasoning training incentivizes LLMs to produce long chains of thought (long\nCoT), which among other things, allows them to explore solution strategies with\nself-checking. This results in higher accuracy, but inflates context length,\ntoken/compute cost, and answer latency. We ask: Can current models leverage\ntheir metacognition to provide other combinations on this Pareto frontier,\ne.g., better accuracy with lower context length and/or latency? Abstractly, we\nview the model as an improvement operator on its own \"thoughts\" with a\ncontinuum of possible strategies. We identify an interesting inference family\nParallel-Distill-Refine (PDR), which performs the following: (i) generate\ndiverse drafts in parallel; (ii) distill them into a bounded, textual\nworkspace; and (iii) refine conditioned on this workspace, producing an output\nthat seeds the next round. Importantly, context length (hence compute cost) is\ncontrollable via degree of parallelism, and is no longer conflated with the\ntotal number of generated tokens. We report PDR instantiations of current\nmodels that give better accuracy than long CoT while incurring lower latency.\nSetting degree of parallelism to 1 yields an interesting subcase, Sequential\nRefinement (SR) (iteratively improve a single candidate answer) which provides\nperformance superior to long CoT. Success of such model orchestrations raises\nthe question whether further training could shift the Pareto frontier. To this\nend, we train an 8B thinking model with Reinforcement Learning (RL) to make it\nconsistent with PDR as the inference method. On math tasks with verifiable\nanswers, iterative pipelines surpass single-pass baselines at matched\nsequential budgets, with PDR delivering the largest gains (e.g., +11% on AIME\n2024 and +9% on AIME 2025).",
            "upvotes": 1,
            "discussionId": "68e043c573e20ab577841b94",
            "ai_summary": "Parallel-Distill-Refine (PDR) and Sequential Refinement (SR) improve the performance of LLMs by optimizing accuracy and latency through metacognitive strategies, with PDR showing significant gains on math tasks.",
            "ai_keywords": [
                "LLMs",
                "long chains of thought",
                "long CoT",
                "self-checking",
                "context length",
                "token/compute cost",
                "answer latency",
                "metacognition",
                "improvement operator",
                "Parallel-Distill-Refine",
                "PDR",
                "bounded",
                "textual workspace",
                "refine",
                "degree of parallelism",
                "Sequential Refinement",
                "SR",
                "Reinforcement Learning",
                "RL",
                "AIME 2024",
                "AIME 2025"
            ]
        },
        "publishedAt": "2025-10-01T13:08:59.000Z",
        "title": "Rethinking Thinking Tokens: LLMs as Improvement Operators",
        "summary": "Reasoning training incentivizes LLMs to produce long chains of thought (long\nCoT), which among other things, allows them to explore solution strategies with\nself-checking. This results in higher accuracy, but inflates context length,\ntoken/compute cost, and answer latency. We ask: Can current models leverage\ntheir metacognition to provide other combinations on this Pareto frontier,\ne.g., better accuracy with lower context length and/or latency? Abstractly, we\nview the model as an improvement operator on its own \"thoughts\" with a\ncontinuum of possible strategies. We identify an interesting inference family\nParallel-Distill-Refine (PDR), which performs the following: (i) generate\ndiverse drafts in parallel; (ii) distill them into a bounded, textual\nworkspace; and (iii) refine conditioned on this workspace, producing an output\nthat seeds the next round. Importantly, context length (hence compute cost) is\ncontrollable via degree of parallelism, and is no longer conflated with the\ntotal number of generated tokens. We report PDR instantiations of current\nmodels that give better accuracy than long CoT while incurring lower latency.\nSetting degree of parallelism to 1 yields an interesting subcase, Sequential\nRefinement (SR) (iteratively improve a single candidate answer) which provides\nperformance superior to long CoT. Success of such model orchestrations raises\nthe question whether further training could shift the Pareto frontier. To this\nend, we train an 8B thinking model with Reinforcement Learning (RL) to make it\nconsistent with PDR as the inference method. On math tasks with verifiable\nanswers, iterative pipelines surpass single-pass baselines at matched\nsequential budgets, with PDR delivering the largest gains (e.g., +11% on AIME\n2024 and +9% on AIME 2025).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01123.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65cbfa6c968742be942e6cba",
            "avatarUrl": "/avatars/1a6cc0983edc28fa92178d3abc283ba1.svg",
            "fullname": "Feng",
            "name": "Yunzhen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.00537",
            "authors": [
                {
                    "_id": "68df3f9edf49fb0df1e03d1c",
                    "user": {
                        "_id": "670ec3f6db1a6bcfe832e0a6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-mZNLeLJoXzkwPgYO38lF.png",
                        "isPro": false,
                        "fullname": "Nandan Kumar Jha",
                        "user": "nandan523",
                        "type": "user"
                    },
                    "name": "Nandan Kumar Jha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:29:24.444Z",
                    "hidden": false
                },
                {
                    "_id": "68df3f9edf49fb0df1e03d1d",
                    "name": "Brandon Reagen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T05:38:21.000Z",
            "submittedOnDailyAt": "2025-10-03T02:04:12.607Z",
            "title": "Spectral Scaling Laws in Language Models: How Effectively Do\n  Feed-Forward Networks Use Their Latent Space?",
            "submittedOnDailyBy": {
                "_id": "670ec3f6db1a6bcfe832e0a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-mZNLeLJoXzkwPgYO38lF.png",
                "isPro": false,
                "fullname": "Nandan Kumar Jha",
                "user": "nandan523",
                "type": "user"
            },
            "summary": "As large language models (LLMs) scale, the question is not only how large\nthey become, but how much of their capacity is effectively utilized. Existing\nscaling laws relate model size to loss, yet overlook how components exploit\ntheir latent space. We study feed-forward networks (FFNs) and recast width\nselection as a spectral utilization problem. Using a lightweight diagnostic\nsuite -- Hard Rank (participation ratio), Soft Rank (Shannon rank), Spectral\nConcentration, and the composite Spectral Utilization Index (SUI) -- we\nquantify how many latent directions are meaningfully activated across LLaMA,\nGPT-2, and nGPT families. Our key finding is an asymmetric spectral scaling\nlaw: soft rank follows an almost perfect power law with FFN width, while hard\nrank grows only sublinearly and with high variance. This asymmetry suggests\nthat widening FFNs mostly adds low-energy tail directions, while dominant-mode\nsubspaces saturate early. Moreover, at larger widths, variance further\ncollapses into a narrow subspace, leaving much of the latent space\nunder-utilized. These results recast FFN width selection as a principled\ntrade-off between tail capacity and dominant-mode capacity, offering concrete\nguidance for inference-efficient LLM design.",
            "upvotes": 1,
            "discussionId": "68df3f9edf49fb0df1e03d1e",
            "ai_summary": "Research on large language models reveals an asymmetric spectral scaling law in feed-forward networks, indicating that increasing width primarily adds low-energy directions while dominant modes saturate early, leading to underutilized latent space.",
            "ai_keywords": [
                "feed-forward networks",
                "FFNs",
                "Hard Rank",
                "Soft Rank",
                "Spectral Concentration",
                "Spectral Utilization Index",
                "SUI",
                "latent space",
                "spectral scaling law",
                "dominant-mode subspaces",
                "tail capacity",
                "dominant-mode capacity",
                "inference-efficient LLM design"
            ],
            "organization": {
                "_id": "63f68bebb29015adc33fb06b",
                "name": "nyuniversity",
                "fullname": "New York University"
            }
        },
        "publishedAt": "2025-10-01T01:38:21.000Z",
        "title": "Spectral Scaling Laws in Language Models: How Effectively Do\n  Feed-Forward Networks Use Their Latent Space?",
        "summary": "As large language models (LLMs) scale, the question is not only how large\nthey become, but how much of their capacity is effectively utilized. Existing\nscaling laws relate model size to loss, yet overlook how components exploit\ntheir latent space. We study feed-forward networks (FFNs) and recast width\nselection as a spectral utilization problem. Using a lightweight diagnostic\nsuite -- Hard Rank (participation ratio), Soft Rank (Shannon rank), Spectral\nConcentration, and the composite Spectral Utilization Index (SUI) -- we\nquantify how many latent directions are meaningfully activated across LLaMA,\nGPT-2, and nGPT families. Our key finding is an asymmetric spectral scaling\nlaw: soft rank follows an almost perfect power law with FFN width, while hard\nrank grows only sublinearly and with high variance. This asymmetry suggests\nthat widening FFNs mostly adds low-energy tail directions, while dominant-mode\nsubspaces saturate early. Moreover, at larger widths, variance further\ncollapses into a narrow subspace, leaving much of the latent space\nunder-utilized. These results recast FFN width selection as a principled\ntrade-off between tail capacity and dominant-mode capacity, offering concrete\nguidance for inference-efficient LLM design.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00537.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "670ec3f6db1a6bcfe832e0a6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-mZNLeLJoXzkwPgYO38lF.png",
            "fullname": "Nandan Kumar Jha",
            "name": "nandan523",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "63f68bebb29015adc33fb06b",
            "name": "nyuniversity",
            "fullname": "New York University"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.26313",
            "authors": [
                {
                    "_id": "68dfc7cf73e20ab5778419b3",
                    "user": {
                        "_id": "66d6ce5e0b8483353663a6ce",
                        "avatarUrl": "/avatars/f008af97b87b6288f8f3279e3d0e9107.svg",
                        "isPro": false,
                        "fullname": "Rui",
                        "user": "Yalimu",
                        "type": "user"
                    },
                    "name": "Rui Ming",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:56:45.158Z",
                    "hidden": false
                },
                {
                    "_id": "68dfc7cf73e20ab5778419b4",
                    "name": "Haoyuan Wu",
                    "hidden": false
                },
                {
                    "_id": "68dfc7cf73e20ab5778419b5",
                    "name": "Shoubo Hu",
                    "hidden": false
                },
                {
                    "_id": "68dfc7cf73e20ab5778419b6",
                    "name": "Zhuolun He",
                    "hidden": false
                },
                {
                    "_id": "68dfc7cf73e20ab5778419b7",
                    "name": "Bei Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T14:25:56.000Z",
            "submittedOnDailyAt": "2025-10-03T11:41:40.922Z",
            "title": "One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy\n  Gradient",
            "submittedOnDailyBy": {
                "_id": "66d6ce5e0b8483353663a6ce",
                "avatarUrl": "/avatars/f008af97b87b6288f8f3279e3d0e9107.svg",
                "isPro": false,
                "fullname": "Rui",
                "user": "Yalimu",
                "type": "user"
            },
            "summary": "Supervised fine-tuning (SFT) is the predominant method for adapting large\nlanguage models (LLMs), yet it often struggles with generalization compared to\nreinforcement learning (RL). In this work, we posit that this performance\ndisparity stems not just from the loss function, but from a more fundamental\ndifference: SFT learns from a fixed, pre-collected dataset, whereas RL utilizes\non-policy data sampled from the current policy. Building on this hypothesis, we\nintroduce one-token rollout (OTR), a novel fine-tuning algorithm that guides\nSFT with the policy gradient method. OTR reframes the autoregressive learning\nprocess by treating each token generation as a single-step reinforcement\nlearning trajectory. At each step, it performs a Monte Carlo ``rollout'' by\nsampling multiple candidate tokens from the current policy's distribution. The\nground-truth token from the supervised data is then used to provide a reward\nsignal to these samples. Guided by policy gradient, our algorithm repurposes\nstatic, off-policy supervised data into a dynamic, on-policy signal at the\ntoken level, capturing the generalization benefits of on-policy learning while\nbypassing the costly overhead of full sentence generation. Through extensive\nexperiments on a diverse suite of challenging benchmarks spanning mathematical\nreasoning, code generation, and general domain reasoning, we demonstrate that\nOTR consistently outperforms standard SFT. Our findings establish OTR as a\npowerful and practical alternative for fine-tuning LLMs and provide compelling\nevidence that the on-policy nature of data is a critical driver of\ngeneralization, offering a promising new direction for fine-tuning LLMs.",
            "upvotes": 1,
            "discussionId": "68dfc7d073e20ab5778419b8",
            "ai_summary": "One-token rollout (OTR) enhances supervised fine-tuning of large language models by incorporating policy gradient methods to improve generalization using on-policy data.",
            "ai_keywords": [
                "supervised fine-tuning",
                "large language models",
                "reinforcement learning",
                "one-token rollout",
                "policy gradient method",
                "autoregressive learning",
                "Monte Carlo rollout",
                "on-policy data",
                "off-policy data",
                "generalization",
                "mathematical reasoning",
                "code generation",
                "domain reasoning"
            ]
        },
        "publishedAt": "2025-09-30T10:25:56.000Z",
        "title": "One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy\n  Gradient",
        "summary": "Supervised fine-tuning (SFT) is the predominant method for adapting large\nlanguage models (LLMs), yet it often struggles with generalization compared to\nreinforcement learning (RL). In this work, we posit that this performance\ndisparity stems not just from the loss function, but from a more fundamental\ndifference: SFT learns from a fixed, pre-collected dataset, whereas RL utilizes\non-policy data sampled from the current policy. Building on this hypothesis, we\nintroduce one-token rollout (OTR), a novel fine-tuning algorithm that guides\nSFT with the policy gradient method. OTR reframes the autoregressive learning\nprocess by treating each token generation as a single-step reinforcement\nlearning trajectory. At each step, it performs a Monte Carlo ``rollout'' by\nsampling multiple candidate tokens from the current policy's distribution. The\nground-truth token from the supervised data is then used to provide a reward\nsignal to these samples. Guided by policy gradient, our algorithm repurposes\nstatic, off-policy supervised data into a dynamic, on-policy signal at the\ntoken level, capturing the generalization benefits of on-policy learning while\nbypassing the costly overhead of full sentence generation. Through extensive\nexperiments on a diverse suite of challenging benchmarks spanning mathematical\nreasoning, code generation, and general domain reasoning, we demonstrate that\nOTR consistently outperforms standard SFT. Our findings establish OTR as a\npowerful and practical alternative for fine-tuning LLMs and provide compelling\nevidence that the on-policy nature of data is a critical driver of\ngeneralization, offering a promising new direction for fine-tuning LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26313.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66d6ce5e0b8483353663a6ce",
            "avatarUrl": "/avatars/f008af97b87b6288f8f3279e3d0e9107.svg",
            "fullname": "Rui",
            "name": "Yalimu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.22582",
            "authors": [
                {
                    "_id": "68e00ca373e20ab577841a58",
                    "name": "Yehonatan Peisakhovsky",
                    "hidden": false
                },
                {
                    "_id": "68e00ca373e20ab577841a59",
                    "name": "Zorik Gekhman",
                    "hidden": false
                },
                {
                    "_id": "68e00ca373e20ab577841a5a",
                    "name": "Yosi Mass",
                    "hidden": false
                },
                {
                    "_id": "68e00ca373e20ab577841a5b",
                    "name": "Liat Ein-Dor",
                    "hidden": false
                },
                {
                    "_id": "68e00ca373e20ab577841a5c",
                    "name": "Roi Reichart",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T17:03:24.000Z",
            "submittedOnDailyAt": "2025-10-03T16:49:23.362Z",
            "title": "Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs",
            "submittedOnDailyBy": {
                "_id": "61499a8f8517f77f09a3a990",
                "avatarUrl": "/avatars/fa15d117e8ce8c50669455d3e483408b.svg",
                "isPro": false,
                "fullname": "Zorik",
                "user": "zorik",
                "type": "user"
            },
            "summary": "Context-grounded hallucinations are cases where model outputs contain\ninformation not verifiable against the source text. We study the applicability\nof LLMs for localizing such hallucinations, as a more practical alternative to\nexisting complex evaluation pipelines. In the absence of established benchmarks\nfor meta-evaluation of hallucinations localization, we construct one tailored\nto LLMs, involving a challenging human annotation of over 1,000 examples. We\ncomplement the benchmark with an LLM-based evaluation protocol, verifying its\nquality in a human evaluation. Since existing representations of hallucinations\nlimit the types of errors that can be expressed, we propose a new\nrepresentation based on free-form textual descriptions, capturing the full\nrange of possible errors. We conduct a comprehensive study, evaluating four\nlarge-scale LLMs, which highlights the benchmark's difficulty, as the best\nmodel achieves an F1 score of only 0.67. Through careful analysis, we offer\ninsights into optimal prompting strategies for the task and identify the main\nfactors that make it challenging for LLMs: (1) a tendency to incorrectly flag\nmissing details as inconsistent, despite being instructed to check only facts\nin the output; and (2) difficulty with outputs containing factually correct\ninformation absent from the source - and thus not verifiable - due to alignment\nwith the model's parametric knowledge.",
            "upvotes": 1,
            "discussionId": "68e00ca373e20ab577841a5d",
            "ai_summary": "A study evaluates the effectiveness of large language models in identifying context-grounded hallucinations using a newly constructed benchmark and free-form textual descriptions, revealing challenges in distinguishing between missing details and unverifiable information.",
            "ai_keywords": [
                "LLMs",
                "context-grounded hallucinations",
                "meta-evaluation",
                "human annotation",
                "F1 score",
                "prompting strategies",
                "parametric knowledge"
            ],
            "organization": {
                "_id": "6393322be2364bc1eea56e45",
                "name": "Technion",
                "fullname": "Technion Israel institute of technology",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
            }
        },
        "publishedAt": "2025-09-26T13:03:24.000Z",
        "title": "Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs",
        "summary": "Context-grounded hallucinations are cases where model outputs contain\ninformation not verifiable against the source text. We study the applicability\nof LLMs for localizing such hallucinations, as a more practical alternative to\nexisting complex evaluation pipelines. In the absence of established benchmarks\nfor meta-evaluation of hallucinations localization, we construct one tailored\nto LLMs, involving a challenging human annotation of over 1,000 examples. We\ncomplement the benchmark with an LLM-based evaluation protocol, verifying its\nquality in a human evaluation. Since existing representations of hallucinations\nlimit the types of errors that can be expressed, we propose a new\nrepresentation based on free-form textual descriptions, capturing the full\nrange of possible errors. We conduct a comprehensive study, evaluating four\nlarge-scale LLMs, which highlights the benchmark's difficulty, as the best\nmodel achieves an F1 score of only 0.67. Through careful analysis, we offer\ninsights into optimal prompting strategies for the task and identify the main\nfactors that make it challenging for LLMs: (1) a tendency to incorrectly flag\nmissing details as inconsistent, despite being instructed to check only facts\nin the output; and (2) difficulty with outputs containing factually correct\ninformation absent from the source - and thus not verifiable - due to alignment\nwith the model's parametric knowledge.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22582.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61499a8f8517f77f09a3a990",
            "avatarUrl": "/avatars/fa15d117e8ce8c50669455d3e483408b.svg",
            "fullname": "Zorik",
            "name": "zorik",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "6393322be2364bc1eea56e45",
            "name": "Technion",
            "fullname": "Technion Israel institute of technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01581",
            "authors": [
                {
                    "_id": "68dfdf1f73e20ab5778419e8",
                    "name": "Joykirat Singh",
                    "hidden": false
                },
                {
                    "_id": "68dfdf1f73e20ab5778419e9",
                    "name": "Justin Chih-Yao Chen",
                    "hidden": false
                },
                {
                    "_id": "68dfdf1f73e20ab5778419ea",
                    "name": "Archiki Prasad",
                    "hidden": false
                },
                {
                    "_id": "68dfdf1f73e20ab5778419eb",
                    "name": "Elias Stengel-Eskin",
                    "hidden": false
                },
                {
                    "_id": "68dfdf1f73e20ab5778419ec",
                    "name": "Akshay Nambi",
                    "hidden": false
                },
                {
                    "_id": "68dfdf1f73e20ab5778419ed",
                    "name": "Mohit Bansal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T02:00:20.000Z",
            "submittedOnDailyAt": "2025-10-03T13:06:21.413Z",
            "title": "Think Right: Learning to Mitigate Under-Over Thinking via Adaptive,\n  Attentive Compression",
            "submittedOnDailyBy": {
                "_id": "62ce26129f723d34cf1f595a",
                "avatarUrl": "/avatars/8e305ac7c170d70fbf83c109789b40d9.svg",
                "isPro": false,
                "fullname": "Justin Chen",
                "user": "dinobby",
                "type": "user"
            },
            "summary": "Recent thinking models solve complex reasoning tasks by scaling test-time\ncompute, but this scaling must be allocated in line with task difficulty. On\none hand, short reasoning (underthinking) leads to errors on harder problems\nthat require extended reasoning steps; but, excessively long reasoning\n(overthinking) can be token-inefficient, generating unnecessary steps even\nafter reaching a correct intermediate solution. We refer to this as\nunder-adaptivity, where the model fails to modulate its response length\nappropriately given problems of varying difficulty. To address under-adaptivity\nand strike a balance between under- and overthinking, we propose TRAAC (Think\nRight with Adaptive, Attentive Compression), an online post-training RL method\nthat leverages the model's self-attention over a long reasoning trajectory to\nidentify important steps and prune redundant ones. TRAAC also estimates\ndifficulty and incorporates it into training rewards, thereby learning to\nallocate reasoning budget commensurate with example difficulty. Our approach\nimproves accuracy, reduces reasoning steps, and enables adaptive thinking\ncompared to base models and other RL baselines. Across a variety of tasks\n(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute\naccuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%\ncompared to the base model, and a 7.9% accuracy gain paired with a 29.4% length\ndrop compared to the best RL baseline. TRAAC also shows strong generalization:\nalthough our models are trained on math datasets, they show accuracy and\nefficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,\nand OptimalThinkingBench. Our analysis further verifies that TRAAC provides\nfine-grained adjustments to thinking budget based on difficulty and that a\ncombination of task-difficulty calibration and attention-based compression\nyields gains across diverse tasks.",
            "upvotes": 0,
            "discussionId": "68dfdf1f73e20ab5778419ee",
            "ai_summary": "TRAAC, an online post-training RL method, improves model accuracy and efficiency by adaptively adjusting reasoning steps based on task difficulty using self-attention.",
            "ai_keywords": [
                "self-attention",
                "long reasoning trajectory",
                "adaptive",
                "attentive compression",
                "under-adaptivity",
                "reasoning budget",
                "task-difficulty calibration",
                "TRAAC",
                "Qwen3-4B",
                "AIME",
                "AMC",
                "GPQA-D",
                "BBEH",
                "OptimalThinkingBench"
            ]
        },
        "publishedAt": "2025-10-01T22:00:20.000Z",
        "title": "Think Right: Learning to Mitigate Under-Over Thinking via Adaptive,\n  Attentive Compression",
        "summary": "Recent thinking models solve complex reasoning tasks by scaling test-time\ncompute, but this scaling must be allocated in line with task difficulty. On\none hand, short reasoning (underthinking) leads to errors on harder problems\nthat require extended reasoning steps; but, excessively long reasoning\n(overthinking) can be token-inefficient, generating unnecessary steps even\nafter reaching a correct intermediate solution. We refer to this as\nunder-adaptivity, where the model fails to modulate its response length\nappropriately given problems of varying difficulty. To address under-adaptivity\nand strike a balance between under- and overthinking, we propose TRAAC (Think\nRight with Adaptive, Attentive Compression), an online post-training RL method\nthat leverages the model's self-attention over a long reasoning trajectory to\nidentify important steps and prune redundant ones. TRAAC also estimates\ndifficulty and incorporates it into training rewards, thereby learning to\nallocate reasoning budget commensurate with example difficulty. Our approach\nimproves accuracy, reduces reasoning steps, and enables adaptive thinking\ncompared to base models and other RL baselines. Across a variety of tasks\n(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute\naccuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%\ncompared to the base model, and a 7.9% accuracy gain paired with a 29.4% length\ndrop compared to the best RL baseline. TRAAC also shows strong generalization:\nalthough our models are trained on math datasets, they show accuracy and\nefficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,\nand OptimalThinkingBench. Our analysis further verifies that TRAAC provides\nfine-grained adjustments to thinking budget based on difficulty and that a\ncombination of task-difficulty calibration and attention-based compression\nyields gains across diverse tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01581.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62ce26129f723d34cf1f595a",
            "avatarUrl": "/avatars/8e305ac7c170d70fbf83c109789b40d9.svg",
            "fullname": "Justin Chen",
            "name": "dinobby",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01260",
            "authors": [
                {
                    "_id": "68dfaa7cdf49fb0df1e03eb7",
                    "name": "Ningyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "68dfaa7cdf49fb0df1e03eb8",
                    "name": "Guanliang Lyu",
                    "hidden": false
                },
                {
                    "_id": "68dfaa7cdf49fb0df1e03eb9",
                    "name": "Mingchen Ma",
                    "hidden": false
                },
                {
                    "_id": "68dfaa7cdf49fb0df1e03eba",
                    "name": "Yiyi Lu",
                    "hidden": false
                },
                {
                    "_id": "68dfaa7cdf49fb0df1e03ebb",
                    "name": "Yiming Li",
                    "hidden": false
                },
                {
                    "_id": "68dfaa7cdf49fb0df1e03ebc",
                    "name": "Zhihui Gao",
                    "hidden": false
                },
                {
                    "_id": "68dfaa7cdf49fb0df1e03ebd",
                    "name": "Hancheng Ye",
                    "hidden": false
                },
                {
                    "_id": "68dfaa7cdf49fb0df1e03ebe",
                    "name": "Jianyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dfaa7cdf49fb0df1e03ebf",
                    "name": "Tingjun Chen",
                    "hidden": false
                },
                {
                    "_id": "68dfaa7cdf49fb0df1e03ec0",
                    "name": "Yiran Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-25T08:35:47.000Z",
            "submittedOnDailyAt": "2025-10-03T09:21:18.578Z",
            "title": "IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol",
            "submittedOnDailyBy": {
                "_id": "60cd95ee15ecba5f2200304a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
                "isPro": false,
                "fullname": "Alexey Dontsov",
                "user": "therem",
                "type": "user"
            },
            "summary": "The integration of Large Language Models (LLMs) with Internet-of-Things (IoT)\nsystems faces significant challenges in hardware heterogeneity and control\ncomplexity. The Model Context Protocol (MCP) emerges as a critical enabler,\nproviding standardized communication between LLMs and physical devices. We\npropose IoT-MCP, a novel framework that implements MCP through edge-deployed\nservers to bridge LLMs and IoT ecosystems. To support rigorous evaluation, we\nintroduce IoT-MCP Bench, the first benchmark containing 114 Basic Tasks (e.g.,\n``What is the current temperature?'') and 1,140 Complex Tasks (e.g., ``I feel\nso hot, do you have any ideas?'') for IoT-enabled LLMs. Experimental validation\nacross 22 sensor types and 6 microcontroller units demonstrates IoT-MCP's 100%\ntask success rate to generate tool calls that fully meet expectations and\nobtain completely accurate results, 205ms average response time, and 74KB peak\nmemory footprint. This work delivers both an open-source integration framework\n(https://github.com/Duke-CEI-Center/IoT-MCP-Servers) and a standardized\nevaluation methodology for LLM-IoT systems.",
            "upvotes": 0,
            "discussionId": "68dfaa7cdf49fb0df1e03ec1",
            "ai_summary": "IoT-MCP, a framework using the Model Context Protocol, enables seamless communication between Large Language Models and IoT devices, achieving high task success rates and low resource usage.",
            "ai_keywords": [
                "Large Language Models",
                "Internet-of-Things",
                "Model Context Protocol",
                "edge-deployed servers",
                "IoT-MCP",
                "IoT-MCP Bench",
                "Basic Tasks",
                "Complex Tasks",
                "sensor types",
                "microcontroller units",
                "tool calls",
                "response time",
                "memory footprint"
            ]
        },
        "publishedAt": "2025-09-25T04:35:47.000Z",
        "title": "IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol",
        "summary": "The integration of Large Language Models (LLMs) with Internet-of-Things (IoT)\nsystems faces significant challenges in hardware heterogeneity and control\ncomplexity. The Model Context Protocol (MCP) emerges as a critical enabler,\nproviding standardized communication between LLMs and physical devices. We\npropose IoT-MCP, a novel framework that implements MCP through edge-deployed\nservers to bridge LLMs and IoT ecosystems. To support rigorous evaluation, we\nintroduce IoT-MCP Bench, the first benchmark containing 114 Basic Tasks (e.g.,\n``What is the current temperature?'') and 1,140 Complex Tasks (e.g., ``I feel\nso hot, do you have any ideas?'') for IoT-enabled LLMs. Experimental validation\nacross 22 sensor types and 6 microcontroller units demonstrates IoT-MCP's 100%\ntask success rate to generate tool calls that fully meet expectations and\nobtain completely accurate results, 205ms average response time, and 74KB peak\nmemory footprint. This work delivers both an open-source integration framework\n(https://github.com/Duke-CEI-Center/IoT-MCP-Servers) and a standardized\nevaluation methodology for LLM-IoT systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01260.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60cd95ee15ecba5f2200304a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
            "fullname": "Alexey Dontsov",
            "name": "therem",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    }
]