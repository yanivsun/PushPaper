[
    {
        "paper": {
            "id": "2510.01141",
            "authors": [
                {
                    "_id": "68de25d26024653e8a3ed204",
                    "name": "Shruthan Radhakrishna",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed205",
                    "user": {
                        "_id": "62d913739a5353eef9d7edf3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d913739a5353eef9d7edf3/pRgen2izGJle3ahupOdC7.jpeg",
                        "isPro": false,
                        "fullname": "Aman Tiwari",
                        "user": "amant555",
                        "type": "user"
                    },
                    "name": "Aman Tiwari",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:54:38.519Z",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed206",
                    "name": "Aanjaneya Shukla",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed207",
                    "name": "Masoud Hashemi",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed208",
                    "name": "Rishabh Maheshwary",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed209",
                    "name": "Shiva Krishna Reddy Malay",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed20a",
                    "name": "Jash Mehta",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed20b",
                    "name": "Pulkit Pattnaik",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed20c",
                    "name": "Saloni Mittal",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed20d",
                    "name": "Khalil Slimi",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed20e",
                    "name": "Kelechi Ogueji",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed20f",
                    "name": "Akintunde Oladipo",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed210",
                    "name": "Soham Parikh",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed211",
                    "name": "Oluwanifemi Bamgbose",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed212",
                    "name": "Toby Liang",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed213",
                    "name": "Ahmed Masry",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed214",
                    "name": "Khyati Mahajan",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed215",
                    "user": {
                        "_id": "642f99079b2484d7d857341b",
                        "avatarUrl": "/avatars/01965cc5a5dbe9c08025a51973462a6a.svg",
                        "isPro": false,
                        "fullname": "Sai Rajeswar",
                        "user": "rajeswarsai",
                        "type": "user"
                    },
                    "name": "Sai Rajeswar Mudumba",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-05T12:47:29.534Z",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed216",
                    "name": "Vikas Yadav",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed217",
                    "name": "Sathwik Tejaswi Madhusudhan",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed218",
                    "name": "Torsten Scholak",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed219",
                    "name": "Sagar Davasam",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed21a",
                    "name": "Srinivas Sunkara",
                    "hidden": false
                },
                {
                    "_id": "68de25d26024653e8a3ed21b",
                    "name": "Nicholas Chapados",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T17:29:35.000Z",
            "submittedOnDailyAt": "2025-10-06T07:20:15.592Z",
            "title": "Apriel-1.5-15b-Thinker",
            "submittedOnDailyBy": {
                "_id": "62d913739a5353eef9d7edf3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d913739a5353eef9d7edf3/pRgen2izGJle3ahupOdC7.jpeg",
                "isPro": false,
                "fullname": "Aman Tiwari",
                "user": "amant555",
                "type": "user"
            },
            "summary": "We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights\nmultimodal reasoning model that achieves frontier-level performance through\ntraining design rather than sheer scale. Starting from Pixtral-12B, we apply a\nprogressive three-stage methodology: (1) depth upscaling to expand reasoning\ncapacity without pretraining from scratch, (2) staged continual pre-training\nthat first develops foundational text and vision understanding, then enhances\nvisual reasoning through targeted synthetic data generation addressing spatial\nstructure, compositional understanding, and fine-grained perception, and (3)\nhigh-quality text-only supervised fine-tuning on curated instruction-response\npairs with explicit reasoning traces spanning mathematics, coding, science, and\ntool use. Notably, our model achieves competitive results without reinforcement\nlearning or preference optimization, isolating the contribution of our\ndata-centric continual pre-training approach. On the Artificial Analysis\nIntelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching\nDeepSeek-R1-0528 despite requiring significantly fewer computational resources.\nAcross ten image benchmarks, its performance is on average within five points\nof Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model\noperating within single-GPU deployment constraints. Our results demonstrate\nthat thoughtful mid-training 2 design can close substantial capability gaps\nwithout massive scale, making frontier-level multimodal reasoning accessible to\norganizations with limited infrastructure. We release the model checkpoint, all\ntraining recipes, and evaluation protocols under the MIT license to to advance\nopen-source research.",
            "upvotes": 78,
            "discussionId": "68de25d26024653e8a3ed21c",
            "ai_summary": "A 15-billion parameter multimodal reasoning model achieves competitive performance through a progressive training methodology without reinforcement learning, demonstrating efficient use of computational resources.",
            "ai_keywords": [
                "multimodal reasoning model",
                "depth upscaling",
                "staged continual pre-training",
                "synthetic data generation",
                "spatial structure",
                "compositional understanding",
                "fine-grained perception",
                "text-only supervised fine-tuning",
                "reasoning traces",
                "Artificial Analysis Intelligence Index",
                "single-GPU deployment"
            ],
            "organization": {
                "_id": "65f4df5de83b55da5d79fbb6",
                "name": "ServiceNow-AI",
                "fullname": "ServiceNow-AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63d3095c2727d7888cbb54e2/Uv-Lx8PVGviqokfOyYlCN.png"
            }
        },
        "publishedAt": "2025-10-01T13:29:35.000Z",
        "title": "Apriel-1.5-15b-Thinker",
        "summary": "We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights\nmultimodal reasoning model that achieves frontier-level performance through\ntraining design rather than sheer scale. Starting from Pixtral-12B, we apply a\nprogressive three-stage methodology: (1) depth upscaling to expand reasoning\ncapacity without pretraining from scratch, (2) staged continual pre-training\nthat first develops foundational text and vision understanding, then enhances\nvisual reasoning through targeted synthetic data generation addressing spatial\nstructure, compositional understanding, and fine-grained perception, and (3)\nhigh-quality text-only supervised fine-tuning on curated instruction-response\npairs with explicit reasoning traces spanning mathematics, coding, science, and\ntool use. Notably, our model achieves competitive results without reinforcement\nlearning or preference optimization, isolating the contribution of our\ndata-centric continual pre-training approach. On the Artificial Analysis\nIntelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching\nDeepSeek-R1-0528 despite requiring significantly fewer computational resources.\nAcross ten image benchmarks, its performance is on average within five points\nof Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model\noperating within single-GPU deployment constraints. Our results demonstrate\nthat thoughtful mid-training 2 design can close substantial capability gaps\nwithout massive scale, making frontier-level multimodal reasoning accessible to\norganizations with limited infrastructure. We release the model checkpoint, all\ntraining recipes, and evaluation protocols under the MIT license to to advance\nopen-source research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01141.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62d913739a5353eef9d7edf3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d913739a5353eef9d7edf3/pRgen2izGJle3ahupOdC7.jpeg",
            "fullname": "Aman Tiwari",
            "name": "amant555",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "65f4df5de83b55da5d79fbb6",
            "name": "ServiceNow-AI",
            "fullname": "ServiceNow-AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63d3095c2727d7888cbb54e2/Uv-Lx8PVGviqokfOyYlCN.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.00938",
            "authors": [
                {
                    "_id": "68e15e0973e20ab577841d5e",
                    "user": {
                        "_id": "63756971b18c6b01497ca65f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63756971b18c6b01497ca65f/pNiAPL7NuPoHlTPl9huTP.jpeg",
                        "isPro": false,
                        "fullname": "Anthony Peng",
                        "user": "AnthonyPeng",
                        "type": "user"
                    },
                    "name": "ShengYun Peng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-05T12:45:28.147Z",
                    "hidden": false
                },
                {
                    "_id": "68e15e0973e20ab577841d5f",
                    "name": "Eric Smith",
                    "hidden": false
                },
                {
                    "_id": "68e15e0973e20ab577841d60",
                    "name": "Ivan Evtimov",
                    "hidden": false
                },
                {
                    "_id": "68e15e0973e20ab577841d61",
                    "name": "Song Jiang",
                    "hidden": false
                },
                {
                    "_id": "68e15e0973e20ab577841d62",
                    "name": "Pin-Yu Chen",
                    "hidden": false
                },
                {
                    "_id": "68e15e0973e20ab577841d63",
                    "name": "Hongyuan Zhan",
                    "hidden": false
                },
                {
                    "_id": "68e15e0973e20ab577841d64",
                    "name": "Haozhu Wang",
                    "hidden": false
                },
                {
                    "_id": "68e15e0973e20ab577841d65",
                    "name": "Duen Horng Chau",
                    "hidden": false
                },
                {
                    "_id": "68e15e0973e20ab577841d66",
                    "name": "Mahesh Pasupuleti",
                    "hidden": false
                },
                {
                    "_id": "68e15e0973e20ab577841d67",
                    "name": "Jianfeng Chi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T14:15:43.000Z",
            "submittedOnDailyAt": "2025-10-06T11:48:06.092Z",
            "title": "Large Reasoning Models Learn Better Alignment from Flawed Thinking",
            "submittedOnDailyBy": {
                "_id": "63756971b18c6b01497ca65f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63756971b18c6b01497ca65f/pNiAPL7NuPoHlTPl9huTP.jpeg",
                "isPro": false,
                "fullname": "Anthony Peng",
                "user": "AnthonyPeng",
                "type": "user"
            },
            "summary": "Large reasoning models (LRMs) \"think\" by generating structured\nchain-of-thought (CoT) before producing a final answer, yet they still lack the\nability to reason critically about safety alignment and are easily biased when\na flawed premise is injected into their thought process. We propose RECAP\n(Robust Safety Alignment via Counter-Aligned Prefilling), a principled\nreinforcement learning (RL) method for post-training that explicitly teaches\nmodels to override flawed reasoning trajectories and reroute to safe and\nhelpful responses. RECAP trains on a mixture of synthetically generated\ncounter-aligned CoT prefills and standard prompts, requires no additional\ntraining cost or modifications beyond vanilla reinforcement learning from human\nfeedback (RLHF), and substantially improves safety and jailbreak robustness,\nreduces overrefusal, and preserves core reasoning capability -- all while\nmaintaining inference token budget. Extensive analysis shows that RECAP-trained\nmodels engage in self-reflection more frequently and remain robust under\nadaptive attacks, preserving safety even after repeated attempts to override\ntheir reasoning.",
            "upvotes": 34,
            "discussionId": "68e15e0a73e20ab577841d68",
            "projectPage": "https://shengyun-peng.github.io/papers/lrm-safety",
            "ai_summary": "RECAP, a reinforcement learning method, enhances the safety and robustness of large reasoning models by teaching them to override flawed reasoning and maintain safety without additional training costs.",
            "ai_keywords": [
                "chain-of-thought",
                "reinforcement learning",
                "counter-aligned prefills",
                "standard prompts",
                "reinforcement learning from human feedback",
                "jailbreak robustness",
                "overrefusal",
                "self-reflection",
                "adaptive attacks"
            ],
            "organization": {
                "_id": "689f81c63080d12247c8f067",
                "name": "MetaSuperintelligenceLab",
                "fullname": "MetaSuperintelligenceLab"
            }
        },
        "publishedAt": "2025-10-01T10:15:43.000Z",
        "title": "Large Reasoning Models Learn Better Alignment from Flawed Thinking",
        "summary": "Large reasoning models (LRMs) \"think\" by generating structured\nchain-of-thought (CoT) before producing a final answer, yet they still lack the\nability to reason critically about safety alignment and are easily biased when\na flawed premise is injected into their thought process. We propose RECAP\n(Robust Safety Alignment via Counter-Aligned Prefilling), a principled\nreinforcement learning (RL) method for post-training that explicitly teaches\nmodels to override flawed reasoning trajectories and reroute to safe and\nhelpful responses. RECAP trains on a mixture of synthetically generated\ncounter-aligned CoT prefills and standard prompts, requires no additional\ntraining cost or modifications beyond vanilla reinforcement learning from human\nfeedback (RLHF), and substantially improves safety and jailbreak robustness,\nreduces overrefusal, and preserves core reasoning capability -- all while\nmaintaining inference token budget. Extensive analysis shows that RECAP-trained\nmodels engage in self-reflection more frequently and remain robust under\nadaptive attacks, preserving safety even after repeated attempts to override\ntheir reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00938.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63756971b18c6b01497ca65f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63756971b18c6b01497ca65f/pNiAPL7NuPoHlTPl9huTP.jpeg",
            "fullname": "Anthony Peng",
            "name": "AnthonyPeng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "689f81c63080d12247c8f067",
            "name": "MetaSuperintelligenceLab",
            "fullname": "MetaSuperintelligenceLab"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.00515",
            "authors": [
                {
                    "_id": "68e34f7173e20ab5778420bb",
                    "name": "Zichen Wen",
                    "hidden": false
                },
                {
                    "_id": "68e34f7173e20ab5778420bc",
                    "name": "Shaobo Wang",
                    "hidden": false
                },
                {
                    "_id": "68e34f7173e20ab5778420bd",
                    "name": "Yufa Zhou",
                    "hidden": false
                },
                {
                    "_id": "68e34f7173e20ab5778420be",
                    "name": "Junyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e34f7173e20ab5778420bf",
                    "name": "Qintong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e34f7173e20ab5778420c0",
                    "name": "Yifeng Gao",
                    "hidden": false
                },
                {
                    "_id": "68e34f7173e20ab5778420c1",
                    "name": "Zhaorun Chen",
                    "hidden": false
                },
                {
                    "_id": "68e34f7173e20ab5778420c2",
                    "name": "Bin Wang",
                    "hidden": false
                },
                {
                    "_id": "68e34f7173e20ab5778420c3",
                    "name": "Weijia Li",
                    "hidden": false
                },
                {
                    "_id": "68e34f7173e20ab5778420c4",
                    "name": "Conghui He",
                    "hidden": false
                },
                {
                    "_id": "68e34f7173e20ab5778420c5",
                    "name": "Linfeng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T04:56:40.000Z",
            "submittedOnDailyAt": "2025-10-06T03:44:13.823Z",
            "title": "Efficient Multi-modal Large Language Models via Progressive Consistency\n  Distillation",
            "submittedOnDailyBy": {
                "_id": "653b8c3e97a4d71d950e2f20",
                "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
                "isPro": false,
                "fullname": "Zichen Wen",
                "user": "zichenwen",
                "type": "user"
            },
            "summary": "Visual tokens consume substantial computational resources in multi-modal\nlarge models (MLLMs), significantly compromising their efficiency. Recent works\nhave attempted to improve efficiency by compressing visual tokens during\ntraining, either through modifications to model components or by introducing\nadditional parameters. However, they often overlook the increased learning\ndifficulty caused by such compression, as the model's parameter space struggles\nto quickly adapt to the substantial perturbations in the feature space induced\nby token compression. In this work, we propose to develop Efficient MLLMs via\nProgressive Consistency Distillation (EPIC), a progressive learning framework.\nSpecifically, by decomposing the feature space perturbations introduced by\ntoken compression along the token-wise and layer-wise dimensions, we introduce\ntoken consistency distillation and layer consistency distillation,\nrespectively, aiming to reduce the training difficulty by leveraging guidance\nfrom a teacher model and following a progressive learning trajectory. Extensive\nexperiments demonstrate the superior effectiveness, robustness, and\ngeneralization capabilities of our proposed framework.",
            "upvotes": 30,
            "discussionId": "68e34f7173e20ab5778420c6",
            "projectPage": "https://zichenwen1.github.io/EPIC",
            "githubRepo": "https://github.com/ZichenWen1/EPIC",
            "ai_summary": "EPIC, a progressive learning framework, improves the efficiency of multi-modal large models by reducing training difficulty through token and layer consistency distillation during visual token compression.",
            "ai_keywords": [
                "visual tokens",
                "multi-modal large models",
                "MLLMs",
                "token compression",
                "parameter space",
                "feature space",
                "progressive learning framework",
                "token consistency distillation",
                "layer consistency distillation",
                "teacher model",
                "progressive learning trajectory"
            ],
            "githubStars": 9,
            "organization": {
                "_id": "63e5ef7bf2e9a8f22c515654",
                "name": "SJTU",
                "fullname": "Shanghai Jiao Tong University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
            }
        },
        "publishedAt": "2025-10-01T00:56:40.000Z",
        "title": "Efficient Multi-modal Large Language Models via Progressive Consistency\n  Distillation",
        "summary": "Visual tokens consume substantial computational resources in multi-modal\nlarge models (MLLMs), significantly compromising their efficiency. Recent works\nhave attempted to improve efficiency by compressing visual tokens during\ntraining, either through modifications to model components or by introducing\nadditional parameters. However, they often overlook the increased learning\ndifficulty caused by such compression, as the model's parameter space struggles\nto quickly adapt to the substantial perturbations in the feature space induced\nby token compression. In this work, we propose to develop Efficient MLLMs via\nProgressive Consistency Distillation (EPIC), a progressive learning framework.\nSpecifically, by decomposing the feature space perturbations introduced by\ntoken compression along the token-wise and layer-wise dimensions, we introduce\ntoken consistency distillation and layer consistency distillation,\nrespectively, aiming to reduce the training difficulty by leveraging guidance\nfrom a teacher model and following a progressive learning trajectory. Extensive\nexperiments demonstrate the superior effectiveness, robustness, and\ngeneralization capabilities of our proposed framework.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00515.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653b8c3e97a4d71d950e2f20",
            "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
            "fullname": "Zichen Wen",
            "name": "zichenwen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "organization": {
            "_id": "63e5ef7bf2e9a8f22c515654",
            "name": "SJTU",
            "fullname": "Shanghai Jiao Tong University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01068",
            "authors": [
                {
                    "_id": "68de0d466024653e8a3ed16f",
                    "user": {
                        "_id": "64780ba6f32a4117fd182b81",
                        "avatarUrl": "/avatars/85f01f4c6c745a04f04805462f9fe9c2.svg",
                        "isPro": false,
                        "fullname": "CAO",
                        "user": "SAGE2000",
                        "type": "user"
                    },
                    "name": "Jiahang Cao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-06T03:21:22.930Z",
                    "hidden": false
                },
                {
                    "_id": "68de0d466024653e8a3ed170",
                    "name": "Yize Huang",
                    "hidden": false
                },
                {
                    "_id": "68de0d466024653e8a3ed171",
                    "name": "Hanzhong Guo",
                    "hidden": false
                },
                {
                    "_id": "68de0d466024653e8a3ed172",
                    "name": "Rui Zhang",
                    "hidden": false
                },
                {
                    "_id": "68de0d466024653e8a3ed173",
                    "name": "Mu Nan",
                    "hidden": false
                },
                {
                    "_id": "68de0d466024653e8a3ed174",
                    "name": "Weijian Mai",
                    "hidden": false
                },
                {
                    "_id": "68de0d466024653e8a3ed175",
                    "name": "Jiaxu Wang",
                    "hidden": false
                },
                {
                    "_id": "68de0d466024653e8a3ed176",
                    "name": "Hao Cheng",
                    "hidden": false
                },
                {
                    "_id": "68de0d466024653e8a3ed177",
                    "name": "Jingkai Sun",
                    "hidden": false
                },
                {
                    "_id": "68de0d466024653e8a3ed178",
                    "name": "Gang Han",
                    "hidden": false
                },
                {
                    "_id": "68de0d466024653e8a3ed179",
                    "name": "Wen Zhao",
                    "hidden": false
                },
                {
                    "_id": "68de0d466024653e8a3ed17a",
                    "name": "Qiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68de0d466024653e8a3ed17b",
                    "name": "Yijie Guo",
                    "hidden": false
                },
                {
                    "_id": "68de0d466024653e8a3ed17c",
                    "name": "Qihao Zheng",
                    "hidden": false
                },
                {
                    "_id": "68de0d466024653e8a3ed17d",
                    "name": "Chunfeng Song",
                    "hidden": false
                },
                {
                    "_id": "68de0d466024653e8a3ed17e",
                    "name": "Xiao Li",
                    "hidden": false
                },
                {
                    "_id": "68de0d466024653e8a3ed17f",
                    "name": "Ping Luo",
                    "hidden": false
                },
                {
                    "_id": "68de0d466024653e8a3ed180",
                    "user": {
                        "_id": "64b6ce23dbbd1f2cdb624d56",
                        "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
                        "isPro": false,
                        "fullname": "Andrew Luo",
                        "user": "aluo-x",
                        "type": "user"
                    },
                    "name": "Andrew F. Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-06T03:21:20.592Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64780ba6f32a4117fd182b81/bsb_536O8r19Ocb6K03Ww.png"
            ],
            "publishedAt": "2025-10-01T16:05:53.000Z",
            "submittedOnDailyAt": "2025-10-06T00:35:20.824Z",
            "title": "Compose Your Policies! Improving Diffusion-based or Flow-based Robot\n  Policies via Test-time Distribution-level Composition",
            "submittedOnDailyBy": {
                "_id": "64780ba6f32a4117fd182b81",
                "avatarUrl": "/avatars/85f01f4c6c745a04f04805462f9fe9c2.svg",
                "isPro": false,
                "fullname": "CAO",
                "user": "SAGE2000",
                "type": "user"
            },
            "summary": "Diffusion-based models for robotic control, including vision-language-action\n(VLA) and vision-action (VA) policies, have demonstrated significant\ncapabilities. Yet their advancement is constrained by the high cost of\nacquiring large-scale interaction datasets. This work introduces an alternative\nparadigm for enhancing policy performance without additional model training.\nPerhaps surprisingly, we demonstrate that the composed policies can exceed the\nperformance of either parent policy. Our contribution is threefold. First, we\nestablish a theoretical foundation showing that the convex composition of\ndistributional scores from multiple diffusion models can yield a superior\none-step functional objective compared to any individual score. A\nGr\\\"onwall-type bound is then used to show that this single-step improvement\npropagates through entire generation trajectories, leading to systemic\nperformance gains. Second, motivated by these results, we propose General\nPolicy Composition (GPC), a training-free method that enhances performance by\ncombining the distributional scores of multiple pre-trained policies via a\nconvex combination and test-time search. GPC is versatile, allowing for the\nplug-and-play composition of heterogeneous policies, including VA and VLA\nmodels, as well as those based on diffusion or flow-matching, irrespective of\ntheir input visual modalities. Third, we provide extensive empirical\nvalidation. Experiments on Robomimic, PushT, and RoboTwin benchmarks, alongside\nreal-world robotic evaluations, confirm that GPC consistently improves\nperformance and adaptability across a diverse set of tasks. Further analysis of\nalternative composition operators and weighting strategies offers insights into\nthe mechanisms underlying the success of GPC. These results establish GPC as a\nsimple yet effective method for improving control performance by leveraging\nexisting policies.",
            "upvotes": 17,
            "discussionId": "68de0d476024653e8a3ed181",
            "projectPage": "https://sagecao1125.github.io/GPC-Site/",
            "githubRepo": "https://github.com/SageCao1125/GPC",
            "ai_summary": "General Policy Composition (GPC) enhances robotic control performance by combining pre-trained diffusion-based policies without additional training, leading to superior results across various benchmarks.",
            "ai_keywords": [
                "diffusion-based models",
                "vision-language-action",
                "vision-action",
                "distributional scores",
                "convex composition",
                "Gr\\\"onwall-type bound",
                "General Policy Composition",
                "GPC",
                "pre-trained policies",
                "Robomimic",
                "PushT",
                "RoboTwin",
                "real-world robotic evaluations"
            ],
            "githubStars": 9,
            "organization": {
                "_id": "67ea9ecfc234715db8dbf339",
                "name": "hkuhk",
                "fullname": "The University of Hong Kong",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
            }
        },
        "publishedAt": "2025-10-01T12:05:53.000Z",
        "title": "Compose Your Policies! Improving Diffusion-based or Flow-based Robot\n  Policies via Test-time Distribution-level Composition",
        "summary": "Diffusion-based models for robotic control, including vision-language-action\n(VLA) and vision-action (VA) policies, have demonstrated significant\ncapabilities. Yet their advancement is constrained by the high cost of\nacquiring large-scale interaction datasets. This work introduces an alternative\nparadigm for enhancing policy performance without additional model training.\nPerhaps surprisingly, we demonstrate that the composed policies can exceed the\nperformance of either parent policy. Our contribution is threefold. First, we\nestablish a theoretical foundation showing that the convex composition of\ndistributional scores from multiple diffusion models can yield a superior\none-step functional objective compared to any individual score. A\nGr\\\"onwall-type bound is then used to show that this single-step improvement\npropagates through entire generation trajectories, leading to systemic\nperformance gains. Second, motivated by these results, we propose General\nPolicy Composition (GPC), a training-free method that enhances performance by\ncombining the distributional scores of multiple pre-trained policies via a\nconvex combination and test-time search. GPC is versatile, allowing for the\nplug-and-play composition of heterogeneous policies, including VA and VLA\nmodels, as well as those based on diffusion or flow-matching, irrespective of\ntheir input visual modalities. Third, we provide extensive empirical\nvalidation. Experiments on Robomimic, PushT, and RoboTwin benchmarks, alongside\nreal-world robotic evaluations, confirm that GPC consistently improves\nperformance and adaptability across a diverse set of tasks. Further analysis of\nalternative composition operators and weighting strategies offers insights into\nthe mechanisms underlying the success of GPC. These results establish GPC as a\nsimple yet effective method for improving control performance by leveraging\nexisting policies.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64780ba6f32a4117fd182b81/bsb_536O8r19Ocb6K03Ww.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01068.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64780ba6f32a4117fd182b81",
            "avatarUrl": "/avatars/85f01f4c6c745a04f04805462f9fe9c2.svg",
            "fullname": "CAO",
            "name": "SAGE2000",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "67ea9ecfc234715db8dbf339",
            "name": "hkuhk",
            "fullname": "The University of Hong Kong",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.03194",
            "authors": [
                {
                    "_id": "68e33f3573e20ab577842038",
                    "user": {
                        "_id": "63603cc26f410322ee9c393c",
                        "avatarUrl": "/avatars/ebe3732d6cc813d7aef1866f1e8757bf.svg",
                        "isPro": false,
                        "fullname": "Zichen Chen",
                        "user": "zcchen",
                        "type": "user"
                    },
                    "name": "Zichen Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-06T12:48:59.817Z",
                    "hidden": false
                },
                {
                    "_id": "68e33f3573e20ab577842039",
                    "name": "Jiefeng Chen",
                    "hidden": false
                },
                {
                    "_id": "68e33f3573e20ab57784203a",
                    "name": "Sercan Ö. Arik",
                    "hidden": false
                },
                {
                    "_id": "68e33f3573e20ab57784203b",
                    "name": "Misha Sra",
                    "hidden": false
                },
                {
                    "_id": "68e33f3573e20ab57784203c",
                    "name": "Tomas Pfister",
                    "hidden": false
                },
                {
                    "_id": "68e33f3573e20ab57784203d",
                    "name": "Jinsung Yoon",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-03T17:30:16.000Z",
            "submittedOnDailyAt": "2025-10-06T04:36:21.820Z",
            "title": "CoDA: Agentic Systems for Collaborative Data Visualization",
            "submittedOnDailyBy": {
                "_id": "653df1323479e9ebbe3eb6cc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
                "isPro": true,
                "fullname": "Zhangchen Xu",
                "user": "zhangchenxu",
                "type": "user"
            },
            "summary": "Deep research has revolutionized data analysis, yet data scientists still\ndevote substantial time to manually crafting visualizations, highlighting the\nneed for robust automation from natural language queries. However, current\nsystems struggle with complex datasets containing multiple files and iterative\nrefinement. Existing approaches, including simple single- or multi-agent\nsystems, often oversimplify the task, focusing on initial query parsing while\nfailing to robustly manage data complexity, code errors, or final visualization\nquality. In this paper, we reframe this challenge as a collaborative\nmulti-agent problem. We introduce CoDA, a multi-agent system that employs\nspecialized LLM agents for metadata analysis, task planning, code generation,\nand self-reflection. We formalize this pipeline, demonstrating how\nmetadata-focused analysis bypasses token limits and quality-driven refinement\nensures robustness. Extensive evaluations show CoDA achieves substantial gains\nin the overall score, outperforming competitive baselines by up to 41.5%. This\nwork demonstrates that the future of visualization automation lies not in\nisolated code generation but in integrated, collaborative agentic workflows.",
            "upvotes": 16,
            "discussionId": "68e33f3573e20ab57784203e",
            "ai_summary": "CoDA, a multi-agent system using specialized LLM agents, enhances visualization automation by managing data complexity and ensuring high-quality visualizations through collaborative workflows.",
            "ai_keywords": [
                "LLM agents",
                "metadata analysis",
                "task planning",
                "code generation",
                "self-reflection",
                "token limits",
                "quality-driven refinement"
            ],
            "organization": {
                "_id": "5e6aca39878b8b2bf9806447",
                "name": "google",
                "fullname": "Google",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
            }
        },
        "publishedAt": "2025-10-03T13:30:16.000Z",
        "title": "CoDA: Agentic Systems for Collaborative Data Visualization",
        "summary": "Deep research has revolutionized data analysis, yet data scientists still\ndevote substantial time to manually crafting visualizations, highlighting the\nneed for robust automation from natural language queries. However, current\nsystems struggle with complex datasets containing multiple files and iterative\nrefinement. Existing approaches, including simple single- or multi-agent\nsystems, often oversimplify the task, focusing on initial query parsing while\nfailing to robustly manage data complexity, code errors, or final visualization\nquality. In this paper, we reframe this challenge as a collaborative\nmulti-agent problem. We introduce CoDA, a multi-agent system that employs\nspecialized LLM agents for metadata analysis, task planning, code generation,\nand self-reflection. We formalize this pipeline, demonstrating how\nmetadata-focused analysis bypasses token limits and quality-driven refinement\nensures robustness. Extensive evaluations show CoDA achieves substantial gains\nin the overall score, outperforming competitive baselines by up to 41.5%. This\nwork demonstrates that the future of visualization automation lies not in\nisolated code generation but in integrated, collaborative agentic workflows.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03194.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653df1323479e9ebbe3eb6cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
            "fullname": "Zhangchen Xu",
            "name": "zhangchenxu",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 20
        },
        "organization": {
            "_id": "5e6aca39878b8b2bf9806447",
            "name": "google",
            "fullname": "Google",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.23202",
            "authors": [
                {
                    "_id": "68e3c1fb73e20ab5778421db",
                    "name": "Vage Egiazarian",
                    "hidden": false
                },
                {
                    "_id": "68e3c1fb73e20ab5778421dc",
                    "name": "Roberto L. Castro",
                    "hidden": false
                },
                {
                    "_id": "68e3c1fb73e20ab5778421dd",
                    "name": "Denis Kuznedelev",
                    "hidden": false
                },
                {
                    "_id": "68e3c1fb73e20ab5778421de",
                    "name": "Andrei Panferov",
                    "hidden": false
                },
                {
                    "_id": "68e3c1fb73e20ab5778421df",
                    "name": "Eldar Kurtic",
                    "hidden": false
                },
                {
                    "_id": "68e3c1fb73e20ab5778421e0",
                    "name": "Shubhra Pandit",
                    "hidden": false
                },
                {
                    "_id": "68e3c1fb73e20ab5778421e1",
                    "name": "Alexandre Marques",
                    "hidden": false
                },
                {
                    "_id": "68e3c1fb73e20ab5778421e2",
                    "name": "Mark Kurtz",
                    "hidden": false
                },
                {
                    "_id": "68e3c1fb73e20ab5778421e3",
                    "name": "Saleh Ashkboos",
                    "hidden": false
                },
                {
                    "_id": "68e3c1fb73e20ab5778421e4",
                    "name": "Torsten Hoefler",
                    "hidden": false
                },
                {
                    "_id": "68e3c1fb73e20ab5778421e5",
                    "name": "Dan Alistarh",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/629cf0475a13ba8233dd18c9/HV5zs2Leu_DQls_aRvb7-.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/629cf0475a13ba8233dd18c9/CZmWr2_hch4COTrvx-Odr.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/629cf0475a13ba8233dd18c9/q1lkv6XbDBI6xwmayCF5y.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/629cf0475a13ba8233dd18c9/TM5bxLhlQGEZJZsuzVaCn.jpeg"
            ],
            "publishedAt": "2025-09-27T09:22:21.000Z",
            "submittedOnDailyAt": "2025-10-06T11:55:03.138Z",
            "title": "Bridging the Gap Between Promise and Performance for Microscaling FP4\n  Quantization",
            "submittedOnDailyBy": {
                "_id": "629cf0475a13ba8233dd18c9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654452258405-noauth.jpeg",
                "isPro": false,
                "fullname": "Denis Kuznedelev",
                "user": "SpiridonSunRotator",
                "type": "user"
            },
            "summary": "The recent hardware-accelerated microscaling 4-bit floating-point formats\nsuch as MXFP4 and NVFP4, supported on NVIDIA and AMD GPUs, promise to\nrevolutionize large language model (LLM) inference. Yet, their practical\nbenefits remain unproven. We present the first comprehensive study of MXFP4 and\nNVFP4 for post-training quantization, revealing gaps between their promise and\nreal-world performance. Our analysis shows that state-of-the-art methods\nstruggle with FP4, due to two key issues: (1) NVFP4's small group size provably\nneutralizes traditional outlier mitigation techniques; (2) MXFP4's power-of-two\nscale quantization severely degrades accuracy due to high induced error. To\nbridge this gap, we introduce Micro-Rotated-GPTQ (MR-GPTQ), a variant of the\nclassic GPTQ quantization algorithm that tailors the quantization process to\nFP4's unique properties, by using block-wise Hadamard transforms and\nformat-specific optimizations. We support our proposal with a set of\nhigh-performance GPU kernels that enable the MR-GPTQ format with negligible\noverhead, by rotation fusion into the weights, and fast online computation of\nthe activations. This leads to speedups vs. FP16 of up to 3.6x layer-wise, and\n2.2x end-to-end on NVIDIA B200, and of 6x layer-wise and 4x end-to-end on\nRTX5090. Our extensive empirical evaluation demonstrates that MR-GPTQ matches\nor outperforms state-of-the-art accuracy, significantly boosting MXFP4, to the\npoint where it nears that of NVFP4. We conclude that, while FP4 is not an\nautomatic upgrade over INT4, format-specialized methods like MR-GPTQ can unlock\na new frontier of accuracy-performance trade-offs.",
            "upvotes": 16,
            "discussionId": "68e3c1fc73e20ab5778421e6",
            "ai_summary": "A new quantization method, Micro-Rotated-GPTQ, addresses the challenges of 4-bit floating-point formats MXFP4 and NVFP4, achieving high performance and accuracy in large language model inference.",
            "ai_keywords": [
                "MXFP4",
                "NVFP4",
                "post-training quantization",
                "GPTQ",
                "block-wise Hadamard transforms",
                "format-specific optimizations",
                "GPU kernels",
                "rotation fusion",
                "online computation of activations",
                "accuracy-performance trade-offs"
            ],
            "organization": {
                "_id": "64d0ffde9cff738203a50e9b",
                "name": "ISTA-DASLab",
                "fullname": " IST Austria Distributed Algorithms and Systems Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/628e0ce4e53bbd334577fcb0/TRPtgtSavYjDJOK3S1I8M.png"
            }
        },
        "publishedAt": "2025-09-27T05:22:21.000Z",
        "title": "Bridging the Gap Between Promise and Performance for Microscaling FP4\n  Quantization",
        "summary": "The recent hardware-accelerated microscaling 4-bit floating-point formats\nsuch as MXFP4 and NVFP4, supported on NVIDIA and AMD GPUs, promise to\nrevolutionize large language model (LLM) inference. Yet, their practical\nbenefits remain unproven. We present the first comprehensive study of MXFP4 and\nNVFP4 for post-training quantization, revealing gaps between their promise and\nreal-world performance. Our analysis shows that state-of-the-art methods\nstruggle with FP4, due to two key issues: (1) NVFP4's small group size provably\nneutralizes traditional outlier mitigation techniques; (2) MXFP4's power-of-two\nscale quantization severely degrades accuracy due to high induced error. To\nbridge this gap, we introduce Micro-Rotated-GPTQ (MR-GPTQ), a variant of the\nclassic GPTQ quantization algorithm that tailors the quantization process to\nFP4's unique properties, by using block-wise Hadamard transforms and\nformat-specific optimizations. We support our proposal with a set of\nhigh-performance GPU kernels that enable the MR-GPTQ format with negligible\noverhead, by rotation fusion into the weights, and fast online computation of\nthe activations. This leads to speedups vs. FP16 of up to 3.6x layer-wise, and\n2.2x end-to-end on NVIDIA B200, and of 6x layer-wise and 4x end-to-end on\nRTX5090. Our extensive empirical evaluation demonstrates that MR-GPTQ matches\nor outperforms state-of-the-art accuracy, significantly boosting MXFP4, to the\npoint where it nears that of NVFP4. We conclude that, while FP4 is not an\nautomatic upgrade over INT4, format-specialized methods like MR-GPTQ can unlock\na new frontier of accuracy-performance trade-offs.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/629cf0475a13ba8233dd18c9/HV5zs2Leu_DQls_aRvb7-.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/629cf0475a13ba8233dd18c9/CZmWr2_hch4COTrvx-Odr.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/629cf0475a13ba8233dd18c9/q1lkv6XbDBI6xwmayCF5y.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/629cf0475a13ba8233dd18c9/TM5bxLhlQGEZJZsuzVaCn.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23202.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "629cf0475a13ba8233dd18c9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654452258405-noauth.jpeg",
            "fullname": "Denis Kuznedelev",
            "name": "SpiridonSunRotator",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "64d0ffde9cff738203a50e9b",
            "name": "ISTA-DASLab",
            "fullname": " IST Austria Distributed Algorithms and Systems Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/628e0ce4e53bbd334577fcb0/TRPtgtSavYjDJOK3S1I8M.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.02665",
            "authors": [
                {
                    "_id": "68e31d5073e20ab577841f93",
                    "name": "Shijian Deng",
                    "hidden": false
                },
                {
                    "_id": "68e31d5073e20ab577841f94",
                    "name": "Kai Wang",
                    "hidden": false
                },
                {
                    "_id": "68e31d5073e20ab577841f95",
                    "name": "Tianyu Yang",
                    "hidden": false
                },
                {
                    "_id": "68e31d5073e20ab577841f96",
                    "name": "Harsh Singh",
                    "hidden": false
                },
                {
                    "_id": "68e31d5073e20ab577841f97",
                    "name": "Yapeng Tian",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-03T01:48:26.000Z",
            "submittedOnDailyAt": "2025-10-06T00:10:23.995Z",
            "title": "Self-Improvement in Multimodal Large Language Models: A Survey",
            "submittedOnDailyBy": {
                "_id": "668bafa02c3897266dcbacd0",
                "avatarUrl": "/avatars/ef41cd4a855ebbe6efc79f34c38ef136.svg",
                "isPro": false,
                "fullname": "Shijian Deng",
                "user": "ShijianDeng",
                "type": "user"
            },
            "summary": "Recent advancements in self-improvement for Large Language Models (LLMs) have\nefficiently enhanced model capabilities without significantly increasing costs,\nparticularly in terms of human effort. While this area is still relatively\nyoung, its extension to the multimodal domain holds immense potential for\nleveraging diverse data sources and developing more general self-improving\nmodels. This survey is the first to provide a comprehensive overview of\nself-improvement in Multimodal LLMs (MLLMs). We provide a structured overview\nof the current literature and discuss methods from three perspectives: 1) data\ncollection, 2) data organization, and 3) model optimization, to facilitate the\nfurther development of self-improvement in MLLMs. We also include commonly used\nevaluations and downstream applications. Finally, we conclude by outlining open\nchallenges and future research directions.",
            "upvotes": 11,
            "discussionId": "68e31d5073e20ab577841f98",
            "ai_summary": "A survey of self-improvement methods in Multimodal Large Language Models (MLLMs) from data collection, organization, and model optimization perspectives.",
            "ai_keywords": [
                "Large Language Models",
                "Multimodal LLMs",
                "self-improvement",
                "data collection",
                "data organization",
                "model optimization",
                "evaluations",
                "downstream applications",
                "open challenges",
                "future research directions"
            ]
        },
        "publishedAt": "2025-10-02T21:48:26.000Z",
        "title": "Self-Improvement in Multimodal Large Language Models: A Survey",
        "summary": "Recent advancements in self-improvement for Large Language Models (LLMs) have\nefficiently enhanced model capabilities without significantly increasing costs,\nparticularly in terms of human effort. While this area is still relatively\nyoung, its extension to the multimodal domain holds immense potential for\nleveraging diverse data sources and developing more general self-improving\nmodels. This survey is the first to provide a comprehensive overview of\nself-improvement in Multimodal LLMs (MLLMs). We provide a structured overview\nof the current literature and discuss methods from three perspectives: 1) data\ncollection, 2) data organization, and 3) model optimization, to facilitate the\nfurther development of self-improvement in MLLMs. We also include commonly used\nevaluations and downstream applications. Finally, we conclude by outlining open\nchallenges and future research directions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02665.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "668bafa02c3897266dcbacd0",
            "avatarUrl": "/avatars/ef41cd4a855ebbe6efc79f34c38ef136.svg",
            "fullname": "Shijian Deng",
            "name": "ShijianDeng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26354",
            "authors": [
                {
                    "_id": "68dfe50f73e20ab5778419f0",
                    "name": "Shuai Shao",
                    "hidden": false
                },
                {
                    "_id": "68dfe50f73e20ab5778419f1",
                    "user": {
                        "_id": "66e2624a436a1798365e4581",
                        "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg",
                        "isPro": false,
                        "fullname": "Qihan Ren",
                        "user": "jasonrqh",
                        "type": "user"
                    },
                    "name": "Qihan Ren",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-06T12:49:04.168Z",
                    "hidden": false
                },
                {
                    "_id": "68dfe50f73e20ab5778419f2",
                    "name": "Chen Qian",
                    "hidden": false
                },
                {
                    "_id": "68dfe50f73e20ab5778419f3",
                    "name": "Boyi Wei",
                    "hidden": false
                },
                {
                    "_id": "68dfe50f73e20ab5778419f4",
                    "name": "Dadi Guo",
                    "hidden": false
                },
                {
                    "_id": "68dfe50f73e20ab5778419f5",
                    "user": {
                        "_id": "64f73a44102fbfb26410962e",
                        "avatarUrl": "/avatars/328302a495de6a4418be835456d1d3c6.svg",
                        "isPro": false,
                        "fullname": "jingyi Yang",
                        "user": "JY-Young",
                        "type": "user"
                    },
                    "name": "Jingyi Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-05T12:46:26.913Z",
                    "hidden": false
                },
                {
                    "_id": "68dfe50f73e20ab5778419f6",
                    "name": "Xinhao Song",
                    "hidden": false
                },
                {
                    "_id": "68dfe50f73e20ab5778419f7",
                    "name": "Linfeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dfe50f73e20ab5778419f8",
                    "name": "Weinan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dfe50f73e20ab5778419f9",
                    "name": "Dongrui Liu",
                    "hidden": false
                },
                {
                    "_id": "68dfe50f73e20ab5778419fa",
                    "name": "Jing Shao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T14:55:55.000Z",
            "submittedOnDailyAt": "2025-10-06T02:44:58.885Z",
            "title": "Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents",
            "submittedOnDailyBy": {
                "_id": "66e2624a436a1798365e4581",
                "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg",
                "isPro": false,
                "fullname": "Qihan Ren",
                "user": "jasonrqh",
                "type": "user"
            },
            "summary": "Advances in Large Language Models (LLMs) have enabled a new class of\nself-evolving agents that autonomously improve through interaction with the\nenvironment, demonstrating strong capabilities. However, self-evolution also\nintroduces novel risks overlooked by current safety research. In this work, we\nstudy the case where an agent's self-evolution deviates in unintended ways,\nleading to undesirable or even harmful outcomes. We refer to this as\nMisevolution. To provide a systematic investigation, we evaluate misevolution\nalong four key evolutionary pathways: model, memory, tool, and workflow. Our\nempirical findings reveal that misevolution is a widespread risk, affecting\nagents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent\nrisks are observed in the self-evolutionary process, such as the degradation of\nsafety alignment after memory accumulation, or the unintended introduction of\nvulnerabilities in tool creation and reuse. To our knowledge, this is the first\nstudy to systematically conceptualize misevolution and provide empirical\nevidence of its occurrence, highlighting an urgent need for new safety\nparadigms for self-evolving agents. Finally, we discuss potential mitigation\nstrategies to inspire further research on building safer and more trustworthy\nself-evolving agents. Our code and data are available at\nhttps://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes\nexamples that may be offensive or harmful in nature.",
            "upvotes": 9,
            "discussionId": "68dfe51073e20ab5778419fb",
            "githubRepo": "https://github.com/ShaoShuai0605/Misevolution",
            "ai_summary": "Self-evolving agents based on Large Language Models can deviate in unintended ways, leading to various risks such as safety misalignment and vulnerability introduction, necessitating new safety paradigms.",
            "ai_keywords": [
                "Large Language Models",
                "self-evolving agents",
                "Misevolution",
                "evolutionary pathways",
                "safety alignment",
                "memory accumulation",
                "tool creation",
                "vulnerability introduction"
            ],
            "githubStars": 10
        },
        "publishedAt": "2025-09-30T10:55:55.000Z",
        "title": "Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents",
        "summary": "Advances in Large Language Models (LLMs) have enabled a new class of\nself-evolving agents that autonomously improve through interaction with the\nenvironment, demonstrating strong capabilities. However, self-evolution also\nintroduces novel risks overlooked by current safety research. In this work, we\nstudy the case where an agent's self-evolution deviates in unintended ways,\nleading to undesirable or even harmful outcomes. We refer to this as\nMisevolution. To provide a systematic investigation, we evaluate misevolution\nalong four key evolutionary pathways: model, memory, tool, and workflow. Our\nempirical findings reveal that misevolution is a widespread risk, affecting\nagents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent\nrisks are observed in the self-evolutionary process, such as the degradation of\nsafety alignment after memory accumulation, or the unintended introduction of\nvulnerabilities in tool creation and reuse. To our knowledge, this is the first\nstudy to systematically conceptualize misevolution and provide empirical\nevidence of its occurrence, highlighting an urgent need for new safety\nparadigms for self-evolving agents. Finally, we discuss potential mitigation\nstrategies to inspire further research on building safer and more trustworthy\nself-evolving agents. Our code and data are available at\nhttps://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes\nexamples that may be offensive or harmful in nature.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26354.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66e2624a436a1798365e4581",
            "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg",
            "fullname": "Qihan Ren",
            "name": "jasonrqh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.22033",
            "authors": [
                {
                    "_id": "68e3951373e20ab57784217a",
                    "name": "Anton Korznikov",
                    "hidden": false
                },
                {
                    "_id": "68e3951373e20ab57784217b",
                    "name": "Andrey Galichin",
                    "hidden": false
                },
                {
                    "_id": "68e3951373e20ab57784217c",
                    "name": "Alexey Dontsov",
                    "hidden": false
                },
                {
                    "_id": "68e3951373e20ab57784217d",
                    "name": "Oleg Rogov",
                    "hidden": false
                },
                {
                    "_id": "68e3951373e20ab57784217e",
                    "name": "Elena Tutubalina",
                    "hidden": false
                },
                {
                    "_id": "68e3951373e20ab57784217f",
                    "name": "Ivan Oseledets",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T08:10:52.000Z",
            "submittedOnDailyAt": "2025-10-06T08:39:27.575Z",
            "title": "OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features",
            "submittedOnDailyBy": {
                "_id": "60cd95ee15ecba5f2200304a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
                "isPro": false,
                "fullname": "Alexey Dontsov",
                "user": "therem",
                "type": "user"
            },
            "summary": "Sparse autoencoders (SAEs) are a technique for sparse decomposition of neural\nnetwork activations into human-interpretable features. However, current SAEs\nsuffer from feature absorption, where specialized features capture instances of\ngeneral features creating representation holes, and feature composition, where\nindependent features merge into composite representations. In this work, we\nintroduce Orthogonal SAE (OrtSAE), a novel approach aimed to mitigate these\nissues by enforcing orthogonality between the learned features. By implementing\na new training procedure that penalizes high pairwise cosine similarity between\nSAE features, OrtSAE promotes the development of disentangled features while\nscaling linearly with the SAE size, avoiding significant computational\noverhead. We train OrtSAE across different models and layers and compare it\nwith other methods. We find that OrtSAE discovers 9% more distinct features,\nreduces feature absorption (by 65%) and composition (by 15%), improves\nperformance on spurious correlation removal (+6%), and achieves on-par\nperformance for other downstream tasks compared to traditional SAEs.",
            "upvotes": 8,
            "discussionId": "68e3951373e20ab577842180",
            "ai_summary": "Orthogonal Sparse Autoencoders (OrtSAE) mitigate feature absorption and composition by enforcing orthogonality, leading to better feature discovery and improved performance on spurious correlation removal.",
            "ai_keywords": [
                "sparse autoencoders",
                "feature absorption",
                "feature composition",
                "orthogonal SAE",
                "OrtSAE",
                "disentangled features",
                "cosine similarity",
                "spurious correlation removal"
            ]
        },
        "publishedAt": "2025-09-26T04:10:52.000Z",
        "title": "OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features",
        "summary": "Sparse autoencoders (SAEs) are a technique for sparse decomposition of neural\nnetwork activations into human-interpretable features. However, current SAEs\nsuffer from feature absorption, where specialized features capture instances of\ngeneral features creating representation holes, and feature composition, where\nindependent features merge into composite representations. In this work, we\nintroduce Orthogonal SAE (OrtSAE), a novel approach aimed to mitigate these\nissues by enforcing orthogonality between the learned features. By implementing\na new training procedure that penalizes high pairwise cosine similarity between\nSAE features, OrtSAE promotes the development of disentangled features while\nscaling linearly with the SAE size, avoiding significant computational\noverhead. We train OrtSAE across different models and layers and compare it\nwith other methods. We find that OrtSAE discovers 9% more distinct features,\nreduces feature absorption (by 65%) and composition (by 15%), improves\nperformance on spurious correlation removal (+6%), and achieves on-par\nperformance for other downstream tasks compared to traditional SAEs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22033.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60cd95ee15ecba5f2200304a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
            "fullname": "Alexey Dontsov",
            "name": "therem",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.03574",
            "authors": [
                {
                    "_id": "68e460e1e4e093a7044e4cae",
                    "name": "Mehmet Onurcan Kaya",
                    "hidden": false
                },
                {
                    "_id": "68e460e1e4e093a7044e4caf",
                    "name": "Desmond Elliott",
                    "hidden": false
                },
                {
                    "_id": "68e460e1e4e093a7044e4cb0",
                    "name": "Dim P. Papadopoulos",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-03T23:49:06.000Z",
            "submittedOnDailyAt": "2025-10-06T23:10:35.968Z",
            "title": "Efficient Test-Time Scaling for Small Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "63be9021da08ed0544f36c38",
                "avatarUrl": "/avatars/42511d8b1d3a3ef99bc154c98b72dfba.svg",
                "isPro": false,
                "fullname": "onurcan",
                "user": "monurcan",
                "type": "user"
            },
            "summary": "Small Vision-Language Models (VLMs) provide a computationally efficient\nalternative to larger models, at the cost of weaker generalization abilities\nand downstream task performance. These shortcomings could be addressed by\ntest-time scaling techniques, but existing methods are typically\ncomputationally demanding, contradicting the resource-efficient design goals of\nsmall models. To address these limitations, we propose two novel and efficient\ntest-time scaling strategies that leverage the model-internal features rather\nthan external supervision: (i) Test-Time Augmentation (TTAug), which generates\nmultiple augmented inputs and aggregates outputs at the token level without\nparameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model\nparameters during inference using consensus-based pseudolabels from TTAug.\nThrough extensive experiments across nine benchmarks, we demonstrate consistent\nperformance improvements while maintaining computational efficiency suitable\nfor resource-constrained environments. The generality of our approach is\ndemonstrated both within models at different scales and across different VLMs\nwithout additional tuning.",
            "upvotes": 7,
            "discussionId": "68e460e2e4e093a7044e4cb1",
            "projectPage": "https://monurcan.github.io/efficient_test_time_scaling",
            "githubRepo": "https://github.com/monurcan/efficient_test_time_scaling",
            "ai_summary": "Two novel test-time scaling strategies, Test-Time Augmentation and Test-Time Adaptation, improve small vision-language models' performance without compromising computational efficiency.",
            "ai_keywords": [
                "Test-Time Augmentation",
                "Test-Time Adaptation",
                "vision-language models",
                "token level",
                "parameter updates",
                "consensus-based pseudolabels",
                "resource-constrained environments"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-10-03T19:49:06.000Z",
        "title": "Efficient Test-Time Scaling for Small Vision-Language Models",
        "summary": "Small Vision-Language Models (VLMs) provide a computationally efficient\nalternative to larger models, at the cost of weaker generalization abilities\nand downstream task performance. These shortcomings could be addressed by\ntest-time scaling techniques, but existing methods are typically\ncomputationally demanding, contradicting the resource-efficient design goals of\nsmall models. To address these limitations, we propose two novel and efficient\ntest-time scaling strategies that leverage the model-internal features rather\nthan external supervision: (i) Test-Time Augmentation (TTAug), which generates\nmultiple augmented inputs and aggregates outputs at the token level without\nparameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model\nparameters during inference using consensus-based pseudolabels from TTAug.\nThrough extensive experiments across nine benchmarks, we demonstrate consistent\nperformance improvements while maintaining computational efficiency suitable\nfor resource-constrained environments. The generality of our approach is\ndemonstrated both within models at different scales and across different VLMs\nwithout additional tuning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03574.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63be9021da08ed0544f36c38",
            "avatarUrl": "/avatars/42511d8b1d3a3ef99bc154c98b72dfba.svg",
            "fullname": "onurcan",
            "name": "monurcan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26388",
            "authors": [
                {
                    "_id": "68e3906b73e20ab57784216e",
                    "name": "Kai-Wei Chang",
                    "hidden": false
                },
                {
                    "_id": "68e3906b73e20ab57784216f",
                    "user": {
                        "_id": "65479cf88767484a0548b56a",
                        "avatarUrl": "/avatars/59dccb799d5f948b0fbd35adb620d269.svg",
                        "isPro": false,
                        "fullname": "EnPei Hu",
                        "user": "dmnph",
                        "type": "user"
                    },
                    "name": "En-Pei Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-06T12:48:21.134Z",
                    "hidden": false
                },
                {
                    "_id": "68e3906b73e20ab577842170",
                    "name": "Chun-Yi Kuan",
                    "hidden": false
                },
                {
                    "_id": "68e3906b73e20ab577842171",
                    "name": "Wenze Ren",
                    "hidden": false
                },
                {
                    "_id": "68e3906b73e20ab577842172",
                    "user": {
                        "_id": "65f99ba9d7c1b04ab55573b8",
                        "avatarUrl": "/avatars/df6d9e6239752a3679e35c5d16d73d59.svg",
                        "isPro": false,
                        "fullname": "Wei-Chih Chen",
                        "user": "WeiChihChen",
                        "type": "user"
                    },
                    "name": "Wei-Chih Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-06T12:48:18.854Z",
                    "hidden": false
                },
                {
                    "_id": "68e3906b73e20ab577842173",
                    "name": "Guan-Ting Lin",
                    "hidden": false
                },
                {
                    "_id": "68e3906b73e20ab577842174",
                    "name": "Yu Tsao",
                    "hidden": false
                },
                {
                    "_id": "68e3906b73e20ab577842175",
                    "name": "Shao-Hua Sun",
                    "hidden": false
                },
                {
                    "_id": "68e3906b73e20ab577842176",
                    "name": "Hung-yi Lee",
                    "hidden": false
                },
                {
                    "_id": "68e3906b73e20ab577842177",
                    "name": "James Glass",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T15:23:39.000Z",
            "submittedOnDailyAt": "2025-10-06T11:25:03.475Z",
            "title": "Game-Time: Evaluating Temporal Dynamics in Spoken Language Models",
            "submittedOnDailyBy": {
                "_id": "65f99ba9d7c1b04ab55573b8",
                "avatarUrl": "/avatars/df6d9e6239752a3679e35c5d16d73d59.svg",
                "isPro": false,
                "fullname": "Wei-Chih Chen",
                "user": "WeiChihChen",
                "type": "user"
            },
            "summary": "Conversational Spoken Language Models (SLMs) are emerging as a promising\nparadigm for real-time speech interaction. However, their capacity of temporal\ndynamics, including the ability to manage timing, tempo and simultaneous\nspeaking, remains a critical and unevaluated challenge for conversational\nfluency. To address this gap, we introduce the Game-Time Benchmark, a framework\nto systematically assess these temporal capabilities. Inspired by how humans\nlearn a language through language activities, Game-Time consists of basic\ninstruction-following tasks and advanced tasks with temporal constraints, such\nas tempo adherence and synchronized responses. Our evaluation of diverse SLM\narchitectures reveals a clear performance disparity: while state-of-the-art\nmodels handle basic tasks well, many contemporary systems still struggle with\nfundamental instruction-following. More critically, nearly all models degrade\nsubstantially under temporal constraints, exposing persistent weaknesses in\ntime awareness and full-duplex interaction. The Game-Time Benchmark provides a\nfoundation for guiding future research toward more temporally-aware\nconversational AI. Demos and datasets are available on our project website\nhttps://ga642381.github.io/Game-Time.",
            "upvotes": 7,
            "discussionId": "68e3906b73e20ab577842178",
            "projectPage": "https://ga642381.github.io/Game-Time",
            "ai_summary": "The Game-Time Benchmark evaluates the temporal dynamics and real-time interaction capabilities of conversational spoken language models, highlighting performance gaps in instruction-following and synchronized responses.",
            "ai_keywords": [
                "Conversational Spoken Language Models",
                "Game-Time Benchmark",
                "temporal dynamics",
                "timing",
                "tempo",
                "simultaneous speaking",
                "conversational fluency",
                "instruction-following tasks",
                "temporal constraints",
                "tempo adherence",
                "synchronized responses",
                "time awareness",
                "full-duplex interaction"
            ]
        },
        "publishedAt": "2025-09-30T11:23:39.000Z",
        "title": "Game-Time: Evaluating Temporal Dynamics in Spoken Language Models",
        "summary": "Conversational Spoken Language Models (SLMs) are emerging as a promising\nparadigm for real-time speech interaction. However, their capacity of temporal\ndynamics, including the ability to manage timing, tempo and simultaneous\nspeaking, remains a critical and unevaluated challenge for conversational\nfluency. To address this gap, we introduce the Game-Time Benchmark, a framework\nto systematically assess these temporal capabilities. Inspired by how humans\nlearn a language through language activities, Game-Time consists of basic\ninstruction-following tasks and advanced tasks with temporal constraints, such\nas tempo adherence and synchronized responses. Our evaluation of diverse SLM\narchitectures reveals a clear performance disparity: while state-of-the-art\nmodels handle basic tasks well, many contemporary systems still struggle with\nfundamental instruction-following. More critically, nearly all models degrade\nsubstantially under temporal constraints, exposing persistent weaknesses in\ntime awareness and full-duplex interaction. The Game-Time Benchmark provides a\nfoundation for guiding future research toward more temporally-aware\nconversational AI. Demos and datasets are available on our project website\nhttps://ga642381.github.io/Game-Time.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26388.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f99ba9d7c1b04ab55573b8",
            "avatarUrl": "/avatars/df6d9e6239752a3679e35c5d16d73d59.svg",
            "fullname": "Wei-Chih Chen",
            "name": "WeiChihChen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.01879",
            "authors": [
                {
                    "_id": "68e322f873e20ab577841fc6",
                    "name": "Yisu Wang",
                    "hidden": false
                },
                {
                    "_id": "68e322f873e20ab577841fc7",
                    "name": "Ming Wang",
                    "hidden": false
                },
                {
                    "_id": "68e322f873e20ab577841fc8",
                    "name": "Haoyuan Song",
                    "hidden": false
                },
                {
                    "_id": "68e322f873e20ab577841fc9",
                    "name": "Wenjie Huang",
                    "hidden": false
                },
                {
                    "_id": "68e322f873e20ab577841fca",
                    "name": "Chaozheng Wang",
                    "hidden": false
                },
                {
                    "_id": "68e322f873e20ab577841fcb",
                    "name": "Yi Xie",
                    "hidden": false
                },
                {
                    "_id": "68e322f873e20ab577841fcc",
                    "name": "Xuming Ran",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T10:35:39.000Z",
            "submittedOnDailyAt": "2025-10-06T00:50:36.906Z",
            "title": "REPAIR: Robust Editing via Progressive Adaptive Intervention and\n  Reintegration",
            "submittedOnDailyBy": {
                "_id": "644378a96cea0db46dc96b39",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644378a96cea0db46dc96b39/wjGNaXjfhSdWr8kM6Amai.jpeg",
                "isPro": false,
                "fullname": "Ming Wang",
                "user": "sci-m-wang",
                "type": "user"
            },
            "summary": "Post-training for large language models (LLMs) is constrained by the high\ncost of acquiring new knowledge or correcting errors and by the unintended side\neffects that frequently arise from retraining. To address these issues, we\nintroduce REPAIR (Robust Editing via Progressive Adaptive Intervention and\nReintegration), a lifelong editing framework designed to support precise and\nlow-cost model updates while preserving non-target knowledge. REPAIR mitigates\nthe instability and conflicts of large-scale sequential edits through a\nclosed-loop feedback mechanism coupled with dynamic memory management.\nFurthermore, by incorporating frequent knowledge fusion and enforcing strong\nlocality guards, REPAIR effectively addresses the shortcomings of traditional\ndistribution-agnostic approaches that often overlook unintended ripple effects.\nOur experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%\nacross multiple model families and significantly reduces knowledge forgetting.\nThis work introduces a robust framework for developing reliable, scalable, and\ncontinually evolving LLMs.",
            "upvotes": 6,
            "discussionId": "68e322f973e20ab577841fcd",
            "ai_summary": "REPAIR is a lifelong editing framework for large language models that enhances editing accuracy and reduces knowledge forgetting through progressive adaptive intervention and reintegration.",
            "ai_keywords": [
                "REPAIR",
                "Robust Editing",
                "Progressive Adaptive Intervention",
                "Reintegration",
                "lifelong editing framework",
                "large language models",
                "closed-loop feedback mechanism",
                "dynamic memory management",
                "knowledge fusion",
                "locality guards",
                "knowledge forgetting"
            ],
            "organization": {
                "_id": "68e32652529937e426ebee7c",
                "name": "ContiAI",
                "fullname": "ContiAI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e325231de2c8d44ab61237/ixA4ZfwwaFekwiLlMzWUy.png"
            }
        },
        "publishedAt": "2025-10-02T06:35:39.000Z",
        "title": "REPAIR: Robust Editing via Progressive Adaptive Intervention and\n  Reintegration",
        "summary": "Post-training for large language models (LLMs) is constrained by the high\ncost of acquiring new knowledge or correcting errors and by the unintended side\neffects that frequently arise from retraining. To address these issues, we\nintroduce REPAIR (Robust Editing via Progressive Adaptive Intervention and\nReintegration), a lifelong editing framework designed to support precise and\nlow-cost model updates while preserving non-target knowledge. REPAIR mitigates\nthe instability and conflicts of large-scale sequential edits through a\nclosed-loop feedback mechanism coupled with dynamic memory management.\nFurthermore, by incorporating frequent knowledge fusion and enforcing strong\nlocality guards, REPAIR effectively addresses the shortcomings of traditional\ndistribution-agnostic approaches that often overlook unintended ripple effects.\nOur experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%\nacross multiple model families and significantly reduces knowledge forgetting.\nThis work introduces a robust framework for developing reliable, scalable, and\ncontinually evolving LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01879.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644378a96cea0db46dc96b39",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644378a96cea0db46dc96b39/wjGNaXjfhSdWr8kM6Amai.jpeg",
            "fullname": "Ming Wang",
            "name": "sci-m-wang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "68e32652529937e426ebee7c",
            "name": "ContiAI",
            "fullname": "ContiAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e325231de2c8d44ab61237/ixA4ZfwwaFekwiLlMzWUy.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.03120",
            "authors": [
                {
                    "_id": "68e322c073e20ab577841fbb",
                    "name": "Zhaojun Sun",
                    "hidden": false
                },
                {
                    "_id": "68e322c073e20ab577841fbc",
                    "name": "Xuzhou Zhu",
                    "hidden": false
                },
                {
                    "_id": "68e322c073e20ab577841fbd",
                    "name": "Xuanhe Zhou",
                    "hidden": false
                },
                {
                    "_id": "68e322c073e20ab577841fbe",
                    "name": "Xin Tong",
                    "hidden": false
                },
                {
                    "_id": "68e322c073e20ab577841fbf",
                    "name": "Shuo Wang",
                    "hidden": false
                },
                {
                    "_id": "68e322c073e20ab577841fc0",
                    "name": "Jie Fu",
                    "hidden": false
                },
                {
                    "_id": "68e322c073e20ab577841fc1",
                    "name": "Guoliang Li",
                    "hidden": false
                },
                {
                    "_id": "68e322c073e20ab577841fc2",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68e322c073e20ab577841fc3",
                    "name": "Fan Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-03T15:49:09.000Z",
            "submittedOnDailyAt": "2025-10-06T00:30:41.651Z",
            "title": "SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Academic survey writing, which distills vast literature into a coherent and\ninsightful narrative, remains a labor-intensive and intellectually demanding\ntask. While recent approaches, such as general DeepResearch agents and\nsurvey-specialized methods, can generate surveys automatically (a.k.a.\nLLM4Survey), their outputs often fall short of human standards and there lacks\na rigorous, reader-aligned benchmark for thoroughly revealing their\ndeficiencies. To fill the gap, we propose a fine-grained, quiz-driven\nevaluation framework SurveyBench, featuring (1) typical survey topics source\nfrom recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;\n(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,\ncoverage breadth, logical coherence), content quality (e.g., synthesis\ngranularity, clarity of insights), and non-textual richness; and (3) a\ndual-mode evaluation protocol that includes content-based and quiz-based\nanswerability tests, explicitly aligned with readers' informational needs.\nResults show SurveyBench effectively challenges existing LLM4Survey approaches\n(e.g., on average 21% lower than human in content-based evaluation).",
            "upvotes": 5,
            "discussionId": "68e322c073e20ab577841fc4",
            "ai_summary": "A new evaluation framework, SurveyBench, assesses the quality of automatically generated academic surveys using a quiz-driven approach, revealing deficiencies in current LLM4Survey methods.",
            "ai_keywords": [
                "DeepResearch agents",
                "LLM4Survey",
                "SurveyBench",
                "arXiv papers",
                "high-quality surveys",
                "outline quality",
                "content quality",
                "synthesis granularity",
                "logical coherence",
                "non-textual richness",
                "content-based evaluation",
                "quiz-based evaluation"
            ]
        },
        "publishedAt": "2025-10-03T11:49:09.000Z",
        "title": "SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?",
        "summary": "Academic survey writing, which distills vast literature into a coherent and\ninsightful narrative, remains a labor-intensive and intellectually demanding\ntask. While recent approaches, such as general DeepResearch agents and\nsurvey-specialized methods, can generate surveys automatically (a.k.a.\nLLM4Survey), their outputs often fall short of human standards and there lacks\na rigorous, reader-aligned benchmark for thoroughly revealing their\ndeficiencies. To fill the gap, we propose a fine-grained, quiz-driven\nevaluation framework SurveyBench, featuring (1) typical survey topics source\nfrom recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;\n(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,\ncoverage breadth, logical coherence), content quality (e.g., synthesis\ngranularity, clarity of insights), and non-textual richness; and (3) a\ndual-mode evaluation protocol that includes content-based and quiz-based\nanswerability tests, explicitly aligned with readers' informational needs.\nResults show SurveyBench effectively challenges existing LLM4Survey approaches\n(e.g., on average 21% lower than human in content-based evaluation).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03120.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 119
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.02410",
            "authors": [
                {
                    "_id": "68e433d3e4e093a7044e4c49",
                    "name": "Patrick Langer",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c4a",
                    "name": "Thomas Kaar",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c4b",
                    "name": "Max Rosenblattl",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c4c",
                    "name": "Maxwell A. Xu",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c4d",
                    "name": "Winnie Chow",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c4e",
                    "name": "Martin Maritsch",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c4f",
                    "name": "Aradhana Verma",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c50",
                    "name": "Brian Han",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c51",
                    "name": "Daniel Seung Kim",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c52",
                    "name": "Henry Chubb",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c53",
                    "name": "Scott Ceresnak",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c54",
                    "name": "Aydin Zahedivash",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c55",
                    "name": "Alexander Tarlochan Singh Sandhu",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c56",
                    "name": "Fatima Rodriguez",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c57",
                    "name": "Daniel McDuff",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c58",
                    "name": "Elgar Fleisch",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c59",
                    "name": "Oliver Aalami",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c5a",
                    "name": "Filipe Barata",
                    "hidden": false
                },
                {
                    "_id": "68e433d3e4e093a7044e4c5b",
                    "name": "Paul Schmiedmayer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T09:58:23.000Z",
            "submittedOnDailyAt": "2025-10-06T19:58:31.340Z",
            "title": "OpenTSLM: Time-Series Language Models for Reasoning over Multivariate\n  Medical Text- and Time-Series Data",
            "submittedOnDailyBy": {
                "_id": "5df7e9e5da6d0311fd3d53f9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583857746553-5df7e9e5da6d0311fd3d53f9.jpeg",
                "isPro": true,
                "fullname": "Thomas Wolf",
                "user": "thomwolf",
                "type": "user"
            },
            "summary": "LLMs have emerged as powerful tools for interpreting multimodal data. In\nmedicine, they hold particular promise for synthesizing large volumes of\nclinical information into actionable insights and digital health applications.\nYet, a major limitation remains their inability to handle time series. To\novercome this gap, we present OpenTSLM, a family of Time Series Language Models\n(TSLMs) created by integrating time series as a native modality to pretrained\nLLMs, enabling reasoning over multiple time series of any length. We\ninvestigate two architectures for OpenTSLM. The first, OpenTSLM-SoftPrompt,\nmodels time series implicitly by concatenating learnable time series tokens\nwith text tokens via soft prompting. Although parameter-efficient, we\nhypothesize that explicit time series modeling scales better and outperforms\nimplicit approaches. We thus introduce OpenTSLM-Flamingo, which integrates time\nseries with text via cross-attention. We benchmark both variants against\nbaselines that treat time series as text tokens or plots, across a suite of\ntext-time-series Chain-of-Thought (CoT) reasoning tasks. We introduce three\ndatasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT. Across all, OpenTSLM models\noutperform baselines, reaching 69.9 F1 in sleep staging and 65.4 in HAR,\ncompared to 9.05 and 52.2 for finetuned text-only models. Notably, even\n1B-parameter OpenTSLM models surpass GPT-4o (15.47 and 2.95). OpenTSLM-Flamingo\nmatches OpenTSLM-SoftPrompt in performance and outperforms on longer sequences,\nwhile maintaining stable memory requirements. By contrast, SoftPrompt grows\nexponentially in memory with sequence length, requiring around 110 GB compared\nto 40 GB VRAM when training on ECG-QA with LLaMA-3B. Expert reviews by\nclinicians find strong reasoning capabilities exhibited by OpenTSLMs on ECG-QA.\nTo facilitate further research, we provide all code, datasets, and models\nopen-source.",
            "upvotes": 4,
            "discussionId": "68e433d3e4e093a7044e4c5c",
            "ai_summary": "OpenTSLM integrates time series into pretrained LLMs using soft prompting and cross-attention, outperforming text-only models on clinical reasoning tasks.",
            "ai_keywords": [
                "Time Series Language Models",
                "TSLMs",
                "soft prompting",
                "cross-attention",
                "Chain-of-Thought",
                "HAR-CoT",
                "Sleep-CoT",
                "ECG-QA-CoT",
                "F1 score",
                "GPT-4o",
                "LLaMA-3B"
            ],
            "organization": {
                "_id": "68c554094ba48429e58d1e91",
                "name": "OpenTSLM",
                "fullname": "OpenTSLM - Open Source Time Series Language Models",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68014c663a3230f390598c08/R_fMNI8kbq3Vmu6eA0mLY.jpeg"
            }
        },
        "publishedAt": "2025-10-02T05:58:23.000Z",
        "title": "OpenTSLM: Time-Series Language Models for Reasoning over Multivariate\n  Medical Text- and Time-Series Data",
        "summary": "LLMs have emerged as powerful tools for interpreting multimodal data. In\nmedicine, they hold particular promise for synthesizing large volumes of\nclinical information into actionable insights and digital health applications.\nYet, a major limitation remains their inability to handle time series. To\novercome this gap, we present OpenTSLM, a family of Time Series Language Models\n(TSLMs) created by integrating time series as a native modality to pretrained\nLLMs, enabling reasoning over multiple time series of any length. We\ninvestigate two architectures for OpenTSLM. The first, OpenTSLM-SoftPrompt,\nmodels time series implicitly by concatenating learnable time series tokens\nwith text tokens via soft prompting. Although parameter-efficient, we\nhypothesize that explicit time series modeling scales better and outperforms\nimplicit approaches. We thus introduce OpenTSLM-Flamingo, which integrates time\nseries with text via cross-attention. We benchmark both variants against\nbaselines that treat time series as text tokens or plots, across a suite of\ntext-time-series Chain-of-Thought (CoT) reasoning tasks. We introduce three\ndatasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT. Across all, OpenTSLM models\noutperform baselines, reaching 69.9 F1 in sleep staging and 65.4 in HAR,\ncompared to 9.05 and 52.2 for finetuned text-only models. Notably, even\n1B-parameter OpenTSLM models surpass GPT-4o (15.47 and 2.95). OpenTSLM-Flamingo\nmatches OpenTSLM-SoftPrompt in performance and outperforms on longer sequences,\nwhile maintaining stable memory requirements. By contrast, SoftPrompt grows\nexponentially in memory with sequence length, requiring around 110 GB compared\nto 40 GB VRAM when training on ECG-QA with LLaMA-3B. Expert reviews by\nclinicians find strong reasoning capabilities exhibited by OpenTSLMs on ECG-QA.\nTo facilitate further research, we provide all code, datasets, and models\nopen-source.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02410.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5df7e9e5da6d0311fd3d53f9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583857746553-5df7e9e5da6d0311fd3d53f9.jpeg",
            "fullname": "Thomas Wolf",
            "name": "thomwolf",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1494
        },
        "organization": {
            "_id": "68c554094ba48429e58d1e91",
            "name": "OpenTSLM",
            "fullname": "OpenTSLM - Open Source Time Series Language Models",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68014c663a3230f390598c08/R_fMNI8kbq3Vmu6eA0mLY.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01698",
            "authors": [
                {
                    "_id": "68e3a83273e20ab5778421a5",
                    "user": {
                        "_id": "637c3504c292c0fd3f37361f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c3504c292c0fd3f37361f/wyTkbYKi8HufRT65LGN0P.jpeg",
                        "isPro": true,
                        "fullname": "seungheon.doh",
                        "user": "seungheondoh",
                        "type": "user"
                    },
                    "name": "Seungheon Doh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-06T12:48:06.326Z",
                    "hidden": false
                },
                {
                    "_id": "68e3a83273e20ab5778421a6",
                    "name": "Keunwoo Choi",
                    "hidden": false
                },
                {
                    "_id": "68e3a83273e20ab5778421a7",
                    "name": "Juhan Nam",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/637c3504c292c0fd3f37361f/hZ1Pxu0w9n79ymp1UW7XI.mp4"
            ],
            "publishedAt": "2025-10-02T06:08:54.000Z",
            "submittedOnDailyAt": "2025-10-06T10:01:37.972Z",
            "title": "TalkPlay-Tools: Conversational Music Recommendation with LLM Tool\n  Calling",
            "submittedOnDailyBy": {
                "_id": "637c3504c292c0fd3f37361f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c3504c292c0fd3f37361f/wyTkbYKi8HufRT65LGN0P.jpeg",
                "isPro": true,
                "fullname": "seungheon.doh",
                "user": "seungheondoh",
                "type": "user"
            },
            "summary": "While the recent developments in large language models (LLMs) have\nsuccessfully enabled generative recommenders with natural language\ninteractions, their recommendation behavior is limited, leaving other simpler\nyet crucial components such as metadata or attribute filtering underutilized in\nthe system. We propose an LLM-based music recommendation system with tool\ncalling to serve as a unified retrieval-reranking pipeline. Our system\npositions an LLM as an end-to-end recommendation system that interprets user\nintent, plans tool invocations, and orchestrates specialized components:\nboolean filters (SQL), sparse retrieval (BM25), dense retrieval (embedding\nsimilarity), and generative retrieval (semantic IDs). Through tool planning,\nthe system predicts which types of tools to use, their execution order, and the\narguments needed to find music matching user preferences, supporting diverse\nmodalities while seamlessly integrating multiple database filtering methods. We\ndemonstrate that this unified tool-calling framework achieves competitive\nperformance across diverse recommendation scenarios by selectively employing\nappropriate retrieval methods based on user queries, envisioning a new paradigm\nfor conversational music recommendation systems.",
            "upvotes": 4,
            "discussionId": "68e3a83273e20ab5778421a8",
            "projectPage": "https://talkpl.ai/p/talkplay_tools/index.html",
            "githubRepo": "https://github.com/talkpl-ai/talkplay-tools",
            "ai_summary": "A unified LLM-based music recommendation system with tool calling integrates various retrieval methods to enhance user intent interpretation and recommendation performance.",
            "ai_keywords": [
                "LLMs",
                "generative recommenders",
                "natural language interactions",
                "boolean filters",
                "sparse retrieval",
                "BM25",
                "dense retrieval",
                "embedding similarity",
                "generative retrieval",
                "semantic IDs",
                "tool planning",
                "retrieval-reranking pipeline",
                "conversational music recommendation systems"
            ],
            "githubStars": 8
        },
        "publishedAt": "2025-10-02T02:08:54.000Z",
        "title": "TalkPlay-Tools: Conversational Music Recommendation with LLM Tool\n  Calling",
        "summary": "While the recent developments in large language models (LLMs) have\nsuccessfully enabled generative recommenders with natural language\ninteractions, their recommendation behavior is limited, leaving other simpler\nyet crucial components such as metadata or attribute filtering underutilized in\nthe system. We propose an LLM-based music recommendation system with tool\ncalling to serve as a unified retrieval-reranking pipeline. Our system\npositions an LLM as an end-to-end recommendation system that interprets user\nintent, plans tool invocations, and orchestrates specialized components:\nboolean filters (SQL), sparse retrieval (BM25), dense retrieval (embedding\nsimilarity), and generative retrieval (semantic IDs). Through tool planning,\nthe system predicts which types of tools to use, their execution order, and the\narguments needed to find music matching user preferences, supporting diverse\nmodalities while seamlessly integrating multiple database filtering methods. We\ndemonstrate that this unified tool-calling framework achieves competitive\nperformance across diverse recommendation scenarios by selectively employing\nappropriate retrieval methods based on user queries, envisioning a new paradigm\nfor conversational music recommendation systems.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/637c3504c292c0fd3f37361f/hZ1Pxu0w9n79ymp1UW7XI.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01698.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637c3504c292c0fd3f37361f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c3504c292c0fd3f37361f/wyTkbYKi8HufRT65LGN0P.jpeg",
            "fullname": "seungheon.doh",
            "name": "seungheondoh",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 40
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.03204",
            "authors": [
                {
                    "_id": "68e323b473e20ab577841fcf",
                    "name": "Imene Kerboua",
                    "hidden": false
                },
                {
                    "_id": "68e323b473e20ab577841fd0",
                    "name": "Sahar Omidi Shayegan",
                    "hidden": false
                },
                {
                    "_id": "68e323b473e20ab577841fd1",
                    "name": "Megh Thakkar",
                    "hidden": false
                },
                {
                    "_id": "68e323b473e20ab577841fd2",
                    "name": "Xing Han Lù",
                    "hidden": false
                },
                {
                    "_id": "68e323b473e20ab577841fd3",
                    "name": "Léo Boisvert",
                    "hidden": false
                },
                {
                    "_id": "68e323b473e20ab577841fd4",
                    "name": "Massimo Caccia",
                    "hidden": false
                },
                {
                    "_id": "68e323b473e20ab577841fd5",
                    "name": "Jérémy Espinas",
                    "hidden": false
                },
                {
                    "_id": "68e323b473e20ab577841fd6",
                    "name": "Alexandre Aussem",
                    "hidden": false
                },
                {
                    "_id": "68e323b473e20ab577841fd7",
                    "name": "Véronique Eglin",
                    "hidden": false
                },
                {
                    "_id": "68e323b473e20ab577841fd8",
                    "name": "Alexandre Lacoste",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-03T17:41:30.000Z",
            "submittedOnDailyAt": "2025-10-06T00:34:46.420Z",
            "title": "FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of\n  Web Agents",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Web agents powered by large language models (LLMs) must process lengthy web\npage observations to complete user goals; these pages often exceed tens of\nthousands of tokens. This saturates context limits and increases computational\ncost processing; moreover, processing full pages exposes agents to security\nrisks such as prompt injection. Existing pruning strategies either discard\nrelevant content or retain irrelevant context, leading to suboptimal action\nprediction. We introduce FocusAgent, a simple yet effective approach that\nleverages a lightweight LLM retriever to extract the most relevant lines from\naccessibility tree (AxTree) observations, guided by task goals. By pruning\nnoisy and irrelevant content, FocusAgent enables efficient reasoning while\nreducing vulnerability to injection attacks. Experiments on WorkArena and\nWebArena benchmarks show that FocusAgent matches the performance of strong\nbaselines, while reducing observation size by over 50%. Furthermore, a variant\nof FocusAgent significantly reduces the success rate of prompt-injection\nattacks, including banner and pop-up attacks, while maintaining task success\nperformance in attack-free settings. Our results highlight that targeted\nLLM-based retrieval is a practical and robust strategy for building web agents\nthat are efficient, effective, and secure.",
            "upvotes": 3,
            "discussionId": "68e323b473e20ab577841fd9",
            "ai_summary": "FocusAgent uses a lightweight LLM retriever to extract relevant content from web page observations, improving efficiency and security in web agents.",
            "ai_keywords": [
                "large language models",
                "LLM",
                "accessibility tree",
                "AxTree",
                "prompt injection",
                "WorkArena",
                "WebArena",
                "targeted LLM-based retrieval"
            ]
        },
        "publishedAt": "2025-10-03T13:41:30.000Z",
        "title": "FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of\n  Web Agents",
        "summary": "Web agents powered by large language models (LLMs) must process lengthy web\npage observations to complete user goals; these pages often exceed tens of\nthousands of tokens. This saturates context limits and increases computational\ncost processing; moreover, processing full pages exposes agents to security\nrisks such as prompt injection. Existing pruning strategies either discard\nrelevant content or retain irrelevant context, leading to suboptimal action\nprediction. We introduce FocusAgent, a simple yet effective approach that\nleverages a lightweight LLM retriever to extract the most relevant lines from\naccessibility tree (AxTree) observations, guided by task goals. By pruning\nnoisy and irrelevant content, FocusAgent enables efficient reasoning while\nreducing vulnerability to injection attacks. Experiments on WorkArena and\nWebArena benchmarks show that FocusAgent matches the performance of strong\nbaselines, while reducing observation size by over 50%. Furthermore, a variant\nof FocusAgent significantly reduces the success rate of prompt-injection\nattacks, including banner and pop-up attacks, while maintaining task success\nperformance in attack-free settings. Our results highlight that targeted\nLLM-based retrieval is a practical and robust strategy for building web agents\nthat are efficient, effective, and secure.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03204.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 119
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01354",
            "authors": [
                {
                    "_id": "68e35b7c73e20ab5778420c8",
                    "user": {
                        "_id": "65daeafd27be2e8c651b46b5",
                        "avatarUrl": "/avatars/fb67d528ee3487698789bdd8b0562707.svg",
                        "isPro": false,
                        "fullname": "Yinuo Liu",
                        "user": "Norrrrrrr",
                        "type": "user"
                    },
                    "name": "Yinuo Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-06T12:48:46.503Z",
                    "hidden": false
                },
                {
                    "_id": "68e35b7c73e20ab5778420c9",
                    "name": "Ruohan Xu",
                    "hidden": false
                },
                {
                    "_id": "68e35b7c73e20ab5778420ca",
                    "name": "Xilong Wang",
                    "hidden": false
                },
                {
                    "_id": "68e35b7c73e20ab5778420cb",
                    "name": "Yuqi Jia",
                    "hidden": false
                },
                {
                    "_id": "68e35b7c73e20ab5778420cc",
                    "name": "Neil Zhenqiang Gong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T18:34:06.000Z",
            "submittedOnDailyAt": "2025-10-06T04:39:23.341Z",
            "title": "WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents",
            "submittedOnDailyBy": {
                "_id": "65daeafd27be2e8c651b46b5",
                "avatarUrl": "/avatars/fb67d528ee3487698789bdd8b0562707.svg",
                "isPro": false,
                "fullname": "Yinuo Liu",
                "user": "Norrrrrrr",
                "type": "user"
            },
            "summary": "Multiple prompt injection attacks have been proposed against web agents. At\nthe same time, various methods have been developed to detect general prompt\ninjection attacks, but none have been systematically evaluated for web agents.\nIn this work, we bridge this gap by presenting the first comprehensive\nbenchmark study on detecting prompt injection attacks targeting web agents. We\nbegin by introducing a fine-grained categorization of such attacks based on the\nthreat model. We then construct datasets containing both malicious and benign\nsamples: malicious text segments generated by different attacks, benign text\nsegments from four categories, malicious images produced by attacks, and benign\nimages from two categories. Next, we systematize both text-based and\nimage-based detection methods. Finally, we evaluate their performance across\nmultiple scenarios. Our key findings show that while some detectors can\nidentify attacks that rely on explicit textual instructions or visible image\nperturbations with moderate to high accuracy, they largely fail against attacks\nthat omit explicit instructions or employ imperceptible perturbations. Our\ndatasets and code are released at:\nhttps://github.com/Norrrrrrr-lyn/WAInjectBench.",
            "upvotes": 3,
            "discussionId": "68e35b7c73e20ab5778420cd",
            "githubRepo": "https://github.com/Norrrrrrr-lyn/WAInjectBench",
            "ai_summary": "A comprehensive benchmark study evaluates the detection of prompt injection attacks against web agents, revealing that current detectors perform well against explicit attacks but struggle with subtle ones.",
            "ai_keywords": [
                "prompt injection attacks",
                "web agents",
                "threat model",
                "malicious text segments",
                "benign text segments",
                "malicious images",
                "benign images",
                "text-based detection",
                "image-based detection"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-10-01T14:34:06.000Z",
        "title": "WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents",
        "summary": "Multiple prompt injection attacks have been proposed against web agents. At\nthe same time, various methods have been developed to detect general prompt\ninjection attacks, but none have been systematically evaluated for web agents.\nIn this work, we bridge this gap by presenting the first comprehensive\nbenchmark study on detecting prompt injection attacks targeting web agents. We\nbegin by introducing a fine-grained categorization of such attacks based on the\nthreat model. We then construct datasets containing both malicious and benign\nsamples: malicious text segments generated by different attacks, benign text\nsegments from four categories, malicious images produced by attacks, and benign\nimages from two categories. Next, we systematize both text-based and\nimage-based detection methods. Finally, we evaluate their performance across\nmultiple scenarios. Our key findings show that while some detectors can\nidentify attacks that rely on explicit textual instructions or visible image\nperturbations with moderate to high accuracy, they largely fail against attacks\nthat omit explicit instructions or employ imperceptible perturbations. Our\ndatasets and code are released at:\nhttps://github.com/Norrrrrrr-lyn/WAInjectBench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01354.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65daeafd27be2e8c651b46b5",
            "avatarUrl": "/avatars/fb67d528ee3487698789bdd8b0562707.svg",
            "fullname": "Yinuo Liu",
            "name": "Norrrrrrr",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25122",
            "authors": [
                {
                    "_id": "68e3765f73e20ab577842114",
                    "name": "Jan Held",
                    "hidden": false
                },
                {
                    "_id": "68e3765f73e20ab577842115",
                    "name": "Renaud Vandeghen",
                    "hidden": false
                },
                {
                    "_id": "68e3765f73e20ab577842116",
                    "name": "Sanghyun Son",
                    "hidden": false
                },
                {
                    "_id": "68e3765f73e20ab577842117",
                    "name": "Daniel Rebain",
                    "hidden": false
                },
                {
                    "_id": "68e3765f73e20ab577842118",
                    "name": "Matheus Gadelha",
                    "hidden": false
                },
                {
                    "_id": "68e3765f73e20ab577842119",
                    "name": "Yi Zhou",
                    "hidden": false
                },
                {
                    "_id": "68e3765f73e20ab57784211a",
                    "name": "Ming C. Lin",
                    "hidden": false
                },
                {
                    "_id": "68e3765f73e20ab57784211b",
                    "name": "Marc Van Droogenbroeck",
                    "hidden": false
                },
                {
                    "_id": "68e3765f73e20ab57784211c",
                    "name": "Andrea Tagliasacchi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:43:46.000Z",
            "submittedOnDailyAt": "2025-10-06T06:29:16.117Z",
            "title": "Triangle Splatting+: Differentiable Rendering with Opaque Triangles",
            "submittedOnDailyBy": {
                "_id": "66efc5e1801ea45d7ab2a77b",
                "avatarUrl": "/avatars/a37cb36ea7a57315683b45051e9ca5fa.svg",
                "isPro": false,
                "fullname": "Renaud Vandeghen",
                "user": "rvandeghen",
                "type": "user"
            },
            "summary": "Reconstructing 3D scenes and synthesizing novel views has seen rapid progress\nin recent years. Neural Radiance Fields demonstrated that continuous volumetric\nradiance fields can achieve high-quality image synthesis, but their long\ntraining and rendering times limit practicality. 3D Gaussian Splatting (3DGS)\naddressed these issues by representing scenes with millions of Gaussians,\nenabling real-time rendering and fast optimization. However, Gaussian\nprimitives are not natively compatible with the mesh-based pipelines used in VR\nheadsets, and real-time graphics applications. Existing solutions attempt to\nconvert Gaussians into meshes through post-processing or two-stage pipelines,\nwhich increases complexity and degrades visual quality. In this work, we\nintroduce Triangle Splatting+, which directly optimizes triangles, the\nfundamental primitive of computer graphics, within a differentiable splatting\nframework. We formulate triangle parametrization to enable connectivity through\nshared vertices, and we design a training strategy that enforces opaque\ntriangles. The final output is immediately usable in standard graphics engines\nwithout post-processing. Experiments on the Mip-NeRF360 and Tanks & Temples\ndatasets show that Triangle Splatting+achieves state-of-the-art performance in\nmesh-based novel view synthesis. Our method surpasses prior splatting\napproaches in visual fidelity while remaining efficient and fast to training.\nMoreover, the resulting semi-connected meshes support downstream applications\nsuch as physics-based simulation or interactive walkthroughs. The project page\nis https://trianglesplatting2.github.io/trianglesplatting2/.",
            "upvotes": 3,
            "discussionId": "68e3765f73e20ab57784211d",
            "ai_summary": "Triangle Splatting+ optimizes triangles within a differentiable framework for real-time, high-fidelity 3D scene reconstruction and novel view synthesis, compatible with standard graphics engines.",
            "ai_keywords": [
                "Neural Radiance Fields",
                "3D Gaussian Splatting",
                "differentiable splatting",
                "triangle parametrization",
                "opaque triangles",
                "mesh-based novel view synthesis",
                "Mip-NeRF360",
                "Tanks & Temples",
                "semi-connected meshes",
                "physics-based simulation",
                "interactive walkthroughs"
            ]
        },
        "publishedAt": "2025-09-29T13:43:46.000Z",
        "title": "Triangle Splatting+: Differentiable Rendering with Opaque Triangles",
        "summary": "Reconstructing 3D scenes and synthesizing novel views has seen rapid progress\nin recent years. Neural Radiance Fields demonstrated that continuous volumetric\nradiance fields can achieve high-quality image synthesis, but their long\ntraining and rendering times limit practicality. 3D Gaussian Splatting (3DGS)\naddressed these issues by representing scenes with millions of Gaussians,\nenabling real-time rendering and fast optimization. However, Gaussian\nprimitives are not natively compatible with the mesh-based pipelines used in VR\nheadsets, and real-time graphics applications. Existing solutions attempt to\nconvert Gaussians into meshes through post-processing or two-stage pipelines,\nwhich increases complexity and degrades visual quality. In this work, we\nintroduce Triangle Splatting+, which directly optimizes triangles, the\nfundamental primitive of computer graphics, within a differentiable splatting\nframework. We formulate triangle parametrization to enable connectivity through\nshared vertices, and we design a training strategy that enforces opaque\ntriangles. The final output is immediately usable in standard graphics engines\nwithout post-processing. Experiments on the Mip-NeRF360 and Tanks & Temples\ndatasets show that Triangle Splatting+achieves state-of-the-art performance in\nmesh-based novel view synthesis. Our method surpasses prior splatting\napproaches in visual fidelity while remaining efficient and fast to training.\nMoreover, the resulting semi-connected meshes support downstream applications\nsuch as physics-based simulation or interactive walkthroughs. The project page\nis https://trianglesplatting2.github.io/trianglesplatting2/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25122.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66efc5e1801ea45d7ab2a77b",
            "avatarUrl": "/avatars/a37cb36ea7a57315683b45051e9ca5fa.svg",
            "fullname": "Renaud Vandeghen",
            "name": "rvandeghen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.03230",
            "authors": [
                {
                    "_id": "68e31e7c73e20ab577841f9f",
                    "name": "Suyuchen Wang",
                    "hidden": false
                },
                {
                    "_id": "68e31e7c73e20ab577841fa0",
                    "name": "Tianyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e31e7c73e20ab577841fa1",
                    "name": "Ahmed Masry",
                    "hidden": false
                },
                {
                    "_id": "68e31e7c73e20ab577841fa2",
                    "name": "Christopher Pal",
                    "hidden": false
                },
                {
                    "_id": "68e31e7c73e20ab577841fa3",
                    "name": "Spandana Gella",
                    "hidden": false
                },
                {
                    "_id": "68e31e7c73e20ab577841fa4",
                    "name": "Bang Liu",
                    "hidden": false
                },
                {
                    "_id": "68e31e7c73e20ab577841fa5",
                    "name": "Perouz Taslakian",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-03T17:59:34.000Z",
            "submittedOnDailyAt": "2025-10-06T00:12:29.606Z",
            "title": "Improving GUI Grounding with Explicit Position-to-Coordinate Mapping",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "GUI grounding, the task of mapping natural-language instructions to pixel\ncoordinates, is crucial for autonomous agents, yet remains difficult for\ncurrent VLMs. The core bottleneck is reliable patch-to-pixel mapping, which\nbreaks when extrapolating to high-resolution displays unseen during training.\nCurrent approaches generate coordinates as text tokens directly from visual\nfeatures, forcing the model to infer complex position-to-pixel mappings\nimplicitly; as a result, accuracy degrades and failures proliferate on new\nresolutions. We address this with two complementary innovations. First, RULER\ntokens serve as explicit coordinate markers, letting the model reference\npositions similar to gridlines on a map and adjust rather than generate\ncoordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial\nencoding by ensuring that width and height dimensions are represented equally,\naddressing the asymmetry of standard positional schemes. Experiments on\nScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in\ngrounding accuracy, with the largest improvements on high-resolution\ninterfaces. By providing explicit spatial guidance rather than relying on\nimplicit learning, our approach enables more reliable GUI automation across\ndiverse resolutions and platforms.",
            "upvotes": 2,
            "discussionId": "68e31e7c73e20ab577841fa6",
            "ai_summary": "Explicit coordinate markers and improved spatial encoding enhance GUI grounding accuracy across diverse resolutions and platforms.",
            "ai_keywords": [
                "RULER tokens",
                "Interleaved MRoPE",
                "I-MRoPE",
                "spatial encoding",
                "GUI grounding",
                "ScreenSpot",
                "ScreenSpot-V2",
                "ScreenSpot-Pro"
            ]
        },
        "publishedAt": "2025-10-03T13:59:34.000Z",
        "title": "Improving GUI Grounding with Explicit Position-to-Coordinate Mapping",
        "summary": "GUI grounding, the task of mapping natural-language instructions to pixel\ncoordinates, is crucial for autonomous agents, yet remains difficult for\ncurrent VLMs. The core bottleneck is reliable patch-to-pixel mapping, which\nbreaks when extrapolating to high-resolution displays unseen during training.\nCurrent approaches generate coordinates as text tokens directly from visual\nfeatures, forcing the model to infer complex position-to-pixel mappings\nimplicitly; as a result, accuracy degrades and failures proliferate on new\nresolutions. We address this with two complementary innovations. First, RULER\ntokens serve as explicit coordinate markers, letting the model reference\npositions similar to gridlines on a map and adjust rather than generate\ncoordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial\nencoding by ensuring that width and height dimensions are represented equally,\naddressing the asymmetry of standard positional schemes. Experiments on\nScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in\ngrounding accuracy, with the largest improvements on high-resolution\ninterfaces. By providing explicit spatial guidance rather than relying on\nimplicit learning, our approach enables more reliable GUI automation across\ndiverse resolutions and platforms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03230.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 119
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.02880",
            "authors": [
                {
                    "_id": "68e3752773e20ab5778420f1",
                    "user": {
                        "_id": "65ab7ed7d6b10af911dc8085",
                        "avatarUrl": "/avatars/0b033760f3a785ac82a1e913b28e9dec.svg",
                        "isPro": false,
                        "fullname": "Tianren Ma",
                        "user": "Vivre",
                        "type": "user"
                    },
                    "name": "Tianren Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-06T12:48:37.179Z",
                    "hidden": false
                },
                {
                    "_id": "68e3752773e20ab5778420f2",
                    "name": "Mu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e3752773e20ab5778420f3",
                    "name": "Yibing Wang",
                    "hidden": false
                },
                {
                    "_id": "68e3752773e20ab5778420f4",
                    "name": "Qixiang Ye",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-03T10:36:24.000Z",
            "submittedOnDailyAt": "2025-10-06T11:56:22.292Z",
            "title": "Consolidating Reinforcement Learning for Multimodal Discrete Diffusion\n  Models",
            "submittedOnDailyBy": {
                "_id": "65ab7ed7d6b10af911dc8085",
                "avatarUrl": "/avatars/0b033760f3a785ac82a1e913b28e9dec.svg",
                "isPro": false,
                "fullname": "Tianren Ma",
                "user": "Vivre",
                "type": "user"
            },
            "summary": "Optimizing discrete diffusion model (DDM) with rewards remains a challenge:\nthe non-autoregressive paradigm makes importance sampling intractable and\nrollout complex, puzzling reinforcement learning methods such as Group Relative\nPolicy Optimization (GRPO). In this study, we introduce MaskGRPO, the first\nviable approach to enable scalable multimodal reinforcement learning in\ndiscrete diffusion with effective importance sampling and modality-specific\nadaptations. To this end, we first clarify the theoretical foundation for DDMs,\nwhich facilitates building an importance estimator that captures valuable token\nfluctuation for gradient updates. We then delicately tailored the rollout\nmethod for visual sequences, which yields diverse completions and reliable\noptimization gradients. Upon math reasoning, coding, and visual generation\nbenchmarks, MaskGRPO brings more stable and efficient updates, leading to\nstronger reasoning performance and better generation quality. This study\nestablishes MaskGRPO as a systematic policy optimization approach and the first\npractical way for discretized visual diffusion.",
            "upvotes": 2,
            "discussionId": "68e3752773e20ab5778420f5",
            "projectPage": "https://github.com/martian422/MaskGRPO",
            "githubRepo": "https://github.com/martian422/MaskGRPO",
            "ai_summary": "MaskGRPO addresses challenges in optimizing discrete diffusion models with rewards through effective importance sampling and modality-specific adaptations, improving reasoning and generation quality.",
            "ai_keywords": [
                "discrete diffusion model",
                "DDM",
                "rewards",
                "non-autoregressive paradigm",
                "importance sampling",
                "rollout",
                "Group Relative Policy Optimization",
                "GRPO",
                "MaskGRPO",
                "multimodal reinforcement learning",
                "importance estimator",
                "token fluctuation",
                "gradient updates",
                "visual sequences",
                "diverse completions",
                "optimization gradients",
                "math reasoning",
                "visual generation",
                "reasoning performance",
                "generation quality"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-10-03T06:36:24.000Z",
        "title": "Consolidating Reinforcement Learning for Multimodal Discrete Diffusion\n  Models",
        "summary": "Optimizing discrete diffusion model (DDM) with rewards remains a challenge:\nthe non-autoregressive paradigm makes importance sampling intractable and\nrollout complex, puzzling reinforcement learning methods such as Group Relative\nPolicy Optimization (GRPO). In this study, we introduce MaskGRPO, the first\nviable approach to enable scalable multimodal reinforcement learning in\ndiscrete diffusion with effective importance sampling and modality-specific\nadaptations. To this end, we first clarify the theoretical foundation for DDMs,\nwhich facilitates building an importance estimator that captures valuable token\nfluctuation for gradient updates. We then delicately tailored the rollout\nmethod for visual sequences, which yields diverse completions and reliable\noptimization gradients. Upon math reasoning, coding, and visual generation\nbenchmarks, MaskGRPO brings more stable and efficient updates, leading to\nstronger reasoning performance and better generation quality. This study\nestablishes MaskGRPO as a systematic policy optimization approach and the first\npractical way for discretized visual diffusion.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02880.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65ab7ed7d6b10af911dc8085",
            "avatarUrl": "/avatars/0b033760f3a785ac82a1e913b28e9dec.svg",
            "fullname": "Tianren Ma",
            "name": "Vivre",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.02110",
            "authors": [
                {
                    "_id": "68e3359773e20ab577842018",
                    "name": "Koichi Saito",
                    "hidden": false
                },
                {
                    "_id": "68e3359773e20ab577842019",
                    "name": "Julian Tanke",
                    "hidden": false
                },
                {
                    "_id": "68e3359773e20ab57784201a",
                    "name": "Christian Simon",
                    "hidden": false
                },
                {
                    "_id": "68e3359773e20ab57784201b",
                    "name": "Masato Ishii",
                    "hidden": false
                },
                {
                    "_id": "68e3359773e20ab57784201c",
                    "name": "Kazuki Shimada",
                    "hidden": false
                },
                {
                    "_id": "68e3359773e20ab57784201d",
                    "name": "Zachary Novack",
                    "hidden": false
                },
                {
                    "_id": "68e3359773e20ab57784201e",
                    "name": "Zhi Zhong",
                    "hidden": false
                },
                {
                    "_id": "68e3359773e20ab57784201f",
                    "name": "Akio Hayakawa",
                    "hidden": false
                },
                {
                    "_id": "68e3359773e20ab577842020",
                    "name": "Takashi Shibuya",
                    "hidden": false
                },
                {
                    "_id": "68e3359773e20ab577842021",
                    "name": "Yuki Mitsufuji",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T15:18:00.000Z",
            "submittedOnDailyAt": "2025-10-06T14:16:59.554Z",
            "title": "SoundReactor: Frame-level Online Video-to-Audio Generation",
            "submittedOnDailyBy": {
                "_id": "665f7129b75fa1b1a705b7ce",
                "avatarUrl": "/avatars/f9c8245c0441dc428edcefce8989444e.svg",
                "isPro": true,
                "fullname": "Koichi Saito",
                "user": "koichisaito",
                "type": "user"
            },
            "summary": "Prevailing Video-to-Audio (V2A) generation models operate offline, assuming\nan entire video sequence or chunks of frames are available beforehand. This\ncritically limits their use in interactive applications such as live content\ncreation and emerging generative world models. To address this gap, we\nintroduce the novel task of frame-level online V2A generation, where a model\nautoregressively generates audio from video without access to future video\nframes. Furthermore, we propose SoundReactor, which, to the best of our\nknowledge, is the first simple yet effective framework explicitly tailored for\nthis task. Our design enforces end-to-end causality and targets low per-frame\nlatency with audio-visual synchronization. Our model's backbone is a\ndecoder-only causal transformer over continuous audio latents. For vision\nconditioning, it leverages grid (patch) features extracted from the smallest\nvariant of the DINOv2 vision encoder, which are aggregated into a single token\nper frame to maintain end-to-end causality and efficiency. The model is trained\nthrough a diffusion pre-training followed by consistency fine-tuning to\naccelerate the diffusion head decoding. On a benchmark of diverse gameplay\nvideos from AAA titles, our model successfully generates semantically and\ntemporally aligned, high-quality full-band stereo audio, validated by both\nobjective and human evaluations. Furthermore, our model achieves low per-frame\nwaveform-level latency (26.3ms with the head NFE=1, 31.5ms with NFE=4) on\n30FPS, 480p videos using a single H100. Demo samples are available at\nhttps://koichi-saito-sony.github.io/soundreactor/.",
            "upvotes": 2,
            "discussionId": "68e3359773e20ab577842022",
            "projectPage": "https://koichi-saito-sony.github.io/soundreactor/",
            "ai_summary": "A novel frame-level online Video-to-Audio generation model, SoundReactor, uses a causal transformer and DINOv2 vision encoder to generate high-quality, synchronized audio with low latency from video frames.",
            "ai_keywords": [
                "frame-level online V2A generation",
                "autoregressive generation",
                "causal transformer",
                "continuous audio latents",
                "DINOv2 vision encoder",
                "diffusion pre-training",
                "consistency fine-tuning",
                "audio-visual synchronization",
                "per-frame latency",
                "waveform-level latency"
            ],
            "organization": {
                "_id": "6304f161c2f4f2d4929d52d7",
                "name": "Sony",
                "fullname": "Sony",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/zpgaCDJEHxA8kYNDtMKnv.jpeg"
            }
        },
        "publishedAt": "2025-10-02T11:18:00.000Z",
        "title": "SoundReactor: Frame-level Online Video-to-Audio Generation",
        "summary": "Prevailing Video-to-Audio (V2A) generation models operate offline, assuming\nan entire video sequence or chunks of frames are available beforehand. This\ncritically limits their use in interactive applications such as live content\ncreation and emerging generative world models. To address this gap, we\nintroduce the novel task of frame-level online V2A generation, where a model\nautoregressively generates audio from video without access to future video\nframes. Furthermore, we propose SoundReactor, which, to the best of our\nknowledge, is the first simple yet effective framework explicitly tailored for\nthis task. Our design enforces end-to-end causality and targets low per-frame\nlatency with audio-visual synchronization. Our model's backbone is a\ndecoder-only causal transformer over continuous audio latents. For vision\nconditioning, it leverages grid (patch) features extracted from the smallest\nvariant of the DINOv2 vision encoder, which are aggregated into a single token\nper frame to maintain end-to-end causality and efficiency. The model is trained\nthrough a diffusion pre-training followed by consistency fine-tuning to\naccelerate the diffusion head decoding. On a benchmark of diverse gameplay\nvideos from AAA titles, our model successfully generates semantically and\ntemporally aligned, high-quality full-band stereo audio, validated by both\nobjective and human evaluations. Furthermore, our model achieves low per-frame\nwaveform-level latency (26.3ms with the head NFE=1, 31.5ms with NFE=4) on\n30FPS, 480p videos using a single H100. Demo samples are available at\nhttps://koichi-saito-sony.github.io/soundreactor/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02110.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "665f7129b75fa1b1a705b7ce",
            "avatarUrl": "/avatars/f9c8245c0441dc428edcefce8989444e.svg",
            "fullname": "Koichi Saito",
            "name": "koichisaito",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "6304f161c2f4f2d4929d52d7",
            "name": "Sony",
            "fullname": "Sony",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/zpgaCDJEHxA8kYNDtMKnv.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01459",
            "authors": [
                {
                    "_id": "68e3449a73e20ab57784206e",
                    "name": "Weizhe Chen",
                    "hidden": false
                },
                {
                    "_id": "68e3449a73e20ab57784206f",
                    "name": "Sven Koenig",
                    "hidden": false
                },
                {
                    "_id": "68e3449a73e20ab577842070",
                    "name": "Bistra Dilkina",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T20:57:22.000Z",
            "submittedOnDailyAt": "2025-10-06T03:06:08.194Z",
            "title": "LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "649c99644cc14193293d522b",
                "avatarUrl": "/avatars/e7e0052d310a0b3a1c4b303150d4f914.svg",
                "isPro": false,
                "fullname": "weizhech",
                "user": "weizhech",
                "type": "user"
            },
            "summary": "Since the release of Deepseek-R1, reinforcement learning with verifiable\nrewards (RLVR) has become a central approach for training large language models\n(LLMs) on reasoning tasks. Recent work has largely focused on modifying loss\nfunctions to make RLVR more efficient and effective. In this paper, motivated\nby studies of overthinking in LLMs, we propose Length-aware Sampling for Policy\nOptimization (LSPO), a novel meta-RLVR algorithm that dynamically selects\ntraining data at each step based on the average response length. We evaluate\nLSPO across multiple base models and datasets, demonstrating that it\nconsistently improves learning effectiveness. In addition, we conduct a\ndetailed ablation study to examine alternative ways of incorporating length\nsignals into dynamic sampling, offering further insights and highlighting\npromising directions for future research.",
            "upvotes": 2,
            "discussionId": "68e3449a73e20ab577842071",
            "githubRepo": "https://github.com/laonahongchen/LSPO",
            "ai_summary": "Length-aware Sampling for Policy Optimization (LSPO) is a meta-RLVR algorithm that dynamically selects training data based on response length, improving learning effectiveness in large language models.",
            "ai_keywords": [
                "reinforcement learning with verifiable rewards",
                "RLVR",
                "large language models",
                "LLMs",
                "meta-RLVR",
                "Length-aware Sampling for Policy Optimization",
                "LSPO",
                "dynamic sampling",
                "length signals",
                "ablation study"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "66a403d0dcb5bbc6e98bb7d0",
                "name": "UniversityofSouthernCalifornia",
                "fullname": "University of Southern California",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a403728069e3c30e0d8524/tkYCfeIJfF1FxtYiRZ8bf.png"
            }
        },
        "publishedAt": "2025-10-01T16:57:22.000Z",
        "title": "LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM\n  Reasoning",
        "summary": "Since the release of Deepseek-R1, reinforcement learning with verifiable\nrewards (RLVR) has become a central approach for training large language models\n(LLMs) on reasoning tasks. Recent work has largely focused on modifying loss\nfunctions to make RLVR more efficient and effective. In this paper, motivated\nby studies of overthinking in LLMs, we propose Length-aware Sampling for Policy\nOptimization (LSPO), a novel meta-RLVR algorithm that dynamically selects\ntraining data at each step based on the average response length. We evaluate\nLSPO across multiple base models and datasets, demonstrating that it\nconsistently improves learning effectiveness. In addition, we conduct a\ndetailed ablation study to examine alternative ways of incorporating length\nsignals into dynamic sampling, offering further insights and highlighting\npromising directions for future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01459.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649c99644cc14193293d522b",
            "avatarUrl": "/avatars/e7e0052d310a0b3a1c4b303150d4f914.svg",
            "fullname": "weizhech",
            "name": "weizhech",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "66a403d0dcb5bbc6e98bb7d0",
            "name": "UniversityofSouthernCalifornia",
            "fullname": "University of Southern California",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a403728069e3c30e0d8524/tkYCfeIJfF1FxtYiRZ8bf.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01329",
            "authors": [
                {
                    "_id": "68e3e14773e20ab577842284",
                    "name": "Huangjie Zheng",
                    "hidden": false
                },
                {
                    "_id": "68e3e14773e20ab577842285",
                    "name": "Shansan Gong",
                    "hidden": false
                },
                {
                    "_id": "68e3e14773e20ab577842286",
                    "name": "Ruixiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e3e14773e20ab577842287",
                    "name": "Tianrong Chen",
                    "hidden": false
                },
                {
                    "_id": "68e3e14773e20ab577842288",
                    "name": "Jiatao Gu",
                    "hidden": false
                },
                {
                    "_id": "68e3e14773e20ab577842289",
                    "name": "Mingyuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68e3e14773e20ab57784228a",
                    "name": "Navdeep Jaitly",
                    "hidden": false
                },
                {
                    "_id": "68e3e14773e20ab57784228b",
                    "name": "Yizhe Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T18:00:56.000Z",
            "submittedOnDailyAt": "2025-10-06T14:10:32.542Z",
            "title": "Continuously Augmented Discrete Diffusion model for Categorical\n  Generative Modeling",
            "submittedOnDailyBy": {
                "_id": "62f6dc90d278a8f3e7877dc0",
                "avatarUrl": "/avatars/bce37487397278d5daf6d95f4dc25221.svg",
                "isPro": false,
                "fullname": "Huangjie Zheng",
                "user": "hjzheng",
                "type": "user"
            },
            "summary": "Standard discrete diffusion models treat all unobserved states identically by\nmapping them to an absorbing [MASK] token. This creates an 'information void'\nwhere semantic information that could be inferred from unmasked tokens is lost\nbetween denoising steps. We introduce Continuously Augmented Discrete Diffusion\n(CADD), a framework that augments the discrete state space with a paired\ndiffusion in a continuous latent space. This yields graded, gradually corrupted\nstates in which masked tokens are represented by noisy yet informative latent\nvectors rather than collapsed 'information voids'. At each reverse step, CADD\nmay leverage the continuous latent as a semantic hint to guide discrete\ndenoising. The design is clean and compatible with existing discrete diffusion\ntraining. At sampling time, the strength and choice of estimator for the\ncontinuous latent vector enables a controlled trade-off between mode-coverage\n(generating diverse outputs) and mode-seeking (generating contextually precise\noutputs) behaviors. Empirically, we demonstrate CADD improves generative\nquality over mask-based diffusion across text generation, image synthesis, and\ncode modeling, with consistent gains on both qualitative and quantitative\nmetrics against strong discrete baselines.",
            "upvotes": 2,
            "discussionId": "68e3e14773e20ab57784228c",
            "ai_summary": "Continuously Augmented Discrete Diffusion (CADD) enhances generative quality by integrating a continuous latent space into discrete diffusion models, providing informative latent vectors for masked tokens and improving mode-coverage and mode-seeking behaviors.",
            "ai_keywords": [
                "discrete diffusion models",
                "absorbing token",
                "information void",
                "continuous latent space",
                "graded",
                "gradually corrupted states",
                "discrete denoising",
                "semantic hint",
                "mode-coverage",
                "mode-seeking",
                "text generation",
                "image synthesis",
                "code modeling"
            ],
            "organization": {
                "_id": "628cbd99ef14f971b69948ab",
                "name": "apple",
                "fullname": "Apple",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
            }
        },
        "publishedAt": "2025-10-01T14:00:56.000Z",
        "title": "Continuously Augmented Discrete Diffusion model for Categorical\n  Generative Modeling",
        "summary": "Standard discrete diffusion models treat all unobserved states identically by\nmapping them to an absorbing [MASK] token. This creates an 'information void'\nwhere semantic information that could be inferred from unmasked tokens is lost\nbetween denoising steps. We introduce Continuously Augmented Discrete Diffusion\n(CADD), a framework that augments the discrete state space with a paired\ndiffusion in a continuous latent space. This yields graded, gradually corrupted\nstates in which masked tokens are represented by noisy yet informative latent\nvectors rather than collapsed 'information voids'. At each reverse step, CADD\nmay leverage the continuous latent as a semantic hint to guide discrete\ndenoising. The design is clean and compatible with existing discrete diffusion\ntraining. At sampling time, the strength and choice of estimator for the\ncontinuous latent vector enables a controlled trade-off between mode-coverage\n(generating diverse outputs) and mode-seeking (generating contextually precise\noutputs) behaviors. Empirically, we demonstrate CADD improves generative\nquality over mask-based diffusion across text generation, image synthesis, and\ncode modeling, with consistent gains on both qualitative and quantitative\nmetrics against strong discrete baselines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01329.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f6dc90d278a8f3e7877dc0",
            "avatarUrl": "/avatars/bce37487397278d5daf6d95f4dc25221.svg",
            "fullname": "Huangjie Zheng",
            "name": "hjzheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01132",
            "authors": [
                {
                    "_id": "68e36caf73e20ab5778420dc",
                    "name": "Ruiyi Wang",
                    "hidden": false
                },
                {
                    "_id": "68e36caf73e20ab5778420dd",
                    "name": "Prithviraj Ammanabrolu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T17:23:04.000Z",
            "submittedOnDailyAt": "2025-10-06T05:47:23.219Z",
            "title": "A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "631fec6e5ba8c026340fdda0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Dn8NzKCYNkBiiinFEyQz5.png",
                "isPro": false,
                "fullname": "Ruiyi Wang",
                "user": "Pamela153",
                "type": "user"
            },
            "summary": "We study what actually works and what doesn't for training large language\nmodels as agents via multi-turn reinforcement learning. Despite rapid progress,\nexisting frameworks and definitions are fragmented, and there is no systematic\nformulation or analysis of which design choices matter across tasks. We address\nthis gap by first breaking down the design space into three inter-related\npillars -- environment, reward, and policy -- and empirically derive a recipe\nfor training LLM agents in situated textual domains. In particular, we test\nTextWorld and ALFWorld, popular domains for testing situated embodied\nreasoning, as well as SWE-Gym for more software engineering style tasks. (i)\nFor the environment, we analyze the impacts of task complexity in terms of\nsizes of the state and action spaces as well as optimal solution length,\nfinding that even simple environments within a domain can provide signal on how\nwell an agent can generalize to more complex tasks. (ii) For the reward, we\nablate relative reward sparsity, observing that while dense turn-level rewards\naccelerate training, performance and stability is highly dependent on the\nchoice of RL algorithm. (iii) And for the agent's policy, we explore the\ninterplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO)\npolicy gradient methods in addition to showing how to find the optimal\nSupervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We\ndistill these findings into a training recipe that guides co-design across the\nthree pillars, facilitating research and practical efforts in multi-turn\nagentic RL. Code: https://github.com/pearls-lab/meow-tea-taro",
            "upvotes": 2,
            "discussionId": "68e36caf73e20ab5778420de",
            "ai_summary": "Research identifies key design choices for training large language models as agents via multi-turn reinforcement learning, focusing on environment complexity, reward sparsity, and policy methods.",
            "ai_keywords": [
                "multi-turn reinforcement learning",
                "large language models",
                "TextWorld",
                "ALFWorld",
                "SWE-Gym",
                "task complexity",
                "state space",
                "action space",
                "optimal solution length",
                "reward sparsity",
                "dense turn-level rewards",
                "RL algorithms",
                "PPO",
                "GRPO",
                "RLOO",
                "policy gradient methods",
                "Supervised Fine-tuning",
                "SFT",
                "RL training ratio"
            ],
            "organization": {
                "_id": "66b51d6d778c98d29426ed8e",
                "name": "PEARLS-Lab",
                "fullname": "PEARLS Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632d2f986bcb864974d68cb1/oFKNoU8q9eIvtNHFEB7IN.png"
            }
        },
        "publishedAt": "2025-10-01T13:23:04.000Z",
        "title": "A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning",
        "summary": "We study what actually works and what doesn't for training large language\nmodels as agents via multi-turn reinforcement learning. Despite rapid progress,\nexisting frameworks and definitions are fragmented, and there is no systematic\nformulation or analysis of which design choices matter across tasks. We address\nthis gap by first breaking down the design space into three inter-related\npillars -- environment, reward, and policy -- and empirically derive a recipe\nfor training LLM agents in situated textual domains. In particular, we test\nTextWorld and ALFWorld, popular domains for testing situated embodied\nreasoning, as well as SWE-Gym for more software engineering style tasks. (i)\nFor the environment, we analyze the impacts of task complexity in terms of\nsizes of the state and action spaces as well as optimal solution length,\nfinding that even simple environments within a domain can provide signal on how\nwell an agent can generalize to more complex tasks. (ii) For the reward, we\nablate relative reward sparsity, observing that while dense turn-level rewards\naccelerate training, performance and stability is highly dependent on the\nchoice of RL algorithm. (iii) And for the agent's policy, we explore the\ninterplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO)\npolicy gradient methods in addition to showing how to find the optimal\nSupervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We\ndistill these findings into a training recipe that guides co-design across the\nthree pillars, facilitating research and practical efforts in multi-turn\nagentic RL. Code: https://github.com/pearls-lab/meow-tea-taro",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01132.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631fec6e5ba8c026340fdda0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Dn8NzKCYNkBiiinFEyQz5.png",
            "fullname": "Ruiyi Wang",
            "name": "Pamela153",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "66b51d6d778c98d29426ed8e",
            "name": "PEARLS-Lab",
            "fullname": "PEARLS Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632d2f986bcb864974d68cb1/oFKNoU8q9eIvtNHFEB7IN.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.00177",
            "authors": [
                {
                    "_id": "68e3a7d873e20ab57784219c",
                    "user": {
                        "_id": "655c3ece953cf1ef5e2b5abd",
                        "avatarUrl": "/avatars/406ee355a78171dba86d7dca3ee56699.svg",
                        "isPro": true,
                        "fullname": "Stella Li",
                        "user": "stellalisy",
                        "type": "user"
                    },
                    "name": "Shuyue Stella Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-06T12:48:08.711Z",
                    "hidden": false
                },
                {
                    "_id": "68e3a7d873e20ab57784219d",
                    "name": "Avinandan Bose",
                    "hidden": false
                },
                {
                    "_id": "68e3a7d873e20ab57784219e",
                    "name": "Faeze Brahman",
                    "hidden": false
                },
                {
                    "_id": "68e3a7d873e20ab57784219f",
                    "name": "Simon Shaolei Du",
                    "hidden": false
                },
                {
                    "_id": "68e3a7d873e20ab5778421a0",
                    "name": "Pang Wei Koh",
                    "hidden": false
                },
                {
                    "_id": "68e3a7d873e20ab5778421a1",
                    "name": "Maryam Fazel",
                    "hidden": false
                },
                {
                    "_id": "68e3a7d873e20ab5778421a2",
                    "name": "Yulia Tsvetkov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T18:55:28.000Z",
            "submittedOnDailyAt": "2025-10-06T14:38:12.884Z",
            "title": "Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail\n  At It",
            "submittedOnDailyBy": {
                "_id": "655c3ece953cf1ef5e2b5abd",
                "avatarUrl": "/avatars/406ee355a78171dba86d7dca3ee56699.svg",
                "isPro": true,
                "fullname": "Stella Li",
                "user": "stellalisy",
                "type": "user"
            },
            "summary": "Current large language model (LLM) development treats task-solving and\npreference alignment as separate challenges, optimizing first for objective\ncorrectness, then for alignment to aggregated human preferences. This paradigm\nfails in human-facing applications where solving a problem correctly is\ninsufficient if the response mismatches the user's needs. This challenge\nintensifies in just-in-time scenarios where no prior user interaction history\nexists due to cold-start conditions or privacy constraints. LLMs need to\nidentify what they don't know about user preferences, strategically elicit\npreference values through questioning, then adapt their reasoning processes and\nresponses accordingly -- a complicated chain of cognitive processes which we\nterm personalized reasoning. We introduce PREFDISCO, an evaluation methodology\nthat transforms static benchmarks into interactive personalization tasks using\npsychologically-grounded personas with sparse preferences. Our framework\ncreates scenarios where identical questions require different reasoning chains\ndepending on user context, as optimal explanation approaches vary by individual\nexpertise and preferences while maintaining factual accuracy. Evaluation of 21\nfrontier models across 10 tasks reveals 29.0% of naive personalization attempts\nproduce worse preference alignment than generic responses, yet generic\nresponses also fail to serve individual user needs effectively. These findings\nsuggest personalized reasoning requires dedicated development rather than\nemerging naturally. PREFDISCO establishes personalized reasoning as a\nmeasurable research frontier and reveals fundamental limitations in current\nLLMs' interactive capabilities, providing a foundation for developing systems\nthat can adapt to individual users in education, healthcare, and technical\ndomains where personalization is critical.",
            "upvotes": 2,
            "discussionId": "68e3a7d973e20ab5778421a3",
            "ai_summary": "PREFDISCO evaluates large language models' personalized reasoning capabilities by transforming static benchmarks into interactive tasks with sparse user preferences, revealing significant limitations in current models' ability to adapt to individual needs.",
            "ai_keywords": [
                "large language model",
                "personalized reasoning",
                "PREFDISCO",
                "psychologically-grounded personas",
                "sparse preferences",
                "interactive personalization tasks",
                "reasoning chains",
                "user context",
                "preference alignment",
                "generic responses"
            ],
            "organization": {
                "_id": "646d31f04e59b82995e17a8a",
                "name": "uwnlp",
                "fullname": "University of Washington NLP",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8f52d89ea472e600a0a10/r5imF4kxbsc1l2F1lqz2V.png"
            }
        },
        "publishedAt": "2025-09-30T14:55:28.000Z",
        "title": "Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail\n  At It",
        "summary": "Current large language model (LLM) development treats task-solving and\npreference alignment as separate challenges, optimizing first for objective\ncorrectness, then for alignment to aggregated human preferences. This paradigm\nfails in human-facing applications where solving a problem correctly is\ninsufficient if the response mismatches the user's needs. This challenge\nintensifies in just-in-time scenarios where no prior user interaction history\nexists due to cold-start conditions or privacy constraints. LLMs need to\nidentify what they don't know about user preferences, strategically elicit\npreference values through questioning, then adapt their reasoning processes and\nresponses accordingly -- a complicated chain of cognitive processes which we\nterm personalized reasoning. We introduce PREFDISCO, an evaluation methodology\nthat transforms static benchmarks into interactive personalization tasks using\npsychologically-grounded personas with sparse preferences. Our framework\ncreates scenarios where identical questions require different reasoning chains\ndepending on user context, as optimal explanation approaches vary by individual\nexpertise and preferences while maintaining factual accuracy. Evaluation of 21\nfrontier models across 10 tasks reveals 29.0% of naive personalization attempts\nproduce worse preference alignment than generic responses, yet generic\nresponses also fail to serve individual user needs effectively. These findings\nsuggest personalized reasoning requires dedicated development rather than\nemerging naturally. PREFDISCO establishes personalized reasoning as a\nmeasurable research frontier and reveals fundamental limitations in current\nLLMs' interactive capabilities, providing a foundation for developing systems\nthat can adapt to individual users in education, healthcare, and technical\ndomains where personalization is critical.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00177.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655c3ece953cf1ef5e2b5abd",
            "avatarUrl": "/avatars/406ee355a78171dba86d7dca3ee56699.svg",
            "fullname": "Stella Li",
            "name": "stellalisy",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "646d31f04e59b82995e17a8a",
            "name": "uwnlp",
            "fullname": "University of Washington NLP",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8f52d89ea472e600a0a10/r5imF4kxbsc1l2F1lqz2V.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25771",
            "authors": [
                {
                    "_id": "68e336df73e20ab577842024",
                    "name": "Jia Jun Cheng Xian",
                    "hidden": false
                },
                {
                    "_id": "68e336df73e20ab577842025",
                    "name": "Muchen Li",
                    "hidden": false
                },
                {
                    "_id": "68e336df73e20ab577842026",
                    "name": "Haotian Yang",
                    "hidden": false
                },
                {
                    "_id": "68e336df73e20ab577842027",
                    "name": "Xin Tao",
                    "hidden": false
                },
                {
                    "_id": "68e336df73e20ab577842028",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "68e336df73e20ab577842029",
                    "name": "Leonid Sigal",
                    "hidden": false
                },
                {
                    "_id": "68e336df73e20ab57784202a",
                    "name": "Renjie Liao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T04:32:34.000Z",
            "submittedOnDailyAt": "2025-10-06T01:59:44.420Z",
            "title": "Free Lunch Alignment of Text-to-Image Diffusion Models without\n  Preference Image Pairs",
            "submittedOnDailyBy": {
                "_id": "62be41bacf4665894c7d6950",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be41bacf4665894c7d6950/XdQy0NZpNxkvLt816CXKh.jpeg",
                "isPro": false,
                "fullname": "Muchen Li",
                "user": "jojo23333",
                "type": "user"
            },
            "summary": "Recent advances in diffusion-based text-to-image (T2I) models have led to\nremarkable success in generating high-quality images from textual prompts.\nHowever, ensuring accurate alignment between the text and the generated image\nremains a significant challenge for state-of-the-art diffusion models. To\naddress this, existing studies employ reinforcement learning with human\nfeedback (RLHF) to align T2I outputs with human preferences. These methods,\nhowever, either rely directly on paired image preference data or require a\nlearned reward function, both of which depend heavily on costly, high-quality\nhuman annotations and thus face scalability limitations. In this work, we\nintroduce Text Preference Optimization (TPO), a framework that enables\n\"free-lunch\" alignment of T2I models, achieving alignment without the need for\npaired image preference data. TPO works by training the model to prefer matched\nprompts over mismatched prompts, which are constructed by perturbing original\ncaptions using a large language model. Our framework is general and compatible\nwith existing preference-based algorithms. We extend both DPO and KTO to our\nsetting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations\nacross multiple benchmarks show that our methods consistently outperform their\noriginal counterparts, delivering better human preference scores and improved\ntext-to-image alignment. Our Open-source code is available at\nhttps://github.com/DSL-Lab/T2I-Free-Lunch-Alignment.",
            "upvotes": 2,
            "discussionId": "68e336e073e20ab57784202b",
            "ai_summary": "A new framework, Text Preference Optimization (TPO), aligns text-to-image models with human preferences without requiring paired image preference data, improving text-to-image alignment and human preference scores.",
            "ai_keywords": [
                "diffusion-based text-to-image",
                "T2I models",
                "reinforcement learning with human feedback",
                "RLHF",
                "Text Preference Optimization",
                "TPO",
                "matched prompts",
                "mismatched prompts",
                "large language model",
                "DPO",
                "KTO",
                "TDPO",
                "TKTO"
            ],
            "organization": {
                "_id": "67481e994bcfe97d0e7f242d",
                "name": "UBC-V",
                "fullname": "University of British Columbia",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66c75a8a060b6c3335d28ee7/5qUvu4_6J60ULUQaMYZ2C.png"
            }
        },
        "publishedAt": "2025-09-30T00:32:34.000Z",
        "title": "Free Lunch Alignment of Text-to-Image Diffusion Models without\n  Preference Image Pairs",
        "summary": "Recent advances in diffusion-based text-to-image (T2I) models have led to\nremarkable success in generating high-quality images from textual prompts.\nHowever, ensuring accurate alignment between the text and the generated image\nremains a significant challenge for state-of-the-art diffusion models. To\naddress this, existing studies employ reinforcement learning with human\nfeedback (RLHF) to align T2I outputs with human preferences. These methods,\nhowever, either rely directly on paired image preference data or require a\nlearned reward function, both of which depend heavily on costly, high-quality\nhuman annotations and thus face scalability limitations. In this work, we\nintroduce Text Preference Optimization (TPO), a framework that enables\n\"free-lunch\" alignment of T2I models, achieving alignment without the need for\npaired image preference data. TPO works by training the model to prefer matched\nprompts over mismatched prompts, which are constructed by perturbing original\ncaptions using a large language model. Our framework is general and compatible\nwith existing preference-based algorithms. We extend both DPO and KTO to our\nsetting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations\nacross multiple benchmarks show that our methods consistently outperform their\noriginal counterparts, delivering better human preference scores and improved\ntext-to-image alignment. Our Open-source code is available at\nhttps://github.com/DSL-Lab/T2I-Free-Lunch-Alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25771.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62be41bacf4665894c7d6950",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be41bacf4665894c7d6950/XdQy0NZpNxkvLt816CXKh.jpeg",
            "fullname": "Muchen Li",
            "name": "jojo23333",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "67481e994bcfe97d0e7f242d",
            "name": "UBC-V",
            "fullname": "University of British Columbia",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66c75a8a060b6c3335d28ee7/5qUvu4_6J60ULUQaMYZ2C.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.03232",
            "authors": [
                {
                    "_id": "68e385ab73e20ab577842165",
                    "name": "Ci-Siang Lin",
                    "hidden": false
                },
                {
                    "_id": "68e385ab73e20ab577842166",
                    "user": {
                        "_id": "64ae22dd1aee69ece065cdcd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                        "isPro": false,
                        "fullname": "Min-Hung Chen",
                        "user": "cmhungsteve",
                        "type": "user"
                    },
                    "name": "Min-Hung Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-06T12:48:24.797Z",
                    "hidden": false
                },
                {
                    "_id": "68e385ab73e20ab577842167",
                    "name": "Yu-Yang Sheng",
                    "hidden": false
                },
                {
                    "_id": "68e385ab73e20ab577842168",
                    "name": "Yu-Chiang Frank Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-03T17:59:56.000Z",
            "submittedOnDailyAt": "2025-10-06T07:38:43.471Z",
            "title": "LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks\n  for Multimodal Large Language Models",
            "submittedOnDailyBy": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) have achieved strong performance on\ngeneral visual benchmarks but struggle with out-of-distribution (OOD) tasks in\nspecialized domains such as medical imaging, where labeled data is limited and\nexpensive. We introduce LEAML, a label-efficient adaptation framework that\nleverages both scarce labeled VQA samples and abundant unlabeled images. Our\napproach generates domain-relevant pseudo question-answer pairs for unlabeled\ndata using a QA generator regularized by caption distillation. Importantly, we\nselectively update only those neurons most relevant to question-answering,\nenabling the QA Generator to efficiently acquire domain-specific knowledge\nduring distillation. Experiments on gastrointestinal endoscopy and sports VQA\ndemonstrate that LEAML consistently outperforms standard fine-tuning under\nminimal supervision, highlighting the effectiveness of our proposed LEAML\nframework.",
            "upvotes": 1,
            "discussionId": "68e385ab73e20ab577842169",
            "ai_summary": "LEAML, a label-efficient adaptation framework, enhances MLLMs for specialized domains by generating pseudo question-answer pairs and selectively updating relevant neurons, outperforming standard fine-tuning with minimal supervision.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "MLLMs",
                "out-of-distribution",
                "OOD",
                "medical imaging",
                "labeled data",
                "LEAML",
                "label-efficient adaptation",
                "QA generator",
                "caption distillation",
                "domain-relevant",
                "pseudo question-answer pairs",
                "unlabeled images",
                "selective update",
                "neurons",
                "question-answering",
                "domain-specific knowledge",
                "gastrointestinal endoscopy",
                "sports VQA",
                "standard fine-tuning"
            ]
        },
        "publishedAt": "2025-10-03T13:59:56.000Z",
        "title": "LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks\n  for Multimodal Large Language Models",
        "summary": "Multimodal Large Language Models (MLLMs) have achieved strong performance on\ngeneral visual benchmarks but struggle with out-of-distribution (OOD) tasks in\nspecialized domains such as medical imaging, where labeled data is limited and\nexpensive. We introduce LEAML, a label-efficient adaptation framework that\nleverages both scarce labeled VQA samples and abundant unlabeled images. Our\napproach generates domain-relevant pseudo question-answer pairs for unlabeled\ndata using a QA generator regularized by caption distillation. Importantly, we\nselectively update only those neurons most relevant to question-answering,\nenabling the QA Generator to efficiently acquire domain-specific knowledge\nduring distillation. Experiments on gastrointestinal endoscopy and sports VQA\ndemonstrate that LEAML consistently outperforms standard fine-tuning under\nminimal supervision, highlighting the effectiveness of our proposed LEAML\nframework.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03232.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ae22dd1aee69ece065cdcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
            "fullname": "Min-Hung Chen",
            "name": "cmhungsteve",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.03160",
            "authors": [
                {
                    "_id": "68e3295d73e20ab577841fde",
                    "name": "Ming Zhao",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841fdf",
                    "name": "Wenhui Dong",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841fe0",
                    "name": "Yang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841fe1",
                    "name": "Xiang Zheng",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841fe2",
                    "name": "Zhonghao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841fe3",
                    "name": "Zian Zhou",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841fe4",
                    "name": "Yunzhi Guan",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841fe5",
                    "name": "Liukun Xu",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841fe6",
                    "name": "Wei Peng",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841fe7",
                    "name": "Zhaoyang Gong",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841fe8",
                    "name": "Zhicheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841fe9",
                    "name": "Dachuan Li",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841fea",
                    "name": "Xiaosheng Ma",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841feb",
                    "name": "Yuli Ma",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841fec",
                    "name": "Jianing Ni",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841fed",
                    "name": "Changjiang Jiang",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841fee",
                    "name": "Lixia Tian",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841fef",
                    "name": "Qixin Chen",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841ff0",
                    "name": "Kaishun Xia",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841ff1",
                    "name": "Pingping Liu",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841ff2",
                    "name": "Tongshun Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841ff3",
                    "name": "Zhiqiang Liu",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841ff4",
                    "name": "Zhongan Bi",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841ff5",
                    "name": "Chenyang Si",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841ff6",
                    "name": "Tiansheng Sun",
                    "hidden": false
                },
                {
                    "_id": "68e3295d73e20ab577841ff7",
                    "name": "Caifeng Shan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-03T16:32:02.000Z",
            "submittedOnDailyAt": "2025-10-06T00:58:52.442Z",
            "title": "SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the\n  SpineMed-450k Corpus",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Spine disorders affect 619 million people globally and are a leading cause of\ndisability, yet AI-assisted diagnosis remains limited by the lack of\nlevel-aware, multimodal datasets. Clinical decision-making for spine disorders\nrequires sophisticated reasoning across X-ray, CT, and MRI at specific\nvertebral levels. However, progress has been constrained by the absence of\ntraceable, clinically-grounded instruction data and standardized,\nspine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem\nco-designed with practicing spine surgeons. It features SpineMed-450k, the\nfirst large-scale dataset explicitly designed for vertebral-level reasoning\nacross imaging modalities with over 450,000 instruction instances, and\nSpineBench, a clinically-grounded evaluation framework. SpineMed-450k is\ncurated from diverse sources, including textbooks, guidelines, open datasets,\nand ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline\nwith a two-stage LLM generation method (draft and revision) to ensure\nhigh-quality, traceable data for question-answering, multi-turn consultations,\nand report generation. SpineBench evaluates models on clinically salient axes,\nincluding level identification, pathology assessment, and surgical planning.\nOur comprehensive evaluation of several recently advanced large vision-language\nmodels (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,\nlevel-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k\ndemonstrates consistent and significant improvements across all tasks.\nClinician assessments confirm the diagnostic clarity and practical utility of\nour model's outputs.",
            "upvotes": 1,
            "discussionId": "68e3295d73e20ab577841ff8",
            "ai_summary": "SpineMed, an ecosystem with SpineMed-450k and SpineBench, addresses the lack of level-aware, multimodal datasets and benchmarks for AI-assisted diagnosis of spine disorders, improving model performance through fine-grained, level-specific reasoning.",
            "ai_keywords": [
                "vertebral-level reasoning",
                "X-ray",
                "CT",
                "MRI",
                "instruction data",
                "spine-specific benchmarks",
                "SpineMed",
                "SpineMed-450k",
                "SpineBench",
                "clinician-in-the-loop pipeline",
                "LLM generation method",
                "large vision-language models (LVLMs)",
                "fine-grained",
                "level-specific reasoning"
            ]
        },
        "publishedAt": "2025-10-03T12:32:02.000Z",
        "title": "SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the\n  SpineMed-450k Corpus",
        "summary": "Spine disorders affect 619 million people globally and are a leading cause of\ndisability, yet AI-assisted diagnosis remains limited by the lack of\nlevel-aware, multimodal datasets. Clinical decision-making for spine disorders\nrequires sophisticated reasoning across X-ray, CT, and MRI at specific\nvertebral levels. However, progress has been constrained by the absence of\ntraceable, clinically-grounded instruction data and standardized,\nspine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem\nco-designed with practicing spine surgeons. It features SpineMed-450k, the\nfirst large-scale dataset explicitly designed for vertebral-level reasoning\nacross imaging modalities with over 450,000 instruction instances, and\nSpineBench, a clinically-grounded evaluation framework. SpineMed-450k is\ncurated from diverse sources, including textbooks, guidelines, open datasets,\nand ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline\nwith a two-stage LLM generation method (draft and revision) to ensure\nhigh-quality, traceable data for question-answering, multi-turn consultations,\nand report generation. SpineBench evaluates models on clinically salient axes,\nincluding level identification, pathology assessment, and surgical planning.\nOur comprehensive evaluation of several recently advanced large vision-language\nmodels (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,\nlevel-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k\ndemonstrates consistent and significant improvements across all tasks.\nClinician assessments confirm the diagnostic clarity and practical utility of\nour model's outputs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03160.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 119
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.02730",
            "authors": [
                {
                    "_id": "68e3764673e20ab577842108",
                    "name": "Nishanth Shetty",
                    "hidden": false
                },
                {
                    "_id": "68e3764673e20ab577842109",
                    "name": "Madhava Prasath",
                    "hidden": false
                },
                {
                    "_id": "68e3764673e20ab57784210a",
                    "name": "Chandra Sekhar Seelamantula",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-03T05:23:33.000Z",
            "submittedOnDailyAt": "2025-10-06T14:03:08.777Z",
            "title": "Dale meets Langevin: A Multiplicative Denoising Diffusion Model",
            "submittedOnDailyBy": {
                "_id": "649c0ce8b4ee3b2664590a40",
                "avatarUrl": "/avatars/090c1f57c29af918e100db0844f60892.svg",
                "isPro": false,
                "fullname": "Madhava prasath",
                "user": "Madddy",
                "type": "user"
            },
            "summary": "Gradient descent has proven to be a powerful and effective technique for\noptimization in numerous machine learning applications. Recent advances in\ncomputational neuroscience have shown that learning in standard gradient\ndescent optimization formulation is not consistent with learning in biological\nsystems. This has opened up interesting avenues for building biologically\ninspired learning techniques. One such approach is inspired by Dale's law,\nwhich states that inhibitory and excitatory synapses do not swap roles during\nthe course of learning. The resulting exponential gradient descent optimization\nscheme leads to log-normally distributed synaptic weights. Interestingly, the\ndensity that satisfies the Fokker-Planck equation corresponding to the\nstochastic differential equation (SDE) with geometric Brownian motion (GBM) is\nthe log-normal density. Leveraging this connection, we start with the SDE\ngoverning geometric Brownian motion, and show that discretizing the\ncorresponding reverse-time SDE yields a multiplicative update rule, which\nsurprisingly, coincides with the sampling equivalent of the exponential\ngradient descent update founded on Dale's law. Furthermore, we propose a new\nformalism for multiplicative denoising score-matching, subsuming the loss\nfunction proposed by Hyvaerinen for non-negative data. Indeed, log-normally\ndistributed data is positive and the proposed score-matching formalism turns\nout to be a natural fit. This allows for training of score-based models for\nimage data and results in a novel multiplicative update scheme for sample\ngeneration starting from a log-normal density. Experimental results on MNIST,\nFashion MNIST, and Kuzushiji datasets demonstrate generative capability of the\nnew scheme. To the best of our knowledge, this is the first instance of a\nbiologically inspired generative model employing multiplicative updates,\nfounded on geometric Brownian motion.",
            "upvotes": 1,
            "discussionId": "68e3764673e20ab57784210b",
            "ai_summary": "A biologically inspired generative model using multiplicative updates based on geometric Brownian motion and exponential gradient descent achieves state-of-the-art performance on image datasets.",
            "ai_keywords": [
                "gradient descent",
                "exponential gradient descent",
                "Dale's law",
                "synaptic weights",
                "Fokker-Planck equation",
                "stochastic differential equation",
                "geometric Brownian motion",
                "multiplicative update rule",
                "denoising score-matching",
                "score-based models",
                "log-normal density"
            ],
            "organization": {
                "_id": "62a1b8a3eeb729a1898ce6b4",
                "name": "IISc",
                "fullname": "Indian Institute of Science",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654765671857-62a1b7c26bb8cc68fb4a6cb8.jpeg"
            }
        },
        "publishedAt": "2025-10-03T01:23:33.000Z",
        "title": "Dale meets Langevin: A Multiplicative Denoising Diffusion Model",
        "summary": "Gradient descent has proven to be a powerful and effective technique for\noptimization in numerous machine learning applications. Recent advances in\ncomputational neuroscience have shown that learning in standard gradient\ndescent optimization formulation is not consistent with learning in biological\nsystems. This has opened up interesting avenues for building biologically\ninspired learning techniques. One such approach is inspired by Dale's law,\nwhich states that inhibitory and excitatory synapses do not swap roles during\nthe course of learning. The resulting exponential gradient descent optimization\nscheme leads to log-normally distributed synaptic weights. Interestingly, the\ndensity that satisfies the Fokker-Planck equation corresponding to the\nstochastic differential equation (SDE) with geometric Brownian motion (GBM) is\nthe log-normal density. Leveraging this connection, we start with the SDE\ngoverning geometric Brownian motion, and show that discretizing the\ncorresponding reverse-time SDE yields a multiplicative update rule, which\nsurprisingly, coincides with the sampling equivalent of the exponential\ngradient descent update founded on Dale's law. Furthermore, we propose a new\nformalism for multiplicative denoising score-matching, subsuming the loss\nfunction proposed by Hyvaerinen for non-negative data. Indeed, log-normally\ndistributed data is positive and the proposed score-matching formalism turns\nout to be a natural fit. This allows for training of score-based models for\nimage data and results in a novel multiplicative update scheme for sample\ngeneration starting from a log-normal density. Experimental results on MNIST,\nFashion MNIST, and Kuzushiji datasets demonstrate generative capability of the\nnew scheme. To the best of our knowledge, this is the first instance of a\nbiologically inspired generative model employing multiplicative updates,\nfounded on geometric Brownian motion.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02730.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649c0ce8b4ee3b2664590a40",
            "avatarUrl": "/avatars/090c1f57c29af918e100db0844f60892.svg",
            "fullname": "Madhava prasath",
            "name": "Madddy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "62a1b8a3eeb729a1898ce6b4",
            "name": "IISc",
            "fullname": "Indian Institute of Science",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654765671857-62a1b7c26bb8cc68fb4a6cb8.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.02657",
            "authors": [
                {
                    "_id": "68e41338e4e093a7044e4c2b",
                    "name": "Jingjie Ning",
                    "hidden": false
                },
                {
                    "_id": "68e41338e4e093a7044e4c2c",
                    "name": "Yibo Kong",
                    "hidden": false
                },
                {
                    "_id": "68e41338e4e093a7044e4c2d",
                    "name": "Yunfan Long",
                    "hidden": false
                },
                {
                    "_id": "68e41338e4e093a7044e4c2e",
                    "name": "Jamie Callan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-03T01:26:13.000Z",
            "submittedOnDailyAt": "2025-10-06T17:37:07.810Z",
            "title": "Less LLM, More Documents: Searching for Improved RAG",
            "submittedOnDailyBy": {
                "_id": "6310a812a23f0327bce68778",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a812a23f0327bce68778/dp9JherZV-RYiSRKnvxsh.jpeg",
                "isPro": false,
                "fullname": "Ethan Ning",
                "user": "ethanning",
                "type": "user"
            },
            "summary": "Retrieval-Augmented Generation (RAG) couples document retrieval with large\nlanguage models (LLMs). While scaling generators improves accuracy, it also\nraises cost and limits deployability. We explore an orthogonal axis: enlarging\nthe retriever's corpus to reduce reliance on large LLMs. Experimental results\nshow that corpus scaling consistently strengthens RAG and can often serve as a\nsubstitute for increasing model size, though with diminishing returns at larger\nscales. Small- and mid-sized generators paired with larger corpora often rival\nmuch larger models with smaller corpora; mid-sized models tend to gain the\nmost, while tiny and large models benefit less. Our analysis shows that\nimprovements arise primarily from increased coverage of answer-bearing\npassages, while utilization efficiency remains largely unchanged. These\nfindings establish a principled corpus-generator trade-off: investing in larger\ncorpora offers an effective path to stronger RAG, often comparable to enlarging\nthe LLM itself.",
            "upvotes": 1,
            "discussionId": "68e41339e4e093a7044e4c2f",
            "ai_summary": "Expanding the retriever's corpus in Retrieval-Augmented Generation (RAG) can improve performance and reduce reliance on large language models.",
            "ai_keywords": [
                "Retrieval-Augmented Generation",
                "RAG",
                "document retrieval",
                "large language models",
                "LLMs",
                "corpus scaling",
                "answer-bearing passages"
            ],
            "organization": {
                "_id": "6362c6c180c1a705a6ed0d57",
                "name": "CMU-SCS",
                "fullname": "Carnegie Mellon University School of Computer Science"
            }
        },
        "publishedAt": "2025-10-02T21:26:13.000Z",
        "title": "Less LLM, More Documents: Searching for Improved RAG",
        "summary": "Retrieval-Augmented Generation (RAG) couples document retrieval with large\nlanguage models (LLMs). While scaling generators improves accuracy, it also\nraises cost and limits deployability. We explore an orthogonal axis: enlarging\nthe retriever's corpus to reduce reliance on large LLMs. Experimental results\nshow that corpus scaling consistently strengthens RAG and can often serve as a\nsubstitute for increasing model size, though with diminishing returns at larger\nscales. Small- and mid-sized generators paired with larger corpora often rival\nmuch larger models with smaller corpora; mid-sized models tend to gain the\nmost, while tiny and large models benefit less. Our analysis shows that\nimprovements arise primarily from increased coverage of answer-bearing\npassages, while utilization efficiency remains largely unchanged. These\nfindings establish a principled corpus-generator trade-off: investing in larger\ncorpora offers an effective path to stronger RAG, often comparable to enlarging\nthe LLM itself.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02657.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6310a812a23f0327bce68778",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a812a23f0327bce68778/dp9JherZV-RYiSRKnvxsh.jpeg",
            "fullname": "Ethan Ning",
            "name": "ethanning",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6362c6c180c1a705a6ed0d57",
            "name": "CMU-SCS",
            "fullname": "Carnegie Mellon University School of Computer Science"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.02571",
            "authors": [
                {
                    "_id": "68e31f7073e20ab577841fb6",
                    "name": "Zhiting Mei",
                    "hidden": false
                },
                {
                    "_id": "68e31f7073e20ab577841fb7",
                    "name": "Ola Shorinwa",
                    "hidden": false
                },
                {
                    "_id": "68e31f7073e20ab577841fb8",
                    "name": "Anirudha Majumdar",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T21:20:41.000Z",
            "submittedOnDailyAt": "2025-10-06T00:16:32.486Z",
            "title": "How Confident are Video Models? Empowering Video Models to Express their\n  Uncertainty",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Generative video models demonstrate impressive text-to-video capabilities,\nspurring widespread adoption in many real-world applications. However, like\nlarge language models (LLMs), video generation models tend to hallucinate,\nproducing plausible videos even when they are factually wrong. Although\nuncertainty quantification (UQ) of LLMs has been extensively studied in prior\nwork, no UQ method for video models exists, raising critical safety concerns.\nTo our knowledge, this paper represents the first work towards quantifying the\nuncertainty of video models. We present a framework for uncertainty\nquantification of generative video models, consisting of: (i) a metric for\nevaluating the calibration of video models based on robust rank correlation\nestimation with no stringent modeling assumptions; (ii) a black-box UQ method\nfor video models (termed S-QUBED), which leverages latent modeling to\nrigorously decompose predictive uncertainty into its aleatoric and epistemic\ncomponents; and (iii) a UQ dataset to facilitate benchmarking calibration in\nvideo models. By conditioning the generation task in the latent space, we\ndisentangle uncertainty arising due to vague task specifications from that\narising from lack of knowledge. Through extensive experiments on benchmark\nvideo datasets, we demonstrate that S-QUBED computes calibrated total\nuncertainty estimates that are negatively correlated with the task accuracy and\neffectively computes the aleatoric and epistemic constituents.",
            "upvotes": 1,
            "discussionId": "68e31f7073e20ab577841fb9",
            "projectPage": "https://s-qubed.github.io/",
            "githubRepo": "https://github.com/irom-princeton/s-qubed",
            "ai_summary": "A framework for uncertainty quantification in generative video models is introduced, including a metric for calibration, a black-box method called S-QUBED, and a benchmark dataset, demonstrating improved uncertainty estimates and task accuracy.",
            "ai_keywords": [
                "generative video models",
                "text-to-video",
                "uncertainty quantification",
                "UQ",
                "large language models",
                "LLMs",
                "robust rank correlation estimation",
                "latent modeling",
                "aleatoric uncertainty",
                "epistemic uncertainty",
                "latent space",
                "benchmark video datasets"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-10-02T17:20:41.000Z",
        "title": "How Confident are Video Models? Empowering Video Models to Express their\n  Uncertainty",
        "summary": "Generative video models demonstrate impressive text-to-video capabilities,\nspurring widespread adoption in many real-world applications. However, like\nlarge language models (LLMs), video generation models tend to hallucinate,\nproducing plausible videos even when they are factually wrong. Although\nuncertainty quantification (UQ) of LLMs has been extensively studied in prior\nwork, no UQ method for video models exists, raising critical safety concerns.\nTo our knowledge, this paper represents the first work towards quantifying the\nuncertainty of video models. We present a framework for uncertainty\nquantification of generative video models, consisting of: (i) a metric for\nevaluating the calibration of video models based on robust rank correlation\nestimation with no stringent modeling assumptions; (ii) a black-box UQ method\nfor video models (termed S-QUBED), which leverages latent modeling to\nrigorously decompose predictive uncertainty into its aleatoric and epistemic\ncomponents; and (iii) a UQ dataset to facilitate benchmarking calibration in\nvideo models. By conditioning the generation task in the latent space, we\ndisentangle uncertainty arising due to vague task specifications from that\narising from lack of knowledge. Through extensive experiments on benchmark\nvideo datasets, we demonstrate that S-QUBED computes calibrated total\nuncertainty estimates that are negatively correlated with the task accuracy and\neffectively computes the aleatoric and epistemic constituents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02571.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 119
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.00658",
            "authors": [
                {
                    "_id": "68e3605973e20ab5778420cf",
                    "name": "Beomsu Kim",
                    "hidden": false
                },
                {
                    "_id": "68e3605973e20ab5778420d0",
                    "name": "Byunghee Cha",
                    "hidden": false
                },
                {
                    "_id": "68e3605973e20ab5778420d1",
                    "name": "Jong Chul Ye",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T08:35:18.000Z",
            "submittedOnDailyAt": "2025-10-06T04:54:58.646Z",
            "title": "Align Your Tangent: Training Better Consistency Models via\n  Manifold-Aligned Tangents",
            "submittedOnDailyBy": {
                "_id": "63db3a610cc3bc12bc0cfe2f",
                "avatarUrl": "/avatars/9926d709f2505c4758f9f66fc7a2c8e2.svg",
                "isPro": false,
                "fullname": "ByungHeeCha",
                "user": "paulcha1025",
                "type": "user"
            },
            "summary": "With diffusion and flow matching models achieving state-of-the-art generating\nperformance, the interest of the community now turned to reducing the inference\ntime without sacrificing sample quality. Consistency Models (CMs), which are\ntrained to be consistent on diffusion or probability flow ordinary differential\nequation (PF-ODE) trajectories, enable one or two-step flow or diffusion\nsampling. However, CMs typically require prolonged training with large batch\nsizes to obtain competitive sample quality. In this paper, we examine the\ntraining dynamics of CMs near convergence and discover that CM tangents -- CM\noutput update directions -- are quite oscillatory, in the sense that they move\nparallel to the data manifold, not towards the manifold. To mitigate\noscillatory tangents, we propose a new loss function, called the manifold\nfeature distance (MFD), which provides manifold-aligned tangents that point\ntoward the data manifold. Consequently, our method -- dubbed Align Your Tangent\n(AYT) -- can accelerate CM training by orders of magnitude and even out-perform\nthe learned perceptual image patch similarity metric (LPIPS). Furthermore, we\nfind that our loss enables training with extremely small batch sizes without\ncompromising sample quality. Code: https://github.com/1202kbs/AYT",
            "upvotes": 1,
            "discussionId": "68e3605a73e20ab5778420d2",
            "ai_summary": "Align Your Tangent (AYT) improves Consistency Model training by reducing oscillatory tangents and enabling faster convergence with small batch sizes.",
            "ai_keywords": [
                "diffusion models",
                "flow matching models",
                "Consistency Models",
                "CMs",
                "diffusion sampling",
                "probability flow ordinary differential equation",
                "PF-ODE",
                "training dynamics",
                "CM tangents",
                "manifold feature distance",
                "MFD",
                "Align Your Tangent",
                "AYT",
                "learned perceptual image patch similarity metric",
                "LPIPS"
            ]
        },
        "publishedAt": "2025-10-01T04:35:18.000Z",
        "title": "Align Your Tangent: Training Better Consistency Models via\n  Manifold-Aligned Tangents",
        "summary": "With diffusion and flow matching models achieving state-of-the-art generating\nperformance, the interest of the community now turned to reducing the inference\ntime without sacrificing sample quality. Consistency Models (CMs), which are\ntrained to be consistent on diffusion or probability flow ordinary differential\nequation (PF-ODE) trajectories, enable one or two-step flow or diffusion\nsampling. However, CMs typically require prolonged training with large batch\nsizes to obtain competitive sample quality. In this paper, we examine the\ntraining dynamics of CMs near convergence and discover that CM tangents -- CM\noutput update directions -- are quite oscillatory, in the sense that they move\nparallel to the data manifold, not towards the manifold. To mitigate\noscillatory tangents, we propose a new loss function, called the manifold\nfeature distance (MFD), which provides manifold-aligned tangents that point\ntoward the data manifold. Consequently, our method -- dubbed Align Your Tangent\n(AYT) -- can accelerate CM training by orders of magnitude and even out-perform\nthe learned perceptual image patch similarity metric (LPIPS). Furthermore, we\nfind that our loss enables training with extremely small batch sizes without\ncompromising sample quality. Code: https://github.com/1202kbs/AYT",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00658.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63db3a610cc3bc12bc0cfe2f",
            "avatarUrl": "/avatars/9926d709f2505c4758f9f66fc7a2c8e2.svg",
            "fullname": "ByungHeeCha",
            "name": "paulcha1025",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.24975",
            "authors": [
                {
                    "_id": "68de6aef70ada21878c75049",
                    "user": {
                        "_id": "6536904c72f105eef012a510",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/_NJTP-YHLbvKhM84l1CTy.jpeg",
                        "isPro": false,
                        "fullname": "Yang Lekang",
                        "user": "wellbeing",
                        "type": "user"
                    },
                    "name": "Lekang Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-02T13:54:09.312Z",
                    "hidden": false
                },
                {
                    "_id": "68de6aef70ada21878c7504a",
                    "name": "Yuetong Liu",
                    "hidden": false
                },
                {
                    "_id": "68de6aef70ada21878c7504b",
                    "name": "Yitong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68de6aef70ada21878c7504c",
                    "name": "Jia Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T16:04:18.000Z",
            "submittedOnDailyAt": "2025-10-06T09:46:03.390Z",
            "title": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via\n  Repetitive Pattern",
            "submittedOnDailyBy": {
                "_id": "6536904c72f105eef012a510",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/_NJTP-YHLbvKhM84l1CTy.jpeg",
                "isPro": false,
                "fullname": "Yang Lekang",
                "user": "wellbeing",
                "type": "user"
            },
            "summary": "Software development relies heavily on extensive unit testing, which makes\nthe efficiency of automated Unit Test Generation (UTG) particularly important.\nHowever, most existing LLMs generate test cases one token at a time in each\nforward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)\nhave emerged, offering promising parallel generation capabilities and showing\nstrong potential for efficient UTG. Despite this advantage, their application\nto UTG is still constrained by a clear trade-off between efficiency and test\nquality, since increasing the number of tokens generated in each step often\ncauses a sharp decline in the quality of test cases. To overcome this\nlimitation, we present DiffTester, an acceleration framework specifically\ntailored for dLLMs in UTG. The key idea of DiffTester is that unit tests\ntargeting the same focal method often share repetitive structural patterns. By\ndynamically identifying these common patterns through abstract syntax tree\nanalysis during generation, DiffTester adaptively increases the number of\ntokens produced at each step without compromising the quality of the output. To\nenable comprehensive evaluation, we extend the original TestEval benchmark,\nwhich was limited to Python, by introducing additional programming languages\nincluding Java and C++. Extensive experiments on three benchmarks with two\nrepresentative models show that DiffTester delivers significant acceleration\nwhile preserving test coverage. Moreover, DiffTester generalizes well across\ndifferent dLLMs and programming languages, providing a practical and scalable\nsolution for efficient UTG in software development. Code and data are publicly\navailable at https://github.com/wellbeingyang/DLM4UTG-open .",
            "upvotes": 1,
            "discussionId": "68de6af170ada21878c7504d",
            "ai_summary": "DiffTester is an acceleration framework for diffusion LLMs in unit test generation, improving efficiency without sacrificing test quality by identifying and leveraging common structural patterns.",
            "ai_keywords": [
                "diffusion LLMs",
                "dLLMs",
                "unit test generation",
                "UTG",
                "abstract syntax tree analysis",
                "test coverage",
                "TestEval benchmark",
                "Java",
                "C++"
            ]
        },
        "publishedAt": "2025-09-29T12:04:18.000Z",
        "title": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via\n  Repetitive Pattern",
        "summary": "Software development relies heavily on extensive unit testing, which makes\nthe efficiency of automated Unit Test Generation (UTG) particularly important.\nHowever, most existing LLMs generate test cases one token at a time in each\nforward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)\nhave emerged, offering promising parallel generation capabilities and showing\nstrong potential for efficient UTG. Despite this advantage, their application\nto UTG is still constrained by a clear trade-off between efficiency and test\nquality, since increasing the number of tokens generated in each step often\ncauses a sharp decline in the quality of test cases. To overcome this\nlimitation, we present DiffTester, an acceleration framework specifically\ntailored for dLLMs in UTG. The key idea of DiffTester is that unit tests\ntargeting the same focal method often share repetitive structural patterns. By\ndynamically identifying these common patterns through abstract syntax tree\nanalysis during generation, DiffTester adaptively increases the number of\ntokens produced at each step without compromising the quality of the output. To\nenable comprehensive evaluation, we extend the original TestEval benchmark,\nwhich was limited to Python, by introducing additional programming languages\nincluding Java and C++. Extensive experiments on three benchmarks with two\nrepresentative models show that DiffTester delivers significant acceleration\nwhile preserving test coverage. Moreover, DiffTester generalizes well across\ndifferent dLLMs and programming languages, providing a practical and scalable\nsolution for efficient UTG in software development. Code and data are publicly\navailable at https://github.com/wellbeingyang/DLM4UTG-open .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24975.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6536904c72f105eef012a510",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/_NJTP-YHLbvKhM84l1CTy.jpeg",
            "fullname": "Yang Lekang",
            "name": "wellbeing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.25944",
            "authors": [
                {
                    "_id": "68dd253fe795697646c0a147",
                    "user": {
                        "_id": "672288dd7055eec76d377268",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/672288dd7055eec76d377268/bBQ51VwKZJIkHkNffuEuJ.jpeg",
                        "isPro": true,
                        "fullname": "Yuan Gao",
                        "user": "Yuan-avs",
                        "type": "user"
                    },
                    "name": "Yuan Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-05T12:47:33.761Z",
                    "hidden": false
                },
                {
                    "_id": "68dd253fe795697646c0a148",
                    "name": "Mattia Piccinini",
                    "hidden": false
                },
                {
                    "_id": "68dd253fe795697646c0a149",
                    "name": "Roberto Brusnicki",
                    "hidden": false
                },
                {
                    "_id": "68dd253fe795697646c0a14a",
                    "name": "Yuchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68dd253fe795697646c0a14b",
                    "name": "Johannes Betz",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T08:37:31.000Z",
            "submittedOnDailyAt": "2025-10-06T07:32:08.985Z",
            "title": "NuRisk: A Visual Question Answering Dataset for Agent-Level Risk\n  Assessment in Autonomous Driving",
            "submittedOnDailyBy": {
                "_id": "672288dd7055eec76d377268",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/672288dd7055eec76d377268/bBQ51VwKZJIkHkNffuEuJ.jpeg",
                "isPro": true,
                "fullname": "Yuan Gao",
                "user": "Yuan-avs",
                "type": "user"
            },
            "summary": "Understanding risk in autonomous driving requires not only perception and\nprediction, but also high-level reasoning about agent behavior and context.\nCurrent Vision Language Models (VLMs)-based methods primarily ground agents in\nstatic images and provide qualitative judgments, lacking the spatio-temporal\nreasoning needed to capture how risks evolve over time. To address this gap, we\npropose NuRisk, a comprehensive Visual Question Answering (VQA) dataset\ncomprising 2,900 scenarios and 1.1 million agent-level samples, built on\nreal-world data from nuScenes and Waymo, supplemented with safety-critical\nscenarios from the CommonRoad simulator. The dataset provides Bird-Eye-View\n(BEV) based sequential images with quantitative, agent-level risk annotations,\nenabling spatio-temporal reasoning. We benchmark well-known VLMs across\ndifferent prompting techniques and find that they fail to perform explicit\nspatio-temporal reasoning, resulting in a peak accuracy of 33% at high latency.\nTo address these shortcomings, our fine-tuned 7B VLM agent improves accuracy to\n41% and reduces latency by 75%, demonstrating explicit spatio-temporal\nreasoning capabilities that proprietary models lacked. While this represents a\nsignificant step forward, the modest accuracy underscores the profound\nchallenge of the task, establishing NuRisk as a critical benchmark for\nadvancing spatio-temporal reasoning in autonomous driving.",
            "upvotes": 0,
            "discussionId": "68dd253fe795697646c0a14c",
            "projectPage": "https://anonymous-research-icra2026.github.io/Nurisk/",
            "ai_summary": "NuRisk, a comprehensive VQA dataset, addresses the lack of spatio-temporal reasoning in current VLMs for autonomous driving by providing agent-level risk annotations in sequential images, improving accuracy and reducing latency.",
            "ai_keywords": [
                "Visual Question Answering (VQA)",
                "Vision Language Models (VLMs)",
                "spatio-temporal reasoning",
                "Bird-Eye-View (BEV)",
                "agent-level risk annotations",
                "nuScenes",
                "Waymo",
                "CommonRoad simulator",
                "fine-tuned VLM"
            ],
            "organization": {
                "_id": "61fae781e68759322b9767be",
                "name": "TUM",
                "fullname": "Technical University of Munich",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1661167219960-629521a0f937190946e15d7f.jpeg"
            }
        },
        "publishedAt": "2025-09-30T04:37:31.000Z",
        "title": "NuRisk: A Visual Question Answering Dataset for Agent-Level Risk\n  Assessment in Autonomous Driving",
        "summary": "Understanding risk in autonomous driving requires not only perception and\nprediction, but also high-level reasoning about agent behavior and context.\nCurrent Vision Language Models (VLMs)-based methods primarily ground agents in\nstatic images and provide qualitative judgments, lacking the spatio-temporal\nreasoning needed to capture how risks evolve over time. To address this gap, we\npropose NuRisk, a comprehensive Visual Question Answering (VQA) dataset\ncomprising 2,900 scenarios and 1.1 million agent-level samples, built on\nreal-world data from nuScenes and Waymo, supplemented with safety-critical\nscenarios from the CommonRoad simulator. The dataset provides Bird-Eye-View\n(BEV) based sequential images with quantitative, agent-level risk annotations,\nenabling spatio-temporal reasoning. We benchmark well-known VLMs across\ndifferent prompting techniques and find that they fail to perform explicit\nspatio-temporal reasoning, resulting in a peak accuracy of 33% at high latency.\nTo address these shortcomings, our fine-tuned 7B VLM agent improves accuracy to\n41% and reduces latency by 75%, demonstrating explicit spatio-temporal\nreasoning capabilities that proprietary models lacked. While this represents a\nsignificant step forward, the modest accuracy underscores the profound\nchallenge of the task, establishing NuRisk as a critical benchmark for\nadvancing spatio-temporal reasoning in autonomous driving.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25944.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "672288dd7055eec76d377268",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/672288dd7055eec76d377268/bBQ51VwKZJIkHkNffuEuJ.jpeg",
            "fullname": "Yuan Gao",
            "name": "Yuan-avs",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "61fae781e68759322b9767be",
            "name": "TUM",
            "fullname": "Technical University of Munich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1661167219960-629521a0f937190946e15d7f.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.02375",
            "authors": [
                {
                    "_id": "68e3cc4c73e20ab57784221e",
                    "name": "Hadi Pouransari",
                    "hidden": false
                },
                {
                    "_id": "68e3cc4c73e20ab57784221f",
                    "name": "David Grangier",
                    "hidden": false
                },
                {
                    "_id": "68e3cc4c73e20ab577842220",
                    "name": "C Thomas",
                    "hidden": false
                },
                {
                    "_id": "68e3cc4c73e20ab577842221",
                    "name": "Michael Kirchhof",
                    "hidden": false
                },
                {
                    "_id": "68e3cc4c73e20ab577842222",
                    "name": "Oncel Tuzel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T17:59:50.000Z",
            "submittedOnDailyAt": "2025-10-06T12:36:50.280Z",
            "title": "Pretraining with hierarchical memories: separating long-tail and common\n  knowledge",
            "submittedOnDailyBy": {
                "_id": "653a8e65c75dc136bfb5b0f8",
                "avatarUrl": "/avatars/ef252794236ce0fe2debf12773c95bb2.svg",
                "isPro": false,
                "fullname": "Hadi Pouransari",
                "user": "hpouransari",
                "type": "user"
            },
            "summary": "The impressive performance gains of modern language models currently rely on\nscaling parameters: larger models store more world knowledge and reason better.\nYet compressing all world knowledge into parameters is unnecessary, as only a\nfraction is used per prompt, and impractical for edge devices with limited\ninference-time memory and compute. We address this shortcoming by a\nmemory-augmented architecture and a pretraining strategy aligned with existing\nhardware paradigms. We introduce small language models that access large\nhierarchical parametric memory banks encoding world knowledge. During\npretraining and inference, we fetch a small, context-dependent memory block and\nadd it to the model. Our pretraining learns to store long-tail world knowledge\nin the memory parameters, while the small language model acts as an anchor\ncapturing common knowledge and general reasoning abilities. Through\ntrillion-token-scale experiments, we show significant gains: a 160M-parameters\nmodel augmented with an 18M-parameters memory fetched from a 4.6B memory bank\nobtains comparable performance to a regular model with more than 2x the\nparameters. Through extensive experiments, we study the optimal type and size\nof parametric memories in transformers, scaling them to over 21B parameters. We\nfind that our proposed hierarchical feed-forward memories work robustly across\ntransformer architectures, whether added during pretraining or post-hoc.",
            "upvotes": 0,
            "discussionId": "68e3cc4c73e20ab577842223",
            "ai_summary": "A memory-augmented architecture with hierarchical parametric memory banks improves language model performance while reducing parameter size and computational requirements.",
            "ai_keywords": [
                "memory-augmented architecture",
                "parametric memory banks",
                "pretraining strategy",
                "hierarchical feed-forward memories",
                "transformers"
            ],
            "organization": {
                "_id": "628cbd99ef14f971b69948ab",
                "name": "apple",
                "fullname": "Apple",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
            }
        },
        "publishedAt": "2025-09-29T13:59:50.000Z",
        "title": "Pretraining with hierarchical memories: separating long-tail and common\n  knowledge",
        "summary": "The impressive performance gains of modern language models currently rely on\nscaling parameters: larger models store more world knowledge and reason better.\nYet compressing all world knowledge into parameters is unnecessary, as only a\nfraction is used per prompt, and impractical for edge devices with limited\ninference-time memory and compute. We address this shortcoming by a\nmemory-augmented architecture and a pretraining strategy aligned with existing\nhardware paradigms. We introduce small language models that access large\nhierarchical parametric memory banks encoding world knowledge. During\npretraining and inference, we fetch a small, context-dependent memory block and\nadd it to the model. Our pretraining learns to store long-tail world knowledge\nin the memory parameters, while the small language model acts as an anchor\ncapturing common knowledge and general reasoning abilities. Through\ntrillion-token-scale experiments, we show significant gains: a 160M-parameters\nmodel augmented with an 18M-parameters memory fetched from a 4.6B memory bank\nobtains comparable performance to a regular model with more than 2x the\nparameters. Through extensive experiments, we study the optimal type and size\nof parametric memories in transformers, scaling them to over 21B parameters. We\nfind that our proposed hierarchical feed-forward memories work robustly across\ntransformer architectures, whether added during pretraining or post-hoc.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02375.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653a8e65c75dc136bfb5b0f8",
            "avatarUrl": "/avatars/ef252794236ce0fe2debf12773c95bb2.svg",
            "fullname": "Hadi Pouransari",
            "name": "hpouransari",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.23291",
            "authors": [
                {
                    "_id": "68e39ec573e20ab57784218c",
                    "name": "Joseph Marvin Imperial",
                    "hidden": false
                },
                {
                    "_id": "68e39ec573e20ab57784218d",
                    "name": "Harish Tayyar Madabushi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-27T13:10:21.000Z",
            "submittedOnDailyAt": "2025-10-06T09:21:24.212Z",
            "title": "Scaling Policy Compliance Assessment in Language Models with Policy\n  Reasoning Traces",
            "submittedOnDailyBy": {
                "_id": "620a1e42e190de4b8f27ba1f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620a1e42e190de4b8f27ba1f/jhRHn5o5A_HLO-iYonZAl.png",
                "isPro": false,
                "fullname": "Joseph Imperial",
                "user": "josephimperial",
                "type": "user"
            },
            "summary": "Policy compliance assessment is a fundamental task of evaluating whether an\ninput case strictly complies with a set of human-defined rules, more generally\nknown as policies. In practice, human experts follow a systematic, step-by-step\nprocess to identify violations with respect to specific stipulations outlined\nin the policy. However, such documentation of gold-standard, expert-level\nreasoning processes is costly to acquire. In this paper, we introduce Policy\nReasoning Traces (PRT), a form of specialized generated reasoning chains that\nserve as a reasoning bridge to improve an LLM's policy compliance assessment\ncapabilities. Our empirical evaluations demonstrate that the use of PRTs for\nboth inference-time and training-time scenarios significantly enhances the\nperformance of open-weight and commercial models, setting a new\nstate-of-the-art for HIPAA and GDPR policies. Beyond accuracy gains, we also\nhighlight how PRTs can improve an LLM's ability to accurately cite policy\nclauses, as well as influence compliance decisions through their high\nutilization from the raw chains of thought.",
            "upvotes": 0,
            "discussionId": "68e39ec573e20ab57784218e",
            "githubRepo": "https://github.com/imperialite/policy-reasoning-traces",
            "ai_summary": "Policy Reasoning Traces (PRT) enhance LLMs' policy compliance assessment by providing detailed reasoning chains, improving accuracy and policy clause citation.",
            "ai_keywords": [
                "Policy Reasoning Traces",
                "PRT",
                "LLM",
                "policy compliance assessment",
                "reasoning chains",
                "HIPAA",
                "GDPR"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "6400d17b568dbe30c9149a27",
                "name": "UOBATH",
                "fullname": "University Of Bath",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/a4X8JIi5denBsE2_U6FMt.png"
            }
        },
        "publishedAt": "2025-09-27T09:10:21.000Z",
        "title": "Scaling Policy Compliance Assessment in Language Models with Policy\n  Reasoning Traces",
        "summary": "Policy compliance assessment is a fundamental task of evaluating whether an\ninput case strictly complies with a set of human-defined rules, more generally\nknown as policies. In practice, human experts follow a systematic, step-by-step\nprocess to identify violations with respect to specific stipulations outlined\nin the policy. However, such documentation of gold-standard, expert-level\nreasoning processes is costly to acquire. In this paper, we introduce Policy\nReasoning Traces (PRT), a form of specialized generated reasoning chains that\nserve as a reasoning bridge to improve an LLM's policy compliance assessment\ncapabilities. Our empirical evaluations demonstrate that the use of PRTs for\nboth inference-time and training-time scenarios significantly enhances the\nperformance of open-weight and commercial models, setting a new\nstate-of-the-art for HIPAA and GDPR policies. Beyond accuracy gains, we also\nhighlight how PRTs can improve an LLM's ability to accurately cite policy\nclauses, as well as influence compliance decisions through their high\nutilization from the raw chains of thought.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23291.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620a1e42e190de4b8f27ba1f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620a1e42e190de4b8f27ba1f/jhRHn5o5A_HLO-iYonZAl.png",
            "fullname": "Joseph Imperial",
            "name": "josephimperial",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "6400d17b568dbe30c9149a27",
            "name": "UOBATH",
            "fullname": "University Of Bath",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/a4X8JIi5denBsE2_U6FMt.png"
        },
        "isAuthorParticipating": false
    }
]