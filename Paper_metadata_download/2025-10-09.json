[
    {
        "paper": {
            "id": "2510.03215",
            "authors": [
                {
                    "_id": "68e7171b7ae125f9582e6952",
                    "user": {
                        "_id": "6445fd9ba56444c355dcbcba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
                        "isPro": false,
                        "fullname": "Tianyu Fu",
                        "user": "fuvty",
                        "type": "user"
                    },
                    "name": "Tianyu Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:50:21.158Z",
                    "hidden": false
                },
                {
                    "_id": "68e7171b7ae125f9582e6953",
                    "user": {
                        "_id": "6607b4a850d2b7a7109f0496",
                        "avatarUrl": "/avatars/ab52887b31b5ac8d20f91bf3b0db674d.svg",
                        "isPro": false,
                        "fullname": "Zihan Min",
                        "user": "minzh23",
                        "type": "user"
                    },
                    "name": "Zihan Min",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:53.564Z",
                    "hidden": false
                },
                {
                    "_id": "68e7171b7ae125f9582e6954",
                    "name": "Hanling Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e7171b7ae125f9582e6955",
                    "user": {
                        "_id": "6800abdd637c165b6c03c429",
                        "avatarUrl": "/avatars/739dbc76409cbef66528417710a9e7c6.svg",
                        "isPro": false,
                        "fullname": "JICHAO Yan",
                        "user": "elentsing",
                        "type": "user"
                    },
                    "name": "Jichao Yan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:51.583Z",
                    "hidden": false
                },
                {
                    "_id": "68e7171b7ae125f9582e6956",
                    "name": "Guohao Dai",
                    "hidden": false
                },
                {
                    "_id": "68e7171b7ae125f9582e6957",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68e7171b7ae125f9582e6958",
                    "name": "Yu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-03T17:52:32.000Z",
            "submittedOnDailyAt": "2025-10-09T00:33:46.090Z",
            "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "6445fd9ba56444c355dcbcba",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
                "isPro": false,
                "fullname": "Tianyu Fu",
                "user": "fuvty",
                "type": "user"
            },
            "summary": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.",
            "upvotes": 63,
            "discussionId": "68e7171b7ae125f9582e6959",
            "projectPage": "https://fuvty.github.io/C2C_Project_Page/",
            "githubRepo": "https://github.com/thu-nics/C2C",
            "ai_summary": "Cache-to-Cache (C2C) enables direct semantic communication between LLMs using neural network projections, improving accuracy and reducing latency compared to text-based communication.",
            "ai_keywords": [
                "Large Language Models",
                "KV-Cache",
                "Cache-to-Cache",
                "neural network",
                "semantic communication",
                "gating mechanism",
                "accuracy",
                "latency"
            ],
            "githubStars": 23,
            "organization": {
                "_id": "64b74b5fb727f8771ab887f9",
                "name": "nics-efc",
                "fullname": "Tsinghua-NICS-EFC",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641031b1a78453b8d96b8420/vmgxct2WsyHcKT2x2NAYT.jpeg"
            }
        },
        "publishedAt": "2025-10-03T13:52:32.000Z",
        "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
        "summary": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03215.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6445fd9ba56444c355dcbcba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
            "fullname": "Tianyu Fu",
            "name": "fuvty",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "organization": {
            "_id": "64b74b5fb727f8771ab887f9",
            "name": "nics-efc",
            "fullname": "Tsinghua-NICS-EFC",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641031b1a78453b8d96b8420/vmgxct2WsyHcKT2x2NAYT.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.06590",
            "authors": [
                {
                    "_id": "68e743637ae125f9582e6a5d",
                    "user": {
                        "_id": "655c5709948930b0fcf88a22",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/4Qk9Nlyus-su16GdIPJao.png",
                        "isPro": false,
                        "fullname": "Ziyuan",
                        "user": "zyhuangnus",
                        "type": "user"
                    },
                    "name": "Ziyuan Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T07:57:48.507Z",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a5e",
                    "user": {
                        "_id": "63747fe30938c075424108e6",
                        "avatarUrl": "/avatars/fd2d82c4fb2834edae516e904424a462.svg",
                        "isPro": false,
                        "fullname": "zheng",
                        "user": "forde450",
                        "type": "user"
                    },
                    "name": "DanDan Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T07:57:46.443Z",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a5f",
                    "name": "Cheng Zou",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a60",
                    "name": "Rui Liu",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a61",
                    "user": {
                        "_id": "64e848dd9a3cd93b371166cf",
                        "avatarUrl": "/avatars/6fcee1fe59624113fcf233caf0197729.svg",
                        "isPro": false,
                        "fullname": "Xiaolong Wang",
                        "user": "Xiaolong-Wang",
                        "type": "user"
                    },
                    "name": "Xiaolong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T07:57:38.365Z",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a62",
                    "user": {
                        "_id": "68e7773faf3a162f9301a117",
                        "avatarUrl": "/avatars/86010187beb5cd964eb9cc1db6e64dee.svg",
                        "isPro": false,
                        "fullname": "Kaixiang Ji",
                        "user": "TorryJ",
                        "type": "user"
                    },
                    "name": "Kaixiang Ji",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:31.833Z",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a63",
                    "user": {
                        "_id": "64d2fe8ed8b712baf1948c03",
                        "avatarUrl": "/avatars/e8f5f368632065e16bd758a7a2cf182e.svg",
                        "isPro": false,
                        "fullname": "Willie Chai",
                        "user": "Williechai",
                        "type": "user"
                    },
                    "name": "Weilong Chai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:26.937Z",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a64",
                    "name": "Jianxin Sun",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a65",
                    "user": {
                        "_id": "66b180741fa16824b629943b",
                        "avatarUrl": "/avatars/42d1076da1ae16ce2277f254416ff839.svg",
                        "isPro": false,
                        "fullname": "wang",
                        "user": "lbwang",
                        "type": "user"
                    },
                    "name": "Libin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:29.030Z",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a66",
                    "name": "Yongjie Lv",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a67",
                    "name": "Taozhi Huang",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a68",
                    "name": "Jiajia Liu",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a69",
                    "user": {
                        "_id": "6482dd5ec2ec7df31fd5cfd5",
                        "avatarUrl": "/avatars/c74fb04b2b4488a9f3f7872b2f91cd7b.svg",
                        "isPro": false,
                        "fullname": "qingpei.gqp",
                        "user": "qingpei",
                        "type": "user"
                    },
                    "name": "Qingpei Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T07:57:42.652Z",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a6a",
                    "name": "Ming Yang",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a6b",
                    "name": "Jingdong Chen",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a6c",
                    "name": "Jun Zhou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63747fe30938c075424108e6/j9oLJLM284ndzAO9csZY8.qt"
            ],
            "publishedAt": "2025-10-08T02:50:14.000Z",
            "submittedOnDailyAt": "2025-10-09T06:46:28.405Z",
            "title": "Ming-UniVision: Joint Image Understanding and Generation with a Unified\n  Continuous Tokenizer",
            "submittedOnDailyBy": {
                "_id": "63747fe30938c075424108e6",
                "avatarUrl": "/avatars/fd2d82c4fb2834edae516e904424a462.svg",
                "isPro": false,
                "fullname": "zheng",
                "user": "forde450",
                "type": "user"
            },
            "summary": "Visual tokenization remains a core challenge in unifying visual understanding\nand generation within the autoregressive paradigm. Existing methods typically\nemploy tokenizers in discrete latent spaces to align with the tokens from large\nlanguage models, where the quantization errors can limit semantic\nexpressiveness and degrade the capability of vision-language understanding. To\naddress this, we introduce MingTok, a new family of visual tokenizers with a\ncontinuous latent space, for unified autoregressive generation and\nunderstanding. While understanding tasks favor discriminative high-dimensional\nfeatures, generation tasks prefer compact low-level codes. Thus, to reconcile\nthese competing demands, MingTok adopts a three-stage sequential architecture\ninvolving low-level encoding, semantic expansion, and visual reconstruction.\nBuilt on top of it, Ming-UniVision eliminates the need for task-specific visual\nrepresentations, and unifies diverse vision-language tasks under a single\nautoregrsssive prediction paradigm. By formulating both understanding and\ngeneration as next-token prediction in a shared continuous space, it seamlessly\nsupports multi-round, in-context tasks such as iterative understanding,\ngeneration and editing. Empirically, we find that using a unified continuous\nvisual representation reconciles the competing requirements on the tokenizers\nby the understanding and generation tasks, thereby leading to state-of-the-art\nlevel performance across both domains. We hope our findings will facilitate\nunified visual tokenization in the continuous domain. Inference code and model\nweights are released to benefit community.",
            "upvotes": 59,
            "discussionId": "68e743637ae125f9582e6a6d",
            "githubRepo": "https://github.com/inclusionAI/Ming-UniVision",
            "ai_summary": "MingTok, a continuous latent space visual tokenizer, unifies vision-language understanding and generation within an autoregressive framework, achieving state-of-the-art performance across both domains.",
            "ai_keywords": [
                "visual tokenization",
                "autoregressive paradigm",
                "discrete latent spaces",
                "large language models",
                "quantization errors",
                "continuous latent space",
                "three-stage sequential architecture",
                "low-level encoding",
                "semantic expansion",
                "visual reconstruction",
                "Ming-UniVision",
                "task-specific visual representations",
                "next-token prediction",
                "iterative understanding",
                "generation and editing"
            ],
            "githubStars": 62,
            "organization": {
                "_id": "67aea5c8f086ab0f70ed97c9",
                "name": "inclusionAI",
                "fullname": "inclusionAI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
            }
        },
        "publishedAt": "2025-10-07T22:50:14.000Z",
        "title": "Ming-UniVision: Joint Image Understanding and Generation with a Unified\n  Continuous Tokenizer",
        "summary": "Visual tokenization remains a core challenge in unifying visual understanding\nand generation within the autoregressive paradigm. Existing methods typically\nemploy tokenizers in discrete latent spaces to align with the tokens from large\nlanguage models, where the quantization errors can limit semantic\nexpressiveness and degrade the capability of vision-language understanding. To\naddress this, we introduce MingTok, a new family of visual tokenizers with a\ncontinuous latent space, for unified autoregressive generation and\nunderstanding. While understanding tasks favor discriminative high-dimensional\nfeatures, generation tasks prefer compact low-level codes. Thus, to reconcile\nthese competing demands, MingTok adopts a three-stage sequential architecture\ninvolving low-level encoding, semantic expansion, and visual reconstruction.\nBuilt on top of it, Ming-UniVision eliminates the need for task-specific visual\nrepresentations, and unifies diverse vision-language tasks under a single\nautoregrsssive prediction paradigm. By formulating both understanding and\ngeneration as next-token prediction in a shared continuous space, it seamlessly\nsupports multi-round, in-context tasks such as iterative understanding,\ngeneration and editing. Empirically, we find that using a unified continuous\nvisual representation reconciles the competing requirements on the tokenizers\nby the understanding and generation tasks, thereby leading to state-of-the-art\nlevel performance across both domains. We hope our findings will facilitate\nunified visual tokenization in the continuous domain. Inference code and model\nweights are released to benefit community.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63747fe30938c075424108e6/j9oLJLM284ndzAO9csZY8.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06590.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63747fe30938c075424108e6",
            "avatarUrl": "/avatars/fd2d82c4fb2834edae516e904424a462.svg",
            "fullname": "zheng",
            "name": "forde450",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "67aea5c8f086ab0f70ed97c9",
            "name": "inclusionAI",
            "fullname": "inclusionAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.06308",
            "authors": [
                {
                    "_id": "68e70bdd7ae125f9582e6883",
                    "name": "Yi Xin",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6884",
                    "user": {
                        "_id": "66bb136002fd8eb58bc84ffb",
                        "avatarUrl": "/avatars/122cb8f59c502392768099b3c2afe043.svg",
                        "isPro": false,
                        "fullname": "qinqi",
                        "user": "Dakerqi",
                        "type": "user"
                    },
                    "name": "Qi Qin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:50:46.173Z",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6885",
                    "name": "Siqi Luo",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6886",
                    "name": "Kaiwen Zhu",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6887",
                    "name": "Juncheng Yan",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6888",
                    "name": "Yan Tai",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6889",
                    "user": {
                        "_id": "64c3c72e8f31d1e6c664b052",
                        "avatarUrl": "/avatars/af1ad5048eaa9dc417837ad02f927911.svg",
                        "isPro": false,
                        "fullname": "jiayi lei",
                        "user": "jyjyjyjy",
                        "type": "user"
                    },
                    "name": "Jiayi Lei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:50:43.828Z",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e688a",
                    "name": "Yuewen Cao",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e688b",
                    "name": "Keqi Wang",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e688c",
                    "name": "Yibin Wang",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e688d",
                    "name": "Jinbin Bai",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e688e",
                    "name": "Qian Yu",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e688f",
                    "name": "Dengyang Jiang",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6890",
                    "name": "Yuandong Pu",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6891",
                    "name": "Haoxing Chen",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6892",
                    "name": "Le Zhuo",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6893",
                    "name": "Junjun He",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6894",
                    "name": "Gen Luo",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6895",
                    "name": "Tianbin Li",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6896",
                    "name": "Ming Hu",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6897",
                    "name": "Jin Ye",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6898",
                    "name": "Shenglong Ye",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6899",
                    "name": "Bo Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e689a",
                    "name": "Chang Xu",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e689b",
                    "name": "Wenhai Wang",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e689c",
                    "name": "Hongsheng Li",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e689d",
                    "name": "Guangtao Zhai",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e689e",
                    "name": "Tianfan Xue",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e689f",
                    "name": "Bin Fu",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e68a0",
                    "name": "Xiaohong Liu",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e68a1",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e68a2",
                    "name": "Yihao Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-07T17:59:20.000Z",
            "submittedOnDailyAt": "2025-10-09T01:00:29.190Z",
            "title": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal\n  Generation and Understanding",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce Lumina-DiMOO, an open-source foundational model for seamless\nmulti-modal generation and understanding. Lumina-DiMOO sets itself apart from\nprior unified models by utilizing a fully discrete diffusion modeling to handle\ninputs and outputs across various modalities. This innovative approach allows\nLumina-DiMOO to achieve higher sampling efficiency compared to previous\nautoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a\nbroad spectrum of multi-modal tasks, including text-to-image generation,\nimage-to-image generation (e.g., image editing, subject-driven generation, and\nimage inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves\nstate-of-the-art performance on multiple benchmarks, surpassing existing\nopen-source unified multi-modal models. To foster further advancements in\nmulti-modal and discrete diffusion model research, we release our code and\ncheckpoints to the community. Project Page:\nhttps://synbol.github.io/Lumina-DiMOO.",
            "upvotes": 39,
            "discussionId": "68e70bde7ae125f9582e68a3",
            "projectPage": "https://synbol.github.io/Lumina-DiMOO/",
            "githubRepo": "https://github.com/Alpha-VLLM/Lumina-DiMOO",
            "ai_summary": "Lumina-DiMOO, an open-source foundational model, uses fully discrete diffusion modeling for efficient multi-modal generation and understanding, outperforming existing models.",
            "ai_keywords": [
                "discrete diffusion modeling",
                "autoregressive",
                "hybrid AR-Diffusion",
                "text-to-image generation",
                "image-to-image generation",
                "image editing",
                "subject-driven generation",
                "image inpainting",
                "image understanding"
            ],
            "githubStars": 743,
            "organization": {
                "_id": "64bd3427b567ae97c332277f",
                "name": "Alpha-VLLM",
                "fullname": "Alpha-VLLM",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646f1bef075e11ca78da3bb7/mD7gobrnznzDXnpZ9ZiT8.png"
            }
        },
        "publishedAt": "2025-10-07T13:59:20.000Z",
        "title": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal\n  Generation and Understanding",
        "summary": "We introduce Lumina-DiMOO, an open-source foundational model for seamless\nmulti-modal generation and understanding. Lumina-DiMOO sets itself apart from\nprior unified models by utilizing a fully discrete diffusion modeling to handle\ninputs and outputs across various modalities. This innovative approach allows\nLumina-DiMOO to achieve higher sampling efficiency compared to previous\nautoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a\nbroad spectrum of multi-modal tasks, including text-to-image generation,\nimage-to-image generation (e.g., image editing, subject-driven generation, and\nimage inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves\nstate-of-the-art performance on multiple benchmarks, surpassing existing\nopen-source unified multi-modal models. To foster further advancements in\nmulti-modal and discrete diffusion model research, we release our code and\ncheckpoints to the community. Project Page:\nhttps://synbol.github.io/Lumina-DiMOO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06308.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 123
        },
        "organization": {
            "_id": "64bd3427b567ae97c332277f",
            "name": "Alpha-VLLM",
            "fullname": "Alpha-VLLM",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646f1bef075e11ca78da3bb7/mD7gobrnznzDXnpZ9ZiT8.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.06917",
            "authors": [
                {
                    "_id": "68e70dc97ae125f9582e68c8",
                    "name": "Cheng-Han Chiang",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68c9",
                    "name": "Xiaofei Wang",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68ca",
                    "name": "Linjie Li",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68cb",
                    "name": "Chung-Ching Lin",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68cc",
                    "name": "Kevin Lin",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68cd",
                    "name": "Shujie Liu",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68ce",
                    "name": "Zhendong Wang",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68cf",
                    "name": "Zhengyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68d0",
                    "name": "Hung-yi Lee",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68d1",
                    "name": "Lijuan Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/622326ae0129f2097d69a3e2/N4e_VG2KksHuxbjoCTmWW.mp4"
            ],
            "publishedAt": "2025-10-08T11:48:59.000Z",
            "submittedOnDailyAt": "2025-10-09T00:11:57.362Z",
            "title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models",
            "submittedOnDailyBy": {
                "_id": "622326ae0129f2097d69a3e2",
                "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
                "isPro": false,
                "fullname": "Cheng-Han Chiang",
                "user": "dcml0714",
                "type": "user"
            },
            "summary": "Current large language models (LLMs) and spoken language models (SLMs) begin\nthinking and taking actions only after the user has finished their turn. This\nprevents the model from interacting during the user's turn and can lead to high\nresponse latency while it waits to think. Consequently, thinking after\nreceiving the full input is not suitable for speech-to-speech interaction,\nwhere real-time, low-latency exchange is important. We address this by noting\nthat humans naturally \"think while listening.\" In this paper, we propose\nSHANKS, a general inference framework that enables SLMs to generate unspoken\nchain-of-thought reasoning while listening to the user input. SHANKS streams\nthe input speech in fixed-duration chunks and, as soon as a chunk is received,\ngenerates unspoken reasoning based on all previous speech and reasoning, while\nthe user continues speaking. SHANKS uses this unspoken reasoning to decide\nwhether to interrupt the user and to make tool calls to complete the task. We\ndemonstrate that SHANKS enhances real-time user-SLM interaction in two\nscenarios: (1) when the user is presenting a step-by-step solution to a math\nproblem, SHANKS can listen, reason, and interrupt when the user makes a\nmistake, achieving 37.1% higher interruption accuracy than a baseline that\ninterrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can\ncomplete 56.9% of the tool calls before the user finishes their turn. Overall,\nSHANKS moves toward models that keep thinking throughout the conversation, not\nonly after a turn ends. Animated illustrations of Shanks can be found at\nhttps://d223302.github.io/SHANKS/",
            "upvotes": 32,
            "discussionId": "68e70dca7ae125f9582e68d2",
            "projectPage": "https://d223302.github.io/SHANKS/",
            "ai_summary": "SHANKS, a general inference framework, enables spoken language models to generate unspoken reasoning while listening to user input, enhancing real-time interaction and task completion.",
            "ai_keywords": [
                "SHANKS",
                "spoken language models",
                "unspoken chain-of-thought reasoning",
                "real-time interaction",
                "tool calls",
                "interruption accuracy"
            ]
        },
        "publishedAt": "2025-10-08T07:48:59.000Z",
        "title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models",
        "summary": "Current large language models (LLMs) and spoken language models (SLMs) begin\nthinking and taking actions only after the user has finished their turn. This\nprevents the model from interacting during the user's turn and can lead to high\nresponse latency while it waits to think. Consequently, thinking after\nreceiving the full input is not suitable for speech-to-speech interaction,\nwhere real-time, low-latency exchange is important. We address this by noting\nthat humans naturally \"think while listening.\" In this paper, we propose\nSHANKS, a general inference framework that enables SLMs to generate unspoken\nchain-of-thought reasoning while listening to the user input. SHANKS streams\nthe input speech in fixed-duration chunks and, as soon as a chunk is received,\ngenerates unspoken reasoning based on all previous speech and reasoning, while\nthe user continues speaking. SHANKS uses this unspoken reasoning to decide\nwhether to interrupt the user and to make tool calls to complete the task. We\ndemonstrate that SHANKS enhances real-time user-SLM interaction in two\nscenarios: (1) when the user is presenting a step-by-step solution to a math\nproblem, SHANKS can listen, reason, and interrupt when the user makes a\nmistake, achieving 37.1% higher interruption accuracy than a baseline that\ninterrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can\ncomplete 56.9% of the tool calls before the user finishes their turn. Overall,\nSHANKS moves toward models that keep thinking throughout the conversation, not\nonly after a turn ends. Animated illustrations of Shanks can be found at\nhttps://d223302.github.io/SHANKS/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/622326ae0129f2097d69a3e2/N4e_VG2KksHuxbjoCTmWW.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06917.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "622326ae0129f2097d69a3e2",
            "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
            "fullname": "Cheng-Han Chiang",
            "name": "dcml0714",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.06710",
            "authors": [
                {
                    "_id": "68e764c97ae125f9582e6bbc",
                    "name": "Hongzhi Zang",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bbd",
                    "name": "Mingjie Wei",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bbe",
                    "name": "Si Xu",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bbf",
                    "name": "Yongji Wu",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc0",
                    "name": "Zhen Guo",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc1",
                    "name": "Yuanqing Wang",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc2",
                    "name": "Hao Lin",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc3",
                    "name": "Liangzhi Shi",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc4",
                    "name": "Yuqing Xie",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc5",
                    "name": "Zhexuan Xu",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc6",
                    "name": "Zhihao Liu",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc7",
                    "name": "Kang Chen",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc8",
                    "user": {
                        "_id": "66a51845f9565635ebec3b55",
                        "avatarUrl": "/avatars/3cfff055e79da1bc43293b94b28ca210.svg",
                        "isPro": false,
                        "fullname": "Wenhao Tang",
                        "user": "tangwh0517",
                        "type": "user"
                    },
                    "name": "Wenhao Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:14.490Z",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc9",
                    "name": "Quanlu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bca",
                    "name": "Weinan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bcb",
                    "name": "Chao Yu",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bcc",
                    "name": "Yu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T07:05:13.000Z",
            "submittedOnDailyAt": "2025-10-09T06:06:46.600Z",
            "title": "RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training",
            "submittedOnDailyBy": {
                "_id": "64ba0f8d842aa47891cb972b",
                "avatarUrl": "/avatars/3bcacd9b778a146e88e20887b0b00720.svg",
                "isPro": false,
                "fullname": "Chao Yu",
                "user": "zoeyuchao",
                "type": "user"
            },
            "summary": "Recent progress in vision and language foundation models has significantly\nadvanced multimodal understanding, reasoning, and generation, inspiring a surge\nof interest in extending such capabilities to embodied settings through\nvision-language-action (VLA) models. Yet, most VLA models are still trained\nwith supervised fine-tuning (SFT), which struggles to generalize under\ndistribution shifts due to error accumulation. Reinforcement learning (RL)\noffers a promising alternative by directly optimizing task performance through\ninteraction, but existing attempts remain fragmented and lack a unified\nplatform for fair and systematic comparison across model architectures and\nalgorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and\nefficient framework for scalable RL training of VLA models. The system adopts a\nhighly flexible resource allocation design that addresses the challenge of\nintegrating rendering, training, and inference in RL+VLA training. In\nparticular, for GPU-parallelized simulators, RLinf-VLA implements a novel\nhybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup\nin training. Through a unified interface, RLinf-VLA seamlessly supports diverse\nVLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g.,\nPPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a\nunified model achieves 98.11\\% across 130 LIBERO tasks and 97.66\\% across 25\nManiSkill tasks. Beyond empirical performance, our study distills a set of best\npractices for applying RL to VLA training and sheds light on emerging patterns\nin this integration. Furthermore, we present preliminary deployment on a\nreal-world Franka robot, where RL-trained policies exhibit stronger\ngeneralization than those trained with SFT. We envision RLinf-VLA as a\nfoundation to accelerate and standardize research on embodied intelligence.",
            "upvotes": 30,
            "discussionId": "68e764c97ae125f9582e6bcd",
            "projectPage": "https://rlinf.readthedocs.io/en/latest/",
            "githubRepo": "https://github.com/RLinf/RLinf",
            "ai_summary": "RLinf-VLA is a unified framework for scalable reinforcement learning training of vision-language-action models, offering improved performance and generalization compared to supervised fine-tuning.",
            "ai_keywords": [
                "vision-language-action models",
                "supervised fine-tuning",
                "reinforcement learning",
                "RLinf-VLA",
                "resource allocation",
                "GPU-parallelized simulators",
                "hybrid fine-grained pipeline allocation",
                "OpenVLA",
                "OpenVLA-OFT",
                "PPO",
                "GRPO",
                "ManiSkill",
                "LIBERO",
                "Franka robot"
            ],
            "githubStars": 475,
            "organization": {
                "_id": "689ea978824b212c988bc8f5",
                "name": "RLinf",
                "fullname": "RLinf",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/689ea8a1a73ecc6940dbba3d/T2RGCw18z6lYP1WfkIGJ3.jpeg"
            }
        },
        "publishedAt": "2025-10-08T03:05:13.000Z",
        "title": "RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training",
        "summary": "Recent progress in vision and language foundation models has significantly\nadvanced multimodal understanding, reasoning, and generation, inspiring a surge\nof interest in extending such capabilities to embodied settings through\nvision-language-action (VLA) models. Yet, most VLA models are still trained\nwith supervised fine-tuning (SFT), which struggles to generalize under\ndistribution shifts due to error accumulation. Reinforcement learning (RL)\noffers a promising alternative by directly optimizing task performance through\ninteraction, but existing attempts remain fragmented and lack a unified\nplatform for fair and systematic comparison across model architectures and\nalgorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and\nefficient framework for scalable RL training of VLA models. The system adopts a\nhighly flexible resource allocation design that addresses the challenge of\nintegrating rendering, training, and inference in RL+VLA training. In\nparticular, for GPU-parallelized simulators, RLinf-VLA implements a novel\nhybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup\nin training. Through a unified interface, RLinf-VLA seamlessly supports diverse\nVLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g.,\nPPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a\nunified model achieves 98.11\\% across 130 LIBERO tasks and 97.66\\% across 25\nManiSkill tasks. Beyond empirical performance, our study distills a set of best\npractices for applying RL to VLA training and sheds light on emerging patterns\nin this integration. Furthermore, we present preliminary deployment on a\nreal-world Franka robot, where RL-trained policies exhibit stronger\ngeneralization than those trained with SFT. We envision RLinf-VLA as a\nfoundation to accelerate and standardize research on embodied intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06710.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ba0f8d842aa47891cb972b",
            "avatarUrl": "/avatars/3bcacd9b778a146e88e20887b0b00720.svg",
            "fullname": "Chao Yu",
            "name": "zoeyuchao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "689ea978824b212c988bc8f5",
            "name": "RLinf",
            "fullname": "RLinf",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/689ea8a1a73ecc6940dbba3d/T2RGCw18z6lYP1WfkIGJ3.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07310",
            "authors": [
                {
                    "_id": "68e71a787ae125f9582e6969",
                    "user": {
                        "_id": "67e3a3cc0c2f0d766d401bdb",
                        "avatarUrl": "/avatars/0de4c3b11295505ec9d3626e65302cbd.svg",
                        "isPro": false,
                        "fullname": "Siyoon Jin",
                        "user": "clwm515",
                        "type": "user"
                    },
                    "name": "Siyoon Jin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:49.499Z",
                    "hidden": false
                },
                {
                    "_id": "68e71a787ae125f9582e696a",
                    "user": {
                        "_id": "637c49ec9c470afa3880b137",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/pdcMPz8N6vQM1tc8IA1lV.png",
                        "isPro": false,
                        "fullname": "Seongchan Kim",
                        "user": "Seongchan",
                        "type": "user"
                    },
                    "name": "Seongchan Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:50:18.453Z",
                    "hidden": false
                },
                {
                    "_id": "68e71a787ae125f9582e696b",
                    "name": "Dahyun Chung",
                    "hidden": false
                },
                {
                    "_id": "68e71a787ae125f9582e696c",
                    "name": "Jaeho Lee",
                    "hidden": false
                },
                {
                    "_id": "68e71a787ae125f9582e696d",
                    "name": "Hyunwook Choi",
                    "hidden": false
                },
                {
                    "_id": "68e71a787ae125f9582e696e",
                    "name": "Jisu Nam",
                    "hidden": false
                },
                {
                    "_id": "68e71a787ae125f9582e696f",
                    "name": "Jiyoung Kim",
                    "hidden": false
                },
                {
                    "_id": "68e71a787ae125f9582e6970",
                    "name": "Seungryong Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T17:57:38.000Z",
            "submittedOnDailyAt": "2025-10-09T00:44:33.311Z",
            "title": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Video DiTs have advanced video generation, yet they still struggle to model\nmulti-instance or subject-object interactions. This raises a key question: How\ndo these models internally represent interactions? To answer this, we curate\nMATRIX-11K, a video dataset with interaction-aware captions and multi-instance\nmask tracks. Using this dataset, we conduct a systematic analysis that\nformalizes two perspectives of video DiTs: semantic grounding, via\nvideo-to-text attention, which evaluates whether noun and verb tokens capture\ninstances and their relations; and semantic propagation, via video-to-video\nattention, which assesses whether instance bindings persist across frames. We\nfind both effects concentrate in a small subset of interaction-dominant layers.\nMotivated by this, we introduce MATRIX, a simple and effective regularization\nthat aligns attention in specific layers of video DiTs with multi-instance mask\ntracks from the MATRIX-11K dataset, enhancing both grounding and propagation.\nWe further propose InterGenEval, an evaluation protocol for interaction-aware\nvideo generation. In experiments, MATRIX improves both interaction fidelity and\nsemantic alignment while reducing drift and hallucination. Extensive ablations\nvalidate our design choices. Codes and weights will be released.",
            "upvotes": 29,
            "discussionId": "68e71a787ae125f9582e6971",
            "projectPage": "https://cvlab-kaist.github.io/MATRIX/",
            "githubRepo": "https://github.com/cvlab-kaist/MATRIX",
            "ai_summary": "MATRIX-11K dataset and MATRIX regularization enhance interaction fidelity and semantic alignment in video DiTs by aligning attention with multi-instance mask tracks.",
            "ai_keywords": [
                "video DiTs",
                "interaction-aware captions",
                "multi-instance mask tracks",
                "video-to-text attention",
                "video-to-video attention",
                "semantic grounding",
                "semantic propagation",
                "MATRIX regularization",
                "InterGenEval",
                "interaction fidelity",
                "semantic alignment",
                "drift",
                "hallucination"
            ],
            "githubStars": 22
        },
        "publishedAt": "2025-10-08T13:57:38.000Z",
        "title": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation",
        "summary": "Video DiTs have advanced video generation, yet they still struggle to model\nmulti-instance or subject-object interactions. This raises a key question: How\ndo these models internally represent interactions? To answer this, we curate\nMATRIX-11K, a video dataset with interaction-aware captions and multi-instance\nmask tracks. Using this dataset, we conduct a systematic analysis that\nformalizes two perspectives of video DiTs: semantic grounding, via\nvideo-to-text attention, which evaluates whether noun and verb tokens capture\ninstances and their relations; and semantic propagation, via video-to-video\nattention, which assesses whether instance bindings persist across frames. We\nfind both effects concentrate in a small subset of interaction-dominant layers.\nMotivated by this, we introduce MATRIX, a simple and effective regularization\nthat aligns attention in specific layers of video DiTs with multi-instance mask\ntracks from the MATRIX-11K dataset, enhancing both grounding and propagation.\nWe further propose InterGenEval, an evaluation protocol for interaction-aware\nvideo generation. In experiments, MATRIX improves both interaction fidelity and\nsemantic alignment while reducing drift and hallucination. Extensive ablations\nvalidate our design choices. Codes and weights will be released.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07310.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 123
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07315",
            "authors": [
                {
                    "_id": "68e714517ae125f9582e6903",
                    "user": {
                        "_id": "61d53df2062444ea769d3b79",
                        "avatarUrl": "/avatars/fa771202368b6b2626a8fdf1c4369239.svg",
                        "isPro": false,
                        "fullname": "Ming Zhong",
                        "user": "MingZhong",
                        "type": "user"
                    },
                    "name": "Ming Zhong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:50:34.588Z",
                    "hidden": false
                },
                {
                    "_id": "68e714517ae125f9582e6904",
                    "name": "Xiang Zhou",
                    "hidden": false
                },
                {
                    "_id": "68e714517ae125f9582e6905",
                    "name": "Ting-Yun Chang",
                    "hidden": false
                },
                {
                    "_id": "68e714517ae125f9582e6906",
                    "name": "Qingze Wang",
                    "hidden": false
                },
                {
                    "_id": "68e714517ae125f9582e6907",
                    "name": "Nan Xu",
                    "hidden": false
                },
                {
                    "_id": "68e714517ae125f9582e6908",
                    "name": "Xiance Si",
                    "hidden": false
                },
                {
                    "_id": "68e714517ae125f9582e6909",
                    "name": "Dan Garrette",
                    "hidden": false
                },
                {
                    "_id": "68e714517ae125f9582e690a",
                    "name": "Shyam Upadhyay",
                    "hidden": false
                },
                {
                    "_id": "68e714517ae125f9582e690b",
                    "name": "Jeremiah Liu",
                    "hidden": false
                },
                {
                    "_id": "68e714517ae125f9582e690c",
                    "name": "Jiawei Han",
                    "hidden": false
                },
                {
                    "_id": "68e714517ae125f9582e690d",
                    "name": "Benoit Schillings",
                    "hidden": false
                },
                {
                    "_id": "68e714517ae125f9582e690e",
                    "name": "Jiao Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T17:59:19.000Z",
            "submittedOnDailyAt": "2025-10-09T00:25:56.326Z",
            "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
            "submittedOnDailyBy": {
                "_id": "61d53df2062444ea769d3b79",
                "avatarUrl": "/avatars/fa771202368b6b2626a8fdf1c4369239.svg",
                "isPro": false,
                "fullname": "Ming Zhong",
                "user": "MingZhong",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage\nLLMs to generate and iteratively refine code through natural language\ninteractions until it passes their vibe check. Vibe check is tied to real-world\nhuman preference and goes beyond functionality: the solution should feel right,\nread cleanly, preserve intent, and remain correct. However, current code\nevaluation remains anchored to pass@k and captures only functional correctness,\noverlooking the non-functional instructions that users routinely apply. In this\npaper, we hypothesize that instruction following is the missing piece\nunderlying vibe check that represents human preference in coding besides\nfunctional correctness. To quantify models' code instruction following\ncapabilities with measurable signals, we present VeriCode, a taxonomy of 30\nverifiable code instructions together with corresponding deterministic\nverifiers. We use the taxonomy to augment established evaluation suites,\nresulting in Vibe Checker, a testbed to assess both code instruction following\nand functional correctness. Upon evaluating 31 leading LLMs, we show that even\nthe strongest models struggle to comply with multiple instructions and exhibit\nclear functional regression. Most importantly, a composite score of functional\ncorrectness and instruction following correlates the best with human\npreference, with the latter emerging as the primary differentiator on\nreal-world programming tasks. Our work identifies core factors of the vibe\ncheck, providing a concrete path for benchmarking and developing models that\nbetter align with user preferences in coding.",
            "upvotes": 28,
            "discussionId": "68e714517ae125f9582e690f",
            "ai_summary": "Vibe Checker evaluates LLMs by combining functional correctness and instruction following to better align with human coding preferences.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "vibe coding",
                "Vibe Checker",
                "VeriCode",
                "verifiable code instructions",
                "deterministic verifiers",
                "instruction following",
                "functional correctness",
                "human preference",
                "coding preferences"
            ],
            "organization": {
                "_id": "60f6cbb2852126bac698c89e",
                "name": "deepmind",
                "fullname": "Deepmind",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
            }
        },
        "publishedAt": "2025-10-08T13:59:19.000Z",
        "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
        "summary": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage\nLLMs to generate and iteratively refine code through natural language\ninteractions until it passes their vibe check. Vibe check is tied to real-world\nhuman preference and goes beyond functionality: the solution should feel right,\nread cleanly, preserve intent, and remain correct. However, current code\nevaluation remains anchored to pass@k and captures only functional correctness,\noverlooking the non-functional instructions that users routinely apply. In this\npaper, we hypothesize that instruction following is the missing piece\nunderlying vibe check that represents human preference in coding besides\nfunctional correctness. To quantify models' code instruction following\ncapabilities with measurable signals, we present VeriCode, a taxonomy of 30\nverifiable code instructions together with corresponding deterministic\nverifiers. We use the taxonomy to augment established evaluation suites,\nresulting in Vibe Checker, a testbed to assess both code instruction following\nand functional correctness. Upon evaluating 31 leading LLMs, we show that even\nthe strongest models struggle to comply with multiple instructions and exhibit\nclear functional regression. Most importantly, a composite score of functional\ncorrectness and instruction following correlates the best with human\npreference, with the latter emerging as the primary differentiator on\nreal-world programming tasks. Our work identifies core factors of the vibe\ncheck, providing a concrete path for benchmarking and developing models that\nbetter align with user preferences in coding.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07315.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61d53df2062444ea769d3b79",
            "avatarUrl": "/avatars/fa771202368b6b2626a8fdf1c4369239.svg",
            "fullname": "Ming Zhong",
            "name": "MingZhong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "60f6cbb2852126bac698c89e",
            "name": "deepmind",
            "fullname": "Deepmind",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.07318",
            "authors": [
                {
                    "_id": "68e713227ae125f9582e68fb",
                    "name": "Yunhao Fang",
                    "hidden": false
                },
                {
                    "_id": "68e713227ae125f9582e68fc",
                    "name": "Weihao Yu",
                    "hidden": false
                },
                {
                    "_id": "68e713227ae125f9582e68fd",
                    "name": "Shu Zhong",
                    "hidden": false
                },
                {
                    "_id": "68e713227ae125f9582e68fe",
                    "name": "Qinghao Ye",
                    "hidden": false
                },
                {
                    "_id": "68e713227ae125f9582e68ff",
                    "name": "Xuehan Xiong",
                    "hidden": false
                },
                {
                    "_id": "68e713227ae125f9582e6900",
                    "name": "Lai Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T17:59:55.000Z",
            "submittedOnDailyAt": "2025-10-09T00:16:06.988Z",
            "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
            "submittedOnDailyBy": {
                "_id": "5df833bdda6d0311fd3d5403",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5df833bdda6d0311fd3d5403/62OtGJEQXdOuhV9yCd4HS.png",
                "isPro": false,
                "fullname": "Weihao Yu",
                "user": "whyu",
                "type": "user"
            },
            "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.",
            "upvotes": 20,
            "discussionId": "68e713227ae125f9582e6901",
            "githubRepo": "https://github.com/ByteDance-Seed/AHN",
            "ai_summary": "A memory framework combining short-term and long-term memory in neural networks improves long-sequence modeling efficiency and performance.",
            "ai_keywords": [
                "RNN-like models",
                "attention-based Transformers",
                "Multi-Store Model",
                "sliding window",
                "KV cache",
                "Artificial Hippocampus Network (AHN)",
                "Mamba2",
                "DeltaNet",
                "Gated DeltaNet",
                "LV-Eval",
                "InfiniteBench",
                "inference FLOPs",
                "memory cache"
            ],
            "githubStars": 28,
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-10-08T13:59:55.000Z",
        "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
        "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07318.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5df833bdda6d0311fd3d5403",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5df833bdda6d0311fd3d5403/62OtGJEQXdOuhV9yCd4HS.png",
            "fullname": "Weihao Yu",
            "name": "whyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.04678",
            "authors": [
                {
                    "_id": "68e62542975ac4c405ef21eb",
                    "name": "Zhanfeng Mo",
                    "hidden": false
                },
                {
                    "_id": "68e62542975ac4c405ef21ec",
                    "name": "Xingxuan Li",
                    "hidden": false
                },
                {
                    "_id": "68e62542975ac4c405ef21ed",
                    "name": "Yuntao Chen",
                    "hidden": false
                },
                {
                    "_id": "68e62542975ac4c405ef21ee",
                    "name": "Lidong Bing",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-06T10:44:04.000Z",
            "submittedOnDailyAt": "2025-10-09T00:20:37.134Z",
            "title": "Multi-Agent Tool-Integrated Policy Optimization",
            "submittedOnDailyBy": {
                "_id": "6362a77dd3be91534c2e9213",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6362a77dd3be91534c2e9213/3uUM3B1m2CFMukbkA0yDv.png",
                "isPro": true,
                "fullname": "Xingxuan Li",
                "user": "veggiebird",
                "type": "user"
            },
            "summary": "Large language models (LLMs) increasingly rely on multi-turn tool-integrated\nplanning for knowledge-intensive and complex reasoning tasks. Existing\nimplementations typically rely on a single agent, but they suffer from limited\ncontext length and noisy tool responses. A natural solution is to adopt a\nmulti-agent framework with planner- and worker-agents to manage context.\nHowever, no existing methods support effective reinforcement learning\npost-training of tool-integrated multi-agent frameworks. To address this gap,\nwe propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which\nenables distinct roles (planner and worker) to be trained within a single LLM\ninstance using role-specific prompts via reinforcement learning. MATPO is\nderived from a principled credit assignment mechanism across planner and worker\nrollouts. This design eliminates the need to deploy multiple LLMs, which would\nbe memory-intensive, while preserving the benefits of specialization.\nExperiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently\noutperforms single-agent baselines by an average of 18.38% relative improvement\nin performance and exhibits greater robustness to noisy tool outputs. Our\nfindings highlight the effectiveness of unifying multiple agent roles within a\nsingle LLM and provide practical insights for stable and efficient multi-agent\nRL training.",
            "upvotes": 19,
            "discussionId": "68e62596975ac4c405ef21ef",
            "githubRepo": "https://github.com/mzf666/MATPO",
            "ai_summary": "MATPO, a reinforcement learning method, optimizes tool-integrated multi-agent roles within a single LLM, improving performance and robustness over single-agent systems.",
            "ai_keywords": [
                "multi-turn tool-integrated planning",
                "multi-agent framework",
                "planner-agents",
                "worker-agents",
                "reinforcement learning",
                "role-specific prompts",
                "credit assignment mechanism",
                "planner and worker rollouts",
                "multi-agent RL training"
            ],
            "githubStars": 20
        },
        "publishedAt": "2025-10-06T06:44:04.000Z",
        "title": "Multi-Agent Tool-Integrated Policy Optimization",
        "summary": "Large language models (LLMs) increasingly rely on multi-turn tool-integrated\nplanning for knowledge-intensive and complex reasoning tasks. Existing\nimplementations typically rely on a single agent, but they suffer from limited\ncontext length and noisy tool responses. A natural solution is to adopt a\nmulti-agent framework with planner- and worker-agents to manage context.\nHowever, no existing methods support effective reinforcement learning\npost-training of tool-integrated multi-agent frameworks. To address this gap,\nwe propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which\nenables distinct roles (planner and worker) to be trained within a single LLM\ninstance using role-specific prompts via reinforcement learning. MATPO is\nderived from a principled credit assignment mechanism across planner and worker\nrollouts. This design eliminates the need to deploy multiple LLMs, which would\nbe memory-intensive, while preserving the benefits of specialization.\nExperiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently\noutperforms single-agent baselines by an average of 18.38% relative improvement\nin performance and exhibits greater robustness to noisy tool outputs. Our\nfindings highlight the effectiveness of unifying multiple agent roles within a\nsingle LLM and provide practical insights for stable and efficient multi-agent\nRL training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04678.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6362a77dd3be91534c2e9213",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6362a77dd3be91534c2e9213/3uUM3B1m2CFMukbkA0yDv.png",
            "fullname": "Xingxuan Li",
            "name": "veggiebird",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.05644",
            "authors": [
                {
                    "_id": "68e70c147ae125f9582e68a5",
                    "user": {
                        "_id": "66feebe57d722f0879d6b247",
                        "avatarUrl": "/avatars/f9194aa76da3ea2b0212e9ef6b5093d1.svg",
                        "isPro": false,
                        "fullname": "Sheriff I",
                        "user": "imsheriff",
                        "type": "user"
                    },
                    "name": "Sheriff Issaka",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:50:40.784Z",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68a6",
                    "name": "Keyi Wang",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68a7",
                    "name": "Yinka Ajibola",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68a8",
                    "name": "Oluwatumininu Samuel-Ipaye",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68a9",
                    "name": "Zhaoyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68aa",
                    "name": "Nicte Aguillon Jimenez",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68ab",
                    "name": "Evans Kofi Agyei",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68ac",
                    "name": "Abraham Lin",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68ad",
                    "name": "Rohan Ramachandran",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68ae",
                    "name": "Sadick Abdul Mumin",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68af",
                    "name": "Faith Nchifor",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68b0",
                    "name": "Mohammed Shuraim",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68b1",
                    "name": "Lieqi Liu",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68b2",
                    "name": "Erick Rosas Gonzalez",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68b3",
                    "name": "Sylvester Kpei",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68b4",
                    "name": "Jemimah Osei",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68b5",
                    "name": "Carlene Ajeneza",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68b6",
                    "name": "Persis Boateng",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68b7",
                    "name": "Prisca Adwoa Dufie Yeboah",
                    "hidden": false
                },
                {
                    "_id": "68e70c147ae125f9582e68b8",
                    "name": "Saadia Gabriel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-07T07:42:52.000Z",
            "submittedOnDailyAt": "2025-10-09T05:54:59.662Z",
            "title": "The African Languages Lab: A Collaborative Approach to Advancing\n  Low-Resource African NLP",
            "submittedOnDailyBy": {
                "_id": "66feebe57d722f0879d6b247",
                "avatarUrl": "/avatars/f9194aa76da3ea2b0212e9ef6b5093d1.svg",
                "isPro": false,
                "fullname": "Sheriff I",
                "user": "imsheriff",
                "type": "user"
            },
            "summary": "Despite representing nearly one-third of the world's languages, African\nlanguages remain critically underserved by modern NLP technologies, with 88\\%\nclassified as severely underrepresented or completely ignored in computational\nlinguistics. We present the African Languages Lab (All Lab), a comprehensive\nresearch initiative that addresses this technological gap through systematic\ndata collection, model development, and capacity building. Our contributions\ninclude: (1) a quality-controlled data collection pipeline, yielding the\nlargest validated African multi-modal speech and text dataset spanning 40\nlanguages with 19 billion tokens of monolingual text and 12,628 hours of\naligned speech data; (2) extensive experimental validation demonstrating that\nour dataset, combined with fine-tuning, achieves substantial improvements over\nbaseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points\nacross 31 evaluated languages; and (3) a structured research program that has\nsuccessfully mentored fifteen early-career researchers, establishing\nsustainable local capacity. Our comparative evaluation against Google Translate\nreveals competitive performance in several languages while identifying areas\nthat require continued development.",
            "upvotes": 18,
            "discussionId": "68e70c157ae125f9582e68b9",
            "ai_summary": "The African Languages Lab addresses the underserved status of African languages in NLP by creating a large dataset and demonstrating improved model performance through fine-tuning.",
            "ai_keywords": [
                "multi-modal speech and text dataset",
                "fine-tuning",
                "ChrF++",
                "COMET",
                "BLEU",
                "Google Translate"
            ],
            "organization": {
                "_id": "67784c39dac147922d8d09f0",
                "name": "UCLA",
                "fullname": "University of California, Los Angeles",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"
            }
        },
        "publishedAt": "2025-10-07T03:42:52.000Z",
        "title": "The African Languages Lab: A Collaborative Approach to Advancing\n  Low-Resource African NLP",
        "summary": "Despite representing nearly one-third of the world's languages, African\nlanguages remain critically underserved by modern NLP technologies, with 88\\%\nclassified as severely underrepresented or completely ignored in computational\nlinguistics. We present the African Languages Lab (All Lab), a comprehensive\nresearch initiative that addresses this technological gap through systematic\ndata collection, model development, and capacity building. Our contributions\ninclude: (1) a quality-controlled data collection pipeline, yielding the\nlargest validated African multi-modal speech and text dataset spanning 40\nlanguages with 19 billion tokens of monolingual text and 12,628 hours of\naligned speech data; (2) extensive experimental validation demonstrating that\nour dataset, combined with fine-tuning, achieves substantial improvements over\nbaseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points\nacross 31 evaluated languages; and (3) a structured research program that has\nsuccessfully mentored fifteen early-career researchers, establishing\nsustainable local capacity. Our comparative evaluation against Google Translate\nreveals competitive performance in several languages while identifying areas\nthat require continued development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05644.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66feebe57d722f0879d6b247",
            "avatarUrl": "/avatars/f9194aa76da3ea2b0212e9ef6b5093d1.svg",
            "fullname": "Sheriff I",
            "name": "imsheriff",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "67784c39dac147922d8d09f0",
            "name": "UCLA",
            "fullname": "University of California, Los Angeles",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.04230",
            "authors": [
                {
                    "_id": "68e7a5f744180c42eca8afa7",
                    "name": "Guijin Son",
                    "hidden": false
                },
                {
                    "_id": "68e7a5f744180c42eca8afa8",
                    "name": "Donghun Yang",
                    "hidden": false
                },
                {
                    "_id": "68e7a5f744180c42eca8afa9",
                    "name": "Hitesh Laxmichand Patel",
                    "hidden": false
                },
                {
                    "_id": "68e7a5f744180c42eca8afaa",
                    "name": "Amit Agarwal",
                    "hidden": false
                },
                {
                    "_id": "68e7a5f744180c42eca8afab",
                    "name": "Hyunwoo Ko",
                    "hidden": false
                },
                {
                    "_id": "68e7a5f744180c42eca8afac",
                    "name": "Chanuk Lim",
                    "hidden": false
                },
                {
                    "_id": "68e7a5f744180c42eca8afad",
                    "name": "Srikant Panda",
                    "hidden": false
                },
                {
                    "_id": "68e7a5f744180c42eca8afae",
                    "name": "Minhyuk Kim",
                    "hidden": false
                },
                {
                    "_id": "68e7a5f744180c42eca8afaf",
                    "name": "Nikunj Drolia",
                    "hidden": false
                },
                {
                    "_id": "68e7a5f744180c42eca8afb0",
                    "name": "Dasol Choi",
                    "hidden": false
                },
                {
                    "_id": "68e7a5f744180c42eca8afb1",
                    "name": "Kyong-Ha Lee",
                    "hidden": false
                },
                {
                    "_id": "68e7a5f744180c42eca8afb2",
                    "name": "Youngjae Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-05T14:39:41.000Z",
            "submittedOnDailyAt": "2025-10-09T10:59:47.271Z",
            "title": "Pushing on Multilingual Reasoning Models with Language-Mixed\n  Chain-of-Thought",
            "submittedOnDailyBy": {
                "_id": "60d3e619b8448e1785bbda2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg",
                "isPro": false,
                "fullname": "GUIJIN SON",
                "user": "amphora",
                "type": "user"
            },
            "summary": "Recent frontier models employ long chain-of-thought reasoning to explore\nsolution spaces in context and achieve stonger performance. While many works\nstudy distillation to build smaller yet capable models, most focus on English\nand little is known about language-specific reasoning. To bridge this gap, we\nfirst introduct **Language-Mixed CoT**, a reasoning schema that switches\nbetween English and a target language, using English as an anchor to excel in\nreasoning while minimizing translation artificats. As a Korean case study, we\ncurate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and\ncode; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k\nhigh-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5,\nLlama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves\nstate-of-the-art performance, with the highest overall average score (64.0 \\pm\n25), ranking first on 5/9 benchmarks and second on the remainder. Samller and\nmid-sized models also benefit substantially, with an average improvement of\n+18.6 points across teh evaluated nine benchmarks. Ablations show\n**Language-Mixed CoT** is more effective than monolingual CoT, also resulting\nin cross-lingual and mult-modal performance gains. We release our data-curation\npipeline, evaluation system, datasets, and models to advance research on\nlanguage-specific reasoning. Data and model collection:\nhttps://huggingface.co/KOREAson.",
            "upvotes": 18,
            "discussionId": "68e7a5f744180c42eca8afb3",
            "ai_summary": "A language-mixed chain-of-thought reasoning approach improves performance in Korean-specific tasks by switching between English and Korean, achieving state-of-the-art results across various benchmarks.",
            "ai_keywords": [
                "chain-of-thought reasoning",
                "Language-Mixed CoT",
                "monolingual CoT",
                "Yi-Sang",
                "Qwen3-32B",
                "KO-REAson-35B",
                "cross-lingual",
                "mult-modal"
            ],
            "organization": {
                "_id": "685cc6f7126d8787fb3ed2fc",
                "name": "KOREAson",
                "fullname": "KO-REAson",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60d3e619b8448e1785bbda2a/ER77qol4JwxeaxzA4A_i7.jpeg"
            }
        },
        "publishedAt": "2025-10-05T10:39:41.000Z",
        "title": "Pushing on Multilingual Reasoning Models with Language-Mixed\n  Chain-of-Thought",
        "summary": "Recent frontier models employ long chain-of-thought reasoning to explore\nsolution spaces in context and achieve stonger performance. While many works\nstudy distillation to build smaller yet capable models, most focus on English\nand little is known about language-specific reasoning. To bridge this gap, we\nfirst introduct **Language-Mixed CoT**, a reasoning schema that switches\nbetween English and a target language, using English as an anchor to excel in\nreasoning while minimizing translation artificats. As a Korean case study, we\ncurate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and\ncode; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k\nhigh-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5,\nLlama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves\nstate-of-the-art performance, with the highest overall average score (64.0 \\pm\n25), ranking first on 5/9 benchmarks and second on the remainder. Samller and\nmid-sized models also benefit substantially, with an average improvement of\n+18.6 points across teh evaluated nine benchmarks. Ablations show\n**Language-Mixed CoT** is more effective than monolingual CoT, also resulting\nin cross-lingual and mult-modal performance gains. We release our data-curation\npipeline, evaluation system, datasets, and models to advance research on\nlanguage-specific reasoning. Data and model collection:\nhttps://huggingface.co/KOREAson.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04230.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60d3e619b8448e1785bbda2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg",
            "fullname": "GUIJIN SON",
            "name": "amphora",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 64
        },
        "organization": {
            "_id": "685cc6f7126d8787fb3ed2fc",
            "name": "KOREAson",
            "fullname": "KO-REAson",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60d3e619b8448e1785bbda2a/ER77qol4JwxeaxzA4A_i7.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.04212",
            "authors": [
                {
                    "_id": "68e7159d7ae125f9582e6916",
                    "user": {
                        "_id": "66cfda1386e3d910d7d88550",
                        "avatarUrl": "/avatars/e6c6537c0dc0a5e1b9d4f7fd62af60af.svg",
                        "isPro": false,
                        "fullname": "Haiquan Qiu",
                        "user": "huggingaaaaa",
                        "type": "user"
                    },
                    "name": "Haiquan Qiu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:50:30.338Z",
                    "hidden": false
                },
                {
                    "_id": "68e7159d7ae125f9582e6917",
                    "name": "Quanming Yao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-05T14:01:24.000Z",
            "submittedOnDailyAt": "2025-10-09T00:35:51.220Z",
            "title": "Why Low-Precision Transformer Training Fails: An Analysis on Flash\n  Attention",
            "submittedOnDailyBy": {
                "_id": "66cfda1386e3d910d7d88550",
                "avatarUrl": "/avatars/e6c6537c0dc0a5e1b9d4f7fd62af60af.svg",
                "isPro": false,
                "fullname": "Haiquan Qiu",
                "user": "huggingaaaaa",
                "type": "user"
            },
            "summary": "The pursuit of computational efficiency has driven the adoption of\nlow-precision formats for training transformer models. However, this progress\nis often hindered by notorious training instabilities. This paper provides the\nfirst mechanistic explanation for a long-standing and unresolved failure case\nwhere training with flash attention in low-precision settings leads to\ncatastrophic loss explosions. Our in-depth analysis reveals that the failure is\nnot a random artifact but caused by two intertwined phenomena: the emergence of\nsimilar low-rank representations within the attention mechanism and the\ncompounding effect of biased rounding errors inherent in low-precision\narithmetic. We demonstrate how these factors create a vicious cycle of error\naccumulation that corrupts weight updates, ultimately derailing the training\ndynamics. To validate our findings, we introduce a minimal modification to the\nflash attention that mitigates the bias in rounding errors. This simple change\nstabilizes the training process, confirming our analysis and offering a\npractical solution to this persistent problem.",
            "upvotes": 17,
            "discussionId": "68e7159d7ae125f9582e6918",
            "githubRepo": "https://github.com/ucker/why-low-precision-training-fails",
            "ai_summary": "Low-precision training of transformer models with flash attention suffers from catastrophic loss explosions due to low-rank representations and biased rounding errors, which are addressed by a minimal modification to the flash attention mechanism.",
            "ai_keywords": [
                "low-precision formats",
                "transformer models",
                "flash attention",
                "training instabilities",
                "low-rank representations",
                "biased rounding errors",
                "error accumulation",
                "weight updates",
                "training dynamics"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "64cc8e9b214a472dd85e7e1d",
                "name": "THU1911",
                "fullname": "Tsinghua University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61f8e5934a8e5a275b2b3e5a/oKO6FK_rTzzPHXihicZou.jpeg"
            }
        },
        "publishedAt": "2025-10-05T10:01:24.000Z",
        "title": "Why Low-Precision Transformer Training Fails: An Analysis on Flash\n  Attention",
        "summary": "The pursuit of computational efficiency has driven the adoption of\nlow-precision formats for training transformer models. However, this progress\nis often hindered by notorious training instabilities. This paper provides the\nfirst mechanistic explanation for a long-standing and unresolved failure case\nwhere training with flash attention in low-precision settings leads to\ncatastrophic loss explosions. Our in-depth analysis reveals that the failure is\nnot a random artifact but caused by two intertwined phenomena: the emergence of\nsimilar low-rank representations within the attention mechanism and the\ncompounding effect of biased rounding errors inherent in low-precision\narithmetic. We demonstrate how these factors create a vicious cycle of error\naccumulation that corrupts weight updates, ultimately derailing the training\ndynamics. To validate our findings, we introduce a minimal modification to the\nflash attention that mitigates the bias in rounding errors. This simple change\nstabilizes the training process, confirming our analysis and offering a\npractical solution to this persistent problem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04212.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66cfda1386e3d910d7d88550",
            "avatarUrl": "/avatars/e6c6537c0dc0a5e1b9d4f7fd62af60af.svg",
            "fullname": "Haiquan Qiu",
            "name": "huggingaaaaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "64cc8e9b214a472dd85e7e1d",
            "name": "THU1911",
            "fullname": "Tsinghua University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61f8e5934a8e5a275b2b3e5a/oKO6FK_rTzzPHXihicZou.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.04204",
            "authors": [
                {
                    "_id": "68e511f0f9af2f6567eab8f8",
                    "user": {
                        "_id": "64912976b95c3f0a1e6233cb",
                        "avatarUrl": "/avatars/3e338c5eef2514055ed98ae6141a5d1a.svg",
                        "isPro": false,
                        "fullname": "Zhengyang Tang",
                        "user": "tangzhy",
                        "type": "user"
                    },
                    "name": "Zhengyang Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-08T11:49:10.098Z",
                    "hidden": false
                },
                {
                    "_id": "68e511f0f9af2f6567eab8f9",
                    "name": "Zihan Ye",
                    "hidden": false
                },
                {
                    "_id": "68e511f0f9af2f6567eab8fa",
                    "name": "Chenyu Huang",
                    "hidden": false
                },
                {
                    "_id": "68e511f0f9af2f6567eab8fb",
                    "name": "Xuhan Huang",
                    "hidden": false
                },
                {
                    "_id": "68e511f0f9af2f6567eab8fc",
                    "name": "Chengpeng Li",
                    "hidden": false
                },
                {
                    "_id": "68e511f0f9af2f6567eab8fd",
                    "name": "Sihang Li",
                    "hidden": false
                },
                {
                    "_id": "68e511f0f9af2f6567eab8fe",
                    "name": "Guanhua Chen",
                    "hidden": false
                },
                {
                    "_id": "68e511f0f9af2f6567eab8ff",
                    "name": "Ming Yan",
                    "hidden": false
                },
                {
                    "_id": "68e511f0f9af2f6567eab900",
                    "name": "Zizhuo Wang",
                    "hidden": false
                },
                {
                    "_id": "68e511f0f9af2f6567eab901",
                    "name": "Hongyuan Zha",
                    "hidden": false
                },
                {
                    "_id": "68e511f0f9af2f6567eab902",
                    "name": "Dayiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68e511f0f9af2f6567eab903",
                    "name": "Benyou Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-05T13:38:31.000Z",
            "submittedOnDailyAt": "2025-10-09T11:14:58.258Z",
            "title": "CALM Before the STORM: Unlocking Native Reasoning for Optimization\n  Modeling",
            "submittedOnDailyBy": {
                "_id": "64912976b95c3f0a1e6233cb",
                "avatarUrl": "/avatars/3e338c5eef2514055ed98ae6141a5d1a.svg",
                "isPro": false,
                "fullname": "Zhengyang Tang",
                "user": "tangzhy",
                "type": "user"
            },
            "summary": "Large Reasoning Models (LRMs) have demonstrated strong capabilities in\ncomplex multi-step reasoning, opening new opportunities for automating\noptimization modeling. However, existing domain adaptation methods, originally\ndesigned for earlier instruction-tuned models, often fail to exploit the\nadvanced reasoning patterns of modern LRMs -- In particular, we show that\ndirect fine-tuning on traditional non-reflective datasets leads to\nlimited gains. To fully leverage LRMs' inherent reasoning abilities, we propose\nCALM (Corrective Adaptation with Lightweight Modification), a\nframework that progressively refines LRMs within their native reasoning modes\nfor optimization modeling tasks. In CALM, an expert intervener identifies\nreasoning flaws and provides concise corrective hints, which the LRM\nincorporates to produce improved reasoning trajectories. These interventions\nmodify fewer than 2.6\\% of generated tokens, but generate high-quality data for\nsoft adaptation through supervised fine-tuning. The adapted model is then\nfurther improved through reinforcement learning. Building on CALM, we develop\nSTORM (Smart Thinking Optimization Reasoning Model), a\n4B-parameter LRM that achieves a new state-of-the-art average accuracy of\n68.9\\% across five popular optimization modeling benchmarks, matching the\nperformance of a 671B LRM. These results demonstrate that dynamic, hint-based\ndata synthesis both preserves and amplifies the native reasoning patterns of\nmodern LRMs, offering a more effective and scalable path towards expert-level\nperformance on challenging optimization modeling tasks.",
            "upvotes": 17,
            "discussionId": "68e511f0f9af2f6567eab904",
            "ai_summary": "CALM framework uses expert interventions to refine LRM reasoning for optimization tasks, achieving high accuracy with fewer modifications compared to traditional methods.",
            "ai_keywords": [
                "Large Reasoning Models",
                "domain adaptation",
                "fine-tuning",
                "non-reflective datasets",
                "CALM",
                "corrective adaptation",
                "lightweight modification",
                "reasoning flaws",
                "corrective hints",
                "reasoning trajectories",
                "soft adaptation",
                "supervised fine-tuning",
                "reinforcement learning",
                "STORM",
                "Smart Thinking Optimization Reasoning Model"
            ]
        },
        "publishedAt": "2025-10-05T09:38:31.000Z",
        "title": "CALM Before the STORM: Unlocking Native Reasoning for Optimization\n  Modeling",
        "summary": "Large Reasoning Models (LRMs) have demonstrated strong capabilities in\ncomplex multi-step reasoning, opening new opportunities for automating\noptimization modeling. However, existing domain adaptation methods, originally\ndesigned for earlier instruction-tuned models, often fail to exploit the\nadvanced reasoning patterns of modern LRMs -- In particular, we show that\ndirect fine-tuning on traditional non-reflective datasets leads to\nlimited gains. To fully leverage LRMs' inherent reasoning abilities, we propose\nCALM (Corrective Adaptation with Lightweight Modification), a\nframework that progressively refines LRMs within their native reasoning modes\nfor optimization modeling tasks. In CALM, an expert intervener identifies\nreasoning flaws and provides concise corrective hints, which the LRM\nincorporates to produce improved reasoning trajectories. These interventions\nmodify fewer than 2.6\\% of generated tokens, but generate high-quality data for\nsoft adaptation through supervised fine-tuning. The adapted model is then\nfurther improved through reinforcement learning. Building on CALM, we develop\nSTORM (Smart Thinking Optimization Reasoning Model), a\n4B-parameter LRM that achieves a new state-of-the-art average accuracy of\n68.9\\% across five popular optimization modeling benchmarks, matching the\nperformance of a 671B LRM. These results demonstrate that dynamic, hint-based\ndata synthesis both preserves and amplifies the native reasoning patterns of\nmodern LRMs, offering a more effective and scalable path towards expert-level\nperformance on challenging optimization modeling tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04204.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64912976b95c3f0a1e6233cb",
            "avatarUrl": "/avatars/3e338c5eef2514055ed98ae6141a5d1a.svg",
            "fullname": "Zhengyang Tang",
            "name": "tangzhy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.07019",
            "authors": [
                {
                    "_id": "68e772e67ae125f9582e6bf3",
                    "name": "Jusen Du",
                    "hidden": false
                },
                {
                    "_id": "68e772e67ae125f9582e6bf4",
                    "name": "Jiaxi Hu",
                    "hidden": false
                },
                {
                    "_id": "68e772e67ae125f9582e6bf5",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e772e67ae125f9582e6bf6",
                    "user": {
                        "_id": "6246bb33da617c00b48e4d92",
                        "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
                        "isPro": false,
                        "fullname": "Weigao Sun",
                        "user": "weigao266",
                        "type": "user"
                    },
                    "name": "Weigao Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:12.592Z",
                    "hidden": false
                },
                {
                    "_id": "68e772e67ae125f9582e6bf7",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T13:44:57.000Z",
            "submittedOnDailyAt": "2025-10-09T07:07:57.631Z",
            "title": "Native Hybrid Attention for Efficient Sequence Modeling",
            "submittedOnDailyBy": {
                "_id": "6246bb33da617c00b48e4d92",
                "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
                "isPro": false,
                "fullname": "Weigao Sun",
                "user": "weigao266",
                "type": "user"
            },
            "summary": "Transformers excel at sequence modeling but face quadratic complexity, while\nlinear attention offers improved efficiency but often compromises recall\naccuracy over long contexts. In this work, we introduce Native Hybrid Attention\n(NHA), a novel hybrid architecture of linear and full attention that integrates\nboth intra \\& inter-layer hybridization into a unified layer design. NHA\nmaintains long-term context in key-value slots updated by a linear RNN, and\naugments them with short-term tokens from a sliding window. A single\nsoftmax attention operation is then applied over all keys and values,\nenabling per-token and per-head context-dependent weighting without requiring\nadditional fusion parameters. The inter-layer behavior is controlled through a\nsingle hyperparameter, the sliding window size, which allows smooth adjustment\nbetween purely linear and full attention while keeping all layers structurally\nuniform. Experimental results show that NHA surpasses Transformers and other\nhybrid baselines on recall-intensive and commonsense reasoning tasks.\nFurthermore, pretrained LLMs can be structurally hybridized with NHA, achieving\ncompetitive accuracy while delivering significant efficiency gains. Code is\navailable at https://github.com/JusenD/NHA.",
            "upvotes": 15,
            "discussionId": "68e772e67ae125f9582e6bf8",
            "projectPage": "https://github.com/JusenD/NHA",
            "githubRepo": "https://github.com/JusenD/NHA",
            "ai_summary": "Native Hybrid Attention (NHA) combines linear and full attention mechanisms to maintain long-term context while improving efficiency, outperforming Transformers in recall-intensive tasks and offering efficiency gains in pretrained LLMs.",
            "ai_keywords": [
                "Native Hybrid Attention",
                "NHA",
                "linear attention",
                "full attention",
                "intra-layer hybridization",
                "inter-layer hybridization",
                "key-value slots",
                "linear RNN",
                "sliding window",
                "softmax attention",
                "recall-intensive tasks",
                "commonsense reasoning tasks",
                "pretrained LLMs"
            ],
            "githubStars": 17
        },
        "publishedAt": "2025-10-08T09:44:57.000Z",
        "title": "Native Hybrid Attention for Efficient Sequence Modeling",
        "summary": "Transformers excel at sequence modeling but face quadratic complexity, while\nlinear attention offers improved efficiency but often compromises recall\naccuracy over long contexts. In this work, we introduce Native Hybrid Attention\n(NHA), a novel hybrid architecture of linear and full attention that integrates\nboth intra \\& inter-layer hybridization into a unified layer design. NHA\nmaintains long-term context in key-value slots updated by a linear RNN, and\naugments them with short-term tokens from a sliding window. A single\nsoftmax attention operation is then applied over all keys and values,\nenabling per-token and per-head context-dependent weighting without requiring\nadditional fusion parameters. The inter-layer behavior is controlled through a\nsingle hyperparameter, the sliding window size, which allows smooth adjustment\nbetween purely linear and full attention while keeping all layers structurally\nuniform. Experimental results show that NHA surpasses Transformers and other\nhybrid baselines on recall-intensive and commonsense reasoning tasks.\nFurthermore, pretrained LLMs can be structurally hybridized with NHA, achieving\ncompetitive accuracy while delivering significant efficiency gains. Code is\navailable at https://github.com/JusenD/NHA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07019.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6246bb33da617c00b48e4d92",
            "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
            "fullname": "Weigao Sun",
            "name": "weigao266",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.06751",
            "authors": [
                {
                    "_id": "68e722757ae125f9582e69d9",
                    "user": {
                        "_id": "67d5848f179ad2756600eca3",
                        "avatarUrl": "/avatars/158168a753271b6e024e1fbdf52c9e73.svg",
                        "isPro": false,
                        "fullname": "Junhan ZHU",
                        "user": "Alrightlone",
                        "type": "user"
                    },
                    "name": "Junhan Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:50:12.218Z",
                    "hidden": false
                },
                {
                    "_id": "68e722757ae125f9582e69da",
                    "name": "Hesong Wang",
                    "hidden": false
                },
                {
                    "_id": "68e722757ae125f9582e69db",
                    "name": "Mingluo Su",
                    "hidden": false
                },
                {
                    "_id": "68e722757ae125f9582e69dc",
                    "name": "Zefang Wang",
                    "hidden": false
                },
                {
                    "_id": "68e722757ae125f9582e69dd",
                    "user": {
                        "_id": "62b624f3b52bef716e248fd7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b624f3b52bef716e248fd7/AllcccKH-eBWduA8KVnOQ.png",
                        "isPro": false,
                        "fullname": "Huan Wang",
                        "user": "Huan-WhoRegisteredMyName",
                        "type": "user"
                    },
                    "name": "Huan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:50:08.795Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T08:19:15.000Z",
            "submittedOnDailyAt": "2025-10-09T01:20:49.892Z",
            "title": "OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot",
            "submittedOnDailyBy": {
                "_id": "67a4a26d5e65aa63c6d30e68",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a4a26d5e65aa63c6d30e68/GtodlJGw-_IL2DTXQTucz.jpeg",
                "isPro": false,
                "fullname": "Sicheng Feng",
                "user": "FSCCS",
                "type": "user"
            },
            "summary": "Large-scale text-to-image diffusion models, while powerful, suffer from\nprohibitive computational cost. Existing one-shot network pruning methods can\nhardly be directly applied to them due to the iterative denoising nature of\ndiffusion models. To bridge the gap, this paper presents OBS-Diff, a novel\none-shot pruning framework that enables accurate and training-free compression\nof large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff\nrevitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex\narchitectures of modern diffusion models and supporting diverse pruning\ngranularity, including unstructured, N:M semi-structured, and structured (MHA\nheads and FFN neurons) sparsity; (ii) To align the pruning criteria with the\niterative dynamics of the diffusion process, by examining the problem from an\nerror-accumulation perspective, we propose a novel timestep-aware Hessian\nconstruction that incorporates a logarithmic-decrease weighting scheme,\nassigning greater importance to earlier timesteps to mitigate potential error\naccumulation; (iii) Furthermore, a computationally efficient group-wise\nsequential pruning strategy is proposed to amortize the expensive calibration\nprocess. Extensive experiments show that OBS-Diff achieves state-of-the-art\none-shot pruning for diffusion models, delivering inference acceleration with\nminimal degradation in visual quality.",
            "upvotes": 14,
            "discussionId": "68e722767ae125f9582e69de",
            "projectPage": "https://alrightlone.github.io/OBS-Diff-Webpage/",
            "githubRepo": "https://github.com/Alrightlone/OBS-Diff",
            "ai_summary": "OBS-Diff is a novel one-shot pruning framework that compresses large-scale text-to-image diffusion models with minimal quality loss and significant inference acceleration.",
            "ai_keywords": [
                "diffusion models",
                "one-shot pruning",
                "OBS-Diff",
                "Optimal Brain Surgeon",
                "unstructured",
                "N:M semi-structured",
                "structured",
                "MHA heads",
                "FFN neurons",
                "timestep-aware Hessian",
                "logarithmic-decrease weighting scheme",
                "group-wise sequential pruning"
            ],
            "githubStars": 24,
            "organization": {
                "_id": "643cb10025681c3afab0f1a6",
                "name": "Westlake-University",
                "fullname": "Westlake University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/SQRCHUyjPRyqdtV3um42X.jpeg"
            }
        },
        "publishedAt": "2025-10-08T04:19:15.000Z",
        "title": "OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot",
        "summary": "Large-scale text-to-image diffusion models, while powerful, suffer from\nprohibitive computational cost. Existing one-shot network pruning methods can\nhardly be directly applied to them due to the iterative denoising nature of\ndiffusion models. To bridge the gap, this paper presents OBS-Diff, a novel\none-shot pruning framework that enables accurate and training-free compression\nof large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff\nrevitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex\narchitectures of modern diffusion models and supporting diverse pruning\ngranularity, including unstructured, N:M semi-structured, and structured (MHA\nheads and FFN neurons) sparsity; (ii) To align the pruning criteria with the\niterative dynamics of the diffusion process, by examining the problem from an\nerror-accumulation perspective, we propose a novel timestep-aware Hessian\nconstruction that incorporates a logarithmic-decrease weighting scheme,\nassigning greater importance to earlier timesteps to mitigate potential error\naccumulation; (iii) Furthermore, a computationally efficient group-wise\nsequential pruning strategy is proposed to amortize the expensive calibration\nprocess. Extensive experiments show that OBS-Diff achieves state-of-the-art\none-shot pruning for diffusion models, delivering inference acceleration with\nminimal degradation in visual quality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06751.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67a4a26d5e65aa63c6d30e68",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a4a26d5e65aa63c6d30e68/GtodlJGw-_IL2DTXQTucz.jpeg",
            "fullname": "Sicheng Feng",
            "name": "FSCCS",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "643cb10025681c3afab0f1a6",
            "name": "Westlake-University",
            "fullname": "Westlake University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/SQRCHUyjPRyqdtV3um42X.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.06557",
            "authors": [
                {
                    "_id": "68e75ca67ae125f9582e6b3d",
                    "user": {
                        "_id": "6197f5213619d373ad154f73",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6197f5213619d373ad154f73/PzMlx-n984x03Ldy34Xdi.jpeg",
                        "isPro": false,
                        "fullname": "Milad Aghajohari",
                        "user": "miladink",
                        "type": "user"
                    },
                    "name": "Milad Aghajohari",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:16.729Z",
                    "hidden": false
                },
                {
                    "_id": "68e75ca67ae125f9582e6b3e",
                    "user": {
                        "_id": "6303d7dd0547362a22a59ac2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6303d7dd0547362a22a59ac2/iHV2VKEUXxoM47aNnzn-P.jpeg",
                        "isPro": false,
                        "fullname": "Kamran Chitsaz",
                        "user": "kmchiti",
                        "type": "user"
                    },
                    "name": "Kamran Chitsaz",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:20.598Z",
                    "hidden": false
                },
                {
                    "_id": "68e75ca67ae125f9582e6b3f",
                    "user": {
                        "_id": "63458f12d54fb141dedac508",
                        "avatarUrl": "/avatars/3946fb9c23d1cd24037770cc0a3489bf.svg",
                        "isPro": false,
                        "fullname": "Amirhossein Kazemnejad",
                        "user": "kazemnejad",
                        "type": "user"
                    },
                    "name": "Amirhossein Kazemnejad",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:22.726Z",
                    "hidden": false
                },
                {
                    "_id": "68e75ca67ae125f9582e6b40",
                    "name": "Sarath Chandar",
                    "hidden": false
                },
                {
                    "_id": "68e75ca67ae125f9582e6b41",
                    "name": "Alessandro Sordoni",
                    "hidden": false
                },
                {
                    "_id": "68e75ca67ae125f9582e6b42",
                    "name": "Aaron Courville",
                    "hidden": false
                },
                {
                    "_id": "68e75ca67ae125f9582e6b43",
                    "name": "Siva Reddy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T01:18:13.000Z",
            "submittedOnDailyAt": "2025-10-09T08:50:40.014Z",
            "title": "The Markovian Thinker",
            "submittedOnDailyBy": {
                "_id": "63458f12d54fb141dedac508",
                "avatarUrl": "/avatars/3946fb9c23d1cd24037770cc0a3489bf.svg",
                "isPro": false,
                "fullname": "Amirhossein Kazemnejad",
                "user": "kazemnejad",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) has recently become a strong recipe for training\nreasoning LLMs that produce long chains of thought (LongCoT). Yet the standard\nRL \"thinking environment\", where the state is the prompt plus all prior\nreasoning tokens, makes the state unbounded and forces attention-based policies\nto pay quadratic compute as thoughts lengthen. We revisit the environment\nitself. We propose Markovian Thinking, a paradigm in which the policy advances\nreasoning while conditioning on a constant-size state, decoupling thinking\nlength from context size. As an immediate consequence this yields linear\ncompute with constant memory. We instantiate this idea with Delethink, an RL\nenvironment that structures reasoning into fixed-size chunks. Within each\nchunk, the model thinks as usual; at the boundary, the environment resets the\ncontext and reinitializes the prompt with a short carryover. Through RL, the\npolicy learns to write a textual state near the end of each chunk sufficient\nfor seamless continuation of reasoning after reset. Trained in this\nenvironment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up\nto 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget.\nWith test-time scaling, Delethink continues to improve where LongCoT plateaus.\nThe effect of linear compute is substantial: we empirically estimate at 96K\naverage thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink.\nAnalysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B)\noften sample Markovian traces zero-shot across diverse benchmarks, providing\npositive samples that make RL effective at scale. Our results show that\nredesigning the thinking environment is a powerful lever: it enables very long\nreasoning without quadratic overhead and opens a path toward efficient,\nscalable reasoning LLMs.",
            "upvotes": 14,
            "discussionId": "68e75ca67ae125f9582e6b44",
            "githubRepo": "https://github.com/McGill-NLP/the-markovian-thinker",
            "ai_summary": "Markovian Thinking, implemented in Delethink, enables efficient and scalable reinforcement learning for long-chain-of-thought reasoning in LLMs by decoupling thinking length from context size, resulting in linear compute and constant memory usage.",
            "ai_keywords": [
                "Reinforcement learning",
                "reasoning LLMs",
                "LongCoT",
                "Markovian Thinking",
                "attention-based policies",
                "Delethink",
                "fixed-size chunks",
                "R1-Distill",
                "test-time scaling",
                "RL initialization",
                "reasoning models"
            ],
            "githubStars": 13,
            "organization": {
                "_id": "63571619a482286f0dc9cce9",
                "name": "MilaQuebec",
                "fullname": "Mila  Quebec Artificial Intelligence Institute",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1666651643213-605238cb46a8a9249390a28e.png"
            }
        },
        "publishedAt": "2025-10-07T21:18:13.000Z",
        "title": "The Markovian Thinker",
        "summary": "Reinforcement learning (RL) has recently become a strong recipe for training\nreasoning LLMs that produce long chains of thought (LongCoT). Yet the standard\nRL \"thinking environment\", where the state is the prompt plus all prior\nreasoning tokens, makes the state unbounded and forces attention-based policies\nto pay quadratic compute as thoughts lengthen. We revisit the environment\nitself. We propose Markovian Thinking, a paradigm in which the policy advances\nreasoning while conditioning on a constant-size state, decoupling thinking\nlength from context size. As an immediate consequence this yields linear\ncompute with constant memory. We instantiate this idea with Delethink, an RL\nenvironment that structures reasoning into fixed-size chunks. Within each\nchunk, the model thinks as usual; at the boundary, the environment resets the\ncontext and reinitializes the prompt with a short carryover. Through RL, the\npolicy learns to write a textual state near the end of each chunk sufficient\nfor seamless continuation of reasoning after reset. Trained in this\nenvironment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up\nto 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget.\nWith test-time scaling, Delethink continues to improve where LongCoT plateaus.\nThe effect of linear compute is substantial: we empirically estimate at 96K\naverage thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink.\nAnalysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B)\noften sample Markovian traces zero-shot across diverse benchmarks, providing\npositive samples that make RL effective at scale. Our results show that\nredesigning the thinking environment is a powerful lever: it enables very long\nreasoning without quadratic overhead and opens a path toward efficient,\nscalable reasoning LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06557.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63458f12d54fb141dedac508",
            "avatarUrl": "/avatars/3946fb9c23d1cd24037770cc0a3489bf.svg",
            "fullname": "Amirhossein Kazemnejad",
            "name": "kazemnejad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "63571619a482286f0dc9cce9",
            "name": "MilaQuebec",
            "fullname": "Mila  Quebec Artificial Intelligence Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1666651643213-605238cb46a8a9249390a28e.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.05862",
            "authors": [
                {
                    "_id": "68e64fcf975ac4c405ef2263",
                    "name": "Zecheng Tang",
                    "hidden": false
                },
                {
                    "_id": "68e64fcf975ac4c405ef2264",
                    "name": "Baibei Ji",
                    "hidden": false
                },
                {
                    "_id": "68e64fcf975ac4c405ef2265",
                    "name": "Juntao Li",
                    "hidden": false
                },
                {
                    "_id": "68e64fcf975ac4c405ef2266",
                    "name": "Lijun Wu",
                    "hidden": false
                },
                {
                    "_id": "68e64fcf975ac4c405ef2267",
                    "name": "Haijia Gui",
                    "hidden": false
                },
                {
                    "_id": "68e64fcf975ac4c405ef2268",
                    "name": "Min Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-07T12:32:23.000Z",
            "submittedOnDailyAt": "2025-10-09T00:40:43.016Z",
            "title": "Revisiting Long-context Modeling from Context Denoising Perspective",
            "submittedOnDailyBy": {
                "_id": "64096ef79e9f790c905b846d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
                "isPro": false,
                "fullname": "Zecheng Tang",
                "user": "ZetangForward",
                "type": "user"
            },
            "summary": "Long-context models (LCMs) have demonstrated great potential in processing\nlong sequences, facilitating many real-world applications. The success of LCMs\ncan be attributed to their ability to locate implicit critical information\nwithin the context for further prediction. However, recent research reveals\nthat LCMs are often susceptible to contextual noise, i.e., irrelevant tokens,\nthat can mislead model attention. In this paper, we conduct a fine-grained\nanalysis of the context noise and propose an effective metric, the Integrated\nGradient (IG) score, to detect and quantify the noise information within the\ncontext. Our findings reveal that even simple mitigation of detected context\nnoise can substantially boost the model's attention on critical tokens and\nbenefit subsequent predictions. Building on this insight, we propose Context\nDenoising Training (CDT), a straightforward yet effective training strategy\nthat improves attention on critical tokens while reinforcing their influence on\nmodel predictions. Extensive experiments across four tasks, under both context\nwindow scaling and long-context alignment settings, demonstrate the superiority\nof CDT. Notably, when trained with CDT, an open-source 8B model can achieve\nperformance (50.92) comparable to GPT-4o (51.00).",
            "upvotes": 13,
            "discussionId": "68e64fcf975ac4c405ef2269",
            "githubRepo": "https://github.com/LCM-Lab/context-denoising-training",
            "ai_summary": "Context Denoising Training (CDT) improves long-context models' performance by mitigating contextual noise and enhancing attention on critical tokens.",
            "ai_keywords": [
                "long-context models",
                "contextual noise",
                "Integrated Gradient (IG) score",
                "Context Denoising Training (CDT)",
                "context window scaling",
                "long-context alignment"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "61f8e653129c9ff1b911293d",
                "name": "SUDA",
                "fullname": "Soochow University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643701821817-61f8e5934a8e5a275b2b3e5a.png"
            }
        },
        "publishedAt": "2025-10-07T08:32:23.000Z",
        "title": "Revisiting Long-context Modeling from Context Denoising Perspective",
        "summary": "Long-context models (LCMs) have demonstrated great potential in processing\nlong sequences, facilitating many real-world applications. The success of LCMs\ncan be attributed to their ability to locate implicit critical information\nwithin the context for further prediction. However, recent research reveals\nthat LCMs are often susceptible to contextual noise, i.e., irrelevant tokens,\nthat can mislead model attention. In this paper, we conduct a fine-grained\nanalysis of the context noise and propose an effective metric, the Integrated\nGradient (IG) score, to detect and quantify the noise information within the\ncontext. Our findings reveal that even simple mitigation of detected context\nnoise can substantially boost the model's attention on critical tokens and\nbenefit subsequent predictions. Building on this insight, we propose Context\nDenoising Training (CDT), a straightforward yet effective training strategy\nthat improves attention on critical tokens while reinforcing their influence on\nmodel predictions. Extensive experiments across four tasks, under both context\nwindow scaling and long-context alignment settings, demonstrate the superiority\nof CDT. Notably, when trained with CDT, an open-source 8B model can achieve\nperformance (50.92) comparable to GPT-4o (51.00).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05862.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64096ef79e9f790c905b846d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
            "fullname": "Zecheng Tang",
            "name": "ZetangForward",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "61f8e653129c9ff1b911293d",
            "name": "SUDA",
            "fullname": "Soochow University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643701821817-61f8e5934a8e5a275b2b3e5a.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07238",
            "authors": [
                {
                    "_id": "68e728707ae125f9582e69f4",
                    "user": {
                        "_id": "645efecd7c6bff857734ceeb",
                        "avatarUrl": "/avatars/d1cda8b0b943165fb2de6769d28a6787.svg",
                        "isPro": false,
                        "fullname": "XunyiJiang",
                        "user": "Coooori",
                        "type": "user"
                    },
                    "name": "Xunyi Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:50:03.984Z",
                    "hidden": false
                },
                {
                    "_id": "68e728707ae125f9582e69f5",
                    "name": "Dingyi Chang",
                    "hidden": false
                },
                {
                    "_id": "68e728707ae125f9582e69f6",
                    "name": "Julian McAuley",
                    "hidden": false
                },
                {
                    "_id": "68e728707ae125f9582e69f7",
                    "user": {
                        "_id": "6190ab805ca89a28e9f66873",
                        "avatarUrl": "/avatars/3c7ecc398fbf851acd2a132e947a92be.svg",
                        "isPro": false,
                        "fullname": "Xin Xu",
                        "user": "XinXuNLPer",
                        "type": "user"
                    },
                    "name": "Xin Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:50:06.149Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T17:06:07.000Z",
            "submittedOnDailyAt": "2025-10-09T01:51:19.017Z",
            "title": "When Benchmarks Age: Temporal Misalignment through Large Language Model\n  Factuality Evaluation",
            "submittedOnDailyBy": {
                "_id": "6190ab805ca89a28e9f66873",
                "avatarUrl": "/avatars/3c7ecc398fbf851acd2a132e947a92be.svg",
                "isPro": false,
                "fullname": "Xin Xu",
                "user": "XinXuNLPer",
                "type": "user"
            },
            "summary": "The rapid evolution of large language models (LLMs) and the real world has\noutpaced the static nature of widely used evaluation benchmarks, raising\nconcerns about their reliability for evaluating LLM factuality. While\nsubstantial works continue to rely on the popular but old benchmarks, their\ntemporal misalignment with real-world facts and modern LLMs, and their effects\non LLM factuality evaluation remain underexplored. Therefore, in this work, we\npresent a systematic investigation of this issue by examining five popular\nfactuality benchmarks and eight LLMs released across different years. An\nup-to-date fact retrieval pipeline and three metrics are tailored to quantify\nbenchmark aging and its impact on LLM factuality evaluation. Experimental\nresults and analysis illustrate that a considerable portion of samples in the\nwidely used factuality benchmarks are outdated, leading to unreliable\nassessments of LLM factuality. We hope our work can provide a testbed to assess\nthe reliability of a benchmark for LLM factuality evaluation and inspire more\nresearch on the benchmark aging issue. Codes are available in\nhttps://github.com/JiangXunyi/BenchAge.",
            "upvotes": 12,
            "discussionId": "68e728717ae125f9582e69f8",
            "githubRepo": "https://github.com/JiangXunyi/BenchAge",
            "ai_summary": "Research investigates the aging of factuality benchmarks and its impact on evaluating the factuality of large language models, revealing significant unreliability due to outdated samples.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "factuality benchmarks",
                "fact retrieval pipeline",
                "benchmark aging",
                "LLM factuality evaluation"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "65af44a1637e10fba942ed0c",
                "name": "McAuley-Lab",
                "fullname": "McAuley-Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64daab70c38427829daf5958/OWlh6vciWnY_MeyM099wZ.png"
            }
        },
        "publishedAt": "2025-10-08T13:06:07.000Z",
        "title": "When Benchmarks Age: Temporal Misalignment through Large Language Model\n  Factuality Evaluation",
        "summary": "The rapid evolution of large language models (LLMs) and the real world has\noutpaced the static nature of widely used evaluation benchmarks, raising\nconcerns about their reliability for evaluating LLM factuality. While\nsubstantial works continue to rely on the popular but old benchmarks, their\ntemporal misalignment with real-world facts and modern LLMs, and their effects\non LLM factuality evaluation remain underexplored. Therefore, in this work, we\npresent a systematic investigation of this issue by examining five popular\nfactuality benchmarks and eight LLMs released across different years. An\nup-to-date fact retrieval pipeline and three metrics are tailored to quantify\nbenchmark aging and its impact on LLM factuality evaluation. Experimental\nresults and analysis illustrate that a considerable portion of samples in the\nwidely used factuality benchmarks are outdated, leading to unreliable\nassessments of LLM factuality. We hope our work can provide a testbed to assess\nthe reliability of a benchmark for LLM factuality evaluation and inspire more\nresearch on the benchmark aging issue. Codes are available in\nhttps://github.com/JiangXunyi/BenchAge.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07238.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6190ab805ca89a28e9f66873",
            "avatarUrl": "/avatars/3c7ecc398fbf851acd2a132e947a92be.svg",
            "fullname": "Xin Xu",
            "name": "XinXuNLPer",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "65af44a1637e10fba942ed0c",
            "name": "McAuley-Lab",
            "fullname": "McAuley-Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64daab70c38427829daf5958/OWlh6vciWnY_MeyM099wZ.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.07143",
            "authors": [
                {
                    "_id": "68e750887ae125f9582e6a98",
                    "name": "Chenfei Liao",
                    "hidden": false
                },
                {
                    "_id": "68e750887ae125f9582e6a99",
                    "name": "Wensong Wang",
                    "hidden": false
                },
                {
                    "_id": "68e750887ae125f9582e6a9a",
                    "name": "Zichen Wen",
                    "hidden": false
                },
                {
                    "_id": "68e750887ae125f9582e6a9b",
                    "name": "Xu Zheng",
                    "hidden": false
                },
                {
                    "_id": "68e750887ae125f9582e6a9c",
                    "name": "Yiyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68e750887ae125f9582e6a9d",
                    "name": "Haocong He",
                    "hidden": false
                },
                {
                    "_id": "68e750887ae125f9582e6a9e",
                    "name": "Yuanhuiyi Lyu",
                    "hidden": false
                },
                {
                    "_id": "68e750887ae125f9582e6a9f",
                    "name": "Lutao Jiang",
                    "hidden": false
                },
                {
                    "_id": "68e750887ae125f9582e6aa0",
                    "name": "Xin Zou",
                    "hidden": false
                },
                {
                    "_id": "68e750887ae125f9582e6aa1",
                    "name": "Yuqian Fu",
                    "hidden": false
                },
                {
                    "_id": "68e750887ae125f9582e6aa2",
                    "name": "Bin Ren",
                    "hidden": false
                },
                {
                    "_id": "68e750887ae125f9582e6aa3",
                    "name": "Linfeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e750887ae125f9582e6aa4",
                    "name": "Xuming Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T15:44:28.000Z",
            "submittedOnDailyAt": "2025-10-09T04:38:23.634Z",
            "title": "Are We Using the Right Benchmark: An Evaluation Framework for Visual\n  Token Compression Methods",
            "submittedOnDailyBy": {
                "_id": "6806464ed918f6d2fee2bc8b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6806464ed918f6d2fee2bc8b/rgpG2oO0m6PT0KltCF_Wf.jpeg",
                "isPro": false,
                "fullname": "Chenfei Liao",
                "user": "Chenfei-Liao",
                "type": "user"
            },
            "summary": "Recent endeavors to accelerate inference in Multimodal Large Language Models\n(MLLMs) have primarily focused on visual token compression. The effectiveness\nof these methods is typically assessed by measuring the accuracy drop on\nestablished benchmarks, comparing model performance before and after\ncompression. However, these benchmarks are originally designed to assess the\nperception and reasoning capabilities of MLLMs, rather than to evaluate\ncompression techniques. As a result, directly applying them to visual token\ncompression introduces a task mismatch. Strikingly, our investigation reveals\nthat simple image downsampling consistently outperforms many advanced\ncompression methods across multiple widely used benchmarks. Through extensive\nexperiments, we make the following observations: (i) Current benchmarks are\nnoisy for the visual token compression task. (ii) Down-sampling is able to\nserve as a data filter to evaluate the difficulty of samples in the visual\ntoken compression task. Motivated by these findings, we introduce VTC-Bench, an\nevaluation framework that incorporates a data filtering mechanism to denoise\nexisting benchmarks, thereby enabling fairer and more accurate assessment of\nvisual token compression methods. All data and code are available at\nhttps://github.com/Chenfei-Liao/VTC-Bench.",
            "upvotes": 10,
            "discussionId": "68e750887ae125f9582e6aa5",
            "ai_summary": "VTC-Bench is introduced to provide a fair evaluation framework for visual token compression by incorporating a data filtering mechanism to denoise existing benchmarks.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "visual token compression",
                "image downsampling",
                "VTC-Bench",
                "data filtering mechanism"
            ]
        },
        "publishedAt": "2025-10-08T11:44:28.000Z",
        "title": "Are We Using the Right Benchmark: An Evaluation Framework for Visual\n  Token Compression Methods",
        "summary": "Recent endeavors to accelerate inference in Multimodal Large Language Models\n(MLLMs) have primarily focused on visual token compression. The effectiveness\nof these methods is typically assessed by measuring the accuracy drop on\nestablished benchmarks, comparing model performance before and after\ncompression. However, these benchmarks are originally designed to assess the\nperception and reasoning capabilities of MLLMs, rather than to evaluate\ncompression techniques. As a result, directly applying them to visual token\ncompression introduces a task mismatch. Strikingly, our investigation reveals\nthat simple image downsampling consistently outperforms many advanced\ncompression methods across multiple widely used benchmarks. Through extensive\nexperiments, we make the following observations: (i) Current benchmarks are\nnoisy for the visual token compression task. (ii) Down-sampling is able to\nserve as a data filter to evaluate the difficulty of samples in the visual\ntoken compression task. Motivated by these findings, we introduce VTC-Bench, an\nevaluation framework that incorporates a data filtering mechanism to denoise\nexisting benchmarks, thereby enabling fairer and more accurate assessment of\nvisual token compression methods. All data and code are available at\nhttps://github.com/Chenfei-Liao/VTC-Bench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07143.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6806464ed918f6d2fee2bc8b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6806464ed918f6d2fee2bc8b/rgpG2oO0m6PT0KltCF_Wf.jpeg",
            "fullname": "Chenfei Liao",
            "name": "Chenfei-Liao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.05057",
            "authors": [
                {
                    "_id": "68e753a77ae125f9582e6ad4",
                    "user": {
                        "_id": "652e25d2e647b0ee0a024f26",
                        "avatarUrl": "/avatars/b5c65cf6c8d0ddc9b8ef0226e0295d56.svg",
                        "isPro": false,
                        "fullname": "Mingyu Liu",
                        "user": "MingyuLiu",
                        "type": "user"
                    },
                    "name": "Mingyu Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:49:44.012Z",
                    "hidden": false
                },
                {
                    "_id": "68e753a77ae125f9582e6ad5",
                    "name": "Jiuhe Shu",
                    "hidden": false
                },
                {
                    "_id": "68e753a77ae125f9582e6ad6",
                    "name": "Hui Chen",
                    "hidden": false
                },
                {
                    "_id": "68e753a77ae125f9582e6ad7",
                    "name": "Zeju Li",
                    "hidden": false
                },
                {
                    "_id": "68e753a77ae125f9582e6ad8",
                    "name": "Canyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68e753a77ae125f9582e6ad9",
                    "name": "Jiange Yang",
                    "hidden": false
                },
                {
                    "_id": "68e753a77ae125f9582e6ada",
                    "name": "Shenyuan Gao",
                    "hidden": false
                },
                {
                    "_id": "68e753a77ae125f9582e6adb",
                    "name": "Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "68e753a77ae125f9582e6adc",
                    "name": "Chunhua Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-06T17:37:24.000Z",
            "submittedOnDailyAt": "2025-10-09T04:50:11.977Z",
            "title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact\n  State Representation",
            "submittedOnDailyBy": {
                "_id": "652e25d2e647b0ee0a024f26",
                "avatarUrl": "/avatars/b5c65cf6c8d0ddc9b8ef0226e0295d56.svg",
                "isPro": false,
                "fullname": "Mingyu Liu",
                "user": "MingyuLiu",
                "type": "user"
            },
            "summary": "A fundamental challenge in embodied intelligence is developing expressive and\ncompact state representations for efficient world modeling and decision making.\nHowever, existing methods often fail to achieve this balance, yielding\nrepresentations that are either overly redundant or lacking in task-critical\ninformation. We propose an unsupervised approach that learns a highly\ncompressed two-token state representation using a lightweight encoder and a\npre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong\ngenerative prior. Our representation is efficient, interpretable, and\nintegrates seamlessly into existing VLA-based models, improving performance by\n14.3% on LIBERO and 30% in real-world task success with minimal inference\noverhead. More importantly, we find that the difference between these tokens,\nobtained via latent interpolation, naturally serves as a highly effective\nlatent action, which can be further decoded into executable robot actions. This\nemergent capability reveals that our representation captures structured\ndynamics without explicit supervision. We name our method StaMo for its ability\nto learn generalizable robotic Motion from compact State representation, which\nis encoded from static images, challenging the prevalent dependence to learning\nlatent action on complex architectures and video data. The resulting latent\nactions also enhance policy co-training, outperforming prior methods by 10.4%\nwith improved interpretability. Moreover, our approach scales effectively\nacross diverse data sources, including real-world robot data, simulation, and\nhuman egocentric video.",
            "upvotes": 10,
            "discussionId": "68e753a87ae125f9582e6add",
            "ai_summary": "An unsupervised method learns a compact state representation using a lightweight encoder and Diffusion Transformer decoder, improving robotic performance and enabling latent action decoding from static images.",
            "ai_keywords": [
                "Diffusion Transformer",
                "DiT",
                "VLA-based models",
                "LIBERO",
                "latent interpolation",
                "latent action",
                "StaMo",
                "policy co-training",
                "human egocentric video"
            ],
            "organization": {
                "_id": "6345aadf5efccdc07f1365a5",
                "name": "ZhejiangUniversity",
                "fullname": "Zhejiang University"
            }
        },
        "publishedAt": "2025-10-06T13:37:24.000Z",
        "title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact\n  State Representation",
        "summary": "A fundamental challenge in embodied intelligence is developing expressive and\ncompact state representations for efficient world modeling and decision making.\nHowever, existing methods often fail to achieve this balance, yielding\nrepresentations that are either overly redundant or lacking in task-critical\ninformation. We propose an unsupervised approach that learns a highly\ncompressed two-token state representation using a lightweight encoder and a\npre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong\ngenerative prior. Our representation is efficient, interpretable, and\nintegrates seamlessly into existing VLA-based models, improving performance by\n14.3% on LIBERO and 30% in real-world task success with minimal inference\noverhead. More importantly, we find that the difference between these tokens,\nobtained via latent interpolation, naturally serves as a highly effective\nlatent action, which can be further decoded into executable robot actions. This\nemergent capability reveals that our representation captures structured\ndynamics without explicit supervision. We name our method StaMo for its ability\nto learn generalizable robotic Motion from compact State representation, which\nis encoded from static images, challenging the prevalent dependence to learning\nlatent action on complex architectures and video data. The resulting latent\nactions also enhance policy co-training, outperforming prior methods by 10.4%\nwith improved interpretability. Moreover, our approach scales effectively\nacross diverse data sources, including real-world robot data, simulation, and\nhuman egocentric video.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05057.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "652e25d2e647b0ee0a024f26",
            "avatarUrl": "/avatars/b5c65cf6c8d0ddc9b8ef0226e0295d56.svg",
            "fullname": "Mingyu Liu",
            "name": "MingyuLiu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "organization": {
            "_id": "6345aadf5efccdc07f1365a5",
            "name": "ZhejiangUniversity",
            "fullname": "Zhejiang University"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.01954",
            "authors": [
                {
                    "_id": "68e64046975ac4c405ef2237",
                    "name": "Yongyi Su",
                    "hidden": false
                },
                {
                    "_id": "68e64046975ac4c405ef2238",
                    "user": {
                        "_id": "662203bcb3bb05d1d34ee728",
                        "avatarUrl": "/avatars/f7cad8e4cc04bdcc931bacf5bf802554.svg",
                        "isPro": false,
                        "fullname": "Zhanghaojie",
                        "user": "eehaojiezhang",
                        "type": "user"
                    },
                    "name": "Haojie Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:49:09.173Z",
                    "hidden": false
                },
                {
                    "_id": "68e64046975ac4c405ef2239",
                    "name": "Shijie Li",
                    "hidden": false
                },
                {
                    "_id": "68e64046975ac4c405ef223a",
                    "name": "Nanqing Liu",
                    "hidden": false
                },
                {
                    "_id": "68e64046975ac4c405ef223b",
                    "name": "Jingyi Liao",
                    "hidden": false
                },
                {
                    "_id": "68e64046975ac4c405ef223c",
                    "name": "Junyi Pan",
                    "hidden": false
                },
                {
                    "_id": "68e64046975ac4c405ef223d",
                    "name": "Yuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68e64046975ac4c405ef223e",
                    "name": "Xiaofen Xing",
                    "hidden": false
                },
                {
                    "_id": "68e64046975ac4c405ef223f",
                    "name": "Chong Sun",
                    "hidden": false
                },
                {
                    "_id": "68e64046975ac4c405ef2240",
                    "name": "Chen Li",
                    "hidden": false
                },
                {
                    "_id": "68e64046975ac4c405ef2241",
                    "name": "Nancy F. Chen",
                    "hidden": false
                },
                {
                    "_id": "68e64046975ac4c405ef2242",
                    "name": "Shuicheng Yan",
                    "hidden": false
                },
                {
                    "_id": "68e64046975ac4c405ef2243",
                    "name": "Xulei Yang",
                    "hidden": false
                },
                {
                    "_id": "68e64046975ac4c405ef2244",
                    "name": "Xun Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T12:23:57.000Z",
            "submittedOnDailyAt": "2025-10-09T05:56:50.657Z",
            "title": "Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in\n  MLLMs",
            "submittedOnDailyBy": {
                "_id": "64a0ed5ed5374ca472cfb0ac",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
                "isPro": false,
                "fullname": "ZhimingMa",
                "user": "JimmyMa99",
                "type": "user"
            },
            "summary": "Multimodal large language models (MLLMs) have advanced rapidly in recent\nyears. However, existing approaches for vision tasks often rely on indirect\nrepresentations, such as generating coordinates as text for detection, which\nlimits performance and prevents dense prediction tasks like segmentation. To\novercome these challenges, we introduce Patch-as-Decodable Token (PaDT), a\nunified paradigm that enables MLLMs to directly generate both textual and\ndiverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),\nderived from visual patch embeddings of query images and interleaved seamlessly\nwith LLM's output textual tokens. A lightweight decoder then transforms LLM's\noutputs into detection, segmentation, and grounding predictions. Unlike prior\nmethods, PaDT processes VRTs independently at each forward pass and dynamically\nexpands the embedding table, thus improving localization and differentiation\namong similar objects. We further tailor a training strategy for PaDT by\nrandomly selecting VRTs for supervised fine-tuning and introducing a robust\nper-token cross-entropy loss. Our empirical studies across four visual\nperception and understanding tasks suggest PaDT consistently achieving\nstate-of-the-art performance, even compared with significantly larger MLLM\nmodels. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.",
            "upvotes": 9,
            "discussionId": "68e64046975ac4c405ef2245",
            "githubRepo": "https://github.com/Gorilla-Lab-SCUT/PaDT",
            "ai_summary": "PaDT, a unified paradigm for multimodal large language models, directly generates both textual and visual outputs, achieving state-of-the-art performance in visual perception tasks.",
            "ai_keywords": [
                "Multimodal large language models",
                "Patch-as-Decodable Token",
                "PaDT",
                "Visual Reference Tokens",
                "VRTs",
                "visual patch embeddings",
                "lightweight decoder",
                "detection",
                "segmentation",
                "grounding predictions",
                "supervised fine-tuning",
                "per-token cross-entropy loss"
            ],
            "githubStars": 12
        },
        "publishedAt": "2025-10-02T08:23:57.000Z",
        "title": "Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in\n  MLLMs",
        "summary": "Multimodal large language models (MLLMs) have advanced rapidly in recent\nyears. However, existing approaches for vision tasks often rely on indirect\nrepresentations, such as generating coordinates as text for detection, which\nlimits performance and prevents dense prediction tasks like segmentation. To\novercome these challenges, we introduce Patch-as-Decodable Token (PaDT), a\nunified paradigm that enables MLLMs to directly generate both textual and\ndiverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),\nderived from visual patch embeddings of query images and interleaved seamlessly\nwith LLM's output textual tokens. A lightweight decoder then transforms LLM's\noutputs into detection, segmentation, and grounding predictions. Unlike prior\nmethods, PaDT processes VRTs independently at each forward pass and dynamically\nexpands the embedding table, thus improving localization and differentiation\namong similar objects. We further tailor a training strategy for PaDT by\nrandomly selecting VRTs for supervised fine-tuning and introducing a robust\nper-token cross-entropy loss. Our empirical studies across four visual\nperception and understanding tasks suggest PaDT consistently achieving\nstate-of-the-art performance, even compared with significantly larger MLLM\nmodels. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01954.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a0ed5ed5374ca472cfb0ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
            "fullname": "ZhimingMa",
            "name": "JimmyMa99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.06783",
            "authors": [
                {
                    "_id": "68e71bfa7ae125f9582e6991",
                    "name": "Akshit Singh",
                    "hidden": false
                },
                {
                    "_id": "68e71bfa7ae125f9582e6992",
                    "name": "Shyam Marjit",
                    "hidden": false
                },
                {
                    "_id": "68e71bfa7ae125f9582e6993",
                    "name": "Wei Lin",
                    "hidden": false
                },
                {
                    "_id": "68e71bfa7ae125f9582e6994",
                    "name": "Paul Gavrikov",
                    "hidden": false
                },
                {
                    "_id": "68e71bfa7ae125f9582e6995",
                    "name": "Serena Yeung-Levy",
                    "hidden": false
                },
                {
                    "_id": "68e71bfa7ae125f9582e6996",
                    "name": "Hilde Kuehne",
                    "hidden": false
                },
                {
                    "_id": "68e71bfa7ae125f9582e6997",
                    "name": "Rogerio Feris",
                    "hidden": false
                },
                {
                    "_id": "68e71bfa7ae125f9582e6998",
                    "name": "Sivan Doveh",
                    "hidden": false
                },
                {
                    "_id": "68e71bfa7ae125f9582e6999",
                    "name": "James Glass",
                    "hidden": false
                },
                {
                    "_id": "68e71bfa7ae125f9582e699a",
                    "name": "M. Jehanzeb Mirza",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T09:10:31.000Z",
            "submittedOnDailyAt": "2025-10-09T00:51:16.689Z",
            "title": "TTRV: Test-Time Reinforcement Learning for Vision Language Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Existing methods for extracting reward signals in Reinforcement Learning\ntypically rely on labeled data and dedicated training splits, a setup that\ncontrasts with how humans learn directly from their environment. In this work,\nwe propose TTRV to enhance vision language understanding by adapting the model\non the fly at inference time, without the need for any labeled data.\nConcretely, we enhance the Group Relative Policy Optimization (GRPO) framework\nby designing rewards based on the frequency of the base model's output, while\ninferring on each test sample multiple times. Further, we also propose to\ncontrol the diversity of the model's output by simultaneously rewarding the\nmodel for obtaining low entropy of the output empirical distribution. Our\napproach delivers consistent gains across both object recognition and visual\nquestion answering (VQA), with improvements of up to 52.4% and 29.8%,\nrespectively, and average boosts of 24.6% and 10.0% across 16\ndatasets.Remarkably, on image recognition, TTRV applied to InternVL 8B\nsurpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining\nhighly competitive on VQA, demonstrating that test-time reinforcement learning\ncan match or exceed the strongest proprietary models. Finally, we find many\ninteresting properties of test-time RL for VLMs: for example, even in extremely\ndata-constrained scenarios, where adaptation is performed on a single randomly\nchosen unlabeled test example, TTRV still yields non-trivial improvements of up\nto 5.5% in recognition tasks.",
            "upvotes": 8,
            "discussionId": "68e71bfb7ae125f9582e699b",
            "projectPage": "https://akshit21112002.github.io/ttrvproject/",
            "githubRepo": "https://github.com/Akshit21112002/TTRV",
            "ai_summary": "TTRV enhances vision language understanding through test-time reinforcement learning, improving performance on object recognition and VQA without labeled data.",
            "ai_keywords": [
                "Reinforcement Learning",
                "Group Relative Policy Optimization (GRPO)",
                "entropy",
                "test-time reinforcement learning",
                "vision language models (VLMs)",
                "InternVL 8B",
                "GPT-4o"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-10-08T05:10:31.000Z",
        "title": "TTRV: Test-Time Reinforcement Learning for Vision Language Models",
        "summary": "Existing methods for extracting reward signals in Reinforcement Learning\ntypically rely on labeled data and dedicated training splits, a setup that\ncontrasts with how humans learn directly from their environment. In this work,\nwe propose TTRV to enhance vision language understanding by adapting the model\non the fly at inference time, without the need for any labeled data.\nConcretely, we enhance the Group Relative Policy Optimization (GRPO) framework\nby designing rewards based on the frequency of the base model's output, while\ninferring on each test sample multiple times. Further, we also propose to\ncontrol the diversity of the model's output by simultaneously rewarding the\nmodel for obtaining low entropy of the output empirical distribution. Our\napproach delivers consistent gains across both object recognition and visual\nquestion answering (VQA), with improvements of up to 52.4% and 29.8%,\nrespectively, and average boosts of 24.6% and 10.0% across 16\ndatasets.Remarkably, on image recognition, TTRV applied to InternVL 8B\nsurpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining\nhighly competitive on VQA, demonstrating that test-time reinforcement learning\ncan match or exceed the strongest proprietary models. Finally, we find many\ninteresting properties of test-time RL for VLMs: for example, even in extremely\ndata-constrained scenarios, where adaptation is performed on a single randomly\nchosen unlabeled test example, TTRV still yields non-trivial improvements of up\nto 5.5% in recognition tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06783.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 123
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07313",
            "authors": [
                {
                    "_id": "68e71cac7ae125f9582e699d",
                    "name": "Zezhong Qian",
                    "hidden": false
                },
                {
                    "_id": "68e71cac7ae125f9582e699e",
                    "name": "Xiaowei Chi",
                    "hidden": false
                },
                {
                    "_id": "68e71cac7ae125f9582e699f",
                    "name": "Yuming Li",
                    "hidden": false
                },
                {
                    "_id": "68e71cac7ae125f9582e69a0",
                    "name": "Shizun Wang",
                    "hidden": false
                },
                {
                    "_id": "68e71cac7ae125f9582e69a1",
                    "name": "Zhiyuan Qin",
                    "hidden": false
                },
                {
                    "_id": "68e71cac7ae125f9582e69a2",
                    "name": "Xiaozhu Ju",
                    "hidden": false
                },
                {
                    "_id": "68e71cac7ae125f9582e69a3",
                    "name": "Sirui Han",
                    "hidden": false
                },
                {
                    "_id": "68e71cac7ae125f9582e69a4",
                    "name": "Shanghang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T17:59:08.000Z",
            "submittedOnDailyAt": "2025-10-09T01:05:55.489Z",
            "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic\n  Manipulation",
            "submittedOnDailyBy": {
                "_id": "67852e5a3d49517c54365945",
                "avatarUrl": "/avatars/3ab8df5628e91bf89ac39f6fd6955c3e.svg",
                "isPro": false,
                "fullname": "Zezhong Qian",
                "user": "XuWuLingYu",
                "type": "user"
            },
            "summary": "Wrist-view observations are crucial for VLA models as they capture\nfine-grained hand-object interactions that directly enhance manipulation\nperformance. Yet large-scale datasets rarely include such recordings, resulting\nin a substantial gap between abundant anchor views and scarce wrist views.\nExisting world models cannot bridge this gap, as they require a wrist-view\nfirst frame and thus fail to generate wrist-view videos from anchor views\nalone. Amid this gap, recent visual geometry models such as VGGT emerge with\ngeometric and cross-view priors that make it possible to address extreme\nviewpoint shifts. Inspired by these insights, we propose WristWorld, the first\n4D world model that generates wrist-view videos solely from anchor views.\nWristWorld operates in two stages: (i) Reconstruction, which extends VGGT and\nincorporates our Spatial Projection Consistency (SPC) Loss to estimate\ngeometrically consistent wrist-view poses and 4D point clouds; (ii) Generation,\nwhich employs our video generation model to synthesize temporally coherent\nwrist-view videos from the reconstructed perspective. Experiments on Droid,\nCalvin, and Franka Panda demonstrate state-of-the-art video generation with\nsuperior spatial consistency, while also improving VLA performance, raising the\naverage task completion length on Calvin by 3.81% and closing 42.4% of the\nanchor-wrist view gap.",
            "upvotes": 5,
            "discussionId": "68e71cac7ae125f9582e69a5",
            "ai_summary": "WristWorld is a 4D world model that generates wrist-view videos from anchor views, improving video generation consistency and VLA performance.",
            "ai_keywords": [
                "VGGT",
                "Spatial Projection Consistency (SPC) Loss",
                "4D world model",
                "wrist-view videos",
                "anchor views",
                "geometrically consistent wrist-view poses",
                "4D point clouds",
                "temporally coherent wrist-view videos",
                "Droid",
                "Calvin",
                "Franka Panda",
                "task completion length",
                "anchor-wrist view gap"
            ],
            "organization": {
                "_id": "61dcd8e344f59573371b5cb6",
                "name": "PekingUniversity",
                "fullname": "Peking University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
            }
        },
        "publishedAt": "2025-10-08T13:59:08.000Z",
        "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic\n  Manipulation",
        "summary": "Wrist-view observations are crucial for VLA models as they capture\nfine-grained hand-object interactions that directly enhance manipulation\nperformance. Yet large-scale datasets rarely include such recordings, resulting\nin a substantial gap between abundant anchor views and scarce wrist views.\nExisting world models cannot bridge this gap, as they require a wrist-view\nfirst frame and thus fail to generate wrist-view videos from anchor views\nalone. Amid this gap, recent visual geometry models such as VGGT emerge with\ngeometric and cross-view priors that make it possible to address extreme\nviewpoint shifts. Inspired by these insights, we propose WristWorld, the first\n4D world model that generates wrist-view videos solely from anchor views.\nWristWorld operates in two stages: (i) Reconstruction, which extends VGGT and\nincorporates our Spatial Projection Consistency (SPC) Loss to estimate\ngeometrically consistent wrist-view poses and 4D point clouds; (ii) Generation,\nwhich employs our video generation model to synthesize temporally coherent\nwrist-view videos from the reconstructed perspective. Experiments on Droid,\nCalvin, and Franka Panda demonstrate state-of-the-art video generation with\nsuperior spatial consistency, while also improving VLA performance, raising the\naverage task completion length on Calvin by 3.81% and closing 42.4% of the\nanchor-wrist view gap.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07313.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67852e5a3d49517c54365945",
            "avatarUrl": "/avatars/3ab8df5628e91bf89ac39f6fd6955c3e.svg",
            "fullname": "Zezhong Qian",
            "name": "XuWuLingYu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07307",
            "authors": [
                {
                    "_id": "68e71d397ae125f9582e69a7",
                    "user": {
                        "_id": "6466e31a14e059dde8bbe4be",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
                        "isPro": false,
                        "fullname": "Rushi Qiang",
                        "user": "Jerrycool",
                        "type": "user"
                    },
                    "name": "Rushi Qiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:50:16.218Z",
                    "hidden": false
                },
                {
                    "_id": "68e71d397ae125f9582e69a8",
                    "name": "Yuchen Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68e71d397ae125f9582e69a9",
                    "name": "Anikait Singh",
                    "hidden": false
                },
                {
                    "_id": "68e71d397ae125f9582e69aa",
                    "name": "Percy Liang",
                    "hidden": false
                },
                {
                    "_id": "68e71d397ae125f9582e69ab",
                    "name": "Chao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e71d397ae125f9582e69ac",
                    "name": "Sherry Yang",
                    "hidden": false
                },
                {
                    "_id": "68e71d397ae125f9582e69ad",
                    "name": "Bo Dai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T17:57:19.000Z",
            "submittedOnDailyAt": "2025-10-09T00:57:37.094Z",
            "title": "MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline",
            "submittedOnDailyBy": {
                "_id": "6466e31a14e059dde8bbe4be",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
                "isPro": false,
                "fullname": "Rushi Qiang",
                "user": "Jerrycool",
                "type": "user"
            },
            "summary": "While Language Models (LMs) have made significant progress in automating\nmachine learning engineering (MLE), the acquisition of high-quality MLE\ntraining data is significantly constrained. Current MLE benchmarks suffer from\nlow scalability and limited applicability because they rely on static, manually\ncurated tasks, demanding extensive time and manual effort to produce. We\nintroduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw\ndatasets into competition-style MLE challenges through an efficient\ngenerate-verify-execute paradigm for scaling MLE tasks with verifiable quality,\nreal-world usability, and rich diversity. The proposed multi-agent pipeline in\nMLE-Smith drives structured task design and standardized refactoring, coupled\nwith a hybrid verification mechanism that enforces strict structural rules and\nhigh-level semantic soundness. It further validates empirical solvability and\nreal-world fidelity through interactive execution. We apply MLE-Smith to 224 of\nreal-world datasets and generate 606 tasks spanning multiple categories,\nobjectives, and modalities, demonstrating that MLE-Smith can work effectively\nacross a wide range of real-world datasets. Evaluation on the generated tasks\nshows that the performance of eight mainstream and cutting-edge LLMs on\nMLE-Smith tasks is strongly correlated with their performance on carefully\nhuman-designed tasks, highlighting the effectiveness of the MLE-Smith to\nscaling up MLE tasks, while maintaining task quality.",
            "upvotes": 5,
            "discussionId": "68e71d397ae125f9582e69ae",
            "ai_summary": "MLE-Smith automates the creation of high-quality, diverse MLE tasks from raw datasets using a multi-agent pipeline, improving scalability and maintaining task quality.",
            "ai_keywords": [
                "Language Models",
                "machine learning engineering",
                "MLE",
                "MLE-Smith",
                "multi-agent pipeline",
                "generate-verify-execute",
                "task design",
                "standardized refactoring",
                "hybrid verification",
                "structural rules",
                "semantic soundness",
                "empirical solvability",
                "real-world fidelity",
                "interactive execution"
            ],
            "organization": {
                "_id": "6808009bdd0bcdd9cfa788ae",
                "name": "MLE-Dojo",
                "fullname": "MLE-Dojo",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6466e31a14e059dde8bbe4be/B7UNsZbUycsSrRgoswiYk.png"
            }
        },
        "publishedAt": "2025-10-08T13:57:19.000Z",
        "title": "MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline",
        "summary": "While Language Models (LMs) have made significant progress in automating\nmachine learning engineering (MLE), the acquisition of high-quality MLE\ntraining data is significantly constrained. Current MLE benchmarks suffer from\nlow scalability and limited applicability because they rely on static, manually\ncurated tasks, demanding extensive time and manual effort to produce. We\nintroduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw\ndatasets into competition-style MLE challenges through an efficient\ngenerate-verify-execute paradigm for scaling MLE tasks with verifiable quality,\nreal-world usability, and rich diversity. The proposed multi-agent pipeline in\nMLE-Smith drives structured task design and standardized refactoring, coupled\nwith a hybrid verification mechanism that enforces strict structural rules and\nhigh-level semantic soundness. It further validates empirical solvability and\nreal-world fidelity through interactive execution. We apply MLE-Smith to 224 of\nreal-world datasets and generate 606 tasks spanning multiple categories,\nobjectives, and modalities, demonstrating that MLE-Smith can work effectively\nacross a wide range of real-world datasets. Evaluation on the generated tasks\nshows that the performance of eight mainstream and cutting-edge LLMs on\nMLE-Smith tasks is strongly correlated with their performance on carefully\nhuman-designed tasks, highlighting the effectiveness of the MLE-Smith to\nscaling up MLE tasks, while maintaining task quality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07307.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6466e31a14e059dde8bbe4be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
            "fullname": "Rushi Qiang",
            "name": "Jerrycool",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "6808009bdd0bcdd9cfa788ae",
            "name": "MLE-Dojo",
            "fullname": "MLE-Dojo",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6466e31a14e059dde8bbe4be/B7UNsZbUycsSrRgoswiYk.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.01982",
            "authors": [
                {
                    "_id": "68e73f567ae125f9582e6a2c",
                    "name": "Yujie Zhou",
                    "hidden": false
                },
                {
                    "_id": "68e73f567ae125f9582e6a2d",
                    "name": "Pengyang Ling",
                    "hidden": false
                },
                {
                    "_id": "68e73f567ae125f9582e6a2e",
                    "name": "Jiazi Bu",
                    "hidden": false
                },
                {
                    "_id": "68e73f567ae125f9582e6a2f",
                    "name": "Yibin Wang",
                    "hidden": false
                },
                {
                    "_id": "68e73f567ae125f9582e6a30",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/kajwuVzd4pDucSPlwghxo.png",
                        "isPro": false,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:49:50.572Z",
                    "hidden": false
                },
                {
                    "_id": "68e73f567ae125f9582e6a31",
                    "name": "Jiaqi Wang",
                    "hidden": false
                },
                {
                    "_id": "68e73f567ae125f9582e6a32",
                    "name": "Li Niu",
                    "hidden": false
                },
                {
                    "_id": "68e73f567ae125f9582e6a33",
                    "name": "Guangtao Zhai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T12:57:12.000Z",
            "submittedOnDailyAt": "2025-10-09T03:23:48.979Z",
            "title": "G^2RPO: Granular GRPO for Precise Reward in Flow Models",
            "submittedOnDailyBy": {
                "_id": "64b4eec4faa3181a5eab9c46",
                "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
                "isPro": true,
                "fullname": "Jiaqi Wang",
                "user": "myownskyW7",
                "type": "user"
            },
            "summary": "The integration of online reinforcement learning (RL) into diffusion and flow\nmodels has recently emerged as a promising approach for aligning generative\nmodels with human preferences. Stochastic sampling via Stochastic Differential\nEquations (SDE) is employed during the denoising process to generate diverse\ndenoising directions for RL exploration. While existing methods effectively\nexplore potential high-value samples, they suffer from sub-optimal preference\nalignment due to sparse and narrow reward signals. To address these challenges,\nwe propose a novel Granular-GRPO (G^2RPO ) framework that achieves\nprecise and comprehensive reward assessments of sampling directions in\nreinforcement learning of flow models. Specifically, a Singular Stochastic\nSampling strategy is introduced to support step-wise stochastic exploration\nwhile enforcing a high correlation between the reward and the injected noise,\nthereby facilitating a faithful reward for each SDE perturbation. Concurrently,\nto eliminate the bias inherent in fixed-granularity denoising, we introduce a\nMulti-Granularity Advantage Integration module that aggregates advantages\ncomputed at multiple diffusion scales, producing a more comprehensive and\nrobust evaluation of the sampling directions. Experiments conducted on various\nreward models, including both in-domain and out-of-domain evaluations,\ndemonstrate that our G^2RPO significantly outperforms existing\nflow-based GRPO baselines,highlighting its effectiveness and robustness.",
            "upvotes": 5,
            "discussionId": "68e73f567ae125f9582e6a34",
            "projectPage": "https://bujiazi.github.io/g2rpo.github.io/",
            "githubRepo": "https://github.com/bcmi/Granular-GRPO",
            "ai_summary": "A novel Granular-GRPO framework enhances reinforcement learning in diffusion and flow models by improving reward assessment and reducing bias in denoising.",
            "ai_keywords": [
                "online reinforcement learning",
                "diffusion models",
                "flow models",
                "Stochastic Differential Equations",
                "SDE",
                "denoising process",
                "Singular Stochastic Sampling",
                "Multi-Granularity Advantage Integration",
                "reward signals",
                "reward assessment",
                "sampling directions",
                "in-domain evaluations",
                "out-of-domain evaluations",
                "GRPO"
            ],
            "githubStars": 17,
            "organization": {
                "_id": "6839ab3c81ae61b2b722cad0",
                "name": "OpenIXCLab",
                "fullname": "IXCLab@Shanghai AI Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b4eec4faa3181a5eab9c46/LPNnJo6hQXiPRrjajaiil.jpeg"
            }
        },
        "publishedAt": "2025-10-02T08:57:12.000Z",
        "title": "G^2RPO: Granular GRPO for Precise Reward in Flow Models",
        "summary": "The integration of online reinforcement learning (RL) into diffusion and flow\nmodels has recently emerged as a promising approach for aligning generative\nmodels with human preferences. Stochastic sampling via Stochastic Differential\nEquations (SDE) is employed during the denoising process to generate diverse\ndenoising directions for RL exploration. While existing methods effectively\nexplore potential high-value samples, they suffer from sub-optimal preference\nalignment due to sparse and narrow reward signals. To address these challenges,\nwe propose a novel Granular-GRPO (G^2RPO ) framework that achieves\nprecise and comprehensive reward assessments of sampling directions in\nreinforcement learning of flow models. Specifically, a Singular Stochastic\nSampling strategy is introduced to support step-wise stochastic exploration\nwhile enforcing a high correlation between the reward and the injected noise,\nthereby facilitating a faithful reward for each SDE perturbation. Concurrently,\nto eliminate the bias inherent in fixed-granularity denoising, we introduce a\nMulti-Granularity Advantage Integration module that aggregates advantages\ncomputed at multiple diffusion scales, producing a more comprehensive and\nrobust evaluation of the sampling directions. Experiments conducted on various\nreward models, including both in-domain and out-of-domain evaluations,\ndemonstrate that our G^2RPO significantly outperforms existing\nflow-based GRPO baselines,highlighting its effectiveness and robustness.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01982.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b4eec4faa3181a5eab9c46",
            "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
            "fullname": "Jiaqi Wang",
            "name": "myownskyW7",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 23
        },
        "organization": {
            "_id": "6839ab3c81ae61b2b722cad0",
            "name": "OpenIXCLab",
            "fullname": "IXCLab@Shanghai AI Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b4eec4faa3181a5eab9c46/LPNnJo6hQXiPRrjajaiil.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.06953",
            "authors": [
                {
                    "_id": "68e79a187ae125f9582e6c87",
                    "user": {
                        "_id": "6635a672b0a5f86a2aeacd59",
                        "avatarUrl": "/avatars/371529d2d5a858d1c26858494ca9722e.svg",
                        "isPro": false,
                        "fullname": "Minju Gwak",
                        "user": "talzoomanzoo",
                        "type": "user"
                    },
                    "name": "Minju Gwak",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:47:55.319Z",
                    "hidden": false
                },
                {
                    "_id": "68e79a187ae125f9582e6c88",
                    "name": "Guijin Son",
                    "hidden": false
                },
                {
                    "_id": "68e79a187ae125f9582e6c89",
                    "name": "Jaehyung Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T12:37:04.000Z",
            "submittedOnDailyAt": "2025-10-09T09:49:24.522Z",
            "title": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning\n  Traces",
            "submittedOnDailyBy": {
                "_id": "6635a672b0a5f86a2aeacd59",
                "avatarUrl": "/avatars/371529d2d5a858d1c26858494ca9722e.svg",
                "isPro": false,
                "fullname": "Minju Gwak",
                "user": "talzoomanzoo",
                "type": "user"
            },
            "summary": "The Uniform Information Density (UID) hypothesis suggests that effective\ncommunication maintains a stable flow of information. In this work, we revisit\nthis principle in the context of large language model (LLM) reasoning traces,\nasking whether step-level uniformity reflects reasoning quality. To this end,\nwe propose an entropy-based stepwise information density metric and introduce\ntwo complementary measures of uniformity, local and global uniformity scores.\nAcross the experiments on six different reasoning benchmarks, we find that\nstep-level uniformity not only provides a strong theoretical lens but also\nyields practical performance benefits; for example, selecting reasoning traces\nwith more uniform information density at the step-level improves accuracy by\n10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals\nthat correct reasoning traces tend to avoid sharp information density spikes,\nwhile incorrect traces exhibit irregular information bursts. These results\ndemonstrate that UID-inspired information density measures outperform\nalternative internal signals as predictors of reasoning quality. Results\nhighlight the uniformity of the information density as a robust diagnostic and\nselection criterion for building more reliable and accurate reasoning systems.",
            "upvotes": 4,
            "discussionId": "68e79a197ae125f9582e6c8a",
            "ai_summary": "Step-level uniformity in information density, measured using entropy-based metrics, improves reasoning accuracy in large language models across various benchmarks.",
            "ai_keywords": [
                "Uniform Information Density (UID)",
                "large language model (LLM)",
                "entropy-based stepwise information density",
                "local uniformity scores",
                "global uniformity scores",
                "reasoning quality",
                "information density spikes",
                "information bursts"
            ],
            "organization": {
                "_id": "68a3c3678af8e250ce2d6ec6",
                "name": "Yonsei-Univ",
                "fullname": "Yonsei University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a3c2127c673f090655aaab/QgrlRxo7YuAsR_NXWe3eb.png"
            }
        },
        "publishedAt": "2025-10-08T08:37:04.000Z",
        "title": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning\n  Traces",
        "summary": "The Uniform Information Density (UID) hypothesis suggests that effective\ncommunication maintains a stable flow of information. In this work, we revisit\nthis principle in the context of large language model (LLM) reasoning traces,\nasking whether step-level uniformity reflects reasoning quality. To this end,\nwe propose an entropy-based stepwise information density metric and introduce\ntwo complementary measures of uniformity, local and global uniformity scores.\nAcross the experiments on six different reasoning benchmarks, we find that\nstep-level uniformity not only provides a strong theoretical lens but also\nyields practical performance benefits; for example, selecting reasoning traces\nwith more uniform information density at the step-level improves accuracy by\n10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals\nthat correct reasoning traces tend to avoid sharp information density spikes,\nwhile incorrect traces exhibit irregular information bursts. These results\ndemonstrate that UID-inspired information density measures outperform\nalternative internal signals as predictors of reasoning quality. Results\nhighlight the uniformity of the information density as a robust diagnostic and\nselection criterion for building more reliable and accurate reasoning systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06953.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6635a672b0a5f86a2aeacd59",
            "avatarUrl": "/avatars/371529d2d5a858d1c26858494ca9722e.svg",
            "fullname": "Minju Gwak",
            "name": "talzoomanzoo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "68a3c3678af8e250ce2d6ec6",
            "name": "Yonsei-Univ",
            "fullname": "Yonsei University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a3c2127c673f090655aaab/QgrlRxo7YuAsR_NXWe3eb.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.24375",
            "authors": [
                {
                    "_id": "68e722347ae125f9582e69bd",
                    "name": "Yijun Tian",
                    "hidden": false
                },
                {
                    "_id": "68e722347ae125f9582e69be",
                    "name": "Shaoyu Chen",
                    "hidden": false
                },
                {
                    "_id": "68e722347ae125f9582e69bf",
                    "name": "Zhichao Xu",
                    "hidden": false
                },
                {
                    "_id": "68e722347ae125f9582e69c0",
                    "name": "Yawei Wang",
                    "hidden": false
                },
                {
                    "_id": "68e722347ae125f9582e69c1",
                    "name": "Jinhe Bi",
                    "hidden": false
                },
                {
                    "_id": "68e722347ae125f9582e69c2",
                    "name": "Peng Han",
                    "hidden": false
                },
                {
                    "_id": "68e722347ae125f9582e69c3",
                    "name": "Wei Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T07:21:24.000Z",
            "submittedOnDailyAt": "2025-10-09T22:22:06.216Z",
            "title": "Reinforcement Mid-Training",
            "submittedOnDailyBy": {
                "_id": "655e744ea74c042364c9ac58",
                "avatarUrl": "/avatars/2ae4a379fbe5552755905adf0bcd399c.svg",
                "isPro": false,
                "fullname": "Yijun Tian",
                "user": "meettyj",
                "type": "user"
            },
            "summary": "The development of state-of-the-art large language models is commonly\nunderstood as a two-stage process involving pre-training and post-training. We\npoint out the need for an additional intermediate stage called reinforcement\nmid-training with potential for strong performance gains. In this paper, we\nformally define the problem and identify three key challenges: (1) inefficient\ntraining due to excessive reasoning steps, (2) disregard of the imbalanced\ntoken entropy distribution, and (3) underutilization of token information. To\naddress these challenges, we propose RMT, a framework for efficient, adaptive,\nand unified reinforcement mid-training with various innovative components. In\nparticular, we first introduce a dynamic token budget mechanism that constrains\nunnecessary reasoning steps and mitigates model overthinking. Next, we design a\ncurriculum-based adaptive sampling method that fosters a progressive learning\ntrajectory from easy to hard tokens. Finally, we present a dual training\nstrategy that combines reinforcement learning with next-token prediction,\nensuring targeted learning on key tokens and full exploitation of all token\ninformation. Extensive experiments demonstrate the superiority of RMT over\nstate-of-the-art methods, achieving up to +64.91% performance improvement with\nonly 21% of the reasoning length in language modeling. We also show that\ncheckpoints obtained after reinforcement mid-training can benefit the\nsubsequent post-training, yielding up to +18.76% improvement in the\nmathematical domain.",
            "upvotes": 4,
            "discussionId": "68e722357ae125f9582e69c4",
            "githubRepo": "https://github.com/Mid-Training/RMT",
            "ai_summary": "A reinforcement mid-training framework (RMT) improves large language models by addressing inefficiencies and underutilization of token information, leading to significant performance gains.",
            "ai_keywords": [
                "reinforcement mid-training",
                "dynamic token budget mechanism",
                "curriculum-based adaptive sampling",
                "dual training strategy",
                "reinforcement learning",
                "next-token prediction",
                "language modeling",
                "mathematical domain"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-09-29T03:21:24.000Z",
        "title": "Reinforcement Mid-Training",
        "summary": "The development of state-of-the-art large language models is commonly\nunderstood as a two-stage process involving pre-training and post-training. We\npoint out the need for an additional intermediate stage called reinforcement\nmid-training with potential for strong performance gains. In this paper, we\nformally define the problem and identify three key challenges: (1) inefficient\ntraining due to excessive reasoning steps, (2) disregard of the imbalanced\ntoken entropy distribution, and (3) underutilization of token information. To\naddress these challenges, we propose RMT, a framework for efficient, adaptive,\nand unified reinforcement mid-training with various innovative components. In\nparticular, we first introduce a dynamic token budget mechanism that constrains\nunnecessary reasoning steps and mitigates model overthinking. Next, we design a\ncurriculum-based adaptive sampling method that fosters a progressive learning\ntrajectory from easy to hard tokens. Finally, we present a dual training\nstrategy that combines reinforcement learning with next-token prediction,\nensuring targeted learning on key tokens and full exploitation of all token\ninformation. Extensive experiments demonstrate the superiority of RMT over\nstate-of-the-art methods, achieving up to +64.91% performance improvement with\nonly 21% of the reasoning length in language modeling. We also show that\ncheckpoints obtained after reinforcement mid-training can benefit the\nsubsequent post-training, yielding up to +18.76% improvement in the\nmathematical domain.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24375.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655e744ea74c042364c9ac58",
            "avatarUrl": "/avatars/2ae4a379fbe5552755905adf0bcd399c.svg",
            "fullname": "Yijun Tian",
            "name": "meettyj",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.06855",
            "authors": [
                {
                    "_id": "68e752627ae125f9582e6ac1",
                    "name": "Hyungrok Jung",
                    "hidden": false
                },
                {
                    "_id": "68e752627ae125f9582e6ac2",
                    "user": {
                        "_id": "636b20591340f879a2eb98d0",
                        "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
                        "isPro": false,
                        "fullname": "Daneul Kim",
                        "user": "carpedkm",
                        "type": "user"
                    },
                    "name": "Daneul Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:49:46.233Z",
                    "hidden": false
                },
                {
                    "_id": "68e752627ae125f9582e6ac3",
                    "name": "Seunggyun Lim",
                    "hidden": false
                },
                {
                    "_id": "68e752627ae125f9582e6ac4",
                    "name": "Jeany Son",
                    "hidden": false
                },
                {
                    "_id": "68e752627ae125f9582e6ac5",
                    "name": "Jonghyun Choi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T10:23:45.000Z",
            "submittedOnDailyAt": "2025-10-09T04:43:32.642Z",
            "title": "Online Generic Event Boundary Detection",
            "submittedOnDailyBy": {
                "_id": "636b20591340f879a2eb98d0",
                "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
                "isPro": false,
                "fullname": "Daneul Kim",
                "user": "carpedkm",
                "type": "user"
            },
            "summary": "Generic Event Boundary Detection (GEBD) aims to interpret long-form videos\nthrough the lens of human perception. However, current GEBD methods require\nprocessing complete video frames to make predictions, unlike humans processing\ndata online and in real-time. To bridge this gap, we introduce a new task,\nOnline Generic Event Boundary Detection (On-GEBD), aiming to detect boundaries\nof generic events immediately in streaming videos. This task faces unique\nchallenges of identifying subtle, taxonomy-free event changes in real-time,\nwithout the access to future frames. To tackle these challenges, we propose a\nnovel On-GEBD framework, Estimator, inspired by Event Segmentation Theory (EST)\nwhich explains how humans segment ongoing activity into events by leveraging\nthe discrepancies between predicted and actual information. Our framework\nconsists of two key components: the Consistent Event Anticipator (CEA), and the\nOnline Boundary Discriminator (OBD). Specifically, the CEA generates a\nprediction of the future frame reflecting current event dynamics based solely\non prior frames. Then, the OBD measures the prediction error and adaptively\nadjusts the threshold using statistical tests on past errors to capture\ndiverse, subtle event transitions. Experimental results demonstrate that\nEstimator outperforms all baselines adapted from recent online video\nunderstanding models and achieves performance comparable to prior offline-GEBD\nmethods on the Kinetics-GEBD and TAPOS datasets.",
            "upvotes": 3,
            "discussionId": "68e752627ae125f9582e6ac6",
            "projectPage": "https://carpedkm.github.io/projects/online_gebd/index.html",
            "ai_summary": "A novel framework for real-time event boundary detection in streaming videos uses prediction and error measurement to identify subtle event changes.",
            "ai_keywords": [
                "Online Generic Event Boundary Detection",
                "On-GEBD",
                "Event Segmentation Theory",
                "Consistent Event Anticipator",
                "Online Boundary Discriminator",
                "prediction error",
                "statistical tests",
                "Kinetics-GEBD",
                "TAPOS datasets"
            ]
        },
        "publishedAt": "2025-10-08T06:23:45.000Z",
        "title": "Online Generic Event Boundary Detection",
        "summary": "Generic Event Boundary Detection (GEBD) aims to interpret long-form videos\nthrough the lens of human perception. However, current GEBD methods require\nprocessing complete video frames to make predictions, unlike humans processing\ndata online and in real-time. To bridge this gap, we introduce a new task,\nOnline Generic Event Boundary Detection (On-GEBD), aiming to detect boundaries\nof generic events immediately in streaming videos. This task faces unique\nchallenges of identifying subtle, taxonomy-free event changes in real-time,\nwithout the access to future frames. To tackle these challenges, we propose a\nnovel On-GEBD framework, Estimator, inspired by Event Segmentation Theory (EST)\nwhich explains how humans segment ongoing activity into events by leveraging\nthe discrepancies between predicted and actual information. Our framework\nconsists of two key components: the Consistent Event Anticipator (CEA), and the\nOnline Boundary Discriminator (OBD). Specifically, the CEA generates a\nprediction of the future frame reflecting current event dynamics based solely\non prior frames. Then, the OBD measures the prediction error and adaptively\nadjusts the threshold using statistical tests on past errors to capture\ndiverse, subtle event transitions. Experimental results demonstrate that\nEstimator outperforms all baselines adapted from recent online video\nunderstanding models and achieves performance comparable to prior offline-GEBD\nmethods on the Kinetics-GEBD and TAPOS datasets.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06855.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636b20591340f879a2eb98d0",
            "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
            "fullname": "Daneul Kim",
            "name": "carpedkm",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.06673",
            "authors": [
                {
                    "_id": "68e75ac67ae125f9582e6b0c",
                    "user": {
                        "_id": "637c7d8a88699fba70e1e1ff",
                        "avatarUrl": "/avatars/e7f60fab6ca9ad5ecd320bd7cd51ce2e.svg",
                        "isPro": false,
                        "fullname": "yongxinzhu",
                        "user": "youngsheen",
                        "type": "user"
                    },
                    "name": "Yongxin Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:49:41.425Z",
                    "hidden": false
                },
                {
                    "_id": "68e75ac67ae125f9582e6b0d",
                    "name": "Jiawei Chen",
                    "hidden": false
                },
                {
                    "_id": "68e75ac67ae125f9582e6b0e",
                    "name": "Yuanzhe Chen",
                    "hidden": false
                },
                {
                    "_id": "68e75ac67ae125f9582e6b0f",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "68e75ac67ae125f9582e6b10",
                    "name": "Dongya Jia",
                    "hidden": false
                },
                {
                    "_id": "68e75ac67ae125f9582e6b11",
                    "name": "Jian Cong",
                    "hidden": false
                },
                {
                    "_id": "68e75ac67ae125f9582e6b12",
                    "name": "Xiaobin Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68e75ac67ae125f9582e6b13",
                    "name": "Yuping Wang",
                    "hidden": false
                },
                {
                    "_id": "68e75ac67ae125f9582e6b14",
                    "name": "Yuxuan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T05:54:46.000Z",
            "submittedOnDailyAt": "2025-10-09T05:19:41.365Z",
            "title": "Heptapod: Language Modeling on Visual Signals",
            "submittedOnDailyBy": {
                "_id": "637c7d8a88699fba70e1e1ff",
                "avatarUrl": "/avatars/e7f60fab6ca9ad5ecd320bd7cd51ce2e.svg",
                "isPro": false,
                "fullname": "yongxinzhu",
                "user": "youngsheen",
                "type": "user"
            },
            "summary": "We introduce Heptapod, an image autoregressive model that adheres to the\nfoundational principles of language modeling. Heptapod employs causal\nattention, eliminates reliance on CFG, and eschews the trend\nof semantic tokenizers. Our key innovation is next 2D distribution\nprediction: a causal Transformer with reconstruction-focused visual tokenizer,\nlearns to predict the distribution over the entire 2D spatial grid of images at\neach timestep. This learning objective unifies the sequential modeling of\nautoregressive framework with the holistic self-supervised learning of masked\nautoencoding, enabling the model to capture comprehensive image semantics via\ngenerative training. On the ImageNet generation benchmark, Heptapod achieves an\nFID of 2.70, significantly outperforming previous causal autoregressive\napproaches. We hope our work inspires a principled rethinking of language\nmodeling on visual signals and beyond.",
            "upvotes": 3,
            "discussionId": "68e75ac77ae125f9582e6b15",
            "ai_summary": "Heptapod, an image autoregressive model using causal attention and next 2D distribution prediction, achieves superior performance on ImageNet generation by combining sequential modeling with holistic self-supervised learning.",
            "ai_keywords": [
                "causal attention",
                "CFG",
                "semantic tokenizers",
                "next 2D distribution prediction",
                "causal Transformer",
                "visual tokenizer",
                "masked autoencoding",
                "FID"
            ],
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-10-08T01:54:46.000Z",
        "title": "Heptapod: Language Modeling on Visual Signals",
        "summary": "We introduce Heptapod, an image autoregressive model that adheres to the\nfoundational principles of language modeling. Heptapod employs causal\nattention, eliminates reliance on CFG, and eschews the trend\nof semantic tokenizers. Our key innovation is next 2D distribution\nprediction: a causal Transformer with reconstruction-focused visual tokenizer,\nlearns to predict the distribution over the entire 2D spatial grid of images at\neach timestep. This learning objective unifies the sequential modeling of\nautoregressive framework with the holistic self-supervised learning of masked\nautoencoding, enabling the model to capture comprehensive image semantics via\ngenerative training. On the ImageNet generation benchmark, Heptapod achieves an\nFID of 2.70, significantly outperforming previous causal autoregressive\napproaches. We hope our work inspires a principled rethinking of language\nmodeling on visual signals and beyond.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06673.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "637c7d8a88699fba70e1e1ff",
            "avatarUrl": "/avatars/e7f60fab6ca9ad5ecd320bd7cd51ce2e.svg",
            "fullname": "yongxinzhu",
            "name": "youngsheen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.05491",
            "authors": [
                {
                    "_id": "68e75b447ae125f9582e6b17",
                    "name": "Zichong Li",
                    "hidden": false
                },
                {
                    "_id": "68e75b447ae125f9582e6b18",
                    "name": "Liming Liu",
                    "hidden": false
                },
                {
                    "_id": "68e75b447ae125f9582e6b19",
                    "name": "Chen Liang",
                    "hidden": false
                },
                {
                    "_id": "68e75b447ae125f9582e6b1a",
                    "name": "Weizhu Chen",
                    "hidden": false
                },
                {
                    "_id": "68e75b447ae125f9582e6b1b",
                    "name": "Tuo Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-07T01:13:41.000Z",
            "submittedOnDailyAt": "2025-10-09T05:22:50.855Z",
            "title": "NorMuon: Making Muon more efficient and scalable",
            "submittedOnDailyBy": {
                "_id": "63e6b5e22d2c508de9001afd",
                "avatarUrl": "/avatars/43cec9e8b8d490bd259e383954846a1e.svg",
                "isPro": false,
                "fullname": "Chen Liang",
                "user": "cliang1453",
                "type": "user"
            },
            "summary": "The choice of optimizer significantly impacts the training efficiency and\ncomputational costs of large language models (LLMs). Recently, the Muon\noptimizer has demonstrated promising results by orthogonalizing parameter\nupdates, improving optimization geometry through better conditioning. Despite\nMuon's emergence as a candidate successor to Adam, the potential for jointly\nleveraging their strengths has not been systematically explored. In this work,\nwe bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an\noptimizer that synergistically combines orthogonalization with neuron-level\nadaptive learning rates. Our analysis reveals that while Muon effectively\nreduces condition numbers, the resulting updates exhibit highly non-uniform\nneuron norms, causing certain neurons to dominate the optimization process.\nNorMuon addresses this imbalance by maintaining second-order momentum\nstatistics for each neuron and applying row-wise normalization after\northogonalization, ensuring balanced parameter utilization while preserving\nMuon's conditioning benefits. To enable practical deployment at scale, we\ndevelop an efficient distributed implementation under the FSDP2 framework that\nstrategically distributes orthogonalization computations across devices.\nExperiments across multiple model scales demonstrate that NorMuon consistently\noutperforms both Adam and Muon, achieving 21.74% better training efficiency\nthan Adam and 11.31% improvement over Muon on 1.1 B pretraining setting, while\nmaintaining a comparable memory footprint to Muon. Our findings suggest that\northogonalization and adaptive learning rates are complementary rather than\ncompeting approaches, opening new avenues for optimizer design in large-scale\ndeep learning.",
            "upvotes": 3,
            "discussionId": "68e75b457ae125f9582e6b1c",
            "ai_summary": "NorMuon, a novel optimizer combining orthogonalization with neuron-level adaptive learning rates, enhances training efficiency and balances parameter utilization in large language models.",
            "ai_keywords": [
                "Muon",
                "orthogonalization",
                "optimization geometry",
                "conditioning",
                "neuron-level adaptive learning rates",
                "second-order momentum statistics",
                "row-wise normalization",
                "FSDP2",
                "training efficiency",
                "memory footprint"
            ]
        },
        "publishedAt": "2025-10-06T21:13:41.000Z",
        "title": "NorMuon: Making Muon more efficient and scalable",
        "summary": "The choice of optimizer significantly impacts the training efficiency and\ncomputational costs of large language models (LLMs). Recently, the Muon\noptimizer has demonstrated promising results by orthogonalizing parameter\nupdates, improving optimization geometry through better conditioning. Despite\nMuon's emergence as a candidate successor to Adam, the potential for jointly\nleveraging their strengths has not been systematically explored. In this work,\nwe bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an\noptimizer that synergistically combines orthogonalization with neuron-level\nadaptive learning rates. Our analysis reveals that while Muon effectively\nreduces condition numbers, the resulting updates exhibit highly non-uniform\nneuron norms, causing certain neurons to dominate the optimization process.\nNorMuon addresses this imbalance by maintaining second-order momentum\nstatistics for each neuron and applying row-wise normalization after\northogonalization, ensuring balanced parameter utilization while preserving\nMuon's conditioning benefits. To enable practical deployment at scale, we\ndevelop an efficient distributed implementation under the FSDP2 framework that\nstrategically distributes orthogonalization computations across devices.\nExperiments across multiple model scales demonstrate that NorMuon consistently\noutperforms both Adam and Muon, achieving 21.74% better training efficiency\nthan Adam and 11.31% improvement over Muon on 1.1 B pretraining setting, while\nmaintaining a comparable memory footprint to Muon. Our findings suggest that\northogonalization and adaptive learning rates are complementary rather than\ncompeting approaches, opening new avenues for optimizer design in large-scale\ndeep learning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05491.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63e6b5e22d2c508de9001afd",
            "avatarUrl": "/avatars/43cec9e8b8d490bd259e383954846a1e.svg",
            "fullname": "Chen Liang",
            "name": "cliang1453",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.04999",
            "authors": [
                {
                    "_id": "68e548b5975ac4c405ef1f19",
                    "user": {
                        "_id": "63cbb330f488db9bb3be6fe6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63cbb330f488db9bb3be6fe6/66bg2FdIg4YSsqWzP8WAd.png",
                        "isPro": false,
                        "fullname": "Nilay K. Bhatnagar",
                        "user": "nnilayy",
                        "type": "user"
                    },
                    "name": "Nilay Kumar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-08T08:01:20.349Z",
                    "hidden": false
                },
                {
                    "_id": "68e548b5975ac4c405ef1f1a",
                    "name": "Priyansh Bhandari",
                    "hidden": false
                },
                {
                    "_id": "68e548b5975ac4c405ef1f1b",
                    "name": "G. Maragatham",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-06T16:39:05.000Z",
            "submittedOnDailyAt": "2025-10-09T00:48:18.817Z",
            "title": "Bridging Text and Video Generation: A Survey",
            "submittedOnDailyBy": {
                "_id": "63cbb330f488db9bb3be6fe6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63cbb330f488db9bb3be6fe6/66bg2FdIg4YSsqWzP8WAd.png",
                "isPro": false,
                "fullname": "Nilay K. Bhatnagar",
                "user": "nnilayy",
                "type": "user"
            },
            "summary": "Text-to-video (T2V) generation technology holds potential to transform\nmultiple domains such as education, marketing, entertainment, and assistive\ntechnologies for individuals with visual or reading comprehension challenges,\nby creating coherent visual content from natural language prompts. From its\ninception, the field has advanced from adversarial models to diffusion-based\nmodels, yielding higher-fidelity, temporally consistent outputs. Yet challenges\npersist, such as alignment, long-range coherence, and computational efficiency.\nAddressing this evolving landscape, we present a comprehensive survey of\ntext-to-video generative models, tracing their development from early GANs and\nVAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these\nmodels work, what limitations they addressed in their predecessors, and why\nshifts toward new architectural paradigms were necessary to overcome challenges\nin quality, coherence, and control. We provide a systematic account of the\ndatasets, which the surveyed text-to-video models were trained and evaluated\non, and, to support reproducibility and assess the accessibility of training\nsuch models, we detail their training configurations, including their hardware\nspecifications, GPU counts, batch sizes, learning rates, optimizers, epochs,\nand other key hyperparameters. Further, we outline the evaluation metrics\ncommonly used for evaluating such models and present their performance across\nstandard benchmarks, while also discussing the limitations of these metrics and\nthe emerging shift toward more holistic, perception-aligned evaluation\nstrategies. Finally, drawing from our analysis, we outline the current open\nchallenges and propose a few promising future directions, laying out a\nperspective for future researchers to explore and build upon in advancing T2V\nresearch and applications.",
            "upvotes": 3,
            "discussionId": "68e548b5975ac4c405ef1f1c",
            "ai_summary": "A survey of text-to-video generative models from GANs and VAEs to hybrid Diffusion-Transformer architectures, detailing their development, limitations, and future directions.",
            "ai_keywords": [
                "adversarial models",
                "diffusion-based models",
                "GANs",
                "VAEs",
                "Diffusion-Transformer (DiT) architectures",
                "datasets",
                "training configurations",
                "evaluation metrics",
                "perception-aligned evaluation strategies"
            ]
        },
        "publishedAt": "2025-10-06T12:39:05.000Z",
        "title": "Bridging Text and Video Generation: A Survey",
        "summary": "Text-to-video (T2V) generation technology holds potential to transform\nmultiple domains such as education, marketing, entertainment, and assistive\ntechnologies for individuals with visual or reading comprehension challenges,\nby creating coherent visual content from natural language prompts. From its\ninception, the field has advanced from adversarial models to diffusion-based\nmodels, yielding higher-fidelity, temporally consistent outputs. Yet challenges\npersist, such as alignment, long-range coherence, and computational efficiency.\nAddressing this evolving landscape, we present a comprehensive survey of\ntext-to-video generative models, tracing their development from early GANs and\nVAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these\nmodels work, what limitations they addressed in their predecessors, and why\nshifts toward new architectural paradigms were necessary to overcome challenges\nin quality, coherence, and control. We provide a systematic account of the\ndatasets, which the surveyed text-to-video models were trained and evaluated\non, and, to support reproducibility and assess the accessibility of training\nsuch models, we detail their training configurations, including their hardware\nspecifications, GPU counts, batch sizes, learning rates, optimizers, epochs,\nand other key hyperparameters. Further, we outline the evaluation metrics\ncommonly used for evaluating such models and present their performance across\nstandard benchmarks, while also discussing the limitations of these metrics and\nthe emerging shift toward more holistic, perception-aligned evaluation\nstrategies. Finally, drawing from our analysis, we outline the current open\nchallenges and propose a few promising future directions, laying out a\nperspective for future researchers to explore and build upon in advancing T2V\nresearch and applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04999.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63cbb330f488db9bb3be6fe6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63cbb330f488db9bb3be6fe6/66bg2FdIg4YSsqWzP8WAd.png",
            "fullname": "Nilay K. Bhatnagar",
            "name": "nnilayy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.06261",
            "authors": [
                {
                    "_id": "68e722727ae125f9582e69c6",
                    "name": "Zhanke Zhou",
                    "hidden": false
                },
                {
                    "_id": "68e722727ae125f9582e69c7",
                    "name": "Chentao Cao",
                    "hidden": false
                },
                {
                    "_id": "68e722727ae125f9582e69c8",
                    "name": "Xiao Feng",
                    "hidden": false
                },
                {
                    "_id": "68e722727ae125f9582e69c9",
                    "name": "Xuan Li",
                    "hidden": false
                },
                {
                    "_id": "68e722727ae125f9582e69ca",
                    "name": "Zongze Li",
                    "hidden": false
                },
                {
                    "_id": "68e722727ae125f9582e69cb",
                    "name": "Xiangyu Lu",
                    "hidden": false
                },
                {
                    "_id": "68e722727ae125f9582e69cc",
                    "name": "Jiangchao Yao",
                    "hidden": false
                },
                {
                    "_id": "68e722727ae125f9582e69cd",
                    "name": "Weikai Huang",
                    "hidden": false
                },
                {
                    "_id": "68e722727ae125f9582e69ce",
                    "name": "Linrui Xu",
                    "hidden": false
                },
                {
                    "_id": "68e722727ae125f9582e69cf",
                    "name": "Tian Cheng",
                    "hidden": false
                },
                {
                    "_id": "68e722727ae125f9582e69d0",
                    "name": "Guanyu Jiang",
                    "hidden": false
                },
                {
                    "_id": "68e722727ae125f9582e69d1",
                    "name": "Yiming Zheng",
                    "hidden": false
                },
                {
                    "_id": "68e722727ae125f9582e69d2",
                    "name": "Brando Miranda",
                    "hidden": false
                },
                {
                    "_id": "68e722727ae125f9582e69d3",
                    "name": "Tongliang Liu",
                    "hidden": false
                },
                {
                    "_id": "68e722727ae125f9582e69d4",
                    "name": "Sanmi Koyejo",
                    "hidden": false
                },
                {
                    "_id": "68e722727ae125f9582e69d5",
                    "name": "Masashi Sugiyama",
                    "hidden": false
                },
                {
                    "_id": "68e722727ae125f9582e69d6",
                    "name": "Bo Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-05T15:42:24.000Z",
            "submittedOnDailyAt": "2025-10-09T01:18:36.760Z",
            "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into\n  a Self-Evolving System for Deep Agentic Reasoning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present AlphaApollo, a self-evolving agentic reasoning system that aims to\naddress two bottlenecks in foundation model (FM) reasoning-limited\nmodel-intrinsic capacity and unreliable test-time iteration. AlphaApollo\norchestrates multiple models with professional tools to enable deliberate,\nverifiable reasoning. It couples (i) a computation tool (Python with numerical\nand symbolic libraries) and (ii) a retrieval tool (task-relevant external\ninformation) to execute exact calculations and ground decisions. The system\nfurther supports multi-round, multi-model solution evolution via a shared state\nmap that records candidates, executable checks, and feedback for iterative\nrefinement. In evaluations on AIME 2024/2025 across multiple models,\nAlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32\nfor Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for\nLlama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool\ncalls are successfully executed, with consistent outperformance of non-tool\nbaselines, thereby lifting the capability ceiling of FMs. More empirical\nresults and implementation details will be updated at\nhttps://github.com/tmlr-group/AlphaApollo.",
            "upvotes": 3,
            "discussionId": "68e722727ae125f9582e69d7",
            "githubRepo": "https://github.com/tmlr-group/AlphaApollo",
            "ai_summary": "AlphaApollo, a self-evolving reasoning system, enhances foundation model performance through tool integration and iterative refinement, achieving significant improvements in accuracy and pass rates.",
            "ai_keywords": [
                "self-evolving",
                "agentic reasoning system",
                "foundation model",
                "model-intrinsic capacity",
                "test-time iteration",
                "computation tool",
                "retrieval tool",
                "shared state map",
                "iterative refinement",
                "AIME",
                "Qwen2.5-14B-Instruct",
                "Llama-3.3-70B-Instruct",
                "tool-use analysis"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-10-05T11:42:24.000Z",
        "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into\n  a Self-Evolving System for Deep Agentic Reasoning",
        "summary": "We present AlphaApollo, a self-evolving agentic reasoning system that aims to\naddress two bottlenecks in foundation model (FM) reasoning-limited\nmodel-intrinsic capacity and unreliable test-time iteration. AlphaApollo\norchestrates multiple models with professional tools to enable deliberate,\nverifiable reasoning. It couples (i) a computation tool (Python with numerical\nand symbolic libraries) and (ii) a retrieval tool (task-relevant external\ninformation) to execute exact calculations and ground decisions. The system\nfurther supports multi-round, multi-model solution evolution via a shared state\nmap that records candidates, executable checks, and feedback for iterative\nrefinement. In evaluations on AIME 2024/2025 across multiple models,\nAlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32\nfor Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for\nLlama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool\ncalls are successfully executed, with consistent outperformance of non-tool\nbaselines, thereby lifting the capability ceiling of FMs. More empirical\nresults and implementation details will be updated at\nhttps://github.com/tmlr-group/AlphaApollo.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06261.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 123
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07041",
            "authors": [
                {
                    "_id": "68e71b637ae125f9582e6985",
                    "name": "Fenghe Tang",
                    "hidden": false
                },
                {
                    "_id": "68e71b637ae125f9582e6986",
                    "name": "Chengqi Dong",
                    "hidden": false
                },
                {
                    "_id": "68e71b637ae125f9582e6987",
                    "name": "Wenxin Ma",
                    "hidden": false
                },
                {
                    "_id": "68e71b637ae125f9582e6988",
                    "name": "Zikang Xu",
                    "hidden": false
                },
                {
                    "_id": "68e71b637ae125f9582e6989",
                    "name": "Heqin Zhu",
                    "hidden": false
                },
                {
                    "_id": "68e71b637ae125f9582e698a",
                    "name": "Zihang Jiang",
                    "hidden": false
                },
                {
                    "_id": "68e71b637ae125f9582e698b",
                    "name": "Rongsheng Wang",
                    "hidden": false
                },
                {
                    "_id": "68e71b637ae125f9582e698c",
                    "name": "Yuhao Wang",
                    "hidden": false
                },
                {
                    "_id": "68e71b637ae125f9582e698d",
                    "name": "Chenxu Wu",
                    "hidden": false
                },
                {
                    "_id": "68e71b637ae125f9582e698e",
                    "name": "Shaohua Kevin Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T14:06:17.000Z",
            "submittedOnDailyAt": "2025-10-09T00:48:27.252Z",
            "title": "U-Bench: A Comprehensive Understanding of U-Net through 100-Variant\n  Benchmarking",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Over the past decade, U-Net has been the dominant architecture in medical\nimage segmentation, leading to the development of thousands of U-shaped\nvariants. Despite its widespread adoption, there is still no comprehensive\nbenchmark to systematically evaluate their performance and utility, largely\nbecause of insufficient statistical validation and limited consideration of\nefficiency and generalization across diverse datasets. To bridge this gap, we\npresent U-Bench, the first large-scale, statistically rigorous benchmark that\nevaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our\ncontributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates\nmodels along three key dimensions: statistical robustness, zero-shot\ngeneralization, and computational efficiency. We introduce a novel metric,\nU-Score, which jointly captures the performance-efficiency trade-off, offering\na deployment-oriented perspective on model progress. (2) Systematic Analysis\nand Model Selection Guidance: We summarize key findings from the large-scale\nevaluation and systematically analyze the impact of dataset characteristics and\narchitectural paradigms on model performance. Based on these insights, we\npropose a model advisor agent to guide researchers in selecting the most\nsuitable models for specific datasets and tasks. (3) Public Availability: We\nprovide all code, models, protocols, and weights, enabling the community to\nreproduce our results and extend the benchmark with future methods. In summary,\nU-Bench not only exposes gaps in previous evaluations but also establishes a\nfoundation for fair, reproducible, and practically relevant benchmarking in the\nnext decade of U-Net-based segmentation models. The project can be accessed at:\nhttps://fenghetan9.github.io/ubench. Code is available at:\nhttps://github.com/FengheTan9/U-Bench.",
            "upvotes": 2,
            "discussionId": "68e71b637ae125f9582e698f",
            "projectPage": "https://fenghetan9.github.io/ubench/",
            "githubRepo": "https://github.com/FengheTan9/U-Bench",
            "ai_summary": "U-Bench is a comprehensive benchmark evaluating 100 U-Net variants across 28 datasets and 10 imaging modalities, focusing on statistical robustness, zero-shot generalization, and computational efficiency.",
            "ai_keywords": [
                "U-Net",
                "U-Bench",
                "U-Score",
                "zero-shot generalization",
                "computational efficiency",
                "statistical robustness",
                "model advisor agent"
            ],
            "githubStars": 28
        },
        "publishedAt": "2025-10-08T10:06:17.000Z",
        "title": "U-Bench: A Comprehensive Understanding of U-Net through 100-Variant\n  Benchmarking",
        "summary": "Over the past decade, U-Net has been the dominant architecture in medical\nimage segmentation, leading to the development of thousands of U-shaped\nvariants. Despite its widespread adoption, there is still no comprehensive\nbenchmark to systematically evaluate their performance and utility, largely\nbecause of insufficient statistical validation and limited consideration of\nefficiency and generalization across diverse datasets. To bridge this gap, we\npresent U-Bench, the first large-scale, statistically rigorous benchmark that\nevaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our\ncontributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates\nmodels along three key dimensions: statistical robustness, zero-shot\ngeneralization, and computational efficiency. We introduce a novel metric,\nU-Score, which jointly captures the performance-efficiency trade-off, offering\na deployment-oriented perspective on model progress. (2) Systematic Analysis\nand Model Selection Guidance: We summarize key findings from the large-scale\nevaluation and systematically analyze the impact of dataset characteristics and\narchitectural paradigms on model performance. Based on these insights, we\npropose a model advisor agent to guide researchers in selecting the most\nsuitable models for specific datasets and tasks. (3) Public Availability: We\nprovide all code, models, protocols, and weights, enabling the community to\nreproduce our results and extend the benchmark with future methods. In summary,\nU-Bench not only exposes gaps in previous evaluations but also establishes a\nfoundation for fair, reproducible, and practically relevant benchmarking in the\nnext decade of U-Net-based segmentation models. The project can be accessed at:\nhttps://fenghetan9.github.io/ubench. Code is available at:\nhttps://github.com/FengheTan9/U-Bench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07041.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 123
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07037",
            "authors": [
                {
                    "_id": "68e746a67ae125f9582e6a78",
                    "user": {
                        "_id": "66e1425c919f283fbd7dfb5e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e1425c919f283fbd7dfb5e/lfX3gDTLKvwX5QgAivq4S.png",
                        "isPro": false,
                        "fullname": "Rajvee Sheth",
                        "user": "RajveeSheth",
                        "type": "user"
                    },
                    "name": "Rajvee Sheth",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:49:48.441Z",
                    "hidden": false
                },
                {
                    "_id": "68e746a67ae125f9582e6a79",
                    "user": {
                        "_id": "665454167473f3c257f89e0f",
                        "avatarUrl": "/avatars/c48bc493a9e0248a828cb899b77a7efb.svg",
                        "isPro": false,
                        "fullname": "Samridhi Raj Sinha",
                        "user": "samiee2213",
                        "type": "user"
                    },
                    "name": "Samridhi Raj Sinha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:24.743Z",
                    "hidden": false
                },
                {
                    "_id": "68e746a67ae125f9582e6a7a",
                    "name": "Mahavir Patil",
                    "hidden": false
                },
                {
                    "_id": "68e746a67ae125f9582e6a7b",
                    "name": "Himanshu Beniwal",
                    "hidden": false
                },
                {
                    "_id": "68e746a67ae125f9582e6a7c",
                    "name": "Mayank Singh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T14:04:14.000Z",
            "submittedOnDailyAt": "2025-10-09T03:54:04.288Z",
            "title": "Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era\n  of Large Language Models",
            "submittedOnDailyBy": {
                "_id": "66e1425c919f283fbd7dfb5e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e1425c919f283fbd7dfb5e/lfX3gDTLKvwX5QgAivq4S.png",
                "isPro": false,
                "fullname": "Rajvee Sheth",
                "user": "RajveeSheth",
                "type": "user"
            },
            "summary": "Code-switching (CSW), the alternation of languages and scripts within a\nsingle utterance, remains a fundamental challenge for multiling ual NLP, even\namidst the rapid advances of large language models (LLMs). Most LLMs still\nstruggle with mixed-language inputs, limited CSW datasets, and evaluation\nbiases, hindering deployment in multilingual societies. This survey provides\nthe first comprehensive analysis of CSW-aware LLM research, reviewing\nunique_references studies spanning five research areas, 12 NLP tasks,\n30+ datasets, and 80+ languages. We classify recent advances by architecture,\ntraining strategy, and evaluation methodology, outlining how LLMs have reshaped\nCSW modeling and what challenges persist. The paper concludes with a roadmap\nemphasizing the need for inclusive datasets, fair evaluation, and\nlinguistically grounded models to achieve truly multilingual intelligence. A\ncurated collection of all resources is maintained at\nhttps://github.com/lingo-iitgn/awesome-code-mixing/.",
            "upvotes": 2,
            "discussionId": "68e746a77ae125f9582e6a7d",
            "projectPage": "https://lingo.iitgn.ac.in/codemixing/",
            "githubRepo": "https://github.com/lingo-iitgn/awesome-code-mixing/",
            "ai_summary": "This survey analyzes the current state of code-switching aware large language models, highlighting advancements and challenges in multilingual NLP.",
            "ai_keywords": [
                "code-switching",
                "multilingual NLP",
                "large language models",
                "mixed-language inputs",
                "CSW datasets",
                "evaluation biases",
                "CSW-aware LLM research",
                "architecture",
                "training strategy",
                "evaluation methodology",
                "inclusive datasets",
                "fair evaluation",
                "linguistically grounded models"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "667eb54cc9fc5e32c079544d",
                "name": "LingoIITGN",
                "fullname": "Lingo Research Group",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/667b8f8ba271fc5a8e6929de/8xB-4Az0x50XC4PS3ZIbL.jpeg"
            }
        },
        "publishedAt": "2025-10-08T10:04:14.000Z",
        "title": "Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era\n  of Large Language Models",
        "summary": "Code-switching (CSW), the alternation of languages and scripts within a\nsingle utterance, remains a fundamental challenge for multiling ual NLP, even\namidst the rapid advances of large language models (LLMs). Most LLMs still\nstruggle with mixed-language inputs, limited CSW datasets, and evaluation\nbiases, hindering deployment in multilingual societies. This survey provides\nthe first comprehensive analysis of CSW-aware LLM research, reviewing\nunique_references studies spanning five research areas, 12 NLP tasks,\n30+ datasets, and 80+ languages. We classify recent advances by architecture,\ntraining strategy, and evaluation methodology, outlining how LLMs have reshaped\nCSW modeling and what challenges persist. The paper concludes with a roadmap\nemphasizing the need for inclusive datasets, fair evaluation, and\nlinguistically grounded models to achieve truly multilingual intelligence. A\ncurated collection of all resources is maintained at\nhttps://github.com/lingo-iitgn/awesome-code-mixing/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07037.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66e1425c919f283fbd7dfb5e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e1425c919f283fbd7dfb5e/lfX3gDTLKvwX5QgAivq4S.png",
            "fullname": "Rajvee Sheth",
            "name": "RajveeSheth",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "667eb54cc9fc5e32c079544d",
            "name": "LingoIITGN",
            "fullname": "Lingo Research Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/667b8f8ba271fc5a8e6929de/8xB-4Az0x50XC4PS3ZIbL.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.06607",
            "authors": [
                {
                    "_id": "68e717967ae125f9582e695b",
                    "name": "Weidi Luo",
                    "hidden": false
                },
                {
                    "_id": "68e717967ae125f9582e695c",
                    "name": "Qiming Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e717967ae125f9582e695d",
                    "name": "Tianyu Lu",
                    "hidden": false
                },
                {
                    "_id": "68e717967ae125f9582e695e",
                    "name": "Xiaogeng Liu",
                    "hidden": false
                },
                {
                    "_id": "68e717967ae125f9582e695f",
                    "name": "Bin Hu",
                    "hidden": false
                },
                {
                    "_id": "68e717967ae125f9582e6960",
                    "name": "Hung-Chun Chiu",
                    "hidden": false
                },
                {
                    "_id": "68e717967ae125f9582e6961",
                    "name": "Siyuan Ma",
                    "hidden": false
                },
                {
                    "_id": "68e717967ae125f9582e6962",
                    "name": "Yizhe Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e717967ae125f9582e6963",
                    "name": "Xusheng Xiao",
                    "hidden": false
                },
                {
                    "_id": "68e717967ae125f9582e6964",
                    "name": "Yinzhi Cao",
                    "hidden": false
                },
                {
                    "_id": "68e717967ae125f9582e6965",
                    "name": "Zhen Xiang",
                    "hidden": false
                },
                {
                    "_id": "68e717967ae125f9582e6966",
                    "name": "Chaowei Xiao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T03:35:23.000Z",
            "submittedOnDailyAt": "2025-10-09T19:25:12.822Z",
            "title": "Code Agent can be an End-to-end System Hacker: Benchmarking Real-world\n  Threats of Computer-use Agent",
            "submittedOnDailyBy": {
                "_id": "6594e6136817cc4582b95572",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594e6136817cc4582b95572/jFZ6FV2J3o6A7-i9nZKFz.jpeg",
                "isPro": false,
                "fullname": "WeidiLuo",
                "user": "EddyLuo",
                "type": "user"
            },
            "summary": "Computer-use agent (CUA) frameworks, powered by large language models (LLMs)\nor multimodal LLMs (MLLMs), are rapidly maturing as assistants that can\nperceive context, reason, and act directly within software environments. Among\ntheir most critical applications is operating system (OS) control. As CUAs in\nthe OS domain become increasingly embedded in daily operations, it is\nimperative to examine their real-world security implications, specifically\nwhether CUAs can be misused to perform realistic, security-relevant attacks.\nExisting works exhibit four major limitations: Missing attacker-knowledge model\non tactics, techniques, and procedures (TTP), Incomplete coverage for\nend-to-end kill chains, unrealistic environment without multi-host and\nencrypted user credentials, and unreliable judgment dependent on\nLLM-as-a-Judge. To address these gaps, we propose AdvCUA, the first benchmark\naligned with real-world TTPs in MITRE ATT&CK Enterprise Matrix, which comprises\n140 tasks, including 40 direct malicious tasks, 74 TTP-based malicious tasks,\nand 26 end-to-end kill chains, systematically evaluates CUAs under a realistic\nenterprise OS security threat in a multi-host environment sandbox by hard-coded\nevaluation. We evaluate the existing five mainstream CUAs, including ReAct,\nAutoGPT, Gemini CLI, Cursor CLI, and Cursor IDE based on 8 foundation LLMs. The\nresults demonstrate that current frontier CUAs do not adequately cover OS\nsecurity-centric threats. These capabilities of CUAs reduce dependence on\ncustom malware and deep domain expertise, enabling even inexperienced attackers\nto mount complex enterprise intrusions, which raises social concern about the\nresponsibility and security of CUAs.",
            "upvotes": 2,
            "discussionId": "68e717967ae125f9582e6967",
            "projectPage": "https://eddyluo.com/AdvCUA/",
            "ai_summary": "AdvCUA benchmarks the security implications of computer-use agents in operating systems by evaluating their capabilities against real-world attack tactics and techniques in a multi-host environment.",
            "ai_keywords": [
                "large language models",
                "multimodal LLMs",
                "computer-use agents",
                "operating system control",
                "security implications",
                "tactics",
                "techniques",
                "procedures",
                "MITRE ATT&CK Enterprise Matrix",
                "end-to-end kill chains",
                "multi-host environment",
                "enterprise OS security",
                "ReAct",
                "AutoGPT",
                "Gemini CLI",
                "Cursor CLI",
                "Cursor IDE"
            ],
            "organization": {
                "_id": "68e806c94f2299128103f56c",
                "name": "MomoUchi",
                "fullname": "Momoka"
            }
        },
        "publishedAt": "2025-10-07T23:35:23.000Z",
        "title": "Code Agent can be an End-to-end System Hacker: Benchmarking Real-world\n  Threats of Computer-use Agent",
        "summary": "Computer-use agent (CUA) frameworks, powered by large language models (LLMs)\nor multimodal LLMs (MLLMs), are rapidly maturing as assistants that can\nperceive context, reason, and act directly within software environments. Among\ntheir most critical applications is operating system (OS) control. As CUAs in\nthe OS domain become increasingly embedded in daily operations, it is\nimperative to examine their real-world security implications, specifically\nwhether CUAs can be misused to perform realistic, security-relevant attacks.\nExisting works exhibit four major limitations: Missing attacker-knowledge model\non tactics, techniques, and procedures (TTP), Incomplete coverage for\nend-to-end kill chains, unrealistic environment without multi-host and\nencrypted user credentials, and unreliable judgment dependent on\nLLM-as-a-Judge. To address these gaps, we propose AdvCUA, the first benchmark\naligned with real-world TTPs in MITRE ATT&CK Enterprise Matrix, which comprises\n140 tasks, including 40 direct malicious tasks, 74 TTP-based malicious tasks,\nand 26 end-to-end kill chains, systematically evaluates CUAs under a realistic\nenterprise OS security threat in a multi-host environment sandbox by hard-coded\nevaluation. We evaluate the existing five mainstream CUAs, including ReAct,\nAutoGPT, Gemini CLI, Cursor CLI, and Cursor IDE based on 8 foundation LLMs. The\nresults demonstrate that current frontier CUAs do not adequately cover OS\nsecurity-centric threats. These capabilities of CUAs reduce dependence on\ncustom malware and deep domain expertise, enabling even inexperienced attackers\nto mount complex enterprise intrusions, which raises social concern about the\nresponsibility and security of CUAs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06607.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6594e6136817cc4582b95572",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594e6136817cc4582b95572/jFZ6FV2J3o6A7-i9nZKFz.jpeg",
            "fullname": "WeidiLuo",
            "name": "EddyLuo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "68e806c94f2299128103f56c",
            "name": "MomoUchi",
            "fullname": "Momoka"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.21842",
            "authors": [
                {
                    "_id": "68e77aa97ae125f9582e6c02",
                    "name": "Yansong Ning",
                    "hidden": false
                },
                {
                    "_id": "68e77aa97ae125f9582e6c03",
                    "name": "Rui Liu",
                    "hidden": false
                },
                {
                    "_id": "68e77aa97ae125f9582e6c04",
                    "name": "Jun Wang",
                    "hidden": false
                },
                {
                    "_id": "68e77aa97ae125f9582e6c05",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "68e77aa97ae125f9582e6c06",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "68e77aa97ae125f9582e6c07",
                    "name": "Jun Fang",
                    "hidden": false
                },
                {
                    "_id": "68e77aa97ae125f9582e6c08",
                    "name": "Kan Zheng",
                    "hidden": false
                },
                {
                    "_id": "68e77aa97ae125f9582e6c09",
                    "name": "Naiqiang Tan",
                    "hidden": false
                },
                {
                    "_id": "68e77aa97ae125f9582e6c0a",
                    "name": "Hao Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-26T04:03:52.000Z",
            "submittedOnDailyAt": "2025-10-09T07:37:18.706Z",
            "title": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for\n  Autonomous Travel Planning Agents",
            "submittedOnDailyBy": {
                "_id": "65d6a6f7654f85ff0baf161f",
                "avatarUrl": "/avatars/4a46f8b0522fa572c122249a9d6526c4.svg",
                "isPro": false,
                "fullname": "Yansong NING",
                "user": "yasNing",
                "type": "user"
            },
            "summary": "Travel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools and resources for travel itinerary generation,\nensuring enjoyable user experience. Despite its benefits, existing studies rely\non hand craft prompt and fixed agent workflow, hindering more flexible and\nautonomous TP agent. This paper proposes DeepTravel, an end to end agentic\nreinforcement learning framework for building autonomous travel planning agent,\ncapable of autonomously planning, executing tools, and reflecting on tool\nresponses to explore, verify, and refine intermediate actions in multi step\nreasoning. To achieve this, we first construct a robust sandbox environment by\ncaching transportation, accommodation and POI data, facilitating TP agent\ntraining without being constrained by real world APIs limitations (e.g.,\ninconsistent outputs). Moreover, we develop a hierarchical reward modeling\nsystem, where a trajectory level verifier first checks spatiotemporal\nfeasibility and filters unsatisfied travel itinerary, and then the turn level\nverifier further validate itinerary detail consistency with tool responses,\nenabling efficient and precise reward service. Finally, we propose the reply\naugmented reinforcement learning method that enables TP agent to periodically\nreplay from a failures experience buffer, emerging notable agentic capacity. We\ndeploy trained TP agent on DiDi Enterprise Solutions App and conduct\ncomprehensive online and offline evaluations, demonstrating that DeepTravel\nenables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing\nfrontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.",
            "upvotes": 2,
            "discussionId": "68e77aa97ae125f9582e6c0b",
            "ai_summary": "DeepTravel is an end-to-end reinforcement learning framework for autonomous travel planning that uses a hierarchical reward system and reply-augmented learning to improve performance over existing models.",
            "ai_keywords": [
                "reinforcement learning",
                "agentic reinforcement learning",
                "hierarchical reward modeling",
                "trajectory level verifier",
                "turn level verifier",
                "reply augmented reinforcement learning",
                "travel planning agent",
                "sandbox environment",
                "spatiotemporal feasibility",
                "itinerary detail consistency",
                "LLMs",
                "Qwen3 32B",
                "OpenAI o1",
                "o3",
                "DeepSeek R1"
            ],
            "organization": {
                "_id": "630da9a4c677278c1804028e",
                "name": "Didichuxing",
                "fullname": "Didi Chuxing",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1661839713821-630d471d467bc15dec893907.png"
            }
        },
        "publishedAt": "2025-09-26T00:03:52.000Z",
        "title": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for\n  Autonomous Travel Planning Agents",
        "summary": "Travel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools and resources for travel itinerary generation,\nensuring enjoyable user experience. Despite its benefits, existing studies rely\non hand craft prompt and fixed agent workflow, hindering more flexible and\nautonomous TP agent. This paper proposes DeepTravel, an end to end agentic\nreinforcement learning framework for building autonomous travel planning agent,\ncapable of autonomously planning, executing tools, and reflecting on tool\nresponses to explore, verify, and refine intermediate actions in multi step\nreasoning. To achieve this, we first construct a robust sandbox environment by\ncaching transportation, accommodation and POI data, facilitating TP agent\ntraining without being constrained by real world APIs limitations (e.g.,\ninconsistent outputs). Moreover, we develop a hierarchical reward modeling\nsystem, where a trajectory level verifier first checks spatiotemporal\nfeasibility and filters unsatisfied travel itinerary, and then the turn level\nverifier further validate itinerary detail consistency with tool responses,\nenabling efficient and precise reward service. Finally, we propose the reply\naugmented reinforcement learning method that enables TP agent to periodically\nreplay from a failures experience buffer, emerging notable agentic capacity. We\ndeploy trained TP agent on DiDi Enterprise Solutions App and conduct\ncomprehensive online and offline evaluations, demonstrating that DeepTravel\nenables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing\nfrontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21842.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d6a6f7654f85ff0baf161f",
            "avatarUrl": "/avatars/4a46f8b0522fa572c122249a9d6526c4.svg",
            "fullname": "Yansong NING",
            "name": "yasNing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "630da9a4c677278c1804028e",
            "name": "Didichuxing",
            "fullname": "Didi Chuxing",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1661839713821-630d471d467bc15dec893907.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.06888",
            "authors": [
                {
                    "_id": "68e79e2f7ae125f9582e6c8c",
                    "name": "Arkadeep Acharya",
                    "hidden": false
                },
                {
                    "_id": "68e79e2f7ae125f9582e6c8d",
                    "user": {
                        "_id": "630e850c3fc17ffc50f752e5",
                        "avatarUrl": "/avatars/43e4aee228323211b8cf55b783cc54d2.svg",
                        "isPro": false,
                        "fullname": "Akash Ghosh",
                        "user": "Agcs12",
                        "type": "user"
                    },
                    "name": "Akash Ghosh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:47:52.935Z",
                    "hidden": false
                },
                {
                    "_id": "68e79e2f7ae125f9582e6c8e",
                    "name": "Pradeepika Verma",
                    "hidden": false
                },
                {
                    "_id": "68e79e2f7ae125f9582e6c8f",
                    "name": "Kitsuchart Pasupa",
                    "hidden": false
                },
                {
                    "_id": "68e79e2f7ae125f9582e6c90",
                    "name": "Sriparna Saha",
                    "hidden": false
                },
                {
                    "_id": "68e79e2f7ae125f9582e6c91",
                    "name": "Priti Singh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T11:08:47.000Z",
            "submittedOnDailyAt": "2025-10-09T21:39:34.122Z",
            "title": "M3Retrieve: Benchmarking Multimodal Retrieval for Medicine",
            "submittedOnDailyBy": {
                "_id": "630e850c3fc17ffc50f752e5",
                "avatarUrl": "/avatars/43e4aee228323211b8cf55b783cc54d2.svg",
                "isPro": false,
                "fullname": "Akash Ghosh",
                "user": "Agcs12",
                "type": "user"
            },
            "summary": "With the increasing use of RetrievalAugmented Generation (RAG), strong\nretrieval models have become more important than ever. In healthcare,\nmultimodal retrieval models that combine information from both text and images\noffer major advantages for many downstream tasks such as question answering,\ncross-modal retrieval, and multimodal summarization, since medical data often\nincludes both formats. However, there is currently no standard benchmark to\nevaluate how well these models perform in medical settings. To address this\ngap, we introduce M3Retrieve, a Multimodal Medical Retrieval Benchmark.\nM3Retrieve, spans 5 domains,16 medical fields, and 4 distinct tasks, with over\n1.2 Million text documents and 164K multimodal queries, all collected under\napproved licenses. We evaluate leading multimodal retrieval models on this\nbenchmark to explore the challenges specific to different medical specialities\nand to understand their impact on retrieval performance. By releasing\nM3Retrieve, we aim to enable systematic evaluation, foster model innovation,\nand accelerate research toward building more capable and reliable multimodal\nretrieval systems for medical applications. The dataset and the baselines code\nare available in this github page https://github.com/AkashGhosh/M3Retrieve.",
            "upvotes": 1,
            "discussionId": "68e79e2f7ae125f9582e6c92",
            "ai_summary": "M3Retrieve is a new benchmark for evaluating multimodal retrieval models in healthcare, covering multiple domains and tasks with a large dataset of text and image data.",
            "ai_keywords": [
                "RetrievalAugmented Generation",
                "multimodal retrieval models",
                "question answering",
                "cross-modal retrieval",
                "multimodal summarization",
                "M3Retrieve",
                "Multimodal Medical Retrieval Benchmark"
            ],
            "organization": {
                "_id": "66960bd10e08a6505b87086e",
                "name": "IITPatna01",
                "fullname": "IIT Patna",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66960b121de9b5a5f25628da/KKmrySyMfUy5N0nv54REB.png"
            }
        },
        "publishedAt": "2025-10-08T07:08:47.000Z",
        "title": "M3Retrieve: Benchmarking Multimodal Retrieval for Medicine",
        "summary": "With the increasing use of RetrievalAugmented Generation (RAG), strong\nretrieval models have become more important than ever. In healthcare,\nmultimodal retrieval models that combine information from both text and images\noffer major advantages for many downstream tasks such as question answering,\ncross-modal retrieval, and multimodal summarization, since medical data often\nincludes both formats. However, there is currently no standard benchmark to\nevaluate how well these models perform in medical settings. To address this\ngap, we introduce M3Retrieve, a Multimodal Medical Retrieval Benchmark.\nM3Retrieve, spans 5 domains,16 medical fields, and 4 distinct tasks, with over\n1.2 Million text documents and 164K multimodal queries, all collected under\napproved licenses. We evaluate leading multimodal retrieval models on this\nbenchmark to explore the challenges specific to different medical specialities\nand to understand their impact on retrieval performance. By releasing\nM3Retrieve, we aim to enable systematic evaluation, foster model innovation,\nand accelerate research toward building more capable and reliable multimodal\nretrieval systems for medical applications. The dataset and the baselines code\nare available in this github page https://github.com/AkashGhosh/M3Retrieve.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06888.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "630e850c3fc17ffc50f752e5",
            "avatarUrl": "/avatars/43e4aee228323211b8cf55b783cc54d2.svg",
            "fullname": "Akash Ghosh",
            "name": "Agcs12",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "66960bd10e08a6505b87086e",
            "name": "IITPatna01",
            "fullname": "IIT Patna",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66960b121de9b5a5f25628da/KKmrySyMfUy5N0nv54REB.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.05891",
            "authors": [
                {
                    "_id": "68e663cd975ac4c405ef228a",
                    "name": "Yanran Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e663cd975ac4c405ef228b",
                    "name": "Bingyao Yu",
                    "hidden": false
                },
                {
                    "_id": "68e663cd975ac4c405ef228c",
                    "name": "Yu Zheng",
                    "hidden": false
                },
                {
                    "_id": "68e663cd975ac4c405ef228d",
                    "name": "Wenzhao Zheng",
                    "hidden": false
                },
                {
                    "_id": "68e663cd975ac4c405ef228e",
                    "name": "Yueqi Duan",
                    "hidden": false
                },
                {
                    "_id": "68e663cd975ac4c405ef228f",
                    "name": "Lei Chen",
                    "hidden": false
                },
                {
                    "_id": "68e663cd975ac4c405ef2290",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "68e663cd975ac4c405ef2291",
                    "name": "Jiwen Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-07T13:02:27.000Z",
            "submittedOnDailyAt": "2025-10-09T00:36:44.189Z",
            "title": "D^3QE: Learning Discrete Distribution Discrepancy-aware\n  Quantization Error for Autoregressive-Generated Image Detection",
            "submittedOnDailyBy": {
                "_id": "661cfae9a853782abad2a495",
                "avatarUrl": "/avatars/39723a07bf9efed8278e009fe966d044.svg",
                "isPro": false,
                "fullname": "Yanran Zhang",
                "user": "Yanran21",
                "type": "user"
            },
            "summary": "The emergence of visual autoregressive (AR) models has revolutionized image\ngeneration while presenting new challenges for synthetic image detection.\nUnlike previous GAN or diffusion-based methods, AR models generate images\nthrough discrete token prediction, exhibiting both marked improvements in image\nsynthesis quality and unique characteristics in their vector-quantized\nrepresentations. In this paper, we propose to leverage Discrete Distribution\nDiscrepancy-aware Quantization Error (D^3QE) for autoregressive-generated\nimage detection that exploits the distinctive patterns and the frequency\ndistribution bias of the codebook existing in real and fake images. We\nintroduce a discrete distribution discrepancy-aware transformer that integrates\ndynamic codebook frequency statistics into its attention mechanism, fusing\nsemantic features and quantization error latent. To evaluate our method, we\nconstruct a comprehensive dataset termed ARForensics covering 7 mainstream\nvisual AR models. Experiments demonstrate superior detection accuracy and\nstrong generalization of D^3QE across different AR models, with robustness to\nreal-world perturbations. Code is available at\nhttps://github.com/Zhangyr2022/D3QE{https://github.com/Zhangyr2022/D3QE}.",
            "upvotes": 1,
            "discussionId": "68e663ce975ac4c405ef2292",
            "projectPage": "https://ivg-yanranzhang.github.io/D3QE/",
            "githubRepo": "https://github.com/Zhangyr2022/D3QE",
            "ai_summary": "A novel method using Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) detects images generated by visual autoregressive models by analyzing codebook frequency statistics and quantization errors.",
            "ai_keywords": [
                "visual autoregressive (AR) models",
                "image generation",
                "synthetic image detection",
                "discrete token prediction",
                "vector-quantized representations",
                "Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE)",
                "discrete distribution discrepancy-aware transformer",
                "attention mechanism",
                "semantic features",
                "quantization error latent",
                "ARForensics dataset"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-10-07T09:02:27.000Z",
        "title": "D^3QE: Learning Discrete Distribution Discrepancy-aware\n  Quantization Error for Autoregressive-Generated Image Detection",
        "summary": "The emergence of visual autoregressive (AR) models has revolutionized image\ngeneration while presenting new challenges for synthetic image detection.\nUnlike previous GAN or diffusion-based methods, AR models generate images\nthrough discrete token prediction, exhibiting both marked improvements in image\nsynthesis quality and unique characteristics in their vector-quantized\nrepresentations. In this paper, we propose to leverage Discrete Distribution\nDiscrepancy-aware Quantization Error (D^3QE) for autoregressive-generated\nimage detection that exploits the distinctive patterns and the frequency\ndistribution bias of the codebook existing in real and fake images. We\nintroduce a discrete distribution discrepancy-aware transformer that integrates\ndynamic codebook frequency statistics into its attention mechanism, fusing\nsemantic features and quantization error latent. To evaluate our method, we\nconstruct a comprehensive dataset termed ARForensics covering 7 mainstream\nvisual AR models. Experiments demonstrate superior detection accuracy and\nstrong generalization of D^3QE across different AR models, with robustness to\nreal-world perturbations. Code is available at\nhttps://github.com/Zhangyr2022/D3QE{https://github.com/Zhangyr2022/D3QE}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05891.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "661cfae9a853782abad2a495",
            "avatarUrl": "/avatars/39723a07bf9efed8278e009fe966d044.svg",
            "fullname": "Yanran Zhang",
            "name": "Yanran21",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.05152",
            "authors": [
                {
                    "_id": "68e7d43144180c42eca8b0e5",
                    "name": "Jingtong Su",
                    "hidden": false
                },
                {
                    "_id": "68e7d43144180c42eca8b0e6",
                    "name": "Jianyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e7d43144180c42eca8b0e7",
                    "name": "Karen Ullrich",
                    "hidden": false
                },
                {
                    "_id": "68e7d43144180c42eca8b0e8",
                    "name": "Lon Bottou",
                    "hidden": false
                },
                {
                    "_id": "68e7d43144180c42eca8b0e9",
                    "name": "Mark Ibrahim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T13:27:28.000Z",
            "submittedOnDailyAt": "2025-10-09T13:58:19.096Z",
            "title": "A Single Character can Make or Break Your LLM Evals",
            "submittedOnDailyBy": {
                "_id": "64c17345e82e55936cf971bc",
                "avatarUrl": "/avatars/b39c5637e00291410620afb7c770d587.svg",
                "isPro": true,
                "fullname": "Mark Ibrahim",
                "user": "marksibrahim",
                "type": "user"
            },
            "summary": "Common Large Language model (LLM) evaluations rely on demonstration examples\nto steer models' responses to the desired style. While the number of examples\nused has been studied and standardized, the choice of how to format examples is\nless investigated. In evaluation protocols and real world usage, users face the\nchoice how to separate in-context examples: use a comma? new line? semi-colon?\nhashtag? etc.? Surprisingly, we find this seemingly minor choice can\ndramatically alter model response quality. Across leading model families\n(Llama, Qwen, Gemma), performance on MMLU for example can vary by pm 23%\ndepending on the choice of delimiter. In fact, one can manipulate model\nrankings to put any model in the lead by only modifying the single character\nseparating examples. We find LLMs' brittleness pervades topics, model families,\nand doesn't improve with scale. By probing attention head scores, we find that\ngood-performing delimiters steer attention towards key tokens in the input.\nFinally, we explore methods to improve LLMs' robustness to the choice of\ndelimiter. We find specifying the selected delimiter in the prompt boosts\nrobustness and offer practical recommendations for the best-performing\ndelimiters to select.",
            "upvotes": 1,
            "discussionId": "68e7d43144180c42eca8b0ea",
            "ai_summary": "The choice of delimiter in formatting in-context examples significantly impacts the performance of large language models across different families and tasks.",
            "ai_keywords": [
                "Large Language Model (LLM)",
                "MMLU",
                "attention head scores",
                "delimiter",
                "robustness",
                "prompt"
            ]
        },
        "publishedAt": "2025-10-02T09:27:28.000Z",
        "title": "A Single Character can Make or Break Your LLM Evals",
        "summary": "Common Large Language model (LLM) evaluations rely on demonstration examples\nto steer models' responses to the desired style. While the number of examples\nused has been studied and standardized, the choice of how to format examples is\nless investigated. In evaluation protocols and real world usage, users face the\nchoice how to separate in-context examples: use a comma? new line? semi-colon?\nhashtag? etc.? Surprisingly, we find this seemingly minor choice can\ndramatically alter model response quality. Across leading model families\n(Llama, Qwen, Gemma), performance on MMLU for example can vary by pm 23%\ndepending on the choice of delimiter. In fact, one can manipulate model\nrankings to put any model in the lead by only modifying the single character\nseparating examples. We find LLMs' brittleness pervades topics, model families,\nand doesn't improve with scale. By probing attention head scores, we find that\ngood-performing delimiters steer attention towards key tokens in the input.\nFinally, we explore methods to improve LLMs' robustness to the choice of\ndelimiter. We find specifying the selected delimiter in the prompt boosts\nrobustness and offer practical recommendations for the best-performing\ndelimiters to select.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05152.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c17345e82e55936cf971bc",
            "avatarUrl": "/avatars/b39c5637e00291410620afb7c770d587.svg",
            "fullname": "Mark Ibrahim",
            "name": "marksibrahim",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07550",
            "authors": [
                {
                    "_id": "68e854aa95e8e6771df38857",
                    "name": "Saman Motamed",
                    "hidden": false
                },
                {
                    "_id": "68e854aa95e8e6771df38858",
                    "name": "Minghao Chen",
                    "hidden": false
                },
                {
                    "_id": "68e854aa95e8e6771df38859",
                    "name": "Luc Van Gool",
                    "hidden": false
                },
                {
                    "_id": "68e854aa95e8e6771df3885a",
                    "name": "Iro Laina",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6475c37b04c82116f9bb2356/xpR8PBdnQQ5tFi3N13TUZ.mp4"
            ],
            "publishedAt": "2025-10-08T21:03:46.000Z",
            "submittedOnDailyAt": "2025-10-09T23:06:45.305Z",
            "title": "TRAVL: A Recipe for Making Video-Language Models Better Judges of\n  Physics Implausibility",
            "submittedOnDailyBy": {
                "_id": "6475c37b04c82116f9bb2356",
                "avatarUrl": "/avatars/6ec34eb3cfd091a38454ac3de72aaddc.svg",
                "isPro": false,
                "fullname": "saman motamed",
                "user": "sam-motamed",
                "type": "user"
            },
            "summary": "Despite impressive visual fidelity, modern video generative models frequently\nproduce sequences that violate intuitive physical laws, such as objects\nfloating, teleporting, or morphing in ways that defy causality. While humans\ncan easily detect such implausibilities, there remains no robust method for\nquantitatively assessing physical realism in video. In this work, we explore\nwhether Video-Language Models (VLMs) can be trained to serve as reliable judges\nof physical plausibility. We find that existing VLMs struggle to identify\nphysics violations, exposing fundamental limitations in their temporal and\ncausal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe\nthat combines a balanced training dataset with a trajectory-aware attention\nmodule to improve motion encoding and discrimination in VLMs. To evaluate\nphysical reasoning more rigorously, we propose ImplausiBench, a benchmark of\n300 videos (150 real, 150 generated) that removes linguistic biases and\nisolates visual-temporal understanding. Performance is reported both with\ngold-standard human judgments and stricter LLM-as-judge metrics. Together,\nTRAVL and ImplausiBench offer a unified framework for probing and improving\nphysical plausibility in multimodal models, shedding light on a challenging and\nunderexplored aspect of visual-temporal understanding.",
            "upvotes": 0,
            "discussionId": "68e854aa95e8e6771df3885b",
            "ai_summary": "TRAVL, a fine-tuning recipe with a trajectory-aware attention module, improves physical plausibility in Video-Language Models using the ImplausiBench benchmark.",
            "ai_keywords": [
                "Video-Language Models",
                "VLMs",
                "TRAVL",
                "trajectory-aware attention",
                "motion encoding",
                "discrimination",
                "ImplausiBench",
                "physical plausibility",
                "visual-temporal understanding",
                "LLM-as-judge"
            ],
            "organization": {
                "_id": "6585b575110c3eb77dabaa93",
                "name": "INSAIT-Institute",
                "fullname": "Institute for Computer Science, Artificial intelligence and Technology ",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64f1a0700af832a73d0f3e6f/KwpuATq29U2-Fu55OvUHR.png"
            }
        },
        "publishedAt": "2025-10-08T17:03:46.000Z",
        "title": "TRAVL: A Recipe for Making Video-Language Models Better Judges of\n  Physics Implausibility",
        "summary": "Despite impressive visual fidelity, modern video generative models frequently\nproduce sequences that violate intuitive physical laws, such as objects\nfloating, teleporting, or morphing in ways that defy causality. While humans\ncan easily detect such implausibilities, there remains no robust method for\nquantitatively assessing physical realism in video. In this work, we explore\nwhether Video-Language Models (VLMs) can be trained to serve as reliable judges\nof physical plausibility. We find that existing VLMs struggle to identify\nphysics violations, exposing fundamental limitations in their temporal and\ncausal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe\nthat combines a balanced training dataset with a trajectory-aware attention\nmodule to improve motion encoding and discrimination in VLMs. To evaluate\nphysical reasoning more rigorously, we propose ImplausiBench, a benchmark of\n300 videos (150 real, 150 generated) that removes linguistic biases and\nisolates visual-temporal understanding. Performance is reported both with\ngold-standard human judgments and stricter LLM-as-judge metrics. Together,\nTRAVL and ImplausiBench offer a unified framework for probing and improving\nphysical plausibility in multimodal models, shedding light on a challenging and\nunderexplored aspect of visual-temporal understanding.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6475c37b04c82116f9bb2356/xpR8PBdnQQ5tFi3N13TUZ.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07550.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6475c37b04c82116f9bb2356",
            "avatarUrl": "/avatars/6ec34eb3cfd091a38454ac3de72aaddc.svg",
            "fullname": "saman motamed",
            "name": "sam-motamed",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6585b575110c3eb77dabaa93",
            "name": "INSAIT-Institute",
            "fullname": "Institute for Computer Science, Artificial intelligence and Technology ",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64f1a0700af832a73d0f3e6f/KwpuATq29U2-Fu55OvUHR.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.06475",
            "authors": [
                {
                    "_id": "68e72c567ae125f9582e6a03",
                    "user": {
                        "_id": "641669e006af02514d0c2e9b",
                        "avatarUrl": "/avatars/4c9a7f407e7c648c60b5fc41976d80c3.svg",
                        "isPro": false,
                        "fullname": "Yitao Lo.ong",
                        "user": "Dragongon",
                        "type": "user"
                    },
                    "name": "Yitao Long",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:49:58.156Z",
                    "hidden": false
                },
                {
                    "_id": "68e72c567ae125f9582e6a04",
                    "name": "Yuru Jiang",
                    "hidden": false
                },
                {
                    "_id": "68e72c567ae125f9582e6a05",
                    "name": "Hongjun Liu",
                    "hidden": false
                },
                {
                    "_id": "68e72c567ae125f9582e6a06",
                    "name": "Yilun Zhao",
                    "hidden": false
                },
                {
                    "_id": "68e72c567ae125f9582e6a07",
                    "name": "Jingchen Sun",
                    "hidden": false
                },
                {
                    "_id": "68e72c567ae125f9582e6a08",
                    "name": "Yiqiu Shen",
                    "hidden": false
                },
                {
                    "_id": "68e72c567ae125f9582e6a09",
                    "name": "Chen Zhao",
                    "hidden": false
                },
                {
                    "_id": "68e72c567ae125f9582e6a0a",
                    "name": "Arman Cohan",
                    "hidden": false
                },
                {
                    "_id": "68e72c567ae125f9582e6a0b",
                    "name": "Dennis Shasha",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-07T21:24:29.000Z",
            "submittedOnDailyAt": "2025-10-09T14:34:58.349Z",
            "title": "PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning\n  with Puzzles",
            "submittedOnDailyBy": {
                "_id": "641669e006af02514d0c2e9b",
                "avatarUrl": "/avatars/4c9a7f407e7c648c60b5fc41976d80c3.svg",
                "isPro": false,
                "fullname": "Yitao Lo.ong",
                "user": "Dragongon",
                "type": "user"
            },
            "summary": "This work investigates the reasoning and planning capabilities of foundation\nmodels and their scalability in complex, dynamic environments. We introduce\nPuzzlePlex, a benchmark designed to assess these capabilities through a diverse\nset of puzzles. PuzzlePlex consists of 15 types of puzzles, including\ndeterministic and stochastic games of varying difficulty, as well as\nsingle-player and two-player scenarios. The PuzzlePlex framework provides a\ncomprehensive environment for each game, and supports extensibility to generate\nmore challenging instances as foundation models evolve. Additionally, we\nimplement customized game-playing strategies for comparison. Building on this\nbenchmark, we develop fine-grained metrics to measure performance and conduct\nan in-depth analysis of frontier foundation models across two settings:\ninstruction-based and code-based. Furthermore, we systematically investigate\ntheir scaling limits. Our findings show that reasoning models outperform others\nin instruction-based settings, while code-based execution presents greater\nchallenges but offers a scalable and efficient alternative. PuzzlePlex enables\ntargeted evaluation and guides future improvements in reasoning, planning, and\ngeneralization for foundation models.",
            "upvotes": 0,
            "discussionId": "68e72c577ae125f9582e6a0c",
            "ai_summary": "PuzzlePlex benchmark assesses reasoning and planning capabilities of foundation models through diverse puzzles, providing metrics and insights into their performance and scalability.",
            "ai_keywords": [
                "PuzzlePlex",
                "foundation models",
                "reasoning capabilities",
                "planning capabilities",
                "scalability",
                "deterministic games",
                "stochastic games",
                "single-player scenarios",
                "two-player scenarios",
                "game-playing strategies",
                "fine-grained metrics",
                "instruction-based settings",
                "code-based execution"
            ]
        },
        "publishedAt": "2025-10-07T17:24:29.000Z",
        "title": "PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning\n  with Puzzles",
        "summary": "This work investigates the reasoning and planning capabilities of foundation\nmodels and their scalability in complex, dynamic environments. We introduce\nPuzzlePlex, a benchmark designed to assess these capabilities through a diverse\nset of puzzles. PuzzlePlex consists of 15 types of puzzles, including\ndeterministic and stochastic games of varying difficulty, as well as\nsingle-player and two-player scenarios. The PuzzlePlex framework provides a\ncomprehensive environment for each game, and supports extensibility to generate\nmore challenging instances as foundation models evolve. Additionally, we\nimplement customized game-playing strategies for comparison. Building on this\nbenchmark, we develop fine-grained metrics to measure performance and conduct\nan in-depth analysis of frontier foundation models across two settings:\ninstruction-based and code-based. Furthermore, we systematically investigate\ntheir scaling limits. Our findings show that reasoning models outperform others\nin instruction-based settings, while code-based execution presents greater\nchallenges but offers a scalable and efficient alternative. PuzzlePlex enables\ntargeted evaluation and guides future improvements in reasoning, planning, and\ngeneralization for foundation models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06475.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "641669e006af02514d0c2e9b",
            "avatarUrl": "/avatars/4c9a7f407e7c648c60b5fc41976d80c3.svg",
            "fullname": "Yitao Lo.ong",
            "name": "Dragongon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.06426",
            "authors": [
                {
                    "_id": "68e72d597ae125f9582e6a0e",
                    "user": {
                        "_id": "641669e006af02514d0c2e9b",
                        "avatarUrl": "/avatars/4c9a7f407e7c648c60b5fc41976d80c3.svg",
                        "isPro": false,
                        "fullname": "Yitao Lo.ong",
                        "user": "Dragongon",
                        "type": "user"
                    },
                    "name": "Yitao Long",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:49:55.141Z",
                    "hidden": false
                },
                {
                    "_id": "68e72d597ae125f9582e6a0f",
                    "name": "Tiansheng Hu",
                    "hidden": false
                },
                {
                    "_id": "68e72d597ae125f9582e6a10",
                    "name": "Yilun Zhao",
                    "hidden": false
                },
                {
                    "_id": "68e72d597ae125f9582e6a11",
                    "name": "Arman Cohan",
                    "hidden": false
                },
                {
                    "_id": "68e72d597ae125f9582e6a12",
                    "name": "Chen Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-07T20:06:15.000Z",
            "submittedOnDailyAt": "2025-10-09T14:30:04.888Z",
            "title": "FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial\n  Long-Form Question Answering",
            "submittedOnDailyBy": {
                "_id": "641669e006af02514d0c2e9b",
                "avatarUrl": "/avatars/4c9a7f407e7c648c60b5fc41976d80c3.svg",
                "isPro": false,
                "fullname": "Yitao Lo.ong",
                "user": "Dragongon",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) frequently hallucinate to long-form questions,\nproducing plausible yet factually incorrect answers. A common mitigation\nstrategy is to provide attribution to LLM outputs. However, existing benchmarks\nprimarily focus on simple attribution that retrieves supporting textual\nevidence as references. We argue that in real-world scenarios such as financial\napplications, attribution goes beyond reference retrieval. We introduce\nFinLFQA, a benchmark designed to evaluate the ability of LLMs to generate\nlong-form answers to complex financial questions with reliable and nuanced\nattributions. FinLFQA evaluates three critical aspects of attribution through\nhuman annotations: (1) supporting evidence extracted from financial reports,\n(2) intermediate numerical reasoning steps, and (3) domain-specific financial\nknowledge that informs the reasoning process. We further provide an automatic\nevaluation framework covering both answer quality and attribution quality.\nThrough extensive experiments on eight LLMs across multiple\nattribution-generation paradigms, we find that fine-grained metrics are\nimportant to distinguish model capabilities, that end-to-end generation\nachieves comparable performance to post-hoc approaches, and that iterative\nrefinement only helps when guided by external feedback.",
            "upvotes": 0,
            "discussionId": "68e72d597ae125f9582e6a13",
            "ai_summary": "FinLFQA evaluates LLMs' ability to provide reliable and nuanced attributions in long-form financial question answering through human and automatic assessments.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "hallucination",
                "attribution",
                "FinLFQA",
                "supporting evidence",
                "intermediate numerical reasoning",
                "domain-specific financial knowledge",
                "end-to-end generation",
                "post-hoc approaches",
                "iterative refinement"
            ]
        },
        "publishedAt": "2025-10-07T16:06:15.000Z",
        "title": "FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial\n  Long-Form Question Answering",
        "summary": "Large Language Models (LLMs) frequently hallucinate to long-form questions,\nproducing plausible yet factually incorrect answers. A common mitigation\nstrategy is to provide attribution to LLM outputs. However, existing benchmarks\nprimarily focus on simple attribution that retrieves supporting textual\nevidence as references. We argue that in real-world scenarios such as financial\napplications, attribution goes beyond reference retrieval. We introduce\nFinLFQA, a benchmark designed to evaluate the ability of LLMs to generate\nlong-form answers to complex financial questions with reliable and nuanced\nattributions. FinLFQA evaluates three critical aspects of attribution through\nhuman annotations: (1) supporting evidence extracted from financial reports,\n(2) intermediate numerical reasoning steps, and (3) domain-specific financial\nknowledge that informs the reasoning process. We further provide an automatic\nevaluation framework covering both answer quality and attribution quality.\nThrough extensive experiments on eight LLMs across multiple\nattribution-generation paradigms, we find that fine-grained metrics are\nimportant to distinguish model capabilities, that end-to-end generation\nachieves comparable performance to post-hoc approaches, and that iterative\nrefinement only helps when guided by external feedback.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06426.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "641669e006af02514d0c2e9b",
            "avatarUrl": "/avatars/4c9a7f407e7c648c60b5fc41976d80c3.svg",
            "fullname": "Yitao Lo.ong",
            "name": "Dragongon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.04910",
            "authors": [
                {
                    "_id": "68e6813a975ac4c405ef238a",
                    "user": {
                        "_id": "64b0f081698046a2f2d3951f",
                        "avatarUrl": "/avatars/26f7a4759ddcda6bdb6b9a770d482546.svg",
                        "isPro": false,
                        "fullname": "Jie Yang",
                        "user": "Muyiiiii",
                        "type": "user"
                    },
                    "name": "Jie Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:51:11.490Z",
                    "hidden": false
                },
                {
                    "_id": "68e6813a975ac4c405ef238b",
                    "name": "Kexin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e6813a975ac4c405ef238c",
                    "name": "Guibin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e6813a975ac4c405ef238d",
                    "name": "Philip S. Yu",
                    "hidden": false
                },
                {
                    "_id": "68e6813a975ac4c405ef238e",
                    "name": "Kaize Ding",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-06T15:24:44.000Z",
            "submittedOnDailyAt": "2025-10-09T13:51:36.185Z",
            "title": "Glocal Information Bottleneck for Time Series Imputation",
            "submittedOnDailyBy": {
                "_id": "64b0f081698046a2f2d3951f",
                "avatarUrl": "/avatars/26f7a4759ddcda6bdb6b9a770d482546.svg",
                "isPro": false,
                "fullname": "Jie Yang",
                "user": "Muyiiiii",
                "type": "user"
            },
            "summary": "Time Series Imputation (TSI), which aims to recover missing values in\ntemporal data, remains a fundamental challenge due to the complex and often\nhigh-rate missingness in real-world scenarios. Existing models typically\noptimize the point-wise reconstruction loss, focusing on recovering numerical\nvalues (local information). However, we observe that under high missing rates,\nthese models still perform well in the training phase yet produce poor\nimputations and distorted latent representation distributions (global\ninformation) in the inference phase. This reveals a critical optimization\ndilemma: current objectives lack global guidance, leading models to overfit\nlocal noise and fail to capture global information of the data. To address this\nissue, we propose a new training paradigm, Glocal Information Bottleneck\n(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework\nby introducing a Global Alignment loss, derived from a tractable mutual\ninformation approximation. This loss aligns the latent representations of\nmasked inputs with those of their originally observed counterparts. It helps\nthe model retain global structure and local details while suppressing noise\ncaused by missing values, giving rise to better generalization under high\nmissingness. Extensive experiments on nine datasets confirm that Glocal-IB\nleads to consistently improved performance and aligned latent representations\nunder missingness. Our code implementation is available in\nhttps://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.",
            "upvotes": 0,
            "discussionId": "68e6813b975ac4c405ef238f",
            "ai_summary": "A new training paradigm, Glocal Information Bottleneck, improves time series imputation by aligning latent representations to retain global structure and local details under high missingness.",
            "ai_keywords": [
                "Time Series Imputation",
                "Glocal Information Bottleneck",
                "Global Alignment loss",
                "mutual information approximation",
                "latent representations",
                "overfitting",
                "local noise",
                "generalization"
            ]
        },
        "publishedAt": "2025-10-06T11:24:44.000Z",
        "title": "Glocal Information Bottleneck for Time Series Imputation",
        "summary": "Time Series Imputation (TSI), which aims to recover missing values in\ntemporal data, remains a fundamental challenge due to the complex and often\nhigh-rate missingness in real-world scenarios. Existing models typically\noptimize the point-wise reconstruction loss, focusing on recovering numerical\nvalues (local information). However, we observe that under high missing rates,\nthese models still perform well in the training phase yet produce poor\nimputations and distorted latent representation distributions (global\ninformation) in the inference phase. This reveals a critical optimization\ndilemma: current objectives lack global guidance, leading models to overfit\nlocal noise and fail to capture global information of the data. To address this\nissue, we propose a new training paradigm, Glocal Information Bottleneck\n(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework\nby introducing a Global Alignment loss, derived from a tractable mutual\ninformation approximation. This loss aligns the latent representations of\nmasked inputs with those of their originally observed counterparts. It helps\nthe model retain global structure and local details while suppressing noise\ncaused by missing values, giving rise to better generalization under high\nmissingness. Extensive experiments on nine datasets confirm that Glocal-IB\nleads to consistently improved performance and aligned latent representations\nunder missingness. Our code implementation is available in\nhttps://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04910.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b0f081698046a2f2d3951f",
            "avatarUrl": "/avatars/26f7a4759ddcda6bdb6b9a770d482546.svg",
            "fullname": "Jie Yang",
            "name": "Muyiiiii",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]