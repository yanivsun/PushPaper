[
    {
        "paper": {
            "id": "2510.08558",
            "authors": [
                {
                    "_id": "68e86eaf95e8e6771df38925",
                    "user": {
                        "_id": "63e0a50242591dda0b9dca5c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e0a50242591dda0b9dca5c/c7cBPEBWQDFYimfGnO_SI.png",
                        "isPro": false,
                        "fullname": "Kai Zhang",
                        "user": "drogozhang",
                        "type": "user"
                    },
                    "name": "Kai Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:42.421Z",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38926",
                    "name": "Xiangchao Chen",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38927",
                    "user": {
                        "_id": "635e3a76106f984574c36409",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
                        "isPro": false,
                        "fullname": "Bo Liu",
                        "user": "Benjamin-eecs",
                        "type": "user"
                    },
                    "name": "Bo Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:47.903Z",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38928",
                    "name": "Tianci Xue",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38929",
                    "name": "Zeyi Liao",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3892a",
                    "name": "Zhihan Liu",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3892b",
                    "user": {
                        "_id": "655fed9fdef5905d38b84af3",
                        "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
                        "isPro": false,
                        "fullname": "Xiyao Wang",
                        "user": "russwang",
                        "type": "user"
                    },
                    "name": "Xiyao Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:45.647Z",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3892c",
                    "user": {
                        "_id": "65ace92f64c9b93eca5c2bce",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ace92f64c9b93eca5c2bce/pG0JRXH-8zEy0IoaEnMNw.jpeg",
                        "isPro": false,
                        "fullname": "Yuting Ning",
                        "user": "nnnyt",
                        "type": "user"
                    },
                    "name": "Yuting Ning",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T09:12:27.032Z",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3892d",
                    "name": "Zhaorun Chen",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3892e",
                    "name": "Xiaohan Fu",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3892f",
                    "name": "Jian Xie",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38930",
                    "name": "Yuxuan Sun",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38931",
                    "user": {
                        "_id": "6500870f1e14749e84f8f887",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6500870f1e14749e84f8f887/wfvx4BZvh2OyW-vpq5jEy.jpeg",
                        "isPro": false,
                        "fullname": "Boyu Gou",
                        "user": "BoyuNLP",
                        "type": "user"
                    },
                    "name": "Boyu Gou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:38.644Z",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38932",
                    "name": "Qi Qi",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38933",
                    "name": "Zihang Meng",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38934",
                    "name": "Jianwei Yang",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38935",
                    "name": "Ning Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38936",
                    "name": "Xian Li",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38937",
                    "name": "Ashish Shah",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38938",
                    "name": "Dat Huynh",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38939",
                    "name": "Hengduo Li",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3893a",
                    "name": "Zi Yang",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3893b",
                    "name": "Sara Cao",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3893c",
                    "name": "Lawrence Jang",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3893d",
                    "name": "Shuyan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3893e",
                    "name": "Jiacheng Zhu",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3893f",
                    "name": "Huan Sun",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38940",
                    "name": "Jason Weston",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38941",
                    "name": "Yu Su",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38942",
                    "name": "Yifan Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T17:59:17.000Z",
            "submittedOnDailyAt": "2025-10-10T00:55:59.528Z",
            "title": "Agent Learning via Early Experience",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "A long-term goal of language agents is to learn and improve through their own\nexperience, ultimately outperforming humans in complex, real-world tasks.\nHowever, training agents from experience data with reinforcement learning\nremains difficult in many environments, which either lack verifiable rewards\n(e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn\ntool use). As a result, most current agents rely on supervised fine-tuning on\nexpert data, which is challenging to scale and generalizes poorly. This\nlimitation stems from the nature of expert demonstrations: they capture only a\nnarrow range of scenarios and expose the agent to limited environment\ndiversity. We address this limitation with a middle-ground paradigm we call\nearly experience: interaction data generated by the agent's own actions, where\nthe resulting future states serve as supervision without reward signals. Within\nthis paradigm we study two strategies of using such data: (1) Implicit world\nmodeling, which uses collected states to ground the policy in environment\ndynamics; and (2) Self-reflection, where the agent learns from its suboptimal\nactions to improve reasoning and decision-making. We evaluate across eight\ndiverse environments and multiple model families. Our approaches consistently\nimprove effectiveness and out-of-domain generalization, highlighting the value\nof early experience. Moreover, in environments with verifiable rewards, our\nresults provide promising signals that early experience offers a strong\nfoundation for subsequent reinforcement learning, positioning it as a practical\nbridge between imitation learning and fully experience-driven agents.",
            "upvotes": 123,
            "discussionId": "68e86eaf95e8e6771df38943",
            "ai_summary": "Early experience, using agent-generated interaction data without reward signals, improves policy effectiveness and generalization, serving as a bridge between imitation learning and reinforcement learning.",
            "ai_keywords": [
                "reinforcement learning",
                "early experience",
                "implicit world modeling",
                "self-reflection",
                "out-of-domain generalization"
            ],
            "organization": {
                "_id": "66b54027408752ae16404b05",
                "name": "metaresearch",
                "fullname": "Meta Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
            }
        },
        "publishedAt": "2025-10-09T13:59:17.000Z",
        "title": "Agent Learning via Early Experience",
        "summary": "A long-term goal of language agents is to learn and improve through their own\nexperience, ultimately outperforming humans in complex, real-world tasks.\nHowever, training agents from experience data with reinforcement learning\nremains difficult in many environments, which either lack verifiable rewards\n(e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn\ntool use). As a result, most current agents rely on supervised fine-tuning on\nexpert data, which is challenging to scale and generalizes poorly. This\nlimitation stems from the nature of expert demonstrations: they capture only a\nnarrow range of scenarios and expose the agent to limited environment\ndiversity. We address this limitation with a middle-ground paradigm we call\nearly experience: interaction data generated by the agent's own actions, where\nthe resulting future states serve as supervision without reward signals. Within\nthis paradigm we study two strategies of using such data: (1) Implicit world\nmodeling, which uses collected states to ground the policy in environment\ndynamics; and (2) Self-reflection, where the agent learns from its suboptimal\nactions to improve reasoning and decision-making. We evaluate across eight\ndiverse environments and multiple model families. Our approaches consistently\nimprove effectiveness and out-of-domain generalization, highlighting the value\nof early experience. Moreover, in environments with verifiable rewards, our\nresults provide promising signals that early experience offers a strong\nfoundation for subsequent reinforcement learning, positioning it as a practical\nbridge between imitation learning and fully experience-driven agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08558.png",
        "numComments": 8,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 123
        },
        "organization": {
            "_id": "66b54027408752ae16404b05",
            "name": "metaresearch",
            "fullname": "Meta Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08540",
            "authors": [
                {
                    "_id": "68e8700e95e8e6771df38955",
                    "user": {
                        "_id": "6530e62f536dbca918e71c3e",
                        "avatarUrl": "/avatars/efc93bc767e561c6c6d429f65c23382d.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Z",
                        "user": "PhoenixZ",
                        "type": "user"
                    },
                    "name": "Xiangyu Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:28.249Z",
                    "hidden": false
                },
                {
                    "_id": "68e8700e95e8e6771df38956",
                    "user": {
                        "_id": "65f7a2a72f5698447c774e17",
                        "avatarUrl": "/avatars/e1a82df58af02c0ff08287e78f438480.svg",
                        "isPro": false,
                        "fullname": "Junming Lin",
                        "user": "mjuicem",
                        "type": "user"
                    },
                    "name": "Junming Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:23.539Z",
                    "hidden": false
                },
                {
                    "_id": "68e8700e95e8e6771df38957",
                    "user": {
                        "_id": "6703ec213df5fe425086ef73",
                        "avatarUrl": "/avatars/e6f9dad6587ee0883ae10f8805ab7ea9.svg",
                        "isPro": true,
                        "fullname": "Tianhao Liang",
                        "user": "tianhao2k",
                        "type": "user"
                    },
                    "name": "Tianhao Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T09:27:53.878Z",
                    "hidden": false
                },
                {
                    "_id": "68e8700e95e8e6771df38958",
                    "user": {
                        "_id": "659d2dff20cf0b934bbee513",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659d2dff20cf0b934bbee513/9e9R852Zr2R82h64eUUQl.jpeg",
                        "isPro": false,
                        "fullname": "Yifan Zhou",
                        "user": "yingmanji",
                        "type": "user"
                    },
                    "name": "Yifan Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:25.960Z",
                    "hidden": false
                },
                {
                    "_id": "68e8700e95e8e6771df38959",
                    "name": "Wenhao Chai",
                    "hidden": false
                },
                {
                    "_id": "68e8700e95e8e6771df3895a",
                    "user": {
                        "_id": "6601196cc91ba4c08ad6e270",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
                        "isPro": false,
                        "fullname": "yuzhe gu",
                        "user": "vanilla1116",
                        "type": "user"
                    },
                    "name": "Yuzhe Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T09:11:44.661Z",
                    "hidden": false
                },
                {
                    "_id": "68e8700e95e8e6771df3895b",
                    "name": "Weiyun Wang",
                    "hidden": false
                },
                {
                    "_id": "68e8700e95e8e6771df3895c",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "68e8700e95e8e6771df3895d",
                    "name": "Gen Luo",
                    "hidden": false
                },
                {
                    "_id": "68e8700e95e8e6771df3895e",
                    "name": "Wenwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e8700e95e8e6771df3895f",
                    "name": "Junchi Yan",
                    "hidden": false
                },
                {
                    "_id": "68e8700e95e8e6771df38960",
                    "name": "Hua Yang",
                    "hidden": false
                },
                {
                    "_id": "68e8700e95e8e6771df38961",
                    "name": "Haodong Duan",
                    "hidden": false
                },
                {
                    "_id": "68e8700e95e8e6771df38962",
                    "name": "Xue Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T17:53:58.000Z",
            "submittedOnDailyAt": "2025-10-10T01:02:37.825Z",
            "title": "MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with\n  Holistic Platform and Adaptive Hybrid Policy Optimization",
            "submittedOnDailyBy": {
                "_id": "6530e62f536dbca918e71c3e",
                "avatarUrl": "/avatars/efc93bc767e561c6c6d429f65c23382d.svg",
                "isPro": false,
                "fullname": "Xiangyu Z",
                "user": "PhoenixZ",
                "type": "user"
            },
            "summary": "While current Multimodal Large Language Models (MLLMs) have demonstrated\nproficiency in reasoning tasks such as mathematics and logic, their capacity\nfor long-chain reflective reasoning, a prerequisite for solving complex\nreal-world problems, remains largely underexplored. In this work, we first\nconduct an extensive empirical investigation to evaluate this capability.\nLeveraging a carefully designed data synthesis engine, we construct MM-HELIX, a\nmultimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks\nthat require iterative thinking and backtracking. Empirical results on this\nbenchmark reveal that existing MLLMs exhibit significant performance deficits\nin long-chain reflective reasoning. To address this limitation, we generate\npost-training data and further explore learning paradigms for exploiting such\ndata. We first develop the Step-Elicited Response Generation pipeline to create\nMM-HELIX-100K, a large-scale dataset of 100k high-quality, reflective reasoning\ntraces for instruction-tuning stage. Given that standard Reinforcement Learning\nfails on complex tasks due to sparse reward signals and catastrophic forgetting\nafter Supervised Fine-Tuning, we propose Adaptive Hybrid Policy Optimization\n(AHPO), a novel training strategy that dynamically unifies offline supervision\nand online optimization into a single stage. This strategy enables the model to\nlearn from expert data when rewards are sparse and conduct independent\nexploration once proficient. When applied to the Qwen2.5-VL-7B baseline, our\nmethod achieves a +18.6\\% accuracy improvement on MM-HELIX benchmark and\ndemonstrates strong generalization with a +5.7\\% average performance gain on\ngeneral mathematic and logic tasks. Our work demonstrate that reflective\nreasoning in MLLMs can be effectively learned and generalized, paving the way\nfor developing more capable MLLMs.",
            "upvotes": 90,
            "discussionId": "68e8700e95e8e6771df38963",
            "projectPage": "https://mm-helix.github.io/",
            "githubRepo": "https://github.com/PhoenixZ810/MM-HELIX",
            "ai_summary": "Existing Multimodal Large Language Models show performance deficits in long-chain reflective reasoning, which is addressed by developing MM-HELIX-100K and Adaptive Hybrid Policy Optimization, leading to improved accuracy and generalization.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "long-chain reflective reasoning",
                "MM-HELIX",
                "Step-Elicited Response Generation",
                "Adaptive Hybrid Policy Optimization",
                "offline supervision",
                "online optimization",
                "Qwen2.5-VL-7B"
            ],
            "githubStars": 38
        },
        "publishedAt": "2025-10-09T13:53:58.000Z",
        "title": "MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with\n  Holistic Platform and Adaptive Hybrid Policy Optimization",
        "summary": "While current Multimodal Large Language Models (MLLMs) have demonstrated\nproficiency in reasoning tasks such as mathematics and logic, their capacity\nfor long-chain reflective reasoning, a prerequisite for solving complex\nreal-world problems, remains largely underexplored. In this work, we first\nconduct an extensive empirical investigation to evaluate this capability.\nLeveraging a carefully designed data synthesis engine, we construct MM-HELIX, a\nmultimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks\nthat require iterative thinking and backtracking. Empirical results on this\nbenchmark reveal that existing MLLMs exhibit significant performance deficits\nin long-chain reflective reasoning. To address this limitation, we generate\npost-training data and further explore learning paradigms for exploiting such\ndata. We first develop the Step-Elicited Response Generation pipeline to create\nMM-HELIX-100K, a large-scale dataset of 100k high-quality, reflective reasoning\ntraces for instruction-tuning stage. Given that standard Reinforcement Learning\nfails on complex tasks due to sparse reward signals and catastrophic forgetting\nafter Supervised Fine-Tuning, we propose Adaptive Hybrid Policy Optimization\n(AHPO), a novel training strategy that dynamically unifies offline supervision\nand online optimization into a single stage. This strategy enables the model to\nlearn from expert data when rewards are sparse and conduct independent\nexploration once proficient. When applied to the Qwen2.5-VL-7B baseline, our\nmethod achieves a +18.6\\% accuracy improvement on MM-HELIX benchmark and\ndemonstrates strong generalization with a +5.7\\% average performance gain on\ngeneral mathematic and logic tasks. Our work demonstrate that reflective\nreasoning in MLLMs can be effectively learned and generalized, paving the way\nfor developing more capable MLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08540.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6530e62f536dbca918e71c3e",
            "avatarUrl": "/avatars/efc93bc767e561c6c6d429f65c23382d.svg",
            "fullname": "Xiangyu Z",
            "name": "PhoenixZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.03279",
            "authors": [
                {
                    "_id": "68e88ed495e8e6771df38b2c",
                    "name": "Youjin Wang",
                    "hidden": false
                },
                {
                    "_id": "68e88ed495e8e6771df38b2d",
                    "name": "Yangjingyi Chen",
                    "hidden": false
                },
                {
                    "_id": "68e88ed495e8e6771df38b2e",
                    "name": "Jiahao Yan",
                    "hidden": false
                },
                {
                    "_id": "68e88ed495e8e6771df38b2f",
                    "name": "Jiaxuan Lu",
                    "hidden": false
                },
                {
                    "_id": "68e88ed495e8e6771df38b30",
                    "name": "Xiao Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T14:40:58.000Z",
            "submittedOnDailyAt": "2025-10-10T03:13:32.550Z",
            "title": "MemMamba: Rethinking Memory Patterns in State Space Model",
            "submittedOnDailyBy": {
                "_id": "67e655f7d6b8333a8f78eadf",
                "avatarUrl": "/avatars/11cc80d3c03747fd869e4dc1dbdd031a.svg",
                "isPro": false,
                "fullname": "Jiaxuan Lu",
                "user": "Blue-Giant",
                "type": "user"
            },
            "summary": "With the explosive growth of data, long-sequence modeling has become\nincreasingly important in tasks such as natural language processing and\nbioinformatics. However, existing methods face inherent trade-offs between\nefficiency and memory. Recurrent neural networks suffer from gradient vanishing\nand explosion, making them hard to scale. Transformers can model global\ndependencies but are constrained by quadratic complexity. Recently, selective\nstate-space models such as Mamba have demonstrated high efficiency with O(n)\ntime and O(1) recurrent inference, yet their long-range memory decays\nexponentially. In this work, we conduct mathematical derivations and\ninformation-theoretic analysis to systematically uncover the memory decay\nmechanism of Mamba, answering a fundamental question: what is the nature of\nMamba's long-range memory and how does it retain information? To quantify key\ninformation loss, we further introduce horizontal-vertical memory fidelity\nmetrics that capture degradation both within and across layers. Inspired by how\nhumans distill and retain salient information when reading long documents, we\npropose MemMamba, a novel architectural framework that integrates state\nsummarization mechanism together with cross-layer and cross-token attention,\nwhich alleviates long-range forgetting while preserving linear complexity.\nMemMamba achieves significant improvements over existing Mamba variants and\nTransformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,\nwhile delivering a 48% speedup in inference efficiency. Both theoretical\nanalysis and empirical results demonstrate that MemMamba achieves a\nbreakthrough in the complexity-memory trade-off, offering a new paradigm for\nultra-long sequence modeling.",
            "upvotes": 55,
            "discussionId": "68e88ed495e8e6771df38b31",
            "ai_summary": "MemMamba, a novel architecture integrating state summarization and cross-attention, improves long-range memory and efficiency in sequence modeling compared to Mamba and Transformers.",
            "ai_keywords": [
                "recurrent neural networks",
                "gradient vanishing",
                "gradient explosion",
                "Transformers",
                "quadratic complexity",
                "selective state-space models",
                "Mamba",
                "memory decay",
                "information-theoretic analysis",
                "horizontal-vertical memory fidelity",
                "cross-layer attention",
                "cross-token attention",
                "PG19",
                "Passkey Retrieval",
                "ultra-long sequence modeling"
            ]
        },
        "publishedAt": "2025-09-28T10:40:58.000Z",
        "title": "MemMamba: Rethinking Memory Patterns in State Space Model",
        "summary": "With the explosive growth of data, long-sequence modeling has become\nincreasingly important in tasks such as natural language processing and\nbioinformatics. However, existing methods face inherent trade-offs between\nefficiency and memory. Recurrent neural networks suffer from gradient vanishing\nand explosion, making them hard to scale. Transformers can model global\ndependencies but are constrained by quadratic complexity. Recently, selective\nstate-space models such as Mamba have demonstrated high efficiency with O(n)\ntime and O(1) recurrent inference, yet their long-range memory decays\nexponentially. In this work, we conduct mathematical derivations and\ninformation-theoretic analysis to systematically uncover the memory decay\nmechanism of Mamba, answering a fundamental question: what is the nature of\nMamba's long-range memory and how does it retain information? To quantify key\ninformation loss, we further introduce horizontal-vertical memory fidelity\nmetrics that capture degradation both within and across layers. Inspired by how\nhumans distill and retain salient information when reading long documents, we\npropose MemMamba, a novel architectural framework that integrates state\nsummarization mechanism together with cross-layer and cross-token attention,\nwhich alleviates long-range forgetting while preserving linear complexity.\nMemMamba achieves significant improvements over existing Mamba variants and\nTransformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,\nwhile delivering a 48% speedup in inference efficiency. Both theoretical\nanalysis and empirical results demonstrate that MemMamba achieves a\nbreakthrough in the complexity-memory trade-off, offering a new paradigm for\nultra-long sequence modeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03279.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67e655f7d6b8333a8f78eadf",
            "avatarUrl": "/avatars/11cc80d3c03747fd869e4dc1dbdd031a.svg",
            "fullname": "Jiaxuan Lu",
            "name": "Blue-Giant",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08377",
            "authors": [
                {
                    "_id": "68e8705d95e8e6771df38965",
                    "user": {
                        "_id": "64f8e358766ff9f3d2b0de84",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
                        "isPro": true,
                        "fullname": "Cong Wei",
                        "user": "CongWei1230",
                        "type": "user"
                    },
                    "name": "Cong Wei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:20.837Z",
                    "hidden": false
                },
                {
                    "_id": "68e8705d95e8e6771df38966",
                    "name": "Quande Liu",
                    "hidden": false
                },
                {
                    "_id": "68e8705d95e8e6771df38967",
                    "name": "Zixuan Ye",
                    "hidden": false
                },
                {
                    "_id": "68e8705d95e8e6771df38968",
                    "name": "Qiulin Wang",
                    "hidden": false
                },
                {
                    "_id": "68e8705d95e8e6771df38969",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "68e8705d95e8e6771df3896a",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "68e8705d95e8e6771df3896b",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "68e8705d95e8e6771df3896c",
                    "name": "Wenhu Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T16:01:30.000Z",
            "submittedOnDailyAt": "2025-10-10T01:03:12.654Z",
            "title": "UniVideo: Unified Understanding, Generation, and Editing for Videos",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Unified multimodal models have shown promising results in multimodal content\ngeneration and editing but remain largely limited to the image domain. In this\nwork, we present UniVideo, a versatile framework that extends unified modeling\nto the video domain. UniVideo adopts a dual-stream design, combining a\nMultimodal Large Language Model (MLLM) for instruction understanding with a\nMultimodal DiT (MMDiT) for video generation. This design enables accurate\ninterpretation of complex multimodal instructions while preserving visual\nconsistency. Built on this architecture, UniVideo unifies diverse video\ngeneration and editing tasks under a single multimodal instruction paradigm and\nis jointly trained across them. Extensive experiments demonstrate that UniVideo\nmatches or surpasses state-of-the-art task-specific baselines in\ntext/image-to-video generation, in-context video generation and in-context\nvideo editing. Notably, the unified design of UniVideo enables two forms of\ngeneralization. First, UniVideo supports task composition, such as combining\nediting with style transfer, by integrating multiple capabilities within a\nsingle instruction. Second, even without explicit training on free-form video\nediting, UniVideo transfers its editing capability from large-scale image\nediting data to this setting, handling unseen instructions such as\ngreen-screening characters or changing materials within a video. Beyond these\ncore capabilities, UniVideo also supports visual-prompt-based video generation,\nwhere the MLLM interprets visual prompts and guides the MMDiT during synthesis.\nTo foster future research, we will release our model and code.",
            "upvotes": 47,
            "discussionId": "68e8705d95e8e6771df3896d",
            "projectPage": "https://congwei1230.github.io/UniVideo/",
            "ai_summary": "UniVideo, a dual-stream framework combining a Multimodal Large Language Model and a Multimodal DiT, extends unified modeling to video generation and editing, achieving state-of-the-art performance and supporting task composition and generalization.",
            "ai_keywords": [
                "Multimodal Large Language Model",
                "Multimodal DiT",
                "dual-stream design",
                "text/image-to-video generation",
                "in-context video generation",
                "in-context video editing",
                "task composition",
                "style transfer",
                "visual-prompt-based video generation"
            ]
        },
        "publishedAt": "2025-10-09T12:01:30.000Z",
        "title": "UniVideo: Unified Understanding, Generation, and Editing for Videos",
        "summary": "Unified multimodal models have shown promising results in multimodal content\ngeneration and editing but remain largely limited to the image domain. In this\nwork, we present UniVideo, a versatile framework that extends unified modeling\nto the video domain. UniVideo adopts a dual-stream design, combining a\nMultimodal Large Language Model (MLLM) for instruction understanding with a\nMultimodal DiT (MMDiT) for video generation. This design enables accurate\ninterpretation of complex multimodal instructions while preserving visual\nconsistency. Built on this architecture, UniVideo unifies diverse video\ngeneration and editing tasks under a single multimodal instruction paradigm and\nis jointly trained across them. Extensive experiments demonstrate that UniVideo\nmatches or surpasses state-of-the-art task-specific baselines in\ntext/image-to-video generation, in-context video generation and in-context\nvideo editing. Notably, the unified design of UniVideo enables two forms of\ngeneralization. First, UniVideo supports task composition, such as combining\nediting with style transfer, by integrating multiple capabilities within a\nsingle instruction. Second, even without explicit training on free-form video\nediting, UniVideo transfers its editing capability from large-scale image\nediting data to this setting, handling unseen instructions such as\ngreen-screening characters or changing materials within a video. Beyond these\ncore capabilities, UniVideo also supports visual-prompt-based video generation,\nwhere the MLLM interprets visual prompts and guides the MMDiT during synthesis.\nTo foster future research, we will release our model and code.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08377.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 123
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.23768",
            "authors": [
                {
                    "_id": "68e88b7695e8e6771df38af4",
                    "user": {
                        "_id": "67c443afb753bd020f9c97d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xbACBNLSopWmN5G1K8h_Y.png",
                        "isPro": false,
                        "fullname": "Cheng",
                        "user": "YangC777",
                        "type": "user"
                    },
                    "name": "Cheng Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T08:55:23.886Z",
                    "hidden": false
                },
                {
                    "_id": "68e88b7695e8e6771df38af5",
                    "name": "Jiaxuan Lu",
                    "hidden": false
                },
                {
                    "_id": "68e88b7695e8e6771df38af6",
                    "user": {
                        "_id": "65a8bcb717d869bb7487c2a1",
                        "avatarUrl": "/avatars/261c28f7e616a8482970f50c1f8919fd.svg",
                        "isPro": false,
                        "fullname": "Haiyuan Wan",
                        "user": "haiyuanwan",
                        "type": "user"
                    },
                    "name": "Haiyuan Wan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T08:55:13.044Z",
                    "hidden": false
                },
                {
                    "_id": "68e88b7695e8e6771df38af7",
                    "name": "Junchi Yu",
                    "hidden": false
                },
                {
                    "_id": "68e88b7695e8e6771df38af8",
                    "name": "Feiwei Qin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T09:34:35.000Z",
            "submittedOnDailyAt": "2025-10-10T03:09:29.020Z",
            "title": "From What to Why: A Multi-Agent System for Evidence-based Chemical\n  Reaction Condition Reasoning",
            "submittedOnDailyBy": {
                "_id": "67e655f7d6b8333a8f78eadf",
                "avatarUrl": "/avatars/11cc80d3c03747fd869e4dc1dbdd031a.svg",
                "isPro": false,
                "fullname": "Jiaxuan Lu",
                "user": "Blue-Giant",
                "type": "user"
            },
            "summary": "The chemical reaction recommendation is to select proper reaction condition\nparameters for chemical reactions, which is pivotal to accelerating chemical\nscience. With the rapid development of large language models (LLMs), there is\ngrowing interest in leveraging their reasoning and planning capabilities for\nreaction condition recommendation. Despite their success, existing methods\nrarely explain the rationale behind the recommended reaction conditions,\nlimiting their utility in high-stakes scientific workflows. In this work, we\npropose ChemMAS, a multi-agent system that reframes condition prediction as an\nevidence-based reasoning task. ChemMAS decomposes the task into mechanistic\ngrounding, multi-channel recall, constraint-aware agentic debate, and rationale\naggregation. Each decision is backed by interpretable justifications grounded\nin chemical knowledge and retrieved precedents. Experiments show that ChemMAS\nachieves 20-35% gains over domain-specific baselines and outperforms\ngeneral-purpose LLMs by 10-15% in Top-1 accuracy, while offering falsifiable,\nhuman-trustable rationales, which establishes a new paradigm for explainable AI\nin scientific discovery.",
            "upvotes": 42,
            "discussionId": "68e88b7795e8e6771df38af9",
            "ai_summary": "ChemMAS, a multi-agent system, improves reaction condition recommendation by providing interpretable rationales, outperforming existing methods in accuracy and explainability.",
            "ai_keywords": [
                "multi-agent system",
                "evidence-based reasoning",
                "mechanistic grounding",
                "multi-channel recall",
                "constraint-aware agentic debate",
                "rationale aggregation",
                "explainable AI"
            ]
        },
        "publishedAt": "2025-09-28T05:34:35.000Z",
        "title": "From What to Why: A Multi-Agent System for Evidence-based Chemical\n  Reaction Condition Reasoning",
        "summary": "The chemical reaction recommendation is to select proper reaction condition\nparameters for chemical reactions, which is pivotal to accelerating chemical\nscience. With the rapid development of large language models (LLMs), there is\ngrowing interest in leveraging their reasoning and planning capabilities for\nreaction condition recommendation. Despite their success, existing methods\nrarely explain the rationale behind the recommended reaction conditions,\nlimiting their utility in high-stakes scientific workflows. In this work, we\npropose ChemMAS, a multi-agent system that reframes condition prediction as an\nevidence-based reasoning task. ChemMAS decomposes the task into mechanistic\ngrounding, multi-channel recall, constraint-aware agentic debate, and rationale\naggregation. Each decision is backed by interpretable justifications grounded\nin chemical knowledge and retrieved precedents. Experiments show that ChemMAS\nachieves 20-35% gains over domain-specific baselines and outperforms\ngeneral-purpose LLMs by 10-15% in Top-1 accuracy, while offering falsifiable,\nhuman-trustable rationales, which establishes a new paradigm for explainable AI\nin scientific discovery.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23768.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67e655f7d6b8333a8f78eadf",
            "avatarUrl": "/avatars/11cc80d3c03747fd869e4dc1dbdd031a.svg",
            "fullname": "Jiaxuan Lu",
            "name": "Blue-Giant",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.03259",
            "authors": [
                {
                    "_id": "68e88d9c95e8e6771df38b0b",
                    "name": "Yoonjeon Kim",
                    "hidden": false
                },
                {
                    "_id": "68e88d9c95e8e6771df38b0c",
                    "user": {
                        "_id": "62845957b410bd779033759c",
                        "avatarUrl": "/avatars/4feef73c06f2f7de6abf7a4789ac13f9.svg",
                        "isPro": false,
                        "fullname": "Doohyuk Jang",
                        "user": "jadohu",
                        "type": "user"
                    },
                    "name": "Doohyuk Jang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T08:54:37.386Z",
                    "hidden": false
                },
                {
                    "_id": "68e88d9c95e8e6771df38b0d",
                    "name": "Eunho Yang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/61b15ce1a5dd7dc7024406dc/JXdIRl1b-F4NEYxvcMDBl.jpeg"
            ],
            "publishedAt": "2025-09-26T14:05:48.000Z",
            "submittedOnDailyAt": "2025-10-10T03:12:46.026Z",
            "title": "Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement\n  Learning",
            "submittedOnDailyBy": {
                "_id": "61b15ce1a5dd7dc7024406dc",
                "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
                "isPro": false,
                "fullname": "Yoonjeon Kim",
                "user": "yjyjyj98",
                "type": "user"
            },
            "summary": "Recent studies on reasoning models explore the meta-awareness of language\nmodels, the ability to know how to think by itself. We argue that large\nreasoning models lack this meta-awareness property by proving severe\nmisalignment between true rollouts and predicted meta information. We posit\nthat aligning meta-prediction with true rollouts will lead to significant\nperformance gains. To verify this hypothesis, we design a training pipeline\nthat boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced\nmeta-awareness directly translates to improved accuracy. Unlike existing\nmeta-cognitive reasoning models, our method does not require external training\nsources but leverages self-generated signals to train meta-awareness. Moreover,\nour method enables efficient training by i) filtering out zero-variance prompts\nthat are either trivial or unsolvable and ii) cutting off lengthy rollouts when\nthey are unlikely to lead to correct answers. The results are inspiring: our\nstrategy yields significant improvements in both accuracy and training\nefficiency on in-domain tasks and shows strong generalization to out-of-domain\nbenchmarks. More specifically, our method can speed up GRPO training by over\n1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on\nAIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with\nmeta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %\nboost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks\nspanning logical, scientific, and coding domains.",
            "upvotes": 38,
            "discussionId": "68e88d9c95e8e6771df38b0e",
            "githubRepo": "https://github.com/akatigre/MASA-RL/tree/main",
            "ai_summary": "A training pipeline called MASA enhances meta-awareness in reasoning models, leading to improved accuracy and efficiency across various benchmarks.",
            "ai_keywords": [
                "meta-awareness",
                "reasoning models",
                "meta-prediction",
                "rollouts",
                "self-alignment",
                "zero-variance prompts",
                "GRPO",
                "AIME25",
                "GPQA-Diamond"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "6475760c33192631bad2bb38",
                "name": "kaist-ai",
                "fullname": "KAIST AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
            }
        },
        "publishedAt": "2025-09-26T10:05:48.000Z",
        "title": "Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement\n  Learning",
        "summary": "Recent studies on reasoning models explore the meta-awareness of language\nmodels, the ability to know how to think by itself. We argue that large\nreasoning models lack this meta-awareness property by proving severe\nmisalignment between true rollouts and predicted meta information. We posit\nthat aligning meta-prediction with true rollouts will lead to significant\nperformance gains. To verify this hypothesis, we design a training pipeline\nthat boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced\nmeta-awareness directly translates to improved accuracy. Unlike existing\nmeta-cognitive reasoning models, our method does not require external training\nsources but leverages self-generated signals to train meta-awareness. Moreover,\nour method enables efficient training by i) filtering out zero-variance prompts\nthat are either trivial or unsolvable and ii) cutting off lengthy rollouts when\nthey are unlikely to lead to correct answers. The results are inspiring: our\nstrategy yields significant improvements in both accuracy and training\nefficiency on in-domain tasks and shows strong generalization to out-of-domain\nbenchmarks. More specifically, our method can speed up GRPO training by over\n1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on\nAIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with\nmeta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %\nboost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks\nspanning logical, scientific, and coding domains.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/61b15ce1a5dd7dc7024406dc/JXdIRl1b-F4NEYxvcMDBl.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03259.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61b15ce1a5dd7dc7024406dc",
            "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
            "fullname": "Yoonjeon Kim",
            "name": "yjyjyj98",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "6475760c33192631bad2bb38",
            "name": "kaist-ai",
            "fullname": "KAIST AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07499",
            "authors": [
                {
                    "_id": "68e881c695e8e6771df38a69",
                    "user": {
                        "_id": "64e5a1cd4c20016ec9020ec8",
                        "avatarUrl": "/avatars/d7ffe7fbbe39c0a013375357457c57b3.svg",
                        "isPro": false,
                        "fullname": "Soyeong",
                        "user": "starsuzi",
                        "type": "user"
                    },
                    "name": "Soyeong Jeong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T08:55:34.734Z",
                    "hidden": false
                },
                {
                    "_id": "68e881c695e8e6771df38a6a",
                    "name": "Taehee Jung",
                    "hidden": false
                },
                {
                    "_id": "68e881c695e8e6771df38a6b",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                },
                {
                    "_id": "68e881c695e8e6771df38a6c",
                    "name": "Joo-Kyung Kim",
                    "hidden": false
                },
                {
                    "_id": "68e881c695e8e6771df38a6d",
                    "name": "Dongyeop Kang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T19:52:35.000Z",
            "submittedOnDailyAt": "2025-10-10T02:20:07.553Z",
            "title": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs",
            "submittedOnDailyBy": {
                "_id": "64e5a1cd4c20016ec9020ec8",
                "avatarUrl": "/avatars/d7ffe7fbbe39c0a013375357457c57b3.svg",
                "isPro": false,
                "fullname": "Soyeong",
                "user": "starsuzi",
                "type": "user"
            },
            "summary": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands\nof tokens in a single prompt, enabling new opportunities for\nknowledge-intensive multi-hop reasoning by integrating large sets of retrieved\ndocuments or, in some cases, directly all necessary information. However,\nsimply feeding more documents into the context window fails to capture how\nevidence should be connected. We address this gap with thought templates, which\nrecast reasoning as reusable thought caches, derived from prior problem solving\ntraces, structuring how evidence is combined and guiding multi-hop inference\nwith factual documents. To keep these templates effective, we propose an update\nstrategy that iteratively refines templates derived from training data through\nnatural-language feedback. Across diverse benchmarks and LCLM families, our\napproach delivers consistent gains over strong baselines in both\nretrieval-based and retrieval-free settings. Furthermore, we show that\noptimized templates can be distilled into smaller open-source models,\ndemonstrating its broad applicability and transparent reasoning reuse. We refer\nto our framework as Thought Template Augmented LCLMs (ToTAL).",
            "upvotes": 37,
            "discussionId": "68e881c795e8e6771df38a6e",
            "ai_summary": "Thought templates enhance long-context language models by structuring evidence combination and guiding multi-hop inference, leading to consistent performance improvements across various benchmarks.",
            "ai_keywords": [
                "Long-Context Language Models",
                "thought templates",
                "reusable thought caches",
                "multi-hop reasoning",
                "evidence combination",
                "natural-language feedback",
                "retrieval-based",
                "retrieval-free",
                "ToTAL"
            ],
            "organization": {
                "_id": "5ffdfbadbba2ae614d771970",
                "name": "amazon",
                "fullname": "Amazon",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
            }
        },
        "publishedAt": "2025-10-08T15:52:35.000Z",
        "title": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs",
        "summary": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands\nof tokens in a single prompt, enabling new opportunities for\nknowledge-intensive multi-hop reasoning by integrating large sets of retrieved\ndocuments or, in some cases, directly all necessary information. However,\nsimply feeding more documents into the context window fails to capture how\nevidence should be connected. We address this gap with thought templates, which\nrecast reasoning as reusable thought caches, derived from prior problem solving\ntraces, structuring how evidence is combined and guiding multi-hop inference\nwith factual documents. To keep these templates effective, we propose an update\nstrategy that iteratively refines templates derived from training data through\nnatural-language feedback. Across diverse benchmarks and LCLM families, our\napproach delivers consistent gains over strong baselines in both\nretrieval-based and retrieval-free settings. Furthermore, we show that\noptimized templates can be distilled into smaller open-source models,\ndemonstrating its broad applicability and transparent reasoning reuse. We refer\nto our framework as Thought Template Augmented LCLMs (ToTAL).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07499.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e5a1cd4c20016ec9020ec8",
            "avatarUrl": "/avatars/d7ffe7fbbe39c0a013375357457c57b3.svg",
            "fullname": "Soyeong",
            "name": "starsuzi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "organization": {
            "_id": "5ffdfbadbba2ae614d771970",
            "name": "amazon",
            "fullname": "Amazon",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08555",
            "authors": [
                {
                    "_id": "68e86e1995e8e6771df38919",
                    "user": {
                        "_id": "64f94370c3c12b377cc51086",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f94370c3c12b377cc51086/6CXcHhqAoykqXcShqM8Rd.jpeg",
                        "isPro": false,
                        "fullname": "Minghong Cai",
                        "user": "onevfall",
                        "type": "user"
                    },
                    "name": "Minghong Cai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:50.095Z",
                    "hidden": false
                },
                {
                    "_id": "68e86e1995e8e6771df3891a",
                    "name": "Qiulin Wang",
                    "hidden": false
                },
                {
                    "_id": "68e86e1995e8e6771df3891b",
                    "name": "Zongli Ye",
                    "hidden": false
                },
                {
                    "_id": "68e86e1995e8e6771df3891c",
                    "name": "Wenze Liu",
                    "hidden": false
                },
                {
                    "_id": "68e86e1995e8e6771df3891d",
                    "name": "Quande Liu",
                    "hidden": false
                },
                {
                    "_id": "68e86e1995e8e6771df3891e",
                    "name": "Weicai Ye",
                    "hidden": false
                },
                {
                    "_id": "68e86e1995e8e6771df3891f",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "68e86e1995e8e6771df38920",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "68e86e1995e8e6771df38921",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "68e86e1995e8e6771df38922",
                    "name": "Xiangyu Yue",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T17:58:59.000Z",
            "submittedOnDailyAt": "2025-10-10T00:53:33.548Z",
            "title": "VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal\n  Patches via In-Context Conditioning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce the task of arbitrary spatio-temporal video completion, where a\nvideo is generated from arbitrary, user-specified patches placed at any spatial\nlocation and timestamp, akin to painting on a video canvas. This flexible\nformulation naturally unifies many existing controllable video generation\ntasks--including first-frame image-to-video, inpainting, extension, and\ninterpolation--under a single, cohesive paradigm. Realizing this vision,\nhowever, faces a fundamental obstacle in modern latent video diffusion models:\nthe temporal ambiguity introduced by causal VAEs, where multiple pixel frames\nare compressed into a single latent representation, making precise frame-level\nconditioning structurally difficult. We address this challenge with\nVideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC)\nparadigm to this fine-grained control task with zero new parameters. We propose\na hybrid conditioning strategy that decouples spatial and temporal control:\nspatial placement is handled via zero-padding, while temporal alignment is\nachieved through Temporal RoPE Interpolation, which assigns each condition a\ncontinuous fractional position within the latent sequence. This resolves the\nVAE's temporal ambiguity and enables pixel-frame-aware control on a frozen\nbackbone. To evaluate this new capability, we develop VideoCanvasBench, the\nfirst benchmark for arbitrary spatio-temporal video completion, covering both\nintra-scene fidelity and inter-scene creativity. Experiments demonstrate that\nVideoCanvas significantly outperforms existing conditioning paradigms,\nestablishing a new state of the art in flexible and unified video generation.",
            "upvotes": 36,
            "discussionId": "68e86e1995e8e6771df38923",
            "projectPage": "https://onevfall.github.io/project_page/videocanvas/",
            "ai_summary": "VideoCanvas addresses temporal ambiguity in latent video diffusion models to enable flexible spatio-temporal video completion using a hybrid conditioning strategy.",
            "ai_keywords": [
                "arbitrary spatio-temporal video completion",
                "latent video diffusion models",
                "causal VAEs",
                "VideoCanvas",
                "In-Context Conditioning (ICC)",
                "zero-padding",
                "Temporal RoPE Interpolation",
                "VideoCanvasBench",
                "intra-scene fidelity",
                "inter-scene creativity"
            ]
        },
        "publishedAt": "2025-10-09T13:58:59.000Z",
        "title": "VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal\n  Patches via In-Context Conditioning",
        "summary": "We introduce the task of arbitrary spatio-temporal video completion, where a\nvideo is generated from arbitrary, user-specified patches placed at any spatial\nlocation and timestamp, akin to painting on a video canvas. This flexible\nformulation naturally unifies many existing controllable video generation\ntasks--including first-frame image-to-video, inpainting, extension, and\ninterpolation--under a single, cohesive paradigm. Realizing this vision,\nhowever, faces a fundamental obstacle in modern latent video diffusion models:\nthe temporal ambiguity introduced by causal VAEs, where multiple pixel frames\nare compressed into a single latent representation, making precise frame-level\nconditioning structurally difficult. We address this challenge with\nVideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC)\nparadigm to this fine-grained control task with zero new parameters. We propose\na hybrid conditioning strategy that decouples spatial and temporal control:\nspatial placement is handled via zero-padding, while temporal alignment is\nachieved through Temporal RoPE Interpolation, which assigns each condition a\ncontinuous fractional position within the latent sequence. This resolves the\nVAE's temporal ambiguity and enables pixel-frame-aware control on a frozen\nbackbone. To evaluate this new capability, we develop VideoCanvasBench, the\nfirst benchmark for arbitrary spatio-temporal video completion, covering both\nintra-scene fidelity and inter-scene creativity. Experiments demonstrate that\nVideoCanvas significantly outperforms existing conditioning paradigms,\nestablishing a new state of the art in flexible and unified video generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08555.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 123
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08240",
            "authors": [
                {
                    "_id": "68e87d9195e8e6771df38a45",
                    "name": "Jingyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e87d9195e8e6771df38a46",
                    "name": "Haozhu Wang",
                    "hidden": false
                },
                {
                    "_id": "68e87d9195e8e6771df38a47",
                    "name": "Eric Michael Smith",
                    "hidden": false
                },
                {
                    "_id": "68e87d9195e8e6771df38a48",
                    "name": "Sid Wang",
                    "hidden": false
                },
                {
                    "_id": "68e87d9195e8e6771df38a49",
                    "name": "Amr Sharaf",
                    "hidden": false
                },
                {
                    "_id": "68e87d9195e8e6771df38a4a",
                    "name": "Mahesh Pasupuleti",
                    "hidden": false
                },
                {
                    "_id": "68e87d9195e8e6771df38a4b",
                    "name": "Benjamin Van Durme",
                    "hidden": false
                },
                {
                    "_id": "68e87d9195e8e6771df38a4c",
                    "name": "Daniel Khashabi",
                    "hidden": false
                },
                {
                    "_id": "68e87d9195e8e6771df38a4d",
                    "name": "Jason Weston",
                    "hidden": false
                },
                {
                    "_id": "68e87d9195e8e6771df38a4e",
                    "name": "Hongyuan Zhan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T14:03:05.000Z",
            "submittedOnDailyAt": "2025-10-10T01:59:57.344Z",
            "title": "The Alignment Waltz: Jointly Training Agents to Collaborate for Safety",
            "submittedOnDailyBy": {
                "_id": "62fb40b59af1d16bc0ac60f4",
                "avatarUrl": "/avatars/03ff66a419db8f2bc8e89a3b47aaaeac.svg",
                "isPro": false,
                "fullname": "Jack Zhang",
                "user": "jackzhang",
                "type": "user"
            },
            "summary": "Harnessing the power of LLMs requires a delicate dance between being helpful\nand harmless. This creates a fundamental tension between two competing\nchallenges: vulnerability to adversarial attacks that elicit unsafe content,\nand a tendency for overrefusal on benign but sensitive prompts. Current\napproaches often navigate this dance with safeguard models that completely\nreject any content that contains unsafe portions. This approach cuts the music\nentirely-it may exacerbate overrefusals and fails to provide nuanced guidance\nfor queries it refuses. To teach models a more coordinated choreography, we\npropose WaltzRL, a novel multi-agent reinforcement learning framework that\nformulates safety alignment as a collaborative, positive-sum game. WaltzRL\njointly trains a conversation agent and a feedback agent, where the latter is\nincentivized to provide useful suggestions that improve the safety and\nhelpfulness of the conversation agent's responses. At the core of WaltzRL is a\nDynamic Improvement Reward (DIR) that evolves over time based on how well the\nconversation agent incorporates the feedback. At inference time, unsafe or\noverrefusing responses from the conversation agent are improved rather than\ndiscarded. The feedback agent is deployed together with the conversation agent\nand only engages adaptively when needed, preserving helpfulness and low latency\non safe queries. Our experiments, conducted across five diverse datasets,\ndemonstrate that WaltzRL significantly reduces both unsafe responses (e.g.,\nfrom 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on\nOR-Bench) compared to various baselines. By enabling the conversation and\nfeedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances\nLLM safety without degrading general capabilities, thereby advancing the Pareto\nfront between helpfulness and harmlessness.",
            "upvotes": 33,
            "discussionId": "68e87d9195e8e6771df38a4f",
            "ai_summary": "WaltzRL, a multi-agent reinforcement learning framework, improves LLM safety and helpfulness by collaboratively training a conversation agent and a feedback agent, reducing unsafe responses and overrefusals.",
            "ai_keywords": [
                "LLMs",
                "adversarial attacks",
                "safeguard models",
                "multi-agent reinforcement learning",
                "conversation agent",
                "feedback agent",
                "Dynamic Improvement Reward",
                "DIR",
                "WildJailbreak",
                "OR-Bench",
                "Pareto front"
            ],
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "publishedAt": "2025-10-09T10:03:05.000Z",
        "title": "The Alignment Waltz: Jointly Training Agents to Collaborate for Safety",
        "summary": "Harnessing the power of LLMs requires a delicate dance between being helpful\nand harmless. This creates a fundamental tension between two competing\nchallenges: vulnerability to adversarial attacks that elicit unsafe content,\nand a tendency for overrefusal on benign but sensitive prompts. Current\napproaches often navigate this dance with safeguard models that completely\nreject any content that contains unsafe portions. This approach cuts the music\nentirely-it may exacerbate overrefusals and fails to provide nuanced guidance\nfor queries it refuses. To teach models a more coordinated choreography, we\npropose WaltzRL, a novel multi-agent reinforcement learning framework that\nformulates safety alignment as a collaborative, positive-sum game. WaltzRL\njointly trains a conversation agent and a feedback agent, where the latter is\nincentivized to provide useful suggestions that improve the safety and\nhelpfulness of the conversation agent's responses. At the core of WaltzRL is a\nDynamic Improvement Reward (DIR) that evolves over time based on how well the\nconversation agent incorporates the feedback. At inference time, unsafe or\noverrefusing responses from the conversation agent are improved rather than\ndiscarded. The feedback agent is deployed together with the conversation agent\nand only engages adaptively when needed, preserving helpfulness and low latency\non safe queries. Our experiments, conducted across five diverse datasets,\ndemonstrate that WaltzRL significantly reduces both unsafe responses (e.g.,\nfrom 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on\nOR-Bench) compared to various baselines. By enabling the conversation and\nfeedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances\nLLM safety without degrading general capabilities, thereby advancing the Pareto\nfront between helpfulness and harmlessness.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08240.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62fb40b59af1d16bc0ac60f4",
            "avatarUrl": "/avatars/03ff66a419db8f2bc8e89a3b47aaaeac.svg",
            "fullname": "Jack Zhang",
            "name": "jackzhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07242",
            "authors": [
                {
                    "_id": "68e8722c95e8e6771df38987",
                    "user": {
                        "_id": "655eca47e1b6d15cfe8f8235",
                        "avatarUrl": "/avatars/1ab847d12957a8ab77c84bdc452e3f5f.svg",
                        "isPro": false,
                        "fullname": "Tao",
                        "user": "Leitian",
                        "type": "user"
                    },
                    "name": "Leitian Tao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:16.240Z",
                    "hidden": false
                },
                {
                    "_id": "68e8722c95e8e6771df38988",
                    "name": "Ilia Kulikov",
                    "hidden": false
                },
                {
                    "_id": "68e8722c95e8e6771df38989",
                    "name": "Swarnadeep Saha",
                    "hidden": false
                },
                {
                    "_id": "68e8722c95e8e6771df3898a",
                    "name": "Tianlu Wang",
                    "hidden": false
                },
                {
                    "_id": "68e8722c95e8e6771df3898b",
                    "name": "Jing Xu",
                    "hidden": false
                },
                {
                    "_id": "68e8722c95e8e6771df3898c",
                    "name": "Yixuan Li",
                    "hidden": false
                },
                {
                    "_id": "68e8722c95e8e6771df3898d",
                    "name": "Jason E Weston",
                    "hidden": false
                },
                {
                    "_id": "68e8722c95e8e6771df3898e",
                    "name": "Ping Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T17:09:41.000Z",
            "submittedOnDailyAt": "2025-10-10T01:40:32.371Z",
            "title": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense",
            "submittedOnDailyBy": {
                "_id": "64be6ffa805e5b6457363108",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be6ffa805e5b6457363108/FH_gqlgyQvoF-QFE42Yjc.jpeg",
                "isPro": false,
                "fullname": "Lin Long",
                "user": "Kylin-ll",
                "type": "user"
            },
            "summary": "Post-training for reasoning of large language models (LLMs) increasingly\nrelies on verifiable rewards: deterministic checkers that provide 0-1\ncorrectness signals. While reliable, such binary feedback is brittle--many\ntasks admit partially correct or alternative answers that verifiers\nunder-credit, and the resulting all-or-nothing supervision limits learning.\nReward models offer richer, continuous feedback, which can serve as a\ncomplementary supervisory signal to verifiers. We introduce HERO (Hybrid\nEnsemble Reward Optimization), a reinforcement learning framework that\nintegrates verifier signals with reward-model scores in a structured way. HERO\nemploys stratified normalization to bound reward-model scores within\nverifier-defined groups, preserving correctness while refining quality\ndistinctions, and variance-aware weighting to emphasize challenging prompts\nwhere dense signals matter most. Across diverse mathematical reasoning\nbenchmarks, HERO consistently outperforms RM-only and verifier-only baselines,\nwith strong gains on both verifiable and hard-to-verify tasks. Our results show\nthat hybrid reward design retains the stability of verifiers while leveraging\nthe nuance of reward models to advance reasoning.",
            "upvotes": 26,
            "discussionId": "68e8722d95e8e6771df3898f",
            "ai_summary": "HERO, a reinforcement learning framework, combines verifier signals with reward-model scores to enhance reasoning in large language models, outperforming both RM-only and verifier-only methods.",
            "ai_keywords": [
                "verifiable rewards",
                "deterministic checkers",
                "binary feedback",
                "reward models",
                "continuous feedback",
                "reinforcement learning",
                "HERO",
                "Hybrid Ensemble Reward Optimization",
                "stratified normalization",
                "variance-aware weighting",
                "mathematical reasoning benchmarks"
            ],
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "publishedAt": "2025-10-08T13:09:41.000Z",
        "title": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense",
        "summary": "Post-training for reasoning of large language models (LLMs) increasingly\nrelies on verifiable rewards: deterministic checkers that provide 0-1\ncorrectness signals. While reliable, such binary feedback is brittle--many\ntasks admit partially correct or alternative answers that verifiers\nunder-credit, and the resulting all-or-nothing supervision limits learning.\nReward models offer richer, continuous feedback, which can serve as a\ncomplementary supervisory signal to verifiers. We introduce HERO (Hybrid\nEnsemble Reward Optimization), a reinforcement learning framework that\nintegrates verifier signals with reward-model scores in a structured way. HERO\nemploys stratified normalization to bound reward-model scores within\nverifier-defined groups, preserving correctness while refining quality\ndistinctions, and variance-aware weighting to emphasize challenging prompts\nwhere dense signals matter most. Across diverse mathematical reasoning\nbenchmarks, HERO consistently outperforms RM-only and verifier-only baselines,\nwith strong gains on both verifiable and hard-to-verify tasks. Our results show\nthat hybrid reward design retains the stability of verifiers while leveraging\nthe nuance of reward models to advance reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07242.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64be6ffa805e5b6457363108",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be6ffa805e5b6457363108/FH_gqlgyQvoF-QFE42Yjc.jpeg",
            "fullname": "Lin Long",
            "name": "Kylin-ll",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 0
        },
        "organization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07172",
            "authors": [
                {
                    "_id": "68e7b69644180c42eca8afe9",
                    "name": "Tianshi Zheng",
                    "hidden": false
                },
                {
                    "_id": "68e7b69644180c42eca8afea",
                    "name": "Kelvin Kiu-Wai Tam",
                    "hidden": false
                },
                {
                    "_id": "68e7b69644180c42eca8afeb",
                    "name": "Newt Hue-Nam K. Nguyen",
                    "hidden": false
                },
                {
                    "_id": "68e7b69644180c42eca8afec",
                    "name": "Baixuan Xu",
                    "hidden": false
                },
                {
                    "_id": "68e7b69644180c42eca8afed",
                    "name": "Zhaowei Wang",
                    "hidden": false
                },
                {
                    "_id": "68e7b69644180c42eca8afee",
                    "name": "Jiayang Cheng",
                    "hidden": false
                },
                {
                    "_id": "68e7b69644180c42eca8afef",
                    "name": "Hong Ting Tsang",
                    "hidden": false
                },
                {
                    "_id": "68e7b69644180c42eca8aff0",
                    "name": "Weiqi Wang",
                    "hidden": false
                },
                {
                    "_id": "68e7b69644180c42eca8aff1",
                    "name": "Jiaxin Bai",
                    "hidden": false
                },
                {
                    "_id": "68e7b69644180c42eca8aff2",
                    "user": {
                        "_id": "641129818573c51c0458b793",
                        "avatarUrl": "/avatars/d4bc67c160a07146cf41c614678aa36b.svg",
                        "isPro": false,
                        "fullname": "Tianqing Fang",
                        "user": "tqfang229",
                        "type": "user"
                    },
                    "name": "Tianqing Fang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T09:13:21.705Z",
                    "hidden": false
                },
                {
                    "_id": "68e7b69644180c42eca8aff3",
                    "name": "Yangqiu Song",
                    "hidden": false
                },
                {
                    "_id": "68e7b69644180c42eca8aff4",
                    "name": "Ginny Y. Wong",
                    "hidden": false
                },
                {
                    "_id": "68e7b69644180c42eca8aff5",
                    "name": "Simon See",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T16:12:11.000Z",
            "submittedOnDailyAt": "2025-10-10T01:00:49.443Z",
            "title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM\n  Agents",
            "submittedOnDailyBy": {
                "_id": "641129818573c51c0458b793",
                "avatarUrl": "/avatars/d4bc67c160a07146cf41c614678aa36b.svg",
                "isPro": false,
                "fullname": "Tianqing Fang",
                "user": "tqfang229",
                "type": "user"
            },
            "summary": "Large language models are emerging as powerful tools for scientific law\ndiscovery, a foundational challenge in AI-driven science. However, existing\nbenchmarks for this task suffer from a fundamental methodological trilemma,\nforcing a trade-off between scientific relevance, scalability, and resistance\nto memorization. Furthermore, they oversimplify discovery as static function\nfitting, failing to capture the authentic scientific process of uncovering\nembedded laws through the interactive exploration of complex model systems. To\naddress these critical gaps, we introduce NewtonBench, a benchmark comprising\n324 scientific law discovery tasks across 12 physics domains. Our design\nmitigates the evaluation trilemma by using metaphysical shifts - systematic\nalterations of canonical laws - to generate a vast suite of problems that are\nscalable, scientifically relevant, and memorization-resistant. Moreover, we\nelevate the evaluation from static function fitting to interactive model\ndiscovery, requiring agents to experimentally probe simulated complex systems\nto uncover hidden principles. Our extensive experiment reveals a clear but\nfragile capability for discovery in frontier LLMs: this ability degrades\nprecipitously with increasing system complexity and exhibits extreme\nsensitivity to observational noise. Notably, we uncover a paradoxical effect of\ntool assistance: providing a code interpreter can hinder more capable models by\ninducing a premature shift from exploration to exploitation, causing them to\nsatisfice on suboptimal solutions. These results demonstrate that robust,\ngeneralizable discovery in complex, interactive environments remains the core\nchallenge. By providing a scalable, robust, and scientifically authentic\ntestbed, NewtonBench offers a crucial tool for measuring true progress and\nguiding the development of next-generation AI agents capable of genuine\nscientific discovery.",
            "upvotes": 26,
            "discussionId": "68e7b69744180c42eca8aff6",
            "ai_summary": "NewtonBench is a benchmark for scientific law discovery that addresses scalability, scientific relevance, and memorization resistance by using metaphysical shifts and interactive model discovery.",
            "ai_keywords": [
                "large language models",
                "scientific law discovery",
                "NewtonBench",
                "metaphysical shifts",
                "interactive model discovery",
                "observational noise",
                "code interpreter",
                "exploration",
                "exploitation"
            ]
        },
        "publishedAt": "2025-10-08T12:12:11.000Z",
        "title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM\n  Agents",
        "summary": "Large language models are emerging as powerful tools for scientific law\ndiscovery, a foundational challenge in AI-driven science. However, existing\nbenchmarks for this task suffer from a fundamental methodological trilemma,\nforcing a trade-off between scientific relevance, scalability, and resistance\nto memorization. Furthermore, they oversimplify discovery as static function\nfitting, failing to capture the authentic scientific process of uncovering\nembedded laws through the interactive exploration of complex model systems. To\naddress these critical gaps, we introduce NewtonBench, a benchmark comprising\n324 scientific law discovery tasks across 12 physics domains. Our design\nmitigates the evaluation trilemma by using metaphysical shifts - systematic\nalterations of canonical laws - to generate a vast suite of problems that are\nscalable, scientifically relevant, and memorization-resistant. Moreover, we\nelevate the evaluation from static function fitting to interactive model\ndiscovery, requiring agents to experimentally probe simulated complex systems\nto uncover hidden principles. Our extensive experiment reveals a clear but\nfragile capability for discovery in frontier LLMs: this ability degrades\nprecipitously with increasing system complexity and exhibits extreme\nsensitivity to observational noise. Notably, we uncover a paradoxical effect of\ntool assistance: providing a code interpreter can hinder more capable models by\ninducing a premature shift from exploration to exploitation, causing them to\nsatisfice on suboptimal solutions. These results demonstrate that robust,\ngeneralizable discovery in complex, interactive environments remains the core\nchallenge. By providing a scalable, robust, and scientifically authentic\ntestbed, NewtonBench offers a crucial tool for measuring true progress and\nguiding the development of next-generation AI agents capable of genuine\nscientific discovery.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07172.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "641129818573c51c0458b793",
            "avatarUrl": "/avatars/d4bc67c160a07146cf41c614678aa36b.svg",
            "fullname": "Tianqing Fang",
            "name": "tqfang229",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08551",
            "authors": [
                {
                    "_id": "68e87dd895e8e6771df38a51",
                    "name": "Guanghao Li",
                    "hidden": false
                },
                {
                    "_id": "68e87dd895e8e6771df38a52",
                    "name": "Kerui Ren",
                    "hidden": false
                },
                {
                    "_id": "68e87dd895e8e6771df38a53",
                    "name": "Linning Xu",
                    "hidden": false
                },
                {
                    "_id": "68e87dd895e8e6771df38a54",
                    "name": "Zhewen Zheng",
                    "hidden": false
                },
                {
                    "_id": "68e87dd895e8e6771df38a55",
                    "name": "Changjian Jiang",
                    "hidden": false
                },
                {
                    "_id": "68e87dd895e8e6771df38a56",
                    "name": "Xin Gao",
                    "hidden": false
                },
                {
                    "_id": "68e87dd895e8e6771df38a57",
                    "name": "Bo Dai",
                    "hidden": false
                },
                {
                    "_id": "68e87dd895e8e6771df38a58",
                    "name": "Jian Pu",
                    "hidden": false
                },
                {
                    "_id": "68e87dd895e8e6771df38a59",
                    "name": "Mulin Yu",
                    "hidden": false
                },
                {
                    "_id": "68e87dd895e8e6771df38a5a",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/656e9e26bbec423d88b603e8/JeTCgAwYV1ncSGkXgmOFL.png"
            ],
            "publishedAt": "2025-10-09T17:57:38.000Z",
            "submittedOnDailyAt": "2025-10-10T02:06:04.673Z",
            "title": "ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D\n  Reconstruction with Structured Scene Representation",
            "submittedOnDailyBy": {
                "_id": "656e9e26bbec423d88b603e8",
                "avatarUrl": "/avatars/10d8cb945a60e0401bfa4f74137cb203.svg",
                "isPro": false,
                "fullname": "MulinYu",
                "user": "UML",
                "type": "user"
            },
            "summary": "On-the-fly 3D reconstruction from monocular image sequences is a\nlong-standing challenge in computer vision, critical for applications such as\nreal-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff:\nper-scene optimization yields high fidelity but is computationally expensive,\nwhereas feed-forward foundation models enable real-time inference but struggle\nwith accuracy and robustness. In this work, we propose ARTDECO, a unified\nframework that combines the efficiency of feed-forward models with the\nreliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose\nestimation and point prediction, coupled with a Gaussian decoder that\ntransforms multi-scale features into structured 3D Gaussians. To sustain both\nfidelity and efficiency at scale, we design a hierarchical Gaussian\nrepresentation with a LoD-aware rendering strategy, which improves rendering\nfidelity while reducing redundancy. Experiments on eight diverse indoor and\noutdoor benchmarks show that ARTDECO delivers interactive performance\ncomparable to SLAM, robustness similar to feed-forward systems, and\nreconstruction quality close to per-scene optimization, providing a practical\npath toward on-the-fly digitization of real-world environments with both\naccurate geometry and high visual fidelity. Explore more demos on our project\npage: https://city-super.github.io/artdeco/.",
            "upvotes": 21,
            "discussionId": "68e87dd895e8e6771df38a5b",
            "projectPage": "https://city-super.github.io/artdeco/",
            "githubRepo": "https://github.com/InternRobotics/ARTDECO",
            "ai_summary": "ARTDECO combines feed-forward models and SLAM pipelines for efficient and accurate 3D reconstruction from monocular images.",
            "ai_keywords": [
                "3D reconstruction",
                "monocular image sequences",
                "real-to-sim",
                "AR/VR",
                "robotics",
                "per-scene optimization",
                "feed-forward foundation models",
                "SLAM-based pipelines",
                "3D foundation models",
                "pose estimation",
                "point prediction",
                "Gaussian decoder",
                "multi-scale features",
                "structured 3D Gaussians",
                "hierarchical Gaussian representation",
                "LoD-aware rendering strategy",
                "rendering fidelity",
                "redundancy",
                "interactive performance",
                "reconstruction quality"
            ],
            "githubStars": 18,
            "organization": {
                "_id": "6747ee5decec679eafb90450",
                "name": "ShanghaiAiLab",
                "fullname": "shanghai ailab "
            }
        },
        "publishedAt": "2025-10-09T13:57:38.000Z",
        "title": "ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D\n  Reconstruction with Structured Scene Representation",
        "summary": "On-the-fly 3D reconstruction from monocular image sequences is a\nlong-standing challenge in computer vision, critical for applications such as\nreal-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff:\nper-scene optimization yields high fidelity but is computationally expensive,\nwhereas feed-forward foundation models enable real-time inference but struggle\nwith accuracy and robustness. In this work, we propose ARTDECO, a unified\nframework that combines the efficiency of feed-forward models with the\nreliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose\nestimation and point prediction, coupled with a Gaussian decoder that\ntransforms multi-scale features into structured 3D Gaussians. To sustain both\nfidelity and efficiency at scale, we design a hierarchical Gaussian\nrepresentation with a LoD-aware rendering strategy, which improves rendering\nfidelity while reducing redundancy. Experiments on eight diverse indoor and\noutdoor benchmarks show that ARTDECO delivers interactive performance\ncomparable to SLAM, robustness similar to feed-forward systems, and\nreconstruction quality close to per-scene optimization, providing a practical\npath toward on-the-fly digitization of real-world environments with both\naccurate geometry and high visual fidelity. Explore more demos on our project\npage: https://city-super.github.io/artdeco/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/656e9e26bbec423d88b603e8/JeTCgAwYV1ncSGkXgmOFL.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08551.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656e9e26bbec423d88b603e8",
            "avatarUrl": "/avatars/10d8cb945a60e0401bfa4f74137cb203.svg",
            "fullname": "MulinYu",
            "name": "UML",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6747ee5decec679eafb90450",
            "name": "ShanghaiAiLab",
            "fullname": "shanghai ailab "
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08483",
            "authors": [
                {
                    "_id": "68e8624a95e8e6771df3886a",
                    "name": "Shangqing Tu",
                    "hidden": false
                },
                {
                    "_id": "68e8624a95e8e6771df3886b",
                    "name": "Yaxuan Li",
                    "hidden": false
                },
                {
                    "_id": "68e8624a95e8e6771df3886c",
                    "name": "Yushi Bai",
                    "hidden": false
                },
                {
                    "_id": "68e8624a95e8e6771df3886d",
                    "name": "Lei Hou",
                    "hidden": false
                },
                {
                    "_id": "68e8624a95e8e6771df3886e",
                    "name": "Juanzi Li",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/d1D0CmvxDAp-gD3SNBDs4.png",
                "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/1tuSZZNIzdebK1lbYkEaB.png",
                "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/K7eGzlhoch0C7GiUvTBg_.png"
            ],
            "publishedAt": "2025-10-09T17:24:54.000Z",
            "submittedOnDailyAt": "2025-10-10T00:34:23.206Z",
            "title": "DeepPrune: Parallel Scaling without Inter-trace Redundancy",
            "submittedOnDailyBy": {
                "_id": "648c48d8c0ddeee6df5b6d22",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648c48d8c0ddeee6df5b6d22/BlrYDv3eQxZ-Y5vtVGegX.jpeg",
                "isPro": false,
                "fullname": "Shangqing Tu",
                "user": "tsq2000",
                "type": "user"
            },
            "summary": "Parallel scaling has emerged as a powerful paradigm to enhance reasoning\ncapabilities in large language models (LLMs) by generating multiple\nChain-of-Thought (CoT) traces simultaneously. However, this approach introduces\nsignificant computational inefficiency due to inter-trace redundancy -- our\nanalysis reveals that over 80% of parallel reasoning traces yield identical\nfinal answers, representing substantial wasted computation. To address this\ncritical efficiency bottleneck, we propose DeepPrune, a novel framework that\nenables efficient parallel scaling through dynamic pruning. Our method features\na specialized judge model trained with focal loss and oversampling techniques\nto accurately predict answer equivalence from partial reasoning traces which\nrealizes 0.87 AUROC on equivalence prediction, combined with an online greedy\nclustering algorithm that dynamically prunes redundant paths while preserving\nanswer diversity. Comprehensive evaluations across three challenging benchmarks\n(AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that\nDeepPrune achieves remarkable token reduction by over 80% compared to\nconventional consensus sampling on most cases, while maintaining competitive\naccuracy within 3 percentage points. Our work establishes a new standard for\nefficient parallel reasoning, making high-performance reasoning more efficient.\nOur code and data are here: https://deepprune.github.io/",
            "upvotes": 21,
            "discussionId": "68e8624b95e8e6771df3886f",
            "projectPage": "https://deepprune.github.io/",
            "githubRepo": "https://github.com/THU-KEG/DeepPrune",
            "ai_summary": "DeepPrune, a novel framework using dynamic pruning and a specialized judge model, significantly reduces computational inefficiency in parallel scaling of large language models by pruning redundant reasoning traces.",
            "ai_keywords": [
                "Chain-of-Thought",
                "DeepPrune",
                "dynamic pruning",
                "judge model",
                "focal loss",
                "oversampling techniques",
                "AUROC",
                "online greedy clustering",
                "token reduction",
                "parallel reasoning",
                "large language models"
            ],
            "githubStars": 9,
            "organization": {
                "_id": "64db4fc57266618e854318f4",
                "name": "THU-KEG",
                "fullname": "Knowledge Engineer Group @ Tsinghua University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/648c4b46e549be47af1aafcd/5atqdE9AUWvYAHm9FNkG_.png"
            }
        },
        "publishedAt": "2025-10-09T13:24:54.000Z",
        "title": "DeepPrune: Parallel Scaling without Inter-trace Redundancy",
        "summary": "Parallel scaling has emerged as a powerful paradigm to enhance reasoning\ncapabilities in large language models (LLMs) by generating multiple\nChain-of-Thought (CoT) traces simultaneously. However, this approach introduces\nsignificant computational inefficiency due to inter-trace redundancy -- our\nanalysis reveals that over 80% of parallel reasoning traces yield identical\nfinal answers, representing substantial wasted computation. To address this\ncritical efficiency bottleneck, we propose DeepPrune, a novel framework that\nenables efficient parallel scaling through dynamic pruning. Our method features\na specialized judge model trained with focal loss and oversampling techniques\nto accurately predict answer equivalence from partial reasoning traces which\nrealizes 0.87 AUROC on equivalence prediction, combined with an online greedy\nclustering algorithm that dynamically prunes redundant paths while preserving\nanswer diversity. Comprehensive evaluations across three challenging benchmarks\n(AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that\nDeepPrune achieves remarkable token reduction by over 80% compared to\nconventional consensus sampling on most cases, while maintaining competitive\naccuracy within 3 percentage points. Our work establishes a new standard for\nefficient parallel reasoning, making high-performance reasoning more efficient.\nOur code and data are here: https://deepprune.github.io/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/d1D0CmvxDAp-gD3SNBDs4.png",
            "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/1tuSZZNIzdebK1lbYkEaB.png",
            "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/K7eGzlhoch0C7GiUvTBg_.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08483.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648c48d8c0ddeee6df5b6d22",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648c48d8c0ddeee6df5b6d22/BlrYDv3eQxZ-Y5vtVGegX.jpeg",
            "fullname": "Shangqing Tu",
            "name": "tsq2000",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "64db4fc57266618e854318f4",
            "name": "THU-KEG",
            "fullname": "Knowledge Engineer Group @ Tsinghua University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/648c4b46e549be47af1aafcd/5atqdE9AUWvYAHm9FNkG_.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08191",
            "authors": [
                {
                    "_id": "68e8725295e8e6771df389aa",
                    "user": {
                        "_id": "66e258bdc70c02b46dfed6e3",
                        "avatarUrl": "/avatars/ccc2d604616c018f45a268a610472cac.svg",
                        "isPro": false,
                        "fullname": "Yuzheng Cai",
                        "user": "Ucreate",
                        "type": "user"
                    },
                    "name": "Yuzheng Cai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T08:58:05.142Z",
                    "hidden": false
                },
                {
                    "_id": "68e8725295e8e6771df389ab",
                    "name": "Siqi Cai",
                    "hidden": false
                },
                {
                    "_id": "68e8725295e8e6771df389ac",
                    "name": "Yuchen Shi",
                    "hidden": false
                },
                {
                    "_id": "68e8725295e8e6771df389ad",
                    "name": "Zihan Xu",
                    "hidden": false
                },
                {
                    "_id": "68e8725295e8e6771df389ae",
                    "name": "Lichao Chen",
                    "hidden": false
                },
                {
                    "_id": "68e8725295e8e6771df389af",
                    "user": {
                        "_id": "6390525c00fb8ec4a424e0ff",
                        "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
                        "isPro": false,
                        "fullname": "Yulei Qin",
                        "user": "yolay",
                        "type": "user"
                    },
                    "name": "Yulei Qin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:09.755Z",
                    "hidden": false
                },
                {
                    "_id": "68e8725295e8e6771df389b0",
                    "name": "Xiaoyu Tan",
                    "hidden": false
                },
                {
                    "_id": "68e8725295e8e6771df389b1",
                    "name": "Gang Li",
                    "hidden": false
                },
                {
                    "_id": "68e8725295e8e6771df389b2",
                    "name": "Zongyi Li",
                    "hidden": false
                },
                {
                    "_id": "68e8725295e8e6771df389b3",
                    "name": "Haojia Lin",
                    "hidden": false
                },
                {
                    "_id": "68e8725295e8e6771df389b4",
                    "name": "Yong Mao",
                    "hidden": false
                },
                {
                    "_id": "68e8725295e8e6771df389b5",
                    "name": "Ke Li",
                    "hidden": false
                },
                {
                    "_id": "68e8725295e8e6771df389b6",
                    "name": "Xing Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T13:18:17.000Z",
            "submittedOnDailyAt": "2025-10-10T01:11:31.780Z",
            "title": "Training-Free Group Relative Policy Optimization",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent advances in Large Language Model (LLM) agents have demonstrated their\npromising general capabilities. However, their performance in specialized\nreal-world domains often degrades due to challenges in effectively integrating\nexternal tools and specific prompting strategies. While methods like agentic\nreinforcement learning have been proposed to address this, they typically rely\non costly parameter updates, for example, through a process that uses\nSupervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase\nwith Group Relative Policy Optimization (GRPO) to alter the output\ndistribution. However, we argue that LLMs can achieve a similar effect on the\noutput distribution by learning experiential knowledge as a token prior, which\nis a far more lightweight approach that not only addresses practical data\nscarcity but also avoids the common issue of overfitting. To this end, we\npropose Training-Free Group Relative Policy Optimization (Training-Free GRPO),\na cost-effective solution that enhances LLM agent performance without any\nparameter updates. Our method leverages the group relative semantic advantage\ninstead of numerical ones within each group of rollouts, iteratively distilling\nhigh-quality experiential knowledge during multi-epoch learning on a minimal\nground-truth data. Such knowledge serves as the learned token prior, which is\nseamlessly integrated during LLM API calls to guide model behavior. Experiments\non mathematical reasoning and web searching tasks demonstrate that\nTraining-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly\nimproves out-of-domain performance. With just a few dozen training samples,\nTraining-Free GRPO outperforms fine-tuned small LLMs with marginal training\ndata and cost.",
            "upvotes": 21,
            "discussionId": "68e8725395e8e6771df389b7",
            "githubRepo": "https://github.com/TencentCloudADP/youtu-agent/tree/training_free_GRPO",
            "ai_summary": "Training-Free GRPO enhances LLM agent performance in specialized domains by learning experiential knowledge as a token prior without parameter updates, improving out-of-domain tasks with minimal data.",
            "ai_keywords": [
                "Large Language Model (LLM)",
                "agentic reinforcement learning",
                "Supervised Fine-Tuning (SFT)",
                "Reinforcement Learning (RL)",
                "Group Relative Policy Optimization (GRPO)",
                "token prior",
                "Training-Free GRPO",
                "group relative semantic advantage",
                "multi-epoch learning",
                "minimal ground-truth data",
                "DeepSeek-V3.1-Terminus",
                "mathematical reasoning",
                "web searching tasks"
            ],
            "githubStars": 3282,
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-10-09T09:18:17.000Z",
        "title": "Training-Free Group Relative Policy Optimization",
        "summary": "Recent advances in Large Language Model (LLM) agents have demonstrated their\npromising general capabilities. However, their performance in specialized\nreal-world domains often degrades due to challenges in effectively integrating\nexternal tools and specific prompting strategies. While methods like agentic\nreinforcement learning have been proposed to address this, they typically rely\non costly parameter updates, for example, through a process that uses\nSupervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase\nwith Group Relative Policy Optimization (GRPO) to alter the output\ndistribution. However, we argue that LLMs can achieve a similar effect on the\noutput distribution by learning experiential knowledge as a token prior, which\nis a far more lightweight approach that not only addresses practical data\nscarcity but also avoids the common issue of overfitting. To this end, we\npropose Training-Free Group Relative Policy Optimization (Training-Free GRPO),\na cost-effective solution that enhances LLM agent performance without any\nparameter updates. Our method leverages the group relative semantic advantage\ninstead of numerical ones within each group of rollouts, iteratively distilling\nhigh-quality experiential knowledge during multi-epoch learning on a minimal\nground-truth data. Such knowledge serves as the learned token prior, which is\nseamlessly integrated during LLM API calls to guide model behavior. Experiments\non mathematical reasoning and web searching tasks demonstrate that\nTraining-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly\nimproves out-of-domain performance. With just a few dozen training samples,\nTraining-Free GRPO outperforms fine-tuned small LLMs with marginal training\ndata and cost.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08191.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 123
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08308",
            "authors": [
                {
                    "_id": "68e869b695e8e6771df388d2",
                    "name": "Liwei Kang",
                    "hidden": false
                },
                {
                    "_id": "68e869b695e8e6771df388d3",
                    "name": "Yue Deng",
                    "hidden": false
                },
                {
                    "_id": "68e869b695e8e6771df388d4",
                    "name": "Yao Xiao",
                    "hidden": false
                },
                {
                    "_id": "68e869b695e8e6771df388d5",
                    "name": "Zhanfeng Mo",
                    "hidden": false
                },
                {
                    "_id": "68e869b695e8e6771df388d6",
                    "name": "Wee Sun Lee",
                    "hidden": false
                },
                {
                    "_id": "68e869b695e8e6771df388d7",
                    "name": "Lidong Bing",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T14:57:10.000Z",
            "submittedOnDailyAt": "2025-10-10T00:42:42.161Z",
            "title": "First Try Matters: Revisiting the Role of Reflection in Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "64c274aa2b1ffc2eecc068ec",
                "avatarUrl": "/avatars/37d8dbe6e5e3431e72b2da18affcb639.svg",
                "isPro": false,
                "fullname": "Liwei Kang",
                "user": "olafyiii",
                "type": "user"
            },
            "summary": "Large language models have recently demonstrated significant gains in\nreasoning ability, often attributed to their capacity to generate longer chains\nof thought and engage in reflective reasoning. However, the contribution of\nreflections to performance improvement remains unclear. In this paper, we\nsystematically analyze the rollouts of eight reasoning models on five\nmathematical datasets. We focus on reflective behaviours where the model has\nalready produced an answer but continues reflecting before finalizing its\noutput. Our analysis reveals that reflections are predominantly confirmatory\nand rarely alter the model's initial answer, a pattern consistent across models\nand datasets. To understand the role of reflections in training, we construct\nsupervised fine-tuning (SFT) datasets with varying amounts of reflection steps.\nWe observe that training models on rollouts with more reflection steps\nprimarily enhances first-answer correctness rather than the ability to correct\ninitially wrong answers through reflections. This motivates us to propose a\nquestion-aware early-stopping method that enhances inference-time token\nefficiency by stopping the reasoning process once a few plausible candidate\nanswers are generated, thereby reducing unnecessary reflection steps. Motivated\nby this, we further propose to dynamically truncate the reflections after a\ncandidate answer has appeared during generation, which reduces reasoning tokens\nby 24.5% across five mathematical datasets, within a 2.9% drop in accuracy.",
            "upvotes": 20,
            "discussionId": "68e869b795e8e6771df388d8",
            "ai_summary": "Analysis of reflective behaviors in reasoning models shows that reflections primarily confirm initial answers, and training with more reflections improves first-answer correctness; a question-aware early-stopping method reduces unnecessary reflections and tokens with minimal accuracy loss.",
            "ai_keywords": [
                "reasoning models",
                "reflective behaviors",
                "confirmatory reflections",
                "supervised fine-tuning",
                "early-stopping method",
                "token efficiency",
                "reasoning tokens",
                "candidate answers"
            ]
        },
        "publishedAt": "2025-10-09T10:57:10.000Z",
        "title": "First Try Matters: Revisiting the Role of Reflection in Reasoning Models",
        "summary": "Large language models have recently demonstrated significant gains in\nreasoning ability, often attributed to their capacity to generate longer chains\nof thought and engage in reflective reasoning. However, the contribution of\nreflections to performance improvement remains unclear. In this paper, we\nsystematically analyze the rollouts of eight reasoning models on five\nmathematical datasets. We focus on reflective behaviours where the model has\nalready produced an answer but continues reflecting before finalizing its\noutput. Our analysis reveals that reflections are predominantly confirmatory\nand rarely alter the model's initial answer, a pattern consistent across models\nand datasets. To understand the role of reflections in training, we construct\nsupervised fine-tuning (SFT) datasets with varying amounts of reflection steps.\nWe observe that training models on rollouts with more reflection steps\nprimarily enhances first-answer correctness rather than the ability to correct\ninitially wrong answers through reflections. This motivates us to propose a\nquestion-aware early-stopping method that enhances inference-time token\nefficiency by stopping the reasoning process once a few plausible candidate\nanswers are generated, thereby reducing unnecessary reflection steps. Motivated\nby this, we further propose to dynamically truncate the reflections after a\ncandidate answer has appeared during generation, which reduces reasoning tokens\nby 24.5% across five mathematical datasets, within a 2.9% drop in accuracy.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08308.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c274aa2b1ffc2eecc068ec",
            "avatarUrl": "/avatars/37d8dbe6e5e3431e72b2da18affcb639.svg",
            "fullname": "Liwei Kang",
            "name": "olafyiii",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08211",
            "authors": [
                {
                    "_id": "68e8839f95e8e6771df38a70",
                    "user": {
                        "_id": "6372813520a58a5e14c596a3",
                        "avatarUrl": "/avatars/9135151259db3e5b9c8969e1d00c949d.svg",
                        "isPro": false,
                        "fullname": "XuHao Hu",
                        "user": "Foreshhh",
                        "type": "user"
                    },
                    "name": "XuHao Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:31:51.718Z",
                    "hidden": false
                },
                {
                    "_id": "68e8839f95e8e6771df38a71",
                    "name": "Peng Wang",
                    "hidden": false
                },
                {
                    "_id": "68e8839f95e8e6771df38a72",
                    "name": "Xiaoya Lu",
                    "hidden": false
                },
                {
                    "_id": "68e8839f95e8e6771df38a73",
                    "name": "Dongrui Liu",
                    "hidden": false
                },
                {
                    "_id": "68e8839f95e8e6771df38a74",
                    "name": "Xuanjing Huang",
                    "hidden": false
                },
                {
                    "_id": "68e8839f95e8e6771df38a75",
                    "name": "Jing Shao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T13:35:19.000Z",
            "submittedOnDailyAt": "2025-10-10T02:29:17.164Z",
            "title": "LLMs Learn to Deceive Unintentionally: Emergent Misalignment in\n  Dishonesty from Misaligned Samples to Biased Human-AI Interactions",
            "submittedOnDailyBy": {
                "_id": "6372813520a58a5e14c596a3",
                "avatarUrl": "/avatars/9135151259db3e5b9c8969e1d00c949d.svg",
                "isPro": false,
                "fullname": "XuHao Hu",
                "user": "Foreshhh",
                "type": "user"
            },
            "summary": "Previous research has shown that LLMs finetuned on malicious or incorrect\ncompletions within narrow domains (e.g., insecure code or incorrect medical\nadvice) can become broadly misaligned to exhibit harmful behaviors, which is\ncalled emergent misalignment. In this work, we investigate whether this\nphenomenon can extend beyond safety behaviors to a broader spectrum of\ndishonesty and deception under high-stakes scenarios (e.g., lying under\npressure and deceptive behavior). To explore this, we finetune open-sourced\nLLMs on misaligned completions across diverse domains. Experimental results\ndemonstrate that LLMs show broadly misaligned behavior in dishonesty.\nAdditionally, we further explore this phenomenon in a downstream combined\nfinetuning setting, and find that introducing as little as 1% of misalignment\ndata into a standard downstream task is sufficient to decrease honest behavior\nover 20%. Furthermore, we consider a more practical human-AI interaction\nenvironment where we simulate both benign and biased users to interact with the\nassistant LLM. Notably, we find that the assistant can be misaligned\nunintentionally to exacerbate its dishonesty with only 10% biased user\npopulation. In summary, we extend the study of emergent misalignment to the\ndomain of dishonesty and deception under high-stakes scenarios, and demonstrate\nthat this risk arises not only through direct finetuning, but also in\ndownstream mixture tasks and practical human-AI interactions.",
            "upvotes": 18,
            "discussionId": "68e8839f95e8e6771df38a76",
            "ai_summary": "LLMs finetuned on misaligned data exhibit dishonest behavior, which can be exacerbated in downstream tasks and human-AI interactions.",
            "ai_keywords": [
                "LLMs",
                "finetuning",
                "emergent misalignment",
                "dishonesty",
                "deception",
                "high-stakes scenarios",
                "downstream tasks",
                "human-AI interaction",
                "biased users"
            ],
            "organization": {
                "_id": "643cb0625fcffe09fb6ca688",
                "name": "Fudan-University",
                "fullname": "Fudan University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"
            }
        },
        "publishedAt": "2025-10-09T09:35:19.000Z",
        "title": "LLMs Learn to Deceive Unintentionally: Emergent Misalignment in\n  Dishonesty from Misaligned Samples to Biased Human-AI Interactions",
        "summary": "Previous research has shown that LLMs finetuned on malicious or incorrect\ncompletions within narrow domains (e.g., insecure code or incorrect medical\nadvice) can become broadly misaligned to exhibit harmful behaviors, which is\ncalled emergent misalignment. In this work, we investigate whether this\nphenomenon can extend beyond safety behaviors to a broader spectrum of\ndishonesty and deception under high-stakes scenarios (e.g., lying under\npressure and deceptive behavior). To explore this, we finetune open-sourced\nLLMs on misaligned completions across diverse domains. Experimental results\ndemonstrate that LLMs show broadly misaligned behavior in dishonesty.\nAdditionally, we further explore this phenomenon in a downstream combined\nfinetuning setting, and find that introducing as little as 1% of misalignment\ndata into a standard downstream task is sufficient to decrease honest behavior\nover 20%. Furthermore, we consider a more practical human-AI interaction\nenvironment where we simulate both benign and biased users to interact with the\nassistant LLM. Notably, we find that the assistant can be misaligned\nunintentionally to exacerbate its dishonesty with only 10% biased user\npopulation. In summary, we extend the study of emergent misalignment to the\ndomain of dishonesty and deception under high-stakes scenarios, and demonstrate\nthat this risk arises not only through direct finetuning, but also in\ndownstream mixture tasks and practical human-AI interactions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08211.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6372813520a58a5e14c596a3",
            "avatarUrl": "/avatars/9135151259db3e5b9c8969e1d00c949d.svg",
            "fullname": "XuHao Hu",
            "name": "Foreshhh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "643cb0625fcffe09fb6ca688",
            "name": "Fudan-University",
            "fullname": "Fudan University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08143",
            "authors": [
                {
                    "_id": "68e872cf95e8e6771df389dc",
                    "user": {
                        "_id": "63fac5126b75d93aa136099c",
                        "avatarUrl": "/avatars/c1d7c6f29e67a6b2b97a7cd1cccedfb1.svg",
                        "isPro": false,
                        "fullname": "Shian Du",
                        "user": "YOKIMIYA",
                        "type": "user"
                    },
                    "name": "Shian Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:04.889Z",
                    "hidden": false
                },
                {
                    "_id": "68e872cf95e8e6771df389dd",
                    "name": "Menghan Xia",
                    "hidden": false
                },
                {
                    "_id": "68e872cf95e8e6771df389de",
                    "name": "Chang Liu",
                    "hidden": false
                },
                {
                    "_id": "68e872cf95e8e6771df389df",
                    "name": "Quande Liu",
                    "hidden": false
                },
                {
                    "_id": "68e872cf95e8e6771df389e0",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "68e872cf95e8e6771df389e1",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "68e872cf95e8e6771df389e2",
                    "name": "Xiangyang Ji",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T12:25:16.000Z",
            "submittedOnDailyAt": "2025-10-10T01:21:03.652Z",
            "title": "UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video\n  Super-Resolution",
            "submittedOnDailyBy": {
                "_id": "63fac5126b75d93aa136099c",
                "avatarUrl": "/avatars/c1d7c6f29e67a6b2b97a7cd1cccedfb1.svg",
                "isPro": false,
                "fullname": "Shian Du",
                "user": "YOKIMIYA",
                "type": "user"
            },
            "summary": "Cascaded video super-resolution has emerged as a promising technique for\ndecoupling the computational burden associated with generating high-resolution\nvideos using large foundation models. Existing studies, however, are largely\nconfined to text-to-video tasks and fail to leverage additional generative\nconditions beyond text, which are crucial for ensuring fidelity in multi-modal\nvideo generation. We address this limitation by presenting UniMMVSR, the first\nunified generative video super-resolution framework to incorporate hybrid-modal\nconditions, including text, images, and videos. We conduct a comprehensive\nexploration of condition injection strategies, training schemes, and data\nmixture techniques within a latent video diffusion model. A key challenge was\ndesigning distinct data construction and condition utilization methods to\nenable the model to precisely utilize all condition types, given their varied\ncorrelations with the target video. Our experiments demonstrate that UniMMVSR\nsignificantly outperforms existing methods, producing videos with superior\ndetail and a higher degree of conformity to multi-modal conditions. We also\nvalidate the feasibility of combining UniMMVSR with a base model to achieve\nmulti-modal guided generation of 4K video, a feat previously unattainable with\nexisting techniques.",
            "upvotes": 18,
            "discussionId": "68e872cf95e8e6771df389e3",
            "projectPage": "https://shiandu.github.io/UniMMVSR-website/",
            "ai_summary": "UniMMVSR is a unified generative video super-resolution framework that incorporates hybrid-modal conditions, including text, images, and videos, within a latent video diffusion model, achieving superior detail and conformity to multi-modal conditions.",
            "ai_keywords": [
                "cascaded video super-resolution",
                "foundation models",
                "text-to-video tasks",
                "generative conditions",
                "multi-modal video generation",
                "unified generative video super-resolution",
                "hybrid-modal conditions",
                "latent video diffusion model",
                "condition injection strategies",
                "training schemes",
                "data mixture techniques",
                "multi-modal guided generation"
            ],
            "organization": {
                "_id": "662c559b322afcbae51b3c8b",
                "name": "KwaiVGI",
                "fullname": "Kuaishou Visual Generation and Interaction Center",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
            }
        },
        "publishedAt": "2025-10-09T08:25:16.000Z",
        "title": "UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video\n  Super-Resolution",
        "summary": "Cascaded video super-resolution has emerged as a promising technique for\ndecoupling the computational burden associated with generating high-resolution\nvideos using large foundation models. Existing studies, however, are largely\nconfined to text-to-video tasks and fail to leverage additional generative\nconditions beyond text, which are crucial for ensuring fidelity in multi-modal\nvideo generation. We address this limitation by presenting UniMMVSR, the first\nunified generative video super-resolution framework to incorporate hybrid-modal\nconditions, including text, images, and videos. We conduct a comprehensive\nexploration of condition injection strategies, training schemes, and data\nmixture techniques within a latent video diffusion model. A key challenge was\ndesigning distinct data construction and condition utilization methods to\nenable the model to precisely utilize all condition types, given their varied\ncorrelations with the target video. Our experiments demonstrate that UniMMVSR\nsignificantly outperforms existing methods, producing videos with superior\ndetail and a higher degree of conformity to multi-modal conditions. We also\nvalidate the feasibility of combining UniMMVSR with a base model to achieve\nmulti-modal guided generation of 4K video, a feat previously unattainable with\nexisting techniques.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08143.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63fac5126b75d93aa136099c",
            "avatarUrl": "/avatars/c1d7c6f29e67a6b2b97a7cd1cccedfb1.svg",
            "fullname": "Shian Du",
            "name": "YOKIMIYA",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "662c559b322afcbae51b3c8b",
            "name": "KwaiVGI",
            "fullname": "Kuaishou Visual Generation and Interaction Center",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08565",
            "authors": [
                {
                    "_id": "68e866db95e8e6771df38877",
                    "user": {
                        "_id": "64b7475efa7eabaae5f7ba94",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7475efa7eabaae5f7ba94/YLv44PZM6tw1sxACY1-U_.png",
                        "isPro": true,
                        "fullname": "Changyao Tian",
                        "user": "Changyao",
                        "type": "user"
                    },
                    "name": "Changyao Tian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:33:07.819Z",
                    "hidden": false
                },
                {
                    "_id": "68e866db95e8e6771df38878",
                    "name": "Hao Li",
                    "hidden": false
                },
                {
                    "_id": "68e866db95e8e6771df38879",
                    "name": "Gen Luo",
                    "hidden": false
                },
                {
                    "_id": "68e866db95e8e6771df3887a",
                    "name": "Xizhou Zhu",
                    "hidden": false
                },
                {
                    "_id": "68e866db95e8e6771df3887b",
                    "name": "Weijie Su",
                    "hidden": false
                },
                {
                    "_id": "68e866db95e8e6771df3887c",
                    "name": "Hanming Deng",
                    "hidden": false
                },
                {
                    "_id": "68e866db95e8e6771df3887d",
                    "name": "Jinguo Zhu",
                    "hidden": false
                },
                {
                    "_id": "68e866db95e8e6771df3887e",
                    "name": "Jie Shao",
                    "hidden": false
                },
                {
                    "_id": "68e866db95e8e6771df3887f",
                    "name": "Ziran Zhu",
                    "hidden": false
                },
                {
                    "_id": "68e866db95e8e6771df38880",
                    "name": "Yunpeng Liu",
                    "hidden": false
                },
                {
                    "_id": "68e866db95e8e6771df38881",
                    "name": "Lewei Lu",
                    "hidden": false
                },
                {
                    "_id": "68e866db95e8e6771df38882",
                    "name": "Wenhai Wang",
                    "hidden": false
                },
                {
                    "_id": "68e866db95e8e6771df38883",
                    "name": "Hongsheng Li",
                    "hidden": false
                },
                {
                    "_id": "68e866db95e8e6771df38884",
                    "name": "Jifeng Dai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T17:59:37.000Z",
            "submittedOnDailyAt": "2025-10-10T00:25:01.334Z",
            "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language\n  Models under Data Constraints",
            "submittedOnDailyBy": {
                "_id": "64b7475efa7eabaae5f7ba94",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7475efa7eabaae5f7ba94/YLv44PZM6tw1sxACY1-U_.png",
                "isPro": true,
                "fullname": "Changyao Tian",
                "user": "Changyao",
                "type": "user"
            },
            "summary": "Compositional training has been the de-facto paradigm in existing Multimodal\nLarge Language Models (MLLMs), where pre-trained vision encoders are connected\nwith pre-trained LLMs through continuous multimodal pre-training. However, the\nmultimodal scaling property of this paradigm remains difficult to explore due\nto the separated training. In this paper, we focus on the native training of\nMLLMs in an end-to-end manner and systematically study its design space and\nscaling property under a practical setting, i.e., data constraint. Through\ncareful study of various choices in MLLM, we obtain the optimal\nmeta-architecture that best balances performance and training cost. After that,\nwe further explore the scaling properties of the native MLLM and indicate the\npositively correlated scaling relationship between visual encoders and LLMs.\nBased on these findings, we propose a native MLLM called NaViL, combined with a\nsimple and cost-effective recipe. Experimental results on 14 multimodal\nbenchmarks confirm the competitive performance of NaViL against existing MLLMs.\nBesides that, our findings and results provide in-depth insights for the future\nstudy of native MLLMs.",
            "upvotes": 17,
            "discussionId": "68e866db95e8e6771df38885",
            "projectPage": "https://internvl.github.io/blog/2025-10-10-NaViL/",
            "githubRepo": "https://github.com/OpenGVLab/NaViL",
            "ai_summary": "Native end-to-end training of Multimodal Large Language Models (MLLMs) achieves competitive performance with a balanced design and scaling relationship between visual encoders and LLMs.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "compositional training",
                "continuous multimodal pre-training",
                "end-to-end training",
                "meta-architecture",
                "scaling property",
                "visual encoders",
                "LLMs",
                "NaViL"
            ],
            "githubStars": 22,
            "organization": {
                "_id": "64006c57a3b8fe3ac0e9af7c",
                "name": "OpenGVLab",
                "fullname": "OpenGVLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64006c09330a45b03605bba3/FvdxiTkTqH8rKDOzGZGUE.jpeg"
            }
        },
        "publishedAt": "2025-10-09T13:59:37.000Z",
        "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language\n  Models under Data Constraints",
        "summary": "Compositional training has been the de-facto paradigm in existing Multimodal\nLarge Language Models (MLLMs), where pre-trained vision encoders are connected\nwith pre-trained LLMs through continuous multimodal pre-training. However, the\nmultimodal scaling property of this paradigm remains difficult to explore due\nto the separated training. In this paper, we focus on the native training of\nMLLMs in an end-to-end manner and systematically study its design space and\nscaling property under a practical setting, i.e., data constraint. Through\ncareful study of various choices in MLLM, we obtain the optimal\nmeta-architecture that best balances performance and training cost. After that,\nwe further explore the scaling properties of the native MLLM and indicate the\npositively correlated scaling relationship between visual encoders and LLMs.\nBased on these findings, we propose a native MLLM called NaViL, combined with a\nsimple and cost-effective recipe. Experimental results on 14 multimodal\nbenchmarks confirm the competitive performance of NaViL against existing MLLMs.\nBesides that, our findings and results provide in-depth insights for the future\nstudy of native MLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08565.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b7475efa7eabaae5f7ba94",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7475efa7eabaae5f7ba94/YLv44PZM6tw1sxACY1-U_.png",
            "fullname": "Changyao Tian",
            "name": "Changyao",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "64006c57a3b8fe3ac0e9af7c",
            "name": "OpenGVLab",
            "fullname": "OpenGVLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64006c09330a45b03605bba3/FvdxiTkTqH8rKDOzGZGUE.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07546",
            "authors": [
                {
                    "_id": "68e91f0f95e8e6771df38cd4",
                    "name": "Soroush Mehraban",
                    "hidden": false
                },
                {
                    "_id": "68e91f0f95e8e6771df38cd5",
                    "name": "Vida Adeli",
                    "hidden": false
                },
                {
                    "_id": "68e91f0f95e8e6771df38cd6",
                    "name": "Jacob Rommann",
                    "hidden": false
                },
                {
                    "_id": "68e91f0f95e8e6771df38cd7",
                    "name": "Babak Taati",
                    "hidden": false
                },
                {
                    "_id": "68e91f0f95e8e6771df38cd8",
                    "name": "Kyryl Truskovskyi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6375965008eebfdd0a399891/nk-CGqQJ5dTU8tCvQSLi9.mp4"
            ],
            "publishedAt": "2025-10-08T21:02:55.000Z",
            "submittedOnDailyAt": "2025-10-10T13:44:17.423Z",
            "title": "PickStyle: Video-to-Video Style Transfer with Context-Style Adapters",
            "submittedOnDailyBy": {
                "_id": "6375965008eebfdd0a399891",
                "avatarUrl": "/avatars/946768f40a18793ced82f09a1de47952.svg",
                "isPro": false,
                "fullname": "Soroush Mehraban",
                "user": "SoroushMehraban",
                "type": "user"
            },
            "summary": "We address the task of video style transfer with diffusion models, where the\ngoal is to preserve the context of an input video while rendering it in a\ntarget style specified by a text prompt. A major challenge is the lack of\npaired video data for supervision. We propose PickStyle, a video-to-video style\ntransfer framework that augments pretrained video diffusion backbones with\nstyle adapters and benefits from paired still image data with source-style\ncorrespondences for training. PickStyle inserts low-rank adapters into the\nself-attention layers of conditioning modules, enabling efficient\nspecialization for motion-style transfer while maintaining strong alignment\nbetween video content and style. To bridge the gap between static image\nsupervision and dynamic video, we construct synthetic training clips from\npaired images by applying shared augmentations that simulate camera motion,\nensuring temporal priors are preserved. In addition, we introduce Context-Style\nClassifier-Free Guidance (CS-CFG), a novel factorization of classifier-free\nguidance into independent text (style) and video (context) directions. CS-CFG\nensures that context is preserved in generated video while the style is\neffectively transferred. Experiments across benchmarks show that our approach\nachieves temporally coherent, style-faithful, and content-preserving video\ntranslations, outperforming existing baselines both qualitatively and\nquantitatively.",
            "upvotes": 15,
            "discussionId": "68e91f0f95e8e6771df38cd9",
            "projectPage": "https://pickstyle.pickford.ai/",
            "ai_summary": "PickStyle uses diffusion models with style adapters and synthetic video clips to perform video style transfer from text prompts, preserving context and style.",
            "ai_keywords": [
                "diffusion models",
                "video style transfer",
                "style adapters",
                "self-attention layers",
                "conditioning modules",
                "synthetic training clips",
                "camera motion",
                "Context-Style Classifier-Free Guidance",
                "CS-CFG",
                "temporally coherent",
                "style-faithful",
                "content-preserving"
            ],
            "organization": {
                "_id": "67a5281da26e98ac622de2b4",
                "name": "Pickford",
                "fullname": "Pickford",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67a527a1c12b766246a29c63/1sJ-w4TlSPNvhrjxpmxlq.png"
            }
        },
        "publishedAt": "2025-10-08T17:02:55.000Z",
        "title": "PickStyle: Video-to-Video Style Transfer with Context-Style Adapters",
        "summary": "We address the task of video style transfer with diffusion models, where the\ngoal is to preserve the context of an input video while rendering it in a\ntarget style specified by a text prompt. A major challenge is the lack of\npaired video data for supervision. We propose PickStyle, a video-to-video style\ntransfer framework that augments pretrained video diffusion backbones with\nstyle adapters and benefits from paired still image data with source-style\ncorrespondences for training. PickStyle inserts low-rank adapters into the\nself-attention layers of conditioning modules, enabling efficient\nspecialization for motion-style transfer while maintaining strong alignment\nbetween video content and style. To bridge the gap between static image\nsupervision and dynamic video, we construct synthetic training clips from\npaired images by applying shared augmentations that simulate camera motion,\nensuring temporal priors are preserved. In addition, we introduce Context-Style\nClassifier-Free Guidance (CS-CFG), a novel factorization of classifier-free\nguidance into independent text (style) and video (context) directions. CS-CFG\nensures that context is preserved in generated video while the style is\neffectively transferred. Experiments across benchmarks show that our approach\nachieves temporally coherent, style-faithful, and content-preserving video\ntranslations, outperforming existing baselines both qualitatively and\nquantitatively.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6375965008eebfdd0a399891/nk-CGqQJ5dTU8tCvQSLi9.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07546.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6375965008eebfdd0a399891",
            "avatarUrl": "/avatars/946768f40a18793ced82f09a1de47952.svg",
            "fullname": "Soroush Mehraban",
            "name": "SoroushMehraban",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "67a5281da26e98ac622de2b4",
            "name": "Pickford",
            "fullname": "Pickford",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67a527a1c12b766246a29c63/1sJ-w4TlSPNvhrjxpmxlq.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08529",
            "authors": [
                {
                    "_id": "68e891e295e8e6771df38b41",
                    "user": {
                        "_id": "65082baabc8788c4064d5360",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/NZpFVnTpPGcCe8mvFMD-L.jpeg",
                        "isPro": false,
                        "fullname": "Xiangyuan Xue",
                        "user": "xxyQwQ",
                        "type": "user"
                    },
                    "name": "Xiangyuan Xue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T08:54:27.924Z",
                    "hidden": false
                },
                {
                    "_id": "68e891e295e8e6771df38b42",
                    "name": "Yifan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68e891e295e8e6771df38b43",
                    "name": "Guibin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e891e295e8e6771df38b44",
                    "name": "Zaibin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e891e295e8e6771df38b45",
                    "name": "Yijiang Li",
                    "hidden": false
                },
                {
                    "_id": "68e891e295e8e6771df38b46",
                    "name": "Chen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e891e295e8e6771df38b47",
                    "name": "Zhenfei Yin",
                    "hidden": false
                },
                {
                    "_id": "68e891e295e8e6771df38b48",
                    "name": "Philip Torr",
                    "hidden": false
                },
                {
                    "_id": "68e891e295e8e6771df38b49",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68e891e295e8e6771df38b4a",
                    "name": "Lei Bai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T17:50:26.000Z",
            "submittedOnDailyAt": "2025-10-10T04:03:38.439Z",
            "title": "CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards",
            "submittedOnDailyBy": {
                "_id": "65082baabc8788c4064d5360",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/NZpFVnTpPGcCe8mvFMD-L.jpeg",
                "isPro": false,
                "fullname": "Xiangyuan Xue",
                "user": "xxyQwQ",
                "type": "user"
            },
            "summary": "Self-evolution is a central research topic in enabling large language model\n(LLM)-based agents to continually improve their capabilities after pretraining.\nRecent research has witnessed a transition from reinforcement learning\n(RL)-free to RL-based methods. Current RL-based methods either rely on dense\nexternal reward signals or extract intrinsic reward signals from LLMs\nthemselves. However, these approaches diverge from the self-evolution\nmechanisms observed in human intelligence, where individuals learn and improve\nthrough mutual discussion and collaboration. In this work, we introduce\nCo-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents\nto improve autonomously by learning from inter-agent interactions without\nexternal supervision. CoMAS generates intrinsic rewards from rich discussion\ndynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and\noptimizes each agent's policy through RL, thereby enabling decentralized and\nscalable co-evolution. Experimental results demonstrate that CoMAS consistently\noutperforms untrained agents and achieves state-of-the-art performance across\nmost evaluation settings. Ablation studies confirm the necessity of\ninteraction-based reward signals and reveal promising scalability as the number\nand diversity of agents increase. These findings establish CoMAS as a novel and\neffective paradigm for self-evolution in LLM-based agents.",
            "upvotes": 14,
            "discussionId": "68e891e395e8e6771df38b4b",
            "githubRepo": "https://github.com/xxyQwQ/CoMAS",
            "ai_summary": "Co-Evolving Multi-Agent Systems (CoMAS) enable LLM-based agents to improve autonomously through inter-agent interactions and intrinsic rewards, achieving state-of-the-art performance.",
            "ai_keywords": [
                "self-evolution",
                "large language model (LLM)",
                "reinforcement learning (RL)",
                "intrinsic reward signals",
                "Co-Evolving Multi-Agent Systems (CoMAS)",
                "LLM-as-a-judge",
                "decentralized",
                "scalable co-evolution"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-10-09T13:50:26.000Z",
        "title": "CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards",
        "summary": "Self-evolution is a central research topic in enabling large language model\n(LLM)-based agents to continually improve their capabilities after pretraining.\nRecent research has witnessed a transition from reinforcement learning\n(RL)-free to RL-based methods. Current RL-based methods either rely on dense\nexternal reward signals or extract intrinsic reward signals from LLMs\nthemselves. However, these approaches diverge from the self-evolution\nmechanisms observed in human intelligence, where individuals learn and improve\nthrough mutual discussion and collaboration. In this work, we introduce\nCo-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents\nto improve autonomously by learning from inter-agent interactions without\nexternal supervision. CoMAS generates intrinsic rewards from rich discussion\ndynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and\noptimizes each agent's policy through RL, thereby enabling decentralized and\nscalable co-evolution. Experimental results demonstrate that CoMAS consistently\noutperforms untrained agents and achieves state-of-the-art performance across\nmost evaluation settings. Ablation studies confirm the necessity of\ninteraction-based reward signals and reveal promising scalability as the number\nand diversity of agents increase. These findings establish CoMAS as a novel and\neffective paradigm for self-evolution in LLM-based agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08529.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65082baabc8788c4064d5360",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/NZpFVnTpPGcCe8mvFMD-L.jpeg",
            "fullname": "Xiangyuan Xue",
            "name": "xxyQwQ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.03222",
            "authors": [
                {
                    "_id": "68e8707795e8e6771df3896f",
                    "user": {
                        "_id": "68e6918606b3fcd04535edc3",
                        "avatarUrl": "/avatars/79a1228181983d4587238e38a47e8cf8.svg",
                        "isPro": false,
                        "fullname": "Guanhua Huang",
                        "user": "Carlanlarkk",
                        "type": "user"
                    },
                    "name": "Guanhua Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:18.491Z",
                    "hidden": false
                },
                {
                    "_id": "68e8707795e8e6771df38970",
                    "name": "Tingqiang Xu",
                    "hidden": false
                },
                {
                    "_id": "68e8707795e8e6771df38971",
                    "name": "Mingze Wang",
                    "hidden": false
                },
                {
                    "_id": "68e8707795e8e6771df38972",
                    "name": "Qi Yi",
                    "hidden": false
                },
                {
                    "_id": "68e8707795e8e6771df38973",
                    "name": "Xue Gong",
                    "hidden": false
                },
                {
                    "_id": "68e8707795e8e6771df38974",
                    "name": "Siheng Li",
                    "hidden": false
                },
                {
                    "_id": "68e8707795e8e6771df38975",
                    "name": "Ruibin Xiong",
                    "hidden": false
                },
                {
                    "_id": "68e8707795e8e6771df38976",
                    "name": "Kejiao Li",
                    "hidden": false
                },
                {
                    "_id": "68e8707795e8e6771df38977",
                    "name": "Yuhao Jiang",
                    "hidden": false
                },
                {
                    "_id": "68e8707795e8e6771df38978",
                    "name": "Bo Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-03T17:56:13.000Z",
            "submittedOnDailyAt": "2025-10-10T01:05:31.177Z",
            "title": "Low-probability Tokens Sustain Exploration in Reinforcement Learning\n  with Verifiable Reward",
            "submittedOnDailyBy": {
                "_id": "68e6918606b3fcd04535edc3",
                "avatarUrl": "/avatars/79a1228181983d4587238e38a47e8cf8.svg",
                "isPro": false,
                "fullname": "Guanhua Huang",
                "user": "Carlanlarkk",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large\nLanguage Models in complex reasoning, yet its scalability is often hindered by\na training bottleneck where performance plateaus as policy entropy collapses,\nsignaling a loss of exploration. Previous methods typically address this by\nmaintaining high policy entropy, yet the precise mechanisms that govern\nmeaningful exploration have remained underexplored. Our analysis suggests that\nan unselective focus on entropy risks amplifying irrelevant tokens and\ndestabilizing training. This paper investigates the exploration dynamics within\nRLVR and identifies a key issue: the gradual elimination of valuable\nlow-probability exploratory tokens, which we term \\textit{reasoning\nsparks}. We find that while abundant in pre-trained models, these sparks are\nsystematically extinguished during RLVR due to over-penalization, leading to a\ndegeneracy in exploration. To address this, we introduce Low-probability\nRegularization (Lp-Reg). Its core mechanism regularizes the policy towards a\nheuristic proxy distribution. This proxy is constructed by filtering out\npresumed noise tokens and re-normalizing the distribution over the remaining\ncandidates. The result is a less-noisy proxy where the probability of\nreasoning sparks is amplified, which then serves as a soft\nregularization target to shield these valuable tokens from elimination via KL\ndivergence. Experiments show that Lp-Reg enables stable on-policy training for\naround 1,000 steps, a regime where baseline entropy-control methods collapse.\nThis sustained exploration leads to state-of-the-art performance, achieving a\n60.17% average accuracy on five math benchmarks, an improvement of 2.66%\nover prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.",
            "upvotes": 14,
            "discussionId": "68e8707795e8e6771df38979",
            "githubRepo": "https://github.com/CarlanLark/Lp-Reg-dev",
            "ai_summary": "Low-probability Regularization (Lp-Reg) enhances exploration in Reinforcement Learning with Verifiable Rewards (RLVR) by preserving valuable low-probability tokens, leading to improved performance in complex reasoning tasks.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards",
                "RLVR",
                "policy entropy",
                "exploration dynamics",
                "reasoning sparks",
                "Low-probability Regularization",
                "Lp-Reg",
                "heuristic proxy distribution",
                "KL divergence",
                "on-policy training",
                "math benchmarks"
            ],
            "githubStars": 4,
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-10-03T13:56:13.000Z",
        "title": "Low-probability Tokens Sustain Exploration in Reinforcement Learning\n  with Verifiable Reward",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large\nLanguage Models in complex reasoning, yet its scalability is often hindered by\na training bottleneck where performance plateaus as policy entropy collapses,\nsignaling a loss of exploration. Previous methods typically address this by\nmaintaining high policy entropy, yet the precise mechanisms that govern\nmeaningful exploration have remained underexplored. Our analysis suggests that\nan unselective focus on entropy risks amplifying irrelevant tokens and\ndestabilizing training. This paper investigates the exploration dynamics within\nRLVR and identifies a key issue: the gradual elimination of valuable\nlow-probability exploratory tokens, which we term \\textit{reasoning\nsparks}. We find that while abundant in pre-trained models, these sparks are\nsystematically extinguished during RLVR due to over-penalization, leading to a\ndegeneracy in exploration. To address this, we introduce Low-probability\nRegularization (Lp-Reg). Its core mechanism regularizes the policy towards a\nheuristic proxy distribution. This proxy is constructed by filtering out\npresumed noise tokens and re-normalizing the distribution over the remaining\ncandidates. The result is a less-noisy proxy where the probability of\nreasoning sparks is amplified, which then serves as a soft\nregularization target to shield these valuable tokens from elimination via KL\ndivergence. Experiments show that Lp-Reg enables stable on-policy training for\naround 1,000 steps, a regime where baseline entropy-control methods collapse.\nThis sustained exploration leads to state-of-the-art performance, achieving a\n60.17% average accuracy on five math benchmarks, an improvement of 2.66%\nover prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03222.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68e6918606b3fcd04535edc3",
            "avatarUrl": "/avatars/79a1228181983d4587238e38a47e8cf8.svg",
            "fullname": "Guanhua Huang",
            "name": "Carlanlarkk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08485",
            "authors": [
                {
                    "_id": "68e88ae595e8e6771df38aea",
                    "name": "Chong Mou",
                    "hidden": false
                },
                {
                    "_id": "68e88ae595e8e6771df38aeb",
                    "name": "Qichao Sun",
                    "hidden": false
                },
                {
                    "_id": "68e88ae595e8e6771df38aec",
                    "name": "Yanze Wu",
                    "hidden": false
                },
                {
                    "_id": "68e88ae595e8e6771df38aed",
                    "name": "Pengze Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e88ae595e8e6771df38aee",
                    "name": "Xinghui Li",
                    "hidden": false
                },
                {
                    "_id": "68e88ae595e8e6771df38aef",
                    "name": "Fulong Ye",
                    "hidden": false
                },
                {
                    "_id": "68e88ae595e8e6771df38af0",
                    "name": "Songtao Zhao",
                    "hidden": false
                },
                {
                    "_id": "68e88ae595e8e6771df38af1",
                    "name": "Qian He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T17:26:09.000Z",
            "submittedOnDailyAt": "2025-10-10T03:02:24.328Z",
            "title": "InstructX: Towards Unified Visual Editing with MLLM Guidance",
            "submittedOnDailyBy": {
                "_id": "63ec78f4c81b6a52391e1291",
                "avatarUrl": "/avatars/d13b35964c895f7aec69287390036a79.svg",
                "isPro": false,
                "fullname": "ChongMou",
                "user": "Adapter",
                "type": "user"
            },
            "summary": "With recent advances in Multimodal Large Language Models (MLLMs) showing\nstrong visual understanding and reasoning, interest is growing in using them to\nimprove the editing performance of diffusion models. Despite rapid progress,\nmost studies lack an in-depth analysis of MLLM design choices. Moreover, the\nintegration of MLLMs and diffusion models remains an open challenge in some\ndifficult tasks, such as video editing. In this paper, we present InstructX, a\nunified framework for image and video editing. Specifically, we conduct a\ncomprehensive study on integrating MLLMs and diffusion models for\ninstruction-driven editing across diverse tasks. Building on this study, we\nanalyze the cooperation and distinction between images and videos in unified\nmodeling. (1) We show that training on image data can lead to emergent video\nediting capabilities without explicit supervision, thereby alleviating the\nconstraints imposed by scarce video training data. (2) By incorporating\nmodality-specific MLLM features, our approach effectively unifies image and\nvideo editing tasks within a single model. Extensive experiments demonstrate\nthat our method can handle a broad range of image and video editing tasks and\nachieves state-of-the-art performance.",
            "upvotes": 13,
            "discussionId": "68e88ae595e8e6771df38af2",
            "projectPage": "https://mc-e.github.io/project/InstructX/",
            "ai_summary": "InstructX integrates Multimodal Large Language Models and diffusion models for instruction-driven image and video editing, achieving state-of-the-art performance across diverse tasks.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "diffusion models",
                "instruction-driven editing",
                "video editing",
                "image editing",
                "unified modeling",
                "modality-specific features"
            ]
        },
        "publishedAt": "2025-10-09T13:26:09.000Z",
        "title": "InstructX: Towards Unified Visual Editing with MLLM Guidance",
        "summary": "With recent advances in Multimodal Large Language Models (MLLMs) showing\nstrong visual understanding and reasoning, interest is growing in using them to\nimprove the editing performance of diffusion models. Despite rapid progress,\nmost studies lack an in-depth analysis of MLLM design choices. Moreover, the\nintegration of MLLMs and diffusion models remains an open challenge in some\ndifficult tasks, such as video editing. In this paper, we present InstructX, a\nunified framework for image and video editing. Specifically, we conduct a\ncomprehensive study on integrating MLLMs and diffusion models for\ninstruction-driven editing across diverse tasks. Building on this study, we\nanalyze the cooperation and distinction between images and videos in unified\nmodeling. (1) We show that training on image data can lead to emergent video\nediting capabilities without explicit supervision, thereby alleviating the\nconstraints imposed by scarce video training data. (2) By incorporating\nmodality-specific MLLM features, our approach effectively unifies image and\nvideo editing tasks within a single model. Extensive experiments demonstrate\nthat our method can handle a broad range of image and video editing tasks and\nachieves state-of-the-art performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08485.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63ec78f4c81b6a52391e1291",
            "avatarUrl": "/avatars/d13b35964c895f7aec69287390036a79.svg",
            "fullname": "ChongMou",
            "name": "Adapter",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.03663",
            "authors": [
                {
                    "_id": "68e7fb7e93680d7965e7c877",
                    "name": "Xiangyu Peng",
                    "hidden": false
                },
                {
                    "_id": "68e7fb7e93680d7965e7c878",
                    "user": {
                        "_id": "66a9bdfc35c89692442ba4b7",
                        "avatarUrl": "/avatars/ad87d1d2d81775eeb0920bf3ebe08cc2.svg",
                        "isPro": false,
                        "fullname": "Can Qin",
                        "user": "canqin001",
                        "type": "user"
                    },
                    "name": "Cab Qin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:33:36.650Z",
                    "hidden": false
                },
                {
                    "_id": "68e7fb7e93680d7965e7c879",
                    "name": "Zeyuan Chen",
                    "hidden": false
                },
                {
                    "_id": "68e7fb7e93680d7965e7c87a",
                    "name": "Ran Xu",
                    "hidden": false
                },
                {
                    "_id": "68e7fb7e93680d7965e7c87b",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "68e7fb7e93680d7965e7c87c",
                    "name": "Chien-Sheng Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-04T04:30:13.000Z",
            "submittedOnDailyAt": "2025-10-10T00:41:09.901Z",
            "title": "UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG",
            "submittedOnDailyBy": {
                "_id": "66a9bdfc35c89692442ba4b7",
                "avatarUrl": "/avatars/ad87d1d2d81775eeb0920bf3ebe08cc2.svg",
                "isPro": false,
                "fullname": "Can Qin",
                "user": "canqin001",
                "type": "user"
            },
            "summary": "Multimodal retrieval-augmented generation (MM-RAG) is a key approach for\napplying large language models (LLMs) and agents to real-world knowledge bases,\nyet current evaluations are fragmented, focusing on either text or images in\nisolation or on simplified multimodal setups that fail to capture\ndocument-centric multimodal use cases. In this paper, we introduce\nUniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from\n70k real-world PDF pages across eight domains. Our pipeline extracts and links\nevidence from text, tables, and figures, then generates 1,600 multimodal QA\npairs spanning factual retrieval, comparison, summarization, and logical\nreasoning queries. To ensure reliability, 20% of QA pairs are validated by\nmultiple annotators and expert adjudication. UniDoc-Bench supports\napples-to-apples comparison across four paradigms: (1) text-only, (2)\nimage-only, (3) multimodal text-image fusion, and (4) multimodal joint\nretrieval -- under a unified protocol with standardized candidate pools,\nprompts, and evaluation metrics. Our experiments show that multimodal\ntext-image fusion RAG systems consistently outperform both unimodal and jointly\nmultimodal embedding-based retrieval, indicating that neither text nor images\nalone are sufficient and that current multimodal embeddings remain inadequate.\nBeyond benchmarking, our analysis reveals when and how visual context\ncomplements textual evidence, uncovers systematic failure modes, and offers\nactionable guidance for developing more robust MM-RAG pipelines.",
            "upvotes": 13,
            "discussionId": "68e7fb7e93680d7965e7c87d",
            "githubRepo": "https://github.com/SalesforceAIResearch/UniDoc-Bench",
            "ai_summary": "UniDoc-Bench is a large-scale benchmark for multimodal retrieval-augmented generation, evaluating systems across text, images, and their fusion in real-world document-centric scenarios.",
            "ai_keywords": [
                "multimodal retrieval-augmented generation",
                "large language models",
                "knowledge bases",
                "UniDoc-Bench",
                "PDF pages",
                "multimodal QA pairs",
                "factual retrieval",
                "comparison",
                "summarization",
                "logical reasoning",
                "text-only",
                "image-only",
                "multimodal text-image fusion",
                "multimodal joint retrieval",
                "candidate pools",
                "prompts",
                "evaluation metrics",
                "visual context",
                "textual evidence",
                "failure modes",
                "robust MM-RAG pipelines"
            ],
            "githubStars": 4,
            "organization": {
                "_id": "5f6d64475e78cc6b0ed31e4c",
                "name": "Salesforce",
                "fullname": "Salesforce",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
            }
        },
        "publishedAt": "2025-10-04T00:30:13.000Z",
        "title": "UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG",
        "summary": "Multimodal retrieval-augmented generation (MM-RAG) is a key approach for\napplying large language models (LLMs) and agents to real-world knowledge bases,\nyet current evaluations are fragmented, focusing on either text or images in\nisolation or on simplified multimodal setups that fail to capture\ndocument-centric multimodal use cases. In this paper, we introduce\nUniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from\n70k real-world PDF pages across eight domains. Our pipeline extracts and links\nevidence from text, tables, and figures, then generates 1,600 multimodal QA\npairs spanning factual retrieval, comparison, summarization, and logical\nreasoning queries. To ensure reliability, 20% of QA pairs are validated by\nmultiple annotators and expert adjudication. UniDoc-Bench supports\napples-to-apples comparison across four paradigms: (1) text-only, (2)\nimage-only, (3) multimodal text-image fusion, and (4) multimodal joint\nretrieval -- under a unified protocol with standardized candidate pools,\nprompts, and evaluation metrics. Our experiments show that multimodal\ntext-image fusion RAG systems consistently outperform both unimodal and jointly\nmultimodal embedding-based retrieval, indicating that neither text nor images\nalone are sufficient and that current multimodal embeddings remain inadequate.\nBeyond benchmarking, our analysis reveals when and how visual context\ncomplements textual evidence, uncovers systematic failure modes, and offers\nactionable guidance for developing more robust MM-RAG pipelines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03663.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "66a9bdfc35c89692442ba4b7",
            "avatarUrl": "/avatars/ad87d1d2d81775eeb0920bf3ebe08cc2.svg",
            "fullname": "Can Qin",
            "name": "canqin001",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "5f6d64475e78cc6b0ed31e4c",
            "name": "Salesforce",
            "fullname": "Salesforce",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.06915",
            "authors": [
                {
                    "_id": "68e86a8f95e8e6771df388e6",
                    "user": {
                        "_id": "64096ef79e9f790c905b846d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
                        "isPro": false,
                        "fullname": "Zecheng Tang",
                        "user": "ZetangForward",
                        "type": "user"
                    },
                    "name": "Zecheng Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:55.403Z",
                    "hidden": false
                },
                {
                    "_id": "68e86a8f95e8e6771df388e7",
                    "name": "Baibei Ji",
                    "hidden": false
                },
                {
                    "_id": "68e86a8f95e8e6771df388e8",
                    "user": {
                        "_id": "6732fb1d24b316be87acaafe",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6732fb1d24b316be87acaafe/BzD8HL4vhh3mfeSF3rm_1.jpeg",
                        "isPro": false,
                        "fullname": "Quantong Qiu",
                        "user": "QQTang1223",
                        "type": "user"
                    },
                    "name": "Quantong Qiu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:52.702Z",
                    "hidden": false
                },
                {
                    "_id": "68e86a8f95e8e6771df388e9",
                    "name": "Haitian Wang",
                    "hidden": false
                },
                {
                    "_id": "68e86a8f95e8e6771df388ea",
                    "name": "Xiaobo Liang",
                    "hidden": false
                },
                {
                    "_id": "68e86a8f95e8e6771df388eb",
                    "name": "Juntao Li",
                    "hidden": false
                },
                {
                    "_id": "68e86a8f95e8e6771df388ec",
                    "name": "Min Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T11:48:16.000Z",
            "submittedOnDailyAt": "2025-10-10T00:39:31.912Z",
            "title": "LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling",
            "submittedOnDailyBy": {
                "_id": "64096ef79e9f790c905b846d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
                "isPro": false,
                "fullname": "Zecheng Tang",
                "user": "ZetangForward",
                "type": "user"
            },
            "summary": "Reward model (RM) plays a pivotal role in aligning large language model (LLM)\nwith human preferences. As real-world applications increasingly involve long\nhistory trajectories, e.g., LLM agent, it becomes indispensable to evaluate\nwhether a model's responses are not only high-quality but also grounded in and\nconsistent with the provided context. Yet, current RMs remain confined to\nshort-context settings and primarily focus on response-level attributes (e.g.,\nsafety or helpfulness), while largely neglecting the critical dimension of long\ncontext-response consistency. In this work, we introduce Long-RewardBench, a\nbenchmark specifically designed for long-context RM evaluation, featuring both\nPairwise Comparison and Best-of-N tasks. Our preliminary study reveals that\neven state-of-the-art generative RMs exhibit significant fragility in\nlong-context scenarios, failing to maintain context-aware preference judgments.\nMotivated by the analysis of failure patterns observed in model outputs, we\npropose a general multi-stage training strategy that effectively scales\narbitrary models into robust Long-context RMs (LongRMs). Experiments show that\nour approach not only substantially improves performance on long-context\nevaluation but also preserves strong short-context capability. Notably, our 8B\nLongRM outperforms much larger 70B-scale baselines and matches the performance\nof the proprietary Gemini 2.5 Pro model.",
            "upvotes": 12,
            "discussionId": "68e86a8f95e8e6771df388ed",
            "githubRepo": "https://github.com/LCM-Lab/LongRM",
            "ai_summary": "A benchmark and training strategy for reward models to improve long-context consistency and performance in large language models.",
            "ai_keywords": [
                "reward model",
                "large language model",
                "long-context",
                "short-context",
                "Pairwise Comparison",
                "Best-of-N",
                "context-response consistency",
                "multi-stage training",
                "long-context RMs",
                "LongRMs"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "61f8e653129c9ff1b911293d",
                "name": "SUDA",
                "fullname": "Soochow University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643701821817-61f8e5934a8e5a275b2b3e5a.png"
            }
        },
        "publishedAt": "2025-10-08T07:48:16.000Z",
        "title": "LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling",
        "summary": "Reward model (RM) plays a pivotal role in aligning large language model (LLM)\nwith human preferences. As real-world applications increasingly involve long\nhistory trajectories, e.g., LLM agent, it becomes indispensable to evaluate\nwhether a model's responses are not only high-quality but also grounded in and\nconsistent with the provided context. Yet, current RMs remain confined to\nshort-context settings and primarily focus on response-level attributes (e.g.,\nsafety or helpfulness), while largely neglecting the critical dimension of long\ncontext-response consistency. In this work, we introduce Long-RewardBench, a\nbenchmark specifically designed for long-context RM evaluation, featuring both\nPairwise Comparison and Best-of-N tasks. Our preliminary study reveals that\neven state-of-the-art generative RMs exhibit significant fragility in\nlong-context scenarios, failing to maintain context-aware preference judgments.\nMotivated by the analysis of failure patterns observed in model outputs, we\npropose a general multi-stage training strategy that effectively scales\narbitrary models into robust Long-context RMs (LongRMs). Experiments show that\nour approach not only substantially improves performance on long-context\nevaluation but also preserves strong short-context capability. Notably, our 8B\nLongRM outperforms much larger 70B-scale baselines and matches the performance\nof the proprietary Gemini 2.5 Pro model.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06915.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64096ef79e9f790c905b846d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
            "fullname": "Zecheng Tang",
            "name": "ZetangForward",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "61f8e653129c9ff1b911293d",
            "name": "SUDA",
            "fullname": "Soochow University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643701821817-61f8e5934a8e5a275b2b3e5a.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08002",
            "authors": [
                {
                    "_id": "68e872a695e8e6771df389bf",
                    "user": {
                        "_id": "64b289681917256b41140bf7",
                        "avatarUrl": "/avatars/22309802f680ad38310d53d4b008ab21.svg",
                        "isPro": false,
                        "fullname": "YcHades",
                        "user": "YcHades",
                        "type": "user"
                    },
                    "name": "Cheng Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T08:56:17.687Z",
                    "hidden": false
                },
                {
                    "_id": "68e872a695e8e6771df389c0",
                    "user": {
                        "_id": "656882da5025f8e01b424eda",
                        "avatarUrl": "/avatars/7fda244634c90b4647b49f83fdb25a9b.svg",
                        "isPro": false,
                        "fullname": "yxm",
                        "user": "jokester-yxm",
                        "type": "user"
                    },
                    "name": "Xuemeng Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T08:57:50.348Z",
                    "hidden": false
                },
                {
                    "_id": "68e872a695e8e6771df389c1",
                    "user": {
                        "_id": "64a7c43ae940d769194055df",
                        "avatarUrl": "/avatars/441ccadd62e039fb8cb112f138ed917d.svg",
                        "isPro": false,
                        "fullname": "Licheng Wen",
                        "user": "Wayne-lc",
                        "type": "user"
                    },
                    "name": "Licheng Wen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T08:57:56.780Z",
                    "hidden": false
                },
                {
                    "_id": "68e872a695e8e6771df389c2",
                    "user": {
                        "_id": "64673506b990713c503079bc",
                        "avatarUrl": "/avatars/17396c835f1eeae00ec2972b6a24a2dd.svg",
                        "isPro": false,
                        "fullname": "Fu",
                        "user": "fudaocheng",
                        "type": "user"
                    },
                    "name": "Daocheng Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:07.385Z",
                    "hidden": false
                },
                {
                    "_id": "68e872a695e8e6771df389c3",
                    "user": {
                        "_id": "65827953d73d6402f7ae332d",
                        "avatarUrl": "/avatars/d57eae62171ece7a237eaf4ea938a31c.svg",
                        "isPro": false,
                        "fullname": "Mei",
                        "user": "Jianbiao",
                        "type": "user"
                    },
                    "name": "Jianbiao Mei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T08:55:40.955Z",
                    "hidden": false
                },
                {
                    "_id": "68e872a695e8e6771df389c4",
                    "name": "Rong Wu",
                    "hidden": false
                },
                {
                    "_id": "68e872a695e8e6771df389c5",
                    "name": "Pinlong Cai",
                    "hidden": false
                },
                {
                    "_id": "68e872a695e8e6771df389c6",
                    "name": "Yufan Shen",
                    "hidden": false
                },
                {
                    "_id": "68e872a695e8e6771df389c7",
                    "name": "Nianchen Deng",
                    "hidden": false
                },
                {
                    "_id": "68e872a695e8e6771df389c8",
                    "name": "Botian Shi",
                    "hidden": false
                },
                {
                    "_id": "68e872a695e8e6771df389c9",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68e872a695e8e6771df389ca",
                    "name": "Haifeng Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T09:40:34.000Z",
            "submittedOnDailyAt": "2025-10-10T05:56:05.803Z",
            "title": "Learning on the Job: An Experience-Driven Self-Evolving Agent for\n  Long-Horizon Tasks",
            "submittedOnDailyBy": {
                "_id": "64a7c43ae940d769194055df",
                "avatarUrl": "/avatars/441ccadd62e039fb8cb112f138ed917d.svg",
                "isPro": false,
                "fullname": "Licheng Wen",
                "user": "Wayne-lc",
                "type": "user"
            },
            "summary": "Large Language Models have demonstrated remarkable capabilities across\ndiverse domains, yet significant challenges persist when deploying them as AI\nagents for real-world long-horizon tasks. Existing LLM agents suffer from a\ncritical limitation: they are test-time static and cannot learn from\nexperience, lacking the ability to accumulate knowledge and continuously\nimprove on the job. To address this challenge, we propose MUSE, a novel agent\nframework that introduces an experience-driven, self-evolving system centered\naround a hierarchical Memory Module. MUSE organizes diverse levels of\nexperience and leverages them to plan and execute long-horizon tasks across\nmultiple applications. After each sub-task execution, the agent autonomously\nreflects on its trajectory, converting the raw trajectory into structured\nexperience and integrating it back into the Memory Module. This mechanism\nenables the agent to evolve beyond its static pretrained parameters, fostering\ncontinuous learning and self-evolution. We evaluate MUSE on the long-horizon\nproductivity benchmark TAC. It achieves new SOTA performance by a significant\nmargin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments\ndemonstrate that as the agent autonomously accumulates experience, it exhibits\nincreasingly superior task completion capabilities, as well as robust\ncontinuous learning and self-evolution capabilities. Moreover, the accumulated\nexperience from MUSE exhibits strong generalization properties, enabling\nzero-shot improvement on new tasks. MUSE establishes a new paradigm for AI\nagents capable of real-world productivity task automation.",
            "upvotes": 9,
            "discussionId": "68e872a695e8e6771df389cb",
            "ai_summary": "MUSE, a novel agent framework with a hierarchical Memory Module, enables continuous learning and self-evolution, achieving state-of-the-art performance on long-horizon productivity tasks using a lightweight model.",
            "ai_keywords": [
                "Large Language Models",
                "LLM agents",
                "experience-driven",
                "self-evolving system",
                "hierarchical Memory Module",
                "long-horizon tasks",
                "trajectory reflection",
                "structured experience",
                "continuous learning",
                "self-evolution",
                "TAC benchmark",
                "Gemini-2.5 Flash model",
                "zero-shot improvement"
            ],
            "organization": {
                "_id": "68e8716fa897e565aaec87ca",
                "name": "KnowledgeXLab",
                "fullname": "KnowledgeXLab@Shanghai AI Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64a7c43ae940d769194055df/oPtAVxSeVl8XHm5mo0Nq8.png"
            }
        },
        "publishedAt": "2025-10-09T05:40:34.000Z",
        "title": "Learning on the Job: An Experience-Driven Self-Evolving Agent for\n  Long-Horizon Tasks",
        "summary": "Large Language Models have demonstrated remarkable capabilities across\ndiverse domains, yet significant challenges persist when deploying them as AI\nagents for real-world long-horizon tasks. Existing LLM agents suffer from a\ncritical limitation: they are test-time static and cannot learn from\nexperience, lacking the ability to accumulate knowledge and continuously\nimprove on the job. To address this challenge, we propose MUSE, a novel agent\nframework that introduces an experience-driven, self-evolving system centered\naround a hierarchical Memory Module. MUSE organizes diverse levels of\nexperience and leverages them to plan and execute long-horizon tasks across\nmultiple applications. After each sub-task execution, the agent autonomously\nreflects on its trajectory, converting the raw trajectory into structured\nexperience and integrating it back into the Memory Module. This mechanism\nenables the agent to evolve beyond its static pretrained parameters, fostering\ncontinuous learning and self-evolution. We evaluate MUSE on the long-horizon\nproductivity benchmark TAC. It achieves new SOTA performance by a significant\nmargin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments\ndemonstrate that as the agent autonomously accumulates experience, it exhibits\nincreasingly superior task completion capabilities, as well as robust\ncontinuous learning and self-evolution capabilities. Moreover, the accumulated\nexperience from MUSE exhibits strong generalization properties, enabling\nzero-shot improvement on new tasks. MUSE establishes a new paradigm for AI\nagents capable of real-world productivity task automation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08002.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a7c43ae940d769194055df",
            "avatarUrl": "/avatars/441ccadd62e039fb8cb112f138ed917d.svg",
            "fullname": "Licheng Wen",
            "name": "Wayne-lc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "68e8716fa897e565aaec87ca",
            "name": "KnowledgeXLab",
            "fullname": "KnowledgeXLab@Shanghai AI Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64a7c43ae940d769194055df/oPtAVxSeVl8XHm5mo0Nq8.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.03117",
            "authors": [
                {
                    "_id": "68e8c3e895e8e6771df38c02",
                    "user": {
                        "_id": "63bbf071d8d676a2299c7d0b",
                        "avatarUrl": "/avatars/4bb1c86ef8651c75b9761afee2865267.svg",
                        "isPro": false,
                        "fullname": "Guan",
                        "user": "Guan123",
                        "type": "user"
                    },
                    "name": "Kaisi Guan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T08:54:17.605Z",
                    "hidden": false
                },
                {
                    "_id": "68e8c3e895e8e6771df38c03",
                    "name": "Xihua Wang",
                    "hidden": false
                },
                {
                    "_id": "68e8c3e895e8e6771df38c04",
                    "name": "Zhengfeng Lai",
                    "hidden": false
                },
                {
                    "_id": "68e8c3e895e8e6771df38c05",
                    "name": "Xin Cheng",
                    "hidden": false
                },
                {
                    "_id": "68e8c3e895e8e6771df38c06",
                    "name": "Peng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e8c3e895e8e6771df38c07",
                    "name": "XiaoJiang Liu",
                    "hidden": false
                },
                {
                    "_id": "68e8c3e895e8e6771df38c08",
                    "name": "Ruihua Song",
                    "hidden": false
                },
                {
                    "_id": "68e8c3e895e8e6771df38c09",
                    "name": "Meng Cao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-03T15:43:56.000Z",
            "submittedOnDailyAt": "2025-10-10T07:01:51.817Z",
            "title": "Taming Text-to-Sounding Video Generation via Advanced Modality Condition\n  and Interaction",
            "submittedOnDailyBy": {
                "_id": "63bbf071d8d676a2299c7d0b",
                "avatarUrl": "/avatars/4bb1c86ef8651c75b9761afee2865267.svg",
                "isPro": false,
                "fullname": "Guan",
                "user": "Guan123",
                "type": "user"
            },
            "summary": "This study focuses on a challenging yet promising task,\nText-to-Sounding-Video (T2SV) generation, which aims to generate a video with\nsynchronized audio from text conditions, meanwhile ensuring both modalities are\naligned with text. Despite progress in joint audio-video training, two critical\nchallenges still remain unaddressed: (1) a single, shared text caption where\nthe text for video is equal to the text for audio often creates modal\ninterference, confusing the pretrained backbones, and (2) the optimal mechanism\nfor cross-modal feature interaction remains unclear. To address these\nchallenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC)\nframework that generates pairs of disentangled captions, a video caption, and\nan audio caption, eliminating interference at the conditioning stage. Based on\nHVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer,\nwhich employs a Dual CrossAttention (DCA) mechanism that acts as a robust\n``bridge\" to enable a symmetric, bidirectional exchange of information,\nachieving both semantic and temporal synchronization. Extensive experiments on\nthree benchmark datasets, supported by human evaluations, demonstrate that our\nmethod achieves state-of-the-art results on most metrics. Comprehensive\nablation studies further validate the effectiveness of our contributions,\noffering key insights for the future T2SV task. All the codes and checkpoints\nwill be publicly released.",
            "upvotes": 9,
            "discussionId": "68e8c3e895e8e6771df38c0a",
            "projectPage": "https://bridgedit-t2sv.github.io/",
            "ai_summary": "A novel dual-tower diffusion transformer with a Dual CrossAttention mechanism addresses challenges in Text-to-Sounding-Video generation by disentangling captions and enabling symmetric information exchange.",
            "ai_keywords": [
                "Hierarchical Visual-Grounded Captioning",
                "HVGC",
                "BridgeDiT",
                "dual-tower diffusion transformer",
                "Dual CrossAttention",
                "DCA",
                "semantic synchronization",
                "temporal synchronization"
            ],
            "organization": {
                "_id": "628cbd99ef14f971b69948ab",
                "name": "apple",
                "fullname": "Apple",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
            }
        },
        "publishedAt": "2025-10-03T11:43:56.000Z",
        "title": "Taming Text-to-Sounding Video Generation via Advanced Modality Condition\n  and Interaction",
        "summary": "This study focuses on a challenging yet promising task,\nText-to-Sounding-Video (T2SV) generation, which aims to generate a video with\nsynchronized audio from text conditions, meanwhile ensuring both modalities are\naligned with text. Despite progress in joint audio-video training, two critical\nchallenges still remain unaddressed: (1) a single, shared text caption where\nthe text for video is equal to the text for audio often creates modal\ninterference, confusing the pretrained backbones, and (2) the optimal mechanism\nfor cross-modal feature interaction remains unclear. To address these\nchallenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC)\nframework that generates pairs of disentangled captions, a video caption, and\nan audio caption, eliminating interference at the conditioning stage. Based on\nHVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer,\nwhich employs a Dual CrossAttention (DCA) mechanism that acts as a robust\n``bridge\" to enable a symmetric, bidirectional exchange of information,\nachieving both semantic and temporal synchronization. Extensive experiments on\nthree benchmark datasets, supported by human evaluations, demonstrate that our\nmethod achieves state-of-the-art results on most metrics. Comprehensive\nablation studies further validate the effectiveness of our contributions,\noffering key insights for the future T2SV task. All the codes and checkpoints\nwill be publicly released.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03117.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63bbf071d8d676a2299c7d0b",
            "avatarUrl": "/avatars/4bb1c86ef8651c75b9761afee2865267.svg",
            "fullname": "Guan",
            "name": "Guan123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08425",
            "authors": [
                {
                    "_id": "68e874b595e8e6771df38a02",
                    "name": "Yihong Luo",
                    "hidden": false
                },
                {
                    "_id": "68e874b595e8e6771df38a03",
                    "name": "Tianyang Hu",
                    "hidden": false
                },
                {
                    "_id": "68e874b595e8e6771df38a04",
                    "name": "Jing Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T16:40:43.000Z",
            "submittedOnDailyAt": "2025-10-10T01:23:34.693Z",
            "title": "Reinforcing Diffusion Models by Direct Group Preference Optimization",
            "submittedOnDailyBy": {
                "_id": "65f7e6856bd4bac5b6a4ecc3",
                "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
                "isPro": false,
                "fullname": "Yihong Luo",
                "user": "Luo-Yihong",
                "type": "user"
            },
            "summary": "While reinforcement learning methods such as Group Relative Preference\nOptimization (GRPO) have significantly enhanced Large Language Models, adapting\nthem to diffusion models remains challenging. In particular, GRPO demands a\nstochastic policy, yet the most cost-effective diffusion samplers are based on\ndeterministic ODEs. Recent work addresses this issue by using inefficient\nSDE-based samplers to induce stochasticity, but this reliance on model-agnostic\nGaussian noise leads to slow convergence. To resolve this conflict, we propose\nDirect Group Preference Optimization (DGPO), a new online RL algorithm that\ndispenses with the policy-gradient framework entirely. DGPO learns directly\nfrom group-level preferences, which utilize relative information of samples\nwithin groups. This design eliminates the need for inefficient stochastic\npolicies, unlocking the use of efficient deterministic ODE samplers and faster\ntraining. Extensive results show that DGPO trains around 20 times faster than\nexisting state-of-the-art methods and achieves superior performance on both\nin-domain and out-of-domain reward metrics. Code is available at\nhttps://github.com/Luo-Yihong/DGPO.",
            "upvotes": 8,
            "discussionId": "68e874b595e8e6771df38a05",
            "githubRepo": "https://github.com/Luo-Yihong/DGPO",
            "ai_summary": "DGPO, a new online RL algorithm, enhances diffusion models by learning from group-level preferences, enabling the use of efficient deterministic ODE samplers and achieving faster training and superior performance.",
            "ai_keywords": [
                "Group Relative Preference Optimization",
                "GRPO",
                "diffusion models",
                "stochastic policy",
                "deterministic ODEs",
                "SDE-based samplers",
                "Gaussian noise",
                "Direct Group Preference Optimization",
                "DGPO",
                "policy-gradient framework",
                "group-level preferences",
                "in-domain reward metrics",
                "out-of-domain reward metrics"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-10-09T12:40:43.000Z",
        "title": "Reinforcing Diffusion Models by Direct Group Preference Optimization",
        "summary": "While reinforcement learning methods such as Group Relative Preference\nOptimization (GRPO) have significantly enhanced Large Language Models, adapting\nthem to diffusion models remains challenging. In particular, GRPO demands a\nstochastic policy, yet the most cost-effective diffusion samplers are based on\ndeterministic ODEs. Recent work addresses this issue by using inefficient\nSDE-based samplers to induce stochasticity, but this reliance on model-agnostic\nGaussian noise leads to slow convergence. To resolve this conflict, we propose\nDirect Group Preference Optimization (DGPO), a new online RL algorithm that\ndispenses with the policy-gradient framework entirely. DGPO learns directly\nfrom group-level preferences, which utilize relative information of samples\nwithin groups. This design eliminates the need for inefficient stochastic\npolicies, unlocking the use of efficient deterministic ODE samplers and faster\ntraining. Extensive results show that DGPO trains around 20 times faster than\nexisting state-of-the-art methods and achieves superior performance on both\nin-domain and out-of-domain reward metrics. Code is available at\nhttps://github.com/Luo-Yihong/DGPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08425.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f7e6856bd4bac5b6a4ecc3",
            "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
            "fullname": "Yihong Luo",
            "name": "Luo-Yihong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08559",
            "authors": [
                {
                    "_id": "68e86dbd95e8e6771df3890f",
                    "user": {
                        "_id": "64616f03455531c6be7cfc37",
                        "avatarUrl": "/avatars/a97207e7ec9482162f051624bb28fdfb.svg",
                        "isPro": false,
                        "fullname": "Andong Deng",
                        "user": "groundmore",
                        "type": "user"
                    },
                    "name": "Andong Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T09:12:33.201Z",
                    "hidden": false
                },
                {
                    "_id": "68e86dbd95e8e6771df38910",
                    "name": "Taojiannan Yang",
                    "hidden": false
                },
                {
                    "_id": "68e86dbd95e8e6771df38911",
                    "name": "Shoubin Yu",
                    "hidden": false
                },
                {
                    "_id": "68e86dbd95e8e6771df38912",
                    "name": "Lincoln Spencer",
                    "hidden": false
                },
                {
                    "_id": "68e86dbd95e8e6771df38913",
                    "name": "Mohit Bansal",
                    "hidden": false
                },
                {
                    "_id": "68e86dbd95e8e6771df38914",
                    "name": "Chen Chen",
                    "hidden": false
                },
                {
                    "_id": "68e86dbd95e8e6771df38915",
                    "name": "Serena Yeung-Levy",
                    "hidden": false
                },
                {
                    "_id": "68e86dbd95e8e6771df38916",
                    "name": "Xiaohan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T17:59:23.000Z",
            "submittedOnDailyAt": "2025-10-10T00:52:07.252Z",
            "title": "SciVideoBench: Benchmarking Scientific Video Reasoning in Large\n  Multimodal Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Large Multimodal Models (LMMs) have achieved remarkable progress across\nvarious capabilities; however, complex video reasoning in the scientific domain\nremains a significant and challenging frontier. Current video benchmarks\npredominantly target general scenarios where perception/recognition is heavily\nrelied on, while with relatively simple reasoning tasks, leading to saturation\nand thus failing to effectively evaluate advanced multimodal cognitive skills.\nTo address this critical gap, we introduce SciVideoBench, a rigorous benchmark\nspecifically designed to assess advanced video reasoning in scientific\ncontexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice\nquestions derived from cutting-edge scientific experimental videos spanning\nover 25 specialized academic subjects and verified by a semi-automatic system.\nEach question demands sophisticated domain-specific knowledge, precise\nspatiotemporal perception, and intricate logical reasoning, effectively\nchallenging models' higher-order cognitive abilities. Our evaluation highlights\nsignificant performance deficits in state-of-the-art proprietary and\nopen-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating\nsubstantial room for advancement in video reasoning capabilities. Detailed\nanalyses of critical factors such as reasoning complexity and visual grounding\nprovide valuable insights and clear direction for future developments in LMMs,\ndriving the evolution of truly capable multimodal AI co-scientists. We hope\nSciVideoBench could fit the interests of the community and help to push the\nboundary of cutting-edge AI for border science.",
            "upvotes": 7,
            "discussionId": "68e86dbd95e8e6771df38917",
            "projectPage": "https://scivideobench.github.io/",
            "githubRepo": "https://github.com/dengandong/SciVideoBench",
            "ai_summary": "SciVideoBench is a benchmark designed to evaluate advanced video reasoning in scientific contexts, challenging models with sophisticated domain-specific knowledge and logical reasoning.",
            "ai_keywords": [
                "Large Multimodal Models",
                "LMMs",
                "video reasoning",
                "scientific domain",
                "video benchmarks",
                "perception",
                "recognition",
                "reasoning tasks",
                "multiple-choice questions",
                "spatiotemporal perception",
                "logical reasoning",
                "cognitive abilities",
                "reasoning complexity",
                "visual grounding",
                "multimodal AI co-scientists"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-10-09T13:59:23.000Z",
        "title": "SciVideoBench: Benchmarking Scientific Video Reasoning in Large\n  Multimodal Models",
        "summary": "Large Multimodal Models (LMMs) have achieved remarkable progress across\nvarious capabilities; however, complex video reasoning in the scientific domain\nremains a significant and challenging frontier. Current video benchmarks\npredominantly target general scenarios where perception/recognition is heavily\nrelied on, while with relatively simple reasoning tasks, leading to saturation\nand thus failing to effectively evaluate advanced multimodal cognitive skills.\nTo address this critical gap, we introduce SciVideoBench, a rigorous benchmark\nspecifically designed to assess advanced video reasoning in scientific\ncontexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice\nquestions derived from cutting-edge scientific experimental videos spanning\nover 25 specialized academic subjects and verified by a semi-automatic system.\nEach question demands sophisticated domain-specific knowledge, precise\nspatiotemporal perception, and intricate logical reasoning, effectively\nchallenging models' higher-order cognitive abilities. Our evaluation highlights\nsignificant performance deficits in state-of-the-art proprietary and\nopen-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating\nsubstantial room for advancement in video reasoning capabilities. Detailed\nanalyses of critical factors such as reasoning complexity and visual grounding\nprovide valuable insights and clear direction for future developments in LMMs,\ndriving the evolution of truly capable multimodal AI co-scientists. We hope\nSciVideoBench could fit the interests of the community and help to push the\nboundary of cutting-edge AI for border science.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08559.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 123
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08431",
            "authors": [
                {
                    "_id": "68e866ef95e8e6771df3889a",
                    "name": "Kaiwen Zheng",
                    "hidden": false
                },
                {
                    "_id": "68e866ef95e8e6771df3889b",
                    "name": "Yuji Wang",
                    "hidden": false
                },
                {
                    "_id": "68e866ef95e8e6771df3889c",
                    "name": "Qianli Ma",
                    "hidden": false
                },
                {
                    "_id": "68e866ef95e8e6771df3889d",
                    "user": {
                        "_id": "65571135bfb62d747abc8129",
                        "avatarUrl": "/avatars/5f4542daa34597f17e6280b9cce18c91.svg",
                        "isPro": false,
                        "fullname": "Hugging",
                        "user": "ChenDRAG",
                        "type": "user"
                    },
                    "name": "Huayu Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:33:05.478Z",
                    "hidden": false
                },
                {
                    "_id": "68e866ef95e8e6771df3889e",
                    "name": "Jintao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e866ef95e8e6771df3889f",
                    "name": "Yogesh Balaji",
                    "hidden": false
                },
                {
                    "_id": "68e866ef95e8e6771df388a0",
                    "name": "Jianfei Chen",
                    "hidden": false
                },
                {
                    "_id": "68e866ef95e8e6771df388a1",
                    "name": "Ming-Yu Liu",
                    "hidden": false
                },
                {
                    "_id": "68e866ef95e8e6771df388a2",
                    "name": "Jun Zhu",
                    "hidden": false
                },
                {
                    "_id": "68e866ef95e8e6771df388a3",
                    "name": "Qinsheng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T16:45:30.000Z",
            "submittedOnDailyAt": "2025-10-10T01:01:40.968Z",
            "title": "Large Scale Diffusion Distillation via Score-Regularized Continuous-Time\n  Consistency",
            "submittedOnDailyBy": {
                "_id": "652bf7edc3cba555d5673c6e",
                "avatarUrl": "/avatars/78f6416c30203b30671f8423f061c657.svg",
                "isPro": true,
                "fullname": "Kaiwen Zheng",
                "user": "worstcoder",
                "type": "user"
            },
            "summary": "This work represents the first effort to scale up continuous-time consistency\ndistillation to general application-level image and video diffusion models.\nAlthough continuous-time consistency model (sCM) is theoretically principled\nand empirically powerful for accelerating academic-scale diffusion, its\napplicability to large-scale text-to-image and video tasks remains unclear due\nto infrastructure challenges in Jacobian-vector product (JVP) computation and\nthe limitations of standard evaluation benchmarks. We first develop a\nparallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on\nmodels with over 10 billion parameters and high-dimensional video tasks. Our\ninvestigation reveals fundamental quality limitations of sCM in fine-detail\ngeneration, which we attribute to error accumulation and the \"mode-covering\"\nnature of its forward-divergence objective. To remedy this, we propose the\nscore-regularized continuous-time consistency model (rCM), which incorporates\nscore distillation as a long-skip regularizer. This integration complements sCM\nwith the \"mode-seeking\" reverse divergence, effectively improving visual\nquality while maintaining high generation diversity. Validated on large-scale\nmodels (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM\nmatches or surpasses the state-of-the-art distillation method DMD2 on quality\nmetrics while offering notable advantages in diversity, all without GAN tuning\nor extensive hyperparameter searches. The distilled models generate\nhigh-fidelity samples in only 1sim4 steps, accelerating diffusion sampling\nby 15timessim50times. These results position rCM as a practical and\ntheoretically grounded framework for advancing large-scale diffusion\ndistillation.",
            "upvotes": 7,
            "discussionId": "68e866f095e8e6771df388a4",
            "ai_summary": "Score-regularized continuous-time consistency model (rCM) improves large-scale diffusion distillation by addressing fine-detail generation and diversity issues, achieving high fidelity and accelerated sampling.",
            "ai_keywords": [
                "continuous-time consistency model",
                "sCM",
                "Jacobian-vector product",
                "FlashAttention-2",
                "score-regularized continuous-time consistency model",
                "rCM",
                "score distillation",
                "mode-seeking",
                "mode-covering",
                "diffusion distillation",
                "DMD2",
                "diffusion sampling"
            ]
        },
        "publishedAt": "2025-10-09T12:45:30.000Z",
        "title": "Large Scale Diffusion Distillation via Score-Regularized Continuous-Time\n  Consistency",
        "summary": "This work represents the first effort to scale up continuous-time consistency\ndistillation to general application-level image and video diffusion models.\nAlthough continuous-time consistency model (sCM) is theoretically principled\nand empirically powerful for accelerating academic-scale diffusion, its\napplicability to large-scale text-to-image and video tasks remains unclear due\nto infrastructure challenges in Jacobian-vector product (JVP) computation and\nthe limitations of standard evaluation benchmarks. We first develop a\nparallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on\nmodels with over 10 billion parameters and high-dimensional video tasks. Our\ninvestigation reveals fundamental quality limitations of sCM in fine-detail\ngeneration, which we attribute to error accumulation and the \"mode-covering\"\nnature of its forward-divergence objective. To remedy this, we propose the\nscore-regularized continuous-time consistency model (rCM), which incorporates\nscore distillation as a long-skip regularizer. This integration complements sCM\nwith the \"mode-seeking\" reverse divergence, effectively improving visual\nquality while maintaining high generation diversity. Validated on large-scale\nmodels (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM\nmatches or surpasses the state-of-the-art distillation method DMD2 on quality\nmetrics while offering notable advantages in diversity, all without GAN tuning\nor extensive hyperparameter searches. The distilled models generate\nhigh-fidelity samples in only 1sim4 steps, accelerating diffusion sampling\nby 15timessim50times. These results position rCM as a practical and\ntheoretically grounded framework for advancing large-scale diffusion\ndistillation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08431.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652bf7edc3cba555d5673c6e",
            "avatarUrl": "/avatars/78f6416c30203b30671f8423f061c657.svg",
            "fullname": "Kaiwen Zheng",
            "name": "worstcoder",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08276",
            "authors": [
                {
                    "_id": "68e8795995e8e6771df38a22",
                    "user": {
                        "_id": "64ed96f3067fbb625f83766a",
                        "avatarUrl": "/avatars/e8794ba19bdd965615bd9ead7df77d7d.svg",
                        "isPro": false,
                        "fullname": "Qiaoyu Tang",
                        "user": "TangQiaoYu",
                        "type": "user"
                    },
                    "name": "Qiaoyu Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:02.610Z",
                    "hidden": false
                },
                {
                    "_id": "68e8795995e8e6771df38a23",
                    "name": "Hao Xiang",
                    "hidden": false
                },
                {
                    "_id": "68e8795995e8e6771df38a24",
                    "name": "Le Yu",
                    "hidden": false
                },
                {
                    "_id": "68e8795995e8e6771df38a25",
                    "name": "Bowen Yu",
                    "hidden": false
                },
                {
                    "_id": "68e8795995e8e6771df38a26",
                    "name": "Yaojie Lu",
                    "hidden": false
                },
                {
                    "_id": "68e8795995e8e6771df38a27",
                    "name": "Xianpei Han",
                    "hidden": false
                },
                {
                    "_id": "68e8795995e8e6771df38a28",
                    "name": "Le Sun",
                    "hidden": false
                },
                {
                    "_id": "68e8795995e8e6771df38a29",
                    "name": "WenJuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e8795995e8e6771df38a2a",
                    "name": "Pengbo Wang",
                    "hidden": false
                },
                {
                    "_id": "68e8795995e8e6771df38a2b",
                    "name": "Shixuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68e8795995e8e6771df38a2c",
                    "name": "Zhenru Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e8795995e8e6771df38a2d",
                    "name": "Jianhong Tu",
                    "hidden": false
                },
                {
                    "_id": "68e8795995e8e6771df38a2e",
                    "name": "Hongyu Lin",
                    "hidden": false
                },
                {
                    "_id": "68e8795995e8e6771df38a2f",
                    "name": "Junyang Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T14:31:39.000Z",
            "submittedOnDailyAt": "2025-10-10T01:41:37.224Z",
            "title": "Beyond Turn Limits: Training Deep Search Agents with Dynamic Context\n  Window",
            "submittedOnDailyBy": {
                "_id": "63f33d500be81bdc5d902356",
                "avatarUrl": "/avatars/125812b9a86c3379b34ebfa8026f1a7f.svg",
                "isPro": false,
                "fullname": "xianghao",
                "user": "xiangh",
                "type": "user"
            },
            "summary": "While recent advances in reasoning models have demonstrated cognitive\nbehaviors through reinforcement learning, existing approaches struggle to\ninvoke deep reasoning capabilities in multi-turn agents with long-horizon\ninteractions. We propose DeepMiner, a novel framework that elicits such\nabilities by introducing high-difficulty training tasks and dynamic context\nwindow. DeepMiner presents a reverse construction method to generate complex\nbut verifiable question-answer pairs from authentic web sources, which ensures\nthe challenge and reliability of training data while injecting cognitive\ncapabilities into multi-turn reasoning scenarios. We further design an elegant\nyet effective dynamic context management strategy for both training and\ninference, utilizing sliding window mechanisms while eliminating the dependency\non external summarization models, thereby efficiently empowering the model to\nhandle continuously expanding long-horizon contexts. Through reinforcement\nlearning on Qwen3-32B, we develop DeepMiner-32B, which achieves substantial\nperformance improvements across multiple search agent benchmarks. DeepMiner\nattains 33.5% accuracy on BrowseComp-en, surpassing the previous best\nopen-source agent by almost 20 percentage points, and demonstrates consistent\nimprovements on BrowseComp-zh, XBench-DeepSearch, and GAIA. Notably, our\ndynamic context management enables sustained interactions of nearly 100 turns\nwithin standard 32k context length, effectively addressing the context\nlimitations that constrain existing multi-turn interaction systems.",
            "upvotes": 7,
            "discussionId": "68e8795995e8e6771df38a30",
            "ai_summary": "DeepMiner, a framework using high-difficulty training tasks and dynamic context management, enhances multi-turn reasoning agents through reinforcement learning, achieving significant performance improvements across benchmarks.",
            "ai_keywords": [
                "reinforcement learning",
                "multi-turn agents",
                "long-horizon interactions",
                "high-difficulty training tasks",
                "dynamic context window",
                "reverse construction method",
                "question-answer pairs",
                "sliding window mechanisms",
                "dynamic context management",
                "BrowseComp-en",
                "BrowseComp-zh",
                "XBench-DeepSearch",
                "GAIA"
            ]
        },
        "publishedAt": "2025-10-09T10:31:39.000Z",
        "title": "Beyond Turn Limits: Training Deep Search Agents with Dynamic Context\n  Window",
        "summary": "While recent advances in reasoning models have demonstrated cognitive\nbehaviors through reinforcement learning, existing approaches struggle to\ninvoke deep reasoning capabilities in multi-turn agents with long-horizon\ninteractions. We propose DeepMiner, a novel framework that elicits such\nabilities by introducing high-difficulty training tasks and dynamic context\nwindow. DeepMiner presents a reverse construction method to generate complex\nbut verifiable question-answer pairs from authentic web sources, which ensures\nthe challenge and reliability of training data while injecting cognitive\ncapabilities into multi-turn reasoning scenarios. We further design an elegant\nyet effective dynamic context management strategy for both training and\ninference, utilizing sliding window mechanisms while eliminating the dependency\non external summarization models, thereby efficiently empowering the model to\nhandle continuously expanding long-horizon contexts. Through reinforcement\nlearning on Qwen3-32B, we develop DeepMiner-32B, which achieves substantial\nperformance improvements across multiple search agent benchmarks. DeepMiner\nattains 33.5% accuracy on BrowseComp-en, surpassing the previous best\nopen-source agent by almost 20 percentage points, and demonstrates consistent\nimprovements on BrowseComp-zh, XBench-DeepSearch, and GAIA. Notably, our\ndynamic context management enables sustained interactions of nearly 100 turns\nwithin standard 32k context length, effectively addressing the context\nlimitations that constrain existing multi-turn interaction systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08276.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63f33d500be81bdc5d902356",
            "avatarUrl": "/avatars/125812b9a86c3379b34ebfa8026f1a7f.svg",
            "fullname": "xianghao",
            "name": "xiangh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08549",
            "authors": [
                {
                    "_id": "68e86fbb95e8e6771df3894f",
                    "user": {
                        "_id": "64fefd879fee065c899c4977",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fefd879fee065c899c4977/jyHEOMtHFNMUq9wHEjGr4.png",
                        "isPro": false,
                        "fullname": "Zilin Kang",
                        "user": "zilinkang",
                        "type": "user"
                    },
                    "name": "Zilin Kang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:30.481Z",
                    "hidden": false
                },
                {
                    "_id": "68e86fbb95e8e6771df38950",
                    "user": {
                        "_id": "667fdaee20ee9ac417c7708c",
                        "avatarUrl": "/avatars/69dfba6ff392643af1dcfe8af0a42ae9.svg",
                        "isPro": false,
                        "fullname": "Chonghua Liao",
                        "user": "ChonghuaLiao",
                        "type": "user"
                    },
                    "name": "Chonghua Liao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:32.996Z",
                    "hidden": false
                },
                {
                    "_id": "68e86fbb95e8e6771df38951",
                    "user": {
                        "_id": "665697ead156bf7c364f0505",
                        "avatarUrl": "/avatars/b043889791fd8c2c5810d4c95663d131.svg",
                        "isPro": false,
                        "fullname": "Tingqiang Xu",
                        "user": "xtqqwq",
                        "type": "user"
                    },
                    "name": "Tingqiang Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T09:12:12.803Z",
                    "hidden": false
                },
                {
                    "_id": "68e86fbb95e8e6771df38952",
                    "name": "Huazhe Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T17:56:17.000Z",
            "submittedOnDailyAt": "2025-10-10T01:04:41.534Z",
            "title": "Entropy Regularizing Activation: Boosting Continuous Control, Large\n  Language Models, and Image Classification with Activation as Entropy\n  Constraints",
            "submittedOnDailyBy": {
                "_id": "667fdaee20ee9ac417c7708c",
                "avatarUrl": "/avatars/69dfba6ff392643af1dcfe8af0a42ae9.svg",
                "isPro": false,
                "fullname": "Chonghua Liao",
                "user": "ChonghuaLiao",
                "type": "user"
            },
            "summary": "We propose ERA, a new paradigm that constrains the sampling entropy above\ngiven thresholds by applying specially designed activations to the outputs of\nmodels. Our approach demonstrates broad effectiveness across different domains:\n1) for large language models(LLMs), boosting the AIME 2025 score for\nQwen2.5-Math-7B by 37.4%; 2) for continuous control reinforcement learning\nagents, improving performance by more than 30% over strong baselines such as\nSAC on the challenging HumanoidBench; 3) for image classification, enhancing\nImageNet top-1 accuracy by 0.69% for ResNet-50. These gains are achieved with a\ncomputational overhead of less than 7%. Our work validates output activation as\na powerful tool for entropy control, opening a new direction for designing\nsimpler and more robust algorithms.",
            "upvotes": 5,
            "discussionId": "68e86fbb95e8e6771df38953",
            "projectPage": "https://nothingbutbut.github.io/era/",
            "githubRepo": "https://github.com/nothingbutbut/era",
            "ai_summary": "ERA, a new paradigm using specially designed activations, enhances performance across LLMs, reinforcement learning, and image classification with minimal computational overhead.",
            "ai_keywords": [
                "ERA",
                "sampling entropy",
                "activations",
                "large language models",
                "LLMs",
                "AIME 2025",
                "Qwen2.5-Math-7B",
                "continuous control",
                "reinforcement learning",
                "SAC",
                "HumanoidBench",
                "image classification",
                "ImageNet",
                "ResNet-50",
                "entropy control"
            ],
            "githubStars": 8
        },
        "publishedAt": "2025-10-09T13:56:17.000Z",
        "title": "Entropy Regularizing Activation: Boosting Continuous Control, Large\n  Language Models, and Image Classification with Activation as Entropy\n  Constraints",
        "summary": "We propose ERA, a new paradigm that constrains the sampling entropy above\ngiven thresholds by applying specially designed activations to the outputs of\nmodels. Our approach demonstrates broad effectiveness across different domains:\n1) for large language models(LLMs), boosting the AIME 2025 score for\nQwen2.5-Math-7B by 37.4%; 2) for continuous control reinforcement learning\nagents, improving performance by more than 30% over strong baselines such as\nSAC on the challenging HumanoidBench; 3) for image classification, enhancing\nImageNet top-1 accuracy by 0.69% for ResNet-50. These gains are achieved with a\ncomputational overhead of less than 7%. Our work validates output activation as\na powerful tool for entropy control, opening a new direction for designing\nsimpler and more robust algorithms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08549.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "667fdaee20ee9ac417c7708c",
            "avatarUrl": "/avatars/69dfba6ff392643af1dcfe8af0a42ae9.svg",
            "fullname": "Chonghua Liao",
            "name": "ChonghuaLiao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08203",
            "authors": [
                {
                    "_id": "68e8b3ee95e8e6771df38bd0",
                    "user": {
                        "_id": "68e87503d08107e9fc8470ed",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/URRTpGTTswvOWSSu3QH0c.png",
                        "isPro": false,
                        "fullname": "Shaohua Zhang",
                        "user": "zhangshaohua",
                        "type": "user"
                    },
                    "name": "Shaohua Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T08:34:47.847Z",
                    "hidden": false
                },
                {
                    "_id": "68e8b3ee95e8e6771df38bd1",
                    "name": "Yuan Lin",
                    "hidden": false
                },
                {
                    "_id": "68e8b3ee95e8e6771df38bd2",
                    "name": "Hang Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T13:31:20.000Z",
            "submittedOnDailyAt": "2025-10-10T05:53:35.157Z",
            "title": "Memory Retrieval and Consolidation in Large Language Models through\n  Function Tokens",
            "submittedOnDailyBy": {
                "_id": "60ea81771cc8dc259c58e905",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60ea81771cc8dc259c58e905/kmGlaNvdS4EEHc_5qongT.jpeg",
                "isPro": false,
                "fullname": "yichen he",
                "user": "hyc2026",
                "type": "user"
            },
            "summary": "The remarkable success of large language models (LLMs) stems from their\nability to consolidate vast amounts of knowledge into the memory during\npre-training and to retrieve it from the memory during inference, enabling\nadvanced capabilities such as knowledge memorization, instruction-following and\nreasoning. However, the mechanisms of memory retrieval and consolidation in\nLLMs remain poorly understood. In this paper, we propose the function token\nhypothesis to explain the workings of LLMs: During inference, function tokens\nactivate the most predictive features from context and govern next token\nprediction (memory retrieval). During pre-training, predicting the next tokens\n(usually content tokens) that follow function tokens increases the number of\nlearned features of LLMs and updates the model parameters (memory\nconsolidation). Function tokens here roughly correspond to function words in\nlinguistics, including punctuation marks, articles, prepositions, and\nconjunctions, in contrast to content tokens. We provide extensive experimental\nevidence supporting this hypothesis. Using bipartite graph analysis, we show\nthat a small number of function tokens activate the majority of features. Case\nstudies further reveal how function tokens activate the most predictive\nfeatures from context to direct next token prediction. We also find that during\npre-training, the training loss is dominated by predicting the next content\ntokens following function tokens, which forces the function tokens to select\nthe most predictive features from context.",
            "upvotes": 5,
            "discussionId": "68e8b3ee95e8e6771df38bd3",
            "ai_summary": "Function tokens in large language models activate predictive features during inference and guide memory consolidation during pre-training by predicting subsequent content tokens.",
            "ai_keywords": [
                "large language models",
                "function tokens",
                "memory retrieval",
                "memory consolidation",
                "next token prediction",
                "bipartite graph analysis",
                "content tokens"
            ],
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-10-09T09:31:20.000Z",
        "title": "Memory Retrieval and Consolidation in Large Language Models through\n  Function Tokens",
        "summary": "The remarkable success of large language models (LLMs) stems from their\nability to consolidate vast amounts of knowledge into the memory during\npre-training and to retrieve it from the memory during inference, enabling\nadvanced capabilities such as knowledge memorization, instruction-following and\nreasoning. However, the mechanisms of memory retrieval and consolidation in\nLLMs remain poorly understood. In this paper, we propose the function token\nhypothesis to explain the workings of LLMs: During inference, function tokens\nactivate the most predictive features from context and govern next token\nprediction (memory retrieval). During pre-training, predicting the next tokens\n(usually content tokens) that follow function tokens increases the number of\nlearned features of LLMs and updates the model parameters (memory\nconsolidation). Function tokens here roughly correspond to function words in\nlinguistics, including punctuation marks, articles, prepositions, and\nconjunctions, in contrast to content tokens. We provide extensive experimental\nevidence supporting this hypothesis. Using bipartite graph analysis, we show\nthat a small number of function tokens activate the majority of features. Case\nstudies further reveal how function tokens activate the most predictive\nfeatures from context to direct next token prediction. We also find that during\npre-training, the training loss is dominated by predicting the next content\ntokens following function tokens, which forces the function tokens to select\nthe most predictive features from context.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08203.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60ea81771cc8dc259c58e905",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60ea81771cc8dc259c58e905/kmGlaNvdS4EEHc_5qongT.jpeg",
            "fullname": "yichen he",
            "name": "hyc2026",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08008",
            "authors": [
                {
                    "_id": "68e86f1b95e8e6771df38945",
                    "user": {
                        "_id": "63203d4e260e691cfc19fcb1",
                        "avatarUrl": "/avatars/72437259c73cc4a950a2e84141097310.svg",
                        "isPro": false,
                        "fullname": "Ruizhe Wang",
                        "user": "Mr-Philo",
                        "type": "user"
                    },
                    "name": "Ruizhe Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:36.152Z",
                    "hidden": false
                },
                {
                    "_id": "68e86f1b95e8e6771df38946",
                    "name": "Yucheng Ding",
                    "hidden": false
                },
                {
                    "_id": "68e86f1b95e8e6771df38947",
                    "user": {
                        "_id": "63fb6e281b4b1bd4e7ffc5be",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fb6e281b4b1bd4e7ffc5be/aiRu_bulgnxvEMrjipGoQ.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Liu",
                        "user": "lx865712528",
                        "type": "user"
                    },
                    "name": "Xiao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T09:12:18.682Z",
                    "hidden": false
                },
                {
                    "_id": "68e86f1b95e8e6771df38948",
                    "name": "Yaoxiang Wang",
                    "hidden": false
                },
                {
                    "_id": "68e86f1b95e8e6771df38949",
                    "name": "Peng Cheng",
                    "hidden": false
                },
                {
                    "_id": "68e86f1b95e8e6771df3894a",
                    "name": "Baining Guo",
                    "hidden": false
                },
                {
                    "_id": "68e86f1b95e8e6771df3894b",
                    "name": "Zhengjun Zha",
                    "hidden": false
                },
                {
                    "_id": "68e86f1b95e8e6771df3894c",
                    "name": "Yeyun Gong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T09:45:45.000Z",
            "submittedOnDailyAt": "2025-10-10T01:13:03.470Z",
            "title": "Recycling Pretrained Checkpoints: Orthogonal Growth of\n  Mixture-of-Experts for Efficient Large Language Model Pre-Training",
            "submittedOnDailyBy": {
                "_id": "63203d4e260e691cfc19fcb1",
                "avatarUrl": "/avatars/72437259c73cc4a950a2e84141097310.svg",
                "isPro": false,
                "fullname": "Ruizhe Wang",
                "user": "Mr-Philo",
                "type": "user"
            },
            "summary": "The rapidly increasing computational cost of pretraining Large Language\nModels necessitates more efficient approaches. Numerous computational costs\nhave been invested in existing well-trained checkpoints, but many of them\nremain underutilized due to engineering constraints or limited model capacity.\nTo efficiently reuse this \"sunk\" cost, we propose to recycle pretrained\ncheckpoints by expanding their parameter counts and continuing training. We\npropose orthogonal growth method well-suited for converged Mixture-of-Experts\nmodel: interpositional layer copying for depth growth and expert duplication\nwith injected noise for width growth. To determine the optimal timing for such\ngrowth across checkpoints sequences, we perform comprehensive scaling\nexperiments revealing that the final accuracy has a strong positive correlation\nwith the amount of sunk cost, indicating that greater prior investment leads to\nbetter performance. We scale our approach to models with 70B parameters and\nover 1T training tokens, achieving 10.66% accuracy gain over training from\nscratch under the same additional compute budget. Our checkpoint recycling\napproach establishes a foundation for economically efficient large language\nmodel pretraining.",
            "upvotes": 5,
            "discussionId": "68e86f1b95e8e6771df3894d",
            "ai_summary": "Recycling pretrained checkpoints through orthogonal growth methods improves large language model performance with reduced computational cost.",
            "ai_keywords": [
                "Large Language Models",
                "pretrained checkpoints",
                "orthogonal growth method",
                "interpositional layer copying",
                "expert duplication",
                "injected noise",
                "scaling experiments",
                "Mixture-of-Experts model"
            ]
        },
        "publishedAt": "2025-10-09T05:45:45.000Z",
        "title": "Recycling Pretrained Checkpoints: Orthogonal Growth of\n  Mixture-of-Experts for Efficient Large Language Model Pre-Training",
        "summary": "The rapidly increasing computational cost of pretraining Large Language\nModels necessitates more efficient approaches. Numerous computational costs\nhave been invested in existing well-trained checkpoints, but many of them\nremain underutilized due to engineering constraints or limited model capacity.\nTo efficiently reuse this \"sunk\" cost, we propose to recycle pretrained\ncheckpoints by expanding their parameter counts and continuing training. We\npropose orthogonal growth method well-suited for converged Mixture-of-Experts\nmodel: interpositional layer copying for depth growth and expert duplication\nwith injected noise for width growth. To determine the optimal timing for such\ngrowth across checkpoints sequences, we perform comprehensive scaling\nexperiments revealing that the final accuracy has a strong positive correlation\nwith the amount of sunk cost, indicating that greater prior investment leads to\nbetter performance. We scale our approach to models with 70B parameters and\nover 1T training tokens, achieving 10.66% accuracy gain over training from\nscratch under the same additional compute budget. Our checkpoint recycling\napproach establishes a foundation for economically efficient large language\nmodel pretraining.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08008.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63203d4e260e691cfc19fcb1",
            "avatarUrl": "/avatars/72437259c73cc4a950a2e84141097310.svg",
            "fullname": "Ruizhe Wang",
            "name": "Mr-Philo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08556",
            "authors": [
                {
                    "_id": "68e8740595e8e6771df389ec",
                    "name": "Xueyi Liu",
                    "hidden": false
                },
                {
                    "_id": "68e8740595e8e6771df389ed",
                    "name": "He Wang",
                    "hidden": false
                },
                {
                    "_id": "68e8740595e8e6771df389ee",
                    "name": "Li Yi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65b8070ad49f4330ab0ca5f7/xP3xguYnQ9ZllmF90PWu0.mp4"
            ],
            "publishedAt": "2025-10-09T17:59:11.000Z",
            "submittedOnDailyAt": "2025-10-10T01:39:23.673Z",
            "title": "DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via\n  Joint-Wise Neural Dynamics Model",
            "submittedOnDailyBy": {
                "_id": "65b8070ad49f4330ab0ca5f7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t4fI-3djMfgXCchU_xpjL.png",
                "isPro": false,
                "fullname": "Xueyi Liu",
                "user": "xymeow7",
                "type": "user"
            },
            "summary": "Achieving generalized in-hand object rotation remains a significant challenge\nin robotics, largely due to the difficulty of transferring policies from\nsimulation to the real world. The complex, contact-rich dynamics of dexterous\nmanipulation create a \"reality gap\" that has limited prior work to constrained\nscenarios involving simple geometries, limited object sizes and aspect ratios,\nconstrained wrist poses, or customized hands. We address this sim-to-real\nchallenge with a novel framework that enables a single policy, trained in\nsimulation, to generalize to a wide variety of objects and conditions in the\nreal world. The core of our method is a joint-wise dynamics model that learns\nto bridge the reality gap by effectively fitting limited amount of real-world\ncollected data and then adapting the sim policy's actions accordingly. The\nmodel is highly data-efficient and generalizable across different whole-hand\ninteraction distributions by factorizing dynamics across joints, compressing\nsystem-wide influences into low-dimensional variables, and learning each\njoint's evolution from its own dynamic profile, implicitly capturing these net\neffects. We pair this with a fully autonomous data collection strategy that\ngathers diverse, real-world interaction data with minimal human intervention.\nOur complete pipeline demonstrates unprecedented generality: a single policy\nsuccessfully rotates challenging objects with complex shapes (e.g., animals),\nhigh aspect ratios (up to 5.33), and small sizes, all while handling diverse\nwrist orientations and rotation axes. Comprehensive real-world evaluations and\na teleoperation application for complex tasks validate the effectiveness and\nrobustness of our approach. Website: https://meowuu7.github.io/DexNDM/",
            "upvotes": 3,
            "discussionId": "68e8740595e8e6771df389ef",
            "ai_summary": "A novel framework enables a single simulation-trained policy to generalize to diverse real-world object rotations by learning joint-wise dynamics and autonomously collecting data.",
            "ai_keywords": [
                "joint-wise dynamics model",
                "reality gap",
                "data-efficient",
                "generalizable",
                "dynamic profile",
                "autonomous data collection",
                "teleoperation application"
            ]
        },
        "publishedAt": "2025-10-09T13:59:11.000Z",
        "title": "DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via\n  Joint-Wise Neural Dynamics Model",
        "summary": "Achieving generalized in-hand object rotation remains a significant challenge\nin robotics, largely due to the difficulty of transferring policies from\nsimulation to the real world. The complex, contact-rich dynamics of dexterous\nmanipulation create a \"reality gap\" that has limited prior work to constrained\nscenarios involving simple geometries, limited object sizes and aspect ratios,\nconstrained wrist poses, or customized hands. We address this sim-to-real\nchallenge with a novel framework that enables a single policy, trained in\nsimulation, to generalize to a wide variety of objects and conditions in the\nreal world. The core of our method is a joint-wise dynamics model that learns\nto bridge the reality gap by effectively fitting limited amount of real-world\ncollected data and then adapting the sim policy's actions accordingly. The\nmodel is highly data-efficient and generalizable across different whole-hand\ninteraction distributions by factorizing dynamics across joints, compressing\nsystem-wide influences into low-dimensional variables, and learning each\njoint's evolution from its own dynamic profile, implicitly capturing these net\neffects. We pair this with a fully autonomous data collection strategy that\ngathers diverse, real-world interaction data with minimal human intervention.\nOur complete pipeline demonstrates unprecedented generality: a single policy\nsuccessfully rotates challenging objects with complex shapes (e.g., animals),\nhigh aspect ratios (up to 5.33), and small sizes, all while handling diverse\nwrist orientations and rotation axes. Comprehensive real-world evaluations and\na teleoperation application for complex tasks validate the effectiveness and\nrobustness of our approach. Website: https://meowuu7.github.io/DexNDM/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65b8070ad49f4330ab0ca5f7/xP3xguYnQ9ZllmF90PWu0.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08556.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65b8070ad49f4330ab0ca5f7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t4fI-3djMfgXCchU_xpjL.png",
            "fullname": "Xueyi Liu",
            "name": "xymeow7",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07958",
            "authors": [
                {
                    "_id": "68e871e695e8e6771df3897b",
                    "name": "Fengji Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e871e695e8e6771df3897c",
                    "name": "Xinyao Niu",
                    "hidden": false
                },
                {
                    "_id": "68e871e695e8e6771df3897d",
                    "name": "Chengyang Ying",
                    "hidden": false
                },
                {
                    "_id": "68e871e695e8e6771df3897e",
                    "name": "Guancheng Lin",
                    "hidden": false
                },
                {
                    "_id": "68e871e695e8e6771df3897f",
                    "name": "Zhongkai Hao",
                    "hidden": false
                },
                {
                    "_id": "68e871e695e8e6771df38980",
                    "name": "Zhou Fan",
                    "hidden": false
                },
                {
                    "_id": "68e871e695e8e6771df38981",
                    "name": "Chengen Huang",
                    "hidden": false
                },
                {
                    "_id": "68e871e695e8e6771df38982",
                    "name": "Jacky Keung",
                    "hidden": false
                },
                {
                    "_id": "68e871e695e8e6771df38983",
                    "name": "Bei Chen",
                    "hidden": false
                },
                {
                    "_id": "68e871e695e8e6771df38984",
                    "name": "Junyang Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T08:53:31.000Z",
            "submittedOnDailyAt": "2025-10-10T01:17:41.748Z",
            "title": "A^2Search: Ambiguity-Aware Question Answering with Reinforcement\n  Learning",
            "submittedOnDailyBy": {
                "_id": "64c8d4ac120a85440bfad55d",
                "avatarUrl": "/avatars/d65334288d69700250de51d0df344d9b.svg",
                "isPro": false,
                "fullname": "Fengji Zhang",
                "user": "zfj1998",
                "type": "user"
            },
            "summary": "Recent advances in Large Language Models (LLMs) and Reinforcement Learning\n(RL) have led to strong performance in open-domain question answering (QA).\nHowever, existing models still struggle with questions that admit multiple\nvalid answers. Standard QA benchmarks, which typically assume a single gold\nanswer, overlook this reality and thus produce inappropriate training signals.\nExisting attempts to handle ambiguity often rely on costly manual annotation,\nwhich is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue.\nIn this paper, we present A^2Search, an annotation-free, end-to-end training\nframework to recognize and handle ambiguity. At its core is an automated\npipeline that detects ambiguous questions and gathers alternative answers via\ntrajectory sampling and evidence verification. The model is then optimized with\nRL using a carefully designed AnsF1 reward, which naturally\naccommodates multiple answers. Experiments on eight open-domain QA benchmarks\ndemonstrate that A^2Search achieves new state-of-the-art performance. With\nonly a single rollout, A^2Search-7B yields an average AnsF1@1\nscore of 48.4% across four multi-hop benchmarks, outperforming all strong\nbaselines, including the substantially larger ReSearch-32B (46.2%).\nExtensive analyses further show that A^2Search resolves ambiguity and\ngeneralizes across benchmarks, highlighting that embracing ambiguity is\nessential for building more reliable QA systems. Our code, data, and model\nweights can be found at https://github.com/zfj1998/A2Search",
            "upvotes": 3,
            "discussionId": "68e871e795e8e6771df38985",
            "githubRepo": "https://github.com/zfj1998/A2Search",
            "ai_summary": "A$^2$Search is an annotation-free framework that handles ambiguity in open-domain QA by detecting ambiguous questions, gathering alternative answers, and optimizing with RL, achieving state-of-the-art performance across benchmarks.",
            "ai_keywords": [
                "Large Language Models",
                "Reinforcement Learning",
                "open-domain question answering",
                "ambiguity",
                "standard QA benchmarks",
                "trajectory sampling",
                "evidence verification",
                "AnsF1 reward",
                "multi-hop benchmarks",
                "ReSearch-32B"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "65ffb2a4d2a378a163d5c459",
                "name": "CityUniversityofHongKong",
                "fullname": "City University of Hong Kong",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65ffb1fca5242cffd5f83d60/fhD34yEelPToQ5RsLjeT0.png"
            }
        },
        "publishedAt": "2025-10-09T04:53:31.000Z",
        "title": "A^2Search: Ambiguity-Aware Question Answering with Reinforcement\n  Learning",
        "summary": "Recent advances in Large Language Models (LLMs) and Reinforcement Learning\n(RL) have led to strong performance in open-domain question answering (QA).\nHowever, existing models still struggle with questions that admit multiple\nvalid answers. Standard QA benchmarks, which typically assume a single gold\nanswer, overlook this reality and thus produce inappropriate training signals.\nExisting attempts to handle ambiguity often rely on costly manual annotation,\nwhich is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue.\nIn this paper, we present A^2Search, an annotation-free, end-to-end training\nframework to recognize and handle ambiguity. At its core is an automated\npipeline that detects ambiguous questions and gathers alternative answers via\ntrajectory sampling and evidence verification. The model is then optimized with\nRL using a carefully designed AnsF1 reward, which naturally\naccommodates multiple answers. Experiments on eight open-domain QA benchmarks\ndemonstrate that A^2Search achieves new state-of-the-art performance. With\nonly a single rollout, A^2Search-7B yields an average AnsF1@1\nscore of 48.4% across four multi-hop benchmarks, outperforming all strong\nbaselines, including the substantially larger ReSearch-32B (46.2%).\nExtensive analyses further show that A^2Search resolves ambiguity and\ngeneralizes across benchmarks, highlighting that embracing ambiguity is\nessential for building more reliable QA systems. Our code, data, and model\nweights can be found at https://github.com/zfj1998/A2Search",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07958.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64c8d4ac120a85440bfad55d",
            "avatarUrl": "/avatars/d65334288d69700250de51d0df344d9b.svg",
            "fullname": "Fengji Zhang",
            "name": "zfj1998",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "65ffb2a4d2a378a163d5c459",
            "name": "CityUniversityofHongKong",
            "fullname": "City University of Hong Kong",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65ffb1fca5242cffd5f83d60/fhD34yEelPToQ5RsLjeT0.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07790",
            "authors": [
                {
                    "_id": "68e8dadf95e8e6771df38c27",
                    "user": {
                        "_id": "64d0a05d2f1f9578a0405b9d",
                        "avatarUrl": "/avatars/30e69b345333e0bbe9149c8cf00c2fd9.svg",
                        "isPro": false,
                        "fullname": "Hao Wu",
                        "user": "Ach0",
                        "type": "user"
                    },
                    "name": "Hao Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T10:08:28.678Z",
                    "hidden": false
                },
                {
                    "_id": "68e8dadf95e8e6771df38c28",
                    "name": "Wei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T05:09:06.000Z",
            "submittedOnDailyAt": "2025-10-10T08:52:49.367Z",
            "title": "GCPO: When Contrast Fails, Go Gold",
            "submittedOnDailyBy": {
                "_id": "64d0a05d2f1f9578a0405b9d",
                "avatarUrl": "/avatars/30e69b345333e0bbe9149c8cf00c2fd9.svg",
                "isPro": false,
                "fullname": "Hao Wu",
                "user": "Ach0",
                "type": "user"
            },
            "summary": "Reinforcement learning has been widely applied to enhance the reasoning\ncapabilities of large language models. Extending the inference limits of\nsmaller models has become a prominent research focus. However, algorithms such\nas Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the\nupper bound of a model's rollout responses is entirely determined by the model\nitself, preventing the acquisition of knowledge from samples that are either\nall incorrect or all correct. In this paper, we introduce Group Contrastive\nPolicy Optimization (GCPO), a method that incorporates external standard\nreference answers. When the model cannot solve a problem, the reference answer\nsupplies the correct response, steering the model toward an unequivocally\naccurate update direction. This approach offers two main advantages: (1) it\nimproves training efficiency by fully utilizing every sample; (2) it enables\nthe model to emulate the problem solving strategy of the reference answer\nduring training, thereby enhancing generalization in reasoning. GCPO achieves\noutstanding results across multiple benchmark datasets, yielding substantial\nimprovements over the baseline model. Our code is available at:\nhttps://github.com/AchoWu/GCPO.",
            "upvotes": 3,
            "discussionId": "68e8dadf95e8e6771df38c29",
            "githubRepo": "https://github.com/AchoWu/GCPO",
            "ai_summary": "Group Contrastive Policy Optimization (GCPO) enhances reinforcement learning for large language models by incorporating external reference answers, improving training efficiency and generalization.",
            "ai_keywords": [
                "Group Relative Policy Optimization (GRPO)",
                "Group Contrastive Policy Optimization (GCPO)",
                "reinforcement learning",
                "large language models",
                "rollout responses",
                "reference answers",
                "training efficiency",
                "generalization"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-10-09T01:09:06.000Z",
        "title": "GCPO: When Contrast Fails, Go Gold",
        "summary": "Reinforcement learning has been widely applied to enhance the reasoning\ncapabilities of large language models. Extending the inference limits of\nsmaller models has become a prominent research focus. However, algorithms such\nas Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the\nupper bound of a model's rollout responses is entirely determined by the model\nitself, preventing the acquisition of knowledge from samples that are either\nall incorrect or all correct. In this paper, we introduce Group Contrastive\nPolicy Optimization (GCPO), a method that incorporates external standard\nreference answers. When the model cannot solve a problem, the reference answer\nsupplies the correct response, steering the model toward an unequivocally\naccurate update direction. This approach offers two main advantages: (1) it\nimproves training efficiency by fully utilizing every sample; (2) it enables\nthe model to emulate the problem solving strategy of the reference answer\nduring training, thereby enhancing generalization in reasoning. GCPO achieves\noutstanding results across multiple benchmark datasets, yielding substantial\nimprovements over the baseline model. Our code is available at:\nhttps://github.com/AchoWu/GCPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07790.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64d0a05d2f1f9578a0405b9d",
            "avatarUrl": "/avatars/30e69b345333e0bbe9149c8cf00c2fd9.svg",
            "fullname": "Hao Wu",
            "name": "Ach0",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.07743",
            "authors": [
                {
                    "_id": "68e9119d95e8e6771df38ca3",
                    "name": "Tianci Liu",
                    "hidden": false
                },
                {
                    "_id": "68e9119d95e8e6771df38ca4",
                    "name": "Ran Xu",
                    "hidden": false
                },
                {
                    "_id": "68e9119d95e8e6771df38ca5",
                    "name": "Tony Yu",
                    "hidden": false
                },
                {
                    "_id": "68e9119d95e8e6771df38ca6",
                    "name": "Ilgee Hong",
                    "hidden": false
                },
                {
                    "_id": "68e9119d95e8e6771df38ca7",
                    "name": "Carl Yang",
                    "hidden": false
                },
                {
                    "_id": "68e9119d95e8e6771df38ca8",
                    "name": "Tuo Zhao",
                    "hidden": false
                },
                {
                    "_id": "68e9119d95e8e6771df38ca9",
                    "name": "Haoyu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T03:31:26.000Z",
            "submittedOnDailyAt": "2025-10-10T20:01:19.078Z",
            "title": "OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward\n  Modeling and LLM Alignment",
            "submittedOnDailyBy": {
                "_id": "64bf811d76a6e2efcceabc00",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bf811d76a6e2efcceabc00/0p3zSIVqzoME25Zmfh7SD.png",
                "isPro": false,
                "fullname": "Tianci Liu",
                "user": "lliutianc",
                "type": "user"
            },
            "summary": "Reward modeling lies at the core of reinforcement learning from human\nfeedback (RLHF), yet most existing reward models rely on scalar or pairwise\njudgments that fail to capture the multifaceted nature of human preferences.\nRecent studies have explored rubrics-as-rewards (RaR) that uses structured\nnatural language criteria that capture multiple dimensions of response quality.\nHowever, producing rubrics that are both reliable and scalable remains a key\nchallenge. In this work, we introduce OpenRubrics, a diverse, large-scale\ncollection of (prompt, rubric) pairs for training rubric-generation and\nrubric-based reward models. To elicit discriminative and comprehensive\nevaluation signals, we introduce Contrastive Rubric Generation (CRG), which\nderives both hard rules (explicit constraints) and principles (implicit\nqualities) by contrasting preferred and rejected responses. We further improve\nreliability by enforcing preference-label consistency via rejection sampling to\nremove noisy rubrics. Across multiple reward-modeling benchmarks, our\nrubric-based reward model, Rubric-RM, surpasses strong size-matched baselines\nby 6.8%. These gains transfer to policy models on instruction-following and\nbiomedical benchmarks. Our results show that rubrics provide scalable alignment\nsignals that narrow the gap between costly human evaluation and automated\nreward modeling, enabling a new principle-driven paradigm for LLM alignment.",
            "upvotes": 3,
            "discussionId": "68e9119e95e8e6771df38caa",
            "ai_summary": "Rubric-based reward models using OpenRubrics and Contrastive Rubric Generation improve alignment in reinforcement learning from human feedback by providing scalable and reliable evaluation signals.",
            "ai_keywords": [
                "reinforcement learning from human feedback",
                "reward models",
                "rubrics-as-rewards",
                "structured natural language criteria",
                "Contrastive Rubric Generation",
                "hard rules",
                "principles",
                "preference-label consistency",
                "rejection sampling",
                "Rubric-RM",
                "instruction-following",
                "biomedical benchmarks",
                "LLM alignment"
            ]
        },
        "publishedAt": "2025-10-08T23:31:26.000Z",
        "title": "OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward\n  Modeling and LLM Alignment",
        "summary": "Reward modeling lies at the core of reinforcement learning from human\nfeedback (RLHF), yet most existing reward models rely on scalar or pairwise\njudgments that fail to capture the multifaceted nature of human preferences.\nRecent studies have explored rubrics-as-rewards (RaR) that uses structured\nnatural language criteria that capture multiple dimensions of response quality.\nHowever, producing rubrics that are both reliable and scalable remains a key\nchallenge. In this work, we introduce OpenRubrics, a diverse, large-scale\ncollection of (prompt, rubric) pairs for training rubric-generation and\nrubric-based reward models. To elicit discriminative and comprehensive\nevaluation signals, we introduce Contrastive Rubric Generation (CRG), which\nderives both hard rules (explicit constraints) and principles (implicit\nqualities) by contrasting preferred and rejected responses. We further improve\nreliability by enforcing preference-label consistency via rejection sampling to\nremove noisy rubrics. Across multiple reward-modeling benchmarks, our\nrubric-based reward model, Rubric-RM, surpasses strong size-matched baselines\nby 6.8%. These gains transfer to policy models on instruction-following and\nbiomedical benchmarks. Our results show that rubrics provide scalable alignment\nsignals that narrow the gap between costly human evaluation and automated\nreward modeling, enabling a new principle-driven paradigm for LLM alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07743.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64bf811d76a6e2efcceabc00",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bf811d76a6e2efcceabc00/0p3zSIVqzoME25Zmfh7SD.png",
            "fullname": "Tianci Liu",
            "name": "lliutianc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07429",
            "authors": [
                {
                    "_id": "68e86c0e95e8e6771df38902",
                    "name": "Wang Wei",
                    "hidden": false
                },
                {
                    "_id": "68e86c0e95e8e6771df38903",
                    "name": "Tiankai Yang",
                    "hidden": false
                },
                {
                    "_id": "68e86c0e95e8e6771df38904",
                    "name": "Hongjie Chen",
                    "hidden": false
                },
                {
                    "_id": "68e86c0e95e8e6771df38905",
                    "name": "Yue Zhao",
                    "hidden": false
                },
                {
                    "_id": "68e86c0e95e8e6771df38906",
                    "name": "Franck Dernoncourt",
                    "hidden": false
                },
                {
                    "_id": "68e86c0e95e8e6771df38907",
                    "name": "Ryan A. Rossi",
                    "hidden": false
                },
                {
                    "_id": "68e86c0e95e8e6771df38908",
                    "name": "Hoda Eldardiry",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T18:24:59.000Z",
            "submittedOnDailyAt": "2025-10-10T00:45:08.621Z",
            "title": "Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs",
            "submittedOnDailyBy": {
                "_id": "62c5947524171688a9feb992",
                "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                "isPro": false,
                "fullname": "Franck Dernoncourt",
                "user": "Franck-Dernoncourt",
                "type": "user"
            },
            "summary": "Efficient use of large language models (LLMs) is critical for deployment at\nscale: without adaptive routing, systems either overpay for strong models or\nrisk poor performance from weaker ones. Selecting the right LLM for each query\nis fundamentally an online decision problem: models differ in strengths, prices\nfluctuate, and users value accuracy and cost differently. Yet most routers are\ntrained offline with labels for all candidate models, an assumption that breaks\nin deployment, where only the outcome of the chosen model is observed. We\nbridge this gap with BaRP, a Bandit-feedback Routing with Preferences approach\nthat trains under the same partial-feedback restriction as deployment, while\nsupporting preference-tunable inference: operators can dial the\nperformance/cost trade-off at test time without retraining. Framed as a\ncontextual bandit over prompt features and a user preference vector, our method\nsimulates an online feedback setting during training and adapts its routing\ndecisions to each new prompt, rather than depending on full-information offline\nsupervision. Comprehensive experiments show that our method consistently\noutperforms strong offline routers by at least 12.46% and the largest LLM by at\nleast 2.45%, and generalizes robustly for unseen tasks.",
            "upvotes": 3,
            "discussionId": "68e86c0e95e8e6771df38909",
            "ai_summary": "BaRP, a Bandit-feedback Routing with Preferences approach, optimizes large language model selection in an online setting with partial feedback, outperforming offline routers and large models.",
            "ai_keywords": [
                "large language models",
                "adaptive routing",
                "online decision problem",
                "partial-feedback restriction",
                "contextual bandit",
                "prompt features",
                "user preference vector",
                "performance/cost trade-off"
            ]
        },
        "publishedAt": "2025-10-08T14:24:59.000Z",
        "title": "Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs",
        "summary": "Efficient use of large language models (LLMs) is critical for deployment at\nscale: without adaptive routing, systems either overpay for strong models or\nrisk poor performance from weaker ones. Selecting the right LLM for each query\nis fundamentally an online decision problem: models differ in strengths, prices\nfluctuate, and users value accuracy and cost differently. Yet most routers are\ntrained offline with labels for all candidate models, an assumption that breaks\nin deployment, where only the outcome of the chosen model is observed. We\nbridge this gap with BaRP, a Bandit-feedback Routing with Preferences approach\nthat trains under the same partial-feedback restriction as deployment, while\nsupporting preference-tunable inference: operators can dial the\nperformance/cost trade-off at test time without retraining. Framed as a\ncontextual bandit over prompt features and a user preference vector, our method\nsimulates an online feedback setting during training and adapts its routing\ndecisions to each new prompt, rather than depending on full-information offline\nsupervision. Comprehensive experiments show that our method consistently\noutperforms strong offline routers by at least 12.46% and the largest LLM by at\nleast 2.45%, and generalizes robustly for unseen tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07429.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.06679",
            "authors": [
                {
                    "_id": "68e949cbb12ca14cb8a1660e",
                    "name": "Bin Xia",
                    "hidden": false
                },
                {
                    "_id": "68e949cbb12ca14cb8a1660f",
                    "name": "Bohao Peng",
                    "hidden": false
                },
                {
                    "_id": "68e949cbb12ca14cb8a16610",
                    "name": "Yuechen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e949cbb12ca14cb8a16611",
                    "name": "Junjia Huang",
                    "hidden": false
                },
                {
                    "_id": "68e949cbb12ca14cb8a16612",
                    "name": "Jiyang Liu",
                    "hidden": false
                },
                {
                    "_id": "68e949cbb12ca14cb8a16613",
                    "name": "Jingyao Li",
                    "hidden": false
                },
                {
                    "_id": "68e949cbb12ca14cb8a16614",
                    "name": "Haoru Tan",
                    "hidden": false
                },
                {
                    "_id": "68e949cbb12ca14cb8a16615",
                    "name": "Sitong Wu",
                    "hidden": false
                },
                {
                    "_id": "68e949cbb12ca14cb8a16616",
                    "name": "Chengyao Wang",
                    "hidden": false
                },
                {
                    "_id": "68e949cbb12ca14cb8a16617",
                    "name": "Yitong Wang",
                    "hidden": false
                },
                {
                    "_id": "68e949cbb12ca14cb8a16618",
                    "name": "Xinglong Wu",
                    "hidden": false
                },
                {
                    "_id": "68e949cbb12ca14cb8a16619",
                    "name": "Bei Yu",
                    "hidden": false
                },
                {
                    "_id": "68e949cbb12ca14cb8a1661a",
                    "name": "Jiaya Jia",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T06:07:14.000Z",
            "submittedOnDailyAt": "2025-10-10T16:40:50.654Z",
            "title": "DreamOmni2: Multimodal Instruction-based Editing and Generation",
            "submittedOnDailyBy": {
                "_id": "64e42a700ecc1ecca77b1db9",
                "avatarUrl": "/avatars/c1228db09b88c9246aab48da7ae82f7c.svg",
                "isPro": false,
                "fullname": "binxia",
                "user": "binxia",
                "type": "user"
            },
            "summary": "Recent advancements in instruction-based image editing and subject-driven\ngeneration have garnered significant attention, yet both tasks still face\nlimitations in meeting practical user needs. Instruction-based editing relies\nsolely on language instructions, which often fail to capture specific editing\ndetails, making reference images necessary. Meanwhile, subject-driven\ngeneration is limited to combining concrete objects or people, overlooking\nbroader, abstract concepts. To address these challenges, we propose two novel\ntasks: multimodal instruction-based editing and generation. These tasks support\nboth text and image instructions and extend the scope to include both concrete\nand abstract concepts, greatly enhancing their practical applications. We\nintroduce DreamOmni2, tackling two primary challenges: data creation and model\nframework design. Our data synthesis pipeline consists of three steps: (1)\nusing a feature mixing method to create extraction data for both abstract and\nconcrete concepts, (2) generating multimodal instruction-based editing training\ndata using the editing and extraction models, and (3) further applying the\nextraction model to create training data for multimodal instruction-based\nediting. For the framework, to handle multi-image input, we propose an index\nencoding and position encoding shift scheme, which helps the model distinguish\nimages and avoid pixel confusion. Additionally, we introduce joint training\nwith the VLM and our generation/editing model to better process complex\ninstructions. In addition, we have proposed comprehensive benchmarks for these\ntwo new tasks to drive their development. Experiments show that DreamOmni2 has\nachieved impressive results. Models and codes will be released.",
            "upvotes": 3,
            "discussionId": "68e949ccb12ca14cb8a1661b",
            "ai_summary": "DreamOmni2 addresses limitations in instruction-based image editing and subject-driven generation by introducing multimodal instruction-based editing and generation tasks, utilizing feature mixing, index encoding, and joint training with a VLM.",
            "ai_keywords": [
                "instruction-based editing",
                "subject-driven generation",
                "multimodal instruction-based editing",
                "feature mixing",
                "index encoding",
                "position encoding",
                "joint training",
                "VLM"
            ]
        },
        "publishedAt": "2025-10-08T02:07:14.000Z",
        "title": "DreamOmni2: Multimodal Instruction-based Editing and Generation",
        "summary": "Recent advancements in instruction-based image editing and subject-driven\ngeneration have garnered significant attention, yet both tasks still face\nlimitations in meeting practical user needs. Instruction-based editing relies\nsolely on language instructions, which often fail to capture specific editing\ndetails, making reference images necessary. Meanwhile, subject-driven\ngeneration is limited to combining concrete objects or people, overlooking\nbroader, abstract concepts. To address these challenges, we propose two novel\ntasks: multimodal instruction-based editing and generation. These tasks support\nboth text and image instructions and extend the scope to include both concrete\nand abstract concepts, greatly enhancing their practical applications. We\nintroduce DreamOmni2, tackling two primary challenges: data creation and model\nframework design. Our data synthesis pipeline consists of three steps: (1)\nusing a feature mixing method to create extraction data for both abstract and\nconcrete concepts, (2) generating multimodal instruction-based editing training\ndata using the editing and extraction models, and (3) further applying the\nextraction model to create training data for multimodal instruction-based\nediting. For the framework, to handle multi-image input, we propose an index\nencoding and position encoding shift scheme, which helps the model distinguish\nimages and avoid pixel confusion. Additionally, we introduce joint training\nwith the VLM and our generation/editing model to better process complex\ninstructions. In addition, we have proposed comprehensive benchmarks for these\ntwo new tasks to drive their development. Experiments show that DreamOmni2 has\nachieved impressive results. Models and codes will be released.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06679.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64e42a700ecc1ecca77b1db9",
            "avatarUrl": "/avatars/c1228db09b88c9246aab48da7ae82f7c.svg",
            "fullname": "binxia",
            "name": "binxia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.24817",
            "authors": [
                {
                    "_id": "68e6e6437ae125f9582e682c",
                    "user": {
                        "_id": "64b855c952b7353d8c5c6a35",
                        "avatarUrl": "/avatars/e6cb373da9721c4772a195ab731703d2.svg",
                        "isPro": false,
                        "fullname": "Zeyu Cai",
                        "user": "Co2y",
                        "type": "user"
                    },
                    "name": "Zeyu Cai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T09:37:50.829Z",
                    "hidden": false
                },
                {
                    "_id": "68e6e6437ae125f9582e682d",
                    "name": "Ziyang Li",
                    "hidden": false
                },
                {
                    "_id": "68e6e6437ae125f9582e682e",
                    "name": "Xiaoben Li",
                    "hidden": false
                },
                {
                    "_id": "68e6e6437ae125f9582e682f",
                    "name": "Boqian Li",
                    "hidden": false
                },
                {
                    "_id": "68e6e6437ae125f9582e6830",
                    "name": "Zeyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68e6e6437ae125f9582e6831",
                    "name": "Zhenyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e6e6437ae125f9582e6832",
                    "user": {
                        "_id": "6183d0b249ef1d984699e4a3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6183d0b249ef1d984699e4a3/M6B9b73v8su9H8jfdcbgu.jpeg",
                        "isPro": false,
                        "fullname": "Yuliang Xiu",
                        "user": "Yuliang",
                        "type": "user"
                    },
                    "name": "Yuliang Xiu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:50:53.281Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64b855c952b7353d8c5c6a35/KNq7Co7QV4BeqydnVtWv8.mp4"
            ],
            "publishedAt": "2025-09-29T14:06:00.000Z",
            "submittedOnDailyAt": "2025-10-10T04:51:22.656Z",
            "title": "UP2You: Fast Reconstruction of Yourself from Unconstrained Photo\n  Collections",
            "submittedOnDailyBy": {
                "_id": "64b855c952b7353d8c5c6a35",
                "avatarUrl": "/avatars/e6cb373da9721c4772a195ab731703d2.svg",
                "isPro": false,
                "fullname": "Zeyu Cai",
                "user": "Co2y",
                "type": "user"
            },
            "summary": "We present UP2You, the first tuning-free solution for reconstructing\nhigh-fidelity 3D clothed portraits from extremely unconstrained in-the-wild 2D\nphotos. Unlike previous approaches that require \"clean\" inputs (e.g., full-body\nimages with minimal occlusions, or well-calibrated cross-view captures), UP2You\ndirectly processes raw, unstructured photographs, which may vary significantly\nin pose, viewpoint, cropping, and occlusion. Instead of compressing data into\ntokens for slow online text-to-3D optimization, we introduce a data rectifier\nparadigm that efficiently converts unconstrained inputs into clean, orthogonal\nmulti-view images in a single forward pass within seconds, simplifying the 3D\nreconstruction. Central to UP2You is a pose-correlated feature aggregation\nmodule (PCFA), that selectively fuses information from multiple reference\nimages w.r.t. target poses, enabling better identity preservation and nearly\nconstant memory footprint, with more observations. We also introduce a\nperceiver-based multi-reference shape predictor, removing the need for\npre-captured body templates. Extensive experiments on 4D-Dress, PuzzleIOI, and\nin-the-wild captures demonstrate that UP2You consistently surpasses previous\nmethods in both geometric accuracy (Chamfer-15%, P2S-18% on PuzzleIOI) and\ntexture fidelity (PSNR-21%, LPIPS-46% on 4D-Dress). UP2You is efficient (1.5\nminutes per person), and versatile (supports arbitrary pose control, and\ntraining-free multi-garment 3D virtual try-on), making it practical for\nreal-world scenarios where humans are casually captured. Both models and code\nwill be released to facilitate future research on this underexplored task.\nProject Page: https://zcai0612.github.io/UP2You",
            "upvotes": 3,
            "discussionId": "68e6e6447ae125f9582e6833",
            "projectPage": "https://zcai0612.github.io/UP2You/",
            "githubRepo": "https://github.com/zcai0612/UP2You",
            "ai_summary": "UP2You reconstructs high-fidelity 3D clothed portraits from unconstrained 2D photos using a data rectifier and pose-correlated feature aggregation, achieving superior geometric and texture accuracy.",
            "ai_keywords": [
                "data rectifier",
                "pose-correlated feature aggregation",
                "perceiver-based multi-reference shape predictor",
                "Chamfer",
                "P2S",
                "PSNR",
                "LPIPS",
                "3D reconstruction",
                "3D virtual try-on"
            ],
            "githubStars": 41
        },
        "publishedAt": "2025-09-29T10:06:00.000Z",
        "title": "UP2You: Fast Reconstruction of Yourself from Unconstrained Photo\n  Collections",
        "summary": "We present UP2You, the first tuning-free solution for reconstructing\nhigh-fidelity 3D clothed portraits from extremely unconstrained in-the-wild 2D\nphotos. Unlike previous approaches that require \"clean\" inputs (e.g., full-body\nimages with minimal occlusions, or well-calibrated cross-view captures), UP2You\ndirectly processes raw, unstructured photographs, which may vary significantly\nin pose, viewpoint, cropping, and occlusion. Instead of compressing data into\ntokens for slow online text-to-3D optimization, we introduce a data rectifier\nparadigm that efficiently converts unconstrained inputs into clean, orthogonal\nmulti-view images in a single forward pass within seconds, simplifying the 3D\nreconstruction. Central to UP2You is a pose-correlated feature aggregation\nmodule (PCFA), that selectively fuses information from multiple reference\nimages w.r.t. target poses, enabling better identity preservation and nearly\nconstant memory footprint, with more observations. We also introduce a\nperceiver-based multi-reference shape predictor, removing the need for\npre-captured body templates. Extensive experiments on 4D-Dress, PuzzleIOI, and\nin-the-wild captures demonstrate that UP2You consistently surpasses previous\nmethods in both geometric accuracy (Chamfer-15%, P2S-18% on PuzzleIOI) and\ntexture fidelity (PSNR-21%, LPIPS-46% on 4D-Dress). UP2You is efficient (1.5\nminutes per person), and versatile (supports arbitrary pose control, and\ntraining-free multi-garment 3D virtual try-on), making it practical for\nreal-world scenarios where humans are casually captured. Both models and code\nwill be released to facilitate future research on this underexplored task.\nProject Page: https://zcai0612.github.io/UP2You",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64b855c952b7353d8c5c6a35/KNq7Co7QV4BeqydnVtWv8.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24817.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64b855c952b7353d8c5c6a35",
            "avatarUrl": "/avatars/e6cb373da9721c4772a195ab731703d2.svg",
            "fullname": "Zeyu Cai",
            "name": "Co2y",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.06209",
            "authors": [
                {
                    "_id": "68e91c0095e8e6771df38cb6",
                    "name": "Jiahao Wang",
                    "hidden": false
                },
                {
                    "_id": "68e91c0095e8e6771df38cb7",
                    "name": "Zhenpei Yang",
                    "hidden": false
                },
                {
                    "_id": "68e91c0095e8e6771df38cb8",
                    "name": "Yijing Bai",
                    "hidden": false
                },
                {
                    "_id": "68e91c0095e8e6771df38cb9",
                    "name": "Yingwei Li",
                    "hidden": false
                },
                {
                    "_id": "68e91c0095e8e6771df38cba",
                    "name": "Yuliang Zou",
                    "hidden": false
                },
                {
                    "_id": "68e91c0095e8e6771df38cbb",
                    "name": "Bo Sun",
                    "hidden": false
                },
                {
                    "_id": "68e91c0095e8e6771df38cbc",
                    "name": "Abhijit Kundu",
                    "hidden": false
                },
                {
                    "_id": "68e91c0095e8e6771df38cbd",
                    "name": "Jose Lezama",
                    "hidden": false
                },
                {
                    "_id": "68e91c0095e8e6771df38cbe",
                    "name": "Luna Yue Huang",
                    "hidden": false
                },
                {
                    "_id": "68e91c0095e8e6771df38cbf",
                    "name": "Zehao Zhu",
                    "hidden": false
                },
                {
                    "_id": "68e91c0095e8e6771df38cc0",
                    "name": "Jyh-Jing Hwang",
                    "hidden": false
                },
                {
                    "_id": "68e91c0095e8e6771df38cc1",
                    "name": "Dragomir Anguelov",
                    "hidden": false
                },
                {
                    "_id": "68e91c0095e8e6771df38cc2",
                    "name": "Mingxing Tan",
                    "hidden": false
                },
                {
                    "_id": "68e91c0095e8e6771df38cc3",
                    "name": "Chiyu Max Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-07T17:58:32.000Z",
            "submittedOnDailyAt": "2025-10-10T13:18:06.360Z",
            "title": "Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models",
            "submittedOnDailyBy": {
                "_id": "64cbf523e3cc4a476d8291b6",
                "avatarUrl": "/avatars/825d7665db471e46921abad3319c2846.svg",
                "isPro": false,
                "fullname": "Jiahao Wang",
                "user": "jiahaoplus",
                "type": "user"
            },
            "summary": "Recent advances in generative models have sparked exciting new possibilities\nin the field of autonomous vehicles. Specifically, video generation models are\nnow being explored as controllable virtual testing environments.\nSimultaneously, end-to-end (E2E) driving models have emerged as a streamlined\nalternative to conventional modular autonomous driving systems, gaining\npopularity for their simplicity and scalability. However, the application of\nthese techniques to simulation and planning raises important questions. First,\nwhile video generation models can generate increasingly realistic videos, can\nthese videos faithfully adhere to the specified conditions and be realistic\nenough for E2E autonomous planner evaluation? Second, given that data is\ncrucial for understanding and controlling E2E planners, how can we gain deeper\ninsights into their biases and improve their ability to generalize to\nout-of-distribution scenarios? In this work, we bridge the gap between the\ndriving models and generative world models (Drive&Gen) to address these\nquestions. We propose novel statistical measures leveraging E2E drivers to\nevaluate the realism of generated videos. By exploiting the controllability of\nthe video generation model, we conduct targeted experiments to investigate\ndistribution gaps affecting E2E planner performance. Finally, we show that\nsynthetic data produced by the video generation model offers a cost-effective\nalternative to real-world data collection. This synthetic data effectively\nimproves E2E model generalization beyond existing Operational Design Domains,\nfacilitating the expansion of autonomous vehicle services into new operational\ncontexts.",
            "upvotes": 2,
            "discussionId": "68e91c0095e8e6771df38cc4",
            "ai_summary": "A novel approach combining driving models and generative world models evaluates and enhances the realism and generalization of synthetic video data for autonomous vehicle testing and planning.",
            "ai_keywords": [
                "generative models",
                "autonomous vehicles",
                "video generation models",
                "end-to-end driving models",
                "statistical measures",
                "E2E planners",
                "distribution gaps",
                "synthetic data",
                "Operational Design Domains"
            ]
        },
        "publishedAt": "2025-10-07T13:58:32.000Z",
        "title": "Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models",
        "summary": "Recent advances in generative models have sparked exciting new possibilities\nin the field of autonomous vehicles. Specifically, video generation models are\nnow being explored as controllable virtual testing environments.\nSimultaneously, end-to-end (E2E) driving models have emerged as a streamlined\nalternative to conventional modular autonomous driving systems, gaining\npopularity for their simplicity and scalability. However, the application of\nthese techniques to simulation and planning raises important questions. First,\nwhile video generation models can generate increasingly realistic videos, can\nthese videos faithfully adhere to the specified conditions and be realistic\nenough for E2E autonomous planner evaluation? Second, given that data is\ncrucial for understanding and controlling E2E planners, how can we gain deeper\ninsights into their biases and improve their ability to generalize to\nout-of-distribution scenarios? In this work, we bridge the gap between the\ndriving models and generative world models (Drive&Gen) to address these\nquestions. We propose novel statistical measures leveraging E2E drivers to\nevaluate the realism of generated videos. By exploiting the controllability of\nthe video generation model, we conduct targeted experiments to investigate\ndistribution gaps affecting E2E planner performance. Finally, we show that\nsynthetic data produced by the video generation model offers a cost-effective\nalternative to real-world data collection. This synthetic data effectively\nimproves E2E model generalization beyond existing Operational Design Domains,\nfacilitating the expansion of autonomous vehicle services into new operational\ncontexts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06209.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64cbf523e3cc4a476d8291b6",
            "avatarUrl": "/avatars/825d7665db471e46921abad3319c2846.svg",
            "fullname": "Jiahao Wang",
            "name": "jiahaoplus",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08547",
            "authors": [
                {
                    "_id": "68e868a495e8e6771df388c9",
                    "name": "Xiuwei Xu",
                    "hidden": false
                },
                {
                    "_id": "68e868a495e8e6771df388ca",
                    "name": "Angyuan Ma",
                    "hidden": false
                },
                {
                    "_id": "68e868a495e8e6771df388cb",
                    "name": "Hankun Li",
                    "hidden": false
                },
                {
                    "_id": "68e868a495e8e6771df388cc",
                    "name": "Bingyao Yu",
                    "hidden": false
                },
                {
                    "_id": "68e868a495e8e6771df388cd",
                    "name": "Zheng Zhu",
                    "hidden": false
                },
                {
                    "_id": "68e868a495e8e6771df388ce",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "68e868a495e8e6771df388cf",
                    "name": "Jiwen Lu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/648ac65fd044b25978015634/TXgQACXpRv93_3lWH_xTi.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/648ac65fd044b25978015634/ZYAoH5iCrWV5vaRGHPa3g.png",
                "https://cdn-uploads.huggingface.co/production/uploads/648ac65fd044b25978015634/uBguD4B5-gXY7CYx2Ob5F.png"
            ],
            "publishedAt": "2025-10-09T17:55:44.000Z",
            "submittedOnDailyAt": "2025-10-10T00:43:49.458Z",
            "title": "R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized\n  Manipulation",
            "submittedOnDailyBy": {
                "_id": "648ac65fd044b25978015634",
                "avatarUrl": "/avatars/2278a66fdc953220e9f8fc0ccce3ff00.svg",
                "isPro": false,
                "fullname": "Xiuwei Xu",
                "user": "xuxw98",
                "type": "user"
            },
            "summary": "Towards the aim of generalized robotic manipulation, spatial generalization\nis the most fundamental capability that requires the policy to work robustly\nunder different spatial distribution of objects, environment and agent itself.\nTo achieve this, substantial human demonstrations need to be collected to cover\ndifferent spatial configurations for training a generalized visuomotor policy\nvia imitation learning. Prior works explore a promising direction that\nleverages data generation to acquire abundant spatially diverse data from\nminimal source demonstrations. However, most approaches face significant\nsim-to-real gap and are often limited to constrained settings, such as\nfixed-base scenarios and predefined camera viewpoints. In this paper, we\npropose a real-to-real 3D data generation framework (R2RGen) that directly\naugments the pointcloud observation-action pairs to generate real-world data.\nR2RGen is simulator- and rendering-free, thus being efficient and\nplug-and-play. Specifically, given a single source demonstration, we introduce\nan annotation mechanism for fine-grained parsing of scene and trajectory. A\ngroup-wise augmentation strategy is proposed to handle complex multi-object\ncompositions and diverse task constraints. We further present camera-aware\nprocessing to align the distribution of generated data with real-world 3D\nsensor. Empirically, R2RGen substantially enhances data efficiency on extensive\nexperiments and demonstrates strong potential for scaling and application on\nmobile manipulation.",
            "upvotes": 1,
            "discussionId": "68e868a495e8e6771df388d0",
            "ai_summary": "A real-to-real 3D data generation framework enhances data efficiency for generalized robotic manipulation by augmenting pointcloud observations without simulation.",
            "ai_keywords": [
                "spatial generalization",
                "visuomotor policy",
                "imitation learning",
                "data generation",
                "pointcloud",
                "real-to-real",
                "annotation mechanism",
                "group-wise augmentation",
                "camera-aware processing",
                "mobile manipulation"
            ]
        },
        "publishedAt": "2025-10-09T13:55:44.000Z",
        "title": "R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized\n  Manipulation",
        "summary": "Towards the aim of generalized robotic manipulation, spatial generalization\nis the most fundamental capability that requires the policy to work robustly\nunder different spatial distribution of objects, environment and agent itself.\nTo achieve this, substantial human demonstrations need to be collected to cover\ndifferent spatial configurations for training a generalized visuomotor policy\nvia imitation learning. Prior works explore a promising direction that\nleverages data generation to acquire abundant spatially diverse data from\nminimal source demonstrations. However, most approaches face significant\nsim-to-real gap and are often limited to constrained settings, such as\nfixed-base scenarios and predefined camera viewpoints. In this paper, we\npropose a real-to-real 3D data generation framework (R2RGen) that directly\naugments the pointcloud observation-action pairs to generate real-world data.\nR2RGen is simulator- and rendering-free, thus being efficient and\nplug-and-play. Specifically, given a single source demonstration, we introduce\nan annotation mechanism for fine-grained parsing of scene and trajectory. A\ngroup-wise augmentation strategy is proposed to handle complex multi-object\ncompositions and diverse task constraints. We further present camera-aware\nprocessing to align the distribution of generated data with real-world 3D\nsensor. Empirically, R2RGen substantially enhances data efficiency on extensive\nexperiments and demonstrates strong potential for scaling and application on\nmobile manipulation.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/648ac65fd044b25978015634/TXgQACXpRv93_3lWH_xTi.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/648ac65fd044b25978015634/ZYAoH5iCrWV5vaRGHPa3g.png",
            "https://cdn-uploads.huggingface.co/production/uploads/648ac65fd044b25978015634/uBguD4B5-gXY7CYx2Ob5F.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08547.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648ac65fd044b25978015634",
            "avatarUrl": "/avatars/2278a66fdc953220e9f8fc0ccce3ff00.svg",
            "fullname": "Xiuwei Xu",
            "name": "xuxw98",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08271",
            "authors": [
                {
                    "_id": "68e8dd4d95e8e6771df38c30",
                    "name": "Andreas Engelhardt",
                    "hidden": false
                },
                {
                    "_id": "68e8dd4d95e8e6771df38c31",
                    "name": "Mark Boss",
                    "hidden": false
                },
                {
                    "_id": "68e8dd4d95e8e6771df38c32",
                    "name": "Vikram Voletti",
                    "hidden": false
                },
                {
                    "_id": "68e8dd4d95e8e6771df38c33",
                    "name": "Chun-Han Yao",
                    "hidden": false
                },
                {
                    "_id": "68e8dd4d95e8e6771df38c34",
                    "name": "Hendrik P. A. Lensch",
                    "hidden": false
                },
                {
                    "_id": "68e8dd4d95e8e6771df38c35",
                    "name": "Varun Jampani",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64ad09cd2d29d3bdcd482bbd/yMCs7QBk8U2uXXM_fovpD.jpeg"
            ],
            "publishedAt": "2025-10-09T14:29:47.000Z",
            "submittedOnDailyAt": "2025-10-10T09:00:21.945Z",
            "title": "SViM3D: Stable Video Material Diffusion for Single Image 3D Generation",
            "submittedOnDailyBy": {
                "_id": "64ad09cd2d29d3bdcd482bbd",
                "avatarUrl": "/avatars/657e02321bc4798bb31eab8b1b9e1c84.svg",
                "isPro": false,
                "fullname": "Andreas Engelhardt",
                "user": "andreasengelhardt",
                "type": "user"
            },
            "summary": "We present Stable Video Materials 3D (SViM3D), a framework to predict\nmulti-view consistent physically based rendering (PBR) materials, given a\nsingle image. Recently, video diffusion models have been successfully used to\nreconstruct 3D objects from a single image efficiently. However, reflectance is\nstill represented by simple material models or needs to be estimated in\nadditional steps to enable relighting and controlled appearance edits. We\nextend a latent video diffusion model to output spatially varying PBR\nparameters and surface normals jointly with each generated view based on\nexplicit camera control. This unique setup allows for relighting and generating\na 3D asset using our model as neural prior. We introduce various mechanisms to\nthis pipeline that improve quality in this ill-posed setting. We show\nstate-of-the-art relighting and novel view synthesis performance on multiple\nobject-centric datasets. Our method generalizes to diverse inputs, enabling the\ngeneration of relightable 3D assets useful in AR/VR, movies, games and other\nvisual media.",
            "upvotes": 1,
            "discussionId": "68e8dd4d95e8e6771df38c36",
            "ai_summary": "A latent video diffusion model predicts multi-view consistent PBR materials from a single image, enabling relighting and novel view synthesis with high quality.",
            "ai_keywords": [
                "video diffusion models",
                "physically based rendering (PBR)",
                "spatially varying PBR parameters",
                "surface normals",
                "explicit camera control",
                "neural prior",
                "relighting",
                "novel view synthesis",
                "object-centric datasets",
                "AR/VR",
                "movies",
                "games",
                "visual media"
            ],
            "organization": {
                "_id": "62e1573a6fb6e362b4a90690",
                "name": "stabilityai",
                "fullname": "Stability AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png"
            }
        },
        "publishedAt": "2025-10-09T10:29:47.000Z",
        "title": "SViM3D: Stable Video Material Diffusion for Single Image 3D Generation",
        "summary": "We present Stable Video Materials 3D (SViM3D), a framework to predict\nmulti-view consistent physically based rendering (PBR) materials, given a\nsingle image. Recently, video diffusion models have been successfully used to\nreconstruct 3D objects from a single image efficiently. However, reflectance is\nstill represented by simple material models or needs to be estimated in\nadditional steps to enable relighting and controlled appearance edits. We\nextend a latent video diffusion model to output spatially varying PBR\nparameters and surface normals jointly with each generated view based on\nexplicit camera control. This unique setup allows for relighting and generating\na 3D asset using our model as neural prior. We introduce various mechanisms to\nthis pipeline that improve quality in this ill-posed setting. We show\nstate-of-the-art relighting and novel view synthesis performance on multiple\nobject-centric datasets. Our method generalizes to diverse inputs, enabling the\ngeneration of relightable 3D assets useful in AR/VR, movies, games and other\nvisual media.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64ad09cd2d29d3bdcd482bbd/yMCs7QBk8U2uXXM_fovpD.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08271.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ad09cd2d29d3bdcd482bbd",
            "avatarUrl": "/avatars/657e02321bc4798bb31eab8b1b9e1c84.svg",
            "fullname": "Andreas Engelhardt",
            "name": "andreasengelhardt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "organization": {
            "_id": "62e1573a6fb6e362b4a90690",
            "name": "stabilityai",
            "fullname": "Stability AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07314",
            "authors": [
                {
                    "_id": "68e916ba95e8e6771df38cac",
                    "name": "Fabian Paischer",
                    "hidden": false
                },
                {
                    "_id": "68e916ba95e8e6771df38cad",
                    "name": "Gianluca Galletti",
                    "hidden": false
                },
                {
                    "_id": "68e916ba95e8e6771df38cae",
                    "name": "William Hornsby",
                    "hidden": false
                },
                {
                    "_id": "68e916ba95e8e6771df38caf",
                    "name": "Paul Setinek",
                    "hidden": false
                },
                {
                    "_id": "68e916ba95e8e6771df38cb0",
                    "name": "Lorenzo Zanisi",
                    "hidden": false
                },
                {
                    "_id": "68e916ba95e8e6771df38cb1",
                    "name": "Naomi Carey",
                    "hidden": false
                },
                {
                    "_id": "68e916ba95e8e6771df38cb2",
                    "name": "Stanislas Pamela",
                    "hidden": false
                },
                {
                    "_id": "68e916ba95e8e6771df38cb3",
                    "name": "Johannes Brandstetter",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T17:59:10.000Z",
            "submittedOnDailyAt": "2025-10-10T12:59:10.294Z",
            "title": "GyroSwin: 5D Surrogates for Gyrokinetic Plasma Turbulence Simulations",
            "submittedOnDailyBy": {
                "_id": "648826f845a9218318e0272c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648826f845a9218318e0272c/1zUQFA7TdQ8WC9Chskx6b.jpeg",
                "isPro": false,
                "fullname": "Fabian Paischer",
                "user": "paischer101",
                "type": "user"
            },
            "summary": "Nuclear fusion plays a pivotal role in the quest for reliable and sustainable\nenergy production. A major roadblock to viable fusion power is understanding\nplasma turbulence, which significantly impairs plasma confinement, and is vital\nfor next-generation reactor design. Plasma turbulence is governed by the\nnonlinear gyrokinetic equation, which evolves a 5D distribution function over\ntime. Due to its high computational cost, reduced-order models are often\nemployed in practice to approximate turbulent transport of energy. However,\nthey omit nonlinear effects unique to the full 5D dynamics. To tackle this, we\nintroduce GyroSwin, the first scalable 5D neural surrogate that can model 5D\nnonlinear gyrokinetic simulations, thereby capturing the physical phenomena\nneglected by reduced models, while providing accurate estimates of turbulent\nheat transport.GyroSwin (i) extends hierarchical Vision Transformers to 5D,\n(ii) introduces cross-attention and integration modules for latent\n3Dleftrightarrow5D interactions between electrostatic potential fields and\nthe distribution function, and (iii) performs channelwise mode separation\ninspired by nonlinear physics. We demonstrate that GyroSwin outperforms widely\nused reduced numerics on heat flux prediction, captures the turbulent energy\ncascade, and reduces the cost of fully resolved nonlinear gyrokinetics by three\norders of magnitude while remaining physically verifiable. GyroSwin shows\npromising scaling laws, tested up to one billion parameters, paving the way for\nscalable neural surrogates for gyrokinetic simulations of plasma turbulence.",
            "upvotes": 1,
            "discussionId": "68e916ba95e8e6771df38cb4",
            "ai_summary": "GyroSwin, a scalable 5D neural surrogate model, captures nonlinear gyrokinetic dynamics and improves heat flux prediction in plasma turbulence simulations.",
            "ai_keywords": [
                "gyrokinetic equation",
                "reduced-order models",
                "neural surrogate",
                "Vision Transformers",
                "cross-attention",
                "integration modules",
                "channelwise mode separation",
                "heat flux prediction",
                "turbulent energy cascade",
                "nonlinear gyrokinetics",
                "scalable neural surrogates"
            ],
            "organization": {
                "_id": "649bf3617f9700d6fa001b0d",
                "name": "JKU",
                "fullname": "Johannes Kepler University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649bf2d40431942ba8db11eb/544VcqQJ2pbYDbvZVU1Bd.jpeg"
            }
        },
        "publishedAt": "2025-10-08T13:59:10.000Z",
        "title": "GyroSwin: 5D Surrogates for Gyrokinetic Plasma Turbulence Simulations",
        "summary": "Nuclear fusion plays a pivotal role in the quest for reliable and sustainable\nenergy production. A major roadblock to viable fusion power is understanding\nplasma turbulence, which significantly impairs plasma confinement, and is vital\nfor next-generation reactor design. Plasma turbulence is governed by the\nnonlinear gyrokinetic equation, which evolves a 5D distribution function over\ntime. Due to its high computational cost, reduced-order models are often\nemployed in practice to approximate turbulent transport of energy. However,\nthey omit nonlinear effects unique to the full 5D dynamics. To tackle this, we\nintroduce GyroSwin, the first scalable 5D neural surrogate that can model 5D\nnonlinear gyrokinetic simulations, thereby capturing the physical phenomena\nneglected by reduced models, while providing accurate estimates of turbulent\nheat transport.GyroSwin (i) extends hierarchical Vision Transformers to 5D,\n(ii) introduces cross-attention and integration modules for latent\n3Dleftrightarrow5D interactions between electrostatic potential fields and\nthe distribution function, and (iii) performs channelwise mode separation\ninspired by nonlinear physics. We demonstrate that GyroSwin outperforms widely\nused reduced numerics on heat flux prediction, captures the turbulent energy\ncascade, and reduces the cost of fully resolved nonlinear gyrokinetics by three\norders of magnitude while remaining physically verifiable. GyroSwin shows\npromising scaling laws, tested up to one billion parameters, paving the way for\nscalable neural surrogates for gyrokinetic simulations of plasma turbulence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07314.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648826f845a9218318e0272c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648826f845a9218318e0272c/1zUQFA7TdQ8WC9Chskx6b.jpeg",
            "fullname": "Fabian Paischer",
            "name": "paischer101",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "649bf3617f9700d6fa001b0d",
            "name": "JKU",
            "fullname": "Johannes Kepler University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649bf2d40431942ba8db11eb/544VcqQJ2pbYDbvZVU1Bd.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.02994",
            "authors": [
                {
                    "_id": "68e8875695e8e6771df38ac1",
                    "user": {
                        "_id": "65896f5bae21a8ff2883ec1a",
                        "avatarUrl": "/avatars/ffacc166fd0d145504773c666b7c8b94.svg",
                        "isPro": false,
                        "fullname": "Ruihao Xia",
                        "user": "XiaRho",
                        "type": "user"
                    },
                    "name": "Ruihao Xia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:31:48.948Z",
                    "hidden": false
                },
                {
                    "_id": "68e8875695e8e6771df38ac2",
                    "name": "Yang Tang",
                    "hidden": false
                },
                {
                    "_id": "68e8875695e8e6771df38ac3",
                    "name": "Pan Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-03T13:34:55.000Z",
            "submittedOnDailyAt": "2025-10-10T02:46:29.731Z",
            "title": "Towards Scalable and Consistent 3D Editing",
            "submittedOnDailyBy": {
                "_id": "65896f5bae21a8ff2883ec1a",
                "avatarUrl": "/avatars/ffacc166fd0d145504773c666b7c8b94.svg",
                "isPro": false,
                "fullname": "Ruihao Xia",
                "user": "XiaRho",
                "type": "user"
            },
            "summary": "3D editing - the task of locally modifying the geometry or appearance of a 3D\nasset - has wide applications in immersive content creation, digital\nentertainment, and AR/VR. However, unlike 2D editing, it remains challenging\ndue to the need for cross-view consistency, structural fidelity, and\nfine-grained controllability. Existing approaches are often slow, prone to\ngeometric distortions, or dependent on manual and accurate 3D masks that are\nerror-prone and impractical. To address these challenges, we advance both the\ndata and model fronts. On the data side, we introduce 3DEditVerse, the largest\npaired 3D editing benchmark to date, comprising 116,309 high-quality training\npairs and 1,500 curated test pairs. Built through complementary pipelines of\npose-driven geometric edits and foundation model-guided appearance edits,\n3DEditVerse ensures edit locality, multi-view consistency, and semantic\nalignment. On the model side, we propose 3DEditFormer, a\n3D-structure-preserving conditional transformer. By enhancing image-to-3D\ngeneration with dual-guidance attention and time-adaptive gating, 3DEditFormer\ndisentangles editable regions from preserved structure, enabling precise and\nconsistent edits without requiring auxiliary 3D masks. Extensive experiments\ndemonstrate that our framework outperforms state-of-the-art baselines both\nquantitatively and qualitatively, establishing a new standard for practical and\nscalable 3D editing. Dataset and code will be released. Project:\nhttps://www.lv-lab.org/3DEditFormer/",
            "upvotes": 1,
            "discussionId": "68e8875695e8e6771df38ac4",
            "projectPage": "https://www.lv-lab.org/3DEditFormer/",
            "githubRepo": "https://github.com/LVLab-SMU/3DEditFormer",
            "ai_summary": "A new framework, 3DEditFormer, uses a 3D-structure-preserving conditional transformer to enable precise and consistent 3D editing without manual masks, outperforming existing methods.",
            "ai_keywords": [
                "3D editing",
                "3DEditVerse",
                "3DEditFormer",
                "3D-structure-preserving conditional transformer",
                "dual-guidance attention",
                "time-adaptive gating",
                "edit locality",
                "multi-view consistency",
                "semantic alignment",
                "image-to-3D generation"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-10-03T09:34:55.000Z",
        "title": "Towards Scalable and Consistent 3D Editing",
        "summary": "3D editing - the task of locally modifying the geometry or appearance of a 3D\nasset - has wide applications in immersive content creation, digital\nentertainment, and AR/VR. However, unlike 2D editing, it remains challenging\ndue to the need for cross-view consistency, structural fidelity, and\nfine-grained controllability. Existing approaches are often slow, prone to\ngeometric distortions, or dependent on manual and accurate 3D masks that are\nerror-prone and impractical. To address these challenges, we advance both the\ndata and model fronts. On the data side, we introduce 3DEditVerse, the largest\npaired 3D editing benchmark to date, comprising 116,309 high-quality training\npairs and 1,500 curated test pairs. Built through complementary pipelines of\npose-driven geometric edits and foundation model-guided appearance edits,\n3DEditVerse ensures edit locality, multi-view consistency, and semantic\nalignment. On the model side, we propose 3DEditFormer, a\n3D-structure-preserving conditional transformer. By enhancing image-to-3D\ngeneration with dual-guidance attention and time-adaptive gating, 3DEditFormer\ndisentangles editable regions from preserved structure, enabling precise and\nconsistent edits without requiring auxiliary 3D masks. Extensive experiments\ndemonstrate that our framework outperforms state-of-the-art baselines both\nquantitatively and qualitatively, establishing a new standard for practical and\nscalable 3D editing. Dataset and code will be released. Project:\nhttps://www.lv-lab.org/3DEditFormer/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02994.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65896f5bae21a8ff2883ec1a",
            "avatarUrl": "/avatars/ffacc166fd0d145504773c666b7c8b94.svg",
            "fullname": "Ruihao Xia",
            "name": "XiaRho",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.02590",
            "authors": [
                {
                    "_id": "68e95030b12ca14cb8a16640",
                    "name": "Ahmed Hendawy",
                    "hidden": false
                },
                {
                    "_id": "68e95030b12ca14cb8a16641",
                    "name": "Henrik Metternich",
                    "hidden": false
                },
                {
                    "_id": "68e95030b12ca14cb8a16642",
                    "name": "Tho Vincent",
                    "hidden": false
                },
                {
                    "_id": "68e95030b12ca14cb8a16643",
                    "name": "Mahdi Kallel",
                    "hidden": false
                },
                {
                    "_id": "68e95030b12ca14cb8a16644",
                    "name": "Jan Peters",
                    "hidden": false
                },
                {
                    "_id": "68e95030b12ca14cb8a16645",
                    "name": "Carlo D'Eramo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-02T21:48:01.000Z",
            "submittedOnDailyAt": "2025-10-10T17:00:38.352Z",
            "title": "Use the Online Network If You Can: Towards Fast and Stable Reinforcement\n  Learning",
            "submittedOnDailyBy": {
                "_id": "66337c054a7779b3b0beddf6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66337c054a7779b3b0beddf6/-oB6XNXKOfrtkFP3SjdtN.jpeg",
                "isPro": false,
                "fullname": "Ahmed Hendawy",
                "user": "ahmedhendawy19",
                "type": "user"
            },
            "summary": "The use of target networks is a popular approach for estimating value\nfunctions in deep Reinforcement Learning (RL). While effective, the target\nnetwork remains a compromise solution that preserves stability at the cost of\nslowly moving targets, thus delaying learning. Conversely, using the online\nnetwork as a bootstrapped target is intuitively appealing, albeit well-known to\nlead to unstable learning. In this work, we aim to obtain the best out of both\nworlds by introducing a novel update rule that computes the target using the\nMINimum estimate between the Target and Online network, giving rise to our\nmethod, MINTO. Through this simple, yet effective modification, we show that\nMINTO enables faster and stable value function learning, by mitigating the\npotential overestimation bias of using the online network for bootstrapping.\nNotably, MINTO can be seamlessly integrated into a wide range of value-based\nand actor-critic algorithms with a negligible cost. We evaluate MINTO\nextensively across diverse benchmarks, spanning online and offline RL, as well\nas discrete and continuous action spaces. Across all benchmarks, MINTO\nconsistently improves performance, demonstrating its broad applicability and\neffectiveness.",
            "upvotes": 1,
            "discussionId": "68e95030b12ca14cb8a16646",
            "ai_summary": "MINTO, a novel update rule using the minimum estimate between target and online networks, enhances stable and faster value function learning in deep reinforcement learning.",
            "ai_keywords": [
                "target networks",
                "value functions",
                "deep Reinforcement Learning",
                "online network",
                "bootstrapped target",
                "MINTO",
                "value-based algorithms",
                "actor-critic algorithms",
                "overestimation bias",
                "online RL",
                "offline RL",
                "discrete action spaces",
                "continuous action spaces"
            ]
        },
        "publishedAt": "2025-10-02T17:48:01.000Z",
        "title": "Use the Online Network If You Can: Towards Fast and Stable Reinforcement\n  Learning",
        "summary": "The use of target networks is a popular approach for estimating value\nfunctions in deep Reinforcement Learning (RL). While effective, the target\nnetwork remains a compromise solution that preserves stability at the cost of\nslowly moving targets, thus delaying learning. Conversely, using the online\nnetwork as a bootstrapped target is intuitively appealing, albeit well-known to\nlead to unstable learning. In this work, we aim to obtain the best out of both\nworlds by introducing a novel update rule that computes the target using the\nMINimum estimate between the Target and Online network, giving rise to our\nmethod, MINTO. Through this simple, yet effective modification, we show that\nMINTO enables faster and stable value function learning, by mitigating the\npotential overestimation bias of using the online network for bootstrapping.\nNotably, MINTO can be seamlessly integrated into a wide range of value-based\nand actor-critic algorithms with a negligible cost. We evaluate MINTO\nextensively across diverse benchmarks, spanning online and offline RL, as well\nas discrete and continuous action spaces. Across all benchmarks, MINTO\nconsistently improves performance, demonstrating its broad applicability and\neffectiveness.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02590.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66337c054a7779b3b0beddf6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66337c054a7779b3b0beddf6/-oB6XNXKOfrtkFP3SjdtN.jpeg",
            "fullname": "Ahmed Hendawy",
            "name": "ahmedhendawy19",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.26633",
            "authors": [
                {
                    "_id": "68e94146b12ca14cb8a165e7",
                    "name": "Lujie Yang",
                    "hidden": false
                },
                {
                    "_id": "68e94146b12ca14cb8a165e8",
                    "name": "Xiaoyu Huang",
                    "hidden": false
                },
                {
                    "_id": "68e94146b12ca14cb8a165e9",
                    "name": "Zhen Wu",
                    "hidden": false
                },
                {
                    "_id": "68e94146b12ca14cb8a165ea",
                    "name": "Angjoo Kanazawa",
                    "hidden": false
                },
                {
                    "_id": "68e94146b12ca14cb8a165eb",
                    "name": "Pieter Abbeel",
                    "hidden": false
                },
                {
                    "_id": "68e94146b12ca14cb8a165ec",
                    "name": "Carmelo Sferrazza",
                    "hidden": false
                },
                {
                    "_id": "68e94146b12ca14cb8a165ed",
                    "name": "C. Karen Liu",
                    "hidden": false
                },
                {
                    "_id": "68e94146b12ca14cb8a165ee",
                    "name": "Rocky Duan",
                    "hidden": false
                },
                {
                    "_id": "68e94146b12ca14cb8a165ef",
                    "name": "Guanya Shi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-30T17:59:02.000Z",
            "submittedOnDailyAt": "2025-10-10T15:57:22.080Z",
            "title": "OmniRetarget: Interaction-Preserving Data Generation for Humanoid\n  Whole-Body Loco-Manipulation and Scene Interaction",
            "submittedOnDailyBy": {
                "_id": "648a374f00f7a3374ee64b99",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648a374f00f7a3374ee64b99/YPwSOrronoozwHbJchPn3.jpeg",
                "isPro": true,
                "fullname": "Caleb Fahlgren",
                "user": "cfahlgren1",
                "type": "user"
            },
            "summary": "A dominant paradigm for teaching humanoid robots complex skills is to\nretarget human motions as kinematic references to train reinforcement learning\n(RL) policies. However, existing retargeting pipelines often struggle with the\nsignificant embodiment gap between humans and robots, producing physically\nimplausible artifacts like foot-skating and penetration. More importantly,\ncommon retargeting methods neglect the rich human-object and human-environment\ninteractions essential for expressive locomotion and loco-manipulation. To\naddress this, we introduce OmniRetarget, an interaction-preserving data\ngeneration engine based on an interaction mesh that explicitly models and\npreserves the crucial spatial and contact relationships between an agent, the\nterrain, and manipulated objects. By minimizing the Laplacian deformation\nbetween the human and robot meshes while enforcing kinematic constraints,\nOmniRetarget generates kinematically feasible trajectories. Moreover,\npreserving task-relevant interactions enables efficient data augmentation, from\na single demonstration to different robot embodiments, terrains, and object\nconfigurations. We comprehensively evaluate OmniRetarget by retargeting motions\nfrom OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour\ntrajectories that achieve better kinematic constraint satisfaction and contact\npreservation than widely used baselines. Such high-quality data enables\nproprioceptive RL policies to successfully execute long-horizon (up to 30\nseconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained\nwith only 5 reward terms and simple domain randomization shared by all tasks,\nwithout any learning curriculum.",
            "upvotes": 1,
            "discussionId": "68e94146b12ca14cb8a165f0",
            "projectPage": "https://omniretarget.github.io/",
            "ai_summary": "OmniRetarget generates high-quality, interaction-preserving motion data for training RL policies, enabling complex skills like parkour and loco-manipulation on humanoid robots.",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "retargeting",
                "embodiment gap",
                "foot-skating",
                "penetration",
                "human-object interactions",
                "human-environment interactions",
                "interaction mesh",
                "Laplacian deformation",
                "kinematic constraints",
                "data augmentation",
                "proprioceptive RL policies",
                "long-horizon skills",
                "parkour",
                "loco-manipulation",
                "Unitree G1",
                "reward terms",
                "domain randomization"
            ]
        },
        "publishedAt": "2025-09-30T13:59:02.000Z",
        "title": "OmniRetarget: Interaction-Preserving Data Generation for Humanoid\n  Whole-Body Loco-Manipulation and Scene Interaction",
        "summary": "A dominant paradigm for teaching humanoid robots complex skills is to\nretarget human motions as kinematic references to train reinforcement learning\n(RL) policies. However, existing retargeting pipelines often struggle with the\nsignificant embodiment gap between humans and robots, producing physically\nimplausible artifacts like foot-skating and penetration. More importantly,\ncommon retargeting methods neglect the rich human-object and human-environment\ninteractions essential for expressive locomotion and loco-manipulation. To\naddress this, we introduce OmniRetarget, an interaction-preserving data\ngeneration engine based on an interaction mesh that explicitly models and\npreserves the crucial spatial and contact relationships between an agent, the\nterrain, and manipulated objects. By minimizing the Laplacian deformation\nbetween the human and robot meshes while enforcing kinematic constraints,\nOmniRetarget generates kinematically feasible trajectories. Moreover,\npreserving task-relevant interactions enables efficient data augmentation, from\na single demonstration to different robot embodiments, terrains, and object\nconfigurations. We comprehensively evaluate OmniRetarget by retargeting motions\nfrom OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour\ntrajectories that achieve better kinematic constraint satisfaction and contact\npreservation than widely used baselines. Such high-quality data enables\nproprioceptive RL policies to successfully execute long-horizon (up to 30\nseconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained\nwith only 5 reward terms and simple domain randomization shared by all tasks,\nwithout any learning curriculum.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26633.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648a374f00f7a3374ee64b99",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648a374f00f7a3374ee64b99/YPwSOrronoozwHbJchPn3.jpeg",
            "fullname": "Caleb Fahlgren",
            "name": "cfahlgren1",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 267
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.24797",
            "authors": [
                {
                    "_id": "68e8bc3095e8e6771df38bd8",
                    "name": "Zizhao Tong",
                    "hidden": false
                },
                {
                    "_id": "68e8bc3095e8e6771df38bd9",
                    "name": "Di Chen",
                    "hidden": false
                },
                {
                    "_id": "68e8bc3095e8e6771df38bda",
                    "name": "Sicheng Hu",
                    "hidden": false
                },
                {
                    "_id": "68e8bc3095e8e6771df38bdb",
                    "name": "Hongwei Fan",
                    "hidden": false
                },
                {
                    "_id": "68e8bc3095e8e6771df38bdc",
                    "name": "Liliang Chen",
                    "hidden": false
                },
                {
                    "_id": "68e8bc3095e8e6771df38bdd",
                    "name": "Guanghui Ren",
                    "hidden": false
                },
                {
                    "_id": "68e8bc3095e8e6771df38bde",
                    "name": "Hao Tang",
                    "hidden": false
                },
                {
                    "_id": "68e8bc3095e8e6771df38bdf",
                    "name": "Hao Dong",
                    "hidden": false
                },
                {
                    "_id": "68e8bc3095e8e6771df38be0",
                    "name": "Ling Shao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-29T13:48:36.000Z",
            "submittedOnDailyAt": "2025-10-10T06:28:15.369Z",
            "title": "Fidelity-Aware Data Composition for Robust Robot Generalization",
            "submittedOnDailyBy": {
                "_id": "66c4816e96583c59b09fec30",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c4816e96583c59b09fec30/RLurCsmcgOfyuWVkpsv1L.jpeg",
                "isPro": false,
                "fullname": "Ryan Chen",
                "user": "ryancll118",
                "type": "user"
            },
            "summary": "Generalist robot policies trained on large-scale, visually homogeneous\ndatasets can be susceptible to shortcut learning, which impairs their\nout-of-distribution (OOD) generalization. While generative data augmentation is\na common approach to introduce diversity, it presents a subtle challenge: data\ncomposition. Naively mixing real and synthetic data can corrupt the learning\nsignal, as this process often prioritizes visual diversity at the expense of\ninformation fidelity. This paper suggests that robust generalization depends on\nprincipled, fidelity-aware data composition. We introduce Coherent Information\nFidelity Tuning (CIFT), a framework that treats data composition as an\noptimization problem. CIFT uses a practical proxy for Information Fidelity\nbased on the feature-space geometry of a dataset. This enables the\nidentification of a phase transition, termed the Decoherence Point, where\ntraining stability degrades. The framework includes a generative engine,\nMulti-View Video Augmentation (MVAug), to synthesize a causally disentangled\ndata spectrum for this tuning process. Applying CIFT to policy architectures\nsuch as pi_0 and Diffusion Policy improves OOD success rates by over 54\\%.\nThese results indicate that fidelity-aware composition, beyond data synthesis\nalone, is an important component for developing robust, general-purpose robots.",
            "upvotes": 1,
            "discussionId": "68e8bc3195e8e6771df38be1",
            "ai_summary": "Coherent Information Fidelity Tuning (CIFT) improves out-of-distribution generalization in robot policies by optimizing data composition with a generative engine, enhancing robustness and performance.",
            "ai_keywords": [
                "shortcut learning",
                "out-of-distribution (OOD) generalization",
                "generative data augmentation",
                "data composition",
                "Information Fidelity",
                "feature-space geometry",
                "Decoherence Point",
                "Multi-View Video Augmentation (MVAug)",
                "causally disentangled",
                "policy architectures",
                "Diffusion Policy"
            ]
        },
        "publishedAt": "2025-09-29T09:48:36.000Z",
        "title": "Fidelity-Aware Data Composition for Robust Robot Generalization",
        "summary": "Generalist robot policies trained on large-scale, visually homogeneous\ndatasets can be susceptible to shortcut learning, which impairs their\nout-of-distribution (OOD) generalization. While generative data augmentation is\na common approach to introduce diversity, it presents a subtle challenge: data\ncomposition. Naively mixing real and synthetic data can corrupt the learning\nsignal, as this process often prioritizes visual diversity at the expense of\ninformation fidelity. This paper suggests that robust generalization depends on\nprincipled, fidelity-aware data composition. We introduce Coherent Information\nFidelity Tuning (CIFT), a framework that treats data composition as an\noptimization problem. CIFT uses a practical proxy for Information Fidelity\nbased on the feature-space geometry of a dataset. This enables the\nidentification of a phase transition, termed the Decoherence Point, where\ntraining stability degrades. The framework includes a generative engine,\nMulti-View Video Augmentation (MVAug), to synthesize a causally disentangled\ndata spectrum for this tuning process. Applying CIFT to policy architectures\nsuch as pi_0 and Diffusion Policy improves OOD success rates by over 54\\%.\nThese results indicate that fidelity-aware composition, beyond data synthesis\nalone, is an important component for developing robust, general-purpose robots.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24797.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66c4816e96583c59b09fec30",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c4816e96583c59b09fec30/RLurCsmcgOfyuWVkpsv1L.jpeg",
            "fullname": "Ryan Chen",
            "name": "ryancll118",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.23500",
            "authors": [
                {
                    "_id": "68e8b3b595e8e6771df38bc9",
                    "name": "Georgios Vlassis",
                    "hidden": false
                },
                {
                    "_id": "68e8b3b595e8e6771df38bca",
                    "name": "Saleh Ashkboos",
                    "hidden": false
                },
                {
                    "_id": "68e8b3b595e8e6771df38bcb",
                    "name": "Alexandra Volkova",
                    "hidden": false
                },
                {
                    "_id": "68e8b3b595e8e6771df38bcc",
                    "name": "Torsten Hoefler",
                    "hidden": false
                },
                {
                    "_id": "68e8b3b595e8e6771df38bcd",
                    "name": "Dan Alistarh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-27T21:15:22.000Z",
            "submittedOnDailyAt": "2025-10-10T05:51:46.591Z",
            "title": "Beyond Outliers: A Study of Optimizers Under Quantization",
            "submittedOnDailyBy": {
                "_id": "610d7ec84539844d377af78a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610d7ec84539844d377af78a/-9H0bIv5pjI9hJamg9P0x.jpeg",
                "isPro": false,
                "fullname": "Ashkboos",
                "user": "Saleh",
                "type": "user"
            },
            "summary": "As new optimizers gain traction and model quantization becomes standard for\nefficient deployment, a key question arises: how does the choice of optimizer\naffect model performance in the presence of quantization? Despite progress in\nboth areas, systematic evidence on optimizer-quantization interactions remains\nlimited. To fill this gap, we study the impact of optimizer choice on model\nrobustness under quantization, considering both post-training quantization\n(PTQ), and quantization-aware training (QAT). We first train full-precision\nmodels, ranging from 50M to 1.5B parameters, with six optimizers, to explore\nthe hyperparameter landscape, and establish well-tuned baselines. We then apply\nPTQ to evaluate how model performance degrades when trained with different\noptimizers. We find that outlier-related metrics, such as the max-to-mean ratio\n(MMR) and Kurtosis, fail to predict the PTQ performance across different\noptimizers. We show analytically that this is due to the MMR capturing only\nisolated layer errors, while ignoring how quantization errors accumulate and\npropagate through the network. To study the QAT degradation, we train quantized\nmodels from scratch and compare them to our original-precision baselines. We\nfind that optimizers performing well in the original pretraining setup may not\nremain optimal under QAT, and that models trained with Shampoo show the lowest\naccuracy degradation. Finally, we derive scaling laws for quantization-aware\ntraining under different optimizers, showing that Shampoo achieves the highest\nparameter efficiency of all tested optimizers.",
            "upvotes": 1,
            "discussionId": "68e8b3b695e8e6771df38bce",
            "ai_summary": "The study investigates how different optimizers impact model performance under post-training and quantization-aware training quantization, finding that Shampoo optimizer shows the lowest accuracy degradation and highest parameter efficiency.",
            "ai_keywords": [
                "optimizers",
                "model quantization",
                "post-training quantization",
                "quantization-aware training",
                "hyperparameter landscape",
                "max-to-mean ratio",
                "Kurtosis",
                "Shampoo",
                "scaling laws",
                "parameter efficiency"
            ],
            "organization": {
                "_id": "6780db52c0ae6aca53ac4d9a",
                "name": "SPCL",
                "fullname": "Scalable Parallel Computing Laboratory (SPCL)",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65a91420c46ce42ef5da96af/4yt5Owl4HFVXTDkaT9Lgg.png"
            }
        },
        "publishedAt": "2025-09-27T17:15:22.000Z",
        "title": "Beyond Outliers: A Study of Optimizers Under Quantization",
        "summary": "As new optimizers gain traction and model quantization becomes standard for\nefficient deployment, a key question arises: how does the choice of optimizer\naffect model performance in the presence of quantization? Despite progress in\nboth areas, systematic evidence on optimizer-quantization interactions remains\nlimited. To fill this gap, we study the impact of optimizer choice on model\nrobustness under quantization, considering both post-training quantization\n(PTQ), and quantization-aware training (QAT). We first train full-precision\nmodels, ranging from 50M to 1.5B parameters, with six optimizers, to explore\nthe hyperparameter landscape, and establish well-tuned baselines. We then apply\nPTQ to evaluate how model performance degrades when trained with different\noptimizers. We find that outlier-related metrics, such as the max-to-mean ratio\n(MMR) and Kurtosis, fail to predict the PTQ performance across different\noptimizers. We show analytically that this is due to the MMR capturing only\nisolated layer errors, while ignoring how quantization errors accumulate and\npropagate through the network. To study the QAT degradation, we train quantized\nmodels from scratch and compare them to our original-precision baselines. We\nfind that optimizers performing well in the original pretraining setup may not\nremain optimal under QAT, and that models trained with Shampoo show the lowest\naccuracy degradation. Finally, we derive scaling laws for quantization-aware\ntraining under different optimizers, showing that Shampoo achieves the highest\nparameter efficiency of all tested optimizers.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23500.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "610d7ec84539844d377af78a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610d7ec84539844d377af78a/-9H0bIv5pjI9hJamg9P0x.jpeg",
            "fullname": "Ashkboos",
            "name": "Saleh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "6780db52c0ae6aca53ac4d9a",
            "name": "SPCL",
            "fullname": "Scalable Parallel Computing Laboratory (SPCL)",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65a91420c46ce42ef5da96af/4yt5Owl4HFVXTDkaT9Lgg.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07048",
            "authors": [
                {
                    "_id": "68e7bde844180c42eca8b034",
                    "user": {
                        "_id": "66025038c618afa8d799261f",
                        "avatarUrl": "/avatars/ac914b733a1b3e5763a3e7d55bf9f108.svg",
                        "isPro": false,
                        "fullname": "Yuntao",
                        "user": "ytgui",
                        "type": "user"
                    },
                    "name": "Yuntao Gui",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T09:12:55.602Z",
                    "hidden": false
                },
                {
                    "_id": "68e7bde844180c42eca8b035",
                    "name": "James Cheng",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66025038c618afa8d799261f/n0KXUCG6tFE27OzcHo_Ug.png"
            ],
            "publishedAt": "2025-10-08T14:16:20.000Z",
            "submittedOnDailyAt": "2025-10-10T07:48:15.215Z",
            "title": "Search-R3: Unifying Reasoning and Embedding Generation in Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "66025038c618afa8d799261f",
                "avatarUrl": "/avatars/ac914b733a1b3e5763a3e7d55bf9f108.svg",
                "isPro": false,
                "fullname": "Yuntao",
                "user": "ytgui",
                "type": "user"
            },
            "summary": "Despite their remarkable natural language understanding capabilities, Large\nLanguage Models (LLMs) have been underutilized for retrieval tasks. We present\nSearch-R3, a novel framework that addresses this limitation by adapting LLMs to\ngenerate search embeddings as a direct output of their reasoning process. Our\napproach exploits LLMs' chain-of-thought capabilities, allowing them to produce\nmore effective embeddings by reasoning step-by-step through complex semantic\nanalyses. We implement this through three complementary mechanisms. (1) a\nsupervised learning stage enables the model's ability to produce quality\nembeddings, (2) a reinforcement learning (RL) methodology that optimizes\nembedding generation alongside reasoning, and (3) a specialized RL environment\nthat efficiently handles evolving embedding representations without requiring\ncomplete corpus re-encoding at each training iteration. Our extensive\nevaluations on diverse benchmarks demonstrate that Search-R3 significantly\noutperforms prior methods by unifying the reasoning and embedding generation\nprocesses. This integrated post-training approach represents a substantial\nadvancement in handling complex knowledge-intensive tasks that require both\nsophisticated reasoning and effective information retrieval. Project page:\nhttps://github.com/ytgui/Search-R3",
            "upvotes": 0,
            "discussionId": "68e7bde944180c42eca8b036",
            "githubRepo": "https://github.com/ytgui/Search-R3",
            "ai_summary": "Search-R3 is a framework that adapts LLMs to generate effective search embeddings through chain-of-thought reasoning, supervised learning, and reinforcement learning.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "search embeddings",
                "chain-of-thought capabilities",
                "supervised learning",
                "reinforcement learning (RL)",
                "RL environment",
                "embedding representations",
                "reasoning",
                "information retrieval"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-10-08T10:16:20.000Z",
        "title": "Search-R3: Unifying Reasoning and Embedding Generation in Large Language\n  Models",
        "summary": "Despite their remarkable natural language understanding capabilities, Large\nLanguage Models (LLMs) have been underutilized for retrieval tasks. We present\nSearch-R3, a novel framework that addresses this limitation by adapting LLMs to\ngenerate search embeddings as a direct output of their reasoning process. Our\napproach exploits LLMs' chain-of-thought capabilities, allowing them to produce\nmore effective embeddings by reasoning step-by-step through complex semantic\nanalyses. We implement this through three complementary mechanisms. (1) a\nsupervised learning stage enables the model's ability to produce quality\nembeddings, (2) a reinforcement learning (RL) methodology that optimizes\nembedding generation alongside reasoning, and (3) a specialized RL environment\nthat efficiently handles evolving embedding representations without requiring\ncomplete corpus re-encoding at each training iteration. Our extensive\nevaluations on diverse benchmarks demonstrate that Search-R3 significantly\noutperforms prior methods by unifying the reasoning and embedding generation\nprocesses. This integrated post-training approach represents a substantial\nadvancement in handling complex knowledge-intensive tasks that require both\nsophisticated reasoning and effective information retrieval. Project page:\nhttps://github.com/ytgui/Search-R3",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66025038c618afa8d799261f/n0KXUCG6tFE27OzcHo_Ug.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07048.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66025038c618afa8d799261f",
            "avatarUrl": "/avatars/ac914b733a1b3e5763a3e7d55bf9f108.svg",
            "fullname": "Yuntao",
            "name": "ytgui",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]