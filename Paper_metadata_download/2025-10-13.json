[
    {
        "paper": {
            "id": "2510.05684",
            "authors": [
                {
                    "_id": "68ec8eb7cd07fb414898ca8a",
                    "name": "Suwhan Choi",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca8b",
                    "user": {
                        "_id": "646484cfb90150b2706df03b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
                        "isPro": false,
                        "fullname": "Jaeyoon Jung",
                        "user": "lastdefiance20",
                        "type": "user"
                    },
                    "name": "Jaeyoon Jung",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:04:31.376Z",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca8c",
                    "name": "Haebin Seong",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca8d",
                    "user": {
                        "_id": "647046995b8225e31ae91359",
                        "avatarUrl": "/avatars/dd288377385efc7b8b24bc88bd9c27d6.svg",
                        "isPro": false,
                        "fullname": "Minchan Kim",
                        "user": "shovelingpig",
                        "type": "user"
                    },
                    "name": "Minchan Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:04:33.915Z",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca8e",
                    "name": "Minyeong Kim",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca8f",
                    "user": {
                        "_id": "65e972db5686ed1f5eff2327",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/er_WDGEq0HY8rpIg69v0G.png",
                        "isPro": false,
                        "fullname": "Cho Yongjun",
                        "user": "PurpleSand",
                        "type": "user"
                    },
                    "name": "Yongjun Cho",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T16:09:03.297Z",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca90",
                    "name": "Yoonshik Kim",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca91",
                    "name": "Yubeen Park",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca92",
                    "name": "Youngjae Yu",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca93",
                    "user": {
                        "_id": "63198bf1615c77c25d63e9ab",
                        "avatarUrl": "/avatars/6d591f87366e9990fed3c221dafdfae0.svg",
                        "isPro": false,
                        "fullname": "Yunsung Lee",
                        "user": "Maangeek",
                        "type": "user"
                    },
                    "name": "Yunsung Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:04:28.653Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/646484cfb90150b2706df03b/c36wRRNT6d9wbZQ1iXQ9Y.mp4"
            ],
            "publishedAt": "2025-10-07T08:40:33.000Z",
            "submittedOnDailyAt": "2025-10-13T06:15:34.299Z",
            "title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to\n  Embodied AI",
            "submittedOnDailyBy": {
                "_id": "646484cfb90150b2706df03b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
                "isPro": false,
                "fullname": "Jaeyoon Jung",
                "user": "lastdefiance20",
                "type": "user"
            },
            "summary": "Large language models leverage internet-scale text data, yet embodied AI\nremains constrained by the prohibitive costs of physical trajectory collection.\nDesktop environments -- particularly gaming -- offer a compelling alternative:\nthey provide rich sensorimotor interactions at scale while maintaining the\nstructured observation-action coupling essential for embodied learning. We\npresent D2E (Desktop to Embodied AI), a framework that demonstrates desktop\ninteractions can serve as an effective pretraining substrate for robotics\nembodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT\nfor Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a\ncomplete pipeline from scalable desktop data collection to verified transfer in\nembodied domains. Our framework comprises three components: (1) the OWA Toolkit\nthat unifies diverse desktop interactions into a standardized format with 152x\ncompression, (2) the Generalist-IDM that achieves strong zero-shot\ngeneralization across unseen games through timestamp-based event prediction,\nenabling internet-scale pseudo-labeling, and (3) VAPT that transfers\ndesktop-pretrained representations to physical manipulation and navigation.\nUsing 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of\npseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO\nmanipulation and 83.3% on CANVAS navigation benchmarks. This validates that\nsensorimotor primitives in digital interactions exhibit sufficient invariance\nto transfer meaningfully to physical embodied tasks, establishing desktop\npretraining as a practical paradigm for robotics. We will make all our work\npublic, including the OWA toolkit, datasets of human-collected and\npseudo-labeled, and VAPT-trained models available at\nhttps://worv-ai.github.io/d2e/",
            "upvotes": 103,
            "discussionId": "68ec8eb7cd07fb414898ca94",
            "projectPage": "https://worv-ai.github.io/d2e/",
            "githubRepo": "https://github.com/worv-ai/D2E",
            "ai_summary": "D2E framework uses desktop interactions to pretrain embodied AI, achieving high success rates in physical manipulation and navigation tasks.",
            "ai_keywords": [
                "embodied AI",
                "desktop environments",
                "sensorimotor interactions",
                "OWA Toolkit",
                "Generalist-IDM",
                "timestamp-based event prediction",
                "zero-shot generalization",
                "internet-scale pseudo-labeling",
                "VAPT",
                "LIBERO manipulation",
                "CANVAS navigation",
                "desktop pretraining"
            ],
            "githubStars": 22
        },
        "publishedAt": "2025-10-07T04:40:33.000Z",
        "title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to\n  Embodied AI",
        "summary": "Large language models leverage internet-scale text data, yet embodied AI\nremains constrained by the prohibitive costs of physical trajectory collection.\nDesktop environments -- particularly gaming -- offer a compelling alternative:\nthey provide rich sensorimotor interactions at scale while maintaining the\nstructured observation-action coupling essential for embodied learning. We\npresent D2E (Desktop to Embodied AI), a framework that demonstrates desktop\ninteractions can serve as an effective pretraining substrate for robotics\nembodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT\nfor Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a\ncomplete pipeline from scalable desktop data collection to verified transfer in\nembodied domains. Our framework comprises three components: (1) the OWA Toolkit\nthat unifies diverse desktop interactions into a standardized format with 152x\ncompression, (2) the Generalist-IDM that achieves strong zero-shot\ngeneralization across unseen games through timestamp-based event prediction,\nenabling internet-scale pseudo-labeling, and (3) VAPT that transfers\ndesktop-pretrained representations to physical manipulation and navigation.\nUsing 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of\npseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO\nmanipulation and 83.3% on CANVAS navigation benchmarks. This validates that\nsensorimotor primitives in digital interactions exhibit sufficient invariance\nto transfer meaningfully to physical embodied tasks, establishing desktop\npretraining as a practical paradigm for robotics. We will make all our work\npublic, including the OWA toolkit, datasets of human-collected and\npseudo-labeled, and VAPT-trained models available at\nhttps://worv-ai.github.io/d2e/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/646484cfb90150b2706df03b/c36wRRNT6d9wbZQ1iXQ9Y.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05684.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646484cfb90150b2706df03b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
            "fullname": "Jaeyoon Jung",
            "name": "lastdefiance20",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08673",
            "authors": [
                {
                    "_id": "68ec5edfcd07fb414898c93e",
                    "user": {
                        "_id": "65bc98383b879593a5a2f5e5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65bc98383b879593a5a2f5e5/p2ZtoTFN6tW-QkcPJf7YT.jpeg",
                        "isPro": true,
                        "fullname": "Kang Liao",
                        "user": "KangLiao",
                        "type": "user"
                    },
                    "name": "Kang Liao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:05:58.256Z",
                    "hidden": false
                },
                {
                    "_id": "68ec5edfcd07fb414898c93f",
                    "name": "Size Wu",
                    "hidden": false
                },
                {
                    "_id": "68ec5edfcd07fb414898c940",
                    "name": "Zhonghua Wu",
                    "hidden": false
                },
                {
                    "_id": "68ec5edfcd07fb414898c941",
                    "name": "Linyi Jin",
                    "hidden": false
                },
                {
                    "_id": "68ec5edfcd07fb414898c942",
                    "name": "Chao Wang",
                    "hidden": false
                },
                {
                    "_id": "68ec5edfcd07fb414898c943",
                    "user": {
                        "_id": "63463bc4547c70e4b7d3009f",
                        "avatarUrl": "/avatars/6e5350fd998f0a7a4143d7504218164a.svg",
                        "isPro": false,
                        "fullname": "Yikai Wang",
                        "user": "yikaiwang",
                        "type": "user"
                    },
                    "name": "Yikai Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:06:01.008Z",
                    "hidden": false
                },
                {
                    "_id": "68ec5edfcd07fb414898c944",
                    "name": "Fei Wang",
                    "hidden": false
                },
                {
                    "_id": "68ec5edfcd07fb414898c945",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "68ec5edfcd07fb414898c946",
                    "name": "Chen Change Loy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T17:59:29.000Z",
            "submittedOnDailyAt": "2025-10-13T00:45:06.202Z",
            "title": "Thinking with Camera: A Unified Multimodal Model for Camera-Centric\n  Understanding and Generation",
            "submittedOnDailyBy": {
                "_id": "65bc98383b879593a5a2f5e5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65bc98383b879593a5a2f5e5/p2ZtoTFN6tW-QkcPJf7YT.jpeg",
                "isPro": true,
                "fullname": "Kang Liao",
                "user": "KangLiao",
                "type": "user"
            },
            "summary": "Camera-centric understanding and generation are two cornerstones of spatial\nintelligence, yet they are typically studied in isolation. We present Puffin, a\nunified camera-centric multimodal model that extends spatial awareness along\nthe camera dimension. Puffin integrates language regression and diffusion-based\ngeneration to interpret and create scenes from arbitrary viewpoints. To bridge\nthe modality gap between cameras and vision-language, we introduce a novel\nparadigm that treats camera as language, enabling thinking with camera. This\nguides the model to align spatially grounded visual cues with photographic\nterminology while reasoning across geometric context. Puffin is trained on\nPuffin-4M, a large-scale dataset of 4 million vision-language-camera triplets.\nWe incorporate both global camera parameters and pixel-wise camera maps,\nyielding flexible and reliable spatial generation. Experiments demonstrate\nPuffin superior performance over specialized models for camera-centric\ngeneration and understanding. With instruction tuning, Puffin generalizes to\ndiverse cross-view tasks such as spatial imagination, world exploration, and\nphotography guidance. We will release the code, models, dataset pipeline, and\nbenchmark to advance multimodal spatial intelligence research.",
            "upvotes": 86,
            "discussionId": "68ec5ee0cd07fb414898c947",
            "projectPage": "https://kangliao929.github.io/projects/puffin/",
            "githubRepo": "https://github.com/KangLiao929/Puffin",
            "ai_summary": "Puffin, a unified multimodal model, integrates language regression and diffusion-based generation to enhance camera-centric spatial understanding and generation by treating camera parameters as language.",
            "ai_keywords": [
                "language regression",
                "diffusion-based generation",
                "camera-centric",
                "multimodal model",
                "spatial awareness",
                "vision-language",
                "camera as language",
                "geometric context",
                "Puffin-4M",
                "global camera parameters",
                "pixel-wise camera maps",
                "spatial generation",
                "instruction tuning",
                "spatial imagination",
                "world exploration",
                "photography guidance"
            ],
            "githubStars": 76
        },
        "publishedAt": "2025-10-09T13:59:29.000Z",
        "title": "Thinking with Camera: A Unified Multimodal Model for Camera-Centric\n  Understanding and Generation",
        "summary": "Camera-centric understanding and generation are two cornerstones of spatial\nintelligence, yet they are typically studied in isolation. We present Puffin, a\nunified camera-centric multimodal model that extends spatial awareness along\nthe camera dimension. Puffin integrates language regression and diffusion-based\ngeneration to interpret and create scenes from arbitrary viewpoints. To bridge\nthe modality gap between cameras and vision-language, we introduce a novel\nparadigm that treats camera as language, enabling thinking with camera. This\nguides the model to align spatially grounded visual cues with photographic\nterminology while reasoning across geometric context. Puffin is trained on\nPuffin-4M, a large-scale dataset of 4 million vision-language-camera triplets.\nWe incorporate both global camera parameters and pixel-wise camera maps,\nyielding flexible and reliable spatial generation. Experiments demonstrate\nPuffin superior performance over specialized models for camera-centric\ngeneration and understanding. With instruction tuning, Puffin generalizes to\ndiverse cross-view tasks such as spatial imagination, world exploration, and\nphotography guidance. We will release the code, models, dataset pipeline, and\nbenchmark to advance multimodal spatial intelligence research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08673.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65bc98383b879593a5a2f5e5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65bc98383b879593a5a2f5e5/p2ZtoTFN6tW-QkcPJf7YT.jpeg",
            "fullname": "Kang Liao",
            "name": "KangLiao",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.09558",
            "authors": [
                {
                    "_id": "68ec771ccd07fb414898ca1f",
                    "name": "Qiguang Chen",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca20",
                    "name": "Zheng Yan",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca21",
                    "name": "Mingda Yang",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca22",
                    "name": "Libo Qin",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca23",
                    "name": "Yixin Yuan",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca24",
                    "name": "Hanjing Li",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca25",
                    "name": "Jinhao Liu",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca26",
                    "name": "Yiyan Ji",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca27",
                    "name": "Dengyun Peng",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca28",
                    "name": "Jiannan Guan",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca29",
                    "name": "Mengkang Hu",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca2a",
                    "name": "Yantao Du",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca2b",
                    "name": "Wanxiang Che",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T17:08:36.000Z",
            "submittedOnDailyAt": "2025-10-13T02:29:55.098Z",
            "title": "AutoPR: Let's Automate Your Academic Promotion!",
            "submittedOnDailyBy": {
                "_id": "636f526a6cd69d9a36ff2b53",
                "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
                "isPro": false,
                "fullname": "Qiguang Chen",
                "user": "LightChen2333",
                "type": "user"
            },
            "summary": "As the volume of peer-reviewed research surges, scholars increasingly rely on\nsocial platforms for discovery, while authors invest considerable effort in\npromoting their work to ensure visibility and citations. To streamline this\nprocess and reduce the reliance on human effort, we introduce Automatic\nPromotion (AutoPR), a novel task that transforms research papers into accurate,\nengaging, and timely public content. To enable rigorous evaluation, we release\nPRBench, a multimodal benchmark that links 512 peer-reviewed articles to\nhigh-quality promotional posts, assessing systems along three axes: Fidelity\n(accuracy and tone), Engagement (audience targeting and appeal), and Alignment\n(timing and channel optimization). We also introduce PRAgent, a multi-agent\nframework that automates AutoPR in three stages: content extraction with\nmultimodal preparation, collaborative synthesis for polished outputs, and\nplatform-specific adaptation to optimize norms, tone, and tagging for maximum\nreach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates\nsubstantial improvements, including a 604% increase in total watch time, a 438%\nrise in likes, and at least a 2.9x boost in overall engagement. Ablation\nstudies show that platform modeling and targeted promotion contribute the most\nto these gains. Our results position AutoPR as a tractable, measurable research\nproblem and provide a roadmap for scalable, impactful automated scholarly\ncommunication.",
            "upvotes": 38,
            "discussionId": "68ec771ccd07fb414898ca2c",
            "projectPage": "https://yzweak.github.io/autopr.github.io/",
            "githubRepo": "https://github.com/LightChen233/AutoPR",
            "ai_summary": "AutoPR, a multi-agent framework, automates the promotion of research papers by transforming them into engaging public content, significantly improving engagement metrics compared to direct LLM pipelines.",
            "ai_keywords": [
                "multimodal benchmark",
                "content extraction",
                "collaborative synthesis",
                "platform-specific adaptation",
                "LLM pipelines"
            ],
            "githubStars": 27
        },
        "publishedAt": "2025-10-10T13:08:36.000Z",
        "title": "AutoPR: Let's Automate Your Academic Promotion!",
        "summary": "As the volume of peer-reviewed research surges, scholars increasingly rely on\nsocial platforms for discovery, while authors invest considerable effort in\npromoting their work to ensure visibility and citations. To streamline this\nprocess and reduce the reliance on human effort, we introduce Automatic\nPromotion (AutoPR), a novel task that transforms research papers into accurate,\nengaging, and timely public content. To enable rigorous evaluation, we release\nPRBench, a multimodal benchmark that links 512 peer-reviewed articles to\nhigh-quality promotional posts, assessing systems along three axes: Fidelity\n(accuracy and tone), Engagement (audience targeting and appeal), and Alignment\n(timing and channel optimization). We also introduce PRAgent, a multi-agent\nframework that automates AutoPR in three stages: content extraction with\nmultimodal preparation, collaborative synthesis for polished outputs, and\nplatform-specific adaptation to optimize norms, tone, and tagging for maximum\nreach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates\nsubstantial improvements, including a 604% increase in total watch time, a 438%\nrise in likes, and at least a 2.9x boost in overall engagement. Ablation\nstudies show that platform modeling and targeted promotion contribute the most\nto these gains. Our results position AutoPR as a tractable, measurable research\nproblem and provide a roadmap for scalable, impactful automated scholarly\ncommunication.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09558.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636f526a6cd69d9a36ff2b53",
            "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
            "fullname": "Qiguang Chen",
            "name": "LightChen2333",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.04533",
            "authors": [
                {
                    "_id": "68e67c91975ac4c405ef2334",
                    "user": {
                        "_id": "680ed019a9918bb1cbc66248",
                        "avatarUrl": "/avatars/904d243f2fad99341f11795e93788993.svg",
                        "isPro": true,
                        "fullname": "Hyunmin Cho",
                        "user": "hyeoncho01",
                        "type": "user"
                    },
                    "name": "Hyunmin Cho",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:51:20.523Z",
                    "hidden": false
                },
                {
                    "_id": "68e67c91975ac4c405ef2335",
                    "name": "Donghoon Ahn",
                    "hidden": false
                },
                {
                    "_id": "68e67c91975ac4c405ef2336",
                    "name": "Susung Hong",
                    "hidden": false
                },
                {
                    "_id": "68e67c91975ac4c405ef2337",
                    "name": "Jee Eun Kim",
                    "hidden": false
                },
                {
                    "_id": "68e67c91975ac4c405ef2338",
                    "name": "Seungryong Kim",
                    "hidden": false
                },
                {
                    "_id": "68e67c91975ac4c405ef2339",
                    "name": "Kyong Hwan Jin",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/680ed019a9918bb1cbc66248/QFu-1RXfK2GP-IUrFzCwl.png"
            ],
            "publishedAt": "2025-10-06T06:53:29.000Z",
            "submittedOnDailyAt": "2025-10-13T00:05:19.216Z",
            "title": "TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion\n  Sampling",
            "submittedOnDailyBy": {
                "_id": "680ed019a9918bb1cbc66248",
                "avatarUrl": "/avatars/904d243f2fad99341f11795e93788993.svg",
                "isPro": true,
                "fullname": "Hyunmin Cho",
                "user": "hyeoncho01",
                "type": "user"
            },
            "summary": "Recent diffusion models achieve the state-of-the-art performance in image\ngeneration, but often suffer from semantic inconsistencies or hallucinations.\nWhile various inference-time guidance methods can enhance generation, they\noften operate indirectly by relying on external signals or architectural\nmodifications, which introduces additional computational overhead. In this\npaper, we propose Tangential Amplifying Guidance (TAG), a more efficient and\ndirect guidance method that operates solely on trajectory signals without\nmodifying the underlying diffusion model. TAG leverages an intermediate sample\nas a projection basis and amplifies the tangential components of the estimated\nscores with respect to this basis to correct the sampling trajectory. We\nformalize this guidance process by leveraging a first-order Taylor expansion,\nwhich demonstrates that amplifying the tangential component steers the state\ntoward higher-probability regions, thereby reducing inconsistencies and\nenhancing sample quality. TAG is a plug-and-play, architecture-agnostic module\nthat improves diffusion sampling fidelity with minimal computational addition,\noffering a new perspective on diffusion guidance.",
            "upvotes": 38,
            "discussionId": "68e67c91975ac4c405ef233a",
            "projectPage": "https://hyeon-cho.github.io/TAG/",
            "githubRepo": "https://github.com/hyeon-cho/Tangential-Amplifying-Guidance",
            "ai_summary": "Tangential Amplifying Guidance (TAG) improves diffusion model sample quality by directly amplifying tangential components of estimated scores without modifying the model architecture.",
            "ai_keywords": [
                "diffusion models",
                "image generation",
                "semantic inconsistencies",
                "hallucinations",
                "inference-time guidance",
                "trajectory signals",
                "intermediate sample",
                "projection basis",
                "tangential components",
                "first-order Taylor expansion",
                "sampling trajectory",
                "higher-probability regions",
                "plug-and-play",
                "architecture-agnostic"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-10-06T02:53:29.000Z",
        "title": "TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion\n  Sampling",
        "summary": "Recent diffusion models achieve the state-of-the-art performance in image\ngeneration, but often suffer from semantic inconsistencies or hallucinations.\nWhile various inference-time guidance methods can enhance generation, they\noften operate indirectly by relying on external signals or architectural\nmodifications, which introduces additional computational overhead. In this\npaper, we propose Tangential Amplifying Guidance (TAG), a more efficient and\ndirect guidance method that operates solely on trajectory signals without\nmodifying the underlying diffusion model. TAG leverages an intermediate sample\nas a projection basis and amplifies the tangential components of the estimated\nscores with respect to this basis to correct the sampling trajectory. We\nformalize this guidance process by leveraging a first-order Taylor expansion,\nwhich demonstrates that amplifying the tangential component steers the state\ntoward higher-probability regions, thereby reducing inconsistencies and\nenhancing sample quality. TAG is a plug-and-play, architecture-agnostic module\nthat improves diffusion sampling fidelity with minimal computational addition,\noffering a new perspective on diffusion guidance.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/680ed019a9918bb1cbc66248/QFu-1RXfK2GP-IUrFzCwl.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04533.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "680ed019a9918bb1cbc66248",
            "avatarUrl": "/avatars/904d243f2fad99341f11795e93788993.svg",
            "fullname": "Hyunmin Cho",
            "name": "hyeoncho01",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.09201",
            "authors": [
                {
                    "_id": "68ec632fcd07fb414898c97b",
                    "user": {
                        "_id": "64cfa0b9749587dbe01d0079",
                        "avatarUrl": "/avatars/93ca0a1d9c5578d052c5af0d4d1a0252.svg",
                        "isPro": false,
                        "fullname": "Yumin Choi",
                        "user": "YuminChoi",
                        "type": "user"
                    },
                    "name": "Yumin Choi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:05:53.909Z",
                    "hidden": false
                },
                {
                    "_id": "68ec632fcd07fb414898c97c",
                    "name": "Dongki Kim",
                    "hidden": false
                },
                {
                    "_id": "68ec632fcd07fb414898c97d",
                    "name": "Jinheon Baek",
                    "hidden": false
                },
                {
                    "_id": "68ec632fcd07fb414898c97e",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T09:41:25.000Z",
            "submittedOnDailyAt": "2025-10-13T00:56:45.053Z",
            "title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for\n  MLLMs",
            "submittedOnDailyBy": {
                "_id": "64cfa0b9749587dbe01d0079",
                "avatarUrl": "/avatars/93ca0a1d9c5578d052c5af0d4d1a0252.svg",
                "isPro": false,
                "fullname": "Yumin Choi",
                "user": "YuminChoi",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have shown remarkable success, and their\nmultimodal expansions (MLLMs) further unlock capabilities spanning images,\nvideos, and other modalities beyond text. However, despite this shift, prompt\noptimization approaches, designed to reduce the burden of manual prompt\ncrafting while maximizing performance, remain confined to text, ultimately\nlimiting the full potential of MLLMs. Motivated by this gap, we introduce the\nnew problem of multimodal prompt optimization, which expands the prior\ndefinition of prompt optimization to the multimodal space defined by the pairs\nof textual and non-textual prompts. To tackle this problem, we then propose the\nMultimodal Prompt Optimizer (MPO), a unified framework that not only performs\nthe joint optimization of multimodal prompts through alignment-preserving\nupdates but also guides the selection process of candidate prompts by\nleveraging earlier evaluations as priors in a Bayesian-based selection\nstrategy. Through extensive experiments across diverse modalities that go\nbeyond text, such as images, videos, and even molecules, we demonstrate that\nMPO outperforms leading text-only optimization methods, establishing multimodal\nprompt optimization as a crucial step to realizing the potential of MLLMs.",
            "upvotes": 36,
            "discussionId": "68ec632fcd07fb414898c97f",
            "githubRepo": "https://github.com/Dozi01/MPO",
            "ai_summary": "Multimodal Prompt Optimizer (MPO) extends prompt optimization to handle multiple data types, improving performance over text-only methods in various applications.",
            "ai_keywords": [
                "Large Language Models",
                "multimodal expansions",
                "prompt optimization",
                "Multimodal Prompt Optimizer",
                "alignment-preserving updates",
                "Bayesian-based selection strategy"
            ],
            "githubStars": 4,
            "organization": {
                "_id": "6475760c33192631bad2bb38",
                "name": "kaist-ai",
                "fullname": "KAIST AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
            }
        },
        "publishedAt": "2025-10-10T05:41:25.000Z",
        "title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for\n  MLLMs",
        "summary": "Large Language Models (LLMs) have shown remarkable success, and their\nmultimodal expansions (MLLMs) further unlock capabilities spanning images,\nvideos, and other modalities beyond text. However, despite this shift, prompt\noptimization approaches, designed to reduce the burden of manual prompt\ncrafting while maximizing performance, remain confined to text, ultimately\nlimiting the full potential of MLLMs. Motivated by this gap, we introduce the\nnew problem of multimodal prompt optimization, which expands the prior\ndefinition of prompt optimization to the multimodal space defined by the pairs\nof textual and non-textual prompts. To tackle this problem, we then propose the\nMultimodal Prompt Optimizer (MPO), a unified framework that not only performs\nthe joint optimization of multimodal prompts through alignment-preserving\nupdates but also guides the selection process of candidate prompts by\nleveraging earlier evaluations as priors in a Bayesian-based selection\nstrategy. Through extensive experiments across diverse modalities that go\nbeyond text, such as images, videos, and even molecules, we demonstrate that\nMPO outperforms leading text-only optimization methods, establishing multimodal\nprompt optimization as a crucial step to realizing the potential of MLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09201.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64cfa0b9749587dbe01d0079",
            "avatarUrl": "/avatars/93ca0a1d9c5578d052c5af0d4d1a0252.svg",
            "fullname": "Yumin Choi",
            "name": "YuminChoi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "organization": {
            "_id": "6475760c33192631bad2bb38",
            "name": "kaist-ai",
            "fullname": "KAIST AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.09608",
            "authors": [
                {
                    "_id": "68ec5910cd07fb414898c8e1",
                    "name": "Ruyi Xu",
                    "hidden": false
                },
                {
                    "_id": "68ec5910cd07fb414898c8e2",
                    "name": "Guangxuan Xiao",
                    "hidden": false
                },
                {
                    "_id": "68ec5910cd07fb414898c8e3",
                    "user": {
                        "_id": "62919485a29097b211bc7b83",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62919485a29097b211bc7b83/TX8iBGu5JSuFlrRvjPEBV.png",
                        "isPro": false,
                        "fullname": "YukangChen",
                        "user": "Yukang",
                        "type": "user"
                    },
                    "name": "Yukang Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:06:08.409Z",
                    "hidden": false
                },
                {
                    "_id": "68ec5910cd07fb414898c8e4",
                    "name": "Liuning He",
                    "hidden": false
                },
                {
                    "_id": "68ec5910cd07fb414898c8e5",
                    "name": "Kelly Peng",
                    "hidden": false
                },
                {
                    "_id": "68ec5910cd07fb414898c8e6",
                    "name": "Yao Lu",
                    "hidden": false
                },
                {
                    "_id": "68ec5910cd07fb414898c8e7",
                    "name": "Song Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T17:59:58.000Z",
            "submittedOnDailyAt": "2025-10-13T00:12:40.683Z",
            "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
            "upvotes": 23,
            "discussionId": "68ec5910cd07fb414898c8e8",
            "githubRepo": "https://github.com/mit-han-lab/streaming-vlm",
            "ai_summary": "StreamingVLM is a real-time vision-language model that efficiently processes infinite video streams using a compact KV cache and supervised fine-tuning, achieving high performance on long videos and diverse benchmarks.",
            "ai_keywords": [
                "vision-language models",
                "VLMs",
                "real-time assistants",
                "autonomous agents",
                "video streams",
                "quadratic computational costs",
                "sliding window methods",
                "attention sinks",
                "vision tokens",
                "text tokens",
                "supervised fine-tuning",
                "SFT",
                "inference-time attention pattern",
                "Inf-Streams-Eval",
                "win rate",
                "NVIDIA H100",
                "VQA abilities",
                "LongVideoBench",
                "OVOBench Realtime"
            ],
            "githubStars": 112
        },
        "publishedAt": "2025-10-10T13:59:58.000Z",
        "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
        "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09608.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 126
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.06499",
            "authors": [
                {
                    "_id": "68e7f1de93680d7965e7c838",
                    "name": "Zhepeng Cen",
                    "hidden": false
                },
                {
                    "_id": "68e7f1de93680d7965e7c839",
                    "user": {
                        "_id": "65e0c21ebd0c59bbf271004c",
                        "avatarUrl": "/avatars/3210f41fd433d798fd7857c943288625.svg",
                        "isPro": false,
                        "fullname": "Haolin Chen",
                        "user": "hlnchen",
                        "type": "user"
                    },
                    "name": "Haolin Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:33:41.584Z",
                    "hidden": false
                },
                {
                    "_id": "68e7f1de93680d7965e7c83a",
                    "name": "Shiyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68e7f1de93680d7965e7c83b",
                    "name": "Zuxin Liu",
                    "hidden": false
                },
                {
                    "_id": "68e7f1de93680d7965e7c83c",
                    "name": "Zhiwei Liu",
                    "hidden": false
                },
                {
                    "_id": "68e7f1de93680d7965e7c83d",
                    "name": "Ding Zhao",
                    "hidden": false
                },
                {
                    "_id": "68e7f1de93680d7965e7c83e",
                    "name": "Silvio Savarese",
                    "hidden": false
                },
                {
                    "_id": "68e7f1de93680d7965e7c83f",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "68e7f1de93680d7965e7c840",
                    "name": "Huan Wang",
                    "hidden": false
                },
                {
                    "_id": "68e7f1de93680d7965e7c841",
                    "name": "Weiran Yao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/632cdea254e2c512c8f95b12/gvsr0maJXcj_81DKU8MPf.jpeg"
            ],
            "publishedAt": "2025-10-07T22:30:59.000Z",
            "submittedOnDailyAt": "2025-10-13T01:05:10.732Z",
            "title": "Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining\n  Levels",
            "submittedOnDailyBy": {
                "_id": "632cdea254e2c512c8f95b12",
                "avatarUrl": "/avatars/a6d06cdd75861ae7d589f1343d81a5c5.svg",
                "isPro": false,
                "fullname": "Weiran Yao",
                "user": "weirayao",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have achieved remarkable success through\nimitation learning on vast text corpora, but this paradigm creates a\ntraining-generation gap and limits robust reasoning. Reinforcement learning\n(RL) offers a more data-efficient solution capable of bridging this gap, yet\nits application has been constrained by a critical data bottleneck: existing RL\ndatasets are orders of magnitude smaller and less diverse than web-scale\npre-training corpora. To address this, we introduce the Webscale-RL pipeline, a\nscalable data engine that systematically converts large-scale pre-training\ndocuments into millions of diverse, verifiable question-answer pairs for RL.\nUsing this pipeline, we construct the Webscale-RL dataset, containing 1.2\nmillion examples across more than 9 domains. Our experiments show that the\nmodel trained on this dataset significantly outperforms continual pretraining\nand strong data refinement baselines across a suite of benchmarks. Notably, RL\ntraining with our dataset proves substantially more efficient, achieving the\nperformance of continual pre-training with up to 100times fewer tokens. Our\nwork presents a viable path toward scaling RL to pre-training levels, enabling\nmore capable and efficient language models.",
            "upvotes": 22,
            "discussionId": "68e7f1de93680d7965e7c842",
            "projectPage": "https://huggingface.co/datasets/Salesforce/Webscale-RL",
            "githubRepo": "https://github.com/SalesforceAIResearch/PretrainRL-pipeline",
            "ai_summary": "A scalable data engine converts large-scale pre-training documents into diverse question-answer pairs for reinforcement learning, significantly improving model performance and efficiency.",
            "ai_keywords": [
                "Large Language Models",
                "imitation learning",
                "reinforcement learning",
                "Webscale-RL pipeline",
                "Webscale-RL dataset",
                "continual pretraining",
                "data refinement",
                "benchmarks"
            ],
            "githubStars": 43,
            "organization": {
                "_id": "5f6d64475e78cc6b0ed31e4c",
                "name": "Salesforce",
                "fullname": "Salesforce",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
            }
        },
        "publishedAt": "2025-10-07T18:30:59.000Z",
        "title": "Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining\n  Levels",
        "summary": "Large Language Models (LLMs) have achieved remarkable success through\nimitation learning on vast text corpora, but this paradigm creates a\ntraining-generation gap and limits robust reasoning. Reinforcement learning\n(RL) offers a more data-efficient solution capable of bridging this gap, yet\nits application has been constrained by a critical data bottleneck: existing RL\ndatasets are orders of magnitude smaller and less diverse than web-scale\npre-training corpora. To address this, we introduce the Webscale-RL pipeline, a\nscalable data engine that systematically converts large-scale pre-training\ndocuments into millions of diverse, verifiable question-answer pairs for RL.\nUsing this pipeline, we construct the Webscale-RL dataset, containing 1.2\nmillion examples across more than 9 domains. Our experiments show that the\nmodel trained on this dataset significantly outperforms continual pretraining\nand strong data refinement baselines across a suite of benchmarks. Notably, RL\ntraining with our dataset proves substantially more efficient, achieving the\nperformance of continual pre-training with up to 100times fewer tokens. Our\nwork presents a viable path toward scaling RL to pre-training levels, enabling\nmore capable and efficient language models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/632cdea254e2c512c8f95b12/gvsr0maJXcj_81DKU8MPf.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06499.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "632cdea254e2c512c8f95b12",
            "avatarUrl": "/avatars/a6d06cdd75861ae7d589f1343d81a5c5.svg",
            "fullname": "Weiran Yao",
            "name": "weirayao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "5f6d64475e78cc6b0ed31e4c",
            "name": "Salesforce",
            "fullname": "Salesforce",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08697",
            "authors": [
                {
                    "_id": "68ec5e33cd07fb414898c90f",
                    "name": "Terry Yue Zhuo",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c910",
                    "name": "Xiaolong Jin",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c911",
                    "name": "Hange Liu",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c912",
                    "name": "Juyong Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c913",
                    "name": "Tianyang Liu",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c914",
                    "name": "Chen Gong",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c915",
                    "name": "Bhupesh Bishnoi",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c916",
                    "name": "Vaisakhi Mishra",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c917",
                    "name": "Marek Suppa",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c918",
                    "name": "Noah Ziems",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c919",
                    "name": "Saiteja Utpala",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c91a",
                    "name": "Ming Xu",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c91b",
                    "name": "Guangyu Song",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c91c",
                    "name": "Kaixin Li",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c91d",
                    "name": "Yuhan Cao",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c91e",
                    "user": {
                        "_id": "635e3a76106f984574c36409",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
                        "isPro": false,
                        "fullname": "Bo Liu",
                        "user": "Benjamin-eecs",
                        "type": "user"
                    },
                    "name": "Bo Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:06:03.836Z",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c91f",
                    "name": "Zheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c920",
                    "name": "Sabina Abdurakhmanova",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c921",
                    "name": "Wenhao Yu",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c922",
                    "name": "Mengzhao Jia",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c923",
                    "name": "Jihan Yao",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c924",
                    "name": "Kenneth Hamilton",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c925",
                    "name": "Kumar Shridhar",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c926",
                    "name": "Minh Chien Vu",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c927",
                    "name": "Dingmin Wang",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c928",
                    "name": "Jiawei Liu",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c929",
                    "name": "Zijian Wang",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c92a",
                    "name": "Qian Liu",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c92b",
                    "name": "Binyuan Hui",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c92c",
                    "name": "Meg Risdal",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c92d",
                    "user": {
                        "_id": "60f1abe7544c2adfd699860c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                        "isPro": false,
                        "fullname": "AK",
                        "user": "akhaliq",
                        "type": "user"
                    },
                    "name": "Ahsen Khaliq",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-10-13T12:48:46.417Z",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c92e",
                    "name": "Atin Sood",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c92f",
                    "name": "Zhenchang Xing",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c930",
                    "name": "Wasi Uddin Ahmad",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c931",
                    "name": "John Grundy",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c932",
                    "name": "David Lo",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c933",
                    "name": "Banghua Zhu",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c934",
                    "name": "Xiaoning Du",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c935",
                    "name": "Torsten Scholak",
                    "hidden": false
                },
                {
                    "_id": "68ec5e33cd07fb414898c936",
                    "name": "Leandro von Werra",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T18:01:47.000Z",
            "submittedOnDailyAt": "2025-10-13T00:34:36.848Z",
            "title": "BigCodeArena: Unveiling More Reliable Human Preferences in Code\n  Generation via Execution",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Crowdsourced model evaluation platforms, such as Chatbot Arena, enable\nreal-time evaluation from human perspectives to assess the quality of model\nresponses. In the coding domain, manually examining the quality of\nLLM-generated content is extremely challenging, as it requires understanding\nlong chunks of raw code and deliberately simulating code execution. To this\nend, we introduce BigCodeArena, an open human evaluation platform for code\ngeneration backed by a comprehensive and on-the-fly execution environment.\nBuilt on top of Chatbot Arena, BigCodeArena enables the execution of\nLLM-generated code and allows humans to interact with the execution process and\noutcomes. We collected over 14,000 raw code-centric conversation sessions\nacross 10 widely used LLMs, spanning 10 languages and 8 types of execution\nenvironments. Among these conversations, we identified more than 4,700\nmulti-turn samples with pairwise human preferences. Further analysis uncovers\nunderexplored preferences of LLMs in fine-grained domains characterized by\ntasks, languages, and frameworks. To systematically examine code understanding\nand generation capabilities of frontier LLMs, we curated two benchmarks based\non the collected data, namely BigCodeReward and AutoCodeArena. For\nBigCodeReward, we post-processed the 4,700 conversations and evaluated the\nconsistency between reward models and human preferences. The evaluation shows\nthat most LLMs have superior performance in judging coding preferences when the\nexecution results are available. Inspired by these findings, we propose\nAutoCodeArena, an automatic Elo rating benchmark designed to assess the coding\nquality of LLMs without human involvement. We find that proprietary LLMs like\nGPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation\nperformance among recent emerging models.",
            "upvotes": 21,
            "discussionId": "68ec5e33cd07fb414898c937",
            "projectPage": "https://huggingface.co/spaces/bigcode/arena",
            "githubRepo": "https://github.com/bigcode-project/bigcodearena",
            "ai_summary": "BigCodeArena is an open human evaluation platform for code generation that enables real-time execution and interaction, revealing preferences and capabilities of LLMs in coding tasks.",
            "ai_keywords": [
                "Chatbot Arena",
                "BigCodeArena",
                "LLM-generated code",
                "code execution",
                "human evaluation",
                "BigCodeReward",
                "AutoCodeArena",
                "Elo rating benchmark",
                "code understanding",
                "code generation"
            ],
            "githubStars": 37,
            "organization": {
                "_id": "62ce8f4248fbe688600093a0",
                "name": "bigcode",
                "fullname": "BigCode",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1659521200179-5e48005437cb5b49818287a5.png"
            }
        },
        "publishedAt": "2025-10-09T14:01:47.000Z",
        "title": "BigCodeArena: Unveiling More Reliable Human Preferences in Code\n  Generation via Execution",
        "summary": "Crowdsourced model evaluation platforms, such as Chatbot Arena, enable\nreal-time evaluation from human perspectives to assess the quality of model\nresponses. In the coding domain, manually examining the quality of\nLLM-generated content is extremely challenging, as it requires understanding\nlong chunks of raw code and deliberately simulating code execution. To this\nend, we introduce BigCodeArena, an open human evaluation platform for code\ngeneration backed by a comprehensive and on-the-fly execution environment.\nBuilt on top of Chatbot Arena, BigCodeArena enables the execution of\nLLM-generated code and allows humans to interact with the execution process and\noutcomes. We collected over 14,000 raw code-centric conversation sessions\nacross 10 widely used LLMs, spanning 10 languages and 8 types of execution\nenvironments. Among these conversations, we identified more than 4,700\nmulti-turn samples with pairwise human preferences. Further analysis uncovers\nunderexplored preferences of LLMs in fine-grained domains characterized by\ntasks, languages, and frameworks. To systematically examine code understanding\nand generation capabilities of frontier LLMs, we curated two benchmarks based\non the collected data, namely BigCodeReward and AutoCodeArena. For\nBigCodeReward, we post-processed the 4,700 conversations and evaluated the\nconsistency between reward models and human preferences. The evaluation shows\nthat most LLMs have superior performance in judging coding preferences when the\nexecution results are available. Inspired by these findings, we propose\nAutoCodeArena, an automatic Elo rating benchmark designed to assess the coding\nquality of LLMs without human involvement. We find that proprietary LLMs like\nGPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation\nperformance among recent emerging models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08697.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 126
        },
        "organization": {
            "_id": "62ce8f4248fbe688600093a0",
            "name": "bigcode",
            "fullname": "BigCode",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1659521200179-5e48005437cb5b49818287a5.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08189",
            "authors": [
                {
                    "_id": "68e9eab7012880a10864b725",
                    "user": {
                        "_id": "61fde97843eb0913fa2df67b",
                        "avatarUrl": "/avatars/6b739fa8ab23ba69accb5614d96b243b.svg",
                        "isPro": false,
                        "fullname": "Luyi",
                        "user": "lulululuyi",
                        "type": "user"
                    },
                    "name": "Yi Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-11T13:26:23.631Z",
                    "hidden": false
                },
                {
                    "_id": "68e9eab7012880a10864b726",
                    "name": "Jianing Wang",
                    "hidden": false
                },
                {
                    "_id": "68e9eab7012880a10864b727",
                    "name": "Linsen Guo",
                    "hidden": false
                },
                {
                    "_id": "68e9eab7012880a10864b728",
                    "name": "Wei He",
                    "hidden": false
                },
                {
                    "_id": "68e9eab7012880a10864b729",
                    "name": "Hongyin Tang",
                    "hidden": false
                },
                {
                    "_id": "68e9eab7012880a10864b72a",
                    "name": "Tao Gui",
                    "hidden": false
                },
                {
                    "_id": "68e9eab7012880a10864b72b",
                    "name": "Xuanjing Huang",
                    "hidden": false
                },
                {
                    "_id": "68e9eab7012880a10864b72c",
                    "name": "Xuezhi Cao",
                    "hidden": false
                },
                {
                    "_id": "68e9eab7012880a10864b72d",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "68e9eab7012880a10864b72e",
                    "name": "Xunliang Cai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T13:16:22.000Z",
            "submittedOnDailyAt": "2025-10-13T00:44:04.754Z",
            "title": "R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth\n  and Depth?",
            "submittedOnDailyBy": {
                "_id": "61fde97843eb0913fa2df67b",
                "avatarUrl": "/avatars/6b739fa8ab23ba69accb5614d96b243b.svg",
                "isPro": false,
                "fullname": "Luyi",
                "user": "lulululuyi",
                "type": "user"
            },
            "summary": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,\nDeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought\n(CoT). However, existing benchmarks mainly focus on immediate, single-horizon\ntasks, failing to adequately evaluate models' ability to understand and respond\nto complex, long-horizon scenarios. To address this incomplete evaluation of\nLarge Reasoning Models (LRMs), we propose R-HORIZON, a method designed to\nstimulate long-horizon reasoning behaviors in LRMs through query composition.\nBased on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising\ncomplex multi-step reasoning tasks with interdependent problems that span long\nreasoning horizons. Through comprehensive evaluation of LRMs using the\nR-HORIZON benchmark, we find that even the most advanced LRMs suffer\nsignificant performance degradation. Our analysis reveals that LRMs exhibit\nlimited effective reasoning length and struggle to allocate thinking budget\nacross multiple problems appropriately. Recognizing these limitations, we use\nR-HORIZON to construct long-horizon reasoning data for reinforcement learning\nwith verified rewards (RLVR). Compared to training with single-horizon data,\nRLVR with R-HORIZON not only substantially improves performance on the\nmulti-horizon reasoning tasks, but also promotes accuracy on standard reasoning\ntasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as\na scalable, controllable, and low-cost paradigm for enhancing and evaluating\nthe long-horizon reasoning capabilities of LRMs.",
            "upvotes": 21,
            "discussionId": "68e9eab8012880a10864b72f",
            "projectPage": "https://reasoning-horizon.github.io/",
            "githubRepo": "https://github.com/LuLuLuyi/R-HORIZON",
            "ai_summary": "R-HORIZON, a method using query composition, improves long-horizon reasoning in Large Reasoning Models through a benchmark of complex multi-step tasks, enhancing performance and accuracy.",
            "ai_keywords": [
                "Chain-of-Thought",
                "CoT",
                "Large Reasoning Models",
                "LRMs",
                "long-horizon reasoning",
                "query composition",
                "reinforcement learning with verified rewards",
                "RLVR",
                "AIME2024"
            ],
            "githubStars": 11,
            "organization": {
                "_id": "68b28d79a176a9beb30d2049",
                "name": "meituan-longcat",
                "fullname": "LongCat",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
            }
        },
        "publishedAt": "2025-10-09T09:16:22.000Z",
        "title": "R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth\n  and Depth?",
        "summary": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,\nDeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought\n(CoT). However, existing benchmarks mainly focus on immediate, single-horizon\ntasks, failing to adequately evaluate models' ability to understand and respond\nto complex, long-horizon scenarios. To address this incomplete evaluation of\nLarge Reasoning Models (LRMs), we propose R-HORIZON, a method designed to\nstimulate long-horizon reasoning behaviors in LRMs through query composition.\nBased on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising\ncomplex multi-step reasoning tasks with interdependent problems that span long\nreasoning horizons. Through comprehensive evaluation of LRMs using the\nR-HORIZON benchmark, we find that even the most advanced LRMs suffer\nsignificant performance degradation. Our analysis reveals that LRMs exhibit\nlimited effective reasoning length and struggle to allocate thinking budget\nacross multiple problems appropriately. Recognizing these limitations, we use\nR-HORIZON to construct long-horizon reasoning data for reinforcement learning\nwith verified rewards (RLVR). Compared to training with single-horizon data,\nRLVR with R-HORIZON not only substantially improves performance on the\nmulti-horizon reasoning tasks, but also promotes accuracy on standard reasoning\ntasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as\na scalable, controllable, and low-cost paradigm for enhancing and evaluating\nthe long-horizon reasoning capabilities of LRMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08189.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61fde97843eb0913fa2df67b",
            "avatarUrl": "/avatars/6b739fa8ab23ba69accb5614d96b243b.svg",
            "fullname": "Luyi",
            "name": "lulululuyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "68b28d79a176a9beb30d2049",
            "name": "meituan-longcat",
            "fullname": "LongCat",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08759",
            "authors": [
                {
                    "_id": "68ecdddfb0aa732b88f31f4f",
                    "user": {
                        "_id": "661de604f8dcbd5a207c9012",
                        "avatarUrl": "/avatars/58f5689237dc33972703971642c8c8b1.svg",
                        "isPro": false,
                        "fullname": "yu",
                        "user": "yqi19",
                        "type": "user"
                    },
                    "name": "Yu Qi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T16:08:58.686Z",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f50",
                    "name": "Haibo Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f51",
                    "name": "Ziyu Guo",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f52",
                    "name": "Siyuan Ma",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f53",
                    "name": "Ziyan Chen",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f54",
                    "name": "Yaokun Han",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f55",
                    "name": "Renrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f56",
                    "name": "Zitiantao Lin",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f57",
                    "name": "Shiji Xin",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f58",
                    "name": "Yijian Huang",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f59",
                    "name": "Kai Cheng",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f5a",
                    "name": "Peiheng Wang",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f5b",
                    "name": "Jiazheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f5c",
                    "name": "Jiayi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f5d",
                    "name": "Yizhe Zhu",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f5e",
                    "name": "Wenqing Wang",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f5f",
                    "name": "Yiran Qin",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f60",
                    "name": "Xupeng Zhu",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f61",
                    "name": "Haojie Huang",
                    "hidden": false
                },
                {
                    "_id": "68ecdddfb0aa732b88f31f62",
                    "name": "Lawson L. S. Wong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T19:18:36.000Z",
            "submittedOnDailyAt": "2025-10-13T16:30:51.073Z",
            "title": "BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic\n  Embodied Capabilities",
            "submittedOnDailyBy": {
                "_id": "661de604f8dcbd5a207c9012",
                "avatarUrl": "/avatars/58f5689237dc33972703971642c8c8b1.svg",
                "isPro": false,
                "fullname": "yu",
                "user": "yqi19",
                "type": "user"
            },
            "summary": "Embodied capabilities refer to a suite of fundamental abilities for an agent\nto perceive, comprehend, and interact with the physical world. While multimodal\nlarge language models (MLLMs) show promise as embodied agents, a thorough and\nsystematic evaluation of their embodied capabilities remains underexplored, as\nexisting benchmarks primarily focus on specific domains such as planning or\nspatial understanding. To bridge this gap, we introduce BEAR, a comprehensive\nand fine-grained benchmark that evaluates MLLMs on atomic embodied\ncapabilities. BEAR comprises 4,469 interleaved image-video-text entries across\n14 domains in 6 categories, including tasks from low-level pointing, trajectory\nunderstanding, spatial reasoning, to high-level planning. Extensive evaluation\nresults of 20 representative MLLMs reveal their persistent limitations across\nall domains of embodied capabilities. To tackle the shortfall, we propose\nBEAR-Agent, a multimodal conversable agent that integrates pretrained vision\nmodels to strengthen MLLM perception, 3D understanding, and planning\ncapabilities. It substantially enhances MLLM performance across diverse\nembodied capabilities on BEAR, yielding a 9.12% absolute gain and a relative\nimprovement of 17.5% on GPT-5. Furthermore, our experiments indicate that\nimproving MLLM embodied capabilities can benefit embodied tasks in simulated\nenvironments. Project website: https://bear-official66.github.io/",
            "upvotes": 19,
            "discussionId": "68ecdddfb0aa732b88f31f63",
            "projectPage": "https://bear-official66.github.io/",
            "githubRepo": "https://github.com/yqi19/BEAR-official",
            "ai_summary": "BEAR is a comprehensive benchmark evaluating multimodal large language models' embodied capabilities, and BEAR-Agent enhances these models by integrating pretrained vision models, improving performance across various tasks.",
            "ai_keywords": [
                "multimodal large language models",
                "embodied capabilities",
                "BEAR",
                "image-video-text entries",
                "trajectory understanding",
                "spatial reasoning",
                "planning capabilities",
                "BEAR-Agent",
                "pretrained vision models",
                "3D understanding"
            ],
            "githubStars": 14,
            "organization": {
                "_id": "61dcd8e344f59573371b5cb6",
                "name": "PekingUniversity",
                "fullname": "Peking University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
            }
        },
        "publishedAt": "2025-10-09T15:18:36.000Z",
        "title": "BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic\n  Embodied Capabilities",
        "summary": "Embodied capabilities refer to a suite of fundamental abilities for an agent\nto perceive, comprehend, and interact with the physical world. While multimodal\nlarge language models (MLLMs) show promise as embodied agents, a thorough and\nsystematic evaluation of their embodied capabilities remains underexplored, as\nexisting benchmarks primarily focus on specific domains such as planning or\nspatial understanding. To bridge this gap, we introduce BEAR, a comprehensive\nand fine-grained benchmark that evaluates MLLMs on atomic embodied\ncapabilities. BEAR comprises 4,469 interleaved image-video-text entries across\n14 domains in 6 categories, including tasks from low-level pointing, trajectory\nunderstanding, spatial reasoning, to high-level planning. Extensive evaluation\nresults of 20 representative MLLMs reveal their persistent limitations across\nall domains of embodied capabilities. To tackle the shortfall, we propose\nBEAR-Agent, a multimodal conversable agent that integrates pretrained vision\nmodels to strengthen MLLM perception, 3D understanding, and planning\ncapabilities. It substantially enhances MLLM performance across diverse\nembodied capabilities on BEAR, yielding a 9.12% absolute gain and a relative\nimprovement of 17.5% on GPT-5. Furthermore, our experiments indicate that\nimproving MLLM embodied capabilities can benefit embodied tasks in simulated\nenvironments. Project website: https://bear-official66.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08759.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "661de604f8dcbd5a207c9012",
            "avatarUrl": "/avatars/58f5689237dc33972703971642c8c8b1.svg",
            "fullname": "yu",
            "name": "yqi19",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.09606",
            "authors": [
                {
                    "_id": "68ec5975cd07fb414898c8ea",
                    "user": {
                        "_id": "667a518d58120f1b6ac579e8",
                        "avatarUrl": "/avatars/3e7d0e3d1e659ec29c0fca3e79df798e.svg",
                        "isPro": false,
                        "fullname": "Peiwen Sun",
                        "user": "spw2000",
                        "type": "user"
                    },
                    "name": "Peiwen Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:06:05.983Z",
                    "hidden": false
                },
                {
                    "_id": "68ec5975cd07fb414898c8eb",
                    "name": "Shiqiang Lang",
                    "hidden": false
                },
                {
                    "_id": "68ec5975cd07fb414898c8ec",
                    "name": "Dongming Wu",
                    "hidden": false
                },
                {
                    "_id": "68ec5975cd07fb414898c8ed",
                    "name": "Yi Ding",
                    "hidden": false
                },
                {
                    "_id": "68ec5975cd07fb414898c8ee",
                    "name": "Kaituo Feng",
                    "hidden": false
                },
                {
                    "_id": "68ec5975cd07fb414898c8ef",
                    "name": "Huadai Liu",
                    "hidden": false
                },
                {
                    "_id": "68ec5975cd07fb414898c8f0",
                    "name": "Zhen Ye",
                    "hidden": false
                },
                {
                    "_id": "68ec5975cd07fb414898c8f1",
                    "name": "Rui Liu",
                    "hidden": false
                },
                {
                    "_id": "68ec5975cd07fb414898c8f2",
                    "name": "Yun-Hui Liu",
                    "hidden": false
                },
                {
                    "_id": "68ec5975cd07fb414898c8f3",
                    "name": "Jianan Wang",
                    "hidden": false
                },
                {
                    "_id": "68ec5975cd07fb414898c8f4",
                    "name": "Xiangyu Yue",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T17:59:46.000Z",
            "submittedOnDailyAt": "2025-10-13T00:14:31.615Z",
            "title": "SpaceVista: All-Scale Visual Spatial Reasoning from mm to km",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "With the current surge in spatial reasoning explorations, researchers have\nmade significant progress in understanding indoor scenes, but still struggle\nwith diverse applications such as robotics and autonomous driving. This paper\naims to advance all-scale spatial reasoning across diverse scenarios by\ntackling two key challenges: 1) the heavy reliance on indoor 3D scans and\nlabor-intensive manual annotations for dataset curation; 2) the absence of\neffective all-scale scene modeling, which often leads to overfitting to\nindividual scenes. In this paper, we introduce a holistic solution that\nintegrates a structured spatial reasoning knowledge system, scale-aware\nmodeling, and a progressive training paradigm, as the first attempt to broaden\nthe all-scale spatial intelligence of MLLMs to the best of our knowledge. Using\na task-specific, specialist-driven automated pipeline, we curate over 38K video\nscenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising\napproximately 1M spatial QA pairs spanning 19 diverse task types. While\nspecialist models can inject useful domain knowledge, they are not reliable for\nevaluation. We then build an all-scale benchmark with precise annotations by\nmanually recording, retrieving, and assembling video-based data. However, naive\ntraining with SpaceVista-1M often yields suboptimal results due to the\npotential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a\nspatial reasoning model that accepts dense inputs beyond semantics and uses\nscale as an anchor for scale-aware experts and progressive rewards. Finally,\nextensive evaluations across 5 benchmarks, including our SpaceVista-Bench,\ndemonstrate competitive performance, showcasing strong generalization across\nall scales and scenarios. Our dataset, model, and benchmark will be released on\nhttps://peiwensun2000.github.io/mm2km .",
            "upvotes": 16,
            "discussionId": "68ec5975cd07fb414898c8f5",
            "projectPage": "https://peiwensun2000.github.io/mm2km/",
            "githubRepo": "https://github.com/PeiwenSun2000/SpaceVista",
            "ai_summary": "A spatial reasoning model using scale-aware experts and progressive rewards demonstrates competitive performance across diverse tasks and scales using a large, curated dataset.",
            "ai_keywords": [
                "structured spatial reasoning",
                "scale-aware modeling",
                "progressive training",
                "spatial QA pairs",
                "spatial reasoning model",
                "scale-aware experts",
                "progressive rewards",
                "SpaceVista-1M",
                "SpaceVista-7B",
                "SpaceVista-Bench"
            ],
            "githubStars": 23
        },
        "publishedAt": "2025-10-10T13:59:46.000Z",
        "title": "SpaceVista: All-Scale Visual Spatial Reasoning from mm to km",
        "summary": "With the current surge in spatial reasoning explorations, researchers have\nmade significant progress in understanding indoor scenes, but still struggle\nwith diverse applications such as robotics and autonomous driving. This paper\naims to advance all-scale spatial reasoning across diverse scenarios by\ntackling two key challenges: 1) the heavy reliance on indoor 3D scans and\nlabor-intensive manual annotations for dataset curation; 2) the absence of\neffective all-scale scene modeling, which often leads to overfitting to\nindividual scenes. In this paper, we introduce a holistic solution that\nintegrates a structured spatial reasoning knowledge system, scale-aware\nmodeling, and a progressive training paradigm, as the first attempt to broaden\nthe all-scale spatial intelligence of MLLMs to the best of our knowledge. Using\na task-specific, specialist-driven automated pipeline, we curate over 38K video\nscenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising\napproximately 1M spatial QA pairs spanning 19 diverse task types. While\nspecialist models can inject useful domain knowledge, they are not reliable for\nevaluation. We then build an all-scale benchmark with precise annotations by\nmanually recording, retrieving, and assembling video-based data. However, naive\ntraining with SpaceVista-1M often yields suboptimal results due to the\npotential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a\nspatial reasoning model that accepts dense inputs beyond semantics and uses\nscale as an anchor for scale-aware experts and progressive rewards. Finally,\nextensive evaluations across 5 benchmarks, including our SpaceVista-Bench,\ndemonstrate competitive performance, showcasing strong generalization across\nall scales and scenarios. Our dataset, model, and benchmark will be released on\nhttps://peiwensun2000.github.io/mm2km .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09606.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 126
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.09426",
            "authors": [
                {
                    "_id": "68ec6e86cd07fb414898c9af",
                    "user": {
                        "_id": "6435a57b2d0ed796668d8a3f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6435a57b2d0ed796668d8a3f/fsOmfjqCS9TkI9NMjCFM2.png",
                        "isPro": true,
                        "fullname": "min jun kim",
                        "user": "mjkmain",
                        "type": "user"
                    },
                    "name": "Minjun Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:05:34.054Z",
                    "hidden": false
                },
                {
                    "_id": "68ec6e86cd07fb414898c9b0",
                    "name": "Hyeonseok Lim",
                    "hidden": false
                },
                {
                    "_id": "68ec6e86cd07fb414898c9b1",
                    "user": {
                        "_id": "64be114c12afb2f1194cf108",
                        "avatarUrl": "/avatars/ae00fde99b95fcc3a95aae8a558eb049.svg",
                        "isPro": false,
                        "fullname": "Hangyeol Yoo",
                        "user": "PerRing",
                        "type": "user"
                    },
                    "name": "Hangyeol Yoo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:05:31.900Z",
                    "hidden": false
                },
                {
                    "_id": "68ec6e86cd07fb414898c9b2",
                    "name": "Inho Won",
                    "hidden": false
                },
                {
                    "_id": "68ec6e86cd07fb414898c9b3",
                    "user": {
                        "_id": "654c5e461fbef019f2ccbc3c",
                        "avatarUrl": "/avatars/c0527f73e36dd580b3ecb17efb11260c.svg",
                        "isPro": true,
                        "fullname": "SeungWoo Song",
                        "user": "sswoo123",
                        "type": "user"
                    },
                    "name": "Seungwoo Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:05:29.638Z",
                    "hidden": false
                },
                {
                    "_id": "68ec6e86cd07fb414898c9b4",
                    "name": "Minkyung Cho",
                    "hidden": false
                },
                {
                    "_id": "68ec6e86cd07fb414898c9b5",
                    "name": "Junhun Yuk",
                    "hidden": false
                },
                {
                    "_id": "68ec6e86cd07fb414898c9b6",
                    "name": "Changsu Choi",
                    "hidden": false
                },
                {
                    "_id": "68ec6e86cd07fb414898c9b7",
                    "name": "Dongjae Shin",
                    "hidden": false
                },
                {
                    "_id": "68ec6e86cd07fb414898c9b8",
                    "name": "Huige Lee",
                    "hidden": false
                },
                {
                    "_id": "68ec6e86cd07fb414898c9b9",
                    "name": "Hoyun Song",
                    "hidden": false
                },
                {
                    "_id": "68ec6e86cd07fb414898c9ba",
                    "name": "Alice Oh",
                    "hidden": false
                },
                {
                    "_id": "68ec6e86cd07fb414898c9bb",
                    "name": "Kyungtae Lim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T14:31:25.000Z",
            "submittedOnDailyAt": "2025-10-13T07:39:51.855Z",
            "title": "KORMo: Korean Open Reasoning Model for Everyone",
            "submittedOnDailyBy": {
                "_id": "6435a57b2d0ed796668d8a3f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6435a57b2d0ed796668d8a3f/fsOmfjqCS9TkI9NMjCFM2.png",
                "isPro": true,
                "fullname": "min jun kim",
                "user": "mjkmain",
                "type": "user"
            },
            "summary": "This work presents the first large-scale investigation into constructing a\nfully open bilingual large language model (LLM) for a non-English language,\nspecifically Korean, trained predominantly on synthetic data. We introduce\nKORMo-10B, a 10.8B-parameter model trained from scratch on a Korean-English\ncorpus in which 68.74% of the Korean portion is synthetic. Through systematic\nexperimentation, we demonstrate that synthetic data, when carefully curated\nwith balanced linguistic coverage and diverse instruction styles, does not\ncause instability or degradation during large-scale pretraining. Furthermore,\nthe model achieves performance comparable to that of contemporary open-weight\nmultilingual baselines across a wide range of reasoning, knowledge, and\ninstruction-following benchmarks. Our experiments reveal two key findings: (1)\nsynthetic data can reliably sustain long-horizon pretraining without model\ncollapse, and (2) bilingual instruction tuning enables near-native reasoning\nand discourse coherence in Korean. By fully releasing all components including\ndata, code, training recipes, and logs, this work establishes a transparent\nframework for developing synthetic data-driven fully open models (FOMs) in\nlow-resource settings and sets a reproducible precedent for future multilingual\nLLM research.",
            "upvotes": 15,
            "discussionId": "68ec6e86cd07fb414898c9bc",
            "ai_summary": "A large-scale investigation into constructing a fully open bilingual LLM for Korean using synthetic data demonstrates that such data can sustain pretraining and achieve performance comparable to multilingual baselines.",
            "ai_keywords": [
                "large language model",
                "LLM",
                "KORMo-10B",
                "synthetic data",
                "Korean-English corpus",
                "pretraining",
                "model collapse",
                "bilingual instruction tuning",
                "fully open models",
                "FOMs",
                "low-resource settings"
            ],
            "organization": {
                "_id": "68ea4461d7b8e72eb58b1b7f",
                "name": "KORMo-Team",
                "fullname": "KORMo",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/659f7345e98a198ba7fcb89e/PJ4L1myPXBIjzUSO6-T7i.png"
            }
        },
        "publishedAt": "2025-10-10T10:31:25.000Z",
        "title": "KORMo: Korean Open Reasoning Model for Everyone",
        "summary": "This work presents the first large-scale investigation into constructing a\nfully open bilingual large language model (LLM) for a non-English language,\nspecifically Korean, trained predominantly on synthetic data. We introduce\nKORMo-10B, a 10.8B-parameter model trained from scratch on a Korean-English\ncorpus in which 68.74% of the Korean portion is synthetic. Through systematic\nexperimentation, we demonstrate that synthetic data, when carefully curated\nwith balanced linguistic coverage and diverse instruction styles, does not\ncause instability or degradation during large-scale pretraining. Furthermore,\nthe model achieves performance comparable to that of contemporary open-weight\nmultilingual baselines across a wide range of reasoning, knowledge, and\ninstruction-following benchmarks. Our experiments reveal two key findings: (1)\nsynthetic data can reliably sustain long-horizon pretraining without model\ncollapse, and (2) bilingual instruction tuning enables near-native reasoning\nand discourse coherence in Korean. By fully releasing all components including\ndata, code, training recipes, and logs, this work establishes a transparent\nframework for developing synthetic data-driven fully open models (FOMs) in\nlow-resource settings and sets a reproducible precedent for future multilingual\nLLM research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09426.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6435a57b2d0ed796668d8a3f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6435a57b2d0ed796668d8a3f/fsOmfjqCS9TkI9NMjCFM2.png",
            "fullname": "min jun kim",
            "name": "mjkmain",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "organization": {
            "_id": "68ea4461d7b8e72eb58b1b7f",
            "name": "KORMo-Team",
            "fullname": "KORMo",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/659f7345e98a198ba7fcb89e/PJ4L1myPXBIjzUSO6-T7i.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08696",
            "authors": [
                {
                    "_id": "68ec56f8cd07fb414898c8d4",
                    "name": "Yunzhen Feng",
                    "hidden": false
                },
                {
                    "_id": "68ec56f8cd07fb414898c8d5",
                    "name": "Parag Jain",
                    "hidden": false
                },
                {
                    "_id": "68ec56f8cd07fb414898c8d6",
                    "name": "Anthony Hartshorn",
                    "hidden": false
                },
                {
                    "_id": "68ec56f8cd07fb414898c8d7",
                    "name": "Yaqi Duan",
                    "hidden": false
                },
                {
                    "_id": "68ec56f8cd07fb414898c8d8",
                    "name": "Julia Kempe",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T18:01:44.000Z",
            "submittedOnDailyAt": "2025-10-13T00:04:12.104Z",
            "title": "Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence\n  Reweighting",
            "submittedOnDailyBy": {
                "_id": "65cbfa6c968742be942e6cba",
                "avatarUrl": "/avatars/1a6cc0983edc28fa92178d3abc283ba1.svg",
                "isPro": false,
                "fullname": "Feng",
                "user": "Yunzhen",
                "type": "user"
            },
            "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a standard\nrecipe for improving large language models (LLMs) on reasoning tasks, with\nGroup Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO\nwastes substantial compute on negative groups: groups in which no sampled\nresponse is correct yield zero advantage and thus no gradient. We ask whether\nnegative groups can be leveraged without extra supervision. Starting from a\nmaximum-likelihood (MLE) objective in reward modeling, we show that the MLE\ngradient is equivalent to a policy gradient for a modified value function. This\nvalue function adds a confidence-weighted penalty on incorrect responses,\nimposing larger penalties on more confident mistakes. We refer to this as\nLikelihood Estimation with Negative Samples\n(LENS). LENS modifies GRPO to assign non-zero, confidence-dependent\nrewards to incorrect generations, making negative groups informative and\nconverting previously wasted samples into useful gradient updates. On the MATH\nbenchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently\noutperforms GRPO baseline, with significant gains on harder items. These\nresults demonstrate a principled and practical way to \"rescue\" negative groups,\nimproving efficiency and performance in RLVR.",
            "upvotes": 12,
            "discussionId": "68ec56f8cd07fb414898c8d9",
            "ai_summary": "LENS modifies GRPO by assigning confidence-dependent rewards to incorrect responses, improving efficiency and performance in reinforcement learning with verifiable rewards.",
            "ai_keywords": [
                "Reinforcement learning with verifiable rewards",
                "RLVR",
                "Group Relative Policy Optimization",
                "GRPO",
                "maximum-likelihood",
                "MLE",
                "policy gradient",
                "value function",
                "confidence-weighted penalty",
                "LENS",
                "MATH benchmark",
                "Llama-3.1-8B",
                "Qwen-2.5-3B"
            ]
        },
        "publishedAt": "2025-10-09T14:01:44.000Z",
        "title": "Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence\n  Reweighting",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a standard\nrecipe for improving large language models (LLMs) on reasoning tasks, with\nGroup Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO\nwastes substantial compute on negative groups: groups in which no sampled\nresponse is correct yield zero advantage and thus no gradient. We ask whether\nnegative groups can be leveraged without extra supervision. Starting from a\nmaximum-likelihood (MLE) objective in reward modeling, we show that the MLE\ngradient is equivalent to a policy gradient for a modified value function. This\nvalue function adds a confidence-weighted penalty on incorrect responses,\nimposing larger penalties on more confident mistakes. We refer to this as\nLikelihood Estimation with Negative Samples\n(LENS). LENS modifies GRPO to assign non-zero, confidence-dependent\nrewards to incorrect generations, making negative groups informative and\nconverting previously wasted samples into useful gradient updates. On the MATH\nbenchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently\noutperforms GRPO baseline, with significant gains on harder items. These\nresults demonstrate a principled and practical way to \"rescue\" negative groups,\nimproving efficiency and performance in RLVR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08696.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65cbfa6c968742be942e6cba",
            "avatarUrl": "/avatars/1a6cc0983edc28fa92178d3abc283ba1.svg",
            "fullname": "Feng",
            "name": "Yunzhen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08457",
            "authors": [
                {
                    "_id": "68ebaa94012880a10864bbee",
                    "user": {
                        "_id": "66cc19ad6f8945277c39cd86",
                        "avatarUrl": "/avatars/f031e77d7fb9dbebbc00fba9b5dd5357.svg",
                        "isPro": false,
                        "fullname": "Shawn",
                        "user": "csfufu",
                        "type": "user"
                    },
                    "name": "Shuang Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:06:21.942Z",
                    "hidden": false
                },
                {
                    "_id": "68ebaa94012880a10864bbef",
                    "name": "Yue Guo",
                    "hidden": false
                },
                {
                    "_id": "68ebaa94012880a10864bbf0",
                    "name": "Yimeng Ye",
                    "hidden": false
                },
                {
                    "_id": "68ebaa94012880a10864bbf1",
                    "user": {
                        "_id": "64ce05c631c655ff8a2e183c",
                        "avatarUrl": "/avatars/f2de7f8a1348b05f46946085e3e9718e.svg",
                        "isPro": false,
                        "fullname": "Shijue Huang",
                        "user": "JoeYing",
                        "type": "user"
                    },
                    "name": "Shijue Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:06:24.272Z",
                    "hidden": false
                },
                {
                    "_id": "68ebaa94012880a10864bbf2",
                    "name": "Wenbo Hu",
                    "hidden": false
                },
                {
                    "_id": "68ebaa94012880a10864bbf3",
                    "name": "Haoxi Li",
                    "hidden": false
                },
                {
                    "_id": "68ebaa94012880a10864bbf4",
                    "name": "Manyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ebaa94012880a10864bbf5",
                    "name": "Jiayu Chen",
                    "hidden": false
                },
                {
                    "_id": "68ebaa94012880a10864bbf6",
                    "name": "Song Guo",
                    "hidden": false
                },
                {
                    "_id": "68ebaa94012880a10864bbf7",
                    "name": "Nanyun Peng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T17:03:28.000Z",
            "submittedOnDailyAt": "2025-10-13T02:25:56.610Z",
            "title": "ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level\n  Entropy Shaping",
            "submittedOnDailyBy": {
                "_id": "64ce05c631c655ff8a2e183c",
                "avatarUrl": "/avatars/f2de7f8a1348b05f46946085e3e9718e.svg",
                "isPro": false,
                "fullname": "Shijue Huang",
                "user": "JoeYing",
                "type": "user"
            },
            "summary": "Recent advances in multimodal large reasoning models (MLRMs) have\nsubstantially improved their ability to solve complex textual and visual tasks.\nHowever, these models tend to overthink on simple problems, producing\nunnecessarily lengthy reasoning traces, while under-exploring on challenging\nones, leading to missed solutions. To address this imbalance, we propose ARES,\na unified open-source framework for adaptive reasoning that dynamically\nallocates exploration effort based on task difficulty. Our approach is\nmotivated by two key empirical findings: (i) while single-token entropy is\nnoisy, high window-entropy (HWE) tokens (token-level entropies averaged under a\nsliding window) can reliably capture reasoning-critical moments; and (ii)\nreducing HWE usage benefits easy problems, while increasing it is essential for\nsolving hard ones. Building on these insights, ARES introduces a two-stage\ntraining pipeline. In the Adaptive Cold-Start stage, we curate multimodal and\ntextual data paired with reasoning traces of length proportional to problem\ndifficulty, equipping the model with initial difficulty awareness. In the\nsecond stage, we develop Adaptive Entropy Policy Optimization (AEPO), which\nuses HWE tokens as exploration triggers to decide when to explore, and a\nhierarchical entropy reward with dynamic KL control to decide how much to\nexplore. Extensive experiments demonstrate that ARES achieves superior\nperformance and reasoning efficiency across diverse mathematical, logical, and\nmultimodal benchmarks, while closing the gap to leading commercial systems\nunder significantly lower inference costs.",
            "upvotes": 11,
            "discussionId": "68ebaa94012880a10864bbf8",
            "ai_summary": "ARES, a unified framework for adaptive reasoning, dynamically adjusts exploration effort based on task difficulty using high window-entropy tokens and hierarchical entropy rewards, improving performance and efficiency across various benchmarks.",
            "ai_keywords": [
                "multimodal large reasoning models",
                "MLRMs",
                "adaptive reasoning",
                "high window-entropy",
                "HWE",
                "Adaptive Cold-Start",
                "Adaptive Entropy Policy Optimization",
                "AEPO",
                "entropy reward",
                "dynamic KL control"
            ]
        },
        "publishedAt": "2025-10-09T13:03:28.000Z",
        "title": "ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level\n  Entropy Shaping",
        "summary": "Recent advances in multimodal large reasoning models (MLRMs) have\nsubstantially improved their ability to solve complex textual and visual tasks.\nHowever, these models tend to overthink on simple problems, producing\nunnecessarily lengthy reasoning traces, while under-exploring on challenging\nones, leading to missed solutions. To address this imbalance, we propose ARES,\na unified open-source framework for adaptive reasoning that dynamically\nallocates exploration effort based on task difficulty. Our approach is\nmotivated by two key empirical findings: (i) while single-token entropy is\nnoisy, high window-entropy (HWE) tokens (token-level entropies averaged under a\nsliding window) can reliably capture reasoning-critical moments; and (ii)\nreducing HWE usage benefits easy problems, while increasing it is essential for\nsolving hard ones. Building on these insights, ARES introduces a two-stage\ntraining pipeline. In the Adaptive Cold-Start stage, we curate multimodal and\ntextual data paired with reasoning traces of length proportional to problem\ndifficulty, equipping the model with initial difficulty awareness. In the\nsecond stage, we develop Adaptive Entropy Policy Optimization (AEPO), which\nuses HWE tokens as exploration triggers to decide when to explore, and a\nhierarchical entropy reward with dynamic KL control to decide how much to\nexplore. Extensive experiments demonstrate that ARES achieves superior\nperformance and reasoning efficiency across diverse mathematical, logical, and\nmultimodal benchmarks, while closing the gap to leading commercial systems\nunder significantly lower inference costs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08457.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ce05c631c655ff8a2e183c",
            "avatarUrl": "/avatars/f2de7f8a1348b05f46946085e3e9718e.svg",
            "fullname": "Shijue Huang",
            "name": "JoeYing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.07959",
            "authors": [
                {
                    "_id": "68eca176cd07fb414898cb10",
                    "name": "Alexander Rubinstein",
                    "hidden": false
                },
                {
                    "_id": "68eca176cd07fb414898cb11",
                    "name": "Benjamin Raible",
                    "hidden": false
                },
                {
                    "_id": "68eca176cd07fb414898cb12",
                    "user": {
                        "_id": "63ee35c3f599efc7a010c792",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676555600618-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Martin Gubri",
                        "user": "mgubri",
                        "type": "user"
                    },
                    "name": "Martin Gubri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:04:20.876Z",
                    "hidden": false
                },
                {
                    "_id": "68eca176cd07fb414898cb13",
                    "name": "Seong Joon Oh",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64198d7efdfc2970b350f48f/KzJwt3Jv-s5EonmEb4-TA.jpeg"
            ],
            "publishedAt": "2025-10-09T08:53:59.000Z",
            "submittedOnDailyAt": "2025-10-13T05:26:41.534Z",
            "title": "DISCO: Diversifying Sample Condensation for Efficient Model Evaluation",
            "submittedOnDailyBy": {
                "_id": "64198d7efdfc2970b350f48f",
                "avatarUrl": "/avatars/c0a0f30e1cbc22f1eb6bbc4549a5709c.svg",
                "isPro": false,
                "fullname": "Alexander Rubinstein",
                "user": "arubique",
                "type": "user"
            },
            "summary": "Evaluating modern machine learning models has become prohibitively expensive.\nBenchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model.\nCostly evaluation reduces inclusivity, slows the cycle of innovation, and\nworsens environmental impact. The typical approach follows two steps. First,\nselect an anchor subset of data. Second, train a mapping from the accuracy on\nthis subset to the final test result. The drawback is that anchor selection\ndepends on clustering, which can be complex and sensitive to design choices. We\nargue that promoting diversity among samples is not essential; what matters is\nto select samples that maximise diversity in model responses. Our\nmethod, Diversifying Sample Condensation (DISCO), selects the top-k\nsamples with the greatest model disagreements. This uses greedy, sample-wise\nstatistics rather than global clustering. The approach is conceptually simpler.\nFrom a theoretical view, inter-model disagreement provides an\ninformation-theoretically optimal rule for such greedy selection.\nDISCO shows empirical gains over prior methods, achieving\nstate-of-the-art results in performance prediction across MMLU, Hellaswag,\nWinogrande, and ARC. Code is available here:\nhttps://github.com/arubique/disco-public.",
            "upvotes": 11,
            "discussionId": "68eca177cd07fb414898cb14",
            "ai_summary": "A method called DISCO selects samples with the greatest model disagreements to predict performance, achieving state-of-the-art results across various benchmarks with reduced computational cost.",
            "ai_keywords": [
                "Diversifying Sample Condensation (DISCO)",
                "model disagreements",
                "greedy selection",
                "information-theoretically optimal rule",
                "MMLU",
                "Hellaswag",
                "Winogrande",
                "ARC"
            ],
            "organization": {
                "_id": "67efb00c6e51a83b10018b42",
                "name": "UniTuebingen",
                "fullname": "Eberhard Karls Universität Tübingen",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67efaed96e51a83b10013635/IBHUSQLny8BmeCdJro55K.png"
            }
        },
        "publishedAt": "2025-10-09T04:53:59.000Z",
        "title": "DISCO: Diversifying Sample Condensation for Efficient Model Evaluation",
        "summary": "Evaluating modern machine learning models has become prohibitively expensive.\nBenchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model.\nCostly evaluation reduces inclusivity, slows the cycle of innovation, and\nworsens environmental impact. The typical approach follows two steps. First,\nselect an anchor subset of data. Second, train a mapping from the accuracy on\nthis subset to the final test result. The drawback is that anchor selection\ndepends on clustering, which can be complex and sensitive to design choices. We\nargue that promoting diversity among samples is not essential; what matters is\nto select samples that maximise diversity in model responses. Our\nmethod, Diversifying Sample Condensation (DISCO), selects the top-k\nsamples with the greatest model disagreements. This uses greedy, sample-wise\nstatistics rather than global clustering. The approach is conceptually simpler.\nFrom a theoretical view, inter-model disagreement provides an\ninformation-theoretically optimal rule for such greedy selection.\nDISCO shows empirical gains over prior methods, achieving\nstate-of-the-art results in performance prediction across MMLU, Hellaswag,\nWinogrande, and ARC. Code is available here:\nhttps://github.com/arubique/disco-public.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64198d7efdfc2970b350f48f/KzJwt3Jv-s5EonmEb4-TA.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07959.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64198d7efdfc2970b350f48f",
            "avatarUrl": "/avatars/c0a0f30e1cbc22f1eb6bbc4549a5709c.svg",
            "fullname": "Alexander Rubinstein",
            "name": "arubique",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "67efb00c6e51a83b10018b42",
            "name": "UniTuebingen",
            "fullname": "Eberhard Karls Universität Tübingen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67efaed96e51a83b10013635/IBHUSQLny8BmeCdJro55K.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.06274",
            "authors": [
                {
                    "_id": "68ecbbbbcd07fb414898cc39",
                    "name": "Mohammad Mahdi Samiei Paqaleh",
                    "hidden": false
                },
                {
                    "_id": "68ecbbbbcd07fb414898cc3a",
                    "name": "Arash Marioriyad",
                    "hidden": false
                },
                {
                    "_id": "68ecbbbbcd07fb414898cc3b",
                    "user": {
                        "_id": "66aaa0b3ea309ca5ad17f49b",
                        "avatarUrl": "/avatars/a9e4a92e82a4bf8bf7a5add8735cb7d7.svg",
                        "isPro": false,
                        "fullname": "Arman Tahmasebi",
                        "user": "OstadTahmasb",
                        "type": "user"
                    },
                    "name": "Arman Tahmasebi-Zadeh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:03:51.188Z",
                    "hidden": false
                },
                {
                    "_id": "68ecbbbbcd07fb414898cc3c",
                    "name": "Mohamadreza Fereydooni",
                    "hidden": false
                },
                {
                    "_id": "68ecbbbbcd07fb414898cc3d",
                    "name": "Mahdi Ghaznavai",
                    "hidden": false
                },
                {
                    "_id": "68ecbbbbcd07fb414898cc3e",
                    "name": "Mahdieh Soleymani Baghshah",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-06T13:08:31.000Z",
            "submittedOnDailyAt": "2025-10-13T07:15:05.283Z",
            "title": "Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out\n  of Distribution Generalization",
            "submittedOnDailyBy": {
                "_id": "6404a0bf1a3babee78de3a49",
                "avatarUrl": "/avatars/700f0c71946cbe8ef2728526c1ef1683.svg",
                "isPro": false,
                "fullname": "Arash Mari Oriyad",
                "user": "arashmarioriyad",
                "type": "user"
            },
            "summary": "Recent progress has pushed AI frontiers from pattern recognition tasks toward\nproblems that require step by step, System2 style reasoning, especially with\nlarge language models. Yet, unlike learning, where generalization and out of\ndistribution (OoD) evaluation concepts are well formalized, there is no clear,\nconsistent definition or metric for reasoning ability. We propose Complexity\nOut of Distribution (Complexity OoD) generalization as a framework and problem\nsetting to define and measure reasoning. A model exhibits Complexity OoD\ngeneralization when it maintains performance on test instances whose minimal\nrequired solution complexity, either representational (richer solution\nstructure) or computational (more reasoning steps/program length), exceeds that\nof all training examples. We formalize complexity via solution description\nKolmogorov complexity and operational proxies (e.g., object/relation counts;\nreasoning step counts), clarifying how Complexity OoD differs from length and\ncompositional OoD. This lens unifies learning and reasoning: many cases\nsolvable with System1 like processing at low complexity become System2 like\nunder complexity pressure, while System2 can be viewed as generalization over\nsolution structures. We translate this perspective into practice with\nrecommendations for operationalizing Complexity OoD across the stack:\nincorporating complexity into benchmark and evaluation metric design,\nrethinking supervision to target solution traces, seeking and designing\ninductive biases for Complexity OoD generalization, addressing learning to\nreason spillovers such as spurious shortcuts, semantic robustness, catastrophic\nforgetting, and step wise calibration. Because Complexity OoD cannot be solved\nby scaling data alone, progress toward robust reasoning will require\narchitectures and training regimes that explicitly model and allocate\ncomputation with respect to complexity.",
            "upvotes": 9,
            "discussionId": "68ecbbbbcd07fb414898cc3f",
            "ai_summary": "A framework called Complexity Out of Distribution (Complexity OoD) is proposed to define and measure reasoning ability in AI models by evaluating their performance on test instances requiring higher solution complexity than training examples.",
            "ai_keywords": [
                "Complexity OoD",
                "generalization",
                "out of distribution",
                "Kolmogorov complexity",
                "operational proxies",
                "System1",
                "System2",
                "solution traces",
                "inductive biases",
                "spurious shortcuts",
                "semantic robustness",
                "catastrophic forgetting",
                "step-wise calibration"
            ]
        },
        "publishedAt": "2025-10-06T09:08:31.000Z",
        "title": "Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out\n  of Distribution Generalization",
        "summary": "Recent progress has pushed AI frontiers from pattern recognition tasks toward\nproblems that require step by step, System2 style reasoning, especially with\nlarge language models. Yet, unlike learning, where generalization and out of\ndistribution (OoD) evaluation concepts are well formalized, there is no clear,\nconsistent definition or metric for reasoning ability. We propose Complexity\nOut of Distribution (Complexity OoD) generalization as a framework and problem\nsetting to define and measure reasoning. A model exhibits Complexity OoD\ngeneralization when it maintains performance on test instances whose minimal\nrequired solution complexity, either representational (richer solution\nstructure) or computational (more reasoning steps/program length), exceeds that\nof all training examples. We formalize complexity via solution description\nKolmogorov complexity and operational proxies (e.g., object/relation counts;\nreasoning step counts), clarifying how Complexity OoD differs from length and\ncompositional OoD. This lens unifies learning and reasoning: many cases\nsolvable with System1 like processing at low complexity become System2 like\nunder complexity pressure, while System2 can be viewed as generalization over\nsolution structures. We translate this perspective into practice with\nrecommendations for operationalizing Complexity OoD across the stack:\nincorporating complexity into benchmark and evaluation metric design,\nrethinking supervision to target solution traces, seeking and designing\ninductive biases for Complexity OoD generalization, addressing learning to\nreason spillovers such as spurious shortcuts, semantic robustness, catastrophic\nforgetting, and step wise calibration. Because Complexity OoD cannot be solved\nby scaling data alone, progress toward robust reasoning will require\narchitectures and training regimes that explicitly model and allocate\ncomputation with respect to complexity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06274.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6404a0bf1a3babee78de3a49",
            "avatarUrl": "/avatars/700f0c71946cbe8ef2728526c1ef1683.svg",
            "fullname": "Arash Mari Oriyad",
            "name": "arashmarioriyad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08525",
            "authors": [
                {
                    "_id": "68ecd411b0aa732b88f31f43",
                    "user": {
                        "_id": "6624ce8248e016b5ea4ba952",
                        "avatarUrl": "/avatars/6bb405ba3a5a63de371b8dae49a87b65.svg",
                        "isPro": false,
                        "fullname": "wenjiedu",
                        "user": "Kurt232",
                        "type": "user"
                    },
                    "name": "Wenjie Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T11:05:02.293Z",
                    "hidden": false
                },
                {
                    "_id": "68ecd411b0aa732b88f31f44",
                    "name": "Li Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ecd411b0aa732b88f31f45",
                    "name": "Keda Tao",
                    "hidden": false
                },
                {
                    "_id": "68ecd411b0aa732b88f31f46",
                    "name": "Xue Liu",
                    "hidden": false
                },
                {
                    "_id": "68ecd411b0aa732b88f31f47",
                    "name": "Huan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T17:50:00.000Z",
            "submittedOnDailyAt": "2025-10-13T08:59:23.282Z",
            "title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression",
            "submittedOnDailyBy": {
                "_id": "6624ce8248e016b5ea4ba952",
                "avatarUrl": "/avatars/6bb405ba3a5a63de371b8dae49a87b65.svg",
                "isPro": false,
                "fullname": "wenjiedu",
                "user": "Kurt232",
                "type": "user"
            },
            "summary": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results.",
            "upvotes": 7,
            "discussionId": "68ecd412b0aa732b88f31f48",
            "ai_summary": "A reinforcement learning framework identifies and prioritizes critical attention heads for efficient KV cache compression in large language models, maintaining reasoning quality with reduced overhead.",
            "ai_keywords": [
                "chain-of-thought generation",
                "KV cache",
                "token-dropping",
                "head-reallocating",
                "attention heads",
                "functional heterogeneity",
                "RLKV",
                "reinforcement learning",
                "cache usage",
                "reasoning quality",
                "KV compression"
            ]
        },
        "publishedAt": "2025-10-09T13:50:00.000Z",
        "title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression",
        "summary": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08525.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6624ce8248e016b5ea4ba952",
            "avatarUrl": "/avatars/6bb405ba3a5a63de371b8dae49a87b65.svg",
            "fullname": "wenjiedu",
            "name": "Kurt232",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.04759",
            "authors": [
                {
                    "_id": "68e9249395e8e6771df38ce2",
                    "user": {
                        "_id": "68de2ad5e795997c50a409eb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68de2ad5e795997c50a409eb/VdVD6njmMlMbZIqe23cxT.jpeg",
                        "isPro": false,
                        "fullname": "YAN, Chi",
                        "user": "yanchi3dv",
                        "type": "user"
                    },
                    "name": "Chi Yan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-11T13:45:21.924Z",
                    "hidden": false
                },
                {
                    "_id": "68e9249395e8e6771df38ce3",
                    "user": {
                        "_id": "66feab48651e00e22f33222e",
                        "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
                        "isPro": false,
                        "fullname": "Dan Xu",
                        "user": "danxuhk",
                        "type": "user"
                    },
                    "name": "Dan Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-11T13:45:14.344Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-06T12:36:07.000Z",
            "submittedOnDailyAt": "2025-10-13T00:32:19.506Z",
            "title": "Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open\n  Vocabulary Occupancy Prediction",
            "submittedOnDailyBy": {
                "_id": "68de2ad5e795997c50a409eb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68de2ad5e795997c50a409eb/VdVD6njmMlMbZIqe23cxT.jpeg",
                "isPro": false,
                "fullname": "YAN, Chi",
                "user": "yanchi3dv",
                "type": "user"
            },
            "summary": "The 3D occupancy prediction task has witnessed remarkable progress in recent\nyears, playing a crucial role in vision-based autonomous driving systems. While\ntraditional methods are limited to fixed semantic categories, recent approaches\nhave moved towards predicting text-aligned features to enable open-vocabulary\ntext queries in real-world scenes. However, there exists a trade-off in\ntext-aligned scene modeling: sparse Gaussian representation struggles to\ncapture small objects in the scene, while dense representation incurs\nsignificant computational overhead. To address these limitations, we present\nPG-Occ, an innovative Progressive Gaussian Transformer Framework that enables\nopen-vocabulary 3D occupancy prediction. Our framework employs progressive\nonline densification, a feed-forward strategy that gradually enhances the 3D\nGaussian representation to capture fine-grained scene details. By iteratively\nenhancing the representation, the framework achieves increasingly precise and\ndetailed scene understanding. Another key contribution is the introduction of\nan anisotropy-aware sampling strategy with spatio-temporal fusion, which\nadaptively assigns receptive fields to Gaussians at different scales and\nstages, enabling more effective feature aggregation and richer scene\ninformation capture. Through extensive evaluations, we demonstrate that PG-Occ\nachieves state-of-the-art performance with a relative 14.3% mIoU improvement\nover the previous best performing method. Code and pretrained models will be\nreleased upon publication on our project page:\nhttps://yanchi-3dv.github.io/PG-Occ",
            "upvotes": 7,
            "discussionId": "68e9249495e8e6771df38ce4",
            "projectPage": "https://yanchi-3dv.github.io/PG-Occ/",
            "githubRepo": "https://github.com/yanchi-3dv/PG-Occ",
            "ai_summary": "PG-Occ, a Progressive Gaussian Transformer Framework, enhances 3D occupancy prediction with progressive densification and anisotropy-aware sampling, achieving state-of-the-art performance.",
            "ai_keywords": [
                "3D occupancy prediction",
                "Gaussian representation",
                "dense representation",
                "Progressive Gaussian Transformer Framework",
                "progressive online densification",
                "feed-forward strategy",
                "anisotropy-aware sampling",
                "spatio-temporal fusion",
                "mIoU improvement"
            ],
            "githubStars": 15
        },
        "publishedAt": "2025-10-06T08:36:07.000Z",
        "title": "Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open\n  Vocabulary Occupancy Prediction",
        "summary": "The 3D occupancy prediction task has witnessed remarkable progress in recent\nyears, playing a crucial role in vision-based autonomous driving systems. While\ntraditional methods are limited to fixed semantic categories, recent approaches\nhave moved towards predicting text-aligned features to enable open-vocabulary\ntext queries in real-world scenes. However, there exists a trade-off in\ntext-aligned scene modeling: sparse Gaussian representation struggles to\ncapture small objects in the scene, while dense representation incurs\nsignificant computational overhead. To address these limitations, we present\nPG-Occ, an innovative Progressive Gaussian Transformer Framework that enables\nopen-vocabulary 3D occupancy prediction. Our framework employs progressive\nonline densification, a feed-forward strategy that gradually enhances the 3D\nGaussian representation to capture fine-grained scene details. By iteratively\nenhancing the representation, the framework achieves increasingly precise and\ndetailed scene understanding. Another key contribution is the introduction of\nan anisotropy-aware sampling strategy with spatio-temporal fusion, which\nadaptively assigns receptive fields to Gaussians at different scales and\nstages, enabling more effective feature aggregation and richer scene\ninformation capture. Through extensive evaluations, we demonstrate that PG-Occ\nachieves state-of-the-art performance with a relative 14.3% mIoU improvement\nover the previous best performing method. Code and pretrained models will be\nreleased upon publication on our project page:\nhttps://yanchi-3dv.github.io/PG-Occ",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04759.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68de2ad5e795997c50a409eb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68de2ad5e795997c50a409eb/VdVD6njmMlMbZIqe23cxT.jpeg",
            "fullname": "YAN, Chi",
            "name": "yanchi3dv",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.09517",
            "authors": [
                {
                    "_id": "68ec7109cd07fb414898c9da",
                    "name": "Yuchen Lu",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9db",
                    "name": "Run Yang",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9dc",
                    "name": "Yichen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9dd",
                    "name": "Shuguang Yu",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9de",
                    "user": {
                        "_id": "65037565da2d88e201f63b7a",
                        "avatarUrl": "/avatars/d1b6ce17236360e9583b8bb4cb87e506.svg",
                        "isPro": true,
                        "fullname": "Runpeng Dai",
                        "user": "Leo-Dai",
                        "type": "user"
                    },
                    "name": "Runpeng Dai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:05:24.400Z",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9df",
                    "name": "Ziwei Wang",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9e0",
                    "name": "Jiayi Xiang",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9e1",
                    "name": "Wenxin E",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9e2",
                    "name": "Siran Gao",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9e3",
                    "name": "Xinyao Ruan",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9e4",
                    "name": "Yirui Huang",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9e5",
                    "name": "Chenjing Xi",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9e6",
                    "name": "Haibo Hu",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9e7",
                    "name": "Yueming Fu",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9e8",
                    "name": "Qinglan Yu",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9e9",
                    "name": "Xiaobing Wei",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9ea",
                    "name": "Jiani Gu",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9eb",
                    "name": "Rui Sun",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9ec",
                    "name": "Jiaxuan Jia",
                    "hidden": false
                },
                {
                    "_id": "68ec7109cd07fb414898c9ed",
                    "name": "Fan Zhou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65037565da2d88e201f63b7a/2h1nHiFO7alGKj79s1cyr.png"
            ],
            "publishedAt": "2025-10-10T16:28:43.000Z",
            "submittedOnDailyAt": "2025-10-13T01:54:58.129Z",
            "title": "StatEval: A Comprehensive Benchmark for Large Language Models in\n  Statistics",
            "submittedOnDailyBy": {
                "_id": "65037565da2d88e201f63b7a",
                "avatarUrl": "/avatars/d1b6ce17236360e9583b8bb4cb87e506.svg",
                "isPro": true,
                "fullname": "Runpeng Dai",
                "user": "Leo-Dai",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have demonstrated remarkable advances in\nmathematical and logical reasoning, yet statistics, as a distinct and\nintegrative discipline, remains underexplored in benchmarking efforts. To\naddress this gap, we introduce StatEval, the first comprehensive\nbenchmark dedicated to statistics, spanning both breadth and depth across\ndifficulty levels. StatEval consists of 13,817 foundational problems covering\nundergraduate and graduate curricula, together with 2374 research-level proof\ntasks extracted from leading journals. To construct the benchmark, we design a\nscalable multi-agent pipeline with human-in-the-loop validation that automates\nlarge-scale problem extraction, rewriting, and quality control, while ensuring\nacademic rigor. We further propose a robust evaluation framework tailored to\nboth computational and proof-based tasks, enabling fine-grained assessment of\nreasoning ability. Experimental results reveal that while closed-source models\nsuch as GPT5-mini achieve below 57\\% on research-level problems, with\nopen-source models performing significantly lower. These findings highlight the\nunique challenges of statistical reasoning and the limitations of current LLMs.\nWe expect StatEval to serve as a rigorous benchmark for advancing statistical\nintelligence in large language models. All data and code are available on our\nweb platform: https://stateval.github.io/.",
            "upvotes": 6,
            "discussionId": "68ec7109cd07fb414898c9ee",
            "ai_summary": "StatEval is a comprehensive benchmark for statistical reasoning, covering foundational and research-level problems, and highlights the limitations of current LLMs in this domain.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "mathematical reasoning",
                "logical reasoning",
                "statistics",
                "benchmark",
                "foundational problems",
                "research-level proof tasks",
                "multi-agent pipeline",
                "human-in-the-loop validation",
                "evaluation framework",
                "computational tasks",
                "proof-based tasks",
                "reasoning ability",
                "GPT5-mini",
                "statistical intelligence"
            ],
            "organization": {
                "_id": "641ba0cea63c4e8062373b0c",
                "name": "SUFE",
                "fullname": "Shanghai University of Finance and Economics",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641b9fdbaebaa27e0752f494/VeZw5Qs9k0KkqujT7aLJQ.jpeg"
            }
        },
        "publishedAt": "2025-10-10T12:28:43.000Z",
        "title": "StatEval: A Comprehensive Benchmark for Large Language Models in\n  Statistics",
        "summary": "Large language models (LLMs) have demonstrated remarkable advances in\nmathematical and logical reasoning, yet statistics, as a distinct and\nintegrative discipline, remains underexplored in benchmarking efforts. To\naddress this gap, we introduce StatEval, the first comprehensive\nbenchmark dedicated to statistics, spanning both breadth and depth across\ndifficulty levels. StatEval consists of 13,817 foundational problems covering\nundergraduate and graduate curricula, together with 2374 research-level proof\ntasks extracted from leading journals. To construct the benchmark, we design a\nscalable multi-agent pipeline with human-in-the-loop validation that automates\nlarge-scale problem extraction, rewriting, and quality control, while ensuring\nacademic rigor. We further propose a robust evaluation framework tailored to\nboth computational and proof-based tasks, enabling fine-grained assessment of\nreasoning ability. Experimental results reveal that while closed-source models\nsuch as GPT5-mini achieve below 57\\% on research-level problems, with\nopen-source models performing significantly lower. These findings highlight the\nunique challenges of statistical reasoning and the limitations of current LLMs.\nWe expect StatEval to serve as a rigorous benchmark for advancing statistical\nintelligence in large language models. All data and code are available on our\nweb platform: https://stateval.github.io/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65037565da2d88e201f63b7a/2h1nHiFO7alGKj79s1cyr.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09517.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65037565da2d88e201f63b7a",
            "avatarUrl": "/avatars/d1b6ce17236360e9583b8bb4cb87e506.svg",
            "fullname": "Runpeng Dai",
            "name": "Leo-Dai",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "641ba0cea63c4e8062373b0c",
            "name": "SUFE",
            "fullname": "Shanghai University of Finance and Economics",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641b9fdbaebaa27e0752f494/VeZw5Qs9k0KkqujT7aLJQ.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.09510",
            "authors": [
                {
                    "_id": "68ec7396cd07fb414898ca08",
                    "name": "Siyue Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ec7396cd07fb414898ca09",
                    "user": {
                        "_id": "6720a6d52890caca00aae9e7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/A8h2DRemFRQit6NRAhkSv.png",
                        "isPro": false,
                        "fullname": "Yuan Gao",
                        "user": "ItzYog",
                        "type": "user"
                    },
                    "name": "Yuan Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:05:22.025Z",
                    "hidden": false
                },
                {
                    "_id": "68ec7396cd07fb414898ca0a",
                    "name": "Xiao Zhou",
                    "hidden": false
                },
                {
                    "_id": "68ec7396cd07fb414898ca0b",
                    "name": "Yilun Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ec7396cd07fb414898ca0c",
                    "name": "Tingyu Song",
                    "hidden": false
                },
                {
                    "_id": "68ec7396cd07fb414898ca0d",
                    "name": "Arman Cohan",
                    "hidden": false
                },
                {
                    "_id": "68ec7396cd07fb414898ca0e",
                    "name": "Anh Tuan Luu",
                    "hidden": false
                },
                {
                    "_id": "68ec7396cd07fb414898ca0f",
                    "name": "Chen Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T16:14:56.000Z",
            "submittedOnDailyAt": "2025-10-13T02:06:35.111Z",
            "title": "MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for\n  Reasoning-Intensive Multimodal Retrieval",
            "submittedOnDailyBy": {
                "_id": "638f1803c67af472d317a922",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638f1803c67af472d317a922/9BMVXqHa-AsdZPmBprcbd.jpeg",
                "isPro": false,
                "fullname": "siyue zhang",
                "user": "siyue",
                "type": "user"
            },
            "summary": "We introduce MRMR, the first expert-level multidisciplinary multimodal\nretrieval benchmark requiring intensive reasoning. MRMR contains 1,502 queries\nspanning 23 domains, with positive documents carefully verified by human\nexperts. Compared to prior benchmarks, MRMR introduces three key advancements.\nFirst, it challenges retrieval systems across diverse areas of expertise,\nenabling fine-grained model comparison across domains. Second, queries are\nreasoning-intensive, with images requiring deeper interpretation such as\ndiagnosing microscopic slides. We further introduce Contradiction Retrieval, a\nnovel task requiring models to identify conflicting concepts. Finally, queries\nand documents are constructed as image-text interleaved sequences. Unlike\nearlier benchmarks restricted to single images or unimodal documents, MRMR\noffers a realistic setting with multi-image queries and mixed-modality corpus\ndocuments. We conduct an extensive evaluation of 4 categories of multimodal\nretrieval systems and 14 frontier models on MRMR. The text embedding model\nQwen3-Embedding with LLM-generated image captions achieves the highest\nperformance, highlighting substantial room for improving multimodal retrieval\nmodels. Although latest multimodal models such as Ops-MM-Embedding perform\ncompetitively on expert-domain queries, they fall short on reasoning-intensive\ntasks. We believe that MRMR paves the way for advancing multimodal retrieval in\nmore realistic and challenging scenarios.",
            "upvotes": 6,
            "discussionId": "68ec7397cd07fb414898ca10",
            "ai_summary": "MRMR is a benchmark for expert-level multidisciplinary multimodal retrieval that includes reasoning-intensive tasks, contradiction retrieval, and image-text interleaved sequences, highlighting the need for improved multimodal models.",
            "ai_keywords": [
                "multimodal retrieval",
                "expert-level",
                "multidisciplinary",
                "reasoning-intensive",
                "Contradiction Retrieval",
                "image-text interleaved",
                "multimodal models",
                "text embedding",
                "LLM-generated image captions",
                "Ops-MM-Embedding"
            ]
        },
        "publishedAt": "2025-10-10T12:14:56.000Z",
        "title": "MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for\n  Reasoning-Intensive Multimodal Retrieval",
        "summary": "We introduce MRMR, the first expert-level multidisciplinary multimodal\nretrieval benchmark requiring intensive reasoning. MRMR contains 1,502 queries\nspanning 23 domains, with positive documents carefully verified by human\nexperts. Compared to prior benchmarks, MRMR introduces three key advancements.\nFirst, it challenges retrieval systems across diverse areas of expertise,\nenabling fine-grained model comparison across domains. Second, queries are\nreasoning-intensive, with images requiring deeper interpretation such as\ndiagnosing microscopic slides. We further introduce Contradiction Retrieval, a\nnovel task requiring models to identify conflicting concepts. Finally, queries\nand documents are constructed as image-text interleaved sequences. Unlike\nearlier benchmarks restricted to single images or unimodal documents, MRMR\noffers a realistic setting with multi-image queries and mixed-modality corpus\ndocuments. We conduct an extensive evaluation of 4 categories of multimodal\nretrieval systems and 14 frontier models on MRMR. The text embedding model\nQwen3-Embedding with LLM-generated image captions achieves the highest\nperformance, highlighting substantial room for improving multimodal retrieval\nmodels. Although latest multimodal models such as Ops-MM-Embedding perform\ncompetitively on expert-domain queries, they fall short on reasoning-intensive\ntasks. We believe that MRMR paves the way for advancing multimodal retrieval in\nmore realistic and challenging scenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09510.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638f1803c67af472d317a922",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638f1803c67af472d317a922/9BMVXqHa-AsdZPmBprcbd.jpeg",
            "fullname": "siyue zhang",
            "name": "siyue",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.09577",
            "authors": [
                {
                    "_id": "68ec56edcd07fb414898c8c9",
                    "name": "Xiao Yu",
                    "hidden": false
                },
                {
                    "_id": "68ec56edcd07fb414898c8ca",
                    "name": "Baolin Peng",
                    "hidden": false
                },
                {
                    "_id": "68ec56edcd07fb414898c8cb",
                    "name": "Michel Galley",
                    "hidden": false
                },
                {
                    "_id": "68ec56edcd07fb414898c8cc",
                    "name": "Hao Cheng",
                    "hidden": false
                },
                {
                    "_id": "68ec56edcd07fb414898c8cd",
                    "name": "Qianhui Wu",
                    "hidden": false
                },
                {
                    "_id": "68ec56edcd07fb414898c8ce",
                    "name": "Janardhan Kulkarni",
                    "hidden": false
                },
                {
                    "_id": "68ec56edcd07fb414898c8cf",
                    "name": "Suman Nath",
                    "hidden": false
                },
                {
                    "_id": "68ec56edcd07fb414898c8d0",
                    "name": "Zhou Yu",
                    "hidden": false
                },
                {
                    "_id": "68ec56edcd07fb414898c8d1",
                    "name": "Jianfeng Gao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T17:30:18.000Z",
            "submittedOnDailyAt": "2025-10-13T00:04:14.137Z",
            "title": "Dyna-Mind: Learning to Simulate from Experience for Better AI Agents",
            "submittedOnDailyBy": {
                "_id": "6234fd736dcfc5fe9f5b8601",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1647639915850-noauth.jpeg",
                "isPro": false,
                "fullname": "Xiao Yu",
                "user": "jasonyux",
                "type": "user"
            },
            "summary": "Reasoning models have recently shown remarkable progress in domains such as\nmath and coding. However, their expert-level abilities in math and coding\ncontrast sharply with their performance in long-horizon, interactive tasks such\nas web navigation and computer/phone-use. Inspired by literature on human\ncognition, we argue that current AI agents need ''vicarious trial and error'' -\nthe capacity to mentally simulate alternative futures before acting - in order\nto enhance their understanding and performance in complex interactive\nenvironments. We introduce Dyna-Mind, a two-stage training framework that\nexplicitly teaches (V)LM agents to integrate such simulation into their\nreasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which\ntrains the agent to generate structured reasoning traces from expanded search\ntrees built from real experience gathered through environment interactions.\nReSim thus grounds the agent's reasoning in faithful world dynamics and equips\nit with the ability to anticipate future states in its reasoning. In stage 2,\nwe propose Dyna-GRPO, an online reinforcement learning method to further\nstrengthen the agent's simulation and decision-making ability by using both\noutcome rewards and intermediate states as feedback from real rollouts.\nExperiments on two synthetic benchmarks (Sokoban and ALFWorld) and one\nrealistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively\ninfuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome\nand interaction-level signals to learn better policies for long-horizon,\nplanning-intensive tasks. Together, these results highlight the central role of\nsimulation in enabling AI agents to reason, plan, and act more effectively in\nthe ever more challenging environments.",
            "upvotes": 5,
            "discussionId": "68ec56edcd07fb414898c8d2",
            "ai_summary": "Introducing Dyna-Mind, a two-stage training framework that enhances AI agents' reasoning and planning abilities through simulation, leading to improved performance in complex interactive environments.",
            "ai_keywords": [
                "vicarious trial and error",
                "Dyna-Mind",
                "Reasoning with Simulations",
                "ReSim",
                "Dyna-GRPO",
                "online reinforcement learning",
                "simulation ability",
                "reasoning",
                "planning",
                "Sokoban",
                "ALFWorld",
                "AndroidWorld"
            ]
        },
        "publishedAt": "2025-10-10T13:30:18.000Z",
        "title": "Dyna-Mind: Learning to Simulate from Experience for Better AI Agents",
        "summary": "Reasoning models have recently shown remarkable progress in domains such as\nmath and coding. However, their expert-level abilities in math and coding\ncontrast sharply with their performance in long-horizon, interactive tasks such\nas web navigation and computer/phone-use. Inspired by literature on human\ncognition, we argue that current AI agents need ''vicarious trial and error'' -\nthe capacity to mentally simulate alternative futures before acting - in order\nto enhance their understanding and performance in complex interactive\nenvironments. We introduce Dyna-Mind, a two-stage training framework that\nexplicitly teaches (V)LM agents to integrate such simulation into their\nreasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which\ntrains the agent to generate structured reasoning traces from expanded search\ntrees built from real experience gathered through environment interactions.\nReSim thus grounds the agent's reasoning in faithful world dynamics and equips\nit with the ability to anticipate future states in its reasoning. In stage 2,\nwe propose Dyna-GRPO, an online reinforcement learning method to further\nstrengthen the agent's simulation and decision-making ability by using both\noutcome rewards and intermediate states as feedback from real rollouts.\nExperiments on two synthetic benchmarks (Sokoban and ALFWorld) and one\nrealistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively\ninfuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome\nand interaction-level signals to learn better policies for long-horizon,\nplanning-intensive tasks. Together, these results highlight the central role of\nsimulation in enabling AI agents to reason, plan, and act more effectively in\nthe ever more challenging environments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09577.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6234fd736dcfc5fe9f5b8601",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1647639915850-noauth.jpeg",
            "fullname": "Xiao Yu",
            "name": "jasonyux",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.09561",
            "authors": [
                {
                    "_id": "68ec5fdccd07fb414898c964",
                    "name": "Minkyoung Cho",
                    "hidden": false
                },
                {
                    "_id": "68ec5fdccd07fb414898c965",
                    "name": "Ruben Ohana",
                    "hidden": false
                },
                {
                    "_id": "68ec5fdccd07fb414898c966",
                    "name": "Christian Jacobsen",
                    "hidden": false
                },
                {
                    "_id": "68ec5fdccd07fb414898c967",
                    "name": "Adityan Jothi",
                    "hidden": false
                },
                {
                    "_id": "68ec5fdccd07fb414898c968",
                    "user": {
                        "_id": "64ae22dd1aee69ece065cdcd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                        "isPro": false,
                        "fullname": "Min-Hung Chen",
                        "user": "cmhungsteve",
                        "type": "user"
                    },
                    "name": "Min-Hung Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:05:56.116Z",
                    "hidden": false
                },
                {
                    "_id": "68ec5fdccd07fb414898c969",
                    "name": "Z. Morley Mao",
                    "hidden": false
                },
                {
                    "_id": "68ec5fdccd07fb414898c96a",
                    "name": "Ethem Can",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T17:13:02.000Z",
            "submittedOnDailyAt": "2025-10-13T00:43:02.002Z",
            "title": "TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion\n  Control",
            "submittedOnDailyBy": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
            },
            "summary": "Current controllable diffusion models typically rely on fixed architectures\nthat modify intermediate activations to inject guidance conditioned on a new\nmodality. This approach uses a static conditioning strategy for a dynamic,\nmulti-stage denoising process, limiting the model's ability to adapt its\nresponse as the generation evolves from coarse structure to fine detail. We\nintroduce TC-LoRA (Temporally Modulated Conditional LoRA), a new paradigm that\nenables dynamic, context-aware control by conditioning the model's weights\ndirectly. Our framework uses a hypernetwork to generate LoRA adapters\non-the-fly, tailoring weight modifications for the frozen backbone at each\ndiffusion step based on time and the user's condition. This mechanism enables\nthe model to learn and execute an explicit, adaptive strategy for applying\nconditional guidance throughout the entire generation process. Through\nexperiments on various data domains, we demonstrate that this dynamic,\nparametric control significantly enhances generative fidelity and adherence to\nspatial conditions compared to static, activation-based methods. TC-LoRA\nestablishes an alternative approach in which the model's conditioning strategy\nis modified through a deeper functional adaptation of its weights, allowing\ncontrol to align with the dynamic demands of the task and generative stage.",
            "upvotes": 5,
            "discussionId": "68ec5fddcd07fb414898c96b",
            "ai_summary": "TC-LoRA enhances generative fidelity and adherence to spatial conditions by dynamically conditioning model weights through a hypernetwork, improving upon static activation-based methods in diffusion models.",
            "ai_keywords": [
                "diffusion models",
                "TC-LoRA",
                "Temporally Modulated Conditional LoRA",
                "hypernetwork",
                "LoRA adapters",
                "generative fidelity",
                "spatial conditions",
                "dynamic conditioning",
                "static conditioning",
                "activation-based methods"
            ]
        },
        "publishedAt": "2025-10-10T13:13:02.000Z",
        "title": "TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion\n  Control",
        "summary": "Current controllable diffusion models typically rely on fixed architectures\nthat modify intermediate activations to inject guidance conditioned on a new\nmodality. This approach uses a static conditioning strategy for a dynamic,\nmulti-stage denoising process, limiting the model's ability to adapt its\nresponse as the generation evolves from coarse structure to fine detail. We\nintroduce TC-LoRA (Temporally Modulated Conditional LoRA), a new paradigm that\nenables dynamic, context-aware control by conditioning the model's weights\ndirectly. Our framework uses a hypernetwork to generate LoRA adapters\non-the-fly, tailoring weight modifications for the frozen backbone at each\ndiffusion step based on time and the user's condition. This mechanism enables\nthe model to learn and execute an explicit, adaptive strategy for applying\nconditional guidance throughout the entire generation process. Through\nexperiments on various data domains, we demonstrate that this dynamic,\nparametric control significantly enhances generative fidelity and adherence to\nspatial conditions compared to static, activation-based methods. TC-LoRA\nestablishes an alternative approach in which the model's conditioning strategy\nis modified through a deeper functional adaptation of its weights, allowing\ncontrol to align with the dynamic demands of the task and generative stage.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09561.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ae22dd1aee69ece065cdcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
            "fullname": "Min-Hung Chen",
            "name": "cmhungsteve",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.09507",
            "authors": [
                {
                    "_id": "68ec63a6cd07fb414898c989",
                    "name": "Zixin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ec63a6cd07fb414898c98a",
                    "name": "Kanghao Chen",
                    "hidden": false
                },
                {
                    "_id": "68ec63a6cd07fb414898c98b",
                    "name": "Xingwang Lin",
                    "hidden": false
                },
                {
                    "_id": "68ec63a6cd07fb414898c98c",
                    "name": "Lutao Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ec63a6cd07fb414898c98d",
                    "name": "Xu Zheng",
                    "hidden": false
                },
                {
                    "_id": "68ec63a6cd07fb414898c98e",
                    "name": "Yuanhuiyi Lyu",
                    "hidden": false
                },
                {
                    "_id": "68ec63a6cd07fb414898c98f",
                    "name": "Litao Guo",
                    "hidden": false
                },
                {
                    "_id": "68ec63a6cd07fb414898c990",
                    "name": "Yinchuan Li",
                    "hidden": false
                },
                {
                    "_id": "68ec63a6cd07fb414898c991",
                    "name": "Ying-Cong Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T16:10:45.000Z",
            "submittedOnDailyAt": "2025-10-13T00:57:50.917Z",
            "title": "PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The ability to use, understand, and create tools is a hallmark of human\nintelligence, enabling sophisticated interaction with the physical world. For\nany general-purpose intelligent agent to achieve true versatility, it must also\nmaster these fundamental skills. While modern Multimodal Large Language Models\n(MLLMs) leverage their extensive common knowledge for high-level planning in\nembodied AI and in downstream Vision-Language-Action (VLA) models, the extent\nof their true understanding of physical tools remains unquantified. To bridge\nthis gap, we present PhysToolBench, the first benchmark dedicated to evaluating\nthe comprehension of physical tools by MLLMs. Our benchmark is structured as a\nVisual Question Answering (VQA) dataset comprising over 1,000 image-text pairs.\nIt assesses capabilities across three distinct difficulty levels: (1) Tool\nRecognition: Requiring the recognition of a tool's primary function. (2) Tool\nUnderstanding: Testing the ability to grasp the underlying principles of a\ntool's operation. (3) Tool Creation: Challenging the model to fashion a new\ntool from surrounding objects when conventional options are unavailable. Our\ncomprehensive evaluation of 32 MLLMs-spanning proprietary, open-source,\nspecialized embodied, and backbones in VLAs-reveals a significant deficiency in\ntool understanding. Furthermore, we provide an in-depth analysis and propose\npreliminary solutions. Code and dataset are publicly available.",
            "upvotes": 5,
            "discussionId": "68ec63a6cd07fb414898c992",
            "githubRepo": "https://github.com/EnVision-Research/PhysToolBench",
            "ai_summary": "PhysToolBench evaluates MLLMs' comprehension of physical tools through a VQA dataset, revealing significant deficiencies in tool understanding.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "MLLMs",
                "embodied AI",
                "Vision-Language-Action",
                "VLA",
                "Visual Question Answering",
                "VQA",
                "Tool Recognition",
                "Tool Understanding",
                "Tool Creation"
            ],
            "githubStars": 14
        },
        "publishedAt": "2025-10-10T12:10:45.000Z",
        "title": "PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs",
        "summary": "The ability to use, understand, and create tools is a hallmark of human\nintelligence, enabling sophisticated interaction with the physical world. For\nany general-purpose intelligent agent to achieve true versatility, it must also\nmaster these fundamental skills. While modern Multimodal Large Language Models\n(MLLMs) leverage their extensive common knowledge for high-level planning in\nembodied AI and in downstream Vision-Language-Action (VLA) models, the extent\nof their true understanding of physical tools remains unquantified. To bridge\nthis gap, we present PhysToolBench, the first benchmark dedicated to evaluating\nthe comprehension of physical tools by MLLMs. Our benchmark is structured as a\nVisual Question Answering (VQA) dataset comprising over 1,000 image-text pairs.\nIt assesses capabilities across three distinct difficulty levels: (1) Tool\nRecognition: Requiring the recognition of a tool's primary function. (2) Tool\nUnderstanding: Testing the ability to grasp the underlying principles of a\ntool's operation. (3) Tool Creation: Challenging the model to fashion a new\ntool from surrounding objects when conventional options are unavailable. Our\ncomprehensive evaluation of 32 MLLMs-spanning proprietary, open-source,\nspecialized embodied, and backbones in VLAs-reveals a significant deficiency in\ntool understanding. Furthermore, we provide an in-depth analysis and propose\npreliminary solutions. Code and dataset are publicly available.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09507.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 126
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08047",
            "authors": [
                {
                    "_id": "68ec5661cd07fb414898c8b7",
                    "name": "Yi-Cheng Lin",
                    "hidden": false
                },
                {
                    "_id": "68ec5661cd07fb414898c8b8",
                    "name": "Yu-Hsuan Li Liang",
                    "hidden": false
                },
                {
                    "_id": "68ec5661cd07fb414898c8b9",
                    "user": {
                        "_id": "608abf1272b50b02c4b02865",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1619708309549-608abf1272b50b02c4b02865.jpeg",
                        "isPro": false,
                        "fullname": "Hsuan Su",
                        "user": "jacksukk",
                        "type": "user"
                    },
                    "name": "Hsuan Su",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:06:10.398Z",
                    "hidden": false
                },
                {
                    "_id": "68ec5661cd07fb414898c8ba",
                    "name": "Tzu-Quan Lin",
                    "hidden": false
                },
                {
                    "_id": "68ec5661cd07fb414898c8bb",
                    "name": "Shang-Tse Chen",
                    "hidden": false
                },
                {
                    "_id": "68ec5661cd07fb414898c8bc",
                    "name": "Yun-Nung Chen",
                    "hidden": false
                },
                {
                    "_id": "68ec5661cd07fb414898c8bd",
                    "name": "Hung-yi Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T10:31:47.000Z",
            "submittedOnDailyAt": "2025-10-13T00:01:41.326Z",
            "title": "Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic\n  Speech Recognition",
            "submittedOnDailyBy": {
                "_id": "608abf1272b50b02c4b02865",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1619708309549-608abf1272b50b02c4b02865.jpeg",
                "isPro": false,
                "fullname": "Hsuan Su",
                "user": "jacksukk",
                "type": "user"
            },
            "summary": "Robust ASR under domain shift is crucial because real-world systems encounter\nunseen accents and domains with limited labeled data. Although pseudo-labeling\noffers a practical workaround, it often introduces systematic, accent-specific\nerrors that filtering fails to fix. We ask: How can we correct these recurring\nbiases without target ground truth? We propose a simple parameter-space\ncorrection: in a source domain containing both real and pseudo-labeled data,\ntwo ASR models are fine-tuned from the same initialization, one on ground-truth\nlabels and the other on pseudo-labels, and their weight difference forms a\ncorrection vector that captures pseudo-label biases. When applied to a\npseudo-labeled target model, this vector enhances recognition, achieving up to\na 35% relative Word Error Rate (WER) reduction on AfriSpeech-200 across ten\nAfrican accents with the Whisper tiny model.",
            "upvotes": 5,
            "discussionId": "68ec5662cd07fb414898c8be",
            "ai_summary": "A parameter-space correction method reduces Word Error Rate in ASR systems by addressing pseudo-label biases without target ground truth.",
            "ai_keywords": [
                "ASR",
                "domain shift",
                "pseudo-labeling",
                "parameter-space correction",
                "fine-tuning",
                "correction vector",
                "Word Error Rate (WER)",
                "AfriSpeech-200",
                "Whisper tiny model"
            ]
        },
        "publishedAt": "2025-10-09T06:31:47.000Z",
        "title": "Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic\n  Speech Recognition",
        "summary": "Robust ASR under domain shift is crucial because real-world systems encounter\nunseen accents and domains with limited labeled data. Although pseudo-labeling\noffers a practical workaround, it often introduces systematic, accent-specific\nerrors that filtering fails to fix. We ask: How can we correct these recurring\nbiases without target ground truth? We propose a simple parameter-space\ncorrection: in a source domain containing both real and pseudo-labeled data,\ntwo ASR models are fine-tuned from the same initialization, one on ground-truth\nlabels and the other on pseudo-labels, and their weight difference forms a\ncorrection vector that captures pseudo-label biases. When applied to a\npseudo-labeled target model, this vector enhances recognition, achieving up to\na 35% relative Word Error Rate (WER) reduction on AfriSpeech-200 across ten\nAfrican accents with the Whisper tiny model.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08047.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "608abf1272b50b02c4b02865",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1619708309549-608abf1272b50b02c4b02865.jpeg",
            "fullname": "Hsuan Su",
            "name": "jacksukk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.07745",
            "authors": [
                {
                    "_id": "68ec636dcd07fb414898c981",
                    "user": {
                        "_id": "64e14c5b12a5504dda70e60d",
                        "avatarUrl": "/avatars/944b486bb037364ef7d9d2c826526708.svg",
                        "isPro": false,
                        "fullname": "Runyang",
                        "user": "dd101bb",
                        "type": "user"
                    },
                    "name": "Runyang You",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:05:39.701Z",
                    "hidden": false
                },
                {
                    "_id": "68ec636dcd07fb414898c982",
                    "name": "Yongqi Li",
                    "hidden": false
                },
                {
                    "_id": "68ec636dcd07fb414898c983",
                    "name": "Meng Liu",
                    "hidden": false
                },
                {
                    "_id": "68ec636dcd07fb414898c984",
                    "name": "Wenjie Wang",
                    "hidden": false
                },
                {
                    "_id": "68ec636dcd07fb414898c985",
                    "name": "Liqiang Nie",
                    "hidden": false
                },
                {
                    "_id": "68ec636dcd07fb414898c986",
                    "name": "Wenjie Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T03:33:00.000Z",
            "submittedOnDailyAt": "2025-10-13T01:01:52.962Z",
            "title": "Parallel Test-Time Scaling for Latent Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "64e14c5b12a5504dda70e60d",
                "avatarUrl": "/avatars/944b486bb037364ef7d9d2c826526708.svg",
                "isPro": false,
                "fullname": "Runyang",
                "user": "dd101bb",
                "type": "user"
            },
            "summary": "Parallel test-time scaling (TTS) is a pivotal approach for enhancing large\nlanguage models (LLMs), typically by sampling multiple token-based\nchains-of-thought in parallel and aggregating outcomes through voting or\nsearch. Recent advances in latent reasoning, where intermediate reasoning\nunfolds in continuous vector spaces, offer a more efficient alternative to\nexplicit Chain-of-Thought, yet whether such latent models can similarly benefit\nfrom parallel TTS remains open, mainly due to the absence of sampling\nmechanisms in continuous space, and the lack of probabilistic signals for\nadvanced trajectory aggregation. \\ This work enables parallel TTS for latent\nreasoning models by addressing the above issues. For sampling, we introduce two\nuncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive\nGaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM)\ntrained with step-wise contrastive objective to score and guide latent\nreasoning. Extensive experiments and visualization analyses show that both\nsampling strategies scale effectively with compute and exhibit distinct\nexploration dynamics, while LatentRM enables effective trajectory selection.\nTogether, our explorations open a new direction for scalable inference in\ncontinuous spaces. Code released at https://github.com/YRYangang/LatentTTS.",
            "upvotes": 5,
            "discussionId": "68ec636ecd07fb414898c987",
            "githubRepo": "https://github.com/YRYangang/LatentTTS",
            "ai_summary": "Parallel test-time scaling is enabled for latent reasoning models using uncertainty-inspired sampling strategies and a Latent Reward Model for effective trajectory selection.",
            "ai_keywords": [
                "Parallel test-time scaling",
                "large language models",
                "token-based chains-of-thought",
                "latent reasoning",
                "continuous vector spaces",
                "Monte Carlo Dropout",
                "Additive Gaussian Noise",
                "Latent Reward Model",
                "step-wise contrastive objective",
                "scalable inference"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "646ecc368d316fde87b3b6e3",
                "name": "PolyUHK",
                "fullname": "The Hong Kong Polytechnic University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646ecbc0cbb7bb996513e298/Akb4zKqIP9kb9PQoUPUmj.jpeg"
            }
        },
        "publishedAt": "2025-10-08T23:33:00.000Z",
        "title": "Parallel Test-Time Scaling for Latent Reasoning Models",
        "summary": "Parallel test-time scaling (TTS) is a pivotal approach for enhancing large\nlanguage models (LLMs), typically by sampling multiple token-based\nchains-of-thought in parallel and aggregating outcomes through voting or\nsearch. Recent advances in latent reasoning, where intermediate reasoning\nunfolds in continuous vector spaces, offer a more efficient alternative to\nexplicit Chain-of-Thought, yet whether such latent models can similarly benefit\nfrom parallel TTS remains open, mainly due to the absence of sampling\nmechanisms in continuous space, and the lack of probabilistic signals for\nadvanced trajectory aggregation. \\ This work enables parallel TTS for latent\nreasoning models by addressing the above issues. For sampling, we introduce two\nuncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive\nGaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM)\ntrained with step-wise contrastive objective to score and guide latent\nreasoning. Extensive experiments and visualization analyses show that both\nsampling strategies scale effectively with compute and exhibit distinct\nexploration dynamics, while LatentRM enables effective trajectory selection.\nTogether, our explorations open a new direction for scalable inference in\ncontinuous spaces. Code released at https://github.com/YRYangang/LatentTTS.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07745.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e14c5b12a5504dda70e60d",
            "avatarUrl": "/avatars/944b486bb037364ef7d9d2c826526708.svg",
            "fullname": "Runyang",
            "name": "dd101bb",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "646ecc368d316fde87b3b6e3",
            "name": "PolyUHK",
            "fullname": "The Hong Kong Polytechnic University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646ecbc0cbb7bb996513e298/Akb4zKqIP9kb9PQoUPUmj.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.09462",
            "authors": [
                {
                    "_id": "68ecb9a8cd07fb414898cc22",
                    "user": {
                        "_id": "6564d6fd58b686cf6929d364",
                        "avatarUrl": "/avatars/9314b3fcb2b4aa9b36767ed82ab1255b.svg",
                        "isPro": false,
                        "fullname": "Mikhail Terekhov",
                        "user": "terekhov",
                        "type": "user"
                    },
                    "name": "Mikhail Terekhov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T16:09:00.712Z",
                    "hidden": false
                },
                {
                    "_id": "68ecb9a8cd07fb414898cc23",
                    "user": {
                        "_id": "645410c50b50e97d3722f1d2",
                        "avatarUrl": "/avatars/e8e079981316b26dbf0f4ada6dcdc8c6.svg",
                        "isPro": false,
                        "fullname": "Alexander Panfilov",
                        "user": "kotekjedi",
                        "type": "user"
                    },
                    "name": "Alexander Panfilov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:03:53.569Z",
                    "hidden": false
                },
                {
                    "_id": "68ecb9a8cd07fb414898cc24",
                    "name": "Daniil Dzenhaliou",
                    "hidden": false
                },
                {
                    "_id": "68ecb9a8cd07fb414898cc25",
                    "name": "Caglar Gulcehre",
                    "hidden": false
                },
                {
                    "_id": "68ecb9a8cd07fb414898cc26",
                    "name": "Maksym Andriushchenko",
                    "hidden": false
                },
                {
                    "_id": "68ecb9a8cd07fb414898cc27",
                    "name": "Ameya Prabhu",
                    "hidden": false
                },
                {
                    "_id": "68ecb9a8cd07fb414898cc28",
                    "name": "Jonas Geiping",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/645410c50b50e97d3722f1d2/QZIlUN30x85_38NtylxKJ.png"
            ],
            "publishedAt": "2025-10-10T15:12:44.000Z",
            "submittedOnDailyAt": "2025-10-13T07:09:08.297Z",
            "title": "Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols",
            "submittedOnDailyBy": {
                "_id": "645410c50b50e97d3722f1d2",
                "avatarUrl": "/avatars/e8e079981316b26dbf0f4ada6dcdc8c6.svg",
                "isPro": false,
                "fullname": "Alexander Panfilov",
                "user": "kotekjedi",
                "type": "user"
            },
            "summary": "AI control protocols serve as a defense mechanism to stop untrusted LLM\nagents from causing harm in autonomous settings. Prior work treats this as a\nsecurity problem, stress testing with exploits that use the deployment context\nto subtly complete harmful side tasks, such as backdoor insertion. In practice,\nmost AI control protocols are fundamentally based on LLM monitors, which can\nbecome a central point of failure. We study adaptive attacks by an untrusted\nmodel that knows the protocol and the monitor model, which is plausible if the\nuntrusted model was trained with a later knowledge cutoff or can search for\nthis information autonomously. We instantiate a simple adaptive attack vector\nby which the attacker embeds publicly known or zero-shot prompt injections in\nthe model outputs. Using this tactic, frontier models consistently evade\ndiverse monitors and complete malicious tasks on two main AI control\nbenchmarks. The attack works universally against current protocols that rely on\na monitor. Furthermore, the recent Defer-to-Resample protocol even backfires,\nas its resampling amplifies the prompt injection and effectively reframes it as\na best-of-n attack. In general, adaptive attacks on monitor models represent\na major blind spot in current control protocols and should become a standard\ncomponent of evaluations for future AI control mechanisms.",
            "upvotes": 4,
            "discussionId": "68ecb9a9cd07fb414898cc29",
            "ai_summary": "Adaptive attacks on AI control protocols using prompt injections can evade monitors and complete malicious tasks, highlighting a significant vulnerability in current security mechanisms.",
            "ai_keywords": [
                "LLM",
                "AI control protocols",
                "security problem",
                "backdoor insertion",
                "LLM monitors",
                "adaptive attacks",
                "prompt injections",
                "Defer-to-Resample protocol",
                "best-of-n attack"
            ],
            "organization": {
                "_id": "6789455d7d94ef35adb5addd",
                "name": "CLAIRE-Labo",
                "fullname": "CLAIRE Lab @EPFL",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66d04835530bebbbef1d8405/UU474g375sjrAHoHlPxj5.png"
            }
        },
        "publishedAt": "2025-10-10T11:12:44.000Z",
        "title": "Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols",
        "summary": "AI control protocols serve as a defense mechanism to stop untrusted LLM\nagents from causing harm in autonomous settings. Prior work treats this as a\nsecurity problem, stress testing with exploits that use the deployment context\nto subtly complete harmful side tasks, such as backdoor insertion. In practice,\nmost AI control protocols are fundamentally based on LLM monitors, which can\nbecome a central point of failure. We study adaptive attacks by an untrusted\nmodel that knows the protocol and the monitor model, which is plausible if the\nuntrusted model was trained with a later knowledge cutoff or can search for\nthis information autonomously. We instantiate a simple adaptive attack vector\nby which the attacker embeds publicly known or zero-shot prompt injections in\nthe model outputs. Using this tactic, frontier models consistently evade\ndiverse monitors and complete malicious tasks on two main AI control\nbenchmarks. The attack works universally against current protocols that rely on\na monitor. Furthermore, the recent Defer-to-Resample protocol even backfires,\nas its resampling amplifies the prompt injection and effectively reframes it as\na best-of-n attack. In general, adaptive attacks on monitor models represent\na major blind spot in current control protocols and should become a standard\ncomponent of evaluations for future AI control mechanisms.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/645410c50b50e97d3722f1d2/QZIlUN30x85_38NtylxKJ.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09462.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645410c50b50e97d3722f1d2",
            "avatarUrl": "/avatars/e8e079981316b26dbf0f4ada6dcdc8c6.svg",
            "fullname": "Alexander Panfilov",
            "name": "kotekjedi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "6789455d7d94ef35adb5addd",
            "name": "CLAIRE-Labo",
            "fullname": "CLAIRE Lab @EPFL",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66d04835530bebbbef1d8405/UU474g375sjrAHoHlPxj5.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08867",
            "authors": [
                {
                    "_id": "68ec5814cd07fb414898c8db",
                    "name": "Gaurav Sahu",
                    "hidden": false
                },
                {
                    "_id": "68ec5814cd07fb414898c8dc",
                    "name": "Hugo Larochelle",
                    "hidden": false
                },
                {
                    "_id": "68ec5814cd07fb414898c8dd",
                    "name": "Laurent Charlin",
                    "hidden": false
                },
                {
                    "_id": "68ec5814cd07fb414898c8de",
                    "name": "Christopher Pal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T23:53:19.000Z",
            "submittedOnDailyAt": "2025-10-13T00:09:58.734Z",
            "title": "ReviewerToo: Should AI Join The Program Committee? A Look At The Future\n  of Peer Review",
            "submittedOnDailyBy": {
                "_id": "6377ac12f5fe4a39f783b05d",
                "avatarUrl": "/avatars/5f8b6d999cf48dd4703bbd70236c38c8.svg",
                "isPro": false,
                "fullname": "G Sahu",
                "user": "demfier",
                "type": "user"
            },
            "summary": "Peer review is the cornerstone of scientific publishing, yet it suffers from\ninconsistencies, reviewer subjectivity, and scalability challenges. We\nintroduce ReviewerToo, a modular framework for studying and deploying\nAI-assisted peer review to complement human judgment with systematic and\nconsistent assessments. ReviewerToo supports systematic experiments with\nspecialized reviewer personas and structured evaluation criteria, and can be\npartially or fully integrated into real conference workflows. We validate\nReviewerToo on a carefully curated dataset of 1,963 paper submissions from ICLR\n2025, where our experiments with the gpt-oss-120b model achieves 81.8% accuracy\nfor the task of categorizing a paper as accept/reject compared to 83.9% for the\naverage human reviewer. Additionally, ReviewerToo-generated reviews are rated\nas higher quality than the human average by an LLM judge, though still trailing\nthe strongest expert contributions. Our analysis highlights domains where AI\nreviewers excel (e.g., fact-checking, literature coverage) and where they\nstruggle (e.g., assessing methodological novelty and theoretical\ncontributions), underscoring the continued need for human expertise. Based on\nthese findings, we propose guidelines for integrating AI into peer-review\npipelines, showing how AI can enhance consistency, coverage, and fairness while\nleaving complex evaluative judgments to domain experts. Our work provides a\nfoundation for systematic, hybrid peer-review systems that scale with the\ngrowth of scientific publishing.",
            "upvotes": 4,
            "discussionId": "68ec5814cd07fb414898c8df",
            "ai_summary": "ReviewerToo, a modular AI-assisted peer review framework, complements human judgment with systematic assessments, achieving high accuracy and quality in specific domains while highlighting areas where human expertise remains essential.",
            "ai_keywords": [
                "AI-assisted peer review",
                "reviewer personas",
                "structured evaluation criteria",
                "gpt-oss-120b",
                "accuracy",
                "LLM judge",
                "methodological novelty",
                "theoretical contributions",
                "hybrid peer-review systems"
            ]
        },
        "publishedAt": "2025-10-09T19:53:19.000Z",
        "title": "ReviewerToo: Should AI Join The Program Committee? A Look At The Future\n  of Peer Review",
        "summary": "Peer review is the cornerstone of scientific publishing, yet it suffers from\ninconsistencies, reviewer subjectivity, and scalability challenges. We\nintroduce ReviewerToo, a modular framework for studying and deploying\nAI-assisted peer review to complement human judgment with systematic and\nconsistent assessments. ReviewerToo supports systematic experiments with\nspecialized reviewer personas and structured evaluation criteria, and can be\npartially or fully integrated into real conference workflows. We validate\nReviewerToo on a carefully curated dataset of 1,963 paper submissions from ICLR\n2025, where our experiments with the gpt-oss-120b model achieves 81.8% accuracy\nfor the task of categorizing a paper as accept/reject compared to 83.9% for the\naverage human reviewer. Additionally, ReviewerToo-generated reviews are rated\nas higher quality than the human average by an LLM judge, though still trailing\nthe strongest expert contributions. Our analysis highlights domains where AI\nreviewers excel (e.g., fact-checking, literature coverage) and where they\nstruggle (e.g., assessing methodological novelty and theoretical\ncontributions), underscoring the continued need for human expertise. Based on\nthese findings, we propose guidelines for integrating AI into peer-review\npipelines, showing how AI can enhance consistency, coverage, and fairness while\nleaving complex evaluative judgments to domain experts. Our work provides a\nfoundation for systematic, hybrid peer-review systems that scale with the\ngrowth of scientific publishing.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08867.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6377ac12f5fe4a39f783b05d",
            "avatarUrl": "/avatars/5f8b6d999cf48dd4703bbd70236c38c8.svg",
            "fullname": "G Sahu",
            "name": "demfier",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07962",
            "authors": [
                {
                    "_id": "68ed11bfde1fee572713a532",
                    "user": {
                        "_id": "6726e8b29f0682f3f3692dba",
                        "avatarUrl": "/avatars/50577819c9c910581adfd5ae4721ce7e.svg",
                        "isPro": false,
                        "fullname": "Jingyuan Wang",
                        "user": "bearthecoder",
                        "type": "user"
                    },
                    "name": "Jingyuan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T16:08:33.574Z",
                    "hidden": false
                },
                {
                    "_id": "68ed11bfde1fee572713a533",
                    "name": "Yankai Chen",
                    "hidden": false
                },
                {
                    "_id": "68ed11bfde1fee572713a534",
                    "name": "Zhonghang Li",
                    "hidden": false
                },
                {
                    "_id": "68ed11bfde1fee572713a535",
                    "name": "Chao Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T08:55:12.000Z",
            "submittedOnDailyAt": "2025-10-13T13:22:15.772Z",
            "title": "LightReasoner: Can Small Language Models Teach Large Language Models\n  Reasoning?",
            "submittedOnDailyBy": {
                "_id": "631728389e6b629ba04d3012",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631728389e6b629ba04d3012/2yihb49Sw-ue_BKTlhZWE.jpeg",
                "isPro": false,
                "fullname": "Xubin Ren",
                "user": "Rbin",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have demonstrated remarkable progress in\nreasoning, often through supervised fine-tuning (SFT). However, SFT is\nresource-intensive, relying on large curated datasets, rejection-sampled\ndemonstrations, and uniform optimization across all tokens, even though only a\nfraction carry meaningful learning value. In this work, we explore a\ncounterintuitive idea: can smaller language models (SLMs) teach larger language\nmodels (LLMs) by revealing high-value reasoning moments that reflect the\nlatter's unique strength? We propose LightReasoner, a novel framework that\nleverages the behavioral divergence between a stronger expert model (LLM) and a\nweaker amateur model (SLM). LightReasoner operates in two stages: (1) a\nsampling stage that pinpoints critical reasoning moments and constructs\nsupervision examples capturing the expert's advantage through expert-amateur\ncontrast, and (2) a fine-tuning stage that aligns the expert model with these\ndistilled examples, amplifying its reasoning strengths. Across seven\nmathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while\nreducing time consumption by 90%, sampled problems by 80%, and tuned token\nusage by 99%, all without relying on ground-truth labels. By turning weaker\nSLMs into effective teaching signals, LightReasoner offers a scalable and\nresource-efficient approach for advancing LLM reasoning. Code is available at:\nhttps://github.com/HKUDS/LightReasoner",
            "upvotes": 4,
            "discussionId": "68ed11bfde1fee572713a536",
            "ai_summary": "LightReasoner uses behavioral differences between large and small language models to identify and amplify high-value reasoning moments, improving LLM accuracy and efficiency without ground-truth labels.",
            "ai_keywords": [
                "Large language models",
                "small language models",
                "supervised fine-tuning",
                "behavioral divergence",
                "LightReasoner",
                "critical reasoning moments",
                "supervision examples",
                "expert-amateur contrast",
                "fine-tuning stage",
                "mathematical benchmarks"
            ],
            "organization": {
                "_id": "66b9f52ea0ebe4b8a533017d",
                "name": "hkuds",
                "fullname": "Data Intelligence Lab@HKU",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/631728389e6b629ba04d3012/_wpzD-zUk5LR0RMxifyNX.jpeg"
            }
        },
        "publishedAt": "2025-10-09T04:55:12.000Z",
        "title": "LightReasoner: Can Small Language Models Teach Large Language Models\n  Reasoning?",
        "summary": "Large language models (LLMs) have demonstrated remarkable progress in\nreasoning, often through supervised fine-tuning (SFT). However, SFT is\nresource-intensive, relying on large curated datasets, rejection-sampled\ndemonstrations, and uniform optimization across all tokens, even though only a\nfraction carry meaningful learning value. In this work, we explore a\ncounterintuitive idea: can smaller language models (SLMs) teach larger language\nmodels (LLMs) by revealing high-value reasoning moments that reflect the\nlatter's unique strength? We propose LightReasoner, a novel framework that\nleverages the behavioral divergence between a stronger expert model (LLM) and a\nweaker amateur model (SLM). LightReasoner operates in two stages: (1) a\nsampling stage that pinpoints critical reasoning moments and constructs\nsupervision examples capturing the expert's advantage through expert-amateur\ncontrast, and (2) a fine-tuning stage that aligns the expert model with these\ndistilled examples, amplifying its reasoning strengths. Across seven\nmathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while\nreducing time consumption by 90%, sampled problems by 80%, and tuned token\nusage by 99%, all without relying on ground-truth labels. By turning weaker\nSLMs into effective teaching signals, LightReasoner offers a scalable and\nresource-efficient approach for advancing LLM reasoning. Code is available at:\nhttps://github.com/HKUDS/LightReasoner",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07962.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631728389e6b629ba04d3012",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631728389e6b629ba04d3012/2yihb49Sw-ue_BKTlhZWE.jpeg",
            "fullname": "Xubin Ren",
            "name": "Rbin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "66b9f52ea0ebe4b8a533017d",
            "name": "hkuds",
            "fullname": "Data Intelligence Lab@HKU",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/631728389e6b629ba04d3012/_wpzD-zUk5LR0RMxifyNX.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01119",
            "authors": [
                {
                    "_id": "68ec5ec4cd07fb414898c939",
                    "name": "Zhanpeng Luo",
                    "hidden": false
                },
                {
                    "_id": "68ec5ec4cd07fb414898c93a",
                    "name": "Haoxi Ran",
                    "hidden": false
                },
                {
                    "_id": "68ec5ec4cd07fb414898c93b",
                    "name": "Li Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T17:07:21.000Z",
            "submittedOnDailyAt": "2025-10-13T00:37:38.826Z",
            "title": "Instant4D: 4D Gaussian Splatting in Minutes",
            "submittedOnDailyBy": {
                "_id": "62a527f17157a88f920afc50",
                "avatarUrl": "/avatars/d20c9afcdc42db686e05472459fc09f6.svg",
                "isPro": false,
                "fullname": "Haoxi Ran",
                "user": "Hancy",
                "type": "user"
            },
            "summary": "Dynamic view synthesis has seen significant advances, yet reconstructing\nscenes from uncalibrated, casual video remains challenging due to slow\noptimization and complex parameter estimation. In this work, we present\nInstant4D, a monocular reconstruction system that leverages native 4D\nrepresentation to efficiently process casual video sequences within minutes,\nwithout calibrated cameras or depth sensors. Our method begins with geometric\nrecovery through deep visual SLAM, followed by grid pruning to optimize scene\nrepresentation. Our design significantly reduces redundancy while maintaining\ngeometric integrity, cutting model size to under 10% of its original footprint.\nTo handle temporal dynamics efficiently, we introduce a streamlined 4D Gaussian\nrepresentation, achieving a 30x speed-up and reducing training time to within\ntwo minutes, while maintaining competitive performance across several\nbenchmarks. Our method reconstruct a single video within 10 minutes on the\nDycheck dataset or for a typical 200-frame video. We further apply our model to\nin-the-wild videos, showcasing its generalizability. Our project website is\npublished at https://instant4d.github.io/.",
            "upvotes": 4,
            "discussionId": "68ec5ec4cd07fb414898c93c",
            "ai_summary": "Instant4D uses deep visual SLAM and a 4D Gaussian representation to efficiently reconstruct scenes from uncalibrated video sequences in minutes.",
            "ai_keywords": [
                "deep visual SLAM",
                "4D representation",
                "grid pruning",
                "4D Gaussian representation"
            ]
        },
        "publishedAt": "2025-10-01T13:07:21.000Z",
        "title": "Instant4D: 4D Gaussian Splatting in Minutes",
        "summary": "Dynamic view synthesis has seen significant advances, yet reconstructing\nscenes from uncalibrated, casual video remains challenging due to slow\noptimization and complex parameter estimation. In this work, we present\nInstant4D, a monocular reconstruction system that leverages native 4D\nrepresentation to efficiently process casual video sequences within minutes,\nwithout calibrated cameras or depth sensors. Our method begins with geometric\nrecovery through deep visual SLAM, followed by grid pruning to optimize scene\nrepresentation. Our design significantly reduces redundancy while maintaining\ngeometric integrity, cutting model size to under 10% of its original footprint.\nTo handle temporal dynamics efficiently, we introduce a streamlined 4D Gaussian\nrepresentation, achieving a 30x speed-up and reducing training time to within\ntwo minutes, while maintaining competitive performance across several\nbenchmarks. Our method reconstruct a single video within 10 minutes on the\nDycheck dataset or for a typical 200-frame video. We further apply our model to\nin-the-wild videos, showcasing its generalizability. Our project website is\npublished at https://instant4d.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01119.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62a527f17157a88f920afc50",
            "avatarUrl": "/avatars/d20c9afcdc42db686e05472459fc09f6.svg",
            "fullname": "Haoxi Ran",
            "name": "Hancy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.09592",
            "authors": [
                {
                    "_id": "68ec5bc2cd07fb414898c8f7",
                    "name": "Donghang Wu",
                    "hidden": false
                },
                {
                    "_id": "68ec5bc2cd07fb414898c8f8",
                    "name": "Haoyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ec5bc2cd07fb414898c8f9",
                    "name": "Jun Chen",
                    "hidden": false
                },
                {
                    "_id": "68ec5bc2cd07fb414898c8fa",
                    "name": "Xiangyu",
                    "hidden": false
                },
                {
                    "_id": "68ec5bc2cd07fb414898c8fb",
                    "name": "Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ec5bc2cd07fb414898c8fc",
                    "name": "Hexin Liu",
                    "hidden": false
                },
                {
                    "_id": "68ec5bc2cd07fb414898c8fd",
                    "name": "Eng Siong Chng",
                    "hidden": false
                },
                {
                    "_id": "68ec5bc2cd07fb414898c8fe",
                    "name": "Fei Tian",
                    "hidden": false
                },
                {
                    "_id": "68ec5bc2cd07fb414898c8ff",
                    "name": "Xuerui Yang",
                    "hidden": false
                },
                {
                    "_id": "68ec5bc2cd07fb414898c900",
                    "name": "Xiangyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ec5bc2cd07fb414898c901",
                    "name": "Daxin Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ec5bc2cd07fb414898c902",
                    "name": "Gang Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T17:50:59.000Z",
            "submittedOnDailyAt": "2025-10-13T00:24:12.723Z",
            "title": "Mind-Paced Speaking: A Dual-Brain Approach to Real-Time Reasoning in\n  Spoken Language Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Real-time Spoken Language Models (SLMs) struggle to leverage Chain-of-Thought\n(CoT) reasoning due to the prohibitive latency of generating the entire thought\nprocess sequentially. Enabling SLMs to think while speaking, similar to humans,\nis attracting increasing attention. We present, for the first time, Mind-Paced\nSpeaking (MPS), a brain-inspired framework that enables high-fidelity,\nreal-time reasoning. Similar to how humans utilize distinct brain regions for\nthinking and responding, we propose a novel dual-brain approach, employing a\n\"Formulation Brain\" for high-level reasoning to pace and guide a separate\n\"Articulation Brain\" for fluent speech generation. This division of labor\neliminates mode-switching, preserving the integrity of the reasoning process.\nExperiments show that MPS significantly outperforms existing\nthink-while-speaking methods and achieves reasoning performance comparable to\nmodels that pre-compute the full CoT before speaking, while drastically\nreducing latency. Under a zero-latency configuration, the proposed method\nachieves an accuracy of 92.8% on the mathematical reasoning task Spoken-MQA and\nattains a score of 82.5 on the speech conversation task URO-Bench. Our work\neffectively bridges the gap between high-quality reasoning and real-time\ninteraction.",
            "upvotes": 3,
            "discussionId": "68ec5bc3cd07fb414898c903",
            "githubRepo": "https://github.com/stepfun-ai/Step-MPS",
            "ai_summary": "Mind-Paced Speaking (MPS) is a brain-inspired framework that enables real-time reasoning and fluent speech generation by dividing the process into a \"Formulation Brain\" for reasoning and an \"Articulation Brain\" for speech, achieving high accuracy with low latency.",
            "ai_keywords": [
                "Chain-of-Thought (CoT) reasoning",
                "Mind-Paced Speaking (MPS)",
                "dual-brain approach",
                "Formulation Brain",
                "Articulation Brain",
                "real-time reasoning",
                "Spoken-MQA",
                "URO-Bench"
            ],
            "githubStars": 8
        },
        "publishedAt": "2025-10-10T13:50:59.000Z",
        "title": "Mind-Paced Speaking: A Dual-Brain Approach to Real-Time Reasoning in\n  Spoken Language Models",
        "summary": "Real-time Spoken Language Models (SLMs) struggle to leverage Chain-of-Thought\n(CoT) reasoning due to the prohibitive latency of generating the entire thought\nprocess sequentially. Enabling SLMs to think while speaking, similar to humans,\nis attracting increasing attention. We present, for the first time, Mind-Paced\nSpeaking (MPS), a brain-inspired framework that enables high-fidelity,\nreal-time reasoning. Similar to how humans utilize distinct brain regions for\nthinking and responding, we propose a novel dual-brain approach, employing a\n\"Formulation Brain\" for high-level reasoning to pace and guide a separate\n\"Articulation Brain\" for fluent speech generation. This division of labor\neliminates mode-switching, preserving the integrity of the reasoning process.\nExperiments show that MPS significantly outperforms existing\nthink-while-speaking methods and achieves reasoning performance comparable to\nmodels that pre-compute the full CoT before speaking, while drastically\nreducing latency. Under a zero-latency configuration, the proposed method\nachieves an accuracy of 92.8% on the mathematical reasoning task Spoken-MQA and\nattains a score of 82.5 on the speech conversation task URO-Bench. Our work\neffectively bridges the gap between high-quality reasoning and real-time\ninteraction.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09592.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 126
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07861",
            "authors": [
                {
                    "_id": "68ec6ff6cd07fb414898c9c8",
                    "user": {
                        "_id": "65f3ee4efe1935d2e9d4a678",
                        "avatarUrl": "/avatars/ffd9f945183b4c890f885c82739c28a6.svg",
                        "isPro": false,
                        "fullname": "Tianyu Fan",
                        "user": "T1anyu",
                        "type": "user"
                    },
                    "name": "Tianyu Fan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:05:27.374Z",
                    "hidden": false
                },
                {
                    "_id": "68ec6ff6cd07fb414898c9c9",
                    "name": "Xinyao Niu",
                    "hidden": false
                },
                {
                    "_id": "68ec6ff6cd07fb414898c9ca",
                    "name": "Yuxiang Zheng",
                    "hidden": false
                },
                {
                    "_id": "68ec6ff6cd07fb414898c9cb",
                    "name": "Fengji Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ec6ff6cd07fb414898c9cc",
                    "name": "Chengen Huang",
                    "hidden": false
                },
                {
                    "_id": "68ec6ff6cd07fb414898c9cd",
                    "name": "Bei Chen",
                    "hidden": false
                },
                {
                    "_id": "68ec6ff6cd07fb414898c9ce",
                    "name": "Junyang Lin",
                    "hidden": false
                },
                {
                    "_id": "68ec6ff6cd07fb414898c9cf",
                    "name": "Chao Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T07:03:43.000Z",
            "submittedOnDailyAt": "2025-10-13T01:58:17.988Z",
            "title": "Understanding DeepResearch via Reports",
            "submittedOnDailyBy": {
                "_id": "65f3ee4efe1935d2e9d4a678",
                "avatarUrl": "/avatars/ffd9f945183b4c890f885c82739c28a6.svg",
                "isPro": false,
                "fullname": "Tianyu Fan",
                "user": "T1anyu",
                "type": "user"
            },
            "summary": "DeepResearch agents represent a transformative AI paradigm, conducting\nexpert-level research through sophisticated reasoning and multi-tool\nintegration. However, evaluating these systems remains critically challenging\ndue to open-ended research scenarios and existing benchmarks that focus on\nisolated capabilities rather than holistic performance. Unlike traditional LLM\ntasks, DeepResearch systems must synthesize diverse sources, generate insights,\nand present coherent findings, which are capabilities that resist simple\nverification. To address this gap, we introduce DeepResearch-ReportEval, a\ncomprehensive framework designed to assess DeepResearch systems through their\nmost representative outputs: research reports. Our approach systematically\nmeasures three dimensions: quality, redundancy, and factuality, using an\ninnovative LLM-as-a-Judge methodology achieving strong expert concordance. We\ncontribute a standardized benchmark of 100 curated queries spanning 12\nreal-world categories, enabling systematic capability comparison. Our\nevaluation of four leading commercial systems reveals distinct design\nphilosophies and performance trade-offs, establishing foundational insights as\nDeepResearch evolves from information assistants toward intelligent research\npartners. Source code and data are available at:\nhttps://github.com/HKUDS/DeepResearch-Eval.",
            "upvotes": 3,
            "discussionId": "68ec6ff6cd07fb414898c9d0",
            "ai_summary": "A framework evaluates DeepResearch systems by assessing the quality, redundancy, and factuality of their research reports using an LLM-as-a-Judge methodology.",
            "ai_keywords": [
                "DeepResearch",
                "LLM",
                "research reports",
                "LLM-as-a-Judge",
                "expert concordance",
                "standardized benchmark",
                "real-world categories",
                "intelligent research partners"
            ]
        },
        "publishedAt": "2025-10-09T03:03:43.000Z",
        "title": "Understanding DeepResearch via Reports",
        "summary": "DeepResearch agents represent a transformative AI paradigm, conducting\nexpert-level research through sophisticated reasoning and multi-tool\nintegration. However, evaluating these systems remains critically challenging\ndue to open-ended research scenarios and existing benchmarks that focus on\nisolated capabilities rather than holistic performance. Unlike traditional LLM\ntasks, DeepResearch systems must synthesize diverse sources, generate insights,\nand present coherent findings, which are capabilities that resist simple\nverification. To address this gap, we introduce DeepResearch-ReportEval, a\ncomprehensive framework designed to assess DeepResearch systems through their\nmost representative outputs: research reports. Our approach systematically\nmeasures three dimensions: quality, redundancy, and factuality, using an\ninnovative LLM-as-a-Judge methodology achieving strong expert concordance. We\ncontribute a standardized benchmark of 100 curated queries spanning 12\nreal-world categories, enabling systematic capability comparison. Our\nevaluation of four leading commercial systems reveals distinct design\nphilosophies and performance trade-offs, establishing foundational insights as\nDeepResearch evolves from information assistants toward intelligent research\npartners. Source code and data are available at:\nhttps://github.com/HKUDS/DeepResearch-Eval.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07861.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f3ee4efe1935d2e9d4a678",
            "avatarUrl": "/avatars/ffd9f945183b4c890f885c82739c28a6.svg",
            "fullname": "Tianyu Fan",
            "name": "T1anyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.05608",
            "authors": [
                {
                    "_id": "68ec6e11cd07fb414898c998",
                    "name": "Shuzheng Si",
                    "hidden": false
                },
                {
                    "_id": "68ec6e11cd07fb414898c999",
                    "name": "Haozhe Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ec6e11cd07fb414898c99a",
                    "name": "Kangyang Luo",
                    "hidden": false
                },
                {
                    "_id": "68ec6e11cd07fb414898c99b",
                    "name": "Gang Chen",
                    "hidden": false
                },
                {
                    "_id": "68ec6e11cd07fb414898c99c",
                    "name": "Fanchao Qi",
                    "hidden": false
                },
                {
                    "_id": "68ec6e11cd07fb414898c99d",
                    "name": "Minjia Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ec6e11cd07fb414898c99e",
                    "name": "Baobao Chang",
                    "hidden": false
                },
                {
                    "_id": "68ec6e11cd07fb414898c99f",
                    "name": "Maosong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-07T06:10:53.000Z",
            "submittedOnDailyAt": "2025-10-13T01:43:50.352Z",
            "title": "A Goal Without a Plan Is Just a Wish: Efficient and Effective Global\n  Planner Training for Long-Horizon Agent Tasks",
            "submittedOnDailyBy": {
                "_id": "637c99bbfe115289cfedfb44",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
                "isPro": false,
                "fullname": "ssz",
                "user": "ssz1111",
                "type": "user"
            },
            "summary": "Agents based on large language models (LLMs) struggle with brainless\ntrial-and-error and generating hallucinatory actions due to a lack of global\nplanning in long-horizon tasks. In this paper, we introduce a plan-and-execute\nframework and propose EAGLET, an efficient and effective planner training\nmethod to enhance the executor agent's planning abilities without human effort.\nSpecifically, we train a plug-and-play global planner through a two-step\nprocess: we first synthesize high-quality plans from an advanced LLM using our\nproposed homologous consensus filtering strategy, and apply fine-tuning as a\ncold start. Moreover, we further improve the planner with a rule-based\nreinforcement learning stage using a novel executor capability gain reward,\nensuring it can handle task instructions of varying difficulty. Experiments on\nthree long-horizon agent tasks show that executor agents equipped with our\nplanner outperform existing methods, achieving new state-of-the-art\nperformance. Meanwhile, EAGLET reduces training costs by 8x compared to\nRL-based baselines, and it does not require manual effort or extra training\ndata, offering an efficient and effective solution.",
            "upvotes": 3,
            "discussionId": "68ec6e11cd07fb414898c9a0",
            "ai_summary": "A plan-and-execute framework with EAGLET enhances LLM-based agents' planning abilities, achieving state-of-the-art performance in long-horizon tasks with reduced training costs.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "plan-and-execute framework",
                "EAGLET",
                "global planner",
                "homologous consensus filtering",
                "fine-tuning",
                "rule-based reinforcement learning",
                "executor capability gain reward",
                "long-horizon tasks"
            ]
        },
        "publishedAt": "2025-10-07T02:10:53.000Z",
        "title": "A Goal Without a Plan Is Just a Wish: Efficient and Effective Global\n  Planner Training for Long-Horizon Agent Tasks",
        "summary": "Agents based on large language models (LLMs) struggle with brainless\ntrial-and-error and generating hallucinatory actions due to a lack of global\nplanning in long-horizon tasks. In this paper, we introduce a plan-and-execute\nframework and propose EAGLET, an efficient and effective planner training\nmethod to enhance the executor agent's planning abilities without human effort.\nSpecifically, we train a plug-and-play global planner through a two-step\nprocess: we first synthesize high-quality plans from an advanced LLM using our\nproposed homologous consensus filtering strategy, and apply fine-tuning as a\ncold start. Moreover, we further improve the planner with a rule-based\nreinforcement learning stage using a novel executor capability gain reward,\nensuring it can handle task instructions of varying difficulty. Experiments on\nthree long-horizon agent tasks show that executor agents equipped with our\nplanner outperform existing methods, achieving new state-of-the-art\nperformance. Meanwhile, EAGLET reduces training costs by 8x compared to\nRL-based baselines, and it does not require manual effort or extra training\ndata, offering an efficient and effective solution.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05608.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637c99bbfe115289cfedfb44",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
            "fullname": "ssz",
            "name": "ssz1111",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.02898",
            "authors": [
                {
                    "_id": "68e3c2e473e20ab5778421ed",
                    "user": {
                        "_id": "63c67770b167901a39346c27",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673951039741-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Lorenzo Bianchi",
                        "user": "lorebianchi98",
                        "type": "user"
                    },
                    "name": "Lorenzo Bianchi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-08T08:03:13.038Z",
                    "hidden": false
                },
                {
                    "_id": "68e3c2e473e20ab5778421ee",
                    "user": {
                        "_id": "640d000904c679553cfa275b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d000904c679553cfa275b/v18gSEHfT1FvEpYTkWJNj.png",
                        "isPro": false,
                        "fullname": "Giacomo Pacini",
                        "user": "Ruggero1912",
                        "type": "user"
                    },
                    "name": "Giacomo Pacini",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-07T12:28:10.096Z",
                    "hidden": false
                },
                {
                    "_id": "68e3c2e473e20ab5778421ef",
                    "name": "Fabio Carrara",
                    "hidden": false
                },
                {
                    "_id": "68e3c2e473e20ab5778421f0",
                    "name": "Nicola Messina",
                    "hidden": false
                },
                {
                    "_id": "68e3c2e473e20ab5778421f1",
                    "name": "Giuseppe Amato",
                    "hidden": false
                },
                {
                    "_id": "68e3c2e473e20ab5778421f2",
                    "name": "Fabrizio Falchi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/640d000904c679553cfa275b/dX4ywkPPTvdYDFK_3Ansm.png",
                "https://cdn-uploads.huggingface.co/production/uploads/640d000904c679553cfa275b/BfiQpwyxYnalJSk9RbebA.webp"
            ],
            "publishedAt": "2025-10-03T11:05:56.000Z",
            "submittedOnDailyAt": "2025-10-13T06:13:08.962Z",
            "title": "One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework",
            "submittedOnDailyBy": {
                "_id": "640d000904c679553cfa275b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d000904c679553cfa275b/v18gSEHfT1FvEpYTkWJNj.png",
                "isPro": false,
                "fullname": "Giacomo Pacini",
                "user": "Ruggero1912",
                "type": "user"
            },
            "summary": "Zero-shot captioners are recently proposed models that utilize common-space\nvision-language representations to caption images without relying on paired\nimage-text data. To caption an image, they proceed by textually decoding a\ntext-aligned image feature, but they limit their scope to global\nrepresentations and whole-image captions. We present , a\nunified framework for zero-shot captioning that shifts from an image-centric to\na patch-centric paradigm, enabling the captioning of arbitrary regions without\nthe need of region-level supervision. Instead of relying on global image\nrepresentations, we treat individual patches as atomic captioning units and\naggregate them to describe arbitrary regions, from single patches to\nnon-contiguous areas and entire images. We analyze the key ingredients that\nenable current latent captioners to work in our novel proposed framework.\nExperiments demonstrate that backbones producing meaningful, dense visual\nfeatures, such as DINO, are key to achieving state-of-the-art performance in\nmultiple region-based captioning tasks. Compared to other baselines and\nstate-of-the-art competitors, our models achieve better performance on\nzero-shot dense, region-set, and a newly introduced trace captioning task,\nhighlighting the effectiveness of patch-wise semantic representations for\nscalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .",
            "upvotes": 3,
            "discussionId": "68e3c2e473e20ab5778421f3",
            "projectPage": "https://paciosoft.com/Patch-ioner/",
            "githubRepo": "https://github.com/Ruggero1912/Patch-ioner",
            "ai_summary": "A patch-centric framework for zero-shot captioning achieves state-of-the-art performance by using dense visual features from models like DINO to caption arbitrary image regions.",
            "ai_keywords": [
                "zero-shot captioners",
                "common-space vision-language representations",
                "text-aligned image feature",
                "patch-centric paradigm",
                "region-level supervision",
                "atomic captioning units",
                "latent captioners",
                "dense visual features",
                "DINO",
                "region-based captioning tasks",
                "zero-shot dense captioning",
                "region-set captioning",
                "trace captioning task",
                "patch-wise semantic representations"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-10-03T07:05:56.000Z",
        "title": "One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework",
        "summary": "Zero-shot captioners are recently proposed models that utilize common-space\nvision-language representations to caption images without relying on paired\nimage-text data. To caption an image, they proceed by textually decoding a\ntext-aligned image feature, but they limit their scope to global\nrepresentations and whole-image captions. We present , a\nunified framework for zero-shot captioning that shifts from an image-centric to\na patch-centric paradigm, enabling the captioning of arbitrary regions without\nthe need of region-level supervision. Instead of relying on global image\nrepresentations, we treat individual patches as atomic captioning units and\naggregate them to describe arbitrary regions, from single patches to\nnon-contiguous areas and entire images. We analyze the key ingredients that\nenable current latent captioners to work in our novel proposed framework.\nExperiments demonstrate that backbones producing meaningful, dense visual\nfeatures, such as DINO, are key to achieving state-of-the-art performance in\nmultiple region-based captioning tasks. Compared to other baselines and\nstate-of-the-art competitors, our models achieve better performance on\nzero-shot dense, region-set, and a newly introduced trace captioning task,\nhighlighting the effectiveness of patch-wise semantic representations for\nscalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/640d000904c679553cfa275b/dX4ywkPPTvdYDFK_3Ansm.png",
            "https://cdn-uploads.huggingface.co/production/uploads/640d000904c679553cfa275b/BfiQpwyxYnalJSk9RbebA.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02898.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640d000904c679553cfa275b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d000904c679553cfa275b/v18gSEHfT1FvEpYTkWJNj.png",
            "fullname": "Giacomo Pacini",
            "name": "Ruggero1912",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.09535",
            "authors": [
                {
                    "_id": "68ec6e2dcd07fb414898c9a2",
                    "user": {
                        "_id": "6447ca6ca478b20f1755b294",
                        "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
                        "isPro": false,
                        "fullname": "Feifan Song",
                        "user": "songff",
                        "type": "user"
                    },
                    "name": "Feifan Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T16:09:22.284Z",
                    "hidden": false
                },
                {
                    "_id": "68ec6e2dcd07fb414898c9a3",
                    "user": {
                        "_id": "67244a81aa8556c561925ab6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/w-vZ0uwYACagrNq-H1oyO.jpeg",
                        "isPro": false,
                        "fullname": "Shaohang Wei",
                        "user": "SylvainWei",
                        "type": "user"
                    },
                    "name": "Shaohang Wei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T16:09:24.252Z",
                    "hidden": false
                },
                {
                    "_id": "68ec6e2dcd07fb414898c9a4",
                    "name": "Bofei Gao",
                    "hidden": false
                },
                {
                    "_id": "68ec6e2dcd07fb414898c9a5",
                    "name": "Yejie Wang",
                    "hidden": false
                },
                {
                    "_id": "68ec6e2dcd07fb414898c9a6",
                    "name": "Wen Luo",
                    "hidden": false
                },
                {
                    "_id": "68ec6e2dcd07fb414898c9a7",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "68ec6e2dcd07fb414898c9a8",
                    "name": "Linli Yao",
                    "hidden": false
                },
                {
                    "_id": "68ec6e2dcd07fb414898c9a9",
                    "name": "Weimin Xiong",
                    "hidden": false
                },
                {
                    "_id": "68ec6e2dcd07fb414898c9aa",
                    "name": "Liang Chen",
                    "hidden": false
                },
                {
                    "_id": "68ec6e2dcd07fb414898c9ab",
                    "name": "Tianyu Liu",
                    "hidden": false
                },
                {
                    "_id": "68ec6e2dcd07fb414898c9ac",
                    "name": "Houfeng Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T16:49:03.000Z",
            "submittedOnDailyAt": "2025-10-13T01:43:12.548Z",
            "title": "Mitigating Overthinking through Reasoning Shaping",
            "submittedOnDailyBy": {
                "_id": "6447ca6ca478b20f1755b294",
                "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
                "isPro": false,
                "fullname": "Feifan Song",
                "user": "songff",
                "type": "user"
            },
            "summary": "Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier\nReward (RLVR) have shown great power in problem solving, yet they often cause\noverthinking: excessive, meandering reasoning that inflates computational cost.\nPrior designs of penalization in RLVR manage to reduce token consumption while\noften harming model performance, which arises from the oversimplicity of\ntoken-level supervision. In this paper, we argue that the granularity of\nsupervision plays a crucial role in balancing efficiency and accuracy, and\npropose Group Relative Segment Penalization (GRSP), a step-level method to\nregularize reasoning. Since preliminary analyses show that reasoning segments\nare strongly correlated with token consumption and model performance, we design\na length-aware weighting mechanism across segment clusters. Extensive\nexperiments demonstrate that GRSP achieves superior token efficiency without\nheavily compromising accuracy, especially the advantages with harder problems.\nMoreover, GRSP stabilizes RL training and scales effectively across model\nsizes.",
            "upvotes": 2,
            "discussionId": "68ec6e2dcd07fb414898c9ad",
            "ai_summary": "Group Relative Segment Penalization (GRSP) improves token efficiency in large reasoning models without significantly reducing accuracy, especially for complex problems, by regularizing reasoning at the step level.",
            "ai_keywords": [
                "Reinforcement Learning from Verifier Reward (RLVR)",
                "overthinking",
                "token consumption",
                "model performance",
                "token-level supervision",
                "Group Relative Segment Penalization (GRSP)",
                "step-level method",
                "reasoning segments",
                "length-aware weighting mechanism",
                "segment clusters",
                "RL training",
                "model sizes"
            ]
        },
        "publishedAt": "2025-10-10T12:49:03.000Z",
        "title": "Mitigating Overthinking through Reasoning Shaping",
        "summary": "Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier\nReward (RLVR) have shown great power in problem solving, yet they often cause\noverthinking: excessive, meandering reasoning that inflates computational cost.\nPrior designs of penalization in RLVR manage to reduce token consumption while\noften harming model performance, which arises from the oversimplicity of\ntoken-level supervision. In this paper, we argue that the granularity of\nsupervision plays a crucial role in balancing efficiency and accuracy, and\npropose Group Relative Segment Penalization (GRSP), a step-level method to\nregularize reasoning. Since preliminary analyses show that reasoning segments\nare strongly correlated with token consumption and model performance, we design\na length-aware weighting mechanism across segment clusters. Extensive\nexperiments demonstrate that GRSP achieves superior token efficiency without\nheavily compromising accuracy, especially the advantages with harder problems.\nMoreover, GRSP stabilizes RL training and scales effectively across model\nsizes.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09535.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6447ca6ca478b20f1755b294",
            "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
            "fullname": "Feifan Song",
            "name": "songff",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08994",
            "authors": [
                {
                    "_id": "68ec791ccd07fb414898ca2e",
                    "name": "Yao Teng",
                    "hidden": false
                },
                {
                    "_id": "68ec791ccd07fb414898ca2f",
                    "name": "Fuyun Wang",
                    "hidden": false
                },
                {
                    "_id": "68ec791ccd07fb414898ca30",
                    "name": "Xian Liu",
                    "hidden": false
                },
                {
                    "_id": "68ec791ccd07fb414898ca31",
                    "name": "Zhekai Chen",
                    "hidden": false
                },
                {
                    "_id": "68ec791ccd07fb414898ca32",
                    "name": "Han Shi",
                    "hidden": false
                },
                {
                    "_id": "68ec791ccd07fb414898ca33",
                    "name": "Yu Wang",
                    "hidden": false
                },
                {
                    "_id": "68ec791ccd07fb414898ca34",
                    "name": "Zhenguo Li",
                    "hidden": false
                },
                {
                    "_id": "68ec791ccd07fb414898ca35",
                    "name": "Weiyang Liu",
                    "hidden": false
                },
                {
                    "_id": "68ec791ccd07fb414898ca36",
                    "name": "Difan Zou",
                    "hidden": false
                },
                {
                    "_id": "68ec791ccd07fb414898ca37",
                    "name": "Xihui Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T04:30:45.000Z",
            "submittedOnDailyAt": "2025-10-13T02:32:03.582Z",
            "title": "Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive\n  Text-to-image Generation",
            "submittedOnDailyBy": {
                "_id": "6427e08288215cee63b1c44d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6427e08288215cee63b1c44d/rzaG978FF-ywzicWNl_xl.jpeg",
                "isPro": false,
                "fullname": "yao teng",
                "user": "tytyt",
                "type": "user"
            },
            "summary": "As a new paradigm of visual content generation, autoregressive text-to-image\nmodels suffer from slow inference due to their sequential token-by-token\ndecoding process, often requiring thousands of model forward passes to generate\na single image. To address this inefficiency, we propose Speculative\nJacobi-Denoising Decoding (SJD2), a framework that incorporates the denoising\nprocess into Jacobi iterations to enable parallel token generation in\nautoregressive models. Our method introduces a next-clean-token prediction\nparadigm that enables the pre-trained autoregressive models to accept\nnoise-perturbed token embeddings and predict the next clean tokens through\nlow-cost fine-tuning. This denoising paradigm guides the model towards more\nstable Jacobi trajectories. During inference, our method initializes token\nsequences with Gaussian noise and performs iterative\nnext-clean-token-prediction in the embedding space. We employ a probabilistic\ncriterion to verify and accept multiple tokens in parallel, and refine the\nunaccepted tokens for the next iteration with the denoising trajectory.\nExperiments show that our method can accelerate generation by reducing model\nforward passes while maintaining the visual quality of generated images.",
            "upvotes": 2,
            "discussionId": "68ec791ccd07fb414898ca38",
            "ai_summary": "Speculative Jacobi-Denoising Decoding accelerates autoregressive text-to-image generation by enabling parallel token prediction and reducing model forward passes.",
            "ai_keywords": [
                "autoregressive text-to-image models",
                "Speculative Jacobi-Denoising Decoding",
                "denoising process",
                "Jacobi iterations",
                "next-clean-token prediction",
                "token embeddings",
                "low-cost fine-tuning",
                "Gaussian noise",
                "next-clean-token-prediction",
                "probabilistic criterion",
                "visual quality"
            ]
        },
        "publishedAt": "2025-10-10T00:30:45.000Z",
        "title": "Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive\n  Text-to-image Generation",
        "summary": "As a new paradigm of visual content generation, autoregressive text-to-image\nmodels suffer from slow inference due to their sequential token-by-token\ndecoding process, often requiring thousands of model forward passes to generate\na single image. To address this inefficiency, we propose Speculative\nJacobi-Denoising Decoding (SJD2), a framework that incorporates the denoising\nprocess into Jacobi iterations to enable parallel token generation in\nautoregressive models. Our method introduces a next-clean-token prediction\nparadigm that enables the pre-trained autoregressive models to accept\nnoise-perturbed token embeddings and predict the next clean tokens through\nlow-cost fine-tuning. This denoising paradigm guides the model towards more\nstable Jacobi trajectories. During inference, our method initializes token\nsequences with Gaussian noise and performs iterative\nnext-clean-token-prediction in the embedding space. We employ a probabilistic\ncriterion to verify and accept multiple tokens in parallel, and refine the\nunaccepted tokens for the next iteration with the denoising trajectory.\nExperiments show that our method can accelerate generation by reducing model\nforward passes while maintaining the visual quality of generated images.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08994.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6427e08288215cee63b1c44d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6427e08288215cee63b1c44d/rzaG978FF-ywzicWNl_xl.jpeg",
            "fullname": "yao teng",
            "name": "tytyt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08872",
            "authors": [
                {
                    "_id": "68ec51cecd07fb414898c8b1",
                    "user": {
                        "_id": "65621fd68631d43d2baf33b2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1JemNCnPkS1mE3SNygsE2.png",
                        "isPro": false,
                        "fullname": "siqi zhu",
                        "user": "zsqzz",
                        "type": "user"
                    },
                    "name": "Siqi Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:06:14.231Z",
                    "hidden": false
                },
                {
                    "_id": "68ec51cecd07fb414898c8b2",
                    "name": "David Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ec51cecd07fb414898c8b3",
                    "name": "Pedro Cisneros-Velarde",
                    "hidden": false
                },
                {
                    "_id": "68ec51cecd07fb414898c8b4",
                    "name": "Jiaxuan You",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T00:05:14.000Z",
            "submittedOnDailyAt": "2025-10-13T02:22:59.294Z",
            "title": "GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare",
            "submittedOnDailyBy": {
                "_id": "65621fd68631d43d2baf33b2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1JemNCnPkS1mE3SNygsE2.png",
                "isPro": false,
                "fullname": "siqi zhu",
                "user": "zsqzz",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have achieved remarkable progress in reasoning,\nyet sometimes produce responses that are suboptimal for users in tasks such as\nwriting, information seeking, or providing practical guidance. Conventional\nalignment practices typically assume that maximizing model reward also\nmaximizes user welfare, but this assumption frequently fails in practice:\nmodels may over-clarify or generate overly verbose reasoning when users prefer\nconcise answers. Such behaviors resemble the prisoner's dilemma, where\nindividually rational choices lead to socially suboptimal outcomes. The\nfundamental challenge is the lack of a principled decision making mechanism\nthat mutually benefits both the LLM and the user. We propose Game-Theoretic\nAlignment (GTAlign), an alignment framework that integrates game-theoretic\ndecision making into both reasoning and training. During reasoning, the model\nexplicitly treats user-LLM interaction as a strategic game: it constructs\npayoff matrices within its reasoning chain to estimate welfare for both itself\nand the user, and then selects actions that are mutually beneficial. During\ntraining, we introduce a mutual welfare reward that reinforces cooperative\nresponses, aligning model behavior with socially efficient outcomes. In\naddition, we introduce an inference technique that leverages game-theoretic\nreasoning to dynamically adapt LLM's response when pricing policies of LLM\nservice change. Extensive experiments demonstrate that GTAlign substantially\nimproves reasoning efficiency, answer quality, and mutual welfare compared to\nbaselines across diverse tasks. The code is available at\nhttps://github.com/ulab-uiuc/GTAlign .",
            "upvotes": 2,
            "discussionId": "68ec51cecd07fb414898c8b5",
            "ai_summary": "Game-Theoretic Alignment (GTAlign) improves Large Language Model (LLM) performance by integrating game-theoretic decision making into reasoning and training, enhancing efficiency, answer quality, and mutual welfare.",
            "ai_keywords": [
                "Large Language Models",
                "Game-Theoretic Alignment",
                "GTAlign",
                "strategic game",
                "payoff matrices",
                "mutual welfare reward",
                "inference technique",
                "reasoning efficiency",
                "answer quality",
                "mutual welfare"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "65448bef5b5d9185ba3202b9",
                "name": "UIUC-CS",
                "fullname": "University of Illinois at Urbana-Champaign",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
            }
        },
        "publishedAt": "2025-10-09T20:05:14.000Z",
        "title": "GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare",
        "summary": "Large Language Models (LLMs) have achieved remarkable progress in reasoning,\nyet sometimes produce responses that are suboptimal for users in tasks such as\nwriting, information seeking, or providing practical guidance. Conventional\nalignment practices typically assume that maximizing model reward also\nmaximizes user welfare, but this assumption frequently fails in practice:\nmodels may over-clarify or generate overly verbose reasoning when users prefer\nconcise answers. Such behaviors resemble the prisoner's dilemma, where\nindividually rational choices lead to socially suboptimal outcomes. The\nfundamental challenge is the lack of a principled decision making mechanism\nthat mutually benefits both the LLM and the user. We propose Game-Theoretic\nAlignment (GTAlign), an alignment framework that integrates game-theoretic\ndecision making into both reasoning and training. During reasoning, the model\nexplicitly treats user-LLM interaction as a strategic game: it constructs\npayoff matrices within its reasoning chain to estimate welfare for both itself\nand the user, and then selects actions that are mutually beneficial. During\ntraining, we introduce a mutual welfare reward that reinforces cooperative\nresponses, aligning model behavior with socially efficient outcomes. In\naddition, we introduce an inference technique that leverages game-theoretic\nreasoning to dynamically adapt LLM's response when pricing policies of LLM\nservice change. Extensive experiments demonstrate that GTAlign substantially\nimproves reasoning efficiency, answer quality, and mutual welfare compared to\nbaselines across diverse tasks. The code is available at\nhttps://github.com/ulab-uiuc/GTAlign .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08872.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65621fd68631d43d2baf33b2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1JemNCnPkS1mE3SNygsE2.png",
            "fullname": "siqi zhu",
            "name": "zsqzz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "65448bef5b5d9185ba3202b9",
            "name": "UIUC-CS",
            "fullname": "University of Illinois at Urbana-Champaign",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.05528",
            "authors": [
                {
                    "_id": "68e99eec012880a10864b653",
                    "user": {
                        "_id": "66ce9a5a7a06db4ba23faba1",
                        "avatarUrl": "/avatars/615277c22b83126a42a1cf2d4bf66396.svg",
                        "isPro": false,
                        "fullname": "Lawrence Liu",
                        "user": "LawrenceLiu",
                        "type": "user"
                    },
                    "name": "Lawrence Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-11T13:27:45.484Z",
                    "hidden": false
                },
                {
                    "_id": "68e99eec012880a10864b654",
                    "name": "Alexander Liu",
                    "hidden": false
                },
                {
                    "_id": "68e99eec012880a10864b655",
                    "name": "Mengdi Wang",
                    "hidden": false
                },
                {
                    "_id": "68e99eec012880a10864b656",
                    "name": "Tuo Zhao",
                    "hidden": false
                },
                {
                    "_id": "68e99eec012880a10864b657",
                    "name": "Lin F. Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-07T02:39:20.000Z",
            "submittedOnDailyAt": "2025-10-13T17:07:55.838Z",
            "title": "ARMOR: High-Performance Semi-Structured Pruning via Adaptive Matrix\n  Factorization",
            "submittedOnDailyBy": {
                "_id": "66ce9a5a7a06db4ba23faba1",
                "avatarUrl": "/avatars/615277c22b83126a42a1cf2d4bf66396.svg",
                "isPro": false,
                "fullname": "Lawrence Liu",
                "user": "LawrenceLiu",
                "type": "user"
            },
            "summary": "Large language models (LLMs) present significant deployment challenges due to\ntheir immense computational and memory requirements. While semi-structured\npruning, particularly 2:4 sparsity, offers a path to practical hardware\nacceleration, existing methods often incur substantial performance degradation.\nTo bridge this gap, we introduce ARMOR: (Adaptive Representation with\nMatrix-factORization), a novel one-shot post-training pruning algorithm.\nInstead of directly pruning weights, ARMOR factorizes each weight matrix into a\n2:4 sparse core wrapped by two low-overhead, block diagonal matrices. These\nwrappers act as efficient pre and post-transformation error correctors,\noffering greater flexibility to preserve model quality compared to conventional\n2:4 pruning techniques. The sparse core and block diagonal wrappers are chosen\nthrough a block coordinate descent algorithm that minimizes a layer-wise proxy\nloss. We theoretically prove this optimization is guaranteed to converge to a\nsolution with a proxy loss less than or equal to state-of-the-art pruning\nalgorithms. Experiments on Llama (Touvron et al., 2023; Dubey et al., 2024) and\nQwen (Yang et al., 2025) model families demonstrate that ARMOR consistently and\nsignificantly outperforms state-of-the-art 2:4 pruning methods across a wide\nrange of downstream tasks and perplexity evaluations. ARMOR achieves this\nsuperior performance while retaining the inference speedups and substantial\nmemory usage reductions of 2:4 pruning, establishing a more effective trade-off\nbetween model compression and task accuracy",
            "upvotes": 2,
            "discussionId": "68e99eec012880a10864b658",
            "ai_summary": "ARMOR, a novel post-training pruning algorithm, enhances the performance of large language models by factorizing weight matrices into sparse cores and block diagonal matrices, achieving better accuracy and memory efficiency compared to conventional pruning methods.",
            "ai_keywords": [
                "LLMs",
                "semi-structured pruning",
                "2:4 sparsity",
                "ARMOR",
                "matrix-factORization",
                "block diagonal matrices",
                "block coordinate descent",
                "proxy loss",
                "Llama",
                "Qwen",
                "model compression",
                "task accuracy"
            ],
            "organization": {
                "_id": "67784c39dac147922d8d09f0",
                "name": "UCLA",
                "fullname": "University of California, Los Angeles",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"
            }
        },
        "publishedAt": "2025-10-06T22:39:20.000Z",
        "title": "ARMOR: High-Performance Semi-Structured Pruning via Adaptive Matrix\n  Factorization",
        "summary": "Large language models (LLMs) present significant deployment challenges due to\ntheir immense computational and memory requirements. While semi-structured\npruning, particularly 2:4 sparsity, offers a path to practical hardware\nacceleration, existing methods often incur substantial performance degradation.\nTo bridge this gap, we introduce ARMOR: (Adaptive Representation with\nMatrix-factORization), a novel one-shot post-training pruning algorithm.\nInstead of directly pruning weights, ARMOR factorizes each weight matrix into a\n2:4 sparse core wrapped by two low-overhead, block diagonal matrices. These\nwrappers act as efficient pre and post-transformation error correctors,\noffering greater flexibility to preserve model quality compared to conventional\n2:4 pruning techniques. The sparse core and block diagonal wrappers are chosen\nthrough a block coordinate descent algorithm that minimizes a layer-wise proxy\nloss. We theoretically prove this optimization is guaranteed to converge to a\nsolution with a proxy loss less than or equal to state-of-the-art pruning\nalgorithms. Experiments on Llama (Touvron et al., 2023; Dubey et al., 2024) and\nQwen (Yang et al., 2025) model families demonstrate that ARMOR consistently and\nsignificantly outperforms state-of-the-art 2:4 pruning methods across a wide\nrange of downstream tasks and perplexity evaluations. ARMOR achieves this\nsuperior performance while retaining the inference speedups and substantial\nmemory usage reductions of 2:4 pruning, establishing a more effective trade-off\nbetween model compression and task accuracy",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05528.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66ce9a5a7a06db4ba23faba1",
            "avatarUrl": "/avatars/615277c22b83126a42a1cf2d4bf66396.svg",
            "fullname": "Lawrence Liu",
            "name": "LawrenceLiu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "67784c39dac147922d8d09f0",
            "name": "UCLA",
            "fullname": "University of California, Los Angeles",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.09320",
            "authors": [
                {
                    "_id": "68ecedfcb0aa732b88f31fb9",
                    "name": "Wenyao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ecedfcb0aa732b88f31fba",
                    "name": "Hongsi Liu",
                    "hidden": false
                },
                {
                    "_id": "68ecedfcb0aa732b88f31fbb",
                    "name": "Bohan Li",
                    "hidden": false
                },
                {
                    "_id": "68ecedfcb0aa732b88f31fbc",
                    "name": "Jiawei He",
                    "hidden": false
                },
                {
                    "_id": "68ecedfcb0aa732b88f31fbd",
                    "name": "Zekun Qi",
                    "hidden": false
                },
                {
                    "_id": "68ecedfcb0aa732b88f31fbe",
                    "name": "Yunnan Wang",
                    "hidden": false
                },
                {
                    "_id": "68ecedfcb0aa732b88f31fbf",
                    "name": "Shengyang Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ecedfcb0aa732b88f31fc0",
                    "name": "Xinqiang Yu",
                    "hidden": false
                },
                {
                    "_id": "68ecedfcb0aa732b88f31fc1",
                    "name": "Wenjun Zeng",
                    "hidden": false
                },
                {
                    "_id": "68ecedfcb0aa732b88f31fc2",
                    "name": "Xin Jin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T12:20:19.000Z",
            "submittedOnDailyAt": "2025-10-13T10:54:16.375Z",
            "title": "Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance\n  for Self-supervised Monocular Depth Estimation",
            "submittedOnDailyBy": {
                "_id": "65f9533b136fb8ddbd14e1fa",
                "avatarUrl": "/avatars/d88f75da0448093ccd1babba2a37d73f.svg",
                "isPro": false,
                "fullname": "Zhang",
                "user": "WenyaoZhang",
                "type": "user"
            },
            "summary": "Current self-supervised monocular depth estimation (MDE) approaches encounter\nperformance limitations due to insufficient semantic-spatial knowledge\nextraction. To address this challenge, we propose Hybrid-depth, a novel\nframework that systematically integrates foundation models (e.g., CLIP and\nDINO) to extract visual priors and acquire sufficient contextual information\nfor MDE. Our approach introduces a coarse-to-fine progressive learning\nframework: 1) Firstly, we aggregate multi-grained features from CLIP (global\nsemantics) and DINO (local spatial details) under contrastive language\nguidance. A proxy task comparing close-distant image patches is designed to\nenforce depth-aware feature alignment using text prompts; 2) Next, building on\nthe coarse features, we integrate camera pose information and pixel-wise\nlanguage alignment to refine depth predictions. This module seamlessly\nintegrates with existing self-supervised MDE pipelines (e.g., Monodepth2,\nManyDepth) as a plug-and-play depth encoder, enhancing continuous depth\nestimation. By aggregating CLIP's semantic context and DINO's spatial details\nthrough language guidance, our method effectively addresses feature granularity\nmismatches. Extensive experiments on the KITTI benchmark demonstrate that our\nmethod significantly outperforms SOTA methods across all metrics, which also\nindeed benefits downstream tasks like BEV perception. Code is available at\nhttps://github.com/Zhangwenyao1/Hybrid-depth.",
            "upvotes": 1,
            "discussionId": "68ecedfcb0aa732b88f31fc3",
            "githubRepo": "https://github.com/Zhangwenyao1/Hybrid-depth",
            "ai_summary": "Hybrid-depth framework integrates CLIP and DINO with language guidance to enhance self-supervised monocular depth estimation by addressing semantic and spatial feature mismatches.",
            "ai_keywords": [
                "self-supervised monocular depth estimation",
                "foundation models",
                "CLIP",
                "DINO",
                "coarse-to-fine progressive learning",
                "contrastive language guidance",
                "proxy task",
                "depth-aware feature alignment",
                "camera pose information",
                "pixel-wise language alignment",
                "KITTI benchmark",
                "BEV perception"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-10-10T08:20:19.000Z",
        "title": "Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance\n  for Self-supervised Monocular Depth Estimation",
        "summary": "Current self-supervised monocular depth estimation (MDE) approaches encounter\nperformance limitations due to insufficient semantic-spatial knowledge\nextraction. To address this challenge, we propose Hybrid-depth, a novel\nframework that systematically integrates foundation models (e.g., CLIP and\nDINO) to extract visual priors and acquire sufficient contextual information\nfor MDE. Our approach introduces a coarse-to-fine progressive learning\nframework: 1) Firstly, we aggregate multi-grained features from CLIP (global\nsemantics) and DINO (local spatial details) under contrastive language\nguidance. A proxy task comparing close-distant image patches is designed to\nenforce depth-aware feature alignment using text prompts; 2) Next, building on\nthe coarse features, we integrate camera pose information and pixel-wise\nlanguage alignment to refine depth predictions. This module seamlessly\nintegrates with existing self-supervised MDE pipelines (e.g., Monodepth2,\nManyDepth) as a plug-and-play depth encoder, enhancing continuous depth\nestimation. By aggregating CLIP's semantic context and DINO's spatial details\nthrough language guidance, our method effectively addresses feature granularity\nmismatches. Extensive experiments on the KITTI benchmark demonstrate that our\nmethod significantly outperforms SOTA methods across all metrics, which also\nindeed benefits downstream tasks like BEV perception. Code is available at\nhttps://github.com/Zhangwenyao1/Hybrid-depth.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09320.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f9533b136fb8ddbd14e1fa",
            "avatarUrl": "/avatars/d88f75da0448093ccd1babba2a37d73f.svg",
            "fullname": "Zhang",
            "name": "WenyaoZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08564",
            "authors": [
                {
                    "_id": "68e872dd95e8e6771df389e5",
                    "name": "Zhen Zhu",
                    "hidden": false
                },
                {
                    "_id": "68e872dd95e8e6771df389e6",
                    "name": "Yiming Gong",
                    "hidden": false
                },
                {
                    "_id": "68e872dd95e8e6771df389e7",
                    "name": "Yao Xiao",
                    "hidden": false
                },
                {
                    "_id": "68e872dd95e8e6771df389e8",
                    "name": "Yaoyao Liu",
                    "hidden": false
                },
                {
                    "_id": "68e872dd95e8e6771df389e9",
                    "name": "Derek Hoiem",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T17:59:37.000Z",
            "submittedOnDailyAt": "2025-10-13T21:21:44.141Z",
            "title": "How to Teach Large Multimodal Models New Skills",
            "submittedOnDailyBy": {
                "_id": "6494c96259afb52e1594f45c",
                "avatarUrl": "/avatars/a82e2f19cd377b6452932fceaf2d71c8.svg",
                "isPro": false,
                "fullname": "Jesse Mel",
                "user": "Jessemel",
                "type": "user"
            },
            "summary": "How can we teach large multimodal models (LMMs) new skills without erasing\nprior abilities? We study sequential fine-tuning on five target skills while\nmonitoring general ability on eight held-out benchmarks across three model\nfamilies. We observe that apparent \"forgetting\" on held-out tasks after narrow\nfine-tuning can partly recover at later stages. We trace this behavior to a\nmeasurable shift in the output token distribution, manifested through a simple\ncounting-bias probe that co-varies with forgetting. Guided by this picture, we\nidentify two simple, robust tuning recipes that learn strongly while limiting\ndrift: (i) updating only the self-attention projection layers, and (ii)\nupdating only the MLP Gate&Up while freezing the Down projection. Across models\nand tasks, these choices deliver strong target gains while largely preserving\nheld-out performance. Code is available at\nhttps://github.com/jessemelpolio/LMM_CL",
            "upvotes": 1,
            "discussionId": "68e872dd95e8e6771df389ea",
            "githubRepo": "https://github.com/jessemelpolio/LMM_CL",
            "ai_summary": "Sequential fine-tuning of large multimodal models can recover from forgetting by selectively updating specific layers, thus preserving prior abilities.",
            "ai_keywords": [
                "multimodal models",
                "sequential fine-tuning",
                "self-attention projection layers",
                "MLP Gate&Up",
                "Down projection"
            ],
            "githubStars": 13,
            "organization": {
                "_id": "65448bef5b5d9185ba3202b9",
                "name": "UIUC-CS",
                "fullname": "University of Illinois at Urbana-Champaign",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
            }
        },
        "publishedAt": "2025-10-09T13:59:37.000Z",
        "title": "How to Teach Large Multimodal Models New Skills",
        "summary": "How can we teach large multimodal models (LMMs) new skills without erasing\nprior abilities? We study sequential fine-tuning on five target skills while\nmonitoring general ability on eight held-out benchmarks across three model\nfamilies. We observe that apparent \"forgetting\" on held-out tasks after narrow\nfine-tuning can partly recover at later stages. We trace this behavior to a\nmeasurable shift in the output token distribution, manifested through a simple\ncounting-bias probe that co-varies with forgetting. Guided by this picture, we\nidentify two simple, robust tuning recipes that learn strongly while limiting\ndrift: (i) updating only the self-attention projection layers, and (ii)\nupdating only the MLP Gate&Up while freezing the Down projection. Across models\nand tasks, these choices deliver strong target gains while largely preserving\nheld-out performance. Code is available at\nhttps://github.com/jessemelpolio/LMM_CL",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08564.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6494c96259afb52e1594f45c",
            "avatarUrl": "/avatars/a82e2f19cd377b6452932fceaf2d71c8.svg",
            "fullname": "Jesse Mel",
            "name": "Jessemel",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "65448bef5b5d9185ba3202b9",
            "name": "UIUC-CS",
            "fullname": "University of Illinois at Urbana-Champaign",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08492",
            "authors": [
                {
                    "_id": "68eb5c10012880a10864bba4",
                    "name": "Sharut Gupta",
                    "hidden": false
                },
                {
                    "_id": "68eb5c10012880a10864bba5",
                    "name": "Shobhita Sundaram",
                    "hidden": false
                },
                {
                    "_id": "68eb5c10012880a10864bba6",
                    "name": "Chenyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68eb5c10012880a10864bba7",
                    "name": "Stefanie Jegelka",
                    "hidden": false
                },
                {
                    "_id": "68eb5c10012880a10864bba8",
                    "name": "Phillip Isola",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/5f1158120c833276f61f1a84/mNKF2e-xCHiUIdFH5sxxN.jpeg"
            ],
            "publishedAt": "2025-10-09T17:32:23.000Z",
            "submittedOnDailyAt": "2025-10-13T06:01:00.997Z",
            "title": "Better Together: Leveraging Unpaired Multimodal Data for Stronger\n  Unimodal Models",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": true,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "Traditional multimodal learners find unified representations for tasks like\nvisual question answering, but rely heavily on paired datasets. However, an\noverlooked yet potentially powerful question is: can one leverage auxiliary\nunpaired multimodal data to directly enhance representation learning in a\ntarget modality? We introduce UML: Unpaired Multimodal Learner, a\nmodality-agnostic training paradigm in which a single model alternately\nprocesses inputs from different modalities while sharing parameters across\nthem. This design exploits the assumption that different modalities are\nprojections of a shared underlying reality, allowing the model to benefit from\ncross-modal structure without requiring explicit pairs. Theoretically, under\nlinear data-generating assumptions, we show that unpaired auxiliary data can\nyield representations strictly more informative about the data-generating\nprocess than unimodal training. Empirically, we show that using unpaired data\nfrom auxiliary modalities -- such as text, audio, or images -- consistently\nimproves downstream performance across diverse unimodal targets such as image\nand audio. Our project page: https://unpaired-multimodal.github.io/",
            "upvotes": 1,
            "discussionId": "68eb5c10012880a10864bba9",
            "projectPage": "https://unpaired-multimodal.github.io/",
            "githubRepo": "https://github.com/Sharut/Unpaired-Multimodal-Learning/",
            "ai_summary": "UML, an unpaired multimodal learner, enhances representation learning in a target modality by leveraging auxiliary unpaired data from different modalities.",
            "ai_keywords": [
                "multimodal learners",
                "visual question answering",
                "paired datasets",
                "unpaired multimodal data",
                "modality-agnostic training",
                "cross-modal structure",
                "unimodal training",
                "downstream performance"
            ],
            "githubStars": 27,
            "organization": {
                "_id": "63728bde14d543d507ae970d",
                "name": "MIT",
                "fullname": "Massachusetts Institute of Technology",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"
            }
        },
        "publishedAt": "2025-10-09T13:32:23.000Z",
        "title": "Better Together: Leveraging Unpaired Multimodal Data for Stronger\n  Unimodal Models",
        "summary": "Traditional multimodal learners find unified representations for tasks like\nvisual question answering, but rely heavily on paired datasets. However, an\noverlooked yet potentially powerful question is: can one leverage auxiliary\nunpaired multimodal data to directly enhance representation learning in a\ntarget modality? We introduce UML: Unpaired Multimodal Learner, a\nmodality-agnostic training paradigm in which a single model alternately\nprocesses inputs from different modalities while sharing parameters across\nthem. This design exploits the assumption that different modalities are\nprojections of a shared underlying reality, allowing the model to benefit from\ncross-modal structure without requiring explicit pairs. Theoretically, under\nlinear data-generating assumptions, we show that unpaired auxiliary data can\nyield representations strictly more informative about the data-generating\nprocess than unimodal training. Empirically, we show that using unpaired data\nfrom auxiliary modalities -- such as text, audio, or images -- consistently\nimproves downstream performance across diverse unimodal targets such as image\nand audio. Our project page: https://unpaired-multimodal.github.io/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/5f1158120c833276f61f1a84/mNKF2e-xCHiUIdFH5sxxN.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08492.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 993
        },
        "organization": {
            "_id": "63728bde14d543d507ae970d",
            "name": "MIT",
            "fullname": "Massachusetts Institute of Technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07896",
            "authors": [
                {
                    "_id": "68ec7ad6cd07fb414898ca3a",
                    "name": "Jiayu Yang",
                    "hidden": false
                },
                {
                    "_id": "68ec7ad6cd07fb414898ca3b",
                    "user": {
                        "_id": "658247c592b5a9664de63882",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658247c592b5a9664de63882/Jn03voLQjDlB3YjpSi-PI.jpeg",
                        "isPro": false,
                        "fullname": "fan",
                        "user": "EasonFan",
                        "type": "user"
                    },
                    "name": "Yuxuan Fan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T16:09:05.686Z",
                    "hidden": false
                },
                {
                    "_id": "68ec7ad6cd07fb414898ca3c",
                    "name": "Songning Lai",
                    "hidden": false
                },
                {
                    "_id": "68ec7ad6cd07fb414898ca3d",
                    "name": "Shengen Wu",
                    "hidden": false
                },
                {
                    "_id": "68ec7ad6cd07fb414898ca3e",
                    "name": "Jiaqi Tang",
                    "hidden": false
                },
                {
                    "_id": "68ec7ad6cd07fb414898ca3f",
                    "name": "Chun Kang",
                    "hidden": false
                },
                {
                    "_id": "68ec7ad6cd07fb414898ca40",
                    "name": "Zhijiang Guo",
                    "hidden": false
                },
                {
                    "_id": "68ec7ad6cd07fb414898ca41",
                    "name": "Yutao Yue",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T07:46:08.000Z",
            "submittedOnDailyAt": "2025-10-13T02:45:04.644Z",
            "title": "ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual\n  Recall",
            "submittedOnDailyBy": {
                "_id": "658247c592b5a9664de63882",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658247c592b5a9664de63882/Jn03voLQjDlB3YjpSi-PI.jpeg",
                "isPro": false,
                "fullname": "fan",
                "user": "EasonFan",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) require efficient knowledge editing (KE) to\nupdate factual information, yet existing methods exhibit significant\nperformance decay in multi-hop factual recall. This failure is particularly\nacute when edits involve intermediate implicit subjects within reasoning\nchains. Through causal analysis, we reveal that this limitation stems from an\noversight of how chained knowledge is dynamically represented and utilized at\nthe neuron level. We discover that during multi hop reasoning, implicit\nsubjects function as query neurons, which sequentially activate corresponding\nvalue neurons across transformer layers to accumulate information toward the\nfinal answer, a dynamic prior KE work has overlooked. Guided by this insight,\nwe propose ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual\nRecall, a framework that leverages neuron-level attribution to identify and\nedit these critical query-value (Q-V) pathways. ACE provides a mechanistically\ngrounded solution for multi-hop KE, empirically outperforming state-of-the-art\nmethods by 9.44% on GPT-J and 37.46% on Qwen3-8B. Our analysis further reveals\nmore fine-grained activation patterns in Qwen3 and demonstrates that the\nsemantic interpretability of value neurons is orchestrated by query-driven\naccumulation. These findings establish a new pathway for advancing KE\ncapabilities based on the principled understanding of internal reasoning\nmechanisms.",
            "upvotes": 1,
            "discussionId": "68ec7ad6cd07fb414898ca42",
            "ai_summary": "ACE, a framework using neuron-level attribution, enhances multi-hop factual recall in LLMs by editing critical query-value pathways, outperforming existing methods.",
            "ai_keywords": [
                "Large Language Models",
                "knowledge editing",
                "multi-hop factual recall",
                "causal analysis",
                "query neurons",
                "value neurons",
                "transformer layers",
                "Attribution-Controlled Knowledge Editing",
                "GPT-J",
                "Qwen3-8B",
                "semantic interpretability"
            ]
        },
        "publishedAt": "2025-10-09T03:46:08.000Z",
        "title": "ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual\n  Recall",
        "summary": "Large Language Models (LLMs) require efficient knowledge editing (KE) to\nupdate factual information, yet existing methods exhibit significant\nperformance decay in multi-hop factual recall. This failure is particularly\nacute when edits involve intermediate implicit subjects within reasoning\nchains. Through causal analysis, we reveal that this limitation stems from an\noversight of how chained knowledge is dynamically represented and utilized at\nthe neuron level. We discover that during multi hop reasoning, implicit\nsubjects function as query neurons, which sequentially activate corresponding\nvalue neurons across transformer layers to accumulate information toward the\nfinal answer, a dynamic prior KE work has overlooked. Guided by this insight,\nwe propose ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual\nRecall, a framework that leverages neuron-level attribution to identify and\nedit these critical query-value (Q-V) pathways. ACE provides a mechanistically\ngrounded solution for multi-hop KE, empirically outperforming state-of-the-art\nmethods by 9.44% on GPT-J and 37.46% on Qwen3-8B. Our analysis further reveals\nmore fine-grained activation patterns in Qwen3 and demonstrates that the\nsemantic interpretability of value neurons is orchestrated by query-driven\naccumulation. These findings establish a new pathway for advancing KE\ncapabilities based on the principled understanding of internal reasoning\nmechanisms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07896.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "658247c592b5a9664de63882",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658247c592b5a9664de63882/Jn03voLQjDlB3YjpSi-PI.jpeg",
            "fullname": "fan",
            "name": "EasonFan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08649",
            "authors": [
                {
                    "_id": "68ecf7a7b0aa732b88f31ff4",
                    "user": {
                        "_id": "604123b77bfa033a4f7ca59e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1646690236574-604123b77bfa033a4f7ca59e.png",
                        "isPro": false,
                        "fullname": "Gustave Cortal",
                        "user": "gustavecortal",
                        "type": "user"
                    },
                    "name": "Gustave Cortal",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T16:08:45.126Z",
                    "hidden": false
                },
                {
                    "_id": "68ecf7a7b0aa732b88f31ff5",
                    "name": "Alain Finkel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T06:48:06.000Z",
            "submittedOnDailyAt": "2025-10-13T11:30:43.832Z",
            "title": "Formalizing Style in Personal Narratives",
            "submittedOnDailyBy": {
                "_id": "604123b77bfa033a4f7ca59e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1646690236574-604123b77bfa033a4f7ca59e.png",
                "isPro": false,
                "fullname": "Gustave Cortal",
                "user": "gustavecortal",
                "type": "user"
            },
            "summary": "Personal narratives are stories authors construct to make meaning of their\nexperiences. Style, the distinctive way authors use language to express\nthemselves, is fundamental to how these narratives convey subjective\nexperiences. Yet there is a lack of a formal framework for systematically\nanalyzing these stylistic choices. We present a novel approach that formalizes\nstyle in personal narratives as patterns in the linguistic choices authors make\nwhen communicating subjective experiences. Our framework integrates three\ndomains: functional linguistics establishes language as a system of meaningful\nchoices, computer science provides methods for automatically extracting and\nanalyzing sequential patterns, and these patterns are linked to psychological\nobservations. Using language models, we automatically extract linguistic\nfeatures such as processes, participants, and circumstances. We apply our\nframework to hundreds of dream narratives, including a case study on a war\nveteran with post-traumatic stress disorder. Analysis of his narratives\nuncovers distinctive patterns, particularly how verbal processes dominate over\nmental ones, illustrating the relationship between linguistic choices and\npsychological states.",
            "upvotes": 1,
            "discussionId": "68ecf7a7b0aa732b88f31ff6",
            "ai_summary": "A novel framework integrates functional linguistics, computer science, and psychology to analyze stylistic choices in personal narratives, revealing patterns linked to psychological states.",
            "ai_keywords": [
                "functional linguistics",
                "computer science",
                "psychological observations",
                "language models",
                "linguistic features",
                "processes",
                "participants",
                "circumstances",
                "verbal processes",
                "mental processes",
                "post-traumatic stress disorder"
            ]
        },
        "publishedAt": "2025-10-09T02:48:06.000Z",
        "title": "Formalizing Style in Personal Narratives",
        "summary": "Personal narratives are stories authors construct to make meaning of their\nexperiences. Style, the distinctive way authors use language to express\nthemselves, is fundamental to how these narratives convey subjective\nexperiences. Yet there is a lack of a formal framework for systematically\nanalyzing these stylistic choices. We present a novel approach that formalizes\nstyle in personal narratives as patterns in the linguistic choices authors make\nwhen communicating subjective experiences. Our framework integrates three\ndomains: functional linguistics establishes language as a system of meaningful\nchoices, computer science provides methods for automatically extracting and\nanalyzing sequential patterns, and these patterns are linked to psychological\nobservations. Using language models, we automatically extract linguistic\nfeatures such as processes, participants, and circumstances. We apply our\nframework to hundreds of dream narratives, including a case study on a war\nveteran with post-traumatic stress disorder. Analysis of his narratives\nuncovers distinctive patterns, particularly how verbal processes dominate over\nmental ones, illustrating the relationship between linguistic choices and\npsychological states.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08649.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "604123b77bfa033a4f7ca59e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1646690236574-604123b77bfa033a4f7ca59e.png",
            "fullname": "Gustave Cortal",
            "name": "gustavecortal",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.07793",
            "authors": [
                {
                    "_id": "68ece822b0aa732b88f31f8a",
                    "name": "Sajib Acharjee Dip",
                    "hidden": false
                },
                {
                    "_id": "68ece822b0aa732b88f31f8b",
                    "name": "Adrika Zafor",
                    "hidden": false
                },
                {
                    "_id": "68ece822b0aa732b88f31f8c",
                    "name": "Bikash Kumar Paul",
                    "hidden": false
                },
                {
                    "_id": "68ece822b0aa732b88f31f8d",
                    "name": "Uddip Acharjee Shuvo",
                    "hidden": false
                },
                {
                    "_id": "68ece822b0aa732b88f31f8e",
                    "name": "Muhit Islam Emon",
                    "hidden": false
                },
                {
                    "_id": "68ece822b0aa732b88f31f8f",
                    "name": "Xuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68ece822b0aa732b88f31f90",
                    "name": "Liqing Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T05:12:09.000Z",
            "submittedOnDailyAt": "2025-10-13T12:22:48.958Z",
            "title": "LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell\n  Biology",
            "submittedOnDailyBy": {
                "_id": "624af031ff04dbb2756d48dd",
                "avatarUrl": "/avatars/f521c164a429b43db9752618a83920da.svg",
                "isPro": false,
                "fullname": "Sajib Acharjee Dip",
                "user": "Sajib-006",
                "type": "user"
            },
            "summary": "Large language models (LLMs) and emerging agentic frameworks are beginning to\ntransform single-cell biology by enabling natural-language reasoning,\ngenerative annotation, and multimodal data integration. However, progress\nremains fragmented across data modalities, architectures, and evaluation\nstandards. LLM4Cell presents the first unified survey of 58 foundation and\nagentic models developed for single-cell research, spanning RNA, ATAC,\nmulti-omic, and spatial modalities. We categorize these methods into five\nfamilies-foundation, text-bridge, spatial, multimodal, epigenomic, and\nagentic-and map them to eight key analytical tasks including annotation,\ntrajectory and perturbation modeling, and drug-response prediction. Drawing on\nover 40 public datasets, we analyze benchmark suitability, data diversity, and\nethical or scalability constraints, and evaluate models across 10 domain\ndimensions covering biological grounding, multi-omics alignment, fairness,\nprivacy, and explainability. By linking datasets, models, and evaluation\ndomains, LLM4Cell provides the first integrated view of language-driven\nsingle-cell intelligence and outlines open challenges in interpretability,\nstandardization, and trustworthy model development.",
            "upvotes": 1,
            "discussionId": "68ece822b0aa732b88f31f91",
            "ai_summary": "LLM4Cell surveys 58 foundation and agentic models for single-cell biology, categorizing them into families and evaluating them across various analytical tasks and domain dimensions.",
            "ai_keywords": [
                "large language models",
                "LLM4Cell",
                "single-cell biology",
                "natural-language reasoning",
                "generative annotation",
                "multimodal data integration",
                "RNA",
                "ATAC",
                "multi-omic",
                "spatial modalities",
                "text-bridge",
                "trajectory modeling",
                "perturbation modeling",
                "drug-response prediction",
                "benchmark suitability",
                "data diversity",
                "ethical constraints",
                "scalability constraints",
                "biological grounding",
                "multi-omics alignment",
                "fairness",
                "privacy",
                "explainability",
                "interpretability",
                "standardization",
                "trustworthy model development"
            ],
            "organization": {
                "_id": "641f3b58a390e539522a6f88",
                "name": "VirginiaTech",
                "fullname": "Virginia Polytechnic Institute and State University"
            }
        },
        "publishedAt": "2025-10-09T01:12:09.000Z",
        "title": "LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell\n  Biology",
        "summary": "Large language models (LLMs) and emerging agentic frameworks are beginning to\ntransform single-cell biology by enabling natural-language reasoning,\ngenerative annotation, and multimodal data integration. However, progress\nremains fragmented across data modalities, architectures, and evaluation\nstandards. LLM4Cell presents the first unified survey of 58 foundation and\nagentic models developed for single-cell research, spanning RNA, ATAC,\nmulti-omic, and spatial modalities. We categorize these methods into five\nfamilies-foundation, text-bridge, spatial, multimodal, epigenomic, and\nagentic-and map them to eight key analytical tasks including annotation,\ntrajectory and perturbation modeling, and drug-response prediction. Drawing on\nover 40 public datasets, we analyze benchmark suitability, data diversity, and\nethical or scalability constraints, and evaluate models across 10 domain\ndimensions covering biological grounding, multi-omics alignment, fairness,\nprivacy, and explainability. By linking datasets, models, and evaluation\ndomains, LLM4Cell provides the first integrated view of language-driven\nsingle-cell intelligence and outlines open challenges in interpretability,\nstandardization, and trustworthy model development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07793.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "624af031ff04dbb2756d48dd",
            "avatarUrl": "/avatars/f521c164a429b43db9752618a83920da.svg",
            "fullname": "Sajib Acharjee Dip",
            "name": "Sajib-006",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "641f3b58a390e539522a6f88",
            "name": "VirginiaTech",
            "fullname": "Virginia Polytechnic Institute and State University"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07656",
            "authors": [
                {
                    "_id": "68ed1788de1fee572713a55e",
                    "user": {
                        "_id": "637317efbd81fae2b3aa400c",
                        "avatarUrl": "/avatars/898fa184c2d2fb7479ed097b4ddee5e9.svg",
                        "isPro": true,
                        "fullname": "James Baker",
                        "user": "jlbaker361",
                        "type": "user"
                    },
                    "name": "James Baker",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T16:08:30.851Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T01:20:06.000Z",
            "submittedOnDailyAt": "2025-10-13T13:46:47.621Z",
            "title": "MONKEY: Masking ON KEY-Value Activation Adapter for Personalization",
            "submittedOnDailyBy": {
                "_id": "637317efbd81fae2b3aa400c",
                "avatarUrl": "/avatars/898fa184c2d2fb7479ed097b4ddee5e9.svg",
                "isPro": true,
                "fullname": "James Baker",
                "user": "jlbaker361",
                "type": "user"
            },
            "summary": "Personalizing diffusion models allows users to generate new images that\nincorporate a given subject, allowing more control than a text prompt. These\nmodels often suffer somewhat when they end up just recreating the subject\nimage, and ignoring the text prompt. We observe that one popular method for\npersonalization, the IP-Adapter automatically generates masks that we\ndefinitively segment the subject from the background during inference. We\npropose to use this automatically generated mask on a second pass to mask the\nimage tokens, thus restricting them to the subject, not the background,\nallowing the text prompt to attend to the rest of the image. For text prompts\ndescribing locations and places, this produces images that accurately depict\nthe subject while definitively matching the prompt. We compare our method to a\nfew other test time personalization methods, and find our method displays high\nprompt and source image alignment.",
            "upvotes": 1,
            "discussionId": "68ed1788de1fee572713a55f",
            "ai_summary": "Using an automatically generated mask to restrict image tokens during inference improves prompt and source image alignment in personalized diffusion models.",
            "ai_keywords": [
                "diffusion models",
                "IP-Adapter",
                "masks",
                "image tokens",
                "prompt and source image alignment"
            ]
        },
        "publishedAt": "2025-10-08T21:20:06.000Z",
        "title": "MONKEY: Masking ON KEY-Value Activation Adapter for Personalization",
        "summary": "Personalizing diffusion models allows users to generate new images that\nincorporate a given subject, allowing more control than a text prompt. These\nmodels often suffer somewhat when they end up just recreating the subject\nimage, and ignoring the text prompt. We observe that one popular method for\npersonalization, the IP-Adapter automatically generates masks that we\ndefinitively segment the subject from the background during inference. We\npropose to use this automatically generated mask on a second pass to mask the\nimage tokens, thus restricting them to the subject, not the background,\nallowing the text prompt to attend to the rest of the image. For text prompts\ndescribing locations and places, this produces images that accurately depict\nthe subject while definitively matching the prompt. We compare our method to a\nfew other test time personalization methods, and find our method displays high\nprompt and source image alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07656.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637317efbd81fae2b3aa400c",
            "avatarUrl": "/avatars/898fa184c2d2fb7479ed097b4ddee5e9.svg",
            "fullname": "James Baker",
            "name": "jlbaker361",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.07319",
            "authors": [
                {
                    "_id": "68e717097ae125f9582e694a",
                    "name": "Ci-Siang Lin",
                    "hidden": false
                },
                {
                    "_id": "68e717097ae125f9582e694b",
                    "user": {
                        "_id": "64ae22dd1aee69ece065cdcd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                        "isPro": false,
                        "fullname": "Min-Hung Chen",
                        "user": "cmhungsteve",
                        "type": "user"
                    },
                    "name": "Min-Hung Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:50:25.264Z",
                    "hidden": false
                },
                {
                    "_id": "68e717097ae125f9582e694c",
                    "name": "I-Jieh Liu",
                    "hidden": false
                },
                {
                    "_id": "68e717097ae125f9582e694d",
                    "name": "Chien-Yi Wang",
                    "hidden": false
                },
                {
                    "_id": "68e717097ae125f9582e694e",
                    "name": "Sifei Liu",
                    "hidden": false
                },
                {
                    "_id": "68e717097ae125f9582e694f",
                    "name": "Yu-Chiang Frank Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T17:59:57.000Z",
            "submittedOnDailyAt": "2025-10-13T00:41:09.823Z",
            "title": "Temporal Prompting Matters: Rethinking Referring Video Object\n  Segmentation",
            "submittedOnDailyBy": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
            },
            "summary": "Referring Video Object Segmentation (RVOS) aims to segment the object\nreferred to by the query sentence in the video. Most existing methods require\nend-to-end training with dense mask annotations, which could be\ncomputation-consuming and less scalable. In this work, we rethink the RVOS\nproblem and aim to investigate the key to this task. Based on existing\nfoundation segmentation models, we decompose the RVOS task into referring,\nvideo, and segmentation factors, and propose a Temporal Prompt Generation and\nSelection (Tenet) framework to address the referring and video factors while\nleaving the segmentation problem to foundation models. To efficiently adapt\nimage-based foundation segmentation models to referring video object\nsegmentation, we leverage off-the-shelf object detectors and trackers to\nproduce temporal prompts associated with the referring sentence. While\nhigh-quality temporal prompts could be produced, they can not be easily\nidentified from confidence scores. To tackle this issue, we propose Prompt\nPreference Learning to evaluate the quality of the produced temporal prompts.\nBy taking such prompts to instruct image-based foundation segmentation models,\nwe would be able to produce high-quality masks for the referred object,\nenabling efficient model adaptation to referring video object segmentation.\nExperiments on RVOS benchmarks demonstrate the effectiveness of the Tenet\nframework.",
            "upvotes": 1,
            "discussionId": "68e717097ae125f9582e6950",
            "ai_summary": "The Tenet framework decomposes the RVOS task into referring, video, and segmentation factors, using temporal prompts and prompt preference learning to adapt image-based foundation segmentation models for efficient RVOS.",
            "ai_keywords": [
                "Referring Video Object Segmentation (RVOS)",
                "Temporal Prompt Generation and Selection (Tenet)",
                "foundation segmentation models",
                "temporal prompts",
                "Prompt Preference Learning",
                "object detectors",
                "trackers"
            ]
        },
        "publishedAt": "2025-10-08T13:59:57.000Z",
        "title": "Temporal Prompting Matters: Rethinking Referring Video Object\n  Segmentation",
        "summary": "Referring Video Object Segmentation (RVOS) aims to segment the object\nreferred to by the query sentence in the video. Most existing methods require\nend-to-end training with dense mask annotations, which could be\ncomputation-consuming and less scalable. In this work, we rethink the RVOS\nproblem and aim to investigate the key to this task. Based on existing\nfoundation segmentation models, we decompose the RVOS task into referring,\nvideo, and segmentation factors, and propose a Temporal Prompt Generation and\nSelection (Tenet) framework to address the referring and video factors while\nleaving the segmentation problem to foundation models. To efficiently adapt\nimage-based foundation segmentation models to referring video object\nsegmentation, we leverage off-the-shelf object detectors and trackers to\nproduce temporal prompts associated with the referring sentence. While\nhigh-quality temporal prompts could be produced, they can not be easily\nidentified from confidence scores. To tackle this issue, we propose Prompt\nPreference Learning to evaluate the quality of the produced temporal prompts.\nBy taking such prompts to instruct image-based foundation segmentation models,\nwe would be able to produce high-quality masks for the referred object,\nenabling efficient model adaptation to referring video object segmentation.\nExperiments on RVOS benchmarks demonstrate the effectiveness of the Tenet\nframework.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07319.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ae22dd1aee69ece065cdcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
            "fullname": "Min-Hung Chen",
            "name": "cmhungsteve",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.07151",
            "authors": [
                {
                    "_id": "68ecf976b0aa732b88f31ffc",
                    "user": {
                        "_id": "6668687caee0993c95b0eb81",
                        "avatarUrl": "/avatars/301fe1f395e0a129b1c9785868fa9858.svg",
                        "isPro": false,
                        "fullname": "Egor Cherepanov",
                        "user": "avanturist",
                        "type": "user"
                    },
                    "name": "Egor Cherepanov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T16:08:42.968Z",
                    "hidden": false
                },
                {
                    "_id": "68ecf976b0aa732b88f31ffd",
                    "name": "Alexey K. Kovalev",
                    "hidden": false
                },
                {
                    "_id": "68ecf976b0aa732b88f31ffe",
                    "name": "Aleksandr I. Panov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T15:50:34.000Z",
            "submittedOnDailyAt": "2025-10-13T11:37:37.102Z",
            "title": "ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL",
            "submittedOnDailyBy": {
                "_id": "6668687caee0993c95b0eb81",
                "avatarUrl": "/avatars/301fe1f395e0a129b1c9785868fa9858.svg",
                "isPro": false,
                "fullname": "Egor Cherepanov",
                "user": "avanturist",
                "type": "user"
            },
            "summary": "Real-world robotic agents must act under partial observability and long\nhorizons, where key cues may appear long before they affect decision making.\nHowever, most modern approaches rely solely on instantaneous information,\nwithout incorporating insights from the past. Standard recurrent or transformer\nmodels struggle with retaining and leveraging long-term dependencies: context\nwindows truncate history, while naive memory extensions fail under scale and\nsparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a\ntransformer architecture with structured external memory. Each layer maintains\nmemory embeddings, interacts with them via bidirectional cross-attention, and\nupdates them through an Least Recently Used (LRU) memory module using\nreplacement or convex blending. ELMUR extends effective horizons up to 100,000\ntimes beyond the attention window and achieves a 100% success rate on a\nsynthetic T-Maze task with corridors up to one million steps. In POPGym, it\noutperforms baselines on more than half of the tasks. On MIKASA-Robo\nsparse-reward manipulation tasks with visual observations, it nearly doubles\nthe performance of strong baselines. These results demonstrate that structured,\nlayer-local external memory offers a simple and scalable approach to decision\nmaking under partial observability.",
            "upvotes": 1,
            "discussionId": "68ecf976b0aa732b88f31fff",
            "projectPage": "https://elmur-paper.github.io/",
            "githubRepo": "https://github.com/CognitiveAISystems/RATE",
            "ai_summary": "ELMUR, a transformer with structured external memory, enhances decision-making under partial observability by extending effective horizons and improving performance on various tasks.",
            "ai_keywords": [
                "recurrent models",
                "transformer models",
                "long-term dependencies",
                "context windows",
                "memory extensions",
                "ELMUR",
                "External Layer Memory",
                "bidirectional cross-attention",
                "Least Recently Used",
                "LRU memory module",
                "replacement",
                "convex blending",
                "T-Maze task",
                "POPGym",
                "MIKASA-Robo",
                "sparse-reward manipulation tasks",
                "visual observations"
            ],
            "githubStars": 14
        },
        "publishedAt": "2025-10-08T11:50:34.000Z",
        "title": "ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL",
        "summary": "Real-world robotic agents must act under partial observability and long\nhorizons, where key cues may appear long before they affect decision making.\nHowever, most modern approaches rely solely on instantaneous information,\nwithout incorporating insights from the past. Standard recurrent or transformer\nmodels struggle with retaining and leveraging long-term dependencies: context\nwindows truncate history, while naive memory extensions fail under scale and\nsparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a\ntransformer architecture with structured external memory. Each layer maintains\nmemory embeddings, interacts with them via bidirectional cross-attention, and\nupdates them through an Least Recently Used (LRU) memory module using\nreplacement or convex blending. ELMUR extends effective horizons up to 100,000\ntimes beyond the attention window and achieves a 100% success rate on a\nsynthetic T-Maze task with corridors up to one million steps. In POPGym, it\noutperforms baselines on more than half of the tasks. On MIKASA-Robo\nsparse-reward manipulation tasks with visual observations, it nearly doubles\nthe performance of strong baselines. These results demonstrate that structured,\nlayer-local external memory offers a simple and scalable approach to decision\nmaking under partial observability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07151.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6668687caee0993c95b0eb81",
            "avatarUrl": "/avatars/301fe1f395e0a129b1c9785868fa9858.svg",
            "fullname": "Egor Cherepanov",
            "name": "avanturist",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    }
]