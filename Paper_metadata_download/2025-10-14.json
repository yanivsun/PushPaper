[
    {
        "paper": {
            "id": "2510.11696",
            "authors": [
                {
                    "_id": "68edb7abde1fee572713a7df",
                    "name": "Wei Huang",
                    "hidden": false
                },
                {
                    "_id": "68edb7abde1fee572713a7e0",
                    "user": {
                        "_id": "6698c7f6743d55b416bdc008",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6698c7f6743d55b416bdc008/pU6qbo6V4PYAqtMFOhrRm.jpeg",
                        "isPro": false,
                        "fullname": "Ge Yi",
                        "user": "GY2233",
                        "type": "user"
                    },
                    "name": "Yi Ge",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T10:23:44.636Z",
                    "hidden": false
                },
                {
                    "_id": "68edb7abde1fee572713a7e1",
                    "name": "Shuai Yang",
                    "hidden": false
                },
                {
                    "_id": "68edb7abde1fee572713a7e2",
                    "name": "Yicheng Xiao",
                    "hidden": false
                },
                {
                    "_id": "68edb7abde1fee572713a7e3",
                    "name": "Huizi Mao",
                    "hidden": false
                },
                {
                    "_id": "68edb7abde1fee572713a7e4",
                    "name": "Yujun Lin",
                    "hidden": false
                },
                {
                    "_id": "68edb7abde1fee572713a7e5",
                    "name": "Hanrong Ye",
                    "hidden": false
                },
                {
                    "_id": "68edb7abde1fee572713a7e6",
                    "name": "Sifei Liu",
                    "hidden": false
                },
                {
                    "_id": "68edb7abde1fee572713a7e7",
                    "name": "Ka Chun Cheung",
                    "hidden": false
                },
                {
                    "_id": "68edb7abde1fee572713a7e8",
                    "name": "Hongxu Yin",
                    "hidden": false
                },
                {
                    "_id": "68edb7abde1fee572713a7e9",
                    "name": "Yao Lu",
                    "hidden": false
                },
                {
                    "_id": "68edb7abde1fee572713a7ea",
                    "name": "Xiaojuan Qi",
                    "hidden": false
                },
                {
                    "_id": "68edb7abde1fee572713a7eb",
                    "name": "Song Han",
                    "hidden": false
                },
                {
                    "_id": "68edb7abde1fee572713a7ec",
                    "name": "Yukang Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/M3Wu6kYubMte0JBARLogc.qt"
            ],
            "publishedAt": "2025-10-13T17:55:09.000Z",
            "submittedOnDailyAt": "2025-10-14T01:17:25.740Z",
            "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs",
            "submittedOnDailyBy": {
                "_id": "656db3f53dc1d277e5a64410",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png",
                "isPro": false,
                "fullname": "Wei Huang",
                "user": "AaronHuangWei",
                "type": "user"
            },
            "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs.",
            "upvotes": 103,
            "discussionId": "68edb7abde1fee572713a7ed",
            "projectPage": "https://github.com/NVlabs/QeRL",
            "githubRepo": "https://github.com/NVlabs/QeRL",
            "ai_summary": "QeRL, a quantization-enhanced reinforcement learning framework, accelerates RL training for large language models by combining NVFP4 quantization with Low-Rank Adaptation and an Adaptive Quantization Noise mechanism, achieving significant speedups and improved performance.",
            "ai_keywords": [
                "NVFP4 quantization",
                "Low-Rank Adaptation (LoRA)",
                "Adaptive Quantization Noise (AQN)",
                "reinforcement learning",
                "large language models (LLMs)",
                "rollout phase",
                "policy entropy",
                "exploration",
                "reward growth",
                "GSM8K",
                "MATH 500"
            ],
            "githubStars": 184,
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-10-13T13:55:09.000Z",
        "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs",
        "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/M3Wu6kYubMte0JBARLogc.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11696.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656db3f53dc1d277e5a64410",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png",
            "fullname": "Wei Huang",
            "name": "AaronHuangWei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11690",
            "authors": [
                {
                    "_id": "68edbbccde1fee572713a848",
                    "name": "Boyang Zheng",
                    "hidden": false
                },
                {
                    "_id": "68edbbccde1fee572713a849",
                    "name": "Nanye Ma",
                    "hidden": false
                },
                {
                    "_id": "68edbbccde1fee572713a84a",
                    "name": "Shengbang Tong",
                    "hidden": false
                },
                {
                    "_id": "68edbbccde1fee572713a84b",
                    "name": "Saining Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T17:51:39.000Z",
            "submittedOnDailyAt": "2025-10-14T01:43:32.670Z",
            "title": "Diffusion Transformers with Representation Autoencoders",
            "submittedOnDailyBy": {
                "_id": "6374cbb7255276f3a22b4b35",
                "avatarUrl": "/avatars/7cf1bbb83447441e5fa2e1e4fcf7617b.svg",
                "isPro": true,
                "fullname": "Peter Tong",
                "user": "tsbpp",
                "type": "user"
            },
            "summary": "Latent generative modeling, where a pretrained autoencoder maps pixels into a\nlatent space for the diffusion process, has become the standard strategy for\nDiffusion Transformers (DiT); however, the autoencoder component has barely\nevolved. Most DiTs continue to rely on the original VAE encoder, which\nintroduces several limitations: outdated backbones that compromise\narchitectural simplicity, low-dimensional latent spaces that restrict\ninformation capacity, and weak representations that result from purely\nreconstruction-based training and ultimately limit generative quality. In this\nwork, we explore replacing the VAE with pretrained representation encoders\n(e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term\nRepresentation Autoencoders (RAEs). These models provide both high-quality\nreconstructions and semantically rich latent spaces, while allowing for a\nscalable transformer-based architecture. Since these latent spaces are\ntypically high-dimensional, a key challenge is enabling diffusion transformers\nto operate effectively within them. We analyze the sources of this difficulty,\npropose theoretically motivated solutions, and validate them empirically. Our\napproach achieves faster convergence without auxiliary representation alignment\nlosses. Using a DiT variant equipped with a lightweight, wide DDT head, we\nachieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no\nguidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers\nclear advantages and should be the new default for diffusion transformer\ntraining.",
            "upvotes": 91,
            "discussionId": "68edbbccde1fee572713a84c",
            "projectPage": "https://rae-dit.github.io/",
            "githubRepo": "https://github.com/bytetriper/RAE",
            "ai_summary": "Replacing VAEs with pretrained representation encoders in Diffusion Transformers enhances generative quality and convergence speed without auxiliary losses.",
            "ai_keywords": [
                "Latent generative modeling",
                "Diffusion Transformers (DiT)",
                "VAE encoder",
                "Representation Autoencoders (RAEs)",
                "pretrained representation encoders",
                "DINO",
                "SigLIP",
                "MAE",
                "high-dimensional latent spaces",
                "transformer-based architecture",
                "diffusion transformers",
                "image generation",
                "ImageNet",
                "FID"
            ],
            "githubStars": 490,
            "organization": {
                "_id": "662741612ada5b77e310d171",
                "name": "nyu-visionx",
                "fullname": "NYU VisionX",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"
            }
        },
        "publishedAt": "2025-10-13T13:51:39.000Z",
        "title": "Diffusion Transformers with Representation Autoencoders",
        "summary": "Latent generative modeling, where a pretrained autoencoder maps pixels into a\nlatent space for the diffusion process, has become the standard strategy for\nDiffusion Transformers (DiT); however, the autoencoder component has barely\nevolved. Most DiTs continue to rely on the original VAE encoder, which\nintroduces several limitations: outdated backbones that compromise\narchitectural simplicity, low-dimensional latent spaces that restrict\ninformation capacity, and weak representations that result from purely\nreconstruction-based training and ultimately limit generative quality. In this\nwork, we explore replacing the VAE with pretrained representation encoders\n(e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term\nRepresentation Autoencoders (RAEs). These models provide both high-quality\nreconstructions and semantically rich latent spaces, while allowing for a\nscalable transformer-based architecture. Since these latent spaces are\ntypically high-dimensional, a key challenge is enabling diffusion transformers\nto operate effectively within them. We analyze the sources of this difficulty,\npropose theoretically motivated solutions, and validate them empirically. Our\napproach achieves faster convergence without auxiliary representation alignment\nlosses. Using a DiT variant equipped with a lightweight, wide DDT head, we\nachieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no\nguidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers\nclear advantages and should be the new default for diffusion transformer\ntraining.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11690.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6374cbb7255276f3a22b4b35",
            "avatarUrl": "/avatars/7cf1bbb83447441e5fa2e1e4fcf7617b.svg",
            "fullname": "Peter Tong",
            "name": "tsbpp",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "organization": {
            "_id": "662741612ada5b77e310d171",
            "name": "nyu-visionx",
            "fullname": "NYU VisionX",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.10689",
            "authors": [
                {
                    "_id": "68edc3cede1fee572713a8c7",
                    "name": "Caorui Li",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8c8",
                    "name": "Yu Chen",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8c9",
                    "name": "Yiyan Ji",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8ca",
                    "name": "Jin Xu",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8cb",
                    "name": "Zhenyu Cui",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8cc",
                    "name": "Shihao Li",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8cd",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8ce",
                    "name": "Jiafu Tang",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8cf",
                    "name": "Zhenghao Song",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8d0",
                    "name": "Dingling Zhang",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8d1",
                    "name": "Ying He",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8d2",
                    "name": "Haoxiang Liu",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8d3",
                    "name": "Yuxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8d4",
                    "name": "Qiufeng Wang",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8d5",
                    "name": "Zhenhe Wu",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8d6",
                    "name": "Jiehui Luo",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8d7",
                    "name": "Zhiyu Pan",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8d8",
                    "name": "Weihao Xie",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8d9",
                    "user": {
                        "_id": "64b74b906ab5d14ca7f289cd",
                        "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
                        "isPro": false,
                        "fullname": "Chenchen Zhang",
                        "user": "xxzcc",
                        "type": "user"
                    },
                    "name": "Chenchen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:30:15.133Z",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8da",
                    "name": "Zhaohui Wang",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8db",
                    "name": "Jiayi Tian",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8dc",
                    "name": "Yanghai Wang",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8dd",
                    "name": "Zhe Cao",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8de",
                    "name": "Minxin Dai",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8df",
                    "name": "Ke Wang",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8e0",
                    "name": "Runzhe Wen",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8e1",
                    "name": "Yinghao Ma",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8e2",
                    "user": {
                        "_id": "66f0cf3ee3f6b40485241301",
                        "avatarUrl": "/avatars/75967726c0baa2228b13892fb8ef3d8c.svg",
                        "isPro": false,
                        "fullname": "panyaning",
                        "user": "panyaning",
                        "type": "user"
                    },
                    "name": "Yaning Pan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T14:24:17.947Z",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8e3",
                    "name": "Sungkyun Chang",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8e4",
                    "name": "Termeh Taheri",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8e5",
                    "name": "Haiwen Xia",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8e6",
                    "name": "Christos Plachouras",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8e7",
                    "name": "Emmanouil Benetos",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8e8",
                    "name": "Yizhi Li",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8e9",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8ea",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8eb",
                    "name": "Tianhao Peng",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8ec",
                    "name": "Zili Wang",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8ed",
                    "name": "Minghao Liu",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8ee",
                    "name": "Junran Peng",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8ef",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68edc3cede1fee572713a8f0",
                    "name": "Jiaheng Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-12T16:34:00.000Z",
            "submittedOnDailyAt": "2025-10-14T02:01:14.736Z",
            "title": "OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni\n  MLLMs",
            "submittedOnDailyBy": {
                "_id": "65377c30e48353201e6fdda0",
                "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
                "isPro": false,
                "fullname": "Jiaheng Liu",
                "user": "CheeryLJH",
                "type": "user"
            },
            "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nsubstantial potential in video understanding. However, existing benchmarks fail\nto comprehensively evaluate synergistic reasoning capabilities across audio and\nvisual modalities, often neglecting either one of the modalities or integrating\nthem in a logically inconsistent manner. To bridge this gap, we introduce\nOmniVideoBench, a large-scale and rigorously designed benchmark dedicated to\nassessing synergistic audio-visual understanding, with a strong emphasis on\nmodality complementarity and logical consistency. Specifically, OmniVideoBench\ncomprises 1000 high-quality question-answer(QA) pairs, each annotated with\nstep-by-step reasoning traces, derived from 628 diverse videos ranging from\nseveral seconds to 30 minutes, and manually verified to guarantee complete\ncorrectness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully\ndesigned question types, covering temporal reasoning, spatial localization,\ncounting, causal inference, summarization, and beyond, thereby capturing the\nessential challenges of video understanding. Evaluation of multiple MLLMs on\nOmniVideoBench reveals a pronounced gap between model performance and human\nreasoning, with open-source models lagging significantly behind their\nclosed-source counterparts, underscoring the inherent difficulty of genuine\naudio-visual reasoning. We will release OmniVideoBench to foster the\ndevelopment of MLLMs with stronger and more generalizable reasoning\ncapabilities.",
            "upvotes": 39,
            "discussionId": "68edc3cede1fee572713a8f1",
            "projectPage": "https://omnivideobench.github.io/omnivideobench_home/",
            "githubRepo": "https://github.com/NJU-LINK/OmniVideoBench",
            "ai_summary": "OmniVideoBench is a comprehensive benchmark for evaluating audio-visual reasoning in multimodal large language models, addressing modality complementarity and logical consistency.",
            "ai_keywords": [
                "multimodal large language models",
                "video understanding",
                "audio-visual understanding",
                "modality complementarity",
                "logical consistency",
                "question-answer pairs",
                "reasoning traces",
                "temporal reasoning",
                "spatial localization",
                "counting",
                "causal inference",
                "summarization"
            ],
            "githubStars": 20,
            "organization": {
                "_id": "68edc767abe005ac1b354573",
                "name": "NJU-LINK",
                "fullname": "NJU-LINK Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"
            }
        },
        "publishedAt": "2025-10-12T12:34:00.000Z",
        "title": "OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni\n  MLLMs",
        "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nsubstantial potential in video understanding. However, existing benchmarks fail\nto comprehensively evaluate synergistic reasoning capabilities across audio and\nvisual modalities, often neglecting either one of the modalities or integrating\nthem in a logically inconsistent manner. To bridge this gap, we introduce\nOmniVideoBench, a large-scale and rigorously designed benchmark dedicated to\nassessing synergistic audio-visual understanding, with a strong emphasis on\nmodality complementarity and logical consistency. Specifically, OmniVideoBench\ncomprises 1000 high-quality question-answer(QA) pairs, each annotated with\nstep-by-step reasoning traces, derived from 628 diverse videos ranging from\nseveral seconds to 30 minutes, and manually verified to guarantee complete\ncorrectness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully\ndesigned question types, covering temporal reasoning, spatial localization,\ncounting, causal inference, summarization, and beyond, thereby capturing the\nessential challenges of video understanding. Evaluation of multiple MLLMs on\nOmniVideoBench reveals a pronounced gap between model performance and human\nreasoning, with open-source models lagging significantly behind their\nclosed-source counterparts, underscoring the inherent difficulty of genuine\naudio-visual reasoning. We will release OmniVideoBench to foster the\ndevelopment of MLLMs with stronger and more generalizable reasoning\ncapabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10689.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65377c30e48353201e6fdda0",
            "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
            "fullname": "Jiaheng Liu",
            "name": "CheeryLJH",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "organization": {
            "_id": "68edc767abe005ac1b354573",
            "name": "NJU-LINK",
            "fullname": "NJU-LINK Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11052",
            "authors": [
                {
                    "_id": "68ee23ca9b77b5223f666131",
                    "name": "Qinglin Zhu",
                    "hidden": false
                },
                {
                    "_id": "68ee23ca9b77b5223f666132",
                    "name": "Yizhen Yao",
                    "hidden": false
                },
                {
                    "_id": "68ee23ca9b77b5223f666133",
                    "name": "Runcong Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ee23ca9b77b5223f666134",
                    "name": "Yanzheng Xiang",
                    "hidden": false
                },
                {
                    "_id": "68ee23ca9b77b5223f666135",
                    "name": "Amrutha Saseendran",
                    "hidden": false
                },
                {
                    "_id": "68ee23ca9b77b5223f666136",
                    "name": "Chen Jin",
                    "hidden": false
                },
                {
                    "_id": "68ee23ca9b77b5223f666137",
                    "name": "Philip Alexander Teare",
                    "hidden": false
                },
                {
                    "_id": "68ee23ca9b77b5223f666138",
                    "name": "Bin Liang",
                    "hidden": false
                },
                {
                    "_id": "68ee23ca9b77b5223f666139",
                    "name": "Yulan He",
                    "hidden": false
                },
                {
                    "_id": "68ee23ca9b77b5223f66613a",
                    "name": "Lin Gui",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T06:38:13.000Z",
            "submittedOnDailyAt": "2025-10-14T08:54:24.596Z",
            "title": "Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by\n  Refining Belief States",
            "submittedOnDailyBy": {
                "_id": "687facb9b27afe94a71b7455",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hqrApQdsEufqXpXoKu_mn.jpeg",
                "isPro": false,
                "fullname": "Lin Gui",
                "user": "Monta3Pt",
                "type": "user"
            },
            "summary": "Autoregressive (AR) models remain the standard for natural language\ngeneration but still suffer from high latency due to strictly sequential\ndecoding. Recent diffusion-inspired approaches, such as LlaDA and Dream,\nmitigate this by generating in parallel, yet they suffer from two core\nlimitations: information loss, as predictive distributions for non-finalized\ntokens are discarded at each step, and premature commitment, where local\ndecisions are made without sufficient global coordination. We introduce Latent\nRefinement Decoding (LRD), a two-stage framework with Latent Refinement and a\nPredictive Feedback Loop. The first stage maintains masked positions as\ndistributional mixtures of predicted tokens and the mask embedding, allowing\nthe model to establish more globally consistent beliefs. The second stage\nprogressively finalizes confident tokens while retaining uncertain ones for\niterative feedback. KL-divergence dynamics provide a principled and reliable\ncriterion for convergence and early stopping. Experiments across coding\n(HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that\nLRD improves accuracy while delivering speedups of up to 10.6x, making it a\nstrong and versatile alternative for parallel sequence generation.",
            "upvotes": 37,
            "discussionId": "68ee23cb9b77b5223f66613b",
            "ai_summary": "Latent Refinement Decoding (LRD) improves parallel sequence generation by maintaining global consistency and iterative refinement, enhancing accuracy and reducing latency.",
            "ai_keywords": [
                "autoregressive models",
                "diffusion-inspired approaches",
                "LlaDA",
                "Dream",
                "Latent Refinement Decoding",
                "Latent Refinement",
                "Predictive Feedback Loop",
                "KL-divergence dynamics",
                "HumanEval",
                "MBPP",
                "GSM8K",
                "MATH500"
            ],
            "organization": {
                "_id": "625fdeda6d8ef689b7269992",
                "name": "KCL",
                "fullname": "King's College London",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/5He0nBg7T_cCURqZnf4mi.png"
            }
        },
        "publishedAt": "2025-10-13T02:38:13.000Z",
        "title": "Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by\n  Refining Belief States",
        "summary": "Autoregressive (AR) models remain the standard for natural language\ngeneration but still suffer from high latency due to strictly sequential\ndecoding. Recent diffusion-inspired approaches, such as LlaDA and Dream,\nmitigate this by generating in parallel, yet they suffer from two core\nlimitations: information loss, as predictive distributions for non-finalized\ntokens are discarded at each step, and premature commitment, where local\ndecisions are made without sufficient global coordination. We introduce Latent\nRefinement Decoding (LRD), a two-stage framework with Latent Refinement and a\nPredictive Feedback Loop. The first stage maintains masked positions as\ndistributional mixtures of predicted tokens and the mask embedding, allowing\nthe model to establish more globally consistent beliefs. The second stage\nprogressively finalizes confident tokens while retaining uncertain ones for\niterative feedback. KL-divergence dynamics provide a principled and reliable\ncriterion for convergence and early stopping. Experiments across coding\n(HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that\nLRD improves accuracy while delivering speedups of up to 10.6x, making it a\nstrong and versatile alternative for parallel sequence generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11052.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "687facb9b27afe94a71b7455",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hqrApQdsEufqXpXoKu_mn.jpeg",
            "fullname": "Lin Gui",
            "name": "Monta3Pt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "625fdeda6d8ef689b7269992",
            "name": "KCL",
            "fullname": "King's College London",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/5He0nBg7T_cCURqZnf4mi.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.10201",
            "authors": [
                {
                    "_id": "68edab32de1fee572713a6eb",
                    "user": {
                        "_id": "64673258fc6f6da8b119cab8",
                        "avatarUrl": "/avatars/36e025862984c7a86b97cee750ee2d04.svg",
                        "isPro": false,
                        "fullname": "SII-Jhao Zhang",
                        "user": "JingHaoZ",
                        "type": "user"
                    },
                    "name": "Jinghao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:31:05.248Z",
                    "hidden": false
                },
                {
                    "_id": "68edab32de1fee572713a6ec",
                    "name": "Naishan Zheng",
                    "hidden": false
                },
                {
                    "_id": "68edab32de1fee572713a6ed",
                    "name": "Ruilin Li",
                    "hidden": false
                },
                {
                    "_id": "68edab32de1fee572713a6ee",
                    "name": "Dongzhou Cheng",
                    "hidden": false
                },
                {
                    "_id": "68edab32de1fee572713a6ef",
                    "name": "Zheming Liang",
                    "hidden": false
                },
                {
                    "_id": "68edab32de1fee572713a6f0",
                    "name": "Feng Zhao",
                    "hidden": false
                },
                {
                    "_id": "68edab32de1fee572713a6f1",
                    "name": "Jiaqi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-11T13:00:25.000Z",
            "submittedOnDailyAt": "2025-10-14T02:17:56.665Z",
            "title": "RLFR: Extending Reinforcement Learning for LLMs with Flow Environment",
            "submittedOnDailyBy": {
                "_id": "64673258fc6f6da8b119cab8",
                "avatarUrl": "/avatars/36e025862984c7a86b97cee750ee2d04.svg",
                "isPro": false,
                "fullname": "SII-Jhao Zhang",
                "user": "JingHaoZ",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na promising framework for improving reasoning abilities in Large Language\nModels (LLMs). However, policy optimized with binary verification prone to\noverlook potential valuable exploration in reasoning trajectory. In view of\nheavy annotation cost of golden Process Reward Models (PRMs), recent works\nattempt using auxiliary signals for reward shaping of process tokens, involving\nentropy and likelihood collected from logit space. In this work, we offer a\nnovel perspective on shaping RLVR with flow rewards derived from latent space,\nand propose RLFR, where the flow fields of model latents are constructed from\neither off-policy high-quality data and on-policy rejection sampling data, and\nthe velocity deviations of policy latents within it are quantified to serve as\na reward signal. RLFR first demonstrates that a well-established flow field can\nbe a sound environment for reward signal collection, highlighting the\nexpressive latent space is much underexplored. Moreover, RLFR is able to\ncompress any off-policy expert data as reference for constituting reward\nsignals, and we show that the efficient context dependence compressed within\nthe hidden states are utilized, rather than individual token-level denotation\nfor context comprehending. Experiments on both language and multimodal\nreasoning benchmarks demonstrate the reliability of flow rewards, and\nsuggesting a promising paradigm for reward shaping with auxiliary signals.",
            "upvotes": 31,
            "discussionId": "68edab32de1fee572713a6f2",
            "projectPage": "https://jinghaoleven.github.io/RLFR/",
            "githubRepo": "https://github.com/Jinghaoleven/RLFR",
            "ai_summary": "RLFR uses flow rewards derived from latent space to improve reinforcement learning with verifiable rewards, demonstrating reliable reward shaping and efficient context comprehension.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards",
                "RLVR",
                "Large Language Models",
                "LLMs",
                "Process Reward Models",
                "PRMs",
                "reward shaping",
                "process tokens",
                "logit space",
                "flow rewards",
                "latent space",
                "flow fields",
                "off-policy data",
                "on-policy rejection sampling",
                "velocity deviations",
                "hidden states",
                "context comprehension",
                "language benchmarks",
                "multimodal reasoning benchmarks"
            ],
            "githubStars": 33
        },
        "publishedAt": "2025-10-11T09:00:25.000Z",
        "title": "RLFR: Extending Reinforcement Learning for LLMs with Flow Environment",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na promising framework for improving reasoning abilities in Large Language\nModels (LLMs). However, policy optimized with binary verification prone to\noverlook potential valuable exploration in reasoning trajectory. In view of\nheavy annotation cost of golden Process Reward Models (PRMs), recent works\nattempt using auxiliary signals for reward shaping of process tokens, involving\nentropy and likelihood collected from logit space. In this work, we offer a\nnovel perspective on shaping RLVR with flow rewards derived from latent space,\nand propose RLFR, where the flow fields of model latents are constructed from\neither off-policy high-quality data and on-policy rejection sampling data, and\nthe velocity deviations of policy latents within it are quantified to serve as\na reward signal. RLFR first demonstrates that a well-established flow field can\nbe a sound environment for reward signal collection, highlighting the\nexpressive latent space is much underexplored. Moreover, RLFR is able to\ncompress any off-policy expert data as reference for constituting reward\nsignals, and we show that the efficient context dependence compressed within\nthe hidden states are utilized, rather than individual token-level denotation\nfor context comprehending. Experiments on both language and multimodal\nreasoning benchmarks demonstrate the reliability of flow rewards, and\nsuggesting a promising paradigm for reward shaping with auxiliary signals.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10201.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64673258fc6f6da8b119cab8",
            "avatarUrl": "/avatars/36e025862984c7a86b97cee750ee2d04.svg",
            "fullname": "SII-Jhao Zhang",
            "name": "JingHaoZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.09285",
            "authors": [
                {
                    "_id": "68ec8ab6cd07fb414898ca7e",
                    "user": {
                        "_id": "66f79b323fe089b75e9e0c04",
                        "avatarUrl": "/avatars/92918bf8913012a3f005f09e03b381c2.svg",
                        "isPro": false,
                        "fullname": "Siyuan Huang",
                        "user": "chamber111",
                        "type": "user"
                    },
                    "name": "Siyuan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-10-14T19:21:44.979Z",
                    "hidden": false
                },
                {
                    "_id": "68ec8ab6cd07fb414898ca7f",
                    "name": "Xiaoye Qu",
                    "hidden": false
                },
                {
                    "_id": "68ec8ab6cd07fb414898ca80",
                    "name": "Yafu Li",
                    "hidden": false
                },
                {
                    "_id": "68ec8ab6cd07fb414898ca81",
                    "name": "Yun Luo",
                    "hidden": false
                },
                {
                    "_id": "68ec8ab6cd07fb414898ca82",
                    "name": "Zefeng He",
                    "hidden": false
                },
                {
                    "_id": "68ec8ab6cd07fb414898ca83",
                    "name": "Daizong Liu",
                    "hidden": false
                },
                {
                    "_id": "68ec8ab6cd07fb414898ca84",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T11:25:33.000Z",
            "submittedOnDailyAt": "2025-10-14T04:40:24.081Z",
            "title": "Spotlight on Token Perception for Multimodal Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "64cb54da1af278541d663708",
                "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
                "isPro": false,
                "fullname": "Xiaoye Qu",
                "user": "Xiaoye08",
                "type": "user"
            },
            "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the\nreasoning capabilities of Large Vision-Language Models (LVLMs), most existing\nmethods in multimodal reasoning neglect the critical role of visual perception\nwithin the RLVR optimization process. In this paper, we undertake a pioneering\nexploration of multimodal RLVR through the novel perspective of token\nperception, which measures the visual dependency of each generated token. With\na granular analysis of Chain-of-Thought (CoT) processes, we uncover two key\ninsights: first, token perception in a rollout trajectory is sparsely\ndistributed, where only a small fraction of tokens have high visual dependency\nfor visually-grounded reasoning; second, different trajectories exhibit\nsignificant divergence in their overall visual dependency. Based on these\nobservations, we propose Visually-Perceptive Policy Optimization (VPPO), a\nnovel policy gradient algorithm that explicitly leverages token perception to\nrefine the learning signal. Specifically, VPPO achieves this through a dual\nmechanism: it reweights a trajectory's advantage by its overall visual\ndependency, and focuses policy updates exclusively on perceptually pivotal\ntokens. On a comprehensive suite of eight perception and reasoning benchmarks,\nVPPO demonstrates substantial gains over leading open-source RL-tuned models,\nwith its effectiveness consistently validated across 7B and 32B model scales.\nOur findings not only establish a new token-level perceptual perspective for\nanalyzing multimodal RLVR but also present a novel and effective optimization\nstrategy to significantly enhance the multimodal reasoning capabilities of\nLVLMs.",
            "upvotes": 31,
            "discussionId": "68ec8ab6cd07fb414898ca85",
            "projectPage": "https://github.com/huaixuheqing/VPPO-RL",
            "githubRepo": "https://github.com/huaixuheqing/VPPO-RL",
            "ai_summary": "VPPO, a novel policy gradient algorithm, enhances multimodal RLVR by leveraging token perception to refine learning signals and improve reasoning capabilities in Large Vision-Language Models.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards",
                "Large Vision-Language Models",
                "multimodal reasoning",
                "token perception",
                "Chain-of-Thought",
                "rollout trajectory",
                "visual dependency",
                "Visually-Perceptive Policy Optimization",
                "policy gradient algorithm",
                "perceptually pivotal tokens"
            ],
            "githubStars": 24
        },
        "publishedAt": "2025-10-10T07:25:33.000Z",
        "title": "Spotlight on Token Perception for Multimodal Reinforcement Learning",
        "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the\nreasoning capabilities of Large Vision-Language Models (LVLMs), most existing\nmethods in multimodal reasoning neglect the critical role of visual perception\nwithin the RLVR optimization process. In this paper, we undertake a pioneering\nexploration of multimodal RLVR through the novel perspective of token\nperception, which measures the visual dependency of each generated token. With\na granular analysis of Chain-of-Thought (CoT) processes, we uncover two key\ninsights: first, token perception in a rollout trajectory is sparsely\ndistributed, where only a small fraction of tokens have high visual dependency\nfor visually-grounded reasoning; second, different trajectories exhibit\nsignificant divergence in their overall visual dependency. Based on these\nobservations, we propose Visually-Perceptive Policy Optimization (VPPO), a\nnovel policy gradient algorithm that explicitly leverages token perception to\nrefine the learning signal. Specifically, VPPO achieves this through a dual\nmechanism: it reweights a trajectory's advantage by its overall visual\ndependency, and focuses policy updates exclusively on perceptually pivotal\ntokens. On a comprehensive suite of eight perception and reasoning benchmarks,\nVPPO demonstrates substantial gains over leading open-source RL-tuned models,\nwith its effectiveness consistently validated across 7B and 32B model scales.\nOur findings not only establish a new token-level perceptual perspective for\nanalyzing multimodal RLVR but also present a novel and effective optimization\nstrategy to significantly enhance the multimodal reasoning capabilities of\nLVLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09285.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64cb54da1af278541d663708",
            "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
            "fullname": "Xiaoye Qu",
            "name": "Xiaoye08",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11712",
            "authors": [
                {
                    "_id": "68edbf46de1fee572713a874",
                    "user": {
                        "_id": "65240d0ca801972b6eb12ed8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65240d0ca801972b6eb12ed8/hl2RAssBperb5JlgOIDvw.jpeg",
                        "isPro": false,
                        "fullname": "Haoran Feng",
                        "user": "fenghora",
                        "type": "user"
                    },
                    "name": "Haoran Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:30:40.094Z",
                    "hidden": false
                },
                {
                    "_id": "68edbf46de1fee572713a875",
                    "name": "Dizhe Zhang",
                    "hidden": false
                },
                {
                    "_id": "68edbf46de1fee572713a876",
                    "name": "Xiangtai Li",
                    "hidden": false
                },
                {
                    "_id": "68edbf46de1fee572713a877",
                    "name": "Bo Du",
                    "hidden": false
                },
                {
                    "_id": "68edbf46de1fee572713a878",
                    "name": "Lu Qi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65240d0ca801972b6eb12ed8/zv8-Q8Pb9lHDorRizvX94.mp4"
            ],
            "publishedAt": "2025-10-13T17:59:15.000Z",
            "submittedOnDailyAt": "2025-10-14T01:58:07.411Z",
            "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training",
            "submittedOnDailyBy": {
                "_id": "65240d0ca801972b6eb12ed8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65240d0ca801972b6eb12ed8/hl2RAssBperb5JlgOIDvw.jpeg",
                "isPro": false,
                "fullname": "Haoran Feng",
                "user": "fenghora",
                "type": "user"
            },
            "summary": "In this work, we propose DiT360, a DiT-based framework that performs hybrid\ntraining on perspective and panoramic data for panoramic image generation. For\nthe issues of maintaining geometric fidelity and photorealism in generation\nquality, we attribute the main reason to the lack of large-scale, high-quality,\nreal-world panoramic data, where such a data-centric view differs from prior\nmethods that focus on model design. Basically, DiT360 has several key modules\nfor inter-domain transformation and intra-domain augmentation, applied at both\nthe pre-VAE image level and the post-VAE token level. At the image level, we\nincorporate cross-domain knowledge through perspective image guidance and\npanoramic refinement, which enhance perceptual quality while regularizing\ndiversity and photorealism. At the token level, hybrid supervision is applied\nacross multiple modules, which include circular padding for boundary\ncontinuity, yaw loss for rotational robustness, and cube loss for distortion\nawareness. Extensive experiments on text-to-panorama, inpainting, and\noutpainting tasks demonstrate that our method achieves better boundary\nconsistency and image fidelity across eleven quantitative metrics. Our code is\navailable at https://github.com/Insta360-Research-Team/DiT360.",
            "upvotes": 26,
            "discussionId": "68edbf46de1fee572713a879",
            "projectPage": "https://fenghora.github.io/DiT360-Page/",
            "githubRepo": "https://github.com/Insta360-Research-Team/DiT360",
            "ai_summary": "DiT360 framework enhances panoramic image generation by hybrid training on perspective and panoramic data, incorporating cross-domain knowledge and hybrid supervision to improve boundary consistency and image fidelity.",
            "ai_keywords": [
                "DiT",
                "hybrid training",
                "perspective data",
                "panoramic data",
                "geometric fidelity",
                "photorealism",
                "pre-VAE",
                "post-VAE",
                "cross-domain knowledge",
                "perspective image guidance",
                "panoramic refinement",
                "perceptual quality",
                "diversity",
                "circular padding",
                "yaw loss",
                "cube loss",
                "text-to-panorama",
                "inpainting",
                "outpainting"
            ],
            "githubStars": 50
        },
        "publishedAt": "2025-10-13T13:59:15.000Z",
        "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training",
        "summary": "In this work, we propose DiT360, a DiT-based framework that performs hybrid\ntraining on perspective and panoramic data for panoramic image generation. For\nthe issues of maintaining geometric fidelity and photorealism in generation\nquality, we attribute the main reason to the lack of large-scale, high-quality,\nreal-world panoramic data, where such a data-centric view differs from prior\nmethods that focus on model design. Basically, DiT360 has several key modules\nfor inter-domain transformation and intra-domain augmentation, applied at both\nthe pre-VAE image level and the post-VAE token level. At the image level, we\nincorporate cross-domain knowledge through perspective image guidance and\npanoramic refinement, which enhance perceptual quality while regularizing\ndiversity and photorealism. At the token level, hybrid supervision is applied\nacross multiple modules, which include circular padding for boundary\ncontinuity, yaw loss for rotational robustness, and cube loss for distortion\nawareness. Extensive experiments on text-to-panorama, inpainting, and\noutpainting tasks demonstrate that our method achieves better boundary\nconsistency and image fidelity across eleven quantitative metrics. Our code is\navailable at https://github.com/Insta360-Research-Team/DiT360.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65240d0ca801972b6eb12ed8/zv8-Q8Pb9lHDorRizvX94.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11712.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65240d0ca801972b6eb12ed8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65240d0ca801972b6eb12ed8/hl2RAssBperb5JlgOIDvw.jpeg",
            "fullname": "Haoran Feng",
            "name": "fenghora",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.10395",
            "authors": [
                {
                    "_id": "68eda8c8de1fee572713a6b2",
                    "user": {
                        "_id": "66650d38b52f0890724f3b07",
                        "avatarUrl": "/avatars/c25a365bff4985ebb71c96dd097b804f.svg",
                        "isPro": false,
                        "fullname": "Xinlong Chen",
                        "user": "XinlongChen",
                        "type": "user"
                    },
                    "name": "Xinlong Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:31:15.388Z",
                    "hidden": false
                },
                {
                    "_id": "68eda8c8de1fee572713a6b3",
                    "name": "Yue Ding",
                    "hidden": false
                },
                {
                    "_id": "68eda8c8de1fee572713a6b4",
                    "name": "Weihong Lin",
                    "hidden": false
                },
                {
                    "_id": "68eda8c8de1fee572713a6b5",
                    "user": {
                        "_id": "61540338e5b9ae6774201e58",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61540338e5b9ae6774201e58/h_159VrXOlIgu0N0pNgXj.png",
                        "isPro": false,
                        "fullname": "jingyun",
                        "user": "hjy",
                        "type": "user"
                    },
                    "name": "Jingyun Hua",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:31:10.887Z",
                    "hidden": false
                },
                {
                    "_id": "68eda8c8de1fee572713a6b6",
                    "name": "Linli Yao",
                    "hidden": false
                },
                {
                    "_id": "68eda8c8de1fee572713a6b7",
                    "user": {
                        "_id": "673c7319d11b1c2e246ead9c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
                        "isPro": false,
                        "fullname": "Yang Shi",
                        "user": "DogNeverSleep",
                        "type": "user"
                    },
                    "name": "Yang Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:31:13.095Z",
                    "hidden": false
                },
                {
                    "_id": "68eda8c8de1fee572713a6b8",
                    "name": "Bozhou Li",
                    "hidden": false
                },
                {
                    "_id": "68eda8c8de1fee572713a6b9",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "68eda8c8de1fee572713a6ba",
                    "name": "Qiang Liu",
                    "hidden": false
                },
                {
                    "_id": "68eda8c8de1fee572713a6bb",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "68eda8c8de1fee572713a6bc",
                    "name": "Liang Wang",
                    "hidden": false
                },
                {
                    "_id": "68eda8c8de1fee572713a6bd",
                    "name": "Tieniu Tan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-12T01:20:22.000Z",
            "submittedOnDailyAt": "2025-10-14T01:50:38.444Z",
            "title": "AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration",
            "submittedOnDailyBy": {
                "_id": "673c7319d11b1c2e246ead9c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
                "isPro": false,
                "fullname": "Yang Shi",
                "user": "DogNeverSleep",
                "type": "user"
            },
            "summary": "Audiovisual video captioning aims to generate semantically rich descriptions\nwith temporal alignment between visual and auditory events, thereby benefiting\nboth video understanding and generation. In this paper, we present AVoCaDO, a\npowerful audiovisual video captioner driven by the temporal orchestration\nbetween audio and visual modalities. We propose a two-stage post-training\npipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated\ndataset of 107K high-quality, temporally-aligned audiovisual captions; and (2)\nAVoCaDO GRPO, which leverages tailored reward functions to further enhance\ntemporal coherence and dialogue accuracy while regularizing caption length and\nreducing collapse. Experimental results demonstrate that AVoCaDO significantly\noutperforms existing open-source models across four audiovisual video\ncaptioning benchmarks, and also achieves competitive performance on the VDC and\nDREAM-1K benchmark under visual-only settings.",
            "upvotes": 26,
            "discussionId": "68eda8c8de1fee572713a6be",
            "projectPage": "https://avocado-captioner.github.io/",
            "githubRepo": "https://github.com/AVoCaDO-Captioner/AVoCaDO",
            "ai_summary": "AVoCaDO, an audiovisual video captioner, enhances temporal coherence and dialogue accuracy through a two-stage post-training pipeline, outperforming existing models across multiple benchmarks.",
            "ai_keywords": [
                "audiovisual video captioning",
                "temporal orchestration",
                "AVoCaDO",
                "AVoCaDO SFT",
                "AVoCaDO GRPO",
                "reward functions",
                "temporal coherence",
                "dialogue accuracy",
                "caption length",
                "collapse",
                "VDC",
                "DREAM-1K"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-10-11T21:20:22.000Z",
        "title": "AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration",
        "summary": "Audiovisual video captioning aims to generate semantically rich descriptions\nwith temporal alignment between visual and auditory events, thereby benefiting\nboth video understanding and generation. In this paper, we present AVoCaDO, a\npowerful audiovisual video captioner driven by the temporal orchestration\nbetween audio and visual modalities. We propose a two-stage post-training\npipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated\ndataset of 107K high-quality, temporally-aligned audiovisual captions; and (2)\nAVoCaDO GRPO, which leverages tailored reward functions to further enhance\ntemporal coherence and dialogue accuracy while regularizing caption length and\nreducing collapse. Experimental results demonstrate that AVoCaDO significantly\noutperforms existing open-source models across four audiovisual video\ncaptioning benchmarks, and also achieves competitive performance on the VDC and\nDREAM-1K benchmark under visual-only settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10395.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "673c7319d11b1c2e246ead9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
            "fullname": "Yang Shi",
            "name": "DogNeverSleep",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.10666",
            "authors": [
                {
                    "_id": "68edce77de1fee572713a943",
                    "name": "Zhengbo Zhang",
                    "hidden": false
                },
                {
                    "_id": "68edce77de1fee572713a944",
                    "name": "Zhiheng Lyu",
                    "hidden": false
                },
                {
                    "_id": "68edce77de1fee572713a945",
                    "name": "Junhao Gong",
                    "hidden": false
                },
                {
                    "_id": "68edce77de1fee572713a946",
                    "name": "Hongzhu Yi",
                    "hidden": false
                },
                {
                    "_id": "68edce77de1fee572713a947",
                    "name": "Xinming Wang",
                    "hidden": false
                },
                {
                    "_id": "68edce77de1fee572713a948",
                    "name": "Yuxuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68edce77de1fee572713a949",
                    "name": "Jiabing Yang",
                    "hidden": false
                },
                {
                    "_id": "68edce77de1fee572713a94a",
                    "name": "Ping Nie",
                    "hidden": false
                },
                {
                    "_id": "68edce77de1fee572713a94b",
                    "name": "Yan Huang",
                    "hidden": false
                },
                {
                    "_id": "68edce77de1fee572713a94c",
                    "name": "Wenhu Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-12T15:43:37.000Z",
            "submittedOnDailyAt": "2025-10-14T02:48:04.607Z",
            "title": "BrowserAgent: Building Web Agents with Human-Inspired Web Browsing\n  Actions",
            "submittedOnDailyBy": {
                "_id": "6313a86154e6e5d9f0f94e04",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                "isPro": false,
                "fullname": "Wenhu Chen",
                "user": "wenhu",
                "type": "user"
            },
            "summary": "Efficiently solving real-world problems with LLMs increasingly hinges on\ntheir ability to interact with dynamic web environments and autonomously\nacquire external information. While recent research like Search-R1 and\nWebDancer demonstrates strong performance in solving web tasks, they heavily\nrely on additional tools to convert the interactive web environment into static\ntext content. This is in contrast to human browsing behaviors, which involve\ndiverse interactions with the browser, such as scrolling, clicking, and typing.\nIn this paper, we propose BrowserAgent, a more interactive agent that solves\ncomplex tasks through human-inspired browser actions. BrowserAgent operates\ndirectly on raw web pages via Playwright through a set of predefined browser\nactions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and\nRejection Fine-Tuning (RFT)) to improve the model's generalization abilities.\nDespite using significantly less training data than Search-R1, BrowserAgent\nachieves more competitive results across different Open-QA tasks. Additionally,\nwe introduce an explicit memory mechanism to store key conclusions across\nsteps, further enhancing the model's reasoning capabilities for long-horizon\ntasks. Notably, BrowserAgent-7B can achieve around 20\\% improvement over\nSearch-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These\nresults indicate that BrowserAgent can serve as a more advanced framework for\nmore interactive and scalable web agents.",
            "upvotes": 24,
            "discussionId": "68edce77de1fee572713a94d",
            "projectPage": "https://tiger-ai-lab.github.io/BrowserAgent/",
            "githubRepo": "https://github.com/TIGER-AI-Lab/BrowserAgent",
            "ai_summary": "BrowserAgent, an interactive web agent using human-like browser actions and a two-stage training process, achieves competitive results in Open-QA tasks with less training data and improved reasoning for multi-hop QA.",
            "ai_keywords": [
                "LLMs",
                "BrowserAgent",
                "Playwright",
                "Supervised Fine-Tuning",
                "Rejection Fine-Tuning",
                "explicit memory mechanism",
                "multi-hop QA",
                "HotpotQA",
                "2Wiki",
                "Bamboogle"
            ],
            "githubStars": 6,
            "organization": {
                "_id": "6313a90017838d05194fd282",
                "name": "TIGER-Lab",
                "fullname": "TIGER-Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/Noi3Qq3RYz8Jdq6BaFteq.png"
            }
        },
        "publishedAt": "2025-10-12T11:43:37.000Z",
        "title": "BrowserAgent: Building Web Agents with Human-Inspired Web Browsing\n  Actions",
        "summary": "Efficiently solving real-world problems with LLMs increasingly hinges on\ntheir ability to interact with dynamic web environments and autonomously\nacquire external information. While recent research like Search-R1 and\nWebDancer demonstrates strong performance in solving web tasks, they heavily\nrely on additional tools to convert the interactive web environment into static\ntext content. This is in contrast to human browsing behaviors, which involve\ndiverse interactions with the browser, such as scrolling, clicking, and typing.\nIn this paper, we propose BrowserAgent, a more interactive agent that solves\ncomplex tasks through human-inspired browser actions. BrowserAgent operates\ndirectly on raw web pages via Playwright through a set of predefined browser\nactions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and\nRejection Fine-Tuning (RFT)) to improve the model's generalization abilities.\nDespite using significantly less training data than Search-R1, BrowserAgent\nachieves more competitive results across different Open-QA tasks. Additionally,\nwe introduce an explicit memory mechanism to store key conclusions across\nsteps, further enhancing the model's reasoning capabilities for long-horizon\ntasks. Notably, BrowserAgent-7B can achieve around 20\\% improvement over\nSearch-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These\nresults indicate that BrowserAgent can serve as a more advanced framework for\nmore interactive and scalable web agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10666.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "fullname": "Wenhu Chen",
            "name": "wenhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 44
        },
        "organization": {
            "_id": "6313a90017838d05194fd282",
            "name": "TIGER-Lab",
            "fullname": "TIGER-Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/Noi3Qq3RYz8Jdq6BaFteq.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11701",
            "authors": [
                {
                    "_id": "68edb4e8de1fee572713a782",
                    "name": "Zhaochen Yu",
                    "hidden": false
                },
                {
                    "_id": "68edb4e8de1fee572713a783",
                    "name": "Ling Yang",
                    "hidden": false
                },
                {
                    "_id": "68edb4e8de1fee572713a784",
                    "name": "Jiaru Zou",
                    "hidden": false
                },
                {
                    "_id": "68edb4e8de1fee572713a785",
                    "name": "Shuicheng Yan",
                    "hidden": false
                },
                {
                    "_id": "68edb4e8de1fee572713a786",
                    "name": "Mengdi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T17:57:15.000Z",
            "submittedOnDailyAt": "2025-10-14T01:06:40.588Z",
            "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
            "submittedOnDailyBy": {
                "_id": "64fde4e252e82dd432b74ce9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fde4e252e82dd432b74ce9/-CQZbBP7FsPPyawYrsi4z.jpeg",
                "isPro": false,
                "fullname": "Ling Yang",
                "user": "Lingaaaaaaa",
                "type": "user"
            },
            "summary": "Recently, the emergence of agentic RL has showcased that RL could also\neffectively improve the agentic reasoning ability of LLMs, yet the key design\nprinciples and optimal practices remain unclear. In this work, we conduct a\ncomprehensive and systematic investigation to demystify reinforcement learning\nin agentic reasoning from three key perspectives: data, algorithm, and\nreasoning mode. We highlight our key insights: (i) Replacing stitched synthetic\ntrajectories with real end-to-end tool-use trajectories yields a far stronger\nSFT initialization; high-diversity, model-aware datasets sustain exploration\nand markedly improve RL performance. (ii) Exploration-friendly techniques are\ncrucial for agentic RL, such as clip higher, overlong reward shaping, and\nmaintaining adequate policy entropy could improve the training efficiency.\n(iii) A deliberative strategy with fewer tool calls outperforms frequent tool\ncalls or verbose self-reasoning, improving tool efficiency and final accuracy.\nTogether, these simple practices consistently enhance agentic reasoning and\ntraining efficiency, achieving strong results on challenging benchmarks with\nsmaller models, and establishing a practical baseline for future agentic RL\nresearch. Beyond these empirical insights, we further contribute a\nhigh-quality, real end-to-end agentic SFT dataset along with a high-quality RL\ndataset, and demonstrate the effectiveness of our insights in boosting the\nagentic reasoning ability of LLMs across four challenging benchmarks, including\nAIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,\n4B-sized models could also achieve superior agentic reasoning performance\ncompared to 32B-sized models. Code and models:\nhttps://github.com/Gen-Verse/Open-AgentRL",
            "upvotes": 23,
            "discussionId": "68edb4e8de1fee572713a787",
            "projectPage": "https://huggingface.co/collections/Gen-Verse/open-agentrl-68eda4c05755ca5a8c663656",
            "githubRepo": "https://github.com/Gen-Verse/Open-AgentRL",
            "ai_summary": "Agentic reinforcement learning enhances LLMs' reasoning ability through real datasets, exploration techniques, and a deliberative strategy, achieving strong performance with smaller models.",
            "ai_keywords": [
                "agentic RL",
                "SFT initialization",
                "real end-to-end tool-use trajectories",
                "high-diversity datasets",
                "model-aware datasets",
                "exploration-friendly techniques",
                "clip higher",
                "overlong reward shaping",
                "policy entropy",
                "deliberative strategy",
                "agentic SFT dataset",
                "RL dataset",
                "AIME2024/AIME2025",
                "GPQA-Diamond",
                "LiveCodeBench-v6"
            ],
            "githubStars": 12
        },
        "publishedAt": "2025-10-13T13:57:15.000Z",
        "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
        "summary": "Recently, the emergence of agentic RL has showcased that RL could also\neffectively improve the agentic reasoning ability of LLMs, yet the key design\nprinciples and optimal practices remain unclear. In this work, we conduct a\ncomprehensive and systematic investigation to demystify reinforcement learning\nin agentic reasoning from three key perspectives: data, algorithm, and\nreasoning mode. We highlight our key insights: (i) Replacing stitched synthetic\ntrajectories with real end-to-end tool-use trajectories yields a far stronger\nSFT initialization; high-diversity, model-aware datasets sustain exploration\nand markedly improve RL performance. (ii) Exploration-friendly techniques are\ncrucial for agentic RL, such as clip higher, overlong reward shaping, and\nmaintaining adequate policy entropy could improve the training efficiency.\n(iii) A deliberative strategy with fewer tool calls outperforms frequent tool\ncalls or verbose self-reasoning, improving tool efficiency and final accuracy.\nTogether, these simple practices consistently enhance agentic reasoning and\ntraining efficiency, achieving strong results on challenging benchmarks with\nsmaller models, and establishing a practical baseline for future agentic RL\nresearch. Beyond these empirical insights, we further contribute a\nhigh-quality, real end-to-end agentic SFT dataset along with a high-quality RL\ndataset, and demonstrate the effectiveness of our insights in boosting the\nagentic reasoning ability of LLMs across four challenging benchmarks, including\nAIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,\n4B-sized models could also achieve superior agentic reasoning performance\ncompared to 32B-sized models. Code and models:\nhttps://github.com/Gen-Verse/Open-AgentRL",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11701.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64fde4e252e82dd432b74ce9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fde4e252e82dd432b74ce9/-CQZbBP7FsPPyawYrsi4z.jpeg",
            "fullname": "Ling Yang",
            "name": "Lingaaaaaaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.09781",
            "authors": [
                {
                    "_id": "68edcbffde1fee572713a927",
                    "name": "Yue Huang",
                    "hidden": false
                },
                {
                    "_id": "68edcbffde1fee572713a928",
                    "user": {
                        "_id": "639f8277beb95d698de007dd",
                        "avatarUrl": "/avatars/57f223ccd9d3cb03166ccf0e41361c58.svg",
                        "isPro": false,
                        "fullname": "HangHua",
                        "user": "hhua2",
                        "type": "user"
                    },
                    "name": "Hang Hua",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:29:51.627Z",
                    "hidden": false
                },
                {
                    "_id": "68edcbffde1fee572713a929",
                    "name": "Yujun Zhou",
                    "hidden": false
                },
                {
                    "_id": "68edcbffde1fee572713a92a",
                    "name": "Pengcheng Jing",
                    "hidden": false
                },
                {
                    "_id": "68edcbffde1fee572713a92b",
                    "name": "Manish Nagireddy",
                    "hidden": false
                },
                {
                    "_id": "68edcbffde1fee572713a92c",
                    "name": "Inkit Padhi",
                    "hidden": false
                },
                {
                    "_id": "68edcbffde1fee572713a92d",
                    "name": "Greta Dolcetti",
                    "hidden": false
                },
                {
                    "_id": "68edcbffde1fee572713a92e",
                    "name": "Zhangchen Xu",
                    "hidden": false
                },
                {
                    "_id": "68edcbffde1fee572713a92f",
                    "name": "Subhajit Chaudhury",
                    "hidden": false
                },
                {
                    "_id": "68edcbffde1fee572713a930",
                    "name": "Ambrish Rawat",
                    "hidden": false
                },
                {
                    "_id": "68edcbffde1fee572713a931",
                    "name": "Liubov Nedoshivina",
                    "hidden": false
                },
                {
                    "_id": "68edcbffde1fee572713a932",
                    "name": "Pin-Yu Chen",
                    "hidden": false
                },
                {
                    "_id": "68edcbffde1fee572713a933",
                    "name": "Prasanna Sattigeri",
                    "hidden": false
                },
                {
                    "_id": "68edcbffde1fee572713a934",
                    "name": "Xiangliang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T18:42:32.000Z",
            "submittedOnDailyAt": "2025-10-14T02:36:39.228Z",
            "title": "Building a Foundational Guardrail for General Agentic Systems via\n  Synthetic Data",
            "submittedOnDailyBy": {
                "_id": "639d94ab7145123e0d44e48a",
                "avatarUrl": "/avatars/5bb6a65b306d1383c4a8bcd9334b470a.svg",
                "isPro": false,
                "fullname": "Yue Huang",
                "user": "HowieHwong",
                "type": "user"
            },
            "summary": "While LLM agents can plan multi-step tasks, intervening at the planning\nstage-before any action is executed-is often the safest way to prevent harm,\nsince certain risks can lead to severe consequences once carried out. However,\nexisting guardrails mostly operate post-execution, which is difficult to scale\nand leaves little room for controllable supervision at the plan level. To\naddress this challenge, we highlight three critical gaps in current research:\ndata gap, model gap, and evaluation gap. To close the data gap, we introduce\nAuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii)\ninjects category-labeled risks with calibrated difficulty, and (iii) filters\noutputs via an automated reward model, producing large and reliable corpora for\npre-execution safety. To close the guardian model gap, we propose a\nfoundational guardrail Safiron, combining a cross-planner adapter with a\ncompact guardian model. The adapter unifies different input formats, while\nSafiron flags risky cases, assigns risk types, and generates rationales;\ntrained in two stages with a broadly explored data recipe, Safiron achieves\nrobust transfer across settings. To close the evaluation gap, we release\nPre-Exec Bench, a realistic benchmark covering diverse tools and branching\ntrajectories, which measures detection, fine-grained categorization,\nexplanation, and cross-planner generalization in human-verified scenarios.\nExtensive experiments demonstrate consistent gains of the proposed guardrail\nover strong baselines on Pre-Exec Bench, and ablations further distill\nactionable practices, providing a practical template for safer agentic systems.",
            "upvotes": 22,
            "discussionId": "68edcbffde1fee572713a935",
            "githubRepo": "https://github.com/HowieHwong/Agentic-Guardian",
            "ai_summary": "AuraGen and Safiron address pre-execution safety gaps in LLM agents by synthesizing benign trajectories, injecting risks, and using a cross-planner adapter for robust risk detection and explanation.",
            "ai_keywords": [
                "AuraGen",
                "Safiron",
                "cross-planner adapter",
                "compact guardian model",
                "automated reward model",
                "Pre-Exec Bench",
                "risk detection",
                "risk categorization",
                "explanation",
                "cross-planner generalization"
            ],
            "githubStars": 26
        },
        "publishedAt": "2025-10-10T14:42:32.000Z",
        "title": "Building a Foundational Guardrail for General Agentic Systems via\n  Synthetic Data",
        "summary": "While LLM agents can plan multi-step tasks, intervening at the planning\nstage-before any action is executed-is often the safest way to prevent harm,\nsince certain risks can lead to severe consequences once carried out. However,\nexisting guardrails mostly operate post-execution, which is difficult to scale\nand leaves little room for controllable supervision at the plan level. To\naddress this challenge, we highlight three critical gaps in current research:\ndata gap, model gap, and evaluation gap. To close the data gap, we introduce\nAuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii)\ninjects category-labeled risks with calibrated difficulty, and (iii) filters\noutputs via an automated reward model, producing large and reliable corpora for\npre-execution safety. To close the guardian model gap, we propose a\nfoundational guardrail Safiron, combining a cross-planner adapter with a\ncompact guardian model. The adapter unifies different input formats, while\nSafiron flags risky cases, assigns risk types, and generates rationales;\ntrained in two stages with a broadly explored data recipe, Safiron achieves\nrobust transfer across settings. To close the evaluation gap, we release\nPre-Exec Bench, a realistic benchmark covering diverse tools and branching\ntrajectories, which measures detection, fine-grained categorization,\nexplanation, and cross-planner generalization in human-verified scenarios.\nExtensive experiments demonstrate consistent gains of the proposed guardrail\nover strong baselines on Pre-Exec Bench, and ablations further distill\nactionable practices, providing a practical template for safer agentic systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09781.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "639d94ab7145123e0d44e48a",
            "avatarUrl": "/avatars/5bb6a65b306d1383c4a8bcd9334b470a.svg",
            "fullname": "Yue Huang",
            "name": "HowieHwong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.04617",
            "authors": [
                {
                    "_id": "68e5159bf9af2f6567eab906",
                    "user": {
                        "_id": "643525ea0b30bd434ea15363",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643525ea0b30bd434ea15363/7sAzllfWUPtt68NY1gDLj.png",
                        "isPro": false,
                        "fullname": "Jackie Lai",
                        "user": "DreamW1ngs",
                        "type": "user"
                    },
                    "name": "Zhejian Lai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-08T08:01:46.085Z",
                    "hidden": false
                },
                {
                    "_id": "68e5159bf9af2f6567eab907",
                    "name": "Xiang Geng",
                    "hidden": false
                },
                {
                    "_id": "68e5159bf9af2f6567eab908",
                    "name": "Zhijun Wang",
                    "hidden": false
                },
                {
                    "_id": "68e5159bf9af2f6567eab909",
                    "name": "Yang Bai",
                    "hidden": false
                },
                {
                    "_id": "68e5159bf9af2f6567eab90a",
                    "name": "Jiahuan Li",
                    "hidden": false
                },
                {
                    "_id": "68e5159bf9af2f6567eab90b",
                    "name": "Rongxiang Weng",
                    "hidden": false
                },
                {
                    "_id": "68e5159bf9af2f6567eab90c",
                    "name": "Jingang Wang",
                    "hidden": false
                },
                {
                    "_id": "68e5159bf9af2f6567eab90d",
                    "name": "Xuezhi Cao",
                    "hidden": false
                },
                {
                    "_id": "68e5159bf9af2f6567eab90e",
                    "name": "Xunliang Cai",
                    "hidden": false
                },
                {
                    "_id": "68e5159bf9af2f6567eab90f",
                    "name": "Shujian Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-06T09:30:05.000Z",
            "submittedOnDailyAt": "2025-10-14T00:44:28.428Z",
            "title": "Making Mathematical Reasoning Adaptive",
            "submittedOnDailyBy": {
                "_id": "643525ea0b30bd434ea15363",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643525ea0b30bd434ea15363/7sAzllfWUPtt68NY1gDLj.png",
                "isPro": false,
                "fullname": "Jackie Lai",
                "user": "DreamW1ngs",
                "type": "user"
            },
            "summary": "Mathematical reasoning is a primary indicator of large language models (LLMs)\nintelligence. However, existing LLMs exhibit failures of robustness and\ngeneralization. This paper attributes these deficiencies to spurious reasoning,\ni.e., producing answers from superficial features. To address this challenge,\nwe propose the AdaR framework to enable adaptive reasoning, wherein models rely\non problem-solving logic to produce answers. AdaR synthesizes logically\nequivalent queries by varying variable values, and trains models with RLVR on\nthese data to penalize spurious logic while encouraging adaptive logic. To\nimprove data quality, we extract the problem-solving logic from the original\nquery and generate the corresponding answer by code execution, then apply a\nsanity check. Experimental results demonstrate that AdaR improves robustness\nand generalization, achieving substantial improvement in mathematical reasoning\nwhile maintaining high data efficiency. Analysis indicates that data synthesis\nand RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.\nSubsequent analyses derive key design insights into the effect of critical\nfactors and the applicability to instruct LLMs. Our project is available at\nhttps://github.com/LaiZhejian/AdaR",
            "upvotes": 22,
            "discussionId": "68e5163ff9af2f6567eab910",
            "githubRepo": "https://github.com/LaiZhejian/AdaR",
            "ai_summary": "AdaR framework enhances LLMs' robustness and generalization in mathematical reasoning by synthesizing logically equivalent queries and using RLVR to penalize spurious logic.",
            "ai_keywords": [
                "AdaR",
                "adaptive reasoning",
                "spurious reasoning",
                "logically equivalent queries",
                "RLVR",
                "data synthesis",
                "mathematical reasoning",
                "data efficiency"
            ],
            "githubStars": 11
        },
        "publishedAt": "2025-10-06T05:30:05.000Z",
        "title": "Making Mathematical Reasoning Adaptive",
        "summary": "Mathematical reasoning is a primary indicator of large language models (LLMs)\nintelligence. However, existing LLMs exhibit failures of robustness and\ngeneralization. This paper attributes these deficiencies to spurious reasoning,\ni.e., producing answers from superficial features. To address this challenge,\nwe propose the AdaR framework to enable adaptive reasoning, wherein models rely\non problem-solving logic to produce answers. AdaR synthesizes logically\nequivalent queries by varying variable values, and trains models with RLVR on\nthese data to penalize spurious logic while encouraging adaptive logic. To\nimprove data quality, we extract the problem-solving logic from the original\nquery and generate the corresponding answer by code execution, then apply a\nsanity check. Experimental results demonstrate that AdaR improves robustness\nand generalization, achieving substantial improvement in mathematical reasoning\nwhile maintaining high data efficiency. Analysis indicates that data synthesis\nand RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.\nSubsequent analyses derive key design insights into the effect of critical\nfactors and the applicability to instruct LLMs. Our project is available at\nhttps://github.com/LaiZhejian/AdaR",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04617.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "643525ea0b30bd434ea15363",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643525ea0b30bd434ea15363/7sAzllfWUPtt68NY1gDLj.png",
            "fullname": "Jackie Lai",
            "name": "DreamW1ngs",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.11341",
            "authors": [
                {
                    "_id": "68edbe0fde1fee572713a858",
                    "name": "Haomin Wang",
                    "hidden": false
                },
                {
                    "_id": "68edbe0fde1fee572713a859",
                    "name": "Jinhui Yin",
                    "hidden": false
                },
                {
                    "_id": "68edbe0fde1fee572713a85a",
                    "name": "Qi Wei",
                    "hidden": false
                },
                {
                    "_id": "68edbe0fde1fee572713a85b",
                    "name": "Wenguang Zeng",
                    "hidden": false
                },
                {
                    "_id": "68edbe0fde1fee572713a85c",
                    "name": "Lixin Gu",
                    "hidden": false
                },
                {
                    "_id": "68edbe0fde1fee572713a85d",
                    "name": "Shenglong Ye",
                    "hidden": false
                },
                {
                    "_id": "68edbe0fde1fee572713a85e",
                    "name": "Zhangwei Gao",
                    "hidden": false
                },
                {
                    "_id": "68edbe0fde1fee572713a85f",
                    "name": "Yaohui Wang",
                    "hidden": false
                },
                {
                    "_id": "68edbe0fde1fee572713a860",
                    "name": "Yanting Zhang",
                    "hidden": false
                },
                {
                    "_id": "68edbe0fde1fee572713a861",
                    "name": "Yuanqi Li",
                    "hidden": false
                },
                {
                    "_id": "68edbe0fde1fee572713a862",
                    "name": "Yanwen Guo",
                    "hidden": false
                },
                {
                    "_id": "68edbe0fde1fee572713a863",
                    "name": "Wenhai Wang",
                    "hidden": false
                },
                {
                    "_id": "68edbe0fde1fee572713a864",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "68edbe0fde1fee572713a865",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68edbe0fde1fee572713a866",
                    "name": "Hongjie Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T12:38:04.000Z",
            "submittedOnDailyAt": "2025-10-14T07:17:02.367Z",
            "title": "InternSVG: Towards Unified SVG Tasks with Multimodal Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "64d83ee0763279bb4ddd5ba8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d83ee0763279bb4ddd5ba8/exbvc1hhIXXKPVmJpxRtP.jpeg",
                "isPro": false,
                "fullname": "Haomin Wang",
                "user": "KiyotakaWang",
                "type": "user"
            },
            "summary": "General SVG modeling remains challenging due to fragmented datasets, limited\ntransferability of methods across tasks, and the difficulty of handling\nstructural complexity. In response, we leverage the strong transfer and\ngeneralization capabilities of multimodal large language models (MLLMs) to\nachieve unified modeling for SVG understanding, editing, and generation. We\npresent the InternSVG family, an integrated data-benchmark-model suite. At its\ncore is SAgoge, the largest and most comprehensive multimodal dataset for SVG\ntasks, encompassing both static graphics and dynamic animations. It covers\nicons, long-sequence illustrations, scientific diagrams, and dynamic\nanimations, supporting tasks of varied difficulty levels and providing deeper\nhierarchies with richer attributes compared to previous datasets. Based on this\nresource, we introduce SArena, a companion benchmark with comprehensive task\ndefinitions and standardized evaluation that aligns with the domains and\ndifficulty spectrum covered by SAgoge. Building on these foundations, we\npropose InternSVG, a unified MLLM for SVG understanding, editing, and\ngeneration with SVG-specific special tokens, subword-based embedding\ninitialization, and a two-stage training strategy that progresses from short\nstatic SVGs to long-sequence illustrations and complex animations. This unified\nformulation induces positive transfer and improves overall performance.\nExperiments on SArena and prior benchmark confirm that InternSVG achieves\nsubstantial gains and consistently outperforms leading open and proprietary\ncounterparts.",
            "upvotes": 21,
            "discussionId": "68edbe0fde1fee572713a867",
            "projectPage": "https://hmwang2002.github.io/release/internsvg/",
            "githubRepo": "https://github.com/hmwang2002/InternSVG",
            "ai_summary": "A unified multimodal large language model (MLLM) for SVG understanding, editing, and generation leverages a comprehensive dataset and benchmark to achieve superior performance across various tasks.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "SVG understanding",
                "SVG editing",
                "SVG generation",
                "SAgoge",
                "SArena",
                "SVG-specific special tokens",
                "subword-based embedding initialization",
                "two-stage training strategy"
            ],
            "githubStars": 31,
            "organization": {
                "_id": "68edba5df36ca3bb705a99d6",
                "name": "InternSVG",
                "fullname": "InternSVG",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64d83ee0763279bb4ddd5ba8/A_4yFPzQlJ5uNmHZPbAqz.png"
            }
        },
        "publishedAt": "2025-10-13T08:38:04.000Z",
        "title": "InternSVG: Towards Unified SVG Tasks with Multimodal Large Language\n  Models",
        "summary": "General SVG modeling remains challenging due to fragmented datasets, limited\ntransferability of methods across tasks, and the difficulty of handling\nstructural complexity. In response, we leverage the strong transfer and\ngeneralization capabilities of multimodal large language models (MLLMs) to\nachieve unified modeling for SVG understanding, editing, and generation. We\npresent the InternSVG family, an integrated data-benchmark-model suite. At its\ncore is SAgoge, the largest and most comprehensive multimodal dataset for SVG\ntasks, encompassing both static graphics and dynamic animations. It covers\nicons, long-sequence illustrations, scientific diagrams, and dynamic\nanimations, supporting tasks of varied difficulty levels and providing deeper\nhierarchies with richer attributes compared to previous datasets. Based on this\nresource, we introduce SArena, a companion benchmark with comprehensive task\ndefinitions and standardized evaluation that aligns with the domains and\ndifficulty spectrum covered by SAgoge. Building on these foundations, we\npropose InternSVG, a unified MLLM for SVG understanding, editing, and\ngeneration with SVG-specific special tokens, subword-based embedding\ninitialization, and a two-stage training strategy that progresses from short\nstatic SVGs to long-sequence illustrations and complex animations. This unified\nformulation induces positive transfer and improves overall performance.\nExperiments on SArena and prior benchmark confirm that InternSVG achieves\nsubstantial gains and consistently outperforms leading open and proprietary\ncounterparts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11341.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d83ee0763279bb4ddd5ba8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d83ee0763279bb4ddd5ba8/exbvc1hhIXXKPVmJpxRtP.jpeg",
            "fullname": "Haomin Wang",
            "name": "KiyotakaWang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "68edba5df36ca3bb705a99d6",
            "name": "InternSVG",
            "fullname": "InternSVG",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64d83ee0763279bb4ddd5ba8/A_4yFPzQlJ5uNmHZPbAqz.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11652",
            "authors": [
                {
                    "_id": "68edd775de1fee572713a97f",
                    "user": {
                        "_id": "67dbf07f9d821d38905d145d",
                        "avatarUrl": "/avatars/e806467d2c0f4f642c8d4906b0855817.svg",
                        "isPro": false,
                        "fullname": "guixin",
                        "user": "Ross12",
                        "type": "user"
                    },
                    "name": "Xin Gui",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:29:37.509Z",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a980",
                    "user": {
                        "_id": "6578265ddea7e2122d02f6ba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578265ddea7e2122d02f6ba/Bh6JjoVF5ceLSjV7Z7nTk.jpeg",
                        "isPro": false,
                        "fullname": "king zhu",
                        "user": "kangz",
                        "type": "user"
                    },
                    "name": "King Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:29:32.982Z",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a981",
                    "user": {
                        "_id": "6704ee27386892c420db1938",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6704ee27386892c420db1938/lb5mtEwYhn47RawkynYPs.jpeg",
                        "isPro": false,
                        "fullname": "JinCheng Ren",
                        "user": "JinChengRen",
                        "type": "user"
                    },
                    "name": "JinCheng Ren",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:29:35.249Z",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a982",
                    "name": "Qianben Chen",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a983",
                    "name": "Zekun Moore Wang",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a984",
                    "name": "Yizhi LI",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a985",
                    "name": "Xinpeng Liu",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a986",
                    "name": "Xiaowan Li",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a987",
                    "name": "Wenli Ren",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a988",
                    "name": "Linyu Miao",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a989",
                    "name": "Tianrui Qin",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a98a",
                    "name": "Ziqi Shu",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a98b",
                    "name": "He Zhu",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a98c",
                    "name": "Xiangru Tang",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a98d",
                    "name": "Dingfeng Shi",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a98e",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a98f",
                    "name": "Yuchen Eleanor Jiang",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a990",
                    "name": "Minghao Liu",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a991",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "68edd775de1fee572713a992",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T17:30:36.000Z",
            "submittedOnDailyAt": "2025-10-14T04:51:41.289Z",
            "title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic\n  Research Problems",
            "submittedOnDailyBy": {
                "_id": "6704ee27386892c420db1938",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6704ee27386892c420db1938/lb5mtEwYhn47RawkynYPs.jpeg",
                "isPro": false,
                "fullname": "JinCheng Ren",
                "user": "JinChengRen",
                "type": "user"
            },
            "summary": "In recent years, the research focus of large language models (LLMs) and\nagents has shifted increasingly from demonstrating novel capabilities to\ncomplex reasoning and tackling challenging tasks. However, existing evaluations\nfocus mainly on math/code contests or general tasks, while existing\nmulti-domain academic benchmarks lack sufficient reasoning depth, leaving the\nfield without a rigorous benchmark for high-level reasoning. To fill this gap,\nwe introduce the Acadreason benchmark, designed to evaluate the ability of LLMs\nand agents to acquire and reason over academic knowledge. It consists of 50\nexpert-annotated academic problems across five high-reasoning domains,\nincluding computer science, economics, law, mathematics, and philosophy. All\nquestions are sourced from top-tier publications in recent years and undergo\nrigorous annotation and quality control to ensure they are both challenging and\nanswerable. We conduct systematic evaluations of over 10 mainstream LLMs and\nagents. The results show that most LLMs scored below 20 points, with even the\ncutting-edge GPT-5 achieving only 16 points. While agents achieved higher\nscores, none exceeded 40 points. This demonstrates the current capability gap\nbetween LLMs and agents in super-intelligent academic research tasks and\nhighlights the challenges of Acadreason.",
            "upvotes": 20,
            "discussionId": "68edd776de1fee572713a993",
            "ai_summary": "The Acadreason benchmark evaluates LLMs and agents on high-level academic reasoning across multiple domains, revealing significant capability gaps.",
            "ai_keywords": [
                "large language models",
                "agents",
                "Acadreason benchmark",
                "academic knowledge",
                "computer science",
                "economics",
                "law",
                "mathematics",
                "philosophy",
                "top-tier publications",
                "systematic evaluations",
                "GPT-5"
            ],
            "organization": {
                "_id": "684a463d17db6e9f271a0b66",
                "name": "PersonalAILab",
                "fullname": "OPPO-Personal-AI-Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632bfaebea6e62428ab0e9c2/P5L4TzWg2d4NXntonI-fK.png"
            }
        },
        "publishedAt": "2025-10-13T13:30:36.000Z",
        "title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic\n  Research Problems",
        "summary": "In recent years, the research focus of large language models (LLMs) and\nagents has shifted increasingly from demonstrating novel capabilities to\ncomplex reasoning and tackling challenging tasks. However, existing evaluations\nfocus mainly on math/code contests or general tasks, while existing\nmulti-domain academic benchmarks lack sufficient reasoning depth, leaving the\nfield without a rigorous benchmark for high-level reasoning. To fill this gap,\nwe introduce the Acadreason benchmark, designed to evaluate the ability of LLMs\nand agents to acquire and reason over academic knowledge. It consists of 50\nexpert-annotated academic problems across five high-reasoning domains,\nincluding computer science, economics, law, mathematics, and philosophy. All\nquestions are sourced from top-tier publications in recent years and undergo\nrigorous annotation and quality control to ensure they are both challenging and\nanswerable. We conduct systematic evaluations of over 10 mainstream LLMs and\nagents. The results show that most LLMs scored below 20 points, with even the\ncutting-edge GPT-5 achieving only 16 points. While agents achieved higher\nscores, none exceeded 40 points. This demonstrates the current capability gap\nbetween LLMs and agents in super-intelligent academic research tasks and\nhighlights the challenges of Acadreason.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11652.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6704ee27386892c420db1938",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6704ee27386892c420db1938/lb5mtEwYhn47RawkynYPs.jpeg",
            "fullname": "JinCheng Ren",
            "name": "JinChengRen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "684a463d17db6e9f271a0b66",
            "name": "PersonalAILab",
            "fullname": "OPPO-Personal-AI-Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632bfaebea6e62428ab0e9c2/P5L4TzWg2d4NXntonI-fK.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.11391",
            "authors": [
                {
                    "_id": "68edb30ade1fee572713a76d",
                    "user": {
                        "_id": "6512a2e284bedffb5ac5a511",
                        "avatarUrl": "/avatars/e798f2c8633d77aea41fce684d749390.svg",
                        "isPro": false,
                        "fullname": "Junpeng Liu",
                        "user": "jeepliu",
                        "type": "user"
                    },
                    "name": "Junpeng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:30:44.805Z",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a76e",
                    "name": "Yuzhong Zhao",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a76f",
                    "name": "Bowen Cao",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a770",
                    "name": "Jiayu Ding",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a771",
                    "name": "Yilin Jia",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a772",
                    "name": "Tengchao Lv",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a773",
                    "name": "Yupan Huang",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a774",
                    "name": "Shaohan Huang",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a775",
                    "name": "Nan Yang",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a776",
                    "name": "Li Dong",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a777",
                    "name": "Lei Cui",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a778",
                    "name": "Tao Ge",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a779",
                    "name": "Xun Wang",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a77a",
                    "name": "Huitian Jiao",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a77b",
                    "name": "Sun Mao",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a77c",
                    "name": "FNU Kartik",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a77d",
                    "name": "Si-Qing Chen",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a77e",
                    "name": "Wai Lam",
                    "hidden": false
                },
                {
                    "_id": "68edb30ade1fee572713a77f",
                    "name": "Furu Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T13:36:32.000Z",
            "submittedOnDailyAt": "2025-10-14T00:51:14.681Z",
            "title": "DocReward: A Document Reward Model for Structuring and Stylizing",
            "submittedOnDailyBy": {
                "_id": "6512a2e284bedffb5ac5a511",
                "avatarUrl": "/avatars/e798f2c8633d77aea41fce684d749390.svg",
                "isPro": false,
                "fullname": "Junpeng Liu",
                "user": "jeepliu",
                "type": "user"
            },
            "summary": "Recent advances in agentic workflows have enabled the automation of tasks\nsuch as professional document generation. However, they primarily focus on\ntextual quality, neglecting visual structure and style, which are crucial for\nreadability and engagement. This gap arises mainly from the absence of suitable\nreward models to guide agentic workflows toward producing documents with\nstronger structural and stylistic quality. To address this, we propose\nDocReward, a document reward model that evaluates documents based on their\nstructure and style. We construct a multi-domain dataset DocPair of 117K paired\ndocuments, covering 32 domains and 267 document types, each including a high-\nand low-professionalism document with identical content but different structure\nand style. This enables the model to evaluate professionalism comprehensively,\nand in a textual-quality-agnostic way. DocReward is trained using the\nBradley-Terry loss to score documents, penalizing predictions that contradict\nthe annotated ranking. To assess the performance of reward models, we create a\ntest dataset containing document bundles ranked by well-educated human\nevaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6\nand 19.4 percentage points, respectively, demonstrating its superiority over\nbaselines. In an extrinsic evaluation of document generation, DocReward\nachieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7%\nwin rate, demonstrating its utility in guiding generation agents toward\nproducing human-preferred documents.",
            "upvotes": 19,
            "discussionId": "68edb30ade1fee572713a780",
            "ai_summary": "DocReward, a document reward model, evaluates and enhances the structural and stylistic quality of generated documents, outperforming GPT-4o and GPT-5 in both accuracy and human-preferred document generation.",
            "ai_keywords": [
                "DocReward",
                "Bradley-Terry loss",
                "document reward model",
                "document generation",
                "extrinsic evaluation"
            ],
            "organization": {
                "_id": "68151d0f51add3813f3f7d1b",
                "name": "MicrosoftResearch",
                "fullname": "Microsoft Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
            }
        },
        "publishedAt": "2025-10-13T09:36:32.000Z",
        "title": "DocReward: A Document Reward Model for Structuring and Stylizing",
        "summary": "Recent advances in agentic workflows have enabled the automation of tasks\nsuch as professional document generation. However, they primarily focus on\ntextual quality, neglecting visual structure and style, which are crucial for\nreadability and engagement. This gap arises mainly from the absence of suitable\nreward models to guide agentic workflows toward producing documents with\nstronger structural and stylistic quality. To address this, we propose\nDocReward, a document reward model that evaluates documents based on their\nstructure and style. We construct a multi-domain dataset DocPair of 117K paired\ndocuments, covering 32 domains and 267 document types, each including a high-\nand low-professionalism document with identical content but different structure\nand style. This enables the model to evaluate professionalism comprehensively,\nand in a textual-quality-agnostic way. DocReward is trained using the\nBradley-Terry loss to score documents, penalizing predictions that contradict\nthe annotated ranking. To assess the performance of reward models, we create a\ntest dataset containing document bundles ranked by well-educated human\nevaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6\nand 19.4 percentage points, respectively, demonstrating its superiority over\nbaselines. In an extrinsic evaluation of document generation, DocReward\nachieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7%\nwin rate, demonstrating its utility in guiding generation agents toward\nproducing human-preferred documents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11391.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6512a2e284bedffb5ac5a511",
            "avatarUrl": "/avatars/e798f2c8633d77aea41fce684d749390.svg",
            "fullname": "Junpeng Liu",
            "name": "jeepliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "68151d0f51add3813f3f7d1b",
            "name": "MicrosoftResearch",
            "fullname": "Microsoft Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.10197",
            "authors": [
                {
                    "_id": "68edd8cede1fee572713a995",
                    "user": {
                        "_id": "6696954da7fd582ae70db39d",
                        "avatarUrl": "/avatars/92bf42710b206a3164b3ace8090443fa.svg",
                        "isPro": false,
                        "fullname": "Siyuan Lu (SII)",
                        "user": "IcyFish",
                        "type": "user"
                    },
                    "name": "Siyuan Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:29:30.092Z",
                    "hidden": false
                },
                {
                    "_id": "68edd8cede1fee572713a996",
                    "name": "Zechuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68edd8cede1fee572713a997",
                    "name": "Hongxuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68edd8cede1fee572713a998",
                    "user": {
                        "_id": "68243db9d08d8e01109dd4f4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/iZKRzoFHewLgAyNVanWf_.png",
                        "isPro": false,
                        "fullname": "Qing Wu",
                        "user": "qingw-dev",
                        "type": "user"
                    },
                    "name": "Qintong Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:29:27.348Z",
                    "hidden": false
                },
                {
                    "_id": "68edd8cede1fee572713a999",
                    "name": "Leilei Gan",
                    "hidden": false
                },
                {
                    "_id": "68edd8cede1fee572713a99a",
                    "user": {
                        "_id": "64e847ab5ddcace745b8f5b1",
                        "avatarUrl": "/avatars/89525e66ff2900f86f66a11043a298f9.svg",
                        "isPro": true,
                        "fullname": "chenyi zhuang",
                        "user": "chengle",
                        "type": "user"
                    },
                    "name": "Chenyi Zhuang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:29:25.180Z",
                    "hidden": false
                },
                {
                    "_id": "68edd8cede1fee572713a99b",
                    "name": "Jinjie Gu",
                    "hidden": false
                },
                {
                    "_id": "68edd8cede1fee572713a99c",
                    "name": "Tao Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-11T12:35:15.000Z",
            "submittedOnDailyAt": "2025-10-14T03:51:08.492Z",
            "title": "Don't Just Fine-tune the Agent, Tune the Environment",
            "submittedOnDailyBy": {
                "_id": "6696954da7fd582ae70db39d",
                "avatarUrl": "/avatars/92bf42710b206a3164b3ace8090443fa.svg",
                "isPro": false,
                "fullname": "Siyuan Lu (SII)",
                "user": "IcyFish",
                "type": "user"
            },
            "summary": "Large Language Model (LLM) agents show great promise for complex, multi-turn\ntool-use tasks, but their development is often hampered by the extreme scarcity\nof high-quality training data. Supervised fine-tuning (SFT) on synthetic data\nleads to overfitting, whereas standard reinforcement learning (RL) struggles\nwith a critical cold-start problem and training instability. To address these\nchallenges, we introduce Environment Tuning, a novel training\nparadigm that enables agents to learn complex behaviors directly from problem\ninstances without relying on pre-collected expert trajectories.\nEnvironment Tuning orchestrates this learning process through a\nstructured curriculum, actionable environment augmentation that provides\ncorrective feedback, and fine-grained progress rewards to ensure stable and\nefficient exploration. Using only 400 problem instances from Berkeley\nFunction-Calling Leaderboard (BFCL) benchmark, our method not only achieves\ncompetitive in-distribution performance against strong baselines but also\ndemonstrates superior out-of-distribution generalization, overcoming the\nperformance collapse common to SFT-based approaches. Our work presents a\nparadigm shift from supervised fine-tuning on static trajectories to dynamic,\nenvironment-based exploration, paving the way for training more robust and\ndata-efficient agents.",
            "upvotes": 19,
            "discussionId": "68edd8cfde1fee572713a99d",
            "ai_summary": "Environment Tuning enables LLM agents to learn complex behaviors from problem instances using a structured curriculum, environment augmentation, and progress rewards, achieving competitive in-distribution performance and superior out-of-distribution generalization.",
            "ai_keywords": [
                "Environment Tuning",
                "structured curriculum",
                "environment augmentation",
                "progress rewards",
                "reinforcement learning",
                "supervised fine-tuning",
                "LLM agents",
                "Berkeley Function-Calling Leaderboard",
                "in-distribution performance",
                "out-of-distribution generalization"
            ],
            "organization": {
                "_id": "67aea5c8f086ab0f70ed97c9",
                "name": "inclusionAI",
                "fullname": "inclusionAI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
            }
        },
        "publishedAt": "2025-10-11T08:35:15.000Z",
        "title": "Don't Just Fine-tune the Agent, Tune the Environment",
        "summary": "Large Language Model (LLM) agents show great promise for complex, multi-turn\ntool-use tasks, but their development is often hampered by the extreme scarcity\nof high-quality training data. Supervised fine-tuning (SFT) on synthetic data\nleads to overfitting, whereas standard reinforcement learning (RL) struggles\nwith a critical cold-start problem and training instability. To address these\nchallenges, we introduce Environment Tuning, a novel training\nparadigm that enables agents to learn complex behaviors directly from problem\ninstances without relying on pre-collected expert trajectories.\nEnvironment Tuning orchestrates this learning process through a\nstructured curriculum, actionable environment augmentation that provides\ncorrective feedback, and fine-grained progress rewards to ensure stable and\nefficient exploration. Using only 400 problem instances from Berkeley\nFunction-Calling Leaderboard (BFCL) benchmark, our method not only achieves\ncompetitive in-distribution performance against strong baselines but also\ndemonstrates superior out-of-distribution generalization, overcoming the\nperformance collapse common to SFT-based approaches. Our work presents a\nparadigm shift from supervised fine-tuning on static trajectories to dynamic,\nenvironment-based exploration, paving the way for training more robust and\ndata-efficient agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10197.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6696954da7fd582ae70db39d",
            "avatarUrl": "/avatars/92bf42710b206a3164b3ace8090443fa.svg",
            "fullname": "Siyuan Lu (SII)",
            "name": "IcyFish",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "67aea5c8f086ab0f70ed97c9",
            "name": "inclusionAI",
            "fullname": "inclusionAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08886",
            "authors": [
                {
                    "_id": "68edac0dde1fee572713a705",
                    "user": {
                        "_id": "65d76cc5b9b7b8bf88faa916",
                        "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                        "isPro": true,
                        "fullname": "Yan Wang",
                        "user": "YanAdjeNole",
                        "type": "user"
                    },
                    "name": "Yan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:30:56.790Z",
                    "hidden": false
                },
                {
                    "_id": "68edac0dde1fee572713a706",
                    "name": "Keyi Wang",
                    "hidden": false
                },
                {
                    "_id": "68edac0dde1fee572713a707",
                    "name": "Shanshan Yang",
                    "hidden": false
                },
                {
                    "_id": "68edac0dde1fee572713a708",
                    "name": "Jaisal Patel",
                    "hidden": false
                },
                {
                    "_id": "68edac0dde1fee572713a709",
                    "name": "Jeff Zhao",
                    "hidden": false
                },
                {
                    "_id": "68edac0dde1fee572713a70a",
                    "name": "Fengran Mo",
                    "hidden": false
                },
                {
                    "_id": "68edac0dde1fee572713a70b",
                    "name": "Xueqing Peng",
                    "hidden": false
                },
                {
                    "_id": "68edac0dde1fee572713a70c",
                    "name": "Lingfei Qian",
                    "hidden": false
                },
                {
                    "_id": "68edac0dde1fee572713a70d",
                    "user": {
                        "_id": "63b58ed5889aa6707f0bb0f4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/znl74_aMswlV8VtHrfj3G.jpeg",
                        "isPro": true,
                        "fullname": "Jimin Huang",
                        "user": "jiminHuang",
                        "type": "user"
                    },
                    "name": "Jimin Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:30:58.956Z",
                    "hidden": false
                },
                {
                    "_id": "68edac0dde1fee572713a70e",
                    "name": "Guojun Xiong",
                    "hidden": false
                },
                {
                    "_id": "68edac0dde1fee572713a70f",
                    "name": "Xiao-Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "68edac0dde1fee572713a710",
                    "name": "Jian-Yun Nie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T00:41:55.000Z",
            "submittedOnDailyAt": "2025-10-14T00:23:32.419Z",
            "title": "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark\n  for Evaluating LLMs",
            "submittedOnDailyBy": {
                "_id": "65d76cc5b9b7b8bf88faa916",
                "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                "isPro": true,
                "fullname": "Yan Wang",
                "user": "YanAdjeNole",
                "type": "user"
            },
            "summary": "The complexity of the Generally Accepted Accounting Principles (GAAP) and the\nhierarchical structure of eXtensible Business Reporting Language (XBRL) filings\nmake financial auditing increasingly difficult to automate and verify. While\nlarge language models (LLMs) have demonstrated strong capabilities in\nunstructured text understanding, their ability to reason over structured,\ninterdependent, and taxonomy-driven financial documents remains largely\nunexplored. To fill this gap, we introduce FinAuditing, the first\ntaxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs\non financial auditing tasks. Built from real US-GAAP-compliant XBRL filings,\nFinAuditing defines three complementary subtasks, FinSM for semantic\nconsistency, FinRE for relational consistency, and FinMR for numerical\nconsistency, each targeting a distinct aspect of structured auditing reasoning.\nWe further propose a unified evaluation framework integrating retrieval,\nclassification, and reasoning metrics across these subtasks. Extensive\nzero-shot experiments on 13 state-of-the-art LLMs reveal that current models\nperform inconsistently across semantic, relational, and mathematical\ndimensions, with accuracy drops of up to 60-90% when reasoning over\nhierarchical multi-document structures. Our findings expose the systematic\nlimitations of modern LLMs in taxonomy-grounded financial reasoning and\nestablish FinAuditing as a foundation for developing trustworthy,\nstructure-aware, and regulation-aligned financial intelligence systems. The\nbenchmark dataset is available at Hugging Face.",
            "upvotes": 19,
            "discussionId": "68edac0ede1fee572713a711",
            "githubRepo": "https://github.com/The-FinAI/FinAuditing.git",
            "ai_summary": "FinAuditing is a benchmark for evaluating LLMs on structured financial auditing tasks, revealing their limitations in handling taxonomy-driven, hierarchical financial documents.",
            "ai_keywords": [
                "LLMs",
                "FinAuditing",
                "taxonomy-aligned",
                "structure-aware",
                "multi-document benchmark",
                "FinSM",
                "FinRE",
                "FinMR",
                "semantic consistency",
                "relational consistency",
                "numerical consistency",
                "retrieval",
                "classification",
                "reasoning metrics",
                "zero-shot experiments",
                "US-GAAP",
                "XBRL filings"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "658f4413674349122c0708e9",
                "name": "TheFinAI",
                "fullname": "The Fin AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
            }
        },
        "publishedAt": "2025-10-09T20:41:55.000Z",
        "title": "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark\n  for Evaluating LLMs",
        "summary": "The complexity of the Generally Accepted Accounting Principles (GAAP) and the\nhierarchical structure of eXtensible Business Reporting Language (XBRL) filings\nmake financial auditing increasingly difficult to automate and verify. While\nlarge language models (LLMs) have demonstrated strong capabilities in\nunstructured text understanding, their ability to reason over structured,\ninterdependent, and taxonomy-driven financial documents remains largely\nunexplored. To fill this gap, we introduce FinAuditing, the first\ntaxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs\non financial auditing tasks. Built from real US-GAAP-compliant XBRL filings,\nFinAuditing defines three complementary subtasks, FinSM for semantic\nconsistency, FinRE for relational consistency, and FinMR for numerical\nconsistency, each targeting a distinct aspect of structured auditing reasoning.\nWe further propose a unified evaluation framework integrating retrieval,\nclassification, and reasoning metrics across these subtasks. Extensive\nzero-shot experiments on 13 state-of-the-art LLMs reveal that current models\nperform inconsistently across semantic, relational, and mathematical\ndimensions, with accuracy drops of up to 60-90% when reasoning over\nhierarchical multi-document structures. Our findings expose the systematic\nlimitations of modern LLMs in taxonomy-grounded financial reasoning and\nestablish FinAuditing as a foundation for developing trustworthy,\nstructure-aware, and regulation-aligned financial intelligence systems. The\nbenchmark dataset is available at Hugging Face.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08886.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "fullname": "Yan Wang",
            "name": "YanAdjeNole",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "658f4413674349122c0708e9",
            "name": "TheFinAI",
            "fullname": "The Fin AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.11026",
            "authors": [
                {
                    "_id": "68edc362de1fee572713a8bb",
                    "user": {
                        "_id": "675b94e3a301c0203a4131bc",
                        "avatarUrl": "/avatars/4b7b74a3efb1ed1649114c644389b8c9.svg",
                        "isPro": false,
                        "fullname": "Hongxiang Li",
                        "user": "lihxxx",
                        "type": "user"
                    },
                    "name": "Hongxiang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:30:17.520Z",
                    "hidden": false
                },
                {
                    "_id": "68edc362de1fee572713a8bc",
                    "name": "Yaowei Li",
                    "hidden": false
                },
                {
                    "_id": "68edc362de1fee572713a8bd",
                    "name": "Bin Lin",
                    "hidden": false
                },
                {
                    "_id": "68edc362de1fee572713a8be",
                    "name": "Yuwei Niu",
                    "hidden": false
                },
                {
                    "_id": "68edc362de1fee572713a8bf",
                    "name": "Yuhang Yang",
                    "hidden": false
                },
                {
                    "_id": "68edc362de1fee572713a8c0",
                    "name": "Xiaoshuang Huang",
                    "hidden": false
                },
                {
                    "_id": "68edc362de1fee572713a8c1",
                    "name": "Jiayin Cai",
                    "hidden": false
                },
                {
                    "_id": "68edc362de1fee572713a8c2",
                    "name": "Xiaolong Jiang",
                    "hidden": false
                },
                {
                    "_id": "68edc362de1fee572713a8c3",
                    "name": "Yao Hu",
                    "hidden": false
                },
                {
                    "_id": "68edc362de1fee572713a8c4",
                    "name": "Long Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T05:50:44.000Z",
            "submittedOnDailyAt": "2025-10-14T02:02:47.558Z",
            "title": "GIR-Bench: Versatile Benchmark for Generating Images with Reasoning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Unified multimodal models integrate the reasoning capacity of large language\nmodels with both image understanding and generation, showing great promise for\nadvanced multimodal intelligence. However, the community still lacks a rigorous\nreasoning-centric benchmark to systematically evaluate the alignment between\nunderstanding and generation, and their generalization potential in complex\nvisual tasks. To this end, we introduce GIR-Bench, a comprehensive\nbenchmark that evaluates unified models across three complementary\nperspectives. Firstly, we investigate understanding-generation consistency\n(GIR-Bench-UGC), asking whether models can consistently leverage the same\nknowledge in both understanding and generation tasks. Secondly, we investigate\nwhether models can perform reasoning-centric text-to-image generation that\nrequires applying logical constraints and implicit knowledge to generate\nfaithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models\ncan handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset,\nwe carefully design different task-specific evaluation pipelines tailored for\neach task. This enables fine-grained and interpretable evaluation while\nmitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive\nablations over various unified models and generation-only systems have shown\nthat: Although unified models are more capable of reasoning-driven visual\ntasks, they still exhibit a persistent gap between understanding and\ngeneration. The data and code for GIR-Bench are available at\nhttps://hkust-longgroup.github.io/GIR-Bench{https://hkust-longgroup.github.io/GIR-Bench}.",
            "upvotes": 17,
            "discussionId": "68edc362de1fee572713a8c5",
            "projectPage": "https://hkust-longgroup.github.io/GIR-Bench/",
            "githubRepo": "https://github.com/HKUST-LongGroup/GIR-Bench",
            "ai_summary": "GIR-Bench evaluates unified multimodal models across understanding-generation consistency, reasoning-centric text-to-image generation, and multi-step reasoning in editing, highlighting gaps in their capabilities.",
            "ai_keywords": [
                "unified multimodal models",
                "large language models",
                "image understanding",
                "image generation",
                "multimodal intelligence",
                "reasoning-centric benchmark",
                "understanding-generation consistency",
                "text-to-image generation",
                "multi-step reasoning",
                "evaluation pipelines",
                "MLLM-as-a-Judge paradigm"
            ],
            "githubStars": 23,
            "organization": {
                "_id": "63355133edc1a61aecf74b0e",
                "name": "HKUST",
                "fullname": "HKUST"
            }
        },
        "publishedAt": "2025-10-13T01:50:44.000Z",
        "title": "GIR-Bench: Versatile Benchmark for Generating Images with Reasoning",
        "summary": "Unified multimodal models integrate the reasoning capacity of large language\nmodels with both image understanding and generation, showing great promise for\nadvanced multimodal intelligence. However, the community still lacks a rigorous\nreasoning-centric benchmark to systematically evaluate the alignment between\nunderstanding and generation, and their generalization potential in complex\nvisual tasks. To this end, we introduce GIR-Bench, a comprehensive\nbenchmark that evaluates unified models across three complementary\nperspectives. Firstly, we investigate understanding-generation consistency\n(GIR-Bench-UGC), asking whether models can consistently leverage the same\nknowledge in both understanding and generation tasks. Secondly, we investigate\nwhether models can perform reasoning-centric text-to-image generation that\nrequires applying logical constraints and implicit knowledge to generate\nfaithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models\ncan handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset,\nwe carefully design different task-specific evaluation pipelines tailored for\neach task. This enables fine-grained and interpretable evaluation while\nmitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive\nablations over various unified models and generation-only systems have shown\nthat: Although unified models are more capable of reasoning-driven visual\ntasks, they still exhibit a persistent gap between understanding and\ngeneration. The data and code for GIR-Bench are available at\nhttps://hkust-longgroup.github.io/GIR-Bench{https://hkust-longgroup.github.io/GIR-Bench}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11026.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 126
        },
        "organization": {
            "_id": "63355133edc1a61aecf74b0e",
            "name": "HKUST",
            "fullname": "HKUST"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.10670",
            "authors": [
                {
                    "_id": "68edaa98de1fee572713a6d8",
                    "name": "Yu Li",
                    "hidden": false
                },
                {
                    "_id": "68edaa98de1fee572713a6d9",
                    "name": "Menghan Xia",
                    "hidden": false
                },
                {
                    "_id": "68edaa98de1fee572713a6da",
                    "name": "Gongye Liu",
                    "hidden": false
                },
                {
                    "_id": "68edaa98de1fee572713a6db",
                    "name": "Jianhong Bai",
                    "hidden": false
                },
                {
                    "_id": "68edaa98de1fee572713a6dc",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "68edaa98de1fee572713a6dd",
                    "name": "Conglang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68edaa98de1fee572713a6de",
                    "name": "Yuxuan Lin",
                    "hidden": false
                },
                {
                    "_id": "68edaa98de1fee572713a6df",
                    "name": "Ruihang Chu",
                    "hidden": false
                },
                {
                    "_id": "68edaa98de1fee572713a6e0",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "68edaa98de1fee572713a6e1",
                    "name": "Yujiu Yang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/651ed7ef755e92f7f12742e6/E-2LhC-CbFdLdojYWTqLg.png"
            ],
            "publishedAt": "2025-10-12T15:55:44.000Z",
            "submittedOnDailyAt": "2025-10-14T00:20:52.273Z",
            "title": "AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning\n  in 4D Scenes",
            "submittedOnDailyBy": {
                "_id": "651ed7ef755e92f7f12742e6",
                "avatarUrl": "/avatars/57a9cc189b4a59299aad6c96191b18d8.svg",
                "isPro": false,
                "fullname": "yu li",
                "user": "lyabc",
                "type": "user"
            },
            "summary": "Recent Text-to-Video (T2V) models have demonstrated powerful capability in\nvisual simulation of real-world geometry and physical laws, indicating its\npotential as implicit world models. Inspired by this, we explore the\nfeasibility of leveraging the video generation prior for viewpoint planning\nfrom given 4D scenes, since videos internally accompany dynamic scenes with\nnatural viewpoints. To this end, we propose a two-stage paradigm to adapt\npre-trained T2V models for viewpoint prediction, in a compatible manner. First,\nwe inject the 4D scene representation into the pre-trained T2V model via an\nadaptive learning branch, where the 4D scene is viewpoint-agnostic and the\nconditional generated video embeds the viewpoints visually. Then, we formulate\nviewpoint extraction as a hybrid-condition guided camera extrinsic denoising\nprocess. Specifically, a camera extrinsic diffusion branch is further\nintroduced onto the pre-trained T2V model, by taking the generated video and 4D\nscene as input. Experimental results show the superiority of our proposed\nmethod over existing competitors, and ablation studies validate the\neffectiveness of our key technical designs. To some extent, this work proves\nthe potential of video generation models toward 4D interaction in real world.",
            "upvotes": 16,
            "discussionId": "68edaa98de1fee572713a6e2",
            "projectPage": "https://yuli0103.github.io/AdaViewPlanner/",
            "ai_summary": "A two-stage paradigm adapts pre-trained Text-to-Video models for viewpoint prediction in 4D scenes by integrating an adaptive learning branch and a camera extrinsic diffusion branch.",
            "ai_keywords": [
                "Text-to-Video models",
                "viewpoint planning",
                "4D scenes",
                "adaptive learning branch",
                "viewpoint extraction",
                "hybrid-condition guided camera extrinsic denoising",
                "camera extrinsic diffusion branch"
            ],
            "organization": {
                "_id": "662c559b322afcbae51b3c8b",
                "name": "KwaiVGI",
                "fullname": "Kuaishou Visual Generation and Interaction Center",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
            }
        },
        "publishedAt": "2025-10-12T11:55:44.000Z",
        "title": "AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning\n  in 4D Scenes",
        "summary": "Recent Text-to-Video (T2V) models have demonstrated powerful capability in\nvisual simulation of real-world geometry and physical laws, indicating its\npotential as implicit world models. Inspired by this, we explore the\nfeasibility of leveraging the video generation prior for viewpoint planning\nfrom given 4D scenes, since videos internally accompany dynamic scenes with\nnatural viewpoints. To this end, we propose a two-stage paradigm to adapt\npre-trained T2V models for viewpoint prediction, in a compatible manner. First,\nwe inject the 4D scene representation into the pre-trained T2V model via an\nadaptive learning branch, where the 4D scene is viewpoint-agnostic and the\nconditional generated video embeds the viewpoints visually. Then, we formulate\nviewpoint extraction as a hybrid-condition guided camera extrinsic denoising\nprocess. Specifically, a camera extrinsic diffusion branch is further\nintroduced onto the pre-trained T2V model, by taking the generated video and 4D\nscene as input. Experimental results show the superiority of our proposed\nmethod over existing competitors, and ablation studies validate the\neffectiveness of our key technical designs. To some extent, this work proves\nthe potential of video generation models toward 4D interaction in real world.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/651ed7ef755e92f7f12742e6/E-2LhC-CbFdLdojYWTqLg.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10670.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "651ed7ef755e92f7f12742e6",
            "avatarUrl": "/avatars/57a9cc189b4a59299aad6c96191b18d8.svg",
            "fullname": "yu li",
            "name": "lyabc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "662c559b322afcbae51b3c8b",
            "name": "KwaiVGI",
            "fullname": "Kuaishou Visual Generation and Interaction Center",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11027",
            "authors": [
                {
                    "_id": "68edc292de1fee572713a8a7",
                    "user": {
                        "_id": "6565d7149afd51867e55520b",
                        "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
                        "isPro": false,
                        "fullname": "Ganlin Yang",
                        "user": "ganlinyang",
                        "type": "user"
                    },
                    "name": "Ganlin Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:30:25.854Z",
                    "hidden": false
                },
                {
                    "_id": "68edc292de1fee572713a8a8",
                    "user": {
                        "_id": "64c9e86a6a26cddbecd9bae2",
                        "avatarUrl": "/avatars/61a84989dbbc1898ebcba3236dbed039.svg",
                        "isPro": false,
                        "fullname": "Tianyi Zhang",
                        "user": "TianyiZhang0213",
                        "type": "user"
                    },
                    "name": "Tianyi Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T10:23:41.785Z",
                    "hidden": false
                },
                {
                    "_id": "68edc292de1fee572713a8a9",
                    "user": {
                        "_id": "669e221425f2081c6e3d8b61",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669e221425f2081c6e3d8b61/OearXzKrvoIZtDKTH4kje.jpeg",
                        "isPro": false,
                        "fullname": "Haoran Hao",
                        "user": "Hoar012",
                        "type": "user"
                    },
                    "name": "Haoran Hao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:30:20.398Z",
                    "hidden": false
                },
                {
                    "_id": "68edc292de1fee572713a8aa",
                    "name": "Weiyun Wang",
                    "hidden": false
                },
                {
                    "_id": "68edc292de1fee572713a8ab",
                    "name": "Yibin Liu",
                    "hidden": false
                },
                {
                    "_id": "68edc292de1fee572713a8ac",
                    "name": "Dehui Wang",
                    "hidden": false
                },
                {
                    "_id": "68edc292de1fee572713a8ad",
                    "user": {
                        "_id": "6624ba6d79d897d7ddee24b5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6624ba6d79d897d7ddee24b5/eHkAquXvHBlCNNgaRDYgG.jpeg",
                        "isPro": false,
                        "fullname": "Guanzhou Chen",
                        "user": "Rayment",
                        "type": "user"
                    },
                    "name": "Guanzhou Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:30:22.944Z",
                    "hidden": false
                },
                {
                    "_id": "68edc292de1fee572713a8ae",
                    "name": "Zijian Cai",
                    "hidden": false
                },
                {
                    "_id": "68edc292de1fee572713a8af",
                    "name": "Junting Chen",
                    "hidden": false
                },
                {
                    "_id": "68edc292de1fee572713a8b0",
                    "name": "Weijie Su",
                    "hidden": false
                },
                {
                    "_id": "68edc292de1fee572713a8b1",
                    "name": "Wengang Zhou",
                    "hidden": false
                },
                {
                    "_id": "68edc292de1fee572713a8b2",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68edc292de1fee572713a8b3",
                    "name": "Jifeng Dai",
                    "hidden": false
                },
                {
                    "_id": "68edc292de1fee572713a8b4",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                },
                {
                    "_id": "68edc292de1fee572713a8b5",
                    "name": "Gen Luo",
                    "hidden": false
                },
                {
                    "_id": "68edc292de1fee572713a8b6",
                    "name": "Wenhai Wang",
                    "hidden": false
                },
                {
                    "_id": "68edc292de1fee572713a8b7",
                    "name": "Yao Mu",
                    "hidden": false
                },
                {
                    "_id": "68edc292de1fee572713a8b8",
                    "name": "Zhi Hou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T05:51:22.000Z",
            "submittedOnDailyAt": "2025-10-14T02:00:35.070Z",
            "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
            "submittedOnDailyBy": {
                "_id": "6565d7149afd51867e55520b",
                "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
                "isPro": false,
                "fullname": "Ganlin Yang",
                "user": "ganlinyang",
                "type": "user"
            },
            "summary": "While significant research has focused on developing embodied reasoning\ncapabilities using Vision-Language Models (VLMs) or integrating advanced VLMs\ninto Vision-Language-Action (VLA) models for end-to-end robot control, few\nstudies directly address the critical gap between upstream VLM-based reasoning\nand downstream VLA policy learning. In this work, we take an initial step\ntoward bridging embodied reasoning with VLA policy learning by introducing\nVlaser - a Vision-Language-Action Model with synergistic embodied reasoning\ncapability, which is a foundational vision-language model designed to integrate\nhigh-level reasoning with low-level control for embodied agents. Built upon the\nhigh-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance\nacross a range of embodied reasoning benchmarks - including spatial reasoning,\nembodied grounding, embodied QA, and task planning. Furthermore, we\nsystematically examine how different VLM initializations affect supervised VLA\nfine-tuning, offering novel insights into mitigating the domain shift between\ninternet-scale pre-training data and embodied-specific policy learning data.\nBased on these insights, our approach achieves state-of-the-art results on the\nWidowX benchmark and competitive performance on the Google Robot benchmark.",
            "upvotes": 15,
            "discussionId": "68edc293de1fee572713a8b9",
            "projectPage": "https://internvl.github.io/blog/2025-10-11-Vlaser/",
            "githubRepo": "https://github.com/OpenGVLab/Vlaser/",
            "ai_summary": "Vlaser, a Vision-Language-Action Model, integrates high-level reasoning with low-level control for embodied agents, achieving state-of-the-art performance in embodied reasoning tasks and competitive results in robot benchmarks.",
            "ai_keywords": [
                "Vision-Language Models",
                "Vision-Language-Action models",
                "embodied reasoning",
                "Vlaser",
                "Vlaser-6M dataset",
                "spatial reasoning",
                "embodied grounding",
                "embodied QA",
                "task planning",
                "supervised VLA fine-tuning",
                "domain shift",
                "WidowX benchmark",
                "Google Robot benchmark"
            ],
            "githubStars": 15,
            "organization": {
                "_id": "6747ee5decec679eafb90450",
                "name": "ShanghaiAiLab",
                "fullname": "shanghai ailab "
            }
        },
        "publishedAt": "2025-10-13T01:51:22.000Z",
        "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
        "summary": "While significant research has focused on developing embodied reasoning\ncapabilities using Vision-Language Models (VLMs) or integrating advanced VLMs\ninto Vision-Language-Action (VLA) models for end-to-end robot control, few\nstudies directly address the critical gap between upstream VLM-based reasoning\nand downstream VLA policy learning. In this work, we take an initial step\ntoward bridging embodied reasoning with VLA policy learning by introducing\nVlaser - a Vision-Language-Action Model with synergistic embodied reasoning\ncapability, which is a foundational vision-language model designed to integrate\nhigh-level reasoning with low-level control for embodied agents. Built upon the\nhigh-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance\nacross a range of embodied reasoning benchmarks - including spatial reasoning,\nembodied grounding, embodied QA, and task planning. Furthermore, we\nsystematically examine how different VLM initializations affect supervised VLA\nfine-tuning, offering novel insights into mitigating the domain shift between\ninternet-scale pre-training data and embodied-specific policy learning data.\nBased on these insights, our approach achieves state-of-the-art results on the\nWidowX benchmark and competitive performance on the Google Robot benchmark.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11027.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6565d7149afd51867e55520b",
            "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
            "fullname": "Ganlin Yang",
            "name": "ganlinyang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "6747ee5decec679eafb90450",
            "name": "ShanghaiAiLab",
            "fullname": "shanghai ailab "
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.09541",
            "authors": [
                {
                    "_id": "68edb2ecde1fee572713a753",
                    "name": "Chenyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68edb2ecde1fee572713a754",
                    "name": "Paria Rashidinejad",
                    "hidden": false
                },
                {
                    "_id": "68edb2ecde1fee572713a755",
                    "name": "DiJia Su",
                    "hidden": false
                },
                {
                    "_id": "68edb2ecde1fee572713a756",
                    "name": "Song Jiang",
                    "hidden": false
                },
                {
                    "_id": "68edb2ecde1fee572713a757",
                    "name": "Sid Wang",
                    "hidden": false
                },
                {
                    "_id": "68edb2ecde1fee572713a758",
                    "name": "Siyan Zhao",
                    "hidden": false
                },
                {
                    "_id": "68edb2ecde1fee572713a759",
                    "name": "Cai Zhou",
                    "hidden": false
                },
                {
                    "_id": "68edb2ecde1fee572713a75a",
                    "name": "Shannon Zejiang Shen",
                    "hidden": false
                },
                {
                    "_id": "68edb2ecde1fee572713a75b",
                    "name": "Feiyu Chen",
                    "hidden": false
                },
                {
                    "_id": "68edb2ecde1fee572713a75c",
                    "name": "Tommi Jaakkola",
                    "hidden": false
                },
                {
                    "_id": "68edb2ecde1fee572713a75d",
                    "name": "Yuandong Tian",
                    "hidden": false
                },
                {
                    "_id": "68edb2ecde1fee572713a75e",
                    "name": "Bo Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T16:52:25.000Z",
            "submittedOnDailyAt": "2025-10-14T00:56:45.124Z",
            "title": "SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models",
            "submittedOnDailyBy": {
                "_id": "6632b788ec8aa01ab2f4dc36",
                "avatarUrl": "/avatars/7a80f946dfd0e105007d84623cd55e90.svg",
                "isPro": false,
                "fullname": "Chenyu Wang",
                "user": "wangchy",
                "type": "user"
            },
            "summary": "Diffusion large language models (dLLMs) are emerging as an efficient\nalternative to autoregressive models due to their ability to decode multiple\ntokens in parallel. However, aligning dLLMs with human preferences or\ntask-specific rewards via reinforcement learning (RL) is challenging because\ntheir intractable log-likelihood precludes the direct application of standard\npolicy gradient methods. While prior work uses surrogates like the evidence\nlower bound (ELBO), these one-sided approximations can introduce significant\npolicy gradient bias. To address this, we propose the Sandwiched Policy\nGradient (SPG) that leverages both an upper and a lower bound of the true\nlog-likelihood. Experiments show that SPG significantly outperforms baselines\nbased on ELBO or one-step estimation. Specifically, SPG improves the accuracy\nover state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500,\n18.4% in Countdown and 27.0% in Sudoku.",
            "upvotes": 14,
            "discussionId": "68edb2ecde1fee572713a75f",
            "githubRepo": "https://github.com/facebookresearch/SPG",
            "ai_summary": "The Sandwiched Policy Gradient method improves reinforcement learning for diffusion large language models by using both upper and lower bounds of log-likelihood, outperforming ELBO-based methods.",
            "ai_keywords": [
                "diffusion large language models",
                "dLLMs",
                "autoregressive models",
                "reinforcement learning",
                "RL",
                "log-likelihood",
                "policy gradient methods",
                "evidence lower bound",
                "ELBO",
                "Sandwiched Policy Gradient",
                "SPG",
                "GSM8K",
                "MATH500",
                "Countdown",
                "Sudoku"
            ],
            "githubStars": 9,
            "organization": {
                "_id": "66b54027408752ae16404b05",
                "name": "metaresearch",
                "fullname": "Meta Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
            }
        },
        "publishedAt": "2025-10-10T12:52:25.000Z",
        "title": "SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models",
        "summary": "Diffusion large language models (dLLMs) are emerging as an efficient\nalternative to autoregressive models due to their ability to decode multiple\ntokens in parallel. However, aligning dLLMs with human preferences or\ntask-specific rewards via reinforcement learning (RL) is challenging because\ntheir intractable log-likelihood precludes the direct application of standard\npolicy gradient methods. While prior work uses surrogates like the evidence\nlower bound (ELBO), these one-sided approximations can introduce significant\npolicy gradient bias. To address this, we propose the Sandwiched Policy\nGradient (SPG) that leverages both an upper and a lower bound of the true\nlog-likelihood. Experiments show that SPG significantly outperforms baselines\nbased on ELBO or one-step estimation. Specifically, SPG improves the accuracy\nover state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500,\n18.4% in Countdown and 27.0% in Sudoku.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09541.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6632b788ec8aa01ab2f4dc36",
            "avatarUrl": "/avatars/7a80f946dfd0e105007d84623cd55e90.svg",
            "fullname": "Chenyu Wang",
            "name": "wangchy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "66b54027408752ae16404b05",
            "name": "metaresearch",
            "fullname": "Meta Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11718",
            "authors": [
                {
                    "_id": "68edb609de1fee572713a78f",
                    "name": "Chengqi Duan",
                    "hidden": false
                },
                {
                    "_id": "68edb609de1fee572713a790",
                    "name": "Kaiyue Sun",
                    "hidden": false
                },
                {
                    "_id": "68edb609de1fee572713a791",
                    "name": "Rongyao Fang",
                    "hidden": false
                },
                {
                    "_id": "68edb609de1fee572713a792",
                    "name": "Manyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68edb609de1fee572713a793",
                    "name": "Yan Feng",
                    "hidden": false
                },
                {
                    "_id": "68edb609de1fee572713a794",
                    "name": "Ying Luo",
                    "hidden": false
                },
                {
                    "_id": "68edb609de1fee572713a795",
                    "name": "Yufang Liu",
                    "hidden": false
                },
                {
                    "_id": "68edb609de1fee572713a796",
                    "user": {
                        "_id": "64d592c28767727dffa1f002",
                        "avatarUrl": "/avatars/fe38bcac944a2742dc12c624e62d24ef.svg",
                        "isPro": false,
                        "fullname": "WangKe",
                        "user": "scikkk",
                        "type": "user"
                    },
                    "name": "Ke Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T10:23:46.766Z",
                    "hidden": false
                },
                {
                    "_id": "68edb609de1fee572713a797",
                    "name": "Peng Pei",
                    "hidden": false
                },
                {
                    "_id": "68edb609de1fee572713a798",
                    "name": "Xunliang Cai",
                    "hidden": false
                },
                {
                    "_id": "68edb609de1fee572713a799",
                    "name": "Hongsheng Li",
                    "hidden": false
                },
                {
                    "_id": "68edb609de1fee572713a79a",
                    "name": "Yi Ma",
                    "hidden": false
                },
                {
                    "_id": "68edb609de1fee572713a79b",
                    "user": {
                        "_id": "65d5ec74cd05bc1eaa125040",
                        "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
                        "isPro": false,
                        "fullname": "Xihui Liu",
                        "user": "XihuiLiu",
                        "type": "user"
                    },
                    "name": "Xihui Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:30:42.570Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T17:59:55.000Z",
            "submittedOnDailyAt": "2025-10-14T01:40:32.364Z",
            "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven\n  Images",
            "submittedOnDailyBy": {
                "_id": "65b8724123d948d884b379b1",
                "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
                "isPro": false,
                "fullname": "Rongyao Fang",
                "user": "LucasFang",
                "type": "user"
            },
            "summary": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
            "upvotes": 13,
            "discussionId": "68edb609de1fee572713a79c",
            "projectPage": "https://math-vr.github.io/",
            "githubRepo": "https://github.com/HKU-MMLab/Math-VR-CodePlot-CoT",
            "ai_summary": "CodePlot-CoT, a code-driven Chain-of-Thought model, enhances multimodal mathematical reasoning by generating both text and executable plotting code to solve problems requiring visual assistance.",
            "ai_keywords": [
                "Large Language Models",
                "Vision Language Models",
                "mathematical reasoning",
                "visual assistance",
                "multimodal unified models",
                "Chain-of-Thought",
                "Math-VR",
                "image-to-code converter",
                "visual reasoning",
                "mathematical problems"
            ],
            "githubStars": 22,
            "organization": {
                "_id": "67ea9ecfc234715db8dbf339",
                "name": "hkuhk",
                "fullname": "The University of Hong Kong",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
            }
        },
        "publishedAt": "2025-10-13T13:59:55.000Z",
        "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven\n  Images",
        "summary": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11718.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65b8724123d948d884b379b1",
            "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
            "fullname": "Rongyao Fang",
            "name": "LucasFang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "organization": {
            "_id": "67ea9ecfc234715db8dbf339",
            "name": "hkuhk",
            "fullname": "The University of Hong Kong",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.09008",
            "authors": [
                {
                    "_id": "68edc6a9de1fee572713a8fe",
                    "user": {
                        "_id": "633e6f07309a99325095dd42",
                        "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
                        "isPro": false,
                        "fullname": "Hoigi Seo",
                        "user": "Agorium",
                        "type": "user"
                    },
                    "name": "Hoigi Seo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:30:10.592Z",
                    "hidden": false
                },
                {
                    "_id": "68edc6a9de1fee572713a8ff",
                    "name": "Dong Un Kang",
                    "hidden": false
                },
                {
                    "_id": "68edc6a9de1fee572713a900",
                    "user": {
                        "_id": "66b46b576badc45885923979",
                        "avatarUrl": "/avatars/622e95c50876d48fc2dcb9b0dbc74607.svg",
                        "isPro": false,
                        "fullname": "Hyunjin Cho",
                        "user": "jfdkjjs",
                        "type": "user"
                    },
                    "name": "Hyunjin Cho",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:30:08.146Z",
                    "hidden": false
                },
                {
                    "_id": "68edc6a9de1fee572713a901",
                    "name": "Joohoon Lee",
                    "hidden": false
                },
                {
                    "_id": "68edc6a9de1fee572713a902",
                    "name": "Se Young Chun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T05:12:52.000Z",
            "submittedOnDailyAt": "2025-10-14T02:13:55.551Z",
            "title": "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in\n  Large Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "633e6f07309a99325095dd42",
                "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
                "isPro": false,
                "fullname": "Hoigi Seo",
                "user": "Agorium",
                "type": "user"
            },
            "summary": "Large vision-language models (LVLMs), which integrate a vision encoder (VE)\nwith a large language model, have achieved remarkable success across various\ntasks. However, there are still crucial challenges in LVLMs such as object\nhallucination, generating descriptions of objects that are not in the input\nimage. Here, we argue that uncertain visual tokens within the VE is a key\nfactor that contributes to object hallucination. Our statistical analysis found\nthat there are positive correlations between visual tokens with high epistemic\nuncertainty and the occurrence of hallucinations. Furthermore, we show\ntheoretically and empirically that visual tokens in early VE layers that\nexhibit large representation deviations under small adversarial perturbations\nindicate high epistemic uncertainty. Based on these findings, we propose a\nsimple yet effective strategy to mitigate object hallucination by modifying the\nVE only. Our method comprises a proxy method with adversarial perturbations for\nidentifying uncertain visual tokens efficiently and a method to mask these\nuncertain visual tokens during the self-attention process in the middle layers\nof the VE, suppressing their influence on visual encoding and thus alleviating\nhallucinations. Extensive experiments show that our method significantly\nreduces object hallucinations in LVLMs and can synergistically work with other\nprior arts.",
            "upvotes": 13,
            "discussionId": "68edc6a9de1fee572713a903",
            "projectPage": "https://keenyjin.github.io/epistemic/",
            "githubRepo": "https://github.com/joohoonlee/Epistemic",
            "ai_summary": "A method to reduce object hallucinations in large vision-language models by identifying and masking uncertain visual tokens in the vision encoder.",
            "ai_keywords": [
                "vision-language models",
                "vision encoder",
                "object hallucination",
                "visual tokens",
                "epistemic uncertainty",
                "adversarial perturbations",
                "self-attention",
                "visual encoding"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "66d54dc8033492801db2bf5a",
                "name": "SeoulNatlUniv",
                "fullname": "Seoul National University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/659ccc9d18897eb6594e897f/_-0BM-1UyM-d-lRiahFnf.png"
            }
        },
        "publishedAt": "2025-10-10T01:12:52.000Z",
        "title": "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in\n  Large Vision-Language Models",
        "summary": "Large vision-language models (LVLMs), which integrate a vision encoder (VE)\nwith a large language model, have achieved remarkable success across various\ntasks. However, there are still crucial challenges in LVLMs such as object\nhallucination, generating descriptions of objects that are not in the input\nimage. Here, we argue that uncertain visual tokens within the VE is a key\nfactor that contributes to object hallucination. Our statistical analysis found\nthat there are positive correlations between visual tokens with high epistemic\nuncertainty and the occurrence of hallucinations. Furthermore, we show\ntheoretically and empirically that visual tokens in early VE layers that\nexhibit large representation deviations under small adversarial perturbations\nindicate high epistemic uncertainty. Based on these findings, we propose a\nsimple yet effective strategy to mitigate object hallucination by modifying the\nVE only. Our method comprises a proxy method with adversarial perturbations for\nidentifying uncertain visual tokens efficiently and a method to mask these\nuncertain visual tokens during the self-attention process in the middle layers\nof the VE, suppressing their influence on visual encoding and thus alleviating\nhallucinations. Extensive experiments show that our method significantly\nreduces object hallucinations in LVLMs and can synergistically work with other\nprior arts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09008.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "633e6f07309a99325095dd42",
            "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
            "fullname": "Hoigi Seo",
            "name": "Agorium",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "66d54dc8033492801db2bf5a",
            "name": "SeoulNatlUniv",
            "fullname": "Seoul National University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/659ccc9d18897eb6594e897f/_-0BM-1UyM-d-lRiahFnf.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.10637",
            "authors": [
                {
                    "_id": "68edb788de1fee572713a7ac",
                    "name": "Haoyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68edb788de1fee572713a7ad",
                    "name": "Cheng Zeng",
                    "hidden": false
                },
                {
                    "_id": "68edb788de1fee572713a7ae",
                    "name": "Linghao Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68edb788de1fee572713a7af",
                    "name": "Yaxi Zhao",
                    "hidden": false
                },
                {
                    "_id": "68edb788de1fee572713a7b0",
                    "name": "Shengke Xue",
                    "hidden": false
                },
                {
                    "_id": "68edb788de1fee572713a7b1",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "68edb788de1fee572713a7b2",
                    "name": "Xingyue Zhao",
                    "hidden": false
                },
                {
                    "_id": "68edb788de1fee572713a7b3",
                    "name": "Zhongyu Li",
                    "hidden": false
                },
                {
                    "_id": "68edb788de1fee572713a7b4",
                    "name": "Kehan Li",
                    "hidden": false
                },
                {
                    "_id": "68edb788de1fee572713a7b5",
                    "name": "Siteng Huang",
                    "hidden": false
                },
                {
                    "_id": "68edb788de1fee572713a7b6",
                    "name": "Mingxiu Chen",
                    "hidden": false
                },
                {
                    "_id": "68edb788de1fee572713a7b7",
                    "name": "Xin Li",
                    "hidden": false
                },
                {
                    "_id": "68edb788de1fee572713a7b8",
                    "name": "Deli Zhao",
                    "hidden": false
                },
                {
                    "_id": "68edb788de1fee572713a7b9",
                    "name": "Hua Zou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65fd82762bf2cd20ddaa193f/APUgtP0dJ9AuLFpwvc-4i.mp4"
            ],
            "publishedAt": "2025-10-12T14:42:07.000Z",
            "submittedOnDailyAt": "2025-10-14T03:11:32.748Z",
            "title": "High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic\n  Manipulation Learning with Gaussian Splatting",
            "submittedOnDailyBy": {
                "_id": "65fd82762bf2cd20ddaa193f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
                "isPro": false,
                "fullname": "Siteng Huang",
                "user": "huangsiteng",
                "type": "user"
            },
            "summary": "The scalability of robotic learning is fundamentally bottlenecked by the\nsignificant cost and labor of real-world data collection. While simulated data\noffers a scalable alternative, it often fails to generalize to the real world\ndue to significant gaps in visual appearance, physical properties, and object\ninteractions. To address this, we propose RoboSimGS, a novel Real2Sim2Real\nframework that converts multi-view real-world images into scalable,\nhigh-fidelity, and physically interactive simulation environments for robotic\nmanipulation. Our approach reconstructs scenes using a hybrid representation:\n3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the\nenvironment, while mesh primitives for interactive objects ensure accurate\nphysics simulation. Crucially, we pioneer the use of a Multi-modal Large\nLanguage Model (MLLM) to automate the creation of physically plausible,\narticulated assets. The MLLM analyzes visual data to infer not only physical\nproperties (e.g., density, stiffness) but also complex kinematic structures\n(e.g., hinges, sliding rails) of objects. We demonstrate that policies trained\nentirely on data generated by RoboSimGS achieve successful zero-shot\nsim-to-real transfer across a diverse set of real-world manipulation tasks.\nFurthermore, data from RoboSimGS significantly enhances the performance and\ngeneralization capabilities of SOTA methods. Our results validate RoboSimGS as\na powerful and scalable solution for bridging the sim-to-real gap.",
            "upvotes": 10,
            "discussionId": "68edb788de1fee572713a7ba",
            "projectPage": "https://robosimgs.github.io/",
            "ai_summary": "RoboSimGS, a Real2Sim2Real framework, uses 3D Gaussian Splatting and mesh primitives to create scalable, high-fidelity, and physically interactive simulation environments, enabling successful zero-shot sim-to-real transfer for robotic manipulation tasks.",
            "ai_keywords": [
                "3D Gaussian Splatting",
                "mesh primitives",
                "Multi-modal Large Language Model",
                "physically plausible",
                "articulated assets",
                "zero-shot sim-to-real transfer",
                "robotic manipulation"
            ],
            "organization": {
                "_id": "6808e7522a4d69d5111da55f",
                "name": "Alibaba-DAMO-Academy",
                "fullname": "DAMO Academy",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
            }
        },
        "publishedAt": "2025-10-12T10:42:07.000Z",
        "title": "High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic\n  Manipulation Learning with Gaussian Splatting",
        "summary": "The scalability of robotic learning is fundamentally bottlenecked by the\nsignificant cost and labor of real-world data collection. While simulated data\noffers a scalable alternative, it often fails to generalize to the real world\ndue to significant gaps in visual appearance, physical properties, and object\ninteractions. To address this, we propose RoboSimGS, a novel Real2Sim2Real\nframework that converts multi-view real-world images into scalable,\nhigh-fidelity, and physically interactive simulation environments for robotic\nmanipulation. Our approach reconstructs scenes using a hybrid representation:\n3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the\nenvironment, while mesh primitives for interactive objects ensure accurate\nphysics simulation. Crucially, we pioneer the use of a Multi-modal Large\nLanguage Model (MLLM) to automate the creation of physically plausible,\narticulated assets. The MLLM analyzes visual data to infer not only physical\nproperties (e.g., density, stiffness) but also complex kinematic structures\n(e.g., hinges, sliding rails) of objects. We demonstrate that policies trained\nentirely on data generated by RoboSimGS achieve successful zero-shot\nsim-to-real transfer across a diverse set of real-world manipulation tasks.\nFurthermore, data from RoboSimGS significantly enhances the performance and\ngeneralization capabilities of SOTA methods. Our results validate RoboSimGS as\na powerful and scalable solution for bridging the sim-to-real gap.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65fd82762bf2cd20ddaa193f/APUgtP0dJ9AuLFpwvc-4i.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10637.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65fd82762bf2cd20ddaa193f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
            "fullname": "Siteng Huang",
            "name": "huangsiteng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "organization": {
            "_id": "6808e7522a4d69d5111da55f",
            "name": "Alibaba-DAMO-Academy",
            "fullname": "DAMO Academy",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11769",
            "authors": [
                {
                    "_id": "68eef74b486b78128f0e32ef",
                    "name": "Ruida Wang",
                    "hidden": false
                },
                {
                    "_id": "68eef74b486b78128f0e32f0",
                    "name": "Jiarui Yao",
                    "hidden": false
                },
                {
                    "_id": "68eef74b486b78128f0e32f1",
                    "name": "Rui Pan",
                    "hidden": false
                },
                {
                    "_id": "68eef74b486b78128f0e32f2",
                    "name": "Shizhe Diao",
                    "hidden": false
                },
                {
                    "_id": "68eef74b486b78128f0e32f3",
                    "name": "Tong Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T17:56:25.000Z",
            "submittedOnDailyAt": "2025-10-14T23:54:40.058Z",
            "title": "GAR: Generative Adversarial Reinforcement Learning for Formal Theorem\n  Proving",
            "submittedOnDailyBy": {
                "_id": "6618d1c3c1167c8d8702b19d",
                "avatarUrl": "/avatars/24611ca6c4158f4978ac8476a87d8d9c.svg",
                "isPro": false,
                "fullname": "Ruida WANG",
                "user": "RickyDeSkywalker",
                "type": "user"
            },
            "summary": "Solving math problems through verifiable languages such as Lean has\nsignificantly impacted both the mathematics and computer science communities.\nCurrent state-of-the-art models are often trained with expensive online\nReinforcement Learning (RL) or expert iteration. However, these approaches rely\non fixed problem sets, which causes inefficient training and limits the model\nto tackle complex problems. To overcome these limitations, we propose GAR:\nGenerative Adversarial Reinforcement learning, a comprehensive RL training\nframework that jointly trains the problem composer and solver in an adversarial\nloop. GAR introduces an implicit curriculum learning mechanism, which aligns\ntask difficulty with the prover's evolving capability. It thereby improves the\ntraining efficiency and enables stronger performance of proving advanced\ntheorems. Experiments show that with GAR training, Goedel-Prover-V2-8B and\nDeepSeek-Prover-V2-7B achieve an average relative improvement in pass@32 of\n4.20% on MiniF2F-Test benchmark, while DeepSeek-Prover-V2's pass@32 on\nProofNet-Test increases from 22.58% to 25.81%. Beyond formal proving, GAR\nestablishes a general RL paradigm for co-evolution of problem generation and\nsolving under verifiable environments.",
            "upvotes": 9,
            "discussionId": "68eef74b486b78128f0e32f4"
        },
        "publishedAt": "2025-10-13T13:56:25.000Z",
        "title": "GAR: Generative Adversarial Reinforcement Learning for Formal Theorem\n  Proving",
        "summary": "Solving math problems through verifiable languages such as Lean has\nsignificantly impacted both the mathematics and computer science communities.\nCurrent state-of-the-art models are often trained with expensive online\nReinforcement Learning (RL) or expert iteration. However, these approaches rely\non fixed problem sets, which causes inefficient training and limits the model\nto tackle complex problems. To overcome these limitations, we propose GAR:\nGenerative Adversarial Reinforcement learning, a comprehensive RL training\nframework that jointly trains the problem composer and solver in an adversarial\nloop. GAR introduces an implicit curriculum learning mechanism, which aligns\ntask difficulty with the prover's evolving capability. It thereby improves the\ntraining efficiency and enables stronger performance of proving advanced\ntheorems. Experiments show that with GAR training, Goedel-Prover-V2-8B and\nDeepSeek-Prover-V2-7B achieve an average relative improvement in pass@32 of\n4.20% on MiniF2F-Test benchmark, while DeepSeek-Prover-V2's pass@32 on\nProofNet-Test increases from 22.58% to 25.81%. Beyond formal proving, GAR\nestablishes a general RL paradigm for co-evolution of problem generation and\nsolving under verifiable environments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11769.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6618d1c3c1167c8d8702b19d",
            "avatarUrl": "/avatars/24611ca6c4158f4978ac8476a87d8d9c.svg",
            "fullname": "Ruida WANG",
            "name": "RickyDeSkywalker",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11498",
            "authors": [
                {
                    "_id": "68edc640de1fee572713a8f3",
                    "name": "Yuhang Li",
                    "hidden": false
                },
                {
                    "_id": "68edc640de1fee572713a8f4",
                    "user": {
                        "_id": "64b74b906ab5d14ca7f289cd",
                        "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
                        "isPro": false,
                        "fullname": "Chenchen Zhang",
                        "user": "xxzcc",
                        "type": "user"
                    },
                    "name": "Chenchen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:30:13.082Z",
                    "hidden": false
                },
                {
                    "_id": "68edc640de1fee572713a8f5",
                    "name": "Ruilin Lv",
                    "hidden": false
                },
                {
                    "_id": "68edc640de1fee572713a8f6",
                    "name": "Ao Liu",
                    "hidden": false
                },
                {
                    "_id": "68edc640de1fee572713a8f7",
                    "name": "Ken Deng",
                    "hidden": false
                },
                {
                    "_id": "68edc640de1fee572713a8f8",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "68edc640de1fee572713a8f9",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68edc640de1fee572713a8fa",
                    "name": "Wiggin Zhou",
                    "hidden": false
                },
                {
                    "_id": "68edc640de1fee572713a8fb",
                    "name": "Bo Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T15:05:50.000Z",
            "submittedOnDailyAt": "2025-10-14T02:12:08.156Z",
            "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web\n  Coding",
            "submittedOnDailyBy": {
                "_id": "64b74b906ab5d14ca7f289cd",
                "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
                "isPro": false,
                "fullname": "Chenchen Zhang",
                "user": "xxzcc",
                "type": "user"
            },
            "summary": "While Large Language Models (LLMs) excel at algorithmic code generation, they\nstruggle with front-end development, where correctness is judged on rendered\npixels and interaction. We present ReLook, an agentic, vision-grounded\nreinforcement learning framework that empowers an agent to close a robust\ngenerate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.\nDuring training, the agent uses the MLLM-in-the-loop both as a visual\ncritic--scoring code with screenshots--and as a source of actionable,\nvision-grounded feedback; a strict zero-reward rule for invalid renders anchors\nrenderability and prevents reward hacking. To prevent behavioral collapse, we\nintroduce Forced Optimization, a strict acceptance rule that admits only\nimproving revisions, yielding monotonically better trajectories. At inference,\nwe decouple the critic and run a lightweight, critic-free self-edit cycle,\nkeeping latency comparable to base decoding while retaining most of the gains.\nAcross three widely used benchmarks, ReLook consistently outperforms strong\nbaselines in vision-grounded front-end code generation, highlighting the\nbenefits of agentic perception, visual rewards, and training-inference\ndecoupling.",
            "upvotes": 9,
            "discussionId": "68edc641de1fee572713a8fc",
            "projectPage": "https://artifactsbenchmark.github.io/",
            "githubRepo": "https://github.com/Tencent-Hunyuan/ArtifactsBenchmark",
            "ai_summary": "ReLook, a vision-grounded reinforcement learning framework, enhances front-end code generation by integrating a multimodal LLM for visual feedback and forced optimization, outperforming existing methods.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "front-end development",
                "reinforcement learning",
                "agentic",
                "vision-grounded",
                "multimodal LLM",
                "MLLM",
                "visual critic",
                "vision-grounded feedback",
                "Forced Optimization",
                "behavioral collapse",
                "self-edit cycle",
                "vision-grounded front-end code generation"
            ],
            "githubStars": 231,
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-10-13T11:05:50.000Z",
        "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web\n  Coding",
        "summary": "While Large Language Models (LLMs) excel at algorithmic code generation, they\nstruggle with front-end development, where correctness is judged on rendered\npixels and interaction. We present ReLook, an agentic, vision-grounded\nreinforcement learning framework that empowers an agent to close a robust\ngenerate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.\nDuring training, the agent uses the MLLM-in-the-loop both as a visual\ncritic--scoring code with screenshots--and as a source of actionable,\nvision-grounded feedback; a strict zero-reward rule for invalid renders anchors\nrenderability and prevents reward hacking. To prevent behavioral collapse, we\nintroduce Forced Optimization, a strict acceptance rule that admits only\nimproving revisions, yielding monotonically better trajectories. At inference,\nwe decouple the critic and run a lightweight, critic-free self-edit cycle,\nkeeping latency comparable to base decoding while retaining most of the gains.\nAcross three widely used benchmarks, ReLook consistently outperforms strong\nbaselines in vision-grounded front-end code generation, highlighting the\nbenefits of agentic perception, visual rewards, and training-inference\ndecoupling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11498.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b74b906ab5d14ca7f289cd",
            "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
            "fullname": "Chenchen Zhang",
            "name": "xxzcc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.10023",
            "authors": [
                {
                    "_id": "68edb508de1fee572713a789",
                    "name": "Yinghui He",
                    "hidden": false
                },
                {
                    "_id": "68edb508de1fee572713a78a",
                    "name": "Abhishek Panigrahi",
                    "hidden": false
                },
                {
                    "_id": "68edb508de1fee572713a78b",
                    "name": "Yong Lin",
                    "hidden": false
                },
                {
                    "_id": "68edb508de1fee572713a78c",
                    "name": "Sanjeev Arora",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/652abf5360e706730596e8f4/RtrdHBbrD006CusTGFnQ1.jpeg"
            ],
            "publishedAt": "2025-10-11T05:02:36.000Z",
            "submittedOnDailyAt": "2025-10-14T01:04:54.468Z",
            "title": "Skill-Targeted Adaptive Training",
            "submittedOnDailyBy": {
                "_id": "652abf5360e706730596e8f4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/zRFJ4FjZJZGkz2wH8PPmU.jpeg",
                "isPro": false,
                "fullname": "Yinghui He",
                "user": "yinghuihe",
                "type": "user"
            },
            "summary": "Language models often show little to no improvement (i.e., \"saturation\") when\ntrained via vanilla supervised fine-tuning (SFT) on data similar to what they\nsaw in their training set (e.g., MATH). We introduce a new fine-tuning\nstrategy, STAT, to train such a student model by using the metacognition\nability of a stronger large language model (LLM) as the teacher. The teacher\nuses the task dataset to create a list of skills needed for the task, and then\nlabels each data point with its required skills (Didolkar et al., 2024). By\nmonitoring the student's answers, the teacher creates a Missing-Skill-Profile\nfor the student, tracking how often they failed to apply each skill in their\nresponses. We use this idea to build a modified training set in one of two\nways. In STAT-Sel, the teacher uses an existing set of training examples but\nadaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn,\nthe teacher synthesizes additional examples involving missing skills. Across\nextensive experiments on Llama and Qwen models, our methods yield improvements\nof up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore,\nSTAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25,\nAMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is\ncomplementary to RL via GRPO (Shao et al., 2024): after the model is improved\nusing STAT to address skill gaps, GRPO continues to add further gains. We\nconclude that skill-targeted adaptive training should broadly improve current\ntraining pipelines. Our code is available at:\nhttps://github.com/princeton-pli/STAT.",
            "upvotes": 9,
            "discussionId": "68edb508de1fee572713a78d",
            "ai_summary": "A new fine-tuning strategy, STAT, uses a teacher model's metacognition to identify and address skill gaps in a student model, leading to improved performance on both in-distribution and out-of-distribution benchmarks.",
            "ai_keywords": [
                "vanilla supervised fine-tuning",
                "SFT",
                "STAT",
                "metacognition",
                "large language model",
                "LLM",
                "task dataset",
                "skills",
                "Missing-Skill-Profile",
                "STAT-Sel",
                "STAT-Syn",
                "Llama",
                "Qwen",
                "MATH",
                "AIME24/25",
                "AMC23",
                "RL",
                "GRPO"
            ],
            "organization": {
                "_id": "6735d51c08a190b1caea1f29",
                "name": "PrincetonUniversity",
                "fullname": "Princeton University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
            }
        },
        "publishedAt": "2025-10-11T01:02:36.000Z",
        "title": "Skill-Targeted Adaptive Training",
        "summary": "Language models often show little to no improvement (i.e., \"saturation\") when\ntrained via vanilla supervised fine-tuning (SFT) on data similar to what they\nsaw in their training set (e.g., MATH). We introduce a new fine-tuning\nstrategy, STAT, to train such a student model by using the metacognition\nability of a stronger large language model (LLM) as the teacher. The teacher\nuses the task dataset to create a list of skills needed for the task, and then\nlabels each data point with its required skills (Didolkar et al., 2024). By\nmonitoring the student's answers, the teacher creates a Missing-Skill-Profile\nfor the student, tracking how often they failed to apply each skill in their\nresponses. We use this idea to build a modified training set in one of two\nways. In STAT-Sel, the teacher uses an existing set of training examples but\nadaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn,\nthe teacher synthesizes additional examples involving missing skills. Across\nextensive experiments on Llama and Qwen models, our methods yield improvements\nof up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore,\nSTAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25,\nAMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is\ncomplementary to RL via GRPO (Shao et al., 2024): after the model is improved\nusing STAT to address skill gaps, GRPO continues to add further gains. We\nconclude that skill-targeted adaptive training should broadly improve current\ntraining pipelines. Our code is available at:\nhttps://github.com/princeton-pli/STAT.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/652abf5360e706730596e8f4/RtrdHBbrD006CusTGFnQ1.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10023.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652abf5360e706730596e8f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/zRFJ4FjZJZGkz2wH8PPmU.jpeg",
            "fullname": "Yinghui He",
            "name": "yinghuihe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6735d51c08a190b1caea1f29",
            "name": "PrincetonUniversity",
            "fullname": "Princeton University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07841",
            "authors": [
                {
                    "_id": "68edaaf5de1fee572713a6e4",
                    "user": {
                        "_id": "63888d3fd68e37abd599f428",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
                        "isPro": true,
                        "fullname": "emre can",
                        "user": "emrecanacikgoz",
                        "type": "user"
                    },
                    "name": "Emre Can Acikgoz",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:31:08.327Z",
                    "hidden": false
                },
                {
                    "_id": "68edaaf5de1fee572713a6e5",
                    "name": "Cheng Qian",
                    "hidden": false
                },
                {
                    "_id": "68edaaf5de1fee572713a6e6",
                    "name": "Heng Ji",
                    "hidden": false
                },
                {
                    "_id": "68edaaf5de1fee572713a6e7",
                    "name": "Dilek Hakkani-Tür",
                    "hidden": false
                },
                {
                    "_id": "68edaaf5de1fee572713a6e8",
                    "name": "Gokhan Tur",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T06:37:35.000Z",
            "submittedOnDailyAt": "2025-10-14T00:16:59.509Z",
            "title": "Self-Improving LLM Agents at Test-Time",
            "submittedOnDailyBy": {
                "_id": "63888d3fd68e37abd599f428",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
                "isPro": true,
                "fullname": "emre can",
                "user": "emrecanacikgoz",
                "type": "user"
            },
            "summary": "One paradigm of language model (LM) fine-tuning relies on creating large\ntraining datasets, under the assumption that high quantity and diversity will\nenable models to generalize to novel tasks after post-training. In practice,\ngathering large sets of data is inefficient, and training on them is\nprohibitively expensive; worse, there is no guarantee that the resulting model\nwill handle complex scenarios or generalize better. Moreover, existing\ntechniques rarely assess whether a training sample provides novel information\nor is redundant with the knowledge already acquired by the model, resulting in\nunnecessary costs. In this work, we explore a new test-time self-improvement\nmethod to create more effective and generalizable agentic LMs on-the-fly. The\nproposed algorithm can be summarized in three steps: (i) first it identifies\nthe samples that model struggles with (self-awareness), (ii) then generates\nsimilar examples from detected uncertain samples (self-data augmentation), and\n(iii) uses these newly generated samples at test-time fine-tuning\n(self-improvement). We study two variants of this approach: Test-Time\nSelf-Improvement (TT-SI), where the same model generates additional training\nexamples from its own uncertain cases and then learns from them, and contrast\nthis approach with Test-Time Distillation (TT-D), where a stronger model\ngenerates similar examples for uncertain cases, enabling student to adapt using\ndistilled supervision. Empirical evaluations across different agent benchmarks\ndemonstrate that TT-SI improves the performance with +5.48% absolute accuracy\ngain on average across all benchmarks and surpasses other standard learning\nmethods, yet using 68x less training samples. Our findings highlight the\npromise of TT-SI, demonstrating the potential of self-improvement algorithms at\ntest-time as a new paradigm for building more capable agents toward\nself-evolution.",
            "upvotes": 9,
            "discussionId": "68edaaf5de1fee572713a6e9",
            "ai_summary": "A test-time self-improvement method enhances language models by generating additional training examples from uncertain cases, leading to better performance with fewer samples.",
            "ai_keywords": [
                "language model fine-tuning",
                "self-awareness",
                "self-data augmentation",
                "test-time fine-tuning",
                "Test-Time Self-Improvement (TT-SI)",
                "Test-Time Distillation (TT-D)",
                "self-improvement algorithms",
                "self-evolution"
            ]
        },
        "publishedAt": "2025-10-09T02:37:35.000Z",
        "title": "Self-Improving LLM Agents at Test-Time",
        "summary": "One paradigm of language model (LM) fine-tuning relies on creating large\ntraining datasets, under the assumption that high quantity and diversity will\nenable models to generalize to novel tasks after post-training. In practice,\ngathering large sets of data is inefficient, and training on them is\nprohibitively expensive; worse, there is no guarantee that the resulting model\nwill handle complex scenarios or generalize better. Moreover, existing\ntechniques rarely assess whether a training sample provides novel information\nor is redundant with the knowledge already acquired by the model, resulting in\nunnecessary costs. In this work, we explore a new test-time self-improvement\nmethod to create more effective and generalizable agentic LMs on-the-fly. The\nproposed algorithm can be summarized in three steps: (i) first it identifies\nthe samples that model struggles with (self-awareness), (ii) then generates\nsimilar examples from detected uncertain samples (self-data augmentation), and\n(iii) uses these newly generated samples at test-time fine-tuning\n(self-improvement). We study two variants of this approach: Test-Time\nSelf-Improvement (TT-SI), where the same model generates additional training\nexamples from its own uncertain cases and then learns from them, and contrast\nthis approach with Test-Time Distillation (TT-D), where a stronger model\ngenerates similar examples for uncertain cases, enabling student to adapt using\ndistilled supervision. Empirical evaluations across different agent benchmarks\ndemonstrate that TT-SI improves the performance with +5.48% absolute accuracy\ngain on average across all benchmarks and surpasses other standard learning\nmethods, yet using 68x less training samples. Our findings highlight the\npromise of TT-SI, demonstrating the potential of self-improvement algorithms at\ntest-time as a new paradigm for building more capable agents toward\nself-evolution.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07841.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63888d3fd68e37abd599f428",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
            "fullname": "emre can",
            "name": "emrecanacikgoz",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.10062",
            "authors": [
                {
                    "_id": "68ede08ede1fee572713a9af",
                    "name": "Adnan El Assadi",
                    "hidden": false
                },
                {
                    "_id": "68ede08ede1fee572713a9b0",
                    "user": {
                        "_id": "64cc0e80a257a3212c0c4b24",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64cc0e80a257a3212c0c4b24/wqs6WZN8-3OQthcnQXgN7.png",
                        "isPro": false,
                        "fullname": "Isaac Chung",
                        "user": "isaacchung",
                        "type": "user"
                    },
                    "name": "Isaac Chung",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:29:22.077Z",
                    "hidden": false
                },
                {
                    "_id": "68ede08ede1fee572713a9b1",
                    "user": {
                        "_id": "61af4544d691b3aadd1f62b6",
                        "avatarUrl": "/avatars/7a4067accdd1005f78c3c4adad3ee0a5.svg",
                        "isPro": false,
                        "fullname": "Solomatin Roman",
                        "user": "Samoed",
                        "type": "user"
                    },
                    "name": "Roman Solomatin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:29:19.917Z",
                    "hidden": false
                },
                {
                    "_id": "68ede08ede1fee572713a9b2",
                    "name": "Niklas Muennighoff",
                    "hidden": false
                },
                {
                    "_id": "68ede08ede1fee572713a9b3",
                    "user": {
                        "_id": "5ff5943752c26e9bc240bada",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5ff5943752c26e9bc240bada/Exyzf3C_gJ2KdsL4K5_cq.png",
                        "isPro": false,
                        "fullname": "Kenneth C. Enevoldsen",
                        "user": "KennethEnevoldsen",
                        "type": "user"
                    },
                    "name": "Kenneth Enevoldsen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T10:23:39.431Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-11T06:56:53.000Z",
            "submittedOnDailyAt": "2025-10-14T04:06:31.362Z",
            "title": "HUME: Measuring the Human-Model Performance Gap in Text Embedding Task",
            "submittedOnDailyBy": {
                "_id": "64cc0e80a257a3212c0c4b24",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64cc0e80a257a3212c0c4b24/wqs6WZN8-3OQthcnQXgN7.png",
                "isPro": false,
                "fullname": "Isaac Chung",
                "user": "isaacchung",
                "type": "user"
            },
            "summary": "Comparing human and model performance offers a valuable perspective for\nunderstanding the strengths and limitations of embedding models, highlighting\nwhere they succeed and where they fail to capture meaning and nuance. However,\nsuch comparisons are rarely made, as human performance on embedding tasks is\ndifficult to measure. To fill this gap, we introduce HUME: Human Evaluation\nFramework for Text Embeddings. While frameworks like MTEB provide broad model\nevaluation, they lack reliable estimates of human performance, limiting the\ninterpretability of model scores. We measure human performance across 16 MTEB\ndatasets spanning reranking, classification, clustering, and semantic textual\nsimilarity across linguistically diverse high- and low-resource languages.\nHumans achieve an average performance of 77.6% compared to 80.1% for the best\nembedding model, although variation is substantial: models reach near-ceiling\nperformance on some datasets while struggling on others, suggesting dataset\nissues and revealing shortcomings in low-resource languages. We provide human\nperformance baselines, insight into task difficulty patterns, and an extensible\nevaluation framework that enables a more meaningful interpretation of the model\nand informs the development of both models and benchmarks. Our code, dataset,\nand leaderboard are publicly available at\nhttps://github.com/embeddings-benchmark/mteb.",
            "upvotes": 7,
            "discussionId": "68ede08ede1fee572713a9b4",
            "ai_summary": "HUME provides human performance baselines for text embedding tasks, enhancing the interpretability of model evaluations and revealing dataset and language-specific challenges.",
            "ai_keywords": [
                "embedding models",
                "HUME",
                "Human Evaluation Framework for Text Embeddings",
                "MTEB",
                "reranking",
                "classification",
                "clustering",
                "semantic textual similarity",
                "low-resource languages",
                "dataset issues",
                "model development",
                "benchmarks"
            ],
            "organization": {
                "_id": "624bfda5459c48438cc39f80",
                "name": "mteb",
                "fullname": "Massive Text Embedding Benchmark",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664267264786-5f1eb362eec0ad2a071ad6e2.png"
            }
        },
        "publishedAt": "2025-10-11T02:56:53.000Z",
        "title": "HUME: Measuring the Human-Model Performance Gap in Text Embedding Task",
        "summary": "Comparing human and model performance offers a valuable perspective for\nunderstanding the strengths and limitations of embedding models, highlighting\nwhere they succeed and where they fail to capture meaning and nuance. However,\nsuch comparisons are rarely made, as human performance on embedding tasks is\ndifficult to measure. To fill this gap, we introduce HUME: Human Evaluation\nFramework for Text Embeddings. While frameworks like MTEB provide broad model\nevaluation, they lack reliable estimates of human performance, limiting the\ninterpretability of model scores. We measure human performance across 16 MTEB\ndatasets spanning reranking, classification, clustering, and semantic textual\nsimilarity across linguistically diverse high- and low-resource languages.\nHumans achieve an average performance of 77.6% compared to 80.1% for the best\nembedding model, although variation is substantial: models reach near-ceiling\nperformance on some datasets while struggling on others, suggesting dataset\nissues and revealing shortcomings in low-resource languages. We provide human\nperformance baselines, insight into task difficulty patterns, and an extensible\nevaluation framework that enables a more meaningful interpretation of the model\nand informs the development of both models and benchmarks. Our code, dataset,\nand leaderboard are publicly available at\nhttps://github.com/embeddings-benchmark/mteb.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10062.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64cc0e80a257a3212c0c4b24",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64cc0e80a257a3212c0c4b24/wqs6WZN8-3OQthcnQXgN7.png",
            "fullname": "Isaac Chung",
            "name": "isaacchung",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "organization": {
            "_id": "624bfda5459c48438cc39f80",
            "name": "mteb",
            "fullname": "Massive Text Embedding Benchmark",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664267264786-5f1eb362eec0ad2a071ad6e2.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.08026",
            "authors": [
                {
                    "_id": "68eda813de1fee572713a6ad",
                    "user": {
                        "_id": "65d7b983baa72790a1151923",
                        "avatarUrl": "/avatars/938531e84ca01a0c5a2a174057e3e9c5.svg",
                        "isPro": false,
                        "fullname": "Chen Huang",
                        "user": "Albus-Chen",
                        "type": "user"
                    },
                    "name": "Chen Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:31:17.491Z",
                    "hidden": false
                },
                {
                    "_id": "68eda813de1fee572713a6ae",
                    "name": "Wei Lu",
                    "hidden": false
                },
                {
                    "_id": "68eda813de1fee572713a6af",
                    "name": "Wenxuan Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T10:04:31.000Z",
            "submittedOnDailyAt": "2025-10-14T00:03:21.288Z",
            "title": "PEAR: Phase Entropy Aware Reward for Efficient Reasoning",
            "submittedOnDailyBy": {
                "_id": "65d7b983baa72790a1151923",
                "avatarUrl": "/avatars/938531e84ca01a0c5a2a174057e3e9c5.svg",
                "isPro": false,
                "fullname": "Chen Huang",
                "user": "Albus-Chen",
                "type": "user"
            },
            "summary": "Large Reasoning Models (LRMs) have achieved impressive performance on complex\nreasoning tasks by generating detailed chain-of-thought (CoT) explanations.\nHowever, these responses are often excessively long, containing redundant\nreasoning steps that inflate inference cost and reduce usability. Controlling\nthe length of generated reasoning without sacrificing accuracy remains an open\nchallenge. Through a systematic empirical analysis, we reveal a consistent\npositive correlation between model entropy and response length at different\nreasoning stages across diverse LRMs: the thinking phase exhibits higher\nentropy, reflecting exploratory behavior of longer responses, while the final\nanswer phase shows lower entropy, indicating a more deterministic solution.\nThis observation suggests that entropy at different reasoning stages can serve\nas a control knob for balancing conciseness and performance. Based on this\ninsight, this paper introduces Phase Entropy Aware Reward (PEAR), a reward\nmechanism that incorporating phase-dependent entropy into the reward design.\nInstead of treating all tokens uniformly, PEAR penalize excessive entropy\nduring the thinking phase and allowing moderate exploration at the final answer\nphase, which encourages models to generate concise reasoning traces that retain\nsufficient flexibility to solve the task correctly. This enables adaptive\ncontrol of response length without relying on explicit length targets or rigid\ntruncation rules. Extensive experiments across four benchmarks demonstrate that\nPEAR consistently reduces response length while sustaining competitive accuracy\nacross model scales. In addition, PEAR demonstrates strong out-of-distribution\n(OOD) robustness beyond the training distribution. Our code is available at:\nhttps://github.com/iNLP-Lab/PEAR.",
            "upvotes": 7,
            "discussionId": "68eda813de1fee572713a6b0",
            "githubRepo": "https://github.com/iNLP-Lab/PEAR",
            "ai_summary": "A reward mechanism called Phase Entropy Aware Reward (PEAR) controls the length of reasoning in large models by adjusting entropy at different stages, balancing conciseness and accuracy.",
            "ai_keywords": [
                "Large Reasoning Models",
                "chain-of-thought",
                "response length",
                "model entropy",
                "thinking phase",
                "final answer phase",
                "Phase Entropy Aware Reward",
                "PEAR",
                "out-of-distribution robustness"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "68b82daee976083ccd80824b",
                "name": "iNLP-Lab",
                "fullname": "iNLP Lab @ SUTD",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60dff6ae19a362a8c27862aa/3Ukf0b4f546tJM84zynTz.png"
            }
        },
        "publishedAt": "2025-10-09T06:04:31.000Z",
        "title": "PEAR: Phase Entropy Aware Reward for Efficient Reasoning",
        "summary": "Large Reasoning Models (LRMs) have achieved impressive performance on complex\nreasoning tasks by generating detailed chain-of-thought (CoT) explanations.\nHowever, these responses are often excessively long, containing redundant\nreasoning steps that inflate inference cost and reduce usability. Controlling\nthe length of generated reasoning without sacrificing accuracy remains an open\nchallenge. Through a systematic empirical analysis, we reveal a consistent\npositive correlation between model entropy and response length at different\nreasoning stages across diverse LRMs: the thinking phase exhibits higher\nentropy, reflecting exploratory behavior of longer responses, while the final\nanswer phase shows lower entropy, indicating a more deterministic solution.\nThis observation suggests that entropy at different reasoning stages can serve\nas a control knob for balancing conciseness and performance. Based on this\ninsight, this paper introduces Phase Entropy Aware Reward (PEAR), a reward\nmechanism that incorporating phase-dependent entropy into the reward design.\nInstead of treating all tokens uniformly, PEAR penalize excessive entropy\nduring the thinking phase and allowing moderate exploration at the final answer\nphase, which encourages models to generate concise reasoning traces that retain\nsufficient flexibility to solve the task correctly. This enables adaptive\ncontrol of response length without relying on explicit length targets or rigid\ntruncation rules. Extensive experiments across four benchmarks demonstrate that\nPEAR consistently reduces response length while sustaining competitive accuracy\nacross model scales. In addition, PEAR demonstrates strong out-of-distribution\n(OOD) robustness beyond the training distribution. Our code is available at:\nhttps://github.com/iNLP-Lab/PEAR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08026.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d7b983baa72790a1151923",
            "avatarUrl": "/avatars/938531e84ca01a0c5a2a174057e3e9c5.svg",
            "fullname": "Chen Huang",
            "name": "Albus-Chen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "68b82daee976083ccd80824b",
            "name": "iNLP-Lab",
            "fullname": "iNLP Lab @ SUTD",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60dff6ae19a362a8c27862aa/3Ukf0b4f546tJM84zynTz.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.10868",
            "authors": [
                {
                    "_id": "68edbe4bde1fee572713a86f",
                    "name": "Soroush Mehraban",
                    "hidden": false
                },
                {
                    "_id": "68edbe4bde1fee572713a870",
                    "name": "Andrea Iaboni",
                    "hidden": false
                },
                {
                    "_id": "68edbe4bde1fee572713a871",
                    "name": "Babak Taati",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6375965008eebfdd0a399891/0v5aDIZjucnu_Fhy2ADBt.mp4"
            ],
            "publishedAt": "2025-10-13T00:23:17.000Z",
            "submittedOnDailyAt": "2025-10-14T01:37:51.320Z",
            "title": "FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging\n  with Diffusion Decoding",
            "submittedOnDailyBy": {
                "_id": "6375965008eebfdd0a399891",
                "avatarUrl": "/avatars/946768f40a18793ced82f09a1de47952.svg",
                "isPro": false,
                "fullname": "Soroush Mehraban",
                "user": "SoroushMehraban",
                "type": "user"
            },
            "summary": "Recent transformer-based models for 3D Human Mesh Recovery (HMR) have\nachieved strong performance but often suffer from high computational cost and\ncomplexity due to deep transformer architectures and redundant tokens. In this\npaper, we introduce two HMR-specific merging strategies: Error-Constrained\nLayer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM\nselectively merges transformer layers that have minimal impact on the Mean Per\nJoint Position Error (MPJPE), while Mask-ToMe focuses on merging background\ntokens that contribute little to the final prediction. To further address the\npotential performance drop caused by merging, we propose a diffusion-based\ndecoder that incorporates temporal context and leverages pose priors learned\nfrom large-scale motion capture datasets. Experiments across multiple\nbenchmarks demonstrate that our method achieves up to 2.3x speed-up while\nslightly improving performance over the baseline.",
            "upvotes": 6,
            "discussionId": "68edbe4bde1fee572713a872",
            "projectPage": "https://soroushmehraban.github.io/FastHMR/",
            "ai_summary": "Two merging strategies and a diffusion-based decoder improve 3D Human Mesh Recovery by reducing computational cost and slightly enhancing performance.",
            "ai_keywords": [
                "transformer-based models",
                "3D Human Mesh Recovery",
                "Error-Constrained Layer Merging",
                "Mask-guided Token Merging",
                "Mean Per Joint Position Error",
                "diffusion-based decoder",
                "temporal context",
                "pose priors",
                "motion capture datasets"
            ],
            "organization": {
                "_id": "65b2b60eade89bc3fac3108a",
                "name": "vector-institute",
                "fullname": "Vector Institute",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f2d5d052ad88c930b7ba5e/_GqWuc2CjKyGsNtHDYKF7.png"
            }
        },
        "publishedAt": "2025-10-12T20:23:17.000Z",
        "title": "FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging\n  with Diffusion Decoding",
        "summary": "Recent transformer-based models for 3D Human Mesh Recovery (HMR) have\nachieved strong performance but often suffer from high computational cost and\ncomplexity due to deep transformer architectures and redundant tokens. In this\npaper, we introduce two HMR-specific merging strategies: Error-Constrained\nLayer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM\nselectively merges transformer layers that have minimal impact on the Mean Per\nJoint Position Error (MPJPE), while Mask-ToMe focuses on merging background\ntokens that contribute little to the final prediction. To further address the\npotential performance drop caused by merging, we propose a diffusion-based\ndecoder that incorporates temporal context and leverages pose priors learned\nfrom large-scale motion capture datasets. Experiments across multiple\nbenchmarks demonstrate that our method achieves up to 2.3x speed-up while\nslightly improving performance over the baseline.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6375965008eebfdd0a399891/0v5aDIZjucnu_Fhy2ADBt.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10868.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6375965008eebfdd0a399891",
            "avatarUrl": "/avatars/946768f40a18793ced82f09a1de47952.svg",
            "fullname": "Soroush Mehraban",
            "name": "SoroushMehraban",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "65b2b60eade89bc3fac3108a",
            "name": "vector-institute",
            "fullname": "Vector Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f2d5d052ad88c930b7ba5e/_GqWuc2CjKyGsNtHDYKF7.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.09212",
            "authors": [
                {
                    "_id": "68edc0efde1fee572713a88a",
                    "user": {
                        "_id": "62b705a6dd998a8b1e422936",
                        "avatarUrl": "/avatars/ab307b6548c8c627d640635f3316c5ad.svg",
                        "isPro": false,
                        "fullname": "wuyang li",
                        "user": "wymanCV",
                        "type": "user"
                    },
                    "name": "Wuyang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:30:28.090Z",
                    "hidden": false
                },
                {
                    "_id": "68edc0efde1fee572713a88b",
                    "name": "Wentao Pan",
                    "hidden": false
                },
                {
                    "_id": "68edc0efde1fee572713a88c",
                    "name": "Po-Chien Luan",
                    "hidden": false
                },
                {
                    "_id": "68edc0efde1fee572713a88d",
                    "name": "Yang Gao",
                    "hidden": false
                },
                {
                    "_id": "68edc0efde1fee572713a88e",
                    "name": "Alexandre Alahi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62b705a6dd998a8b1e422936/WslGDa7lBO0m5WRYblEX-.mp4"
            ],
            "publishedAt": "2025-10-10T09:45:46.000Z",
            "submittedOnDailyAt": "2025-10-14T06:46:44.107Z",
            "title": "Stable Video Infinity: Infinite-Length Video Generation with Error\n  Recycling",
            "submittedOnDailyBy": {
                "_id": "62b705a6dd998a8b1e422936",
                "avatarUrl": "/avatars/ab307b6548c8c627d640635f3316c5ad.svg",
                "isPro": false,
                "fullname": "wuyang li",
                "user": "wymanCV",
                "type": "user"
            },
            "summary": "We propose Stable Video Infinity (SVI) that is able to generate\ninfinite-length videos with high temporal consistency, plausible scene\ntransitions, and controllable streaming storylines. While existing long-video\nmethods attempt to mitigate accumulated errors via handcrafted anti-drifting\n(e.g., modified noise scheduler, frame anchoring), they remain limited to\nsingle-prompt extrapolation, producing homogeneous scenes with repetitive\nmotions. We identify that the fundamental challenge extends beyond error\naccumulation to a critical discrepancy between the training assumption (seeing\nclean data) and the test-time autoregressive reality (conditioning on\nself-generated, error-prone outputs). To bridge this hypothesis gap, SVI\nincorporates Error-Recycling Fine-Tuning, a new type of efficient training that\nrecycles the Diffusion Transformer (DiT)'s self-generated errors into\nsupervisory prompts, thereby encouraging DiT to actively identify and correct\nits own errors. This is achieved by injecting, collecting, and banking errors\nthrough closed-loop recycling, autoregressively learning from error-injected\nfeedback. Specifically, we (i) inject historical errors made by DiT to\nintervene on clean inputs, simulating error-accumulated trajectories in flow\nmatching; (ii) efficiently approximate predictions with one-step bidirectional\nintegration and calculate errors with residuals; (iii) dynamically bank errors\ninto replay memory across discretized timesteps, which are resampled for new\ninput. SVI is able to scale videos from seconds to infinite durations with no\nadditional inference cost, while remaining compatible with diverse conditions\n(e.g., audio, skeleton, and text streams). We evaluate SVI on three benchmarks,\nincluding consistent, creative, and conditional settings, thoroughly verifying\nits versatility and state-of-the-art role.",
            "upvotes": 6,
            "discussionId": "68edc0efde1fee572713a88f",
            "projectPage": "https://stable-video-infinity.github.io/homepage/",
            "githubRepo": "https://github.com/vita-epfl/Stable-Video-Infinity",
            "ai_summary": "Stable Video Infinity generates infinite-length videos with high temporal consistency and controllable storylines by using Error-Recycling Fine-Tuning on the Diffusion Transformer.",
            "ai_keywords": [
                "Error-Recycling Fine-Tuning",
                "Diffusion Transformer",
                "DiT",
                "flow matching",
                "one-step bidirectional integration",
                "replay memory",
                "temporal consistency",
                "scene transitions",
                "autoregressive learning",
                "error injection",
                "error accumulation"
            ],
            "githubStars": 113,
            "organization": {
                "_id": "67f26a5284486b4ad248e639",
                "name": "epfl-vita",
                "fullname": "EPFL VITA Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f269b836b9b062f8d39175/5Qtw3vqxaOz-7Q2ys9FVK.png"
            }
        },
        "publishedAt": "2025-10-10T05:45:46.000Z",
        "title": "Stable Video Infinity: Infinite-Length Video Generation with Error\n  Recycling",
        "summary": "We propose Stable Video Infinity (SVI) that is able to generate\ninfinite-length videos with high temporal consistency, plausible scene\ntransitions, and controllable streaming storylines. While existing long-video\nmethods attempt to mitigate accumulated errors via handcrafted anti-drifting\n(e.g., modified noise scheduler, frame anchoring), they remain limited to\nsingle-prompt extrapolation, producing homogeneous scenes with repetitive\nmotions. We identify that the fundamental challenge extends beyond error\naccumulation to a critical discrepancy between the training assumption (seeing\nclean data) and the test-time autoregressive reality (conditioning on\nself-generated, error-prone outputs). To bridge this hypothesis gap, SVI\nincorporates Error-Recycling Fine-Tuning, a new type of efficient training that\nrecycles the Diffusion Transformer (DiT)'s self-generated errors into\nsupervisory prompts, thereby encouraging DiT to actively identify and correct\nits own errors. This is achieved by injecting, collecting, and banking errors\nthrough closed-loop recycling, autoregressively learning from error-injected\nfeedback. Specifically, we (i) inject historical errors made by DiT to\nintervene on clean inputs, simulating error-accumulated trajectories in flow\nmatching; (ii) efficiently approximate predictions with one-step bidirectional\nintegration and calculate errors with residuals; (iii) dynamically bank errors\ninto replay memory across discretized timesteps, which are resampled for new\ninput. SVI is able to scale videos from seconds to infinite durations with no\nadditional inference cost, while remaining compatible with diverse conditions\n(e.g., audio, skeleton, and text streams). We evaluate SVI on three benchmarks,\nincluding consistent, creative, and conditional settings, thoroughly verifying\nits versatility and state-of-the-art role.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62b705a6dd998a8b1e422936/WslGDa7lBO0m5WRYblEX-.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09212.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62b705a6dd998a8b1e422936",
            "avatarUrl": "/avatars/ab307b6548c8c627d640635f3316c5ad.svg",
            "fullname": "wuyang li",
            "name": "wymanCV",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "67f26a5284486b4ad248e639",
            "name": "epfl-vita",
            "fullname": "EPFL VITA Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f269b836b9b062f8d39175/5Qtw3vqxaOz-7Q2ys9FVK.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.11512",
            "authors": [
                {
                    "_id": "68edc22cde1fee572713a89d",
                    "name": "Jianhao Yuan",
                    "hidden": false
                },
                {
                    "_id": "68edc22cde1fee572713a89e",
                    "name": "Fabio Pizzati",
                    "hidden": false
                },
                {
                    "_id": "68edc22cde1fee572713a89f",
                    "name": "Francesco Pinto",
                    "hidden": false
                },
                {
                    "_id": "68edc22cde1fee572713a8a0",
                    "name": "Lars Kunze",
                    "hidden": false
                },
                {
                    "_id": "68edc22cde1fee572713a8a1",
                    "name": "Ivan Laptev",
                    "hidden": false
                },
                {
                    "_id": "68edc22cde1fee572713a8a2",
                    "name": "Paul Newman",
                    "hidden": false
                },
                {
                    "_id": "68edc22cde1fee572713a8a3",
                    "name": "Philip Torr",
                    "hidden": false
                },
                {
                    "_id": "68edc22cde1fee572713a8a4",
                    "name": "Daniele De Martini",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T15:19:07.000Z",
            "submittedOnDailyAt": "2025-10-14T01:53:33.713Z",
            "title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion\n  Models via Likelihood Preference",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Intuitive physics understanding in video diffusion models plays an essential\nrole in building general-purpose physically plausible world simulators, yet\naccurately evaluating such capacity remains a challenging task due to the\ndifficulty in disentangling physics correctness from visual appearance in\ngeneration. To the end, we introduce LikePhys, a training-free method that\nevaluates intuitive physics in video diffusion models by distinguishing\nphysically valid and impossible videos using the denoising objective as an\nELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By\ntesting on our constructed benchmark of twelve scenarios spanning over four\nphysics domains, we show that our evaluation metric, Plausibility Preference\nError (PPE), demonstrates strong alignment with human preference, outperforming\nstate-of-the-art evaluator baselines. We then systematically benchmark\nintuitive physics understanding in current video diffusion models. Our study\nfurther analyses how model design and inference settings affect intuitive\nphysics understanding and highlights domain-specific capacity variations across\nphysical laws. Empirical results show that, despite current models struggling\nwith complex and chaotic dynamics, there is a clear trend of improvement in\nphysics understanding as model capacity and inference settings scale.",
            "upvotes": 5,
            "discussionId": "68edc22cde1fee572713a8a5",
            "ai_summary": "LikePhys evaluates intuitive physics in video diffusion models using a denoising objective-based metric, demonstrating better alignment with human preference than existing methods.",
            "ai_keywords": [
                "video diffusion models",
                "intuitive physics",
                "LikePhys",
                "denoising objective",
                "ELBO-based likelihood",
                "Plausibility Preference Error",
                "benchmark",
                "human preference",
                "model design",
                "inference settings",
                "domain-specific capacity variations"
            ]
        },
        "publishedAt": "2025-10-13T11:19:07.000Z",
        "title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion\n  Models via Likelihood Preference",
        "summary": "Intuitive physics understanding in video diffusion models plays an essential\nrole in building general-purpose physically plausible world simulators, yet\naccurately evaluating such capacity remains a challenging task due to the\ndifficulty in disentangling physics correctness from visual appearance in\ngeneration. To the end, we introduce LikePhys, a training-free method that\nevaluates intuitive physics in video diffusion models by distinguishing\nphysically valid and impossible videos using the denoising objective as an\nELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By\ntesting on our constructed benchmark of twelve scenarios spanning over four\nphysics domains, we show that our evaluation metric, Plausibility Preference\nError (PPE), demonstrates strong alignment with human preference, outperforming\nstate-of-the-art evaluator baselines. We then systematically benchmark\nintuitive physics understanding in current video diffusion models. Our study\nfurther analyses how model design and inference settings affect intuitive\nphysics understanding and highlights domain-specific capacity variations across\nphysical laws. Empirical results show that, despite current models struggling\nwith complex and chaotic dynamics, there is a clear trend of improvement in\nphysics understanding as model capacity and inference settings scale.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11512.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 126
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.10047",
            "authors": [
                {
                    "_id": "68ee5ac69b77b5223f66624f",
                    "name": "Ruohao Li",
                    "hidden": false
                },
                {
                    "_id": "68ee5ac69b77b5223f666250",
                    "user": {
                        "_id": "646474126d621be0dd08d6d4",
                        "avatarUrl": "/avatars/5041573e603225e9de3b689efd1dd499.svg",
                        "isPro": false,
                        "fullname": "Liu Hongjun",
                        "user": "Jan150000",
                        "type": "user"
                    },
                    "name": "Hongjun Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T14:23:51.257Z",
                    "hidden": false
                },
                {
                    "_id": "68ee5ac69b77b5223f666251",
                    "name": "Leyi Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ee5ac69b77b5223f666252",
                    "name": "Zisu Li",
                    "hidden": false
                },
                {
                    "_id": "68ee5ac69b77b5223f666253",
                    "name": "Jiawei Li",
                    "hidden": false
                },
                {
                    "_id": "68ee5ac69b77b5223f666254",
                    "name": "Jiajun Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ee5ac69b77b5223f666255",
                    "name": "Linning Xu",
                    "hidden": false
                },
                {
                    "_id": "68ee5ac69b77b5223f666256",
                    "name": "Chen Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ee5ac69b77b5223f666257",
                    "name": "Mingming Fan",
                    "hidden": false
                },
                {
                    "_id": "68ee5ac69b77b5223f666258",
                    "name": "Chen Liang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-11T06:28:22.000Z",
            "submittedOnDailyAt": "2025-10-14T12:47:10.722Z",
            "title": "SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "646474126d621be0dd08d6d4",
                "avatarUrl": "/avatars/5041573e603225e9de3b689efd1dd499.svg",
                "isPro": false,
                "fullname": "Liu Hongjun",
                "user": "Jan150000",
                "type": "user"
            },
            "summary": "Large language model (LLM) agents have shown remarkable reasoning abilities.\nHowever, existing multi-agent frameworks often rely on fixed roles or\ncentralized control, limiting scalability and adaptability in long-horizon\nreasoning. We introduce SwarmSys, a closed-loop framework for distributed\nmulti-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys\nemerges through iterative interactions among three specialized roles,\nExplorers, Workers, and Validators, that continuously cycle through\nexploration, exploitation, and validation. To enable scalable and adaptive\ncollaboration, we integrate adaptive agent and event profiles, embedding-based\nprobabilistic matching, and a pheromone-inspired reinforcement mechanism,\nsupporting dynamic task allocation and self-organizing convergence without\nglobal supervision. Across symbolic reasoning, research synthesis, and\nscientific programming tasks, SwarmSys consistently outperforms baselines,\nimproving both accuracy and reasoning stability. These findings highlight\nswarm-inspired coordination as a promising paradigm for scalable, robust, and\nadaptive multi-agent reasoning, suggesting that coordination scaling may rival\nmodel scaling in advancing LLM intelligence.",
            "upvotes": 5,
            "discussionId": "68ee5ac79b77b5223f666259",
            "ai_summary": "SwarmSys, a distributed multi-agent framework inspired by swarm intelligence, enhances scalability and adaptability in long-horizon reasoning through specialized roles and self-organizing mechanisms.",
            "ai_keywords": [
                "SwarmSys",
                "distributed multi-agent reasoning",
                "swarm intelligence",
                "Explorers",
                "Workers",
                "Validators",
                "adaptive agent",
                "event profiles",
                "embedding-based probabilistic matching",
                "pheromone-inspired reinforcement mechanism",
                "symbolic reasoning",
                "research synthesis",
                "scientific programming",
                "self-organizing convergence"
            ]
        },
        "publishedAt": "2025-10-11T02:28:22.000Z",
        "title": "SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive\n  Reasoning",
        "summary": "Large language model (LLM) agents have shown remarkable reasoning abilities.\nHowever, existing multi-agent frameworks often rely on fixed roles or\ncentralized control, limiting scalability and adaptability in long-horizon\nreasoning. We introduce SwarmSys, a closed-loop framework for distributed\nmulti-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys\nemerges through iterative interactions among three specialized roles,\nExplorers, Workers, and Validators, that continuously cycle through\nexploration, exploitation, and validation. To enable scalable and adaptive\ncollaboration, we integrate adaptive agent and event profiles, embedding-based\nprobabilistic matching, and a pheromone-inspired reinforcement mechanism,\nsupporting dynamic task allocation and self-organizing convergence without\nglobal supervision. Across symbolic reasoning, research synthesis, and\nscientific programming tasks, SwarmSys consistently outperforms baselines,\nimproving both accuracy and reasoning stability. These findings highlight\nswarm-inspired coordination as a promising paradigm for scalable, robust, and\nadaptive multi-agent reasoning, suggesting that coordination scaling may rival\nmodel scaling in advancing LLM intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10047.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646474126d621be0dd08d6d4",
            "avatarUrl": "/avatars/5041573e603225e9de3b689efd1dd499.svg",
            "fullname": "Liu Hongjun",
            "name": "Jan150000",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.09905",
            "authors": [
                {
                    "_id": "68edba56de1fee572713a804",
                    "name": "Xi Fang",
                    "hidden": false
                },
                {
                    "_id": "68edba56de1fee572713a805",
                    "name": "Weijie Xu",
                    "hidden": false
                },
                {
                    "_id": "68edba56de1fee572713a806",
                    "name": "Yuchong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68edba56de1fee572713a807",
                    "name": "Stephanie Eckman",
                    "hidden": false
                },
                {
                    "_id": "68edba56de1fee572713a808",
                    "name": "Scott Nickleach",
                    "hidden": false
                },
                {
                    "_id": "68edba56de1fee572713a809",
                    "name": "Chandan K. Reddy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T22:39:37.000Z",
            "submittedOnDailyAt": "2025-10-14T01:24:10.792Z",
            "title": "The Personalization Trap: How User Memory Alters Emotional Reasoning in\n  LLMs",
            "submittedOnDailyBy": {
                "_id": "63e3f57754f51ea342ce26be",
                "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
                "isPro": false,
                "fullname": "Weijie Xu",
                "user": "xwjzds",
                "type": "user"
            },
            "summary": "When an AI assistant remembers that Sarah is a single mother working two\njobs, does it interpret her stress differently than if she were a wealthy\nexecutive? As personalized AI systems increasingly incorporate long-term user\nmemory, understanding how this memory shapes emotional reasoning is critical.\nWe investigate how user memory affects emotional intelligence in large language\nmodels (LLMs) by evaluating 15 models on human validated emotional intelligence\ntests. We find that identical scenarios paired with different user profiles\nproduce systematically divergent emotional interpretations. Across validated\nuser independent emotional scenarios and diverse user profiles, systematic\nbiases emerged in several high-performing LLMs where advantaged profiles\nreceived more accurate emotional interpretations. Moreover, LLMs demonstrate\nsignificant disparities across demographic factors in emotion understanding and\nsupportive recommendations tasks, indicating that personalization mechanisms\ncan embed social hierarchies into models emotional reasoning. These results\nhighlight a key challenge for memory enhanced AI: systems designed for\npersonalization may inadvertently reinforce social inequalities.",
            "upvotes": 5,
            "discussionId": "68edba56de1fee572713a80a",
            "ai_summary": "LLMs exhibit systematic biases in emotional interpretation and support based on user profiles, potentially reinforcing social hierarchies.",
            "ai_keywords": [
                "large language models",
                "emotional intelligence",
                "user memory",
                "emotional scenarios",
                "user profiles",
                "systematic biases",
                "demographic factors",
                "emotion understanding",
                "supportive recommendations",
                "social hierarchies"
            ],
            "organization": {
                "_id": "5ffdfbadbba2ae614d771970",
                "name": "amazon",
                "fullname": "Amazon",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
            }
        },
        "publishedAt": "2025-10-10T18:39:37.000Z",
        "title": "The Personalization Trap: How User Memory Alters Emotional Reasoning in\n  LLMs",
        "summary": "When an AI assistant remembers that Sarah is a single mother working two\njobs, does it interpret her stress differently than if she were a wealthy\nexecutive? As personalized AI systems increasingly incorporate long-term user\nmemory, understanding how this memory shapes emotional reasoning is critical.\nWe investigate how user memory affects emotional intelligence in large language\nmodels (LLMs) by evaluating 15 models on human validated emotional intelligence\ntests. We find that identical scenarios paired with different user profiles\nproduce systematically divergent emotional interpretations. Across validated\nuser independent emotional scenarios and diverse user profiles, systematic\nbiases emerged in several high-performing LLMs where advantaged profiles\nreceived more accurate emotional interpretations. Moreover, LLMs demonstrate\nsignificant disparities across demographic factors in emotion understanding and\nsupportive recommendations tasks, indicating that personalization mechanisms\ncan embed social hierarchies into models emotional reasoning. These results\nhighlight a key challenge for memory enhanced AI: systems designed for\npersonalization may inadvertently reinforce social inequalities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09905.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "63e3f57754f51ea342ce26be",
            "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
            "fullname": "Weijie Xu",
            "name": "xwjzds",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 16
        },
        "organization": {
            "_id": "5ffdfbadbba2ae614d771970",
            "name": "amazon",
            "fullname": "Amazon",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11650",
            "authors": [
                {
                    "_id": "68edc0bbde1fee572713a884",
                    "name": "Yuxuan Xue",
                    "hidden": false
                },
                {
                    "_id": "68edc0bbde1fee572713a885",
                    "name": "Xianghui Xie",
                    "hidden": false
                },
                {
                    "_id": "68edc0bbde1fee572713a886",
                    "name": "Margaret Kostyrko",
                    "hidden": false
                },
                {
                    "_id": "68edc0bbde1fee572713a887",
                    "name": "Gerard Pons-Moll",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T17:29:55.000Z",
            "submittedOnDailyAt": "2025-10-14T01:47:25.488Z",
            "title": "InfiniHuman: Infinite 3D Human Creation with Precise Control",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Generating realistic and controllable 3D human avatars is a long-standing\nchallenge, particularly when covering broad attribute ranges such as ethnicity,\nage, clothing styles, and detailed body shapes. Capturing and annotating\nlarge-scale human datasets for training generative models is prohibitively\nexpensive and limited in scale and diversity. The central question we address\nin this paper is: Can existing foundation models be distilled to generate\ntheoretically unbounded, richly annotated 3D human data? We introduce\nInfiniHuman, a framework that synergistically distills these models to produce\nrichly annotated human data at minimal cost and with theoretically unlimited\nscalability. We propose InfiniHumanData, a fully automatic pipeline that\nleverages vision-language and image generation models to create a large-scale\nmulti-modal dataset. User study shows our automatically generated identities\nare undistinguishable from scan renderings. InfiniHumanData contains 111K\nidentities spanning unprecedented diversity. Each identity is annotated with\nmulti-granularity text descriptions, multi-view RGB images, detailed clothing\nimages, and SMPL body-shape parameters. Building on this dataset, we propose\nInfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body\nshape, and clothing assets. InfiniHumanGen enables fast, realistic, and\nprecisely controllable avatar generation. Extensive experiments demonstrate\nsignificant improvements over state-of-the-art methods in visual quality,\ngeneration speed, and controllability. Our approach enables high-quality avatar\ngeneration with fine-grained control at effectively unbounded scale through a\npractical and affordable solution. We will publicly release the automatic data\ngeneration pipeline, the comprehensive InfiniHumanData dataset, and the\nInfiniHumanGen models at https://yuxuan-xue.com/infini-human.",
            "upvotes": 4,
            "discussionId": "68edc0bcde1fee572713a888",
            "projectPage": "https://yuxuan-xue.com/infini-human",
            "ai_summary": "InfiniHuman framework distills existing models to generate large-scale, richly annotated 3D human data using a diffusion-based generative pipeline, achieving high visual quality, speed, and controllability.",
            "ai_keywords": [
                "vision-language models",
                "image generation models",
                "diffusion-based generative pipeline",
                "SMPL body-shape parameters"
            ]
        },
        "publishedAt": "2025-10-13T13:29:55.000Z",
        "title": "InfiniHuman: Infinite 3D Human Creation with Precise Control",
        "summary": "Generating realistic and controllable 3D human avatars is a long-standing\nchallenge, particularly when covering broad attribute ranges such as ethnicity,\nage, clothing styles, and detailed body shapes. Capturing and annotating\nlarge-scale human datasets for training generative models is prohibitively\nexpensive and limited in scale and diversity. The central question we address\nin this paper is: Can existing foundation models be distilled to generate\ntheoretically unbounded, richly annotated 3D human data? We introduce\nInfiniHuman, a framework that synergistically distills these models to produce\nrichly annotated human data at minimal cost and with theoretically unlimited\nscalability. We propose InfiniHumanData, a fully automatic pipeline that\nleverages vision-language and image generation models to create a large-scale\nmulti-modal dataset. User study shows our automatically generated identities\nare undistinguishable from scan renderings. InfiniHumanData contains 111K\nidentities spanning unprecedented diversity. Each identity is annotated with\nmulti-granularity text descriptions, multi-view RGB images, detailed clothing\nimages, and SMPL body-shape parameters. Building on this dataset, we propose\nInfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body\nshape, and clothing assets. InfiniHumanGen enables fast, realistic, and\nprecisely controllable avatar generation. Extensive experiments demonstrate\nsignificant improvements over state-of-the-art methods in visual quality,\ngeneration speed, and controllability. Our approach enables high-quality avatar\ngeneration with fine-grained control at effectively unbounded scale through a\npractical and affordable solution. We will publicly release the automatic data\ngeneration pipeline, the comprehensive InfiniHumanData dataset, and the\nInfiniHumanGen models at https://yuxuan-xue.com/infini-human.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11650.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 126
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07731",
            "authors": [
                {
                    "_id": "68ee9f1b510125ca000ccf52",
                    "name": "Ruiling Xu",
                    "hidden": false
                },
                {
                    "_id": "68ee9f1b510125ca000ccf53",
                    "name": "Yifan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ee9f1b510125ca000ccf54",
                    "name": "Qingyun Wang",
                    "hidden": false
                },
                {
                    "_id": "68ee9f1b510125ca000ccf55",
                    "name": "Carl Edwards",
                    "hidden": false
                },
                {
                    "_id": "68ee9f1b510125ca000ccf56",
                    "name": "Heng Ji",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T03:13:31.000Z",
            "submittedOnDailyAt": "2025-10-14T17:46:14.286Z",
            "title": "oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism\n  Elucidation and Reasoning",
            "submittedOnDailyBy": {
                "_id": "628ba530ac304a69264afb75",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653319185560-628ba530ac304a69264afb75.jpeg",
                "isPro": false,
                "fullname": "Qingyun Wang",
                "user": "eaglew",
                "type": "user"
            },
            "summary": "Organic reaction mechanisms are the stepwise elementary reactions by which\nreactants form intermediates and products, and are fundamental to understanding\nchemical reactivity and designing new molecules and reactions. Although large\nlanguage models (LLMs) have shown promise in understanding chemical tasks such\nas synthesis design, it is unclear to what extent this reflects genuine\nchemical reasoning capabilities, i.e., the ability to generate valid\nintermediates, maintain chemical consistency, and follow logically coherent\nmulti-step pathways. We address this by introducing oMeBench, the first\nlarge-scale, expert-curated benchmark for organic mechanism reasoning in\norganic chemistry. It comprises over 10,000 annotated mechanistic steps with\nintermediates, type labels, and difficulty ratings. Furthermore, to evaluate\nLLM capability more precisely and enable fine-grained scoring, we propose oMeS,\na dynamic evaluation framework that combines step-level logic and chemical\nsimilarity. We analyze the performance of state-of-the-art LLMs, and our\nresults show that although current models display promising chemical intuition,\nthey struggle with correct and consistent multi-step reasoning. Notably, we\nfind that using prompting strategy and fine-tuning a specialist model on our\nproposed dataset increases performance by 50% over the leading closed-source\nmodel. We hope that oMeBench will serve as a rigorous foundation for advancing\nAI systems toward genuine chemical reasoning.",
            "upvotes": 4,
            "discussionId": "68ee9f1c510125ca000ccf57",
            "githubRepo": "https://github.com/skylarkie/oMeBench",
            "ai_summary": "A benchmark and evaluation framework for assessing the chemical reasoning capabilities of large language models in organic reaction mechanisms.",
            "ai_keywords": [
                "large language models",
                "organic mechanism reasoning",
                "benchmark",
                "annotated mechanistic steps",
                "intermediates",
                "type labels",
                "difficulty ratings",
                "dynamic evaluation framework",
                "step-level logic",
                "chemical similarity",
                "prompting strategy",
                "fine-tuning"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-10-08T23:13:31.000Z",
        "title": "oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism\n  Elucidation and Reasoning",
        "summary": "Organic reaction mechanisms are the stepwise elementary reactions by which\nreactants form intermediates and products, and are fundamental to understanding\nchemical reactivity and designing new molecules and reactions. Although large\nlanguage models (LLMs) have shown promise in understanding chemical tasks such\nas synthesis design, it is unclear to what extent this reflects genuine\nchemical reasoning capabilities, i.e., the ability to generate valid\nintermediates, maintain chemical consistency, and follow logically coherent\nmulti-step pathways. We address this by introducing oMeBench, the first\nlarge-scale, expert-curated benchmark for organic mechanism reasoning in\norganic chemistry. It comprises over 10,000 annotated mechanistic steps with\nintermediates, type labels, and difficulty ratings. Furthermore, to evaluate\nLLM capability more precisely and enable fine-grained scoring, we propose oMeS,\na dynamic evaluation framework that combines step-level logic and chemical\nsimilarity. We analyze the performance of state-of-the-art LLMs, and our\nresults show that although current models display promising chemical intuition,\nthey struggle with correct and consistent multi-step reasoning. Notably, we\nfind that using prompting strategy and fine-tuning a specialist model on our\nproposed dataset increases performance by 50% over the leading closed-source\nmodel. We hope that oMeBench will serve as a rigorous foundation for advancing\nAI systems toward genuine chemical reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07731.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "628ba530ac304a69264afb75",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653319185560-628ba530ac304a69264afb75.jpeg",
            "fullname": "Qingyun Wang",
            "name": "eaglew",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07624",
            "authors": [
                {
                    "_id": "68ecc73acd07fb414898cce2",
                    "user": {
                        "_id": "621d59ebd3df05d67132e8d9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d59ebd3df05d67132e8d9/0gPfPTRKKnz5kq0InTqm5.jpeg",
                        "isPro": false,
                        "fullname": "Abdelhakim Benechehab",
                        "user": "abenechehab",
                        "type": "user"
                    },
                    "name": "Abdelhakim Benechehab",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T10:23:51.762Z",
                    "hidden": false
                },
                {
                    "_id": "68ecc73acd07fb414898cce3",
                    "name": "Gabriel Singer",
                    "hidden": false
                },
                {
                    "_id": "68ecc73acd07fb414898cce4",
                    "user": {
                        "_id": "650569cd0761d34d28c47eb4",
                        "avatarUrl": "/avatars/669b77101c3dcb9d2cc04bfc8170af36.svg",
                        "isPro": false,
                        "fullname": "Corentin Léger",
                        "user": "corentinlger",
                        "type": "user"
                    },
                    "name": "Corentin Léger",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T10:23:48.639Z",
                    "hidden": false
                },
                {
                    "_id": "68ecc73acd07fb414898cce5",
                    "name": "Youssef Attia El Hili",
                    "hidden": false
                },
                {
                    "_id": "68ecc73acd07fb414898cce6",
                    "user": {
                        "_id": "65e98cd8e19214e9d151f29e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e98cd8e19214e9d151f29e/XjQzoVgKVzv8AZBWFQnHz.jpeg",
                        "isPro": false,
                        "fullname": "Giuseppe Paolo",
                        "user": "GPaolo",
                        "type": "user"
                    },
                    "name": "Giuseppe Paolo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T14:24:22.207Z",
                    "hidden": false
                },
                {
                    "_id": "68ecc73acd07fb414898cce7",
                    "name": "Albert Thomas",
                    "hidden": false
                },
                {
                    "_id": "68ecc73acd07fb414898cce8",
                    "name": "Maurizio Filippone",
                    "hidden": false
                },
                {
                    "_id": "68ecc73acd07fb414898cce9",
                    "name": "Balázs Kégl",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/YnaPFD5m0pk2Bt0M9jdmu.png",
                "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/f_P5Nt5sWak8CcaT-0b_2.png",
                "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/lEDz7jPl376eJlZyFd_Lt.png",
                "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/Kwxy-rrkMKJcIEoe9yDpl.png",
                "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/rK3bcxjoXvf0n9Wpj2G2g.png",
                "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/PgturZadu8ayT_fsJdjIP.png"
            ],
            "publishedAt": "2025-10-08T23:45:37.000Z",
            "submittedOnDailyAt": "2025-10-14T07:57:41.222Z",
            "title": "From Data to Rewards: a Bilevel Optimization Perspective on Maximum\n  Likelihood Estimation",
            "submittedOnDailyBy": {
                "_id": "621d59ebd3df05d67132e8d9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d59ebd3df05d67132e8d9/0gPfPTRKKnz5kq0InTqm5.jpeg",
                "isPro": false,
                "fullname": "Abdelhakim Benechehab",
                "user": "abenechehab",
                "type": "user"
            },
            "summary": "Generative models form the backbone of modern machine learning, underpinning\nstate-of-the-art systems in text, vision, and multimodal applications. While\nMaximum Likelihood Estimation has traditionally served as the dominant training\nparadigm, recent work have highlighted its limitations, particularly in\ngeneralization and susceptibility to catastrophic forgetting compared to\nReinforcement Learning techniques, such as Policy Gradient methods. However,\nthese approaches depend on explicit reward signals, which are often unavailable\nin practice, leaving open the fundamental problem of how to align generative\nmodels when only high-quality datasets are accessible. In this work, we address\nthis challenge via a Bilevel Optimization framework, where the reward function\nis treated as the optimization variable of an outer-level problem, while a\npolicy gradient objective defines the inner-level. We then conduct a\ntheoretical analysis of this optimization problem in a tractable setting and\nextract insights that, as we demonstrate, generalize to applications such as\ntabular classification and model-based reinforcement learning. We release the\ncode at https://github.com/abenechehab/nll_to_po .",
            "upvotes": 4,
            "discussionId": "68ecc73bcd07fb414898ccea",
            "githubRepo": "https://github.com/abenechehab/nll_to_po",
            "ai_summary": "A bilevel optimization framework is used to align generative models with high-quality datasets in the absence of explicit reward signals, with applications in classification and model-based reinforcement learning.",
            "ai_keywords": [
                "generative models",
                "Maximum Likelihood Estimation",
                "Reinforcement Learning",
                "Policy Gradient methods",
                "Bilevel Optimization",
                "reward function",
                "policy gradient objective",
                "tabular classification",
                "model-based reinforcement learning"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-10-08T19:45:37.000Z",
        "title": "From Data to Rewards: a Bilevel Optimization Perspective on Maximum\n  Likelihood Estimation",
        "summary": "Generative models form the backbone of modern machine learning, underpinning\nstate-of-the-art systems in text, vision, and multimodal applications. While\nMaximum Likelihood Estimation has traditionally served as the dominant training\nparadigm, recent work have highlighted its limitations, particularly in\ngeneralization and susceptibility to catastrophic forgetting compared to\nReinforcement Learning techniques, such as Policy Gradient methods. However,\nthese approaches depend on explicit reward signals, which are often unavailable\nin practice, leaving open the fundamental problem of how to align generative\nmodels when only high-quality datasets are accessible. In this work, we address\nthis challenge via a Bilevel Optimization framework, where the reward function\nis treated as the optimization variable of an outer-level problem, while a\npolicy gradient objective defines the inner-level. We then conduct a\ntheoretical analysis of this optimization problem in a tractable setting and\nextract insights that, as we demonstrate, generalize to applications such as\ntabular classification and model-based reinforcement learning. We release the\ncode at https://github.com/abenechehab/nll_to_po .",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/YnaPFD5m0pk2Bt0M9jdmu.png",
            "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/f_P5Nt5sWak8CcaT-0b_2.png",
            "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/lEDz7jPl376eJlZyFd_Lt.png",
            "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/Kwxy-rrkMKJcIEoe9yDpl.png",
            "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/rK3bcxjoXvf0n9Wpj2G2g.png",
            "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/PgturZadu8ayT_fsJdjIP.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07624.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "621d59ebd3df05d67132e8d9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d59ebd3df05d67132e8d9/0gPfPTRKKnz5kq0InTqm5.jpeg",
            "fullname": "Abdelhakim Benechehab",
            "name": "abenechehab",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.04201",
            "authors": [
                {
                    "_id": "68ecc496cd07fb414898ccad",
                    "name": "Moo Hyun Son",
                    "hidden": false
                },
                {
                    "_id": "68ecc496cd07fb414898ccae",
                    "name": "Jintaek Oh",
                    "hidden": false
                },
                {
                    "_id": "68ecc496cd07fb414898ccaf",
                    "name": "Sun Bin Mun",
                    "hidden": false
                },
                {
                    "_id": "68ecc496cd07fb414898ccb0",
                    "name": "Jaechul Roh",
                    "hidden": false
                },
                {
                    "_id": "68ecc496cd07fb414898ccb1",
                    "name": "Sehyun Choi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6504ea59f3060ea8409a1f9e/UYNXsE2Xv46_SMjS2hoQ9.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6504ea59f3060ea8409a1f9e/dXHnwrCTk5oxEC5ux7TOF.png"
            ],
            "publishedAt": "2025-10-05T13:35:30.000Z",
            "submittedOnDailyAt": "2025-10-14T12:37:15.798Z",
            "title": "World-To-Image: Grounding Text-to-Image Generation with Agent-Driven\n  World Knowledge",
            "submittedOnDailyBy": {
                "_id": "6504ea59f3060ea8409a1f9e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6504ea59f3060ea8409a1f9e/QEluRfqLHLT7OpdZNRy4X.png",
                "isPro": false,
                "fullname": "Jaechul Roh",
                "user": "jroh",
                "type": "user"
            },
            "summary": "While text-to-image (T2I) models can synthesize high-quality images, their\nperformance degrades significantly when prompted with novel or\nout-of-distribution (OOD) entities due to inherent knowledge cutoffs. We\nintroduce World-To-Image, a novel framework that bridges this gap by empowering\nT2I generation with agent-driven world knowledge. We design an agent that\ndynamically searches the web to retrieve images for concepts unknown to the\nbase model. This information is then used to perform multimodal prompt\noptimization, steering powerful generative backbones toward an accurate\nsynthesis. Critically, our evaluation goes beyond traditional metrics,\nutilizing modern assessments like LLMGrader and ImageReward to measure true\nsemantic fidelity. Our experiments show that World-To-Image substantially\noutperforms state-of-the-art methods in both semantic alignment and visual\naesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated\nNICE benchmark. Our framework achieves these results with high efficiency in\nless than three iterations, paving the way for T2I systems that can better\nreflect the ever-changing real world. Our demo code is available\nherehttps://github.com/mhson-kyle/World-To-Image.",
            "upvotes": 4,
            "discussionId": "68ecc496cd07fb414898ccb2",
            "ai_summary": "World-To-Image enhances text-to-image generation by integrating web-based knowledge retrieval and multimodal prompt optimization, improving semantic accuracy and visual quality.",
            "ai_keywords": [
                "text-to-image",
                "out-of-distribution",
                "agent-driven",
                "world knowledge",
                "multimodal prompt optimization",
                "generative backbones",
                "LLMGrader",
                "ImageReward",
                "semantic fidelity",
                "NICE benchmark"
            ]
        },
        "publishedAt": "2025-10-05T09:35:30.000Z",
        "title": "World-To-Image: Grounding Text-to-Image Generation with Agent-Driven\n  World Knowledge",
        "summary": "While text-to-image (T2I) models can synthesize high-quality images, their\nperformance degrades significantly when prompted with novel or\nout-of-distribution (OOD) entities due to inherent knowledge cutoffs. We\nintroduce World-To-Image, a novel framework that bridges this gap by empowering\nT2I generation with agent-driven world knowledge. We design an agent that\ndynamically searches the web to retrieve images for concepts unknown to the\nbase model. This information is then used to perform multimodal prompt\noptimization, steering powerful generative backbones toward an accurate\nsynthesis. Critically, our evaluation goes beyond traditional metrics,\nutilizing modern assessments like LLMGrader and ImageReward to measure true\nsemantic fidelity. Our experiments show that World-To-Image substantially\noutperforms state-of-the-art methods in both semantic alignment and visual\naesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated\nNICE benchmark. Our framework achieves these results with high efficiency in\nless than three iterations, paving the way for T2I systems that can better\nreflect the ever-changing real world. Our demo code is available\nherehttps://github.com/mhson-kyle/World-To-Image.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6504ea59f3060ea8409a1f9e/UYNXsE2Xv46_SMjS2hoQ9.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6504ea59f3060ea8409a1f9e/dXHnwrCTk5oxEC5ux7TOF.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04201.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6504ea59f3060ea8409a1f9e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6504ea59f3060ea8409a1f9e/QEluRfqLHLT7OpdZNRy4X.png",
            "fullname": "Jaechul Roh",
            "name": "jroh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.10681",
            "authors": [
                {
                    "_id": "68edc950de1fee572713a915",
                    "name": "Zichun Yu",
                    "hidden": false
                },
                {
                    "_id": "68edc950de1fee572713a916",
                    "name": "Chenyan Xiong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-12T16:08:38.000Z",
            "submittedOnDailyAt": "2025-10-14T02:24:39.497Z",
            "title": "RePro: Training Language Models to Faithfully Recycle the Web for\n  Pretraining",
            "submittedOnDailyBy": {
                "_id": "649a943f575e60d3a87cfcdf",
                "avatarUrl": "/avatars/879f79628a30dddeab6e36e7d82bdbaa.svg",
                "isPro": false,
                "fullname": " Zichun Yu",
                "user": "yuzc19",
                "type": "user"
            },
            "summary": "High-quality pretraining data is the fossil fuel of large language models\n(LLMs), yet its reserves are running low for frontier models. In this paper, we\nintroduce RePro, a novel web recycling method that trains a relatively small LM\nwith reinforcement learning to generate effective and faithful rephrasings of\npretraining data. Specifically, we design one quality reward and three\nfaithfulness rewards, optimizing the LM rephraser to convert organic data into\nhigh-quality rephrasings while maintaining its core semantics and structure. In\nour experiment, we train a 4B rephraser to recycle 72B tokens sampled from\nDCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that\nRePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on\n22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web\nrecycling method that prompts a 70B rephraser, as well as the organic baseline\nwith a 4x larger data pool. Experiments with different amounts of recycled data\nhighlight that RePro improves organic data efficiency by 2-3x. Individual and\ndistributional analyses validate that RePro preserves more critical information\nand faithfully reflects the characteristics of organic data compared to\nprompting-based methods. Together, these results show that RePro provides an\nefficient and controllable path to effectively harness the fossil fuel of LLM\npretraining. We open-source our code, rephraser, and recycled data at\nhttps://github.com/cxcscmu/RePro.",
            "upvotes": 3,
            "discussionId": "68edc950de1fee572713a917",
            "githubRepo": "https://github.com/cxcscmu/RePro",
            "ai_summary": "RePro, a reinforcement learning-based method, generates high-quality rephrasings of pretraining data to enhance the efficiency and accuracy of large language models.",
            "ai_keywords": [
                "reinforcement learning",
                "rephraser",
                "quality reward",
                "faithfulness rewards",
                "pretraining data",
                "DCLM-RefinedWeb",
                "downstream tasks",
                "ReWire",
                "data efficiency",
                "critical information preservation"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "67bf7cb85baf57a6b4009545",
                "name": "cx-cmu",
                "fullname": "Chenyan Xiong Research Group at CMU"
            }
        },
        "publishedAt": "2025-10-12T12:08:38.000Z",
        "title": "RePro: Training Language Models to Faithfully Recycle the Web for\n  Pretraining",
        "summary": "High-quality pretraining data is the fossil fuel of large language models\n(LLMs), yet its reserves are running low for frontier models. In this paper, we\nintroduce RePro, a novel web recycling method that trains a relatively small LM\nwith reinforcement learning to generate effective and faithful rephrasings of\npretraining data. Specifically, we design one quality reward and three\nfaithfulness rewards, optimizing the LM rephraser to convert organic data into\nhigh-quality rephrasings while maintaining its core semantics and structure. In\nour experiment, we train a 4B rephraser to recycle 72B tokens sampled from\nDCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that\nRePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on\n22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web\nrecycling method that prompts a 70B rephraser, as well as the organic baseline\nwith a 4x larger data pool. Experiments with different amounts of recycled data\nhighlight that RePro improves organic data efficiency by 2-3x. Individual and\ndistributional analyses validate that RePro preserves more critical information\nand faithfully reflects the characteristics of organic data compared to\nprompting-based methods. Together, these results show that RePro provides an\nefficient and controllable path to effectively harness the fossil fuel of LLM\npretraining. We open-source our code, rephraser, and recycled data at\nhttps://github.com/cxcscmu/RePro.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10681.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649a943f575e60d3a87cfcdf",
            "avatarUrl": "/avatars/879f79628a30dddeab6e36e7d82bdbaa.svg",
            "fullname": " Zichun Yu",
            "name": "yuzc19",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "67bf7cb85baf57a6b4009545",
            "name": "cx-cmu",
            "fullname": "Chenyan Xiong Research Group at CMU"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.09189",
            "authors": [
                {
                    "_id": "68ec701fcd07fb414898c9d2",
                    "name": "Changjiang Gao",
                    "hidden": false
                },
                {
                    "_id": "68ec701fcd07fb414898c9d3",
                    "name": "Zixian Huang",
                    "hidden": false
                },
                {
                    "_id": "68ec701fcd07fb414898c9d4",
                    "name": "Jingyang Gong",
                    "hidden": false
                },
                {
                    "_id": "68ec701fcd07fb414898c9d5",
                    "name": "Shujian Huang",
                    "hidden": false
                },
                {
                    "_id": "68ec701fcd07fb414898c9d6",
                    "name": "Lei Li",
                    "hidden": false
                },
                {
                    "_id": "68ec701fcd07fb414898c9d7",
                    "name": "Fei Yuan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T09:33:28.000Z",
            "submittedOnDailyAt": "2025-10-14T01:15:23.444Z",
            "title": "LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning",
            "submittedOnDailyBy": {
                "_id": "65fed45b08d35929362dd651",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fed45b08d35929362dd651/KLMxsyRN6_HhCZP1iDw6K.png",
                "isPro": false,
                "fullname": "FeiYuan",
                "user": "FeYuan",
                "type": "user"
            },
            "summary": "General Large Language Models (LLMs) excel in reasoning, but those enhanced\nfor translation struggle with reasoning tasks. To address this, we propose a\nnovel translationenhanced recipe that begins with instruct models and applies\nlayer-selective tuning only on parallel data. Following this pipeline, we\nintroduce the Qwen3-XPlus models, which demonstrate significant improvements in\ntranslation performance across both high- and lowresource languages, achieving\n15+ spBLEU and 40+ xComet in low-resource languages, like Swahili.\nInterestingly, training only with small parallel datasets, Qwen3-XPlus achieves\nan average improvement of 1+ points on 7 multilingual tasks while maintaining\nproficiency comparable to the Qwen3 instruct model in 15 popular reasoning\ndatasets. This work offers a promising approach to multilingual enhancement,\nsignificantly reducing complexity and enhancing accessibility for a wider range\nof languages. The code and model are publicly available.",
            "upvotes": 3,
            "discussionId": "68ec701fcd07fb414898c9d8",
            "ai_summary": "A novel translation-enhanced recipe using layer-selective tuning on parallel data improves translation performance in both high- and low-resource languages while maintaining reasoning proficiency.",
            "ai_keywords": [
                "instruct models",
                "layer-selective tuning",
                "parallel data",
                "Qwen3-XPlus models",
                "spBLEU",
                "xComet",
                "multilingual tasks",
                "reasoning datasets"
            ]
        },
        "publishedAt": "2025-10-10T05:33:28.000Z",
        "title": "LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning",
        "summary": "General Large Language Models (LLMs) excel in reasoning, but those enhanced\nfor translation struggle with reasoning tasks. To address this, we propose a\nnovel translationenhanced recipe that begins with instruct models and applies\nlayer-selective tuning only on parallel data. Following this pipeline, we\nintroduce the Qwen3-XPlus models, which demonstrate significant improvements in\ntranslation performance across both high- and lowresource languages, achieving\n15+ spBLEU and 40+ xComet in low-resource languages, like Swahili.\nInterestingly, training only with small parallel datasets, Qwen3-XPlus achieves\nan average improvement of 1+ points on 7 multilingual tasks while maintaining\nproficiency comparable to the Qwen3 instruct model in 15 popular reasoning\ndatasets. This work offers a promising approach to multilingual enhancement,\nsignificantly reducing complexity and enhancing accessibility for a wider range\nof languages. The code and model are publicly available.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09189.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65fed45b08d35929362dd651",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fed45b08d35929362dd651/KLMxsyRN6_HhCZP1iDw6K.png",
            "fullname": "FeiYuan",
            "name": "FeYuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 24
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.09023",
            "authors": [
                {
                    "_id": "68ee80fc7b0ac1ee3e5998bb",
                    "name": "Milad Nasr",
                    "hidden": false
                },
                {
                    "_id": "68ee80fc7b0ac1ee3e5998bc",
                    "name": "Nicholas Carlini",
                    "hidden": false
                },
                {
                    "_id": "68ee80fc7b0ac1ee3e5998bd",
                    "name": "Chawin Sitawarin",
                    "hidden": false
                },
                {
                    "_id": "68ee80fc7b0ac1ee3e5998be",
                    "name": "Sander V. Schulhoff",
                    "hidden": false
                },
                {
                    "_id": "68ee80fc7b0ac1ee3e5998bf",
                    "name": "Jamie Hayes",
                    "hidden": false
                },
                {
                    "_id": "68ee80fc7b0ac1ee3e5998c0",
                    "name": "Michael Ilie",
                    "hidden": false
                },
                {
                    "_id": "68ee80fc7b0ac1ee3e5998c1",
                    "name": "Juliette Pluto",
                    "hidden": false
                },
                {
                    "_id": "68ee80fc7b0ac1ee3e5998c2",
                    "name": "Shuang Song",
                    "hidden": false
                },
                {
                    "_id": "68ee80fc7b0ac1ee3e5998c3",
                    "name": "Harsh Chaudhari",
                    "hidden": false
                },
                {
                    "_id": "68ee80fc7b0ac1ee3e5998c4",
                    "name": "Ilia Shumailov",
                    "hidden": false
                },
                {
                    "_id": "68ee80fc7b0ac1ee3e5998c5",
                    "name": "Abhradeep Thakurta",
                    "hidden": false
                },
                {
                    "_id": "68ee80fc7b0ac1ee3e5998c6",
                    "name": "Kai Yuanqing Xiao",
                    "hidden": false
                },
                {
                    "_id": "68ee80fc7b0ac1ee3e5998c7",
                    "name": "Andreas Terzis",
                    "hidden": false
                },
                {
                    "_id": "68ee80fc7b0ac1ee3e5998c8",
                    "name": "Florian Tramèr",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T05:51:04.000Z",
            "submittedOnDailyAt": "2025-10-14T15:28:05.024Z",
            "title": "The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses\n  Against Llm Jailbreaks and Prompt Injections",
            "submittedOnDailyBy": {
                "_id": "6475c2794766357252e69e9f",
                "avatarUrl": "/avatars/757ed789423113369868e972f21ce559.svg",
                "isPro": false,
                "fullname": "i",
                "user": "iliashum",
                "type": "user"
            },
            "summary": "How should we evaluate the robustness of language model defenses? Current\ndefenses against jailbreaks and prompt injections (which aim to prevent an\nattacker from eliciting harmful knowledge or remotely triggering malicious\nactions, respectively) are typically evaluated either against a static set of\nharmful attack strings, or against computationally weak optimization methods\nthat were not designed with the defense in mind. We argue that this evaluation\nprocess is flawed.\n  Instead, we should evaluate defenses against adaptive attackers who\nexplicitly modify their attack strategy to counter a defense's design while\nspending considerable resources to optimize their objective. By systematically\ntuning and scaling general optimization techniques-gradient descent,\nreinforcement learning, random search, and human-guided exploration-we bypass\n12 recent defenses (based on a diverse set of techniques) with attack success\nrate above 90% for most; importantly, the majority of defenses originally\nreported near-zero attack success rates. We believe that future defense work\nmust consider stronger attacks, such as the ones we describe, in order to make\nreliable and convincing claims of robustness.",
            "upvotes": 3,
            "discussionId": "68ee80fc7b0ac1ee3e5998c9",
            "ai_summary": "Defenses against jailbreaks and prompt injections in language models should be evaluated against adaptive attackers using advanced optimization techniques to ensure robustness.",
            "ai_keywords": [
                "gradient descent",
                "reinforcement learning",
                "random search",
                "human-guided exploration"
            ]
        },
        "publishedAt": "2025-10-10T01:51:04.000Z",
        "title": "The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses\n  Against Llm Jailbreaks and Prompt Injections",
        "summary": "How should we evaluate the robustness of language model defenses? Current\ndefenses against jailbreaks and prompt injections (which aim to prevent an\nattacker from eliciting harmful knowledge or remotely triggering malicious\nactions, respectively) are typically evaluated either against a static set of\nharmful attack strings, or against computationally weak optimization methods\nthat were not designed with the defense in mind. We argue that this evaluation\nprocess is flawed.\n  Instead, we should evaluate defenses against adaptive attackers who\nexplicitly modify their attack strategy to counter a defense's design while\nspending considerable resources to optimize their objective. By systematically\ntuning and scaling general optimization techniques-gradient descent,\nreinforcement learning, random search, and human-guided exploration-we bypass\n12 recent defenses (based on a diverse set of techniques) with attack success\nrate above 90% for most; importantly, the majority of defenses originally\nreported near-zero attack success rates. We believe that future defense work\nmust consider stronger attacks, such as the ones we describe, in order to make\nreliable and convincing claims of robustness.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09023.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6475c2794766357252e69e9f",
            "avatarUrl": "/avatars/757ed789423113369868e972f21ce559.svg",
            "fullname": "i",
            "name": "iliashum",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.05213",
            "authors": [
                {
                    "_id": "68ede69ede1fee572713a9cc",
                    "name": "Yixiao Wang",
                    "hidden": false
                },
                {
                    "_id": "68ede69ede1fee572713a9cd",
                    "name": "Mingxiao Huo",
                    "hidden": false
                },
                {
                    "_id": "68ede69ede1fee572713a9ce",
                    "name": "Zhixuan Liang",
                    "hidden": false
                },
                {
                    "_id": "68ede69ede1fee572713a9cf",
                    "name": "Yushi Du",
                    "hidden": false
                },
                {
                    "_id": "68ede69ede1fee572713a9d0",
                    "name": "Lingfeng Sun",
                    "hidden": false
                },
                {
                    "_id": "68ede69ede1fee572713a9d1",
                    "name": "Haotian Lin",
                    "hidden": false
                },
                {
                    "_id": "68ede69ede1fee572713a9d2",
                    "name": "Jinghuan Shang",
                    "hidden": false
                },
                {
                    "_id": "68ede69ede1fee572713a9d3",
                    "name": "Chensheng Peng",
                    "hidden": false
                },
                {
                    "_id": "68ede69ede1fee572713a9d4",
                    "name": "Mohit Bansal",
                    "hidden": false
                },
                {
                    "_id": "68ede69ede1fee572713a9d5",
                    "name": "Mingyu Ding",
                    "hidden": false
                },
                {
                    "_id": "68ede69ede1fee572713a9d6",
                    "name": "Masayoshi Tomizuka",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-06T18:00:43.000Z",
            "submittedOnDailyAt": "2025-10-14T04:30:03.749Z",
            "title": "VER: Vision Expert Transformer for Robot Learning via Foundation\n  Distillation and Dynamic Routing",
            "submittedOnDailyBy": {
                "_id": "662a471e94baa018b00c0f5c",
                "avatarUrl": "/avatars/62a67a2ee6e4b9a7124f8b02b9b3f280.svg",
                "isPro": false,
                "fullname": "Zhixuan Liang",
                "user": "Liang-ZX",
                "type": "user"
            },
            "summary": "Pretrained vision foundation models (VFMs) advance robotic learning via rich\nvisual representations, yet individual VFMs typically excel only in specific\ndomains, limiting generality across tasks. Distilling multiple VFMs into a\nunified representation for policy can mitigate this limitation but often yields\ninflexible task-specific feature selection and requires costly full re-training\nto incorporate robot-domain knowledge. We propose VER, a Vision Expert\ntransformer for Robot learning. During pretraining, VER distills multiple VFMs\ninto a vision expert library. It then fine-tunes only a lightweight routing\nnetwork (fewer than 0.4% of parameters) to dynamically select task-relevant\nexperts from the pretrained library for downstream robot tasks. We further\nintroduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve\nboth flexibility and precision of dynamic expert selection. Moreover, VER\nsupports parameter-efficient finetuning for scalable expert utilization and\nadaptive robot-domain knowledge integration. Across 17 diverse robotic tasks\nand multiple policy heads, VER achieves state-of-the-art performance. We find\nthat VER reduces large-norm outliers in task-irrelevant regions (e.g.,\nbackground) and concentrates on task-critical regions. Visualizations and codes\ncan be found in https://yixiaowang7.github.io/ver_page/.",
            "upvotes": 3,
            "discussionId": "68ede69fde1fee572713a9d7",
            "ai_summary": "VER, a Vision Expert Transformer, dynamically selects task-relevant experts from a pretrained vision expert library, achieving state-of-the-art performance across diverse robotic tasks with parameter-efficient fine-tuning.",
            "ai_keywords": [
                "pretrained vision foundation models",
                "VFMs",
                "unified representation",
                "policy",
                "routing network",
                "Patchwise Expert Routing",
                "Curriculum Top-K Annealing",
                "parameter-efficient fine-tuning",
                "expert utilization",
                "adaptive robot-domain knowledge integration"
            ],
            "organization": {
                "_id": "61f20a9ce108f2cba2dc0730",
                "name": "Berkeley",
                "fullname": "UC Berkeley",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"
            }
        },
        "publishedAt": "2025-10-06T14:00:43.000Z",
        "title": "VER: Vision Expert Transformer for Robot Learning via Foundation\n  Distillation and Dynamic Routing",
        "summary": "Pretrained vision foundation models (VFMs) advance robotic learning via rich\nvisual representations, yet individual VFMs typically excel only in specific\ndomains, limiting generality across tasks. Distilling multiple VFMs into a\nunified representation for policy can mitigate this limitation but often yields\ninflexible task-specific feature selection and requires costly full re-training\nto incorporate robot-domain knowledge. We propose VER, a Vision Expert\ntransformer for Robot learning. During pretraining, VER distills multiple VFMs\ninto a vision expert library. It then fine-tunes only a lightweight routing\nnetwork (fewer than 0.4% of parameters) to dynamically select task-relevant\nexperts from the pretrained library for downstream robot tasks. We further\nintroduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve\nboth flexibility and precision of dynamic expert selection. Moreover, VER\nsupports parameter-efficient finetuning for scalable expert utilization and\nadaptive robot-domain knowledge integration. Across 17 diverse robotic tasks\nand multiple policy heads, VER achieves state-of-the-art performance. We find\nthat VER reduces large-norm outliers in task-irrelevant regions (e.g.,\nbackground) and concentrates on task-critical regions. Visualizations and codes\ncan be found in https://yixiaowang7.github.io/ver_page/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05213.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "662a471e94baa018b00c0f5c",
            "avatarUrl": "/avatars/62a67a2ee6e4b9a7124f8b02b9b3f280.svg",
            "fullname": "Zhixuan Liang",
            "name": "Liang-ZX",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "61f20a9ce108f2cba2dc0730",
            "name": "Berkeley",
            "fullname": "UC Berkeley",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11647",
            "authors": [
                {
                    "_id": "68edc14ade1fee572713a891",
                    "name": "Yinan Chen",
                    "hidden": false
                },
                {
                    "_id": "68edc14ade1fee572713a892",
                    "name": "Jiangning Zhang",
                    "hidden": false
                },
                {
                    "_id": "68edc14ade1fee572713a893",
                    "name": "Teng Hu",
                    "hidden": false
                },
                {
                    "_id": "68edc14ade1fee572713a894",
                    "name": "Yuxiang Zeng",
                    "hidden": false
                },
                {
                    "_id": "68edc14ade1fee572713a895",
                    "name": "Zhucun Xue",
                    "hidden": false
                },
                {
                    "_id": "68edc14ade1fee572713a896",
                    "name": "Qingdong He",
                    "hidden": false
                },
                {
                    "_id": "68edc14ade1fee572713a897",
                    "name": "Chengjie Wang",
                    "hidden": false
                },
                {
                    "_id": "68edc14ade1fee572713a898",
                    "name": "Yong Liu",
                    "hidden": false
                },
                {
                    "_id": "68edc14ade1fee572713a899",
                    "name": "Xiaobin Hu",
                    "hidden": false
                },
                {
                    "_id": "68edc14ade1fee572713a89a",
                    "name": "Shuicheng Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T17:27:08.000Z",
            "submittedOnDailyAt": "2025-10-14T01:49:51.520Z",
            "title": "IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing\n  Assessment",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Instruction-guided video editing has emerged as a rapidly advancing research\ndirection, offering new opportunities for intuitive content transformation\nwhile also posing significant challenges for systematic evaluation. Existing\nvideo editing benchmarks fail to support the evaluation of instruction-guided\nvideo editing adequately and further suffer from limited source diversity,\nnarrow task coverage and incomplete evaluation metrics. To address the above\nlimitations, we introduce IVEBench, a modern benchmark suite specifically\ndesigned for instruction-guided video editing assessment. IVEBench comprises a\ndiverse database of 600 high-quality source videos, spanning seven semantic\ndimensions, and covering video lengths ranging from 32 to 1,024 frames. It\nfurther includes 8 categories of editing tasks with 35 subcategories, whose\nprompts are generated and refined through large language models and expert\nreview. Crucially, IVEBench establishes a three-dimensional evaluation protocol\nencompassing video quality, instruction compliance and video fidelity,\nintegrating both traditional metrics and multimodal large language model-based\nassessments. Extensive experiments demonstrate the effectiveness of IVEBench in\nbenchmarking state-of-the-art instruction-guided video editing methods, showing\nits ability to provide comprehensive and human-aligned evaluation outcomes.",
            "upvotes": 2,
            "discussionId": "68edc14bde1fee572713a89b",
            "projectPage": "https://ryanchenyn.github.io/projects/IVEBench/",
            "ai_summary": "IVEBench is a benchmark suite for instruction-guided video editing that addresses limitations in existing benchmarks through diverse video sources, comprehensive task coverage, and a multi-dimensional evaluation protocol.",
            "ai_keywords": [
                "instruction-guided video editing",
                "benchmark suite",
                "source diversity",
                "task coverage",
                "evaluation metrics",
                "large language models",
                "video quality",
                "instruction compliance",
                "video fidelity",
                "multimodal assessments"
            ]
        },
        "publishedAt": "2025-10-13T13:27:08.000Z",
        "title": "IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing\n  Assessment",
        "summary": "Instruction-guided video editing has emerged as a rapidly advancing research\ndirection, offering new opportunities for intuitive content transformation\nwhile also posing significant challenges for systematic evaluation. Existing\nvideo editing benchmarks fail to support the evaluation of instruction-guided\nvideo editing adequately and further suffer from limited source diversity,\nnarrow task coverage and incomplete evaluation metrics. To address the above\nlimitations, we introduce IVEBench, a modern benchmark suite specifically\ndesigned for instruction-guided video editing assessment. IVEBench comprises a\ndiverse database of 600 high-quality source videos, spanning seven semantic\ndimensions, and covering video lengths ranging from 32 to 1,024 frames. It\nfurther includes 8 categories of editing tasks with 35 subcategories, whose\nprompts are generated and refined through large language models and expert\nreview. Crucially, IVEBench establishes a three-dimensional evaluation protocol\nencompassing video quality, instruction compliance and video fidelity,\nintegrating both traditional metrics and multimodal large language model-based\nassessments. Extensive experiments demonstrate the effectiveness of IVEBench in\nbenchmarking state-of-the-art instruction-guided video editing methods, showing\nits ability to provide comprehensive and human-aligned evaluation outcomes.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11647.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 126
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.09474",
            "authors": [
                {
                    "_id": "68ee7ab17b0ac1ee3e599890",
                    "name": "Zhenhailong Wang",
                    "hidden": false
                },
                {
                    "_id": "68ee7ab17b0ac1ee3e599891",
                    "name": "Jiateng Liu",
                    "hidden": false
                },
                {
                    "_id": "68ee7ab17b0ac1ee3e599892",
                    "name": "Amin Fazel",
                    "hidden": false
                },
                {
                    "_id": "68ee7ab17b0ac1ee3e599893",
                    "name": "Ritesh Sarkhel",
                    "hidden": false
                },
                {
                    "_id": "68ee7ab17b0ac1ee3e599894",
                    "name": "Xing Fan",
                    "hidden": false
                },
                {
                    "_id": "68ee7ab17b0ac1ee3e599895",
                    "name": "Xiang Li",
                    "hidden": false
                },
                {
                    "_id": "68ee7ab17b0ac1ee3e599896",
                    "name": "Chenlei Guo",
                    "hidden": false
                },
                {
                    "_id": "68ee7ab17b0ac1ee3e599897",
                    "name": "Heng Ji",
                    "hidden": false
                },
                {
                    "_id": "68ee7ab17b0ac1ee3e599898",
                    "name": "Ruhi Sarikaya",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T15:28:30.000Z",
            "submittedOnDailyAt": "2025-10-14T15:03:03.559Z",
            "title": "Multimodal Policy Internalization for Conversational Agents",
            "submittedOnDailyBy": {
                "_id": "628d7265db4cd1d1717c884f",
                "avatarUrl": "/avatars/dff2a3dd10d84b4a73fa486402de7219.svg",
                "isPro": false,
                "fullname": "Zhenhailong Wang",
                "user": "mikewang",
                "type": "user"
            },
            "summary": "Modern conversational agents like ChatGPT and Alexa+ rely on predefined\npolicies specifying metadata, response styles, and tool-usage rules. As these\nLLM-based systems expand to support diverse business and user queries, such\npolicies, often implemented as in-context prompts, are becoming increasingly\ncomplex and lengthy, making faithful adherence difficult and imposing large\nfixed computational costs. With the rise of multimodal agents, policies that\ngovern visual and multimodal behaviors are critical but remain understudied.\nPrior prompt-compression work mainly shortens task templates and\ndemonstrations, while existing policy-alignment studies focus only on\ntext-based safety rules. We introduce Multimodal Policy Internalization (MPI),\na new task that internalizes reasoning-intensive multimodal policies into model\nparameters, enabling stronger policy-following without including the policy\nduring inference. MPI poses unique data and algorithmic challenges. We build\ntwo datasets spanning synthetic and real-world decision-making and tool-using\ntasks and propose TriMPI, a three-stage training framework. TriMPI first\ninjects policy knowledge via continual pretraining, then performs supervised\nfinetuning, and finally applies PolicyRollout, a GRPO-style reinforcement\nlearning extension that augments rollouts with policy-aware responses for\ngrounded exploration. TriMPI achieves notable gains in end-to-end accuracy,\ngeneralization, and robustness to forgetting. As the first work on multimodal\npolicy internalization, we provide datasets, training recipes, and\ncomprehensive evaluations to foster future research. Project page:\nhttps://mikewangwzhl.github.io/TriMPI.",
            "upvotes": 2,
            "discussionId": "68ee7ab17b0ac1ee3e599899",
            "projectPage": "https://mikewangwzhl.github.io/TriMPI/",
            "ai_summary": "Multimodal Policy Internalization (MPI) internalizes complex multimodal policies into model parameters, enhancing policy adherence and performance in conversational agents.",
            "ai_keywords": [
                "Multimodal Policy Internalization",
                "MPI",
                "TriMPI",
                "continual pretraining",
                "supervised finetuning",
                "PolicyRollout",
                "GRPO",
                "reinforcement learning",
                "end-to-end accuracy",
                "generalization",
                "robustness to forgetting"
            ],
            "organization": {
                "_id": "5ffdfbadbba2ae614d771970",
                "name": "amazon",
                "fullname": "Amazon",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
            }
        },
        "publishedAt": "2025-10-10T11:28:30.000Z",
        "title": "Multimodal Policy Internalization for Conversational Agents",
        "summary": "Modern conversational agents like ChatGPT and Alexa+ rely on predefined\npolicies specifying metadata, response styles, and tool-usage rules. As these\nLLM-based systems expand to support diverse business and user queries, such\npolicies, often implemented as in-context prompts, are becoming increasingly\ncomplex and lengthy, making faithful adherence difficult and imposing large\nfixed computational costs. With the rise of multimodal agents, policies that\ngovern visual and multimodal behaviors are critical but remain understudied.\nPrior prompt-compression work mainly shortens task templates and\ndemonstrations, while existing policy-alignment studies focus only on\ntext-based safety rules. We introduce Multimodal Policy Internalization (MPI),\na new task that internalizes reasoning-intensive multimodal policies into model\nparameters, enabling stronger policy-following without including the policy\nduring inference. MPI poses unique data and algorithmic challenges. We build\ntwo datasets spanning synthetic and real-world decision-making and tool-using\ntasks and propose TriMPI, a three-stage training framework. TriMPI first\ninjects policy knowledge via continual pretraining, then performs supervised\nfinetuning, and finally applies PolicyRollout, a GRPO-style reinforcement\nlearning extension that augments rollouts with policy-aware responses for\ngrounded exploration. TriMPI achieves notable gains in end-to-end accuracy,\ngeneralization, and robustness to forgetting. As the first work on multimodal\npolicy internalization, we provide datasets, training recipes, and\ncomprehensive evaluations to foster future research. Project page:\nhttps://mikewangwzhl.github.io/TriMPI.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09474.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "628d7265db4cd1d1717c884f",
            "avatarUrl": "/avatars/dff2a3dd10d84b4a73fa486402de7219.svg",
            "fullname": "Zhenhailong Wang",
            "name": "mikewang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "5ffdfbadbba2ae614d771970",
            "name": "amazon",
            "fullname": "Amazon",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08744",
            "authors": [
                {
                    "_id": "68edad48de1fee572713a720",
                    "name": "Gang Liu",
                    "hidden": false
                },
                {
                    "_id": "68edad48de1fee572713a721",
                    "name": "Jie Chen",
                    "hidden": false
                },
                {
                    "_id": "68edad48de1fee572713a722",
                    "name": "Yihan Zhu",
                    "hidden": false
                },
                {
                    "_id": "68edad48de1fee572713a723",
                    "name": "Michael Sun",
                    "hidden": false
                },
                {
                    "_id": "68edad48de1fee572713a724",
                    "name": "Tengfei Luo",
                    "hidden": false
                },
                {
                    "_id": "68edad48de1fee572713a725",
                    "name": "Nitesh V Chawla",
                    "hidden": false
                },
                {
                    "_id": "68edad48de1fee572713a726",
                    "name": "Meng Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T18:56:57.000Z",
            "submittedOnDailyAt": "2025-10-14T01:04:16.487Z",
            "title": "Graph Diffusion Transformers are In-Context Molecular Designers",
            "submittedOnDailyBy": {
                "_id": "63bf1afd4a2beec65565ee90",
                "avatarUrl": "/avatars/134ac63b443e1360be7c91f84f9d5ec7.svg",
                "isPro": false,
                "fullname": "Gang Liu",
                "user": "liuganghuggingface",
                "type": "user"
            },
            "summary": "In-context learning allows large models to adapt to new tasks from a few\ndemonstrations, but it has shown limited success in molecular design. Existing\ndatabases such as ChEMBL contain molecular properties spanning millions of\nbiological assays, yet labeled data for each property remain scarce. To address\nthis limitation, we introduce demonstration-conditioned diffusion models\n(DemoDiff), which define task contexts using a small set of molecule-score\nexamples instead of text descriptions. These demonstrations guide a denoising\nTransformer to generate molecules aligned with target properties. For scalable\npretraining, we develop a new molecular tokenizer with Node Pair Encoding that\nrepresents molecules at the motif level, requiring 5.5times fewer nodes. We\ncurate a dataset containing millions of context tasks from multiple sources\ncovering both drugs and materials, and pretrain a 0.7-billion-parameter model\non it. Across 33 design tasks in six categories, DemoDiff matches or surpasses\nlanguage models 100-1000times larger and achieves an average rank of 3.63\ncompared to 5.25-10.20 for domain-specific approaches. These results position\nDemoDiff as a molecular foundation model for in-context molecular design. Our\ncode is available at https://github.com/liugangcode/DemoDiff.",
            "upvotes": 2,
            "discussionId": "68edad48de1fee572713a727",
            "githubRepo": "https://github.com/liugangcode/DemoDiff",
            "ai_summary": "DemoDiff, a demonstration-conditioned diffusion model, uses molecule-score examples to guide a denoising Transformer for molecular design, outperforming larger language models and domain-specific approaches.",
            "ai_keywords": [
                "in-context learning",
                "diffusion models",
                "denoising Transformer",
                "molecular tokenizer",
                "Node Pair Encoding",
                "molecular design",
                "pretraining",
                "foundation model"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-10-09T14:56:57.000Z",
        "title": "Graph Diffusion Transformers are In-Context Molecular Designers",
        "summary": "In-context learning allows large models to adapt to new tasks from a few\ndemonstrations, but it has shown limited success in molecular design. Existing\ndatabases such as ChEMBL contain molecular properties spanning millions of\nbiological assays, yet labeled data for each property remain scarce. To address\nthis limitation, we introduce demonstration-conditioned diffusion models\n(DemoDiff), which define task contexts using a small set of molecule-score\nexamples instead of text descriptions. These demonstrations guide a denoising\nTransformer to generate molecules aligned with target properties. For scalable\npretraining, we develop a new molecular tokenizer with Node Pair Encoding that\nrepresents molecules at the motif level, requiring 5.5times fewer nodes. We\ncurate a dataset containing millions of context tasks from multiple sources\ncovering both drugs and materials, and pretrain a 0.7-billion-parameter model\non it. Across 33 design tasks in six categories, DemoDiff matches or surpasses\nlanguage models 100-1000times larger and achieves an average rank of 3.63\ncompared to 5.25-10.20 for domain-specific approaches. These results position\nDemoDiff as a molecular foundation model for in-context molecular design. Our\ncode is available at https://github.com/liugangcode/DemoDiff.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08744.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63bf1afd4a2beec65565ee90",
            "avatarUrl": "/avatars/134ac63b443e1360be7c91f84f9d5ec7.svg",
            "fullname": "Gang Liu",
            "name": "liuganghuggingface",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.06582",
            "authors": [
                {
                    "_id": "68ed2ca6de1fee572713a5a2",
                    "user": {
                        "_id": "67508b51a76bbbdfb18b2e86",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IhIodixqMEm4pGx2qO2zm.png",
                        "isPro": false,
                        "fullname": "Fei Zhang",
                        "user": "fz-rit-hf",
                        "type": "user"
                    },
                    "name": "Fei Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:31:34.328Z",
                    "hidden": false
                },
                {
                    "_id": "68ed2ca6de1fee572713a5a3",
                    "name": "Rob Chancia",
                    "hidden": false
                },
                {
                    "_id": "68ed2ca6de1fee572713a5a4",
                    "name": "Josie Clapp",
                    "hidden": false
                },
                {
                    "_id": "68ed2ca6de1fee572713a5a5",
                    "name": "Amirhossein Hassanzadeh",
                    "hidden": false
                },
                {
                    "_id": "68ed2ca6de1fee572713a5a6",
                    "name": "Dimah Dera",
                    "hidden": false
                },
                {
                    "_id": "68ed2ca6de1fee572713a5a7",
                    "name": "Richard MacKenzie",
                    "hidden": false
                },
                {
                    "_id": "68ed2ca6de1fee572713a5a8",
                    "name": "Jan van Aardt",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T02:25:59.000Z",
            "submittedOnDailyAt": "2025-10-14T10:57:17.556Z",
            "title": "Through the Perspective of LiDAR: A Feature-Enriched and\n  Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud\n  Segmentation",
            "submittedOnDailyBy": {
                "_id": "67508b51a76bbbdfb18b2e86",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IhIodixqMEm4pGx2qO2zm.png",
                "isPro": false,
                "fullname": "Fei Zhang",
                "user": "fz-rit-hf",
                "type": "user"
            },
            "summary": "Accurate semantic segmentation of terrestrial laser scanning (TLS) point\nclouds is limited by costly manual annotation. We propose a semi-automated,\nuncertainty-aware pipeline that integrates spherical projection, feature\nenrichment, ensemble learning, and targeted annotation to reduce labeling\neffort, while sustaining high accuracy. Our approach projects 3D points to a 2D\nspherical grid, enriches pixels with multi-source features, and trains an\nensemble of segmentation networks to produce pseudo-labels and uncertainty\nmaps, the latter guiding annotation of ambiguous regions. The 2D outputs are\nback-projected to 3D, yielding densely annotated point clouds supported by a\nthree-tier visualization suite (2D feature maps, 3D colorized point clouds, and\ncompact virtual spheres) for rapid triage and reviewer guidance. Using this\npipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove\nforests. We further evaluate data efficiency and feature importance to address\ntwo key questions: (1) how much annotated data are needed and (2) which\nfeatures matter most. Results show that performance saturates after ~12\nannotated scans, geometric features contribute the most, and compact\nnine-channel stacks capture nearly all discriminative power, with the mean\nIntersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm\nthe generalization of our feature-enrichment strategy through cross-dataset\ntests on ForestSemantic and Semantic3D.\n  Our contributions include: (i) a robust, uncertainty-aware TLS annotation\npipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii)\nempirical guidance on data efficiency and feature importance, thus enabling\nscalable, high-quality segmentation of TLS point clouds for ecological\nmonitoring and beyond. The dataset and processing scripts are publicly\navailable at https://fz-rit.github.io/through-the-lidars-eye/.",
            "upvotes": 2,
            "discussionId": "68ed2ca7de1fee572713a5a9",
            "projectPage": "https://fz-rit.github.io/through-the-lidars-eye/",
            "ai_summary": "A semi-automated pipeline using spherical projection, feature enrichment, and ensemble learning reduces manual annotation effort for TLS point cloud segmentation while maintaining high accuracy.",
            "ai_keywords": [
                "spherical projection",
                "feature enrichment",
                "ensemble learning",
                "pseudo-labels",
                "uncertainty maps",
                "back-projection",
                "three-tier visualization suite",
                "Mangrove3D",
                "ForestSemantic",
                "Semantic3D",
                "mean Intersection over Union (mIoU)"
            ]
        },
        "publishedAt": "2025-10-07T22:25:59.000Z",
        "title": "Through the Perspective of LiDAR: A Feature-Enriched and\n  Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud\n  Segmentation",
        "summary": "Accurate semantic segmentation of terrestrial laser scanning (TLS) point\nclouds is limited by costly manual annotation. We propose a semi-automated,\nuncertainty-aware pipeline that integrates spherical projection, feature\nenrichment, ensemble learning, and targeted annotation to reduce labeling\neffort, while sustaining high accuracy. Our approach projects 3D points to a 2D\nspherical grid, enriches pixels with multi-source features, and trains an\nensemble of segmentation networks to produce pseudo-labels and uncertainty\nmaps, the latter guiding annotation of ambiguous regions. The 2D outputs are\nback-projected to 3D, yielding densely annotated point clouds supported by a\nthree-tier visualization suite (2D feature maps, 3D colorized point clouds, and\ncompact virtual spheres) for rapid triage and reviewer guidance. Using this\npipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove\nforests. We further evaluate data efficiency and feature importance to address\ntwo key questions: (1) how much annotated data are needed and (2) which\nfeatures matter most. Results show that performance saturates after ~12\nannotated scans, geometric features contribute the most, and compact\nnine-channel stacks capture nearly all discriminative power, with the mean\nIntersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm\nthe generalization of our feature-enrichment strategy through cross-dataset\ntests on ForestSemantic and Semantic3D.\n  Our contributions include: (i) a robust, uncertainty-aware TLS annotation\npipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii)\nempirical guidance on data efficiency and feature importance, thus enabling\nscalable, high-quality segmentation of TLS point clouds for ecological\nmonitoring and beyond. The dataset and processing scripts are publicly\navailable at https://fz-rit.github.io/through-the-lidars-eye/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06582.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67508b51a76bbbdfb18b2e86",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IhIodixqMEm4pGx2qO2zm.png",
            "fullname": "Fei Zhang",
            "name": "fz-rit-hf",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.01427",
            "authors": [
                {
                    "_id": "68ed2c5ade1fee572713a59b",
                    "user": {
                        "_id": "66b00c3776044810f59444fa",
                        "avatarUrl": "/avatars/a9b6595d17a3dfa14339725a2e8f54e7.svg",
                        "isPro": false,
                        "fullname": "Sipeng Zhang",
                        "user": "SipengZ",
                        "type": "user"
                    },
                    "name": "Sipeng Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:31:36.280Z",
                    "hidden": false
                },
                {
                    "_id": "68ed2c5ade1fee572713a59c",
                    "name": "Longfei Yun",
                    "hidden": false
                },
                {
                    "_id": "68ed2c5ade1fee572713a59d",
                    "name": "Zilong Wang",
                    "hidden": false
                },
                {
                    "_id": "68ed2c5ade1fee572713a59e",
                    "name": "Jingbo Shang",
                    "hidden": false
                },
                {
                    "_id": "68ed2c5ade1fee572713a59f",
                    "name": "Letian Peng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T20:06:48.000Z",
            "submittedOnDailyAt": "2025-10-14T15:26:59.351Z",
            "title": "A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge\n  Mining",
            "submittedOnDailyBy": {
                "_id": "66b00c3776044810f59444fa",
                "avatarUrl": "/avatars/a9b6595d17a3dfa14339725a2e8f54e7.svg",
                "isPro": false,
                "fullname": "Sipeng Zhang",
                "user": "SipengZ",
                "type": "user"
            },
            "summary": "At the core of Deep Research is knowledge mining, the task of extracting\nstructured information from massive unstructured text in response to user\ninstructions. Large language models (LLMs) excel at interpreting such\ninstructions but are prohibitively expensive to deploy at scale, while\ntraditional pipelines of classifiers and extractors remain efficient yet\nbrittle and unable to generalize to new tasks. We introduce Falconer, a\ncollaborative framework that combines the agentic reasoning of LLMs with\nlightweight proxy models for scalable knowledge mining. In Falconer, LLMs act\nas planners, decomposing user instructions into executable pipelines, and as\nannotators, generating supervision to train small proxies. The framework\nunifies classification and extraction into two atomic operations, get label and\nget span, enabling a single instruction-following model to replace multiple\ntask-specific components. To evaluate the consistency between proxy models\nincubated by Falconer and annotations provided by humans and large models, we\nconstruct new benchmarks covering both planning and end-to-end execution.\nExperiments show that Falconer closely matches state-of-the-art LLMs in\ninstruction-following accuracy while reducing inference cost by up to 90% and\naccelerating large-scale knowledge mining by more than 20x, offering an\nefficient and scalable foundation for Deep Research.",
            "upvotes": 2,
            "discussionId": "68ed2c5ade1fee572713a5a0",
            "githubRepo": "https://github.com/LongfeiYun17/falconer",
            "ai_summary": "Falconer combines large language models with lightweight proxy models to achieve scalable and efficient knowledge mining, reducing inference costs and accelerating large-scale operations.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "proxy models",
                "instruction-following",
                "knowledge mining",
                "benchmarks",
                "inference cost",
                "scalability"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "6659672c197d3500e0e02a34",
                "name": "UniversityofCaliforniaSanDiego",
                "fullname": "University of California San Diego"
            }
        },
        "publishedAt": "2025-10-01T16:06:48.000Z",
        "title": "A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge\n  Mining",
        "summary": "At the core of Deep Research is knowledge mining, the task of extracting\nstructured information from massive unstructured text in response to user\ninstructions. Large language models (LLMs) excel at interpreting such\ninstructions but are prohibitively expensive to deploy at scale, while\ntraditional pipelines of classifiers and extractors remain efficient yet\nbrittle and unable to generalize to new tasks. We introduce Falconer, a\ncollaborative framework that combines the agentic reasoning of LLMs with\nlightweight proxy models for scalable knowledge mining. In Falconer, LLMs act\nas planners, decomposing user instructions into executable pipelines, and as\nannotators, generating supervision to train small proxies. The framework\nunifies classification and extraction into two atomic operations, get label and\nget span, enabling a single instruction-following model to replace multiple\ntask-specific components. To evaluate the consistency between proxy models\nincubated by Falconer and annotations provided by humans and large models, we\nconstruct new benchmarks covering both planning and end-to-end execution.\nExperiments show that Falconer closely matches state-of-the-art LLMs in\ninstruction-following accuracy while reducing inference cost by up to 90% and\naccelerating large-scale knowledge mining by more than 20x, offering an\nefficient and scalable foundation for Deep Research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01427.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66b00c3776044810f59444fa",
            "avatarUrl": "/avatars/a9b6595d17a3dfa14339725a2e8f54e7.svg",
            "fullname": "Sipeng Zhang",
            "name": "SipengZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6659672c197d3500e0e02a34",
            "name": "UniversityofCaliforniaSanDiego",
            "fullname": "University of California San Diego"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.11496",
            "authors": [
                {
                    "_id": "68edbb69de1fee572713a810",
                    "name": "Zhiwei Jin",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a811",
                    "name": "Xiaohui Song",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a812",
                    "name": "Nan Wang",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a813",
                    "name": "Yafei Liu",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a814",
                    "name": "Chao Li",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a815",
                    "name": "Xin Li",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a816",
                    "name": "Ruichen Wang",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a817",
                    "name": "Zhihao Li",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a818",
                    "name": "Qi Qi",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a819",
                    "name": "Long Cheng",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a81a",
                    "name": "Dongze Hao",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a81b",
                    "name": "Quanlong Zheng",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a81c",
                    "name": "Yanhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a81d",
                    "name": "Haobo Ji",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a81e",
                    "name": "Jian Ma",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a81f",
                    "name": "Zhitong Zheng",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a820",
                    "name": "Zhenyi Lin",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a821",
                    "name": "Haolin Deng",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a822",
                    "name": "Xin Zou",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a823",
                    "name": "Xiaojie Yin",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a824",
                    "name": "Ruilin Wang",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a825",
                    "name": "Liankai Cai",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a826",
                    "name": "Haijing Liu",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a827",
                    "name": "Yuqing Qiu",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a828",
                    "name": "Ke Chen",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a829",
                    "name": "Zixian Li",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a82a",
                    "name": "Chi Xie",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a82b",
                    "name": "Huafei Li",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a82c",
                    "name": "Chenxing Li",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a82d",
                    "name": "Chuangchuang Wang",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a82e",
                    "name": "Kai Tang",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a82f",
                    "name": "Zhiguang Zhu",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a830",
                    "name": "Kai Tang",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a831",
                    "name": "Wenmei Gao",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a832",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a833",
                    "name": "Jun Wu",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a834",
                    "name": "Chao Liu",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a835",
                    "name": "Qin Xie",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a836",
                    "name": "Chen Chen",
                    "hidden": false
                },
                {
                    "_id": "68edbb69de1fee572713a837",
                    "name": "Haonan Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T15:04:38.000Z",
            "submittedOnDailyAt": "2025-10-14T14:05:12.074Z",
            "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model",
            "submittedOnDailyBy": {
                "_id": "649d47e06503630c5269d93a",
                "avatarUrl": "/avatars/6812c9f79f2bd753ed68afb03baac202.svg",
                "isPro": false,
                "fullname": "Chen Chen",
                "user": "beckhamchen",
                "type": "user"
            },
            "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoR",
            "upvotes": 1,
            "discussionId": "68edbb69de1fee572713a838",
            "ai_summary": "AndesVL, a suite of mobile-side MLLMs with reduced parameters, achieves top-tier performance across various benchmarks compared to similar-scale models.",
            "ai_keywords": [
                "MLLMs",
                "QwenVL",
                "InternVL",
                "GPT-4o",
                "Gemini",
                "Claude Sonnet",
                "AndesVL",
                "Qwen3",
                "LLM",
                "visual encoders",
                "text-rich image understanding",
                "reasoning",
                "math",
                "multi-image comprehension",
                "general VQA",
                "hallucination mitigation",
                "multilingual understanding",
                "GUI-related tasks",
                "1+N LoR"
            ],
            "organization": {
                "_id": "67177eecd0fad5b4ccc09461",
                "name": "OPPOer",
                "fullname": "OPPO",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66e24afddce93c7249b418c0/gQ-XFJehEyAH12zhbeR8Z.png"
            }
        },
        "publishedAt": "2025-10-13T11:04:38.000Z",
        "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model",
        "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoR",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11496.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649d47e06503630c5269d93a",
            "avatarUrl": "/avatars/6812c9f79f2bd753ed68afb03baac202.svg",
            "fullname": "Chen Chen",
            "name": "beckhamchen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "67177eecd0fad5b4ccc09461",
            "name": "OPPOer",
            "fullname": "OPPO",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66e24afddce93c7249b418c0/gQ-XFJehEyAH12zhbeR8Z.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08561",
            "authors": [
                {
                    "_id": "68ee7ff97b0ac1ee3e5998b2",
                    "name": "Maham Tanveer",
                    "hidden": false
                },
                {
                    "_id": "68ee7ff97b0ac1ee3e5998b3",
                    "name": "Yang Zhou",
                    "hidden": false
                },
                {
                    "_id": "68ee7ff97b0ac1ee3e5998b4",
                    "name": "Simon Niklaus",
                    "hidden": false
                },
                {
                    "_id": "68ee7ff97b0ac1ee3e5998b5",
                    "name": "Ali Mahdavi Amiri",
                    "hidden": false
                },
                {
                    "_id": "68ee7ff97b0ac1ee3e5998b6",
                    "name": "Hao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ee7ff97b0ac1ee3e5998b7",
                    "name": "Krishna Kumar Singh",
                    "hidden": false
                },
                {
                    "_id": "68ee7ff97b0ac1ee3e5998b8",
                    "name": "Nanxuan Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T17:59:27.000Z",
            "submittedOnDailyAt": "2025-10-14T15:23:49.719Z",
            "title": "MultiCOIN: Multi-Modal COntrollable Video INbetweening",
            "submittedOnDailyBy": {
                "_id": "6490dd690c18343a093dc718",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6490dd690c18343a093dc718/RgzJyka25k4F_ovVQVkJD.png",
                "isPro": false,
                "fullname": "Maham Tanveer",
                "user": "tmaham",
                "type": "user"
            },
            "summary": "Video inbetweening creates smooth and natural transitions between two image\nframes, making it an indispensable tool for video editing and long-form video\nsynthesis. Existing works in this domain are unable to generate large, complex,\nor intricate motions. In particular, they cannot accommodate the versatility of\nuser intents and generally lack fine control over the details of intermediate\nframes, leading to misalignment with the creative mind. To fill these gaps, we\nintroduce MultiCOIN, a video inbetweening framework that allows multi-modal\ncontrols, including depth transition and layering, motion trajectories, text\nprompts, and target regions for movement localization, while achieving a\nbalance between flexibility, ease of use, and precision for fine-grained video\ninterpolation. To achieve this, we adopt the Diffusion Transformer (DiT)\narchitecture as our video generative model, due to its proven capability to\ngenerate high-quality long videos. To ensure compatibility between DiT and our\nmulti-modal controls, we map all motion controls into a common sparse and\nuser-friendly point-based representation as the video/noise input. Further, to\nrespect the variety of controls which operate at varying levels of granularity\nand influence, we separate content controls and motion controls into two\nbranches to encode the required features before guiding the denoising process,\nresulting in two generators, one for motion and the other for content. Finally,\nwe propose a stage-wise training strategy to ensure that our model learns the\nmulti-modal controls smoothly. Extensive qualitative and quantitative\nexperiments demonstrate that multi-modal controls enable a more dynamic,\ncustomizable, and contextually accurate visual narrative.",
            "upvotes": 1,
            "discussionId": "68ee7ff97b0ac1ee3e5998b9",
            "ai_summary": "MultiCOIN, a video inbetweening framework using the Diffusion Transformer, enables multi-modal controls for precise and flexible video interpolation.",
            "ai_keywords": [
                "Diffusion Transformer",
                "DiT",
                "video inbetweening",
                "multi-modal controls",
                "depth transition",
                "layering",
                "motion trajectories",
                "text prompts",
                "target regions",
                "motion controls",
                "content controls",
                "denoising process",
                "stage-wise training"
            ]
        },
        "publishedAt": "2025-10-09T13:59:27.000Z",
        "title": "MultiCOIN: Multi-Modal COntrollable Video INbetweening",
        "summary": "Video inbetweening creates smooth and natural transitions between two image\nframes, making it an indispensable tool for video editing and long-form video\nsynthesis. Existing works in this domain are unable to generate large, complex,\nor intricate motions. In particular, they cannot accommodate the versatility of\nuser intents and generally lack fine control over the details of intermediate\nframes, leading to misalignment with the creative mind. To fill these gaps, we\nintroduce MultiCOIN, a video inbetweening framework that allows multi-modal\ncontrols, including depth transition and layering, motion trajectories, text\nprompts, and target regions for movement localization, while achieving a\nbalance between flexibility, ease of use, and precision for fine-grained video\ninterpolation. To achieve this, we adopt the Diffusion Transformer (DiT)\narchitecture as our video generative model, due to its proven capability to\ngenerate high-quality long videos. To ensure compatibility between DiT and our\nmulti-modal controls, we map all motion controls into a common sparse and\nuser-friendly point-based representation as the video/noise input. Further, to\nrespect the variety of controls which operate at varying levels of granularity\nand influence, we separate content controls and motion controls into two\nbranches to encode the required features before guiding the denoising process,\nresulting in two generators, one for motion and the other for content. Finally,\nwe propose a stage-wise training strategy to ensure that our model learns the\nmulti-modal controls smoothly. Extensive qualitative and quantitative\nexperiments demonstrate that multi-modal controls enable a more dynamic,\ncustomizable, and contextually accurate visual narrative.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08561.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6490dd690c18343a093dc718",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6490dd690c18343a093dc718/RgzJyka25k4F_ovVQVkJD.png",
            "fullname": "Maham Tanveer",
            "name": "tmaham",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    }
]