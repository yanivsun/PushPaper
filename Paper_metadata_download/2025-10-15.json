[
    {
        "paper": {
            "id": "2510.12276",
            "authors": [
                {
                    "_id": "68ef0057486b78128f0e33b0",
                    "name": "Fuhao Li",
                    "hidden": false
                },
                {
                    "_id": "68ef0057486b78128f0e33b1",
                    "name": "Wenxuan Song",
                    "hidden": false
                },
                {
                    "_id": "68ef0057486b78128f0e33b2",
                    "name": "Han Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ef0057486b78128f0e33b3",
                    "user": {
                        "_id": "66a4ed2a9ba24c30408441b0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a4ed2a9ba24c30408441b0/K9Z4MK3Do2CXjXr7XQDdu.png",
                        "isPro": false,
                        "fullname": "Jingbo Wang",
                        "user": "hhhJB",
                        "type": "user"
                    },
                    "name": "Jingbo Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T03:12:53.766Z",
                    "hidden": false
                },
                {
                    "_id": "68ef0057486b78128f0e33b4",
                    "name": "Pengxiang Ding",
                    "hidden": false
                },
                {
                    "_id": "68ef0057486b78128f0e33b5",
                    "name": "Donglin Wang",
                    "hidden": false
                },
                {
                    "_id": "68ef0057486b78128f0e33b6",
                    "name": "Long Zeng",
                    "hidden": false
                },
                {
                    "_id": "68ef0057486b78128f0e33b7",
                    "name": "Haoang Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T08:27:10.000Z",
            "submittedOnDailyAt": "2025-10-15T01:16:56.905Z",
            "title": "Spatial Forcing: Implicit Spatial Representation Alignment for\n  Vision-language-action Model",
            "submittedOnDailyBy": {
                "_id": "66a0a3405c5e2a42a214c70f",
                "avatarUrl": "/avatars/52b9ee7f899ee5431ed37fd1db378d9e.svg",
                "isPro": false,
                "fullname": "Wenxuan Song",
                "user": "Wenxuan123",
                "type": "user"
            },
            "summary": "Vision-language-action (VLA) models have recently shown strong potential in\nenabling robots to follow language instructions and execute precise actions.\nHowever, most VLAs are built upon vision-language models pretrained solely on\n2D data, which lack accurate spatial awareness and hinder their ability to\noperate in the 3D physical world. Existing solutions attempt to incorporate\nexplicit 3D sensor inputs such as depth maps or point clouds, but these\napproaches face challenges due to sensor noise, hardware heterogeneity, and\nincomplete depth coverage in existing datasets. Alternative methods that\nestimate 3D cues from 2D images also suffer from the limited performance of\ndepth estimators.We propose Spatial Forcing (SF), a simple yet effective\nalignment strategy that implicitly forces VLA models to develop spatial\ncomprehension capabilities without relying on explicit 3D inputs or depth\nestimators. SF aligns intermediate visual embeddings of VLAs with geometric\nrepresentations produced by pretrained 3D foundation models. By enforcing\nalignment at intermediate layers, SF guides VLAs to encode richer spatial\nrepresentations that enhance action precision.Extensive experiments in\nsimulation and real-world environments demonstrate that SF achieves\nstate-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further\naccelerates training by up to 3.8x and improves data efficiency across diverse\nrobotic tasks. Project page is at https://spatial-forcing.github.io/",
            "upvotes": 133,
            "discussionId": "68ef0057486b78128f0e33b8",
            "projectPage": "https://spatial-forcing.github.io/",
            "githubRepo": "https://github.com/OpenHelix-Team/Spatial-Forcing",
            "githubStars": 29,
            "organization": {
                "_id": "65ad19cac14c3cf579ad9b68",
                "name": "HKUSTGZ",
                "fullname": "HKUSTGZ"
            }
        },
        "publishedAt": "2025-10-14T04:27:10.000Z",
        "title": "Spatial Forcing: Implicit Spatial Representation Alignment for\n  Vision-language-action Model",
        "summary": "Vision-language-action (VLA) models have recently shown strong potential in\nenabling robots to follow language instructions and execute precise actions.\nHowever, most VLAs are built upon vision-language models pretrained solely on\n2D data, which lack accurate spatial awareness and hinder their ability to\noperate in the 3D physical world. Existing solutions attempt to incorporate\nexplicit 3D sensor inputs such as depth maps or point clouds, but these\napproaches face challenges due to sensor noise, hardware heterogeneity, and\nincomplete depth coverage in existing datasets. Alternative methods that\nestimate 3D cues from 2D images also suffer from the limited performance of\ndepth estimators.We propose Spatial Forcing (SF), a simple yet effective\nalignment strategy that implicitly forces VLA models to develop spatial\ncomprehension capabilities without relying on explicit 3D inputs or depth\nestimators. SF aligns intermediate visual embeddings of VLAs with geometric\nrepresentations produced by pretrained 3D foundation models. By enforcing\nalignment at intermediate layers, SF guides VLAs to encode richer spatial\nrepresentations that enhance action precision.Extensive experiments in\nsimulation and real-world environments demonstrate that SF achieves\nstate-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further\naccelerates training by up to 3.8x and improves data efficiency across diverse\nrobotic tasks. Project page is at https://spatial-forcing.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12276.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66a0a3405c5e2a42a214c70f",
            "avatarUrl": "/avatars/52b9ee7f899ee5431ed37fd1db378d9e.svg",
            "fullname": "Wenxuan Song",
            "name": "Wenxuan123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "65ad19cac14c3cf579ad9b68",
            "name": "HKUSTGZ",
            "fullname": "HKUSTGZ"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.12586",
            "authors": [
                {
                    "_id": "68eefdd0486b78128f0e3374",
                    "user": {
                        "_id": "648c6537aeff9347218f49f2",
                        "avatarUrl": "/avatars/1891855926eec77f91a389755998212f.svg",
                        "isPro": true,
                        "fullname": "Jiachen Lei",
                        "user": "jiachenlei",
                        "type": "user"
                    },
                    "name": "Jiachen Lei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T03:13:00.511Z",
                    "hidden": false
                },
                {
                    "_id": "68eefdd0486b78128f0e3375",
                    "name": "Keli Liu",
                    "hidden": false
                },
                {
                    "_id": "68eefdd0486b78128f0e3376",
                    "name": "Julius Berner",
                    "hidden": false
                },
                {
                    "_id": "68eefdd0486b78128f0e3377",
                    "name": "Haiming Yu",
                    "hidden": false
                },
                {
                    "_id": "68eefdd0486b78128f0e3378",
                    "name": "Hongkai Zheng",
                    "hidden": false
                },
                {
                    "_id": "68eefdd0486b78128f0e3379",
                    "name": "Jiahong Wu",
                    "hidden": false
                },
                {
                    "_id": "68eefdd0486b78128f0e337a",
                    "name": "Xiangxiang Chu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T14:41:16.000Z",
            "submittedOnDailyAt": "2025-10-15T00:25:14.164Z",
            "title": "Advancing End-to-End Pixel Space Generative Modeling via Self-supervised\n  Pre-training",
            "submittedOnDailyBy": {
                "_id": "66d255e3947594430c723ff6",
                "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
                "isPro": false,
                "fullname": "xiaochonglinghu",
                "user": "xiaochonglinghu",
                "type": "user"
            },
            "summary": "Pixel-space generative models are often more difficult to train and generally\nunderperform compared to their latent-space counterparts, leaving a persistent\nperformance and efficiency gap. In this paper, we introduce a novel two-stage\ntraining framework that closes this gap for pixel-space diffusion and\nconsistency models. In the first stage, we pre-train encoders to capture\nmeaningful semantics from clean images while aligning them with points along\nthe same deterministic sampling trajectory, which evolves points from the prior\nto the data distribution. In the second stage, we integrate the encoder with a\nrandomly initialized decoder and fine-tune the complete model end-to-end for\nboth diffusion and consistency models. Our training framework demonstrates\nstrong empirical performance on ImageNet dataset. Specifically, our diffusion\nmodel reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75\nnumber of function evaluations (NFE), surpassing prior pixel-space methods by a\nlarge margin in both generation quality and efficiency while rivaling leading\nVAE-based models at comparable training cost. Furthermore, on ImageNet-256, our\nconsistency model achieves an impressive FID of 8.82 in a single sampling step,\nsignificantly surpassing its latent-space counterpart. To the best of our\nknowledge, this marks the first successful training of a consistency model\ndirectly on high-resolution images without relying on pre-trained VAEs or\ndiffusion models.",
            "upvotes": 90,
            "discussionId": "68eefdd1486b78128f0e337b",
            "githubRepo": "https://github.com/AMAP-ML/EPG",
            "githubStars": 66,
            "organization": {
                "_id": "67d11771890254196d3174e5",
                "name": "GD-ML",
                "fullname": "AMAP-ML",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"
            }
        },
        "publishedAt": "2025-10-14T10:41:16.000Z",
        "title": "Advancing End-to-End Pixel Space Generative Modeling via Self-supervised\n  Pre-training",
        "summary": "Pixel-space generative models are often more difficult to train and generally\nunderperform compared to their latent-space counterparts, leaving a persistent\nperformance and efficiency gap. In this paper, we introduce a novel two-stage\ntraining framework that closes this gap for pixel-space diffusion and\nconsistency models. In the first stage, we pre-train encoders to capture\nmeaningful semantics from clean images while aligning them with points along\nthe same deterministic sampling trajectory, which evolves points from the prior\nto the data distribution. In the second stage, we integrate the encoder with a\nrandomly initialized decoder and fine-tune the complete model end-to-end for\nboth diffusion and consistency models. Our training framework demonstrates\nstrong empirical performance on ImageNet dataset. Specifically, our diffusion\nmodel reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75\nnumber of function evaluations (NFE), surpassing prior pixel-space methods by a\nlarge margin in both generation quality and efficiency while rivaling leading\nVAE-based models at comparable training cost. Furthermore, on ImageNet-256, our\nconsistency model achieves an impressive FID of 8.82 in a single sampling step,\nsignificantly surpassing its latent-space counterpart. To the best of our\nknowledge, this marks the first successful training of a consistency model\ndirectly on high-resolution images without relying on pre-trained VAEs or\ndiffusion models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12586.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "fullname": "xiaochonglinghu",
            "name": "xiaochonglinghu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "67d11771890254196d3174e5",
            "name": "GD-ML",
            "fullname": "AMAP-ML",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.09116",
            "authors": [
                {
                    "_id": "68edd273de1fee572713a963",
                    "user": {
                        "_id": "67e659905c1ed903debed72a",
                        "avatarUrl": "/avatars/b112b1f5903806e55aabd16aafdeee10.svg",
                        "isPro": false,
                        "fullname": "zez",
                        "user": "Everything-is-Ok",
                        "type": "user"
                    },
                    "name": "Enze Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:29:47.167Z",
                    "hidden": false
                },
                {
                    "_id": "68edd273de1fee572713a964",
                    "user": {
                        "_id": "6852d8a124a68790bddff660",
                        "avatarUrl": "/avatars/0c07abf23c1a099335a7fa0ca15db7ca.svg",
                        "isPro": false,
                        "fullname": "Jiaying Wang",
                        "user": "winniwang",
                        "type": "user"
                    },
                    "name": "Jiaying Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:29:42.425Z",
                    "hidden": false
                },
                {
                    "_id": "68edd273de1fee572713a965",
                    "user": {
                        "_id": "663adb42e14047f710dc1d29",
                        "avatarUrl": "/avatars/7ca49d67a4a8b4cf0ee896e07646715f.svg",
                        "isPro": false,
                        "fullname": "Mengxi Xiao",
                        "user": "ElsaShaw",
                        "type": "user"
                    },
                    "name": "Mengxi Xiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:29:44.631Z",
                    "hidden": false
                },
                {
                    "_id": "68edd273de1fee572713a966",
                    "user": {
                        "_id": "6852d79ec084bc0658b7ec4c",
                        "avatarUrl": "/avatars/1b68a590fd81b92f4860d6855409e31b.svg",
                        "isPro": false,
                        "fullname": "Liu jifei",
                        "user": "whuviolet",
                        "type": "user"
                    },
                    "name": "Jifei Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T15:25:11.506Z",
                    "hidden": false
                },
                {
                    "_id": "68edd273de1fee572713a967",
                    "user": {
                        "_id": "67fd030140e7539fcc5f6521",
                        "avatarUrl": "/avatars/d79c0eed928d964c3b014a0cf1f01d72.svg",
                        "isPro": false,
                        "fullname": "Ziyan Kuang",
                        "user": "plumjane",
                        "type": "user"
                    },
                    "name": "Ziyan Kuang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T15:25:13.629Z",
                    "hidden": false
                },
                {
                    "_id": "68edd273de1fee572713a968",
                    "name": "Rui Dong",
                    "hidden": false
                },
                {
                    "_id": "68edd273de1fee572713a969",
                    "name": "Eric Dong",
                    "hidden": false
                },
                {
                    "_id": "68edd273de1fee572713a96a",
                    "name": "Sophia Ananiadou",
                    "hidden": false
                },
                {
                    "_id": "68edd273de1fee572713a96b",
                    "name": "Min Peng",
                    "hidden": false
                },
                {
                    "_id": "68edd273de1fee572713a96c",
                    "name": "Qianqian Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T08:10:10.000Z",
            "submittedOnDailyAt": "2025-10-15T00:43:50.761Z",
            "title": "DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel\n  Translation",
            "submittedOnDailyBy": {
                "_id": "67e659905c1ed903debed72a",
                "avatarUrl": "/avatars/b112b1f5903806e55aabd16aafdeee10.svg",
                "isPro": false,
                "fullname": "zez",
                "user": "Everything-is-Ok",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have substantially advanced machine translation\n(MT), yet their effectiveness in translating web novels remains unclear.\nExisting benchmarks rely on surface-level metrics that fail to capture the\ndistinctive traits of this genre. To address these gaps, we introduce DITING,\nthe first comprehensive evaluation framework for web novel translation,\nassessing narrative and cultural fidelity across six dimensions: idiom\ntranslation, lexical ambiguity, terminology localization, tense consistency,\nzero-pronoun resolution, and cultural safety, supported by over 18K\nexpert-annotated Chinese-English sentence pairs. We further propose AgentEval,\na reasoning-driven multi-agent evaluation framework that simulates expert\ndeliberation to assess translation quality beyond lexical overlap, achieving\nthe highest correlation with human judgments among seven tested automatic\nmetrics. To enable metric comparison, we develop MetricAlign, a meta-evaluation\ndataset of 300 sentence pairs annotated with error labels and scalar quality\nscores. Comprehensive evaluation of fourteen open, closed, and commercial\nmodels reveals that Chinese-trained LLMs surpass larger foreign counterparts,\nand that DeepSeek-V3 delivers the most faithful and stylistically coherent\ntranslations. Our work establishes a new paradigm for exploring LLM-based web\nnovel translation and provides public resources to advance future research.",
            "upvotes": 90,
            "discussionId": "68edd274de1fee572713a96d",
            "githubRepo": "https://github.com/WHUNextGen/DITING",
            "ai_summary": "A new evaluation framework, DITING, and a reasoning-driven multi-agent evaluation framework, AgentEval, are introduced to assess the quality of web novel translations, revealing that Chinese-trained LLMs outperform larger foreign models.",
            "ai_keywords": [
                "large language models",
                "machine translation",
                "web novels",
                "DITING",
                "narrative fidelity",
                "cultural fidelity",
                "idiom translation",
                "lexical ambiguity",
                "terminology localization",
                "tense consistency",
                "zero-pronoun resolution",
                "cultural safety",
                "AgentEval",
                "reasoning-driven",
                "multi-agent evaluation",
                "MetricAlign",
                "meta-evaluation",
                "DeepSeek-V3",
                "stylistic coherence"
            ],
            "githubStars": 7,
            "organization": {
                "_id": "6693770d0bf3f05db3017f31",
                "name": "NextGenWhu",
                "fullname": "CLAIN-WHU",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6479f4317c18dca75e9a9324/DydmQ18EutkpFraI3FGpy.png"
            }
        },
        "publishedAt": "2025-10-10T04:10:10.000Z",
        "title": "DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel\n  Translation",
        "summary": "Large language models (LLMs) have substantially advanced machine translation\n(MT), yet their effectiveness in translating web novels remains unclear.\nExisting benchmarks rely on surface-level metrics that fail to capture the\ndistinctive traits of this genre. To address these gaps, we introduce DITING,\nthe first comprehensive evaluation framework for web novel translation,\nassessing narrative and cultural fidelity across six dimensions: idiom\ntranslation, lexical ambiguity, terminology localization, tense consistency,\nzero-pronoun resolution, and cultural safety, supported by over 18K\nexpert-annotated Chinese-English sentence pairs. We further propose AgentEval,\na reasoning-driven multi-agent evaluation framework that simulates expert\ndeliberation to assess translation quality beyond lexical overlap, achieving\nthe highest correlation with human judgments among seven tested automatic\nmetrics. To enable metric comparison, we develop MetricAlign, a meta-evaluation\ndataset of 300 sentence pairs annotated with error labels and scalar quality\nscores. Comprehensive evaluation of fourteen open, closed, and commercial\nmodels reveals that Chinese-trained LLMs surpass larger foreign counterparts,\nand that DeepSeek-V3 delivers the most faithful and stylistically coherent\ntranslations. Our work establishes a new paradigm for exploring LLM-based web\nnovel translation and provides public resources to advance future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09116.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67e659905c1ed903debed72a",
            "avatarUrl": "/avatars/b112b1f5903806e55aabd16aafdeee10.svg",
            "fullname": "zez",
            "name": "Everything-is-Ok",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6693770d0bf3f05db3017f31",
            "name": "NextGenWhu",
            "fullname": "CLAIN-WHU",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6479f4317c18dca75e9a9324/DydmQ18EutkpFraI3FGpy.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.11693",
            "authors": [
                {
                    "_id": "68ef04bc486b78128f0e33fc",
                    "user": {
                        "_id": "63108cc834c7d77420b0fd68",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63108cc834c7d77420b0fd68/taDnqEmcI9Rhe3uzcPEE3.jpeg",
                        "isPro": false,
                        "fullname": "Chenghao Xiao",
                        "user": "gowitheflow",
                        "type": "user"
                    },
                    "name": "Chenghao Xiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T03:12:42.130Z",
                    "hidden": false
                },
                {
                    "_id": "68ef04bc486b78128f0e33fd",
                    "user": {
                        "_id": "604f67ef0fe8ff3ec13d71ef",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
                        "isPro": false,
                        "fullname": "Hou Pong (Ken) Chan",
                        "user": "kenchan0226",
                        "type": "user"
                    },
                    "name": "Hou Pong Chan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T07:07:34.613Z",
                    "hidden": false
                },
                {
                    "_id": "68ef04bc486b78128f0e33fe",
                    "user": {
                        "_id": "64b7cd74ff6d81ae297feded",
                        "avatarUrl": "/avatars/880fbc96cc093f5e901ce84f32a1d21d.svg",
                        "isPro": false,
                        "fullname": "ZHANG HAO",
                        "user": "26hzhang",
                        "type": "user"
                    },
                    "name": "Hao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T03:12:44.546Z",
                    "hidden": false
                },
                {
                    "_id": "68ef04bc486b78128f0e33ff",
                    "name": "Weiwen Xu",
                    "hidden": false
                },
                {
                    "_id": "68ef04bc486b78128f0e3400",
                    "name": "Mahani Aljunied",
                    "hidden": false
                },
                {
                    "_id": "68ef04bc486b78128f0e3401",
                    "user": {
                        "_id": "642eecbf9b2484d7d8526781",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642eecbf9b2484d7d8526781/4IvGbd66s49Wx5pZyZGHA.png",
                        "isPro": false,
                        "fullname": "Yu Rong",
                        "user": "Swrooy",
                        "type": "user"
                    },
                    "name": "Yu Rong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T03:12:46.630Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T17:53:52.000Z",
            "submittedOnDailyAt": "2025-10-15T00:59:38.833Z",
            "title": "Scaling Language-Centric Omnimodal Representation Learning",
            "submittedOnDailyBy": {
                "_id": "63108cc834c7d77420b0fd68",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63108cc834c7d77420b0fd68/taDnqEmcI9Rhe3uzcPEE3.jpeg",
                "isPro": false,
                "fullname": "Chenghao Xiao",
                "user": "gowitheflow",
                "type": "user"
            },
            "summary": "Recent multimodal embedding approaches leveraging multimodal large language\nmodels (MLLMs) fine-tuned with contrastive learning (CL) have shown promising\nresults, yet the underlying reasons behind their superiority remain\nunderexplored. This work argues that a crucial advantage of MLLM-based\napproaches stems from implicit cross-modal alignment achieved during generative\npretraining, where the language decoder learns to exploit multimodal signals\nwithin a shared representation space for generating unimodal outputs. Through\nanalysis of anisotropy and kernel similarity structure, we empirically confirm\nthat latent alignment emerges within MLLM representations, allowing CL to serve\nas a lightweight refinement stage. Leveraging this insight, we propose a\nLanguage-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive\nexperiments across diverse backbones and benchmarks demonstrate its\neffectiveness, achieving state-of-the-art performance across modalities.\nFurthermore, we identify a Generation-Representation Scaling Law (GRSL),\nshowing that the representational capabilities gained through contrastive\nrefinement scales positively with the MLLM's generative capabilities. This\nsuggests that improving generative abilities evolves as an effective paradigm\nfor enhancing representation quality. We provide a theoretical explanation of\nGRSL, which formally links the MLLM's generative quality to the upper bound on\nits representation performance, and validate it on a challenging, low-resource\nvisual-document retrieval task, showing that continual generative pretraining\nbefore CL can further enhance the potential of a model's embedding\ncapabilities. Codes, models, and resources are available at\nhttps://github.com/LCO-Embedding/LCO-Embedding.",
            "upvotes": 82,
            "discussionId": "68ef04bc486b78128f0e3402",
            "projectPage": "https://huggingface.co/LCO-Embedding",
            "githubRepo": "https://github.com/LCO-Embedding/LCO-Embedding",
            "githubStars": 14,
            "organization": {
                "_id": "6808e7522a4d69d5111da55f",
                "name": "Alibaba-DAMO-Academy",
                "fullname": "DAMO Academy",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
            }
        },
        "publishedAt": "2025-10-13T13:53:52.000Z",
        "title": "Scaling Language-Centric Omnimodal Representation Learning",
        "summary": "Recent multimodal embedding approaches leveraging multimodal large language\nmodels (MLLMs) fine-tuned with contrastive learning (CL) have shown promising\nresults, yet the underlying reasons behind their superiority remain\nunderexplored. This work argues that a crucial advantage of MLLM-based\napproaches stems from implicit cross-modal alignment achieved during generative\npretraining, where the language decoder learns to exploit multimodal signals\nwithin a shared representation space for generating unimodal outputs. Through\nanalysis of anisotropy and kernel similarity structure, we empirically confirm\nthat latent alignment emerges within MLLM representations, allowing CL to serve\nas a lightweight refinement stage. Leveraging this insight, we propose a\nLanguage-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive\nexperiments across diverse backbones and benchmarks demonstrate its\neffectiveness, achieving state-of-the-art performance across modalities.\nFurthermore, we identify a Generation-Representation Scaling Law (GRSL),\nshowing that the representational capabilities gained through contrastive\nrefinement scales positively with the MLLM's generative capabilities. This\nsuggests that improving generative abilities evolves as an effective paradigm\nfor enhancing representation quality. We provide a theoretical explanation of\nGRSL, which formally links the MLLM's generative quality to the upper bound on\nits representation performance, and validate it on a challenging, low-resource\nvisual-document retrieval task, showing that continual generative pretraining\nbefore CL can further enhance the potential of a model's embedding\ncapabilities. Codes, models, and resources are available at\nhttps://github.com/LCO-Embedding/LCO-Embedding.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11693.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "63108cc834c7d77420b0fd68",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63108cc834c7d77420b0fd68/taDnqEmcI9Rhe3uzcPEE3.jpeg",
            "fullname": "Chenghao Xiao",
            "name": "gowitheflow",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 25
        },
        "organization": {
            "_id": "6808e7522a4d69d5111da55f",
            "name": "Alibaba-DAMO-Academy",
            "fullname": "DAMO Academy",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.12403",
            "authors": [
                {
                    "_id": "68ef4853486b78128f0e35d3",
                    "name": "Francesco Capuano",
                    "hidden": false
                },
                {
                    "_id": "68ef4853486b78128f0e35d4",
                    "name": "Caroline Pascal",
                    "hidden": false
                },
                {
                    "_id": "68ef4853486b78128f0e35d5",
                    "user": {
                        "_id": "64c255b2254239173af0570a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c255b2254239173af0570a/xQtKvcQynqrIc52QgvICp.jpeg",
                        "isPro": true,
                        "fullname": "Adil Zouitine",
                        "user": "AdilZtn",
                        "type": "user"
                    },
                    "name": "Adil Zouitine",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T15:24:45.601Z",
                    "hidden": false
                },
                {
                    "_id": "68ef4853486b78128f0e35d6",
                    "user": {
                        "_id": "5df7e9e5da6d0311fd3d53f9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583857746553-5df7e9e5da6d0311fd3d53f9.jpeg",
                        "isPro": true,
                        "fullname": "Thomas Wolf",
                        "user": "thomwolf",
                        "type": "user"
                    },
                    "name": "Thomas Wolf",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T15:24:48.032Z",
                    "hidden": false
                },
                {
                    "_id": "68ef4853486b78128f0e35d7",
                    "user": {
                        "_id": "668bd06dd58b51a628566d80",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668bd06dd58b51a628566d80/II7Yr5dT5ItMrpoMkQEy3.jpeg",
                        "isPro": true,
                        "fullname": "Michel Aractingi",
                        "user": "aractingi",
                        "type": "user"
                    },
                    "name": "Michel Aractingi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T17:03:46.855Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T11:36:46.000Z",
            "submittedOnDailyAt": "2025-10-15T05:38:53.854Z",
            "title": "Robot Learning: A Tutorial",
            "submittedOnDailyBy": {
                "_id": "63d67eac6f49aa8230601996",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d67eac6f49aa8230601996/djvtWdy718whUgh7tu1Ko.jpeg",
                "isPro": false,
                "fullname": "Francesco Capuano",
                "user": "fracapuano",
                "type": "user"
            },
            "summary": "Robot learning is at an inflection point, driven by rapid advancements in\nmachine learning and the growing availability of large-scale robotics data.\nThis shift from classical, model-based methods to data-driven, learning-based\nparadigms is unlocking unprecedented capabilities in autonomous systems. This\ntutorial navigates the landscape of modern robot learning, charting a course\nfrom the foundational principles of Reinforcement Learning and Behavioral\nCloning to generalist, language-conditioned models capable of operating across\ndiverse tasks and even robot embodiments. This work is intended as a guide for\nresearchers and practitioners, and our goal is to equip the reader with the\nconceptual understanding and practical tools necessary to contribute to\ndevelopments in robot learning, with ready-to-use examples implemented in\nlerobot.",
            "upvotes": 42,
            "discussionId": "68ef4853486b78128f0e35d8",
            "projectPage": "https://huggingface.co/spaces/lerobot/robot-learning-tutorial",
            "githubRepo": "https://github.com/fracapuano/robot-learning-tutorial",
            "ai_summary": "Robot learning transitions from model-based to data-driven methods, leveraging reinforcement learning and behavioral cloning to develop versatile, language-conditioned models for diverse tasks and robot types.",
            "ai_keywords": [
                "Reinforcement Learning",
                "Behavioral Cloning",
                "language-conditioned models"
            ],
            "githubStars": 71,
            "organization": {
                "_id": "65f08dbd118cb0547bb49092",
                "name": "lerobot",
                "fullname": "LeRobot",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/631ce4b244503b72277fc89f/pcLUTLsvMQiR-ujlTgLYF.png"
            }
        },
        "publishedAt": "2025-10-14T07:36:46.000Z",
        "title": "Robot Learning: A Tutorial",
        "summary": "Robot learning is at an inflection point, driven by rapid advancements in\nmachine learning and the growing availability of large-scale robotics data.\nThis shift from classical, model-based methods to data-driven, learning-based\nparadigms is unlocking unprecedented capabilities in autonomous systems. This\ntutorial navigates the landscape of modern robot learning, charting a course\nfrom the foundational principles of Reinforcement Learning and Behavioral\nCloning to generalist, language-conditioned models capable of operating across\ndiverse tasks and even robot embodiments. This work is intended as a guide for\nresearchers and practitioners, and our goal is to equip the reader with the\nconceptual understanding and practical tools necessary to contribute to\ndevelopments in robot learning, with ready-to-use examples implemented in\nlerobot.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12403.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63d67eac6f49aa8230601996",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d67eac6f49aa8230601996/djvtWdy718whUgh7tu1Ko.jpeg",
            "fullname": "Francesco Capuano",
            "name": "fracapuano",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 79
        },
        "organization": {
            "_id": "65f08dbd118cb0547bb49092",
            "name": "lerobot",
            "fullname": "LeRobot",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/631ce4b244503b72277fc89f/pcLUTLsvMQiR-ujlTgLYF.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.12798",
            "authors": [
                {
                    "_id": "68eefe5c486b78128f0e337d",
                    "name": "Qing Jiang",
                    "hidden": false
                },
                {
                    "_id": "68eefe5c486b78128f0e337e",
                    "name": "Junan Huo",
                    "hidden": false
                },
                {
                    "_id": "68eefe5c486b78128f0e337f",
                    "name": "Xingyu Chen",
                    "hidden": false
                },
                {
                    "_id": "68eefe5c486b78128f0e3380",
                    "name": "Yuda Xiong",
                    "hidden": false
                },
                {
                    "_id": "68eefe5c486b78128f0e3381",
                    "name": "Zhaoyang Zeng",
                    "hidden": false
                },
                {
                    "_id": "68eefe5c486b78128f0e3382",
                    "name": "Yihao Chen",
                    "hidden": false
                },
                {
                    "_id": "68eefe5c486b78128f0e3383",
                    "name": "Tianhe Ren",
                    "hidden": false
                },
                {
                    "_id": "68eefe5c486b78128f0e3384",
                    "name": "Junzhi Yu",
                    "hidden": false
                },
                {
                    "_id": "68eefe5c486b78128f0e3385",
                    "name": "Lei Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T17:59:54.000Z",
            "submittedOnDailyAt": "2025-10-15T00:22:45.065Z",
            "title": "Detect Anything via Next Point Prediction",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Object detection has long been dominated by traditional coordinate\nregression-based models, such as YOLO, DETR, and Grounding DINO. Although\nrecent efforts have attempted to leverage MLLMs to tackle this task, they face\nchallenges like low recall rate, duplicate predictions, coordinate\nmisalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a\n3B-scale MLLM that achieves state-of-the-art object perception performance. On\nbenchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or\nexceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot\nsetting. This is enabled by three key designs: 1) Task Formulation: we use\nspecial tokens to represent quantized coordinates from 0 to 999, reducing the\nmodel's learning difficulty and improving token efficiency for coordinate\nprediction; 2) Data Engines: we construct multiple data engines to generate\nhigh-quality grounding, referring, and pointing data, providing semantically\nrich supervision for training; \\3) Training Pipelines: we employ a two-stage\ntraining process, combining supervised fine-tuning on 22 million data with\nGRPO-based reinforcement post-training. This RL post-training leverages\ngeometry-aware rewards to effectively bridge the discrete-to-continuous\ncoordinate prediction gap, improve box accuracy, and mitigate undesirable\nbehaviors like duplicate predictions that stem from the teacher-guided nature\nof the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent\nlanguage understanding enables versatile capabilities such as object referring,\npointing, visual prompting, GUI grounding, spatial referring, OCR and\nkey-pointing, all systematically evaluated on dedicated benchmarks. We believe\nthat Rex-Omni paves the way for more versatile and language-aware visual\nperception systems.",
            "upvotes": 33,
            "discussionId": "68eefe5c486b78128f0e3386",
            "projectPage": "https://rex-omni.github.io/",
            "githubRepo": "https://github.com/IDEA-Research/Rex-Omni",
            "githubStars": 141,
            "organization": {
                "_id": "64390caac62f375ba47da58f",
                "name": "IDEA-Research",
                "fullname": "IDEA-Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6438ff69e1acfc375c693ab1/CSi9zEks9X2rRdCXoEeU_.png"
            }
        },
        "publishedAt": "2025-10-14T13:59:54.000Z",
        "title": "Detect Anything via Next Point Prediction",
        "summary": "Object detection has long been dominated by traditional coordinate\nregression-based models, such as YOLO, DETR, and Grounding DINO. Although\nrecent efforts have attempted to leverage MLLMs to tackle this task, they face\nchallenges like low recall rate, duplicate predictions, coordinate\nmisalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a\n3B-scale MLLM that achieves state-of-the-art object perception performance. On\nbenchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or\nexceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot\nsetting. This is enabled by three key designs: 1) Task Formulation: we use\nspecial tokens to represent quantized coordinates from 0 to 999, reducing the\nmodel's learning difficulty and improving token efficiency for coordinate\nprediction; 2) Data Engines: we construct multiple data engines to generate\nhigh-quality grounding, referring, and pointing data, providing semantically\nrich supervision for training; \\3) Training Pipelines: we employ a two-stage\ntraining process, combining supervised fine-tuning on 22 million data with\nGRPO-based reinforcement post-training. This RL post-training leverages\ngeometry-aware rewards to effectively bridge the discrete-to-continuous\ncoordinate prediction gap, improve box accuracy, and mitigate undesirable\nbehaviors like duplicate predictions that stem from the teacher-guided nature\nof the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent\nlanguage understanding enables versatile capabilities such as object referring,\npointing, visual prompting, GUI grounding, spatial referring, OCR and\nkey-pointing, all systematically evaluated on dedicated benchmarks. We believe\nthat Rex-Omni paves the way for more versatile and language-aware visual\nperception systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12798.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 127
        },
        "organization": {
            "_id": "64390caac62f375ba47da58f",
            "name": "IDEA-Research",
            "fullname": "IDEA-Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6438ff69e1acfc375c693ab1/CSi9zEks9X2rRdCXoEeU_.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.12399",
            "authors": [
                {
                    "_id": "68ef02c7486b78128f0e33dd",
                    "user": {
                        "_id": "656ad93853703dd78f3de7b8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656ad93853703dd78f3de7b8/JkSiNFe7LApux9YBmMPKk.png",
                        "isPro": false,
                        "fullname": "YuyaoGe",
                        "user": "YuyaoGe",
                        "type": "user"
                    },
                    "name": "Yuyao Ge",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T03:12:48.718Z",
                    "hidden": false
                },
                {
                    "_id": "68ef02c7486b78128f0e33de",
                    "user": {
                        "_id": "63120517ae8896941da4c5da",
                        "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
                        "isPro": false,
                        "fullname": "Lingrui Mei",
                        "user": "Chevalier",
                        "type": "user"
                    },
                    "name": "Lingrui Mei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T07:07:36.646Z",
                    "hidden": false
                },
                {
                    "_id": "68ef02c7486b78128f0e33df",
                    "name": "Zenghao Duan",
                    "hidden": false
                },
                {
                    "_id": "68ef02c7486b78128f0e33e0",
                    "user": {
                        "_id": "65b8724f987c4142f3a63e92",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b8724f987c4142f3a63e92/UPQVYutDOzU22yl7wW1st.jpeg",
                        "isPro": false,
                        "fullname": "Tianhao Li",
                        "user": "Tianhao0x01",
                        "type": "user"
                    },
                    "name": "Tianhao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T15:24:52.401Z",
                    "hidden": false
                },
                {
                    "_id": "68ef02c7486b78128f0e33e1",
                    "name": "Yujia Zheng",
                    "hidden": false
                },
                {
                    "_id": "68ef02c7486b78128f0e33e2",
                    "name": "Yiwei Wang",
                    "hidden": false
                },
                {
                    "_id": "68ef02c7486b78128f0e33e3",
                    "user": {
                        "_id": "68efa37f5758f527eaaa2cb9",
                        "avatarUrl": "/avatars/a4d7d27d53a8d027fa8470b368c49f3d.svg",
                        "isPro": false,
                        "fullname": "Lexin Wang",
                        "user": "LiZZieeee",
                        "type": "user"
                    },
                    "name": "Lexin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T15:24:50.080Z",
                    "hidden": false
                },
                {
                    "_id": "68ef02c7486b78128f0e33e4",
                    "name": "Jiayu Yao",
                    "hidden": false
                },
                {
                    "_id": "68ef02c7486b78128f0e33e5",
                    "name": "Tianyu Liu",
                    "hidden": false
                },
                {
                    "_id": "68ef02c7486b78128f0e33e6",
                    "name": "Yujun Cai",
                    "hidden": false
                },
                {
                    "_id": "68ef02c7486b78128f0e33e7",
                    "name": "Baolong Bi",
                    "hidden": false
                },
                {
                    "_id": "68ef02c7486b78128f0e33e8",
                    "name": "Fangda Guo",
                    "hidden": false
                },
                {
                    "_id": "68ef02c7486b78128f0e33e9",
                    "name": "Jiafeng Guo",
                    "hidden": false
                },
                {
                    "_id": "68ef02c7486b78128f0e33ea",
                    "name": "Shenghua Liu",
                    "hidden": false
                },
                {
                    "_id": "68ef02c7486b78128f0e33eb",
                    "name": "Xueqi Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T11:26:56.000Z",
            "submittedOnDailyAt": "2025-10-15T00:41:50.264Z",
            "title": "A Survey of Vibe Coding with Large Language Models",
            "submittedOnDailyBy": {
                "_id": "656ad93853703dd78f3de7b8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656ad93853703dd78f3de7b8/JkSiNFe7LApux9YBmMPKk.png",
                "isPro": false,
                "fullname": "YuyaoGe",
                "user": "YuyaoGe",
                "type": "user"
            },
            "summary": "The advancement of large language models (LLMs) has catalyzed a paradigm\nshift from code generation assistance to autonomous coding agents, enabling a\nnovel development methodology termed \"Vibe Coding\" where developers validate\nAI-generated implementations through outcome observation rather than\nline-by-line code comprehension. Despite its transformative potential, the\neffectiveness of this emergent paradigm remains under-explored, with empirical\nevidence revealing unexpected productivity losses and fundamental challenges in\nhuman-AI collaboration. To address this gap, this survey provides the first\ncomprehensive and systematic review of Vibe Coding with large language models,\nestablishing both theoretical foundations and practical frameworks for this\ntransformative development approach. Drawing from systematic analysis of over\n1000 research papers, we survey the entire vibe coding ecosystem, examining\ncritical infrastructure components including LLMs for coding, LLM-based coding\nagent, development environment of coding agent, and feedback mechanisms. We\nfirst introduce Vibe Coding as a formal discipline by formalizing it through a\nConstrained Markov Decision Process that captures the dynamic triadic\nrelationship among human developers, software projects, and coding agents.\nBuilding upon this theoretical foundation, we then synthesize existing\npractices into five distinct development models: Unconstrained Automation,\nIterative Conversational Collaboration, Planning-Driven, Test-Driven, and\nContext-Enhanced Models, thus providing the first comprehensive taxonomy in\nthis domain. Critically, our analysis reveals that successful Vibe Coding\ndepends not merely on agent capabilities but on systematic context engineering,\nwell-established development environments, and human-agent collaborative\ndevelopment models.",
            "upvotes": 31,
            "discussionId": "68ef02c8486b78128f0e33ec",
            "githubRepo": "https://github.com/YuyaoGe/Awesome-Vibe-Coding",
            "githubStars": 10,
            "organization": {
                "_id": "68ef0ab704f0f03d81964936",
                "name": "ict-cas",
                "fullname": "Institute of Computing Technology, Chinese Academy of Sciences",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/656ad93853703dd78f3de7b8/MR5EsF33Ev3IdnMOHv1be.jpeg"
            }
        },
        "publishedAt": "2025-10-14T07:26:56.000Z",
        "title": "A Survey of Vibe Coding with Large Language Models",
        "summary": "The advancement of large language models (LLMs) has catalyzed a paradigm\nshift from code generation assistance to autonomous coding agents, enabling a\nnovel development methodology termed \"Vibe Coding\" where developers validate\nAI-generated implementations through outcome observation rather than\nline-by-line code comprehension. Despite its transformative potential, the\neffectiveness of this emergent paradigm remains under-explored, with empirical\nevidence revealing unexpected productivity losses and fundamental challenges in\nhuman-AI collaboration. To address this gap, this survey provides the first\ncomprehensive and systematic review of Vibe Coding with large language models,\nestablishing both theoretical foundations and practical frameworks for this\ntransformative development approach. Drawing from systematic analysis of over\n1000 research papers, we survey the entire vibe coding ecosystem, examining\ncritical infrastructure components including LLMs for coding, LLM-based coding\nagent, development environment of coding agent, and feedback mechanisms. We\nfirst introduce Vibe Coding as a formal discipline by formalizing it through a\nConstrained Markov Decision Process that captures the dynamic triadic\nrelationship among human developers, software projects, and coding agents.\nBuilding upon this theoretical foundation, we then synthesize existing\npractices into five distinct development models: Unconstrained Automation,\nIterative Conversational Collaboration, Planning-Driven, Test-Driven, and\nContext-Enhanced Models, thus providing the first comprehensive taxonomy in\nthis domain. Critically, our analysis reveals that successful Vibe Coding\ndepends not merely on agent capabilities but on systematic context engineering,\nwell-established development environments, and human-agent collaborative\ndevelopment models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12399.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656ad93853703dd78f3de7b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656ad93853703dd78f3de7b8/JkSiNFe7LApux9YBmMPKk.png",
            "fullname": "YuyaoGe",
            "name": "YuyaoGe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "68ef0ab704f0f03d81964936",
            "name": "ict-cas",
            "fullname": "Institute of Computing Technology, Chinese Academy of Sciences",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/656ad93853703dd78f3de7b8/MR5EsF33Ev3IdnMOHv1be.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.12747",
            "authors": [
                {
                    "_id": "68eeff63486b78128f0e3393",
                    "user": {
                        "_id": "64970d3d9c3b29dca8633f87",
                        "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
                        "isPro": false,
                        "fullname": "JunhaoZhuang",
                        "user": "JunhaoZhuang",
                        "type": "user"
                    },
                    "name": "Junhao Zhuang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T03:12:58.427Z",
                    "hidden": false
                },
                {
                    "_id": "68eeff63486b78128f0e3394",
                    "name": "Shi Guo",
                    "hidden": false
                },
                {
                    "_id": "68eeff63486b78128f0e3395",
                    "name": "Xin Cai",
                    "hidden": false
                },
                {
                    "_id": "68eeff63486b78128f0e3396",
                    "name": "Xiaohui Li",
                    "hidden": false
                },
                {
                    "_id": "68eeff63486b78128f0e3397",
                    "name": "Yihao Liu",
                    "hidden": false
                },
                {
                    "_id": "68eeff63486b78128f0e3398",
                    "name": "Chun Yuan",
                    "hidden": false
                },
                {
                    "_id": "68eeff63486b78128f0e3399",
                    "name": "Tianfan Xue",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T17:25:54.000Z",
            "submittedOnDailyAt": "2025-10-15T00:27:04.930Z",
            "title": "FlashVSR: Towards Real-Time Diffusion-Based Streaming Video\n  Super-Resolution",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Diffusion models have recently advanced video restoration, but applying them\nto real-world video super-resolution (VSR) remains challenging due to high\nlatency, prohibitive computation, and poor generalization to ultra-high\nresolutions. Our goal in this work is to make diffusion-based VSR practical by\nachieving efficiency, scalability, and real-time performance. To this end, we\npropose FlashVSR, the first diffusion-based one-step streaming framework\ntowards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408\nvideos on a single A100 GPU by combining three complementary innovations: (i) a\ntrain-friendly three-stage distillation pipeline that enables streaming\nsuper-resolution, (ii) locality-constrained sparse attention that cuts\nredundant computation while bridging the train-test resolution gap, and (iii) a\ntiny conditional decoder that accelerates reconstruction without sacrificing\nquality. To support large-scale training, we also construct VSR-120K, a new\ndataset with 120k videos and 180k images. Extensive experiments show that\nFlashVSR scales reliably to ultra-high resolutions and achieves\nstate-of-the-art performance with up to 12x speedup over prior one-step\ndiffusion VSR models. We will release the code, pretrained models, and dataset\nto foster future research in efficient diffusion-based VSR.",
            "upvotes": 30,
            "discussionId": "68eeff63486b78128f0e339a",
            "projectPage": "https://zhuang2002.github.io/FlashVSR/",
            "githubRepo": "https://github.com/OpenImagingLab/FlashVSR",
            "githubStars": 76
        },
        "publishedAt": "2025-10-14T13:25:54.000Z",
        "title": "FlashVSR: Towards Real-Time Diffusion-Based Streaming Video\n  Super-Resolution",
        "summary": "Diffusion models have recently advanced video restoration, but applying them\nto real-world video super-resolution (VSR) remains challenging due to high\nlatency, prohibitive computation, and poor generalization to ultra-high\nresolutions. Our goal in this work is to make diffusion-based VSR practical by\nachieving efficiency, scalability, and real-time performance. To this end, we\npropose FlashVSR, the first diffusion-based one-step streaming framework\ntowards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408\nvideos on a single A100 GPU by combining three complementary innovations: (i) a\ntrain-friendly three-stage distillation pipeline that enables streaming\nsuper-resolution, (ii) locality-constrained sparse attention that cuts\nredundant computation while bridging the train-test resolution gap, and (iii) a\ntiny conditional decoder that accelerates reconstruction without sacrificing\nquality. To support large-scale training, we also construct VSR-120K, a new\ndataset with 120k videos and 180k images. Extensive experiments show that\nFlashVSR scales reliably to ultra-high resolutions and achieves\nstate-of-the-art performance with up to 12x speedup over prior one-step\ndiffusion VSR models. We will release the code, pretrained models, and dataset\nto foster future research in efficient diffusion-based VSR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12747.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 127
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.12773",
            "authors": [
                {
                    "_id": "68ef08b2486b78128f0e340b",
                    "user": {
                        "_id": "656864e12d73834278a8dea7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
                        "isPro": true,
                        "fullname": "Ahmed Heakl",
                        "user": "ahmedheakl",
                        "type": "user"
                    },
                    "name": "Ahmed Heakl",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T03:12:39.051Z",
                    "hidden": false
                },
                {
                    "_id": "68ef08b2486b78128f0e340c",
                    "name": "Martin Gubri",
                    "hidden": false
                },
                {
                    "_id": "68ef08b2486b78128f0e340d",
                    "name": "Salman Khan",
                    "hidden": false
                },
                {
                    "_id": "68ef08b2486b78128f0e340e",
                    "name": "Sangdoo Yun",
                    "hidden": false
                },
                {
                    "_id": "68ef08b2486b78128f0e340f",
                    "name": "Seong Joon Oh",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/AY9q3n-wpcp0MXUL4SMEY.png",
                "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/2n_XtdcAhxlT-jbMl_QWd.png",
                "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/ALKTlIvZzbZkwAQ-eELxU.png"
            ],
            "publishedAt": "2025-10-14T17:51:26.000Z",
            "submittedOnDailyAt": "2025-10-15T01:09:24.997Z",
            "title": "Dr.LLM: Dynamic Layer Routing in LLMs",
            "submittedOnDailyBy": {
                "_id": "656864e12d73834278a8dea7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
                "isPro": true,
                "fullname": "Ahmed Heakl",
                "user": "ahmedheakl",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) process every token through all layers of a\ntransformer stack, causing wasted computation on simple queries and\ninsufficient flexibility for harder ones that need deeper reasoning.\nAdaptive-depth methods can improve efficiency, but prior approaches rely on\ncostly inference-time search, architectural changes, or large-scale retraining,\nand in practice often degrade accuracy despite efficiency gains. We introduce\nDr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that\nequips pretrained models with lightweight per-layer routers deciding to skip,\nexecute, or repeat a block. Routers are trained with explicit supervision:\nusing Monte Carlo Tree Search (MCTS), we derive high-quality layer\nconfigurations that preserve or improve accuracy under a compute budget. Our\ndesign, windowed pooling for stable routing, focal loss with class balancing,\nand bottleneck MLP routers, ensures robustness under class imbalance and long\nsequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to\n+3.4%p while saving 5 layers per example on average. Routers generalize to\nout-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,\nAGIEval) with only 0.85% accuracy drop while retaining efficiency, and\noutperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that\nexplicitly supervised routers retrofit frozen LLMs for budget-aware,\naccuracy-driven inference without altering base weights.",
            "upvotes": 27,
            "discussionId": "68ef08b2486b78128f0e3410",
            "githubRepo": "https://github.com/parameterlab/dr-llm",
            "githubStars": 7,
            "organization": {
                "_id": "673bcad43a2fd2a3b41f64e3",
                "name": "parameterlab",
                "fullname": "Parameter Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/w3hbIZBXXMq09s0BAzwC0.png"
            }
        },
        "publishedAt": "2025-10-14T13:51:26.000Z",
        "title": "Dr.LLM: Dynamic Layer Routing in LLMs",
        "summary": "Large Language Models (LLMs) process every token through all layers of a\ntransformer stack, causing wasted computation on simple queries and\ninsufficient flexibility for harder ones that need deeper reasoning.\nAdaptive-depth methods can improve efficiency, but prior approaches rely on\ncostly inference-time search, architectural changes, or large-scale retraining,\nand in practice often degrade accuracy despite efficiency gains. We introduce\nDr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that\nequips pretrained models with lightweight per-layer routers deciding to skip,\nexecute, or repeat a block. Routers are trained with explicit supervision:\nusing Monte Carlo Tree Search (MCTS), we derive high-quality layer\nconfigurations that preserve or improve accuracy under a compute budget. Our\ndesign, windowed pooling for stable routing, focal loss with class balancing,\nand bottleneck MLP routers, ensures robustness under class imbalance and long\nsequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to\n+3.4%p while saving 5 layers per example on average. Routers generalize to\nout-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,\nAGIEval) with only 0.85% accuracy drop while retaining efficiency, and\noutperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that\nexplicitly supervised routers retrofit frozen LLMs for budget-aware,\naccuracy-driven inference without altering base weights.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/AY9q3n-wpcp0MXUL4SMEY.png",
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/2n_XtdcAhxlT-jbMl_QWd.png",
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/ALKTlIvZzbZkwAQ-eELxU.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12773.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "fullname": "Ahmed Heakl",
            "name": "ahmedheakl",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 51
        },
        "organization": {
            "_id": "673bcad43a2fd2a3b41f64e3",
            "name": "parameterlab",
            "fullname": "Parameter Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/w3hbIZBXXMq09s0BAzwC0.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.11057",
            "authors": [
                {
                    "_id": "68ef3928486b78128f0e356b",
                    "name": "Youngrok Park",
                    "hidden": false
                },
                {
                    "_id": "68ef3928486b78128f0e356c",
                    "name": "Hojung Jung",
                    "hidden": false
                },
                {
                    "_id": "68ef3928486b78128f0e356d",
                    "name": "Sangmin Bae",
                    "hidden": false
                },
                {
                    "_id": "68ef3928486b78128f0e356e",
                    "name": "Se-Young Yun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T06:46:57.000Z",
            "submittedOnDailyAt": "2025-10-15T04:44:04.629Z",
            "title": "Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "6602ca1e10a1441af41637be",
                "avatarUrl": "/avatars/5880e699def320beb352cbed77495b2f.svg",
                "isPro": false,
                "fullname": "Sangmin Bae",
                "user": "raymin0223",
                "type": "user"
            },
            "summary": "Diffusion models have achieved remarkable success as generative models.\nHowever, even a well-trained model can accumulate errors throughout the\ngeneration process. These errors become particularly problematic when arbitrary\nguidance is applied to steer samples toward desired properties, which often\nbreaks sample fidelity. In this paper, we propose a general solution to address\nthe off-manifold phenomenon observed in diffusion models. Our approach\nleverages a time predictor to estimate deviations from the desired data\nmanifold at each timestep, identifying that a larger time gap is associated\nwith reduced generation quality. We then design a novel guidance mechanism,\n`Temporal Alignment Guidance' (TAG), attracting the samples back to the desired\nmanifold at every timestep during generation. Through extensive experiments, we\ndemonstrate that TAG consistently produces samples closely aligned with the\ndesired manifold at each timestep, leading to significant improvements in\ngeneration quality across various downstream tasks.",
            "upvotes": 26,
            "discussionId": "68ef3928486b78128f0e356f",
            "organization": {
                "_id": "6475760c33192631bad2bb38",
                "name": "kaist-ai",
                "fullname": "KAIST AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
            }
        },
        "publishedAt": "2025-10-13T02:46:57.000Z",
        "title": "Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models",
        "summary": "Diffusion models have achieved remarkable success as generative models.\nHowever, even a well-trained model can accumulate errors throughout the\ngeneration process. These errors become particularly problematic when arbitrary\nguidance is applied to steer samples toward desired properties, which often\nbreaks sample fidelity. In this paper, we propose a general solution to address\nthe off-manifold phenomenon observed in diffusion models. Our approach\nleverages a time predictor to estimate deviations from the desired data\nmanifold at each timestep, identifying that a larger time gap is associated\nwith reduced generation quality. We then design a novel guidance mechanism,\n`Temporal Alignment Guidance' (TAG), attracting the samples back to the desired\nmanifold at every timestep during generation. Through extensive experiments, we\ndemonstrate that TAG consistently produces samples closely aligned with the\ndesired manifold at each timestep, leading to significant improvements in\ngeneration quality across various downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11057.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6602ca1e10a1441af41637be",
            "avatarUrl": "/avatars/5880e699def320beb352cbed77495b2f.svg",
            "fullname": "Sangmin Bae",
            "name": "raymin0223",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "6475760c33192631bad2bb38",
            "name": "kaist-ai",
            "fullname": "KAIST AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.12693",
            "authors": [
                {
                    "_id": "68ef0007486b78128f0e339c",
                    "user": {
                        "_id": "6700b1f93381f2db06857fb5",
                        "avatarUrl": "/avatars/c8b9ec7c00773c5a4055ba50de0c6b2f.svg",
                        "isPro": false,
                        "fullname": "Hanyang Chen",
                        "user": "Hanyang81",
                        "type": "user"
                    },
                    "name": "Hanyang Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T07:07:38.765Z",
                    "hidden": false
                },
                {
                    "_id": "68ef0007486b78128f0e339d",
                    "name": "Mark Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ef0007486b78128f0e339e",
                    "user": {
                        "_id": "64d45451c34a346181b130dd",
                        "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
                        "isPro": false,
                        "fullname": "Rui Yang",
                        "user": "Ray2333",
                        "type": "user"
                    },
                    "name": "Rui Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T03:12:56.270Z",
                    "hidden": false
                },
                {
                    "_id": "68ef0007486b78128f0e339f",
                    "name": "Qinwei Ma",
                    "hidden": false
                },
                {
                    "_id": "68ef0007486b78128f0e33a0",
                    "name": "Ke Yang",
                    "hidden": false
                },
                {
                    "_id": "68ef0007486b78128f0e33a1",
                    "name": "Jiarui Yao",
                    "hidden": false
                },
                {
                    "_id": "68ef0007486b78128f0e33a2",
                    "name": "Kangrui Wang",
                    "hidden": false
                },
                {
                    "_id": "68ef0007486b78128f0e33a3",
                    "name": "Hao Bai",
                    "hidden": false
                },
                {
                    "_id": "68ef0007486b78128f0e33a4",
                    "name": "Zhenhailong Wang",
                    "hidden": false
                },
                {
                    "_id": "68ef0007486b78128f0e33a5",
                    "name": "Rui Pan",
                    "hidden": false
                },
                {
                    "_id": "68ef0007486b78128f0e33a6",
                    "name": "Mengchao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ef0007486b78128f0e33a7",
                    "name": "Jose Barreiros",
                    "hidden": false
                },
                {
                    "_id": "68ef0007486b78128f0e33a8",
                    "name": "Aykut Onol",
                    "hidden": false
                },
                {
                    "_id": "68ef0007486b78128f0e33a9",
                    "name": "ChengXiang Zhai",
                    "hidden": false
                },
                {
                    "_id": "68ef0007486b78128f0e33aa",
                    "name": "Heng Ji",
                    "hidden": false
                },
                {
                    "_id": "68ef0007486b78128f0e33ab",
                    "name": "Manling Li",
                    "hidden": false
                },
                {
                    "_id": "68ef0007486b78128f0e33ac",
                    "name": "Huan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ef0007486b78128f0e33ad",
                    "name": "Tong Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T16:25:46.000Z",
            "submittedOnDailyAt": "2025-10-15T00:49:47.828Z",
            "title": "ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning\n  and Online Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "64d45451c34a346181b130dd",
                "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
                "isPro": false,
                "fullname": "Rui Yang",
                "user": "Ray2333",
                "type": "user"
            },
            "summary": "Recent advances in embodied AI highlight the potential of vision language\nmodels (VLMs) as agents capable of perception, reasoning, and interaction in\ncomplex environments. However, top-performing systems rely on large-scale\nmodels that are costly to deploy, while smaller VLMs lack the necessary\nknowledge and skills to succeed. To bridge this gap, we present\nEmbodied Reasoning Agent (ERA), a two-stage framework that integrates\nprior knowledge learning and online reinforcement learning (RL). The first\nstage, Embodied Prior Learning, distills foundational knowledge from\nthree types of data: (1) Trajectory-Augmented Priors, which enrich existing\ntrajectory data with structured reasoning generated by stronger models; (2)\nEnvironment-Anchored Priors, which provide in-environment knowledge and\ngrounding supervision; and (3) External Knowledge Priors, which transfer\ngeneral knowledge from out-of-environment datasets. In the second stage, we\ndevelop an online RL pipeline that builds on these priors to further enhance\nagent performance. To overcome the inherent challenges in agent RL, including\nlong horizons, sparse rewards, and training instability, we introduce three key\ndesigns: self-summarization for context management, dense reward shaping, and\nturn-level policy optimization. Extensive experiments on both high-level\nplanning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate\nthat ERA-3B surpasses both prompting-based large models and previous\ntraining-based baselines. Specifically, it achieves overall improvements of\n8.4\\% on EB-ALFRED and 19.4\\% on EB-Manipulation over GPT-4o, and exhibits\nstrong generalization to unseen tasks. Overall, ERA offers a practical path\ntoward scalable embodied intelligence, providing methodological insights for\nfuture embodied AI systems.",
            "upvotes": 20,
            "discussionId": "68ef0007486b78128f0e33ae",
            "projectPage": "https://embodied-reasoning-agent.github.io",
            "organization": {
                "_id": "65448bef5b5d9185ba3202b9",
                "name": "UIUC-CS",
                "fullname": "University of Illinois at Urbana-Champaign",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
            }
        },
        "publishedAt": "2025-10-14T12:25:46.000Z",
        "title": "ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning\n  and Online Reinforcement Learning",
        "summary": "Recent advances in embodied AI highlight the potential of vision language\nmodels (VLMs) as agents capable of perception, reasoning, and interaction in\ncomplex environments. However, top-performing systems rely on large-scale\nmodels that are costly to deploy, while smaller VLMs lack the necessary\nknowledge and skills to succeed. To bridge this gap, we present\nEmbodied Reasoning Agent (ERA), a two-stage framework that integrates\nprior knowledge learning and online reinforcement learning (RL). The first\nstage, Embodied Prior Learning, distills foundational knowledge from\nthree types of data: (1) Trajectory-Augmented Priors, which enrich existing\ntrajectory data with structured reasoning generated by stronger models; (2)\nEnvironment-Anchored Priors, which provide in-environment knowledge and\ngrounding supervision; and (3) External Knowledge Priors, which transfer\ngeneral knowledge from out-of-environment datasets. In the second stage, we\ndevelop an online RL pipeline that builds on these priors to further enhance\nagent performance. To overcome the inherent challenges in agent RL, including\nlong horizons, sparse rewards, and training instability, we introduce three key\ndesigns: self-summarization for context management, dense reward shaping, and\nturn-level policy optimization. Extensive experiments on both high-level\nplanning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate\nthat ERA-3B surpasses both prompting-based large models and previous\ntraining-based baselines. Specifically, it achieves overall improvements of\n8.4\\% on EB-ALFRED and 19.4\\% on EB-Manipulation over GPT-4o, and exhibits\nstrong generalization to unseen tasks. Overall, ERA offers a practical path\ntoward scalable embodied intelligence, providing methodological insights for\nfuture embodied AI systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12693.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d45451c34a346181b130dd",
            "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
            "fullname": "Rui Yang",
            "name": "Ray2333",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "organization": {
            "_id": "65448bef5b5d9185ba3202b9",
            "name": "UIUC-CS",
            "fullname": "University of Illinois at Urbana-Champaign",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.12784",
            "authors": [
                {
                    "_id": "68ef011a486b78128f0e33ce",
                    "user": {
                        "_id": "66608add236f958513d21d2e",
                        "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg",
                        "isPro": false,
                        "fullname": "Weiyang Jin",
                        "user": "Wayne-King",
                        "type": "user"
                    },
                    "name": "Weiyang Jin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T15:24:54.354Z",
                    "hidden": false
                },
                {
                    "_id": "68ef011a486b78128f0e33cf",
                    "name": "Yuwei Niu",
                    "hidden": false
                },
                {
                    "_id": "68ef011a486b78128f0e33d0",
                    "name": "Jiaqi Liao",
                    "hidden": false
                },
                {
                    "_id": "68ef011a486b78128f0e33d1",
                    "name": "Chengqi Duan",
                    "hidden": false
                },
                {
                    "_id": "68ef011a486b78128f0e33d2",
                    "name": "Aoxue Li",
                    "hidden": false
                },
                {
                    "_id": "68ef011a486b78128f0e33d3",
                    "name": "Shenghua Gao",
                    "hidden": false
                },
                {
                    "_id": "68ef011a486b78128f0e33d4",
                    "name": "Xihui Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66608add236f958513d21d2e/_qfb3S1n3HUhiypTBo-ax.png"
            ],
            "publishedAt": "2025-10-14T17:56:11.000Z",
            "submittedOnDailyAt": "2025-10-15T00:35:56.518Z",
            "title": "SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models",
            "submittedOnDailyBy": {
                "_id": "66608add236f958513d21d2e",
                "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg",
                "isPro": false,
                "fullname": "Weiyang Jin",
                "user": "Wayne-King",
                "type": "user"
            },
            "summary": "Recently, remarkable progress has been made in Unified Multimodal Models\n(UMMs), which integrate vision-language generation and understanding\ncapabilities within a single framework. However, a significant gap exists where\na model's strong visual understanding often fails to transfer to its visual\ngeneration. A model might correctly understand an image based on user\ninstructions, yet be unable to generate a faithful image from text prompts.\nThis phenomenon directly raises a compelling question: Can a model achieve\nself-improvement by using its understanding module to reward its generation\nmodule? To bridge this gap and achieve self-improvement, we introduce SRUM, a\nself-rewarding post-training framework that can be directly applied to existing\nUMMs of various designs. SRUM creates a feedback loop where the model's own\nunderstanding module acts as an internal ``evaluator'', providing corrective\nsignals to improve its generation module, without requiring additional\nhuman-labeled data. To ensure this feedback is comprehensive, we designed a\nglobal-local dual reward system. To tackle the inherent structural complexity\nof images, this system offers multi-scale guidance: a global reward\nensures the correctness of the overall visual semantics and layout, while a\nlocal reward refines fine-grained, object-level fidelity. SRUM leads\nto powerful capabilities and shows strong generalization, boosting performance\non T2I-CompBench from 82.18 to 88.37 and on T2I-ReasonBench from 43.82\nto 46.75. Overall, our work establishes a powerful new paradigm for\nenabling a UMMs' understanding module to guide and enhance its own generation\nvia self-rewarding.",
            "upvotes": 15,
            "discussionId": "68ef011a486b78128f0e33d5",
            "projectPage": "https://waynejin0918.github.io/srum_web/",
            "githubRepo": "https://github.com/WayneJin0918/SRUM",
            "githubStars": 29,
            "organization": {
                "_id": "67ea9ecfc234715db8dbf339",
                "name": "hkuhk",
                "fullname": "The University of Hong Kong",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
            }
        },
        "publishedAt": "2025-10-14T13:56:11.000Z",
        "title": "SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models",
        "summary": "Recently, remarkable progress has been made in Unified Multimodal Models\n(UMMs), which integrate vision-language generation and understanding\ncapabilities within a single framework. However, a significant gap exists where\na model's strong visual understanding often fails to transfer to its visual\ngeneration. A model might correctly understand an image based on user\ninstructions, yet be unable to generate a faithful image from text prompts.\nThis phenomenon directly raises a compelling question: Can a model achieve\nself-improvement by using its understanding module to reward its generation\nmodule? To bridge this gap and achieve self-improvement, we introduce SRUM, a\nself-rewarding post-training framework that can be directly applied to existing\nUMMs of various designs. SRUM creates a feedback loop where the model's own\nunderstanding module acts as an internal ``evaluator'', providing corrective\nsignals to improve its generation module, without requiring additional\nhuman-labeled data. To ensure this feedback is comprehensive, we designed a\nglobal-local dual reward system. To tackle the inherent structural complexity\nof images, this system offers multi-scale guidance: a global reward\nensures the correctness of the overall visual semantics and layout, while a\nlocal reward refines fine-grained, object-level fidelity. SRUM leads\nto powerful capabilities and shows strong generalization, boosting performance\non T2I-CompBench from 82.18 to 88.37 and on T2I-ReasonBench from 43.82\nto 46.75. Overall, our work establishes a powerful new paradigm for\nenabling a UMMs' understanding module to guide and enhance its own generation\nvia self-rewarding.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66608add236f958513d21d2e/_qfb3S1n3HUhiypTBo-ax.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12784.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66608add236f958513d21d2e",
            "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg",
            "fullname": "Weiyang Jin",
            "name": "Wayne-King",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "67ea9ecfc234715db8dbf339",
            "name": "hkuhk",
            "fullname": "The University of Hong Kong",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.12789",
            "authors": [
                {
                    "_id": "68eefdae486b78128f0e336d",
                    "name": "Kevin Li",
                    "hidden": false
                },
                {
                    "_id": "68eefdae486b78128f0e336e",
                    "user": {
                        "_id": "62fa1d95e8c9c532aa75331c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
                        "isPro": false,
                        "fullname": "Manuel Brack",
                        "user": "mbrack",
                        "type": "user"
                    },
                    "name": "Manuel Brack",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T07:07:40.869Z",
                    "hidden": false
                },
                {
                    "_id": "68eefdae486b78128f0e336f",
                    "name": "Sudeep Katakol",
                    "hidden": false
                },
                {
                    "_id": "68eefdae486b78128f0e3370",
                    "name": "Hareesh Ravi",
                    "hidden": false
                },
                {
                    "_id": "68eefdae486b78128f0e3371",
                    "name": "Ajinkya Kale",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T17:57:56.000Z",
            "submittedOnDailyAt": "2025-10-15T00:19:45.469Z",
            "title": "UniFusion: Vision-Language Model as Unified Encoder in Image Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Although recent advances in visual generation have been remarkable, most\nexisting architectures still depend on distinct encoders for images and text.\nThis separation constrains diffusion models' ability to perform cross-modal\nreasoning and knowledge transfer. Prior attempts to bridge this gap often use\nthe last layer information from VLM, employ multiple visual encoders, or train\nlarge unified models jointly for text and image generation, which demands\nsubstantial computational resources and large-scale data, limiting its\naccessibility.We present UniFusion, a diffusion-based generative model\nconditioned on a frozen large vision-language model (VLM) that serves as a\nunified multimodal encoder. At the core of UniFusion is the Layerwise Attention\nPooling (LAP) mechanism that extracts both high level semantics and low level\ndetails from text and visual tokens of a frozen VLM to condition a diffusion\ngenerative model. We demonstrate that LAP outperforms other shallow fusion\narchitectures on text-image alignment for generation and faithful transfer of\nvisual information from VLM to the diffusion model which is key for editing. We\npropose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),\nwhich conditions a diffusion transformer (DiT) only on the text tokens\ngenerated by the VLM during in-model prompt rewriting. VERIFI combines the\nalignment of the conditioning distribution with the VLM's reasoning\ncapabilities for increased capabilities and flexibility at inference. In\naddition, finetuning on editing task not only improves text-image alignment for\ngeneration, indicative of cross-modality knowledge transfer, but also exhibits\ntremendous generalization capabilities. Our model when trained on single image\nediting, zero-shot generalizes to multiple image references further motivating\nthe unified encoder design of UniFusion.",
            "upvotes": 14,
            "discussionId": "68eefdae486b78128f0e3372",
            "projectPage": "https://thekevinli.github.io/unifusion/",
            "organization": {
                "_id": "61e5d14f77496de0a6d95c6b",
                "name": "adobe",
                "fullname": "Adobe",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
            }
        },
        "publishedAt": "2025-10-14T13:57:56.000Z",
        "title": "UniFusion: Vision-Language Model as Unified Encoder in Image Generation",
        "summary": "Although recent advances in visual generation have been remarkable, most\nexisting architectures still depend on distinct encoders for images and text.\nThis separation constrains diffusion models' ability to perform cross-modal\nreasoning and knowledge transfer. Prior attempts to bridge this gap often use\nthe last layer information from VLM, employ multiple visual encoders, or train\nlarge unified models jointly for text and image generation, which demands\nsubstantial computational resources and large-scale data, limiting its\naccessibility.We present UniFusion, a diffusion-based generative model\nconditioned on a frozen large vision-language model (VLM) that serves as a\nunified multimodal encoder. At the core of UniFusion is the Layerwise Attention\nPooling (LAP) mechanism that extracts both high level semantics and low level\ndetails from text and visual tokens of a frozen VLM to condition a diffusion\ngenerative model. We demonstrate that LAP outperforms other shallow fusion\narchitectures on text-image alignment for generation and faithful transfer of\nvisual information from VLM to the diffusion model which is key for editing. We\npropose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),\nwhich conditions a diffusion transformer (DiT) only on the text tokens\ngenerated by the VLM during in-model prompt rewriting. VERIFI combines the\nalignment of the conditioning distribution with the VLM's reasoning\ncapabilities for increased capabilities and flexibility at inference. In\naddition, finetuning on editing task not only improves text-image alignment for\ngeneration, indicative of cross-modality knowledge transfer, but also exhibits\ntremendous generalization capabilities. Our model when trained on single image\nediting, zero-shot generalizes to multiple image references further motivating\nthe unified encoder design of UniFusion.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12789.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 127
        },
        "organization": {
            "_id": "61e5d14f77496de0a6d95c6b",
            "name": "adobe",
            "fullname": "Adobe",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11602",
            "authors": [
                {
                    "_id": "68ee24d79b77b5223f666143",
                    "name": "Huiyin Xue",
                    "hidden": false
                },
                {
                    "_id": "68ee24d79b77b5223f666144",
                    "name": "Nafise Sadat Moosavi",
                    "hidden": false
                },
                {
                    "_id": "68ee24d79b77b5223f666145",
                    "name": "Nikolaos Aletras",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T16:42:14.000Z",
            "submittedOnDailyAt": "2025-10-15T13:20:53.740Z",
            "title": "Deconstructing Attention: Investigating Design Principles for Effective\n  Language Modeling",
            "submittedOnDailyBy": {
                "_id": "643d0a4d8a55b2bbf4f2a90e",
                "avatarUrl": "/avatars/9534aaf81cbf12f015c6826b682fdb84.svg",
                "isPro": false,
                "fullname": "Xingwei Tan",
                "user": "XingweiT",
                "type": "user"
            },
            "summary": "The success of Transformer language models is widely credited to their\ndot-product attention mechanism, which interweaves a set of key design\nprinciples: mixing information across positions (enabling multi-token\ninteractions), sequence-dependent activations (where attention weights adapt to\neach input), a specific mathematical form (dot-product similarities plus\nsoftmax weighting), and coupling of queries and keys to evolving hidden states\n(grounding attention in the current layer). However, the necessity of each of\nthese principles remains largely untested. In this work, we systematically\ndeconstruct attention by designing controlled variants that selectively relax\nthese principles, applied both uniformly across all layers and in hybrid\narchitectures where only some layers retain standard attention. Our empirical\nanalysis reveals that mechanisms for mixing tokens are indispensable, as their\nabsence collapses models to near-random behavior, while the exact mathematical\nform and sequence dependency can be substantially relaxed, especially when\npreserved in just a subset of layers. Surprisingly, even variants that fail in\nisolation can achieve robust performance when interleaved with standard\nattention, highlighting a cooperative effect. These findings deepen our\nunderstanding of what truly underpins attention's effectiveness and open new\navenues for simplifying language models without sacrificing performance.",
            "upvotes": 13,
            "discussionId": "68ee24d79b77b5223f666146",
            "ai_summary": "Systematic analysis of attention mechanisms in Transformer models shows that token mixing is essential, while other aspects like sequence dependency and mathematical form can be relaxed or interleaved to maintain performance.",
            "ai_keywords": [
                "Transformer language models",
                "dot-product attention mechanism",
                "key design principles",
                "multi-token interactions",
                "sequence-dependent activations",
                "dot-product similarities",
                "softmax weighting",
                "queries",
                "keys",
                "hidden states",
                "controlled variants",
                "hybrid architectures",
                "empirical analysis",
                "token mixing",
                "cooperative effect"
            ]
        },
        "publishedAt": "2025-10-13T12:42:14.000Z",
        "title": "Deconstructing Attention: Investigating Design Principles for Effective\n  Language Modeling",
        "summary": "The success of Transformer language models is widely credited to their\ndot-product attention mechanism, which interweaves a set of key design\nprinciples: mixing information across positions (enabling multi-token\ninteractions), sequence-dependent activations (where attention weights adapt to\neach input), a specific mathematical form (dot-product similarities plus\nsoftmax weighting), and coupling of queries and keys to evolving hidden states\n(grounding attention in the current layer). However, the necessity of each of\nthese principles remains largely untested. In this work, we systematically\ndeconstruct attention by designing controlled variants that selectively relax\nthese principles, applied both uniformly across all layers and in hybrid\narchitectures where only some layers retain standard attention. Our empirical\nanalysis reveals that mechanisms for mixing tokens are indispensable, as their\nabsence collapses models to near-random behavior, while the exact mathematical\nform and sequence dependency can be substantially relaxed, especially when\npreserved in just a subset of layers. Surprisingly, even variants that fail in\nisolation can achieve robust performance when interleaved with standard\nattention, highlighting a cooperative effect. These findings deepen our\nunderstanding of what truly underpins attention's effectiveness and open new\navenues for simplifying language models without sacrificing performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11602.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643d0a4d8a55b2bbf4f2a90e",
            "avatarUrl": "/avatars/9534aaf81cbf12f015c6826b682fdb84.svg",
            "fullname": "Xingwei Tan",
            "name": "XingweiT",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.12635",
            "authors": [
                {
                    "_id": "68eef8ab486b78128f0e3304",
                    "user": {
                        "_id": "645b4a2978730bcc103dfe4d",
                        "avatarUrl": "/avatars/de544de899897fd0a83506ff287123bc.svg",
                        "isPro": false,
                        "fullname": "Yuxiang Zhang",
                        "user": "TokerZ",
                        "type": "user"
                    },
                    "name": "Yuxiang Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T03:13:02.636Z",
                    "hidden": false
                },
                {
                    "_id": "68eef8ab486b78128f0e3305",
                    "name": "Jiangming Shu",
                    "hidden": false
                },
                {
                    "_id": "68eef8ab486b78128f0e3306",
                    "name": "Ye Ma",
                    "hidden": false
                },
                {
                    "_id": "68eef8ab486b78128f0e3307",
                    "name": "Xueyuan Lin",
                    "hidden": false
                },
                {
                    "_id": "68eef8ab486b78128f0e3308",
                    "user": {
                        "_id": "665ace40ddba0f825f3a4941",
                        "avatarUrl": "/avatars/ed7e209e14b5ae495aa7f4baec980b38.svg",
                        "isPro": false,
                        "fullname": "Shangxi Wu",
                        "user": "KirinNg",
                        "type": "user"
                    },
                    "name": "Shangxi Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T07:07:51.674Z",
                    "hidden": false
                },
                {
                    "_id": "68eef8ab486b78128f0e3309",
                    "name": "Jitao Sang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T15:29:57.000Z",
            "submittedOnDailyAt": "2025-10-15T00:24:53.390Z",
            "title": "Memory as Action: Autonomous Context Curation for Long-Horizon Agentic\n  Tasks",
            "submittedOnDailyBy": {
                "_id": "645b4a2978730bcc103dfe4d",
                "avatarUrl": "/avatars/de544de899897fd0a83506ff287123bc.svg",
                "isPro": false,
                "fullname": "Yuxiang Zhang",
                "user": "TokerZ",
                "type": "user"
            },
            "summary": "Large Language Models face challenges in long-horizon agentic tasks as their\nconstrained memory is easily overwhelmed by distracting or irrelevant context.\nExisting working memory methods typically rely on external, heuristic\nmechanisms that are decoupled from the agent's core policy. In this work, we\nreframe working memory management as a learnable, intrinsic capability. We\npropose a novel framework, Memory-as-Action, where an agent actively manages\nits working memory by executing explicit editing operations as part of a\nunified policy. This formulation allows an agent, trained via reinforcement\nlearning, to balance memory curation against long-term task objectives under\ngiven resource constraints. However, such memory editing actions break the\nstandard assumption of a continuously growing prefix in LLM interactions,\nleading to what we call trajectory fractures. These non-prefix changes disrupt\nthe causal continuity required by standard policy gradient methods, making\nthose methods inapplicable. To address this, we propose a new algorithm,\nDynamic Context Policy Optimization, which enables stable end-to-end\nreinforcement learning by segmenting trajectories at memory action points and\napplying trajectory-level advantages to the resulting action segments. Our\nresults demonstrate that jointly optimizing for task reasoning and memory\nmanagement in an end-to-end fashion not only reduces overall computational\nconsumption but also improves task performance, driven by adaptive context\ncuration strategies tailored to the model's intrinsic capabilities.",
            "upvotes": 12,
            "discussionId": "68eef8ab486b78128f0e330a"
        },
        "publishedAt": "2025-10-14T11:29:57.000Z",
        "title": "Memory as Action: Autonomous Context Curation for Long-Horizon Agentic\n  Tasks",
        "summary": "Large Language Models face challenges in long-horizon agentic tasks as their\nconstrained memory is easily overwhelmed by distracting or irrelevant context.\nExisting working memory methods typically rely on external, heuristic\nmechanisms that are decoupled from the agent's core policy. In this work, we\nreframe working memory management as a learnable, intrinsic capability. We\npropose a novel framework, Memory-as-Action, where an agent actively manages\nits working memory by executing explicit editing operations as part of a\nunified policy. This formulation allows an agent, trained via reinforcement\nlearning, to balance memory curation against long-term task objectives under\ngiven resource constraints. However, such memory editing actions break the\nstandard assumption of a continuously growing prefix in LLM interactions,\nleading to what we call trajectory fractures. These non-prefix changes disrupt\nthe causal continuity required by standard policy gradient methods, making\nthose methods inapplicable. To address this, we propose a new algorithm,\nDynamic Context Policy Optimization, which enables stable end-to-end\nreinforcement learning by segmenting trajectories at memory action points and\napplying trajectory-level advantages to the resulting action segments. Our\nresults demonstrate that jointly optimizing for task reasoning and memory\nmanagement in an end-to-end fashion not only reduces overall computational\nconsumption but also improves task performance, driven by adaptive context\ncuration strategies tailored to the model's intrinsic capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12635.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645b4a2978730bcc103dfe4d",
            "avatarUrl": "/avatars/de544de899897fd0a83506ff287123bc.svg",
            "fullname": "Yuxiang Zhang",
            "name": "TokerZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.11683",
            "authors": [
                {
                    "_id": "68edd3f6de1fee572713a979",
                    "name": "Nianyi Lin",
                    "hidden": false
                },
                {
                    "_id": "68edd3f6de1fee572713a97a",
                    "name": "Jiajie Zhang",
                    "hidden": false
                },
                {
                    "_id": "68edd3f6de1fee572713a97b",
                    "name": "Lei Hou",
                    "hidden": false
                },
                {
                    "_id": "68edd3f6de1fee572713a97c",
                    "name": "Juanzi Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T17:47:50.000Z",
            "submittedOnDailyAt": "2025-10-15T00:52:19.998Z",
            "title": "Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion\n  Large Language Models",
            "submittedOnDailyBy": {
                "_id": "66cdd285c51a915bd5f2d017",
                "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
                "isPro": false,
                "fullname": "Jiajie Zhang",
                "user": "NeoZ123",
                "type": "user"
            },
            "summary": "A key challenge in applying reinforcement learning (RL) to diffusion large\nlanguage models (dLLMs) lies in the intractability of their likelihood\nfunctions, which are essential for the RL objective, necessitating\ncorresponding approximation in each training step. While existing methods\napproximate the log-likelihoods by their evidence lower bounds (ELBOs) via\ncustomized Monte Carlo (MC) sampling, the forward computational graphs of all\nMC samples need to be retained for the gradient computation of non-linear terms\nin the RL objective, resulting in significant memory overhead. This constraint\nrestricts feasible sample sizes, leading to imprecise likelihood approximations\nand ultimately distorting the RL objective. To overcome this limitation, we\npropose Boundary-Guided Policy Optimization (BGPO), a memory-efficient\nRL algorithm that maximizes a specially constructed lower bound of the\nELBO-based objective. This lower bound is carefully designed to satisfy two key\nproperties: (1) Linearity: it is formulated in a linear sum where each term\ndepends only on a single MC sample, thereby enabling gradient accumulation\nacross samples and ensuring constant memory usage; (2) Equivalence: Both the\nvalue and gradient of this lower bound are equal to those of the ELBO-based\nobjective in on-policy training, making it also an effective approximation for\nthe original RL objective. These properties allow BGPO to adopt a large MC\nsample size, resulting in more accurate likelihood approximations and improved\nRL objective estimation, which in turn leads to enhanced performance.\nExperiments show that BGPO significantly outperforms previous RL algorithms for\ndLLMs in math problem solving, code generation, and planning tasks.",
            "upvotes": 12,
            "discussionId": "68edd3f6de1fee572713a97d",
            "githubRepo": "https://github.com/THU-KEG/BGPO",
            "ai_summary": "Boundary-Guided Policy Optimization (BGPO) improves reinforcement learning for diffusion large language models by efficiently approximating likelihoods with a memory-efficient lower bound, enhancing performance in tasks like math problem solving, code generation, and planning.",
            "ai_keywords": [
                "reinforcement learning",
                "diffusion large language models",
                "likelihood functions",
                "evidence lower bounds",
                "Monte Carlo sampling",
                "gradient computation",
                "memory overhead",
                "Boundary-Guided Policy Optimization",
                "on-policy training"
            ],
            "githubStars": 5,
            "organization": {
                "_id": "62ad27f19096e7f9ecb1853a",
                "name": "zai-org",
                "fullname": "Z.ai",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
            }
        },
        "publishedAt": "2025-10-13T13:47:50.000Z",
        "title": "Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion\n  Large Language Models",
        "summary": "A key challenge in applying reinforcement learning (RL) to diffusion large\nlanguage models (dLLMs) lies in the intractability of their likelihood\nfunctions, which are essential for the RL objective, necessitating\ncorresponding approximation in each training step. While existing methods\napproximate the log-likelihoods by their evidence lower bounds (ELBOs) via\ncustomized Monte Carlo (MC) sampling, the forward computational graphs of all\nMC samples need to be retained for the gradient computation of non-linear terms\nin the RL objective, resulting in significant memory overhead. This constraint\nrestricts feasible sample sizes, leading to imprecise likelihood approximations\nand ultimately distorting the RL objective. To overcome this limitation, we\npropose Boundary-Guided Policy Optimization (BGPO), a memory-efficient\nRL algorithm that maximizes a specially constructed lower bound of the\nELBO-based objective. This lower bound is carefully designed to satisfy two key\nproperties: (1) Linearity: it is formulated in a linear sum where each term\ndepends only on a single MC sample, thereby enabling gradient accumulation\nacross samples and ensuring constant memory usage; (2) Equivalence: Both the\nvalue and gradient of this lower bound are equal to those of the ELBO-based\nobjective in on-policy training, making it also an effective approximation for\nthe original RL objective. These properties allow BGPO to adopt a large MC\nsample size, resulting in more accurate likelihood approximations and improved\nRL objective estimation, which in turn leads to enhanced performance.\nExperiments show that BGPO significantly outperforms previous RL algorithms for\ndLLMs in math problem solving, code generation, and planning tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11683.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66cdd285c51a915bd5f2d017",
            "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
            "fullname": "Jiajie Zhang",
            "name": "NeoZ123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "62ad27f19096e7f9ecb1853a",
            "name": "zai-org",
            "fullname": "Z.ai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.01171",
            "authors": [
                {
                    "_id": "68de624770ada21878c7500c",
                    "name": "Jiayi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68de624770ada21878c7500d",
                    "user": {
                        "_id": "636681feaa6a4af6073ba73e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636681feaa6a4af6073ba73e/u_0moYNu6as9Sszp-ej95.png",
                        "isPro": true,
                        "fullname": "Simon Yu",
                        "user": "simonycl",
                        "type": "user"
                    },
                    "name": "Simon Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T15:27:10.468Z",
                    "hidden": false
                },
                {
                    "_id": "68de624770ada21878c7500e",
                    "user": {
                        "_id": "62813beb2a08e33da8684bec",
                        "avatarUrl": "/avatars/d2be5cf9bda833b56223caea7e79b4c0.svg",
                        "isPro": false,
                        "fullname": "Derek",
                        "user": "dcx",
                        "type": "user"
                    },
                    "name": "Derek Chong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-03T12:52:58.465Z",
                    "hidden": false
                },
                {
                    "_id": "68de624770ada21878c7500f",
                    "name": "Anthony Sicilia",
                    "hidden": false
                },
                {
                    "_id": "68de624770ada21878c75010",
                    "name": "Michael R. Tomz",
                    "hidden": false
                },
                {
                    "_id": "68de624770ada21878c75011",
                    "name": "Christopher D. Manning",
                    "hidden": false
                },
                {
                    "_id": "68de624770ada21878c75012",
                    "name": "Weiyan Shi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-01T17:55:37.000Z",
            "submittedOnDailyAt": "2025-10-15T12:18:04.399Z",
            "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM\n  Diversity",
            "submittedOnDailyBy": {
                "_id": "636681feaa6a4af6073ba73e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636681feaa6a4af6073ba73e/u_0moYNu6as9Sszp-ej95.png",
                "isPro": true,
                "fullname": "Simon Yu",
                "user": "simonycl",
                "type": "user"
            },
            "summary": "Post-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse. Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data, whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse.\nMotivated by this analysis, we introduce Verbalized Sampling, a simple,\ntraining-free prompting strategy to circumvent mode collapse. VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n``Generate 5 jokes about coffee and their corresponding probabilities'').\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation,\nopen-ended QA, and synthetic data generation, without sacrificing factual\naccuracy and safety. For instance, in creative writing, VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity.",
            "upvotes": 11,
            "discussionId": "68de624770ada21878c75013",
            "projectPage": "https://simonucl.notion.site/verbalized-sampling",
            "githubRepo": "https://github.com/CHATS-lab/verbalized-sampling",
            "ai_summary": "Typicality bias in preference data causes mode collapse in LLMs, and Verbalized Sampling is introduced as a prompting strategy to enhance diversity without compromising accuracy or safety.",
            "ai_keywords": [
                "LLM",
                "mode collapse",
                "typicality bias",
                "preference data",
                "cognitive psychology",
                "Verbalized Sampling",
                "creative writing",
                "dialogue simulation",
                "open-ended QA",
                "synthetic data generation",
                "factual accuracy",
                "safety"
            ],
            "githubStars": 51,
            "organization": {
                "_id": "6112d84f8c2e1f4060908c9e",
                "name": "stanfordnlp",
                "fullname": "Stanford NLP",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1628624969199-6032802e1f993496bc14d9e3.png"
            }
        },
        "publishedAt": "2025-10-01T13:55:37.000Z",
        "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM\n  Diversity",
        "summary": "Post-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse. Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data, whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse.\nMotivated by this analysis, we introduce Verbalized Sampling, a simple,\ntraining-free prompting strategy to circumvent mode collapse. VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n``Generate 5 jokes about coffee and their corresponding probabilities'').\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation,\nopen-ended QA, and synthetic data generation, without sacrificing factual\naccuracy and safety. For instance, in creative writing, VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01171.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "636681feaa6a4af6073ba73e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636681feaa6a4af6073ba73e/u_0moYNu6as9Sszp-ej95.png",
            "fullname": "Simon Yu",
            "name": "simonycl",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "6112d84f8c2e1f4060908c9e",
            "name": "stanfordnlp",
            "fullname": "Stanford NLP",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1628624969199-6032802e1f993496bc14d9e3.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.12225",
            "authors": [
                {
                    "_id": "68ef03f8486b78128f0e33ee",
                    "name": "Hritik Bansal",
                    "hidden": false
                },
                {
                    "_id": "68ef03f8486b78128f0e33ef",
                    "name": "Devandra Singh Sachan",
                    "hidden": false
                },
                {
                    "_id": "68ef03f8486b78128f0e33f0",
                    "name": "Kai-Wei Chang",
                    "hidden": false
                },
                {
                    "_id": "68ef03f8486b78128f0e33f1",
                    "name": "Aditya Grover",
                    "hidden": false
                },
                {
                    "_id": "68ef03f8486b78128f0e33f2",
                    "name": "Gargi Ghosh",
                    "hidden": false
                },
                {
                    "_id": "68ef03f8486b78128f0e33f3",
                    "name": "Wen-tau Yih",
                    "hidden": false
                },
                {
                    "_id": "68ef03f8486b78128f0e33f4",
                    "name": "Ramakanth Pasunuru",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T07:23:44.000Z",
            "submittedOnDailyAt": "2025-10-15T00:47:46.720Z",
            "title": "HoneyBee: Data Recipes for Vision-Language Reasoners",
            "submittedOnDailyBy": {
                "_id": "61c5c25705aa54027c52f7b3",
                "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
                "isPro": false,
                "fullname": "Hritik Bansal",
                "user": "hbXNov",
                "type": "user"
            },
            "summary": "Recent advances in vision-language models (VLMs) have made them highly\neffective at reasoning tasks. However, the principles underlying the\nconstruction of performant VL reasoning training datasets remain poorly\nunderstood. In this work, we introduce several data curation approaches and\nstudy their impacts on VL reasoning capabilities by carefully controlling\ntraining and evaluation setups. We analyze the effects of context (image and\nquestion pair) sources, implement targeted data interventions, and explore\nscaling up images, questions, and chain-of-thought (CoT) solutions. Our\nfindings reveal that (a) context source strategies significantly affect VLM\nperformance, (b) interventions such as auxiliary signals from image captions\nand the inclusion of text-only reasoning yield substantial gains, and (c)\nscaling all data dimensions (e.g., unique questions per image and unique CoTs\nper image-question pair) consistently improves reasoning capability. Motivated\nby these insights, we introduce HoneyBee, a large-scale, high-quality CoT\nreasoning dataset with 2.5M examples consisting 350K image-question pairs. VLMs\ntrained with HoneyBee outperform state-of-the-art models across model sizes.\nFor instance, a HoneyBee-trained VLM with 3B parameters outperforms the SOTA\nmodel and the base model by 7.8% and 24.8%, respectively, on MathVerse.\nFurthermore, we propose a test-time scaling strategy that reduces decoding cost\nby 73% without sacrificing accuracy. Overall, this work presents improved\nstrategies for VL reasoning dataset curation research.",
            "upvotes": 9,
            "discussionId": "68ef03f9486b78128f0e33f5",
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "publishedAt": "2025-10-14T03:23:44.000Z",
        "title": "HoneyBee: Data Recipes for Vision-Language Reasoners",
        "summary": "Recent advances in vision-language models (VLMs) have made them highly\neffective at reasoning tasks. However, the principles underlying the\nconstruction of performant VL reasoning training datasets remain poorly\nunderstood. In this work, we introduce several data curation approaches and\nstudy their impacts on VL reasoning capabilities by carefully controlling\ntraining and evaluation setups. We analyze the effects of context (image and\nquestion pair) sources, implement targeted data interventions, and explore\nscaling up images, questions, and chain-of-thought (CoT) solutions. Our\nfindings reveal that (a) context source strategies significantly affect VLM\nperformance, (b) interventions such as auxiliary signals from image captions\nand the inclusion of text-only reasoning yield substantial gains, and (c)\nscaling all data dimensions (e.g., unique questions per image and unique CoTs\nper image-question pair) consistently improves reasoning capability. Motivated\nby these insights, we introduce HoneyBee, a large-scale, high-quality CoT\nreasoning dataset with 2.5M examples consisting 350K image-question pairs. VLMs\ntrained with HoneyBee outperform state-of-the-art models across model sizes.\nFor instance, a HoneyBee-trained VLM with 3B parameters outperforms the SOTA\nmodel and the base model by 7.8% and 24.8%, respectively, on MathVerse.\nFurthermore, we propose a test-time scaling strategy that reduces decoding cost\nby 73% without sacrificing accuracy. Overall, this work presents improved\nstrategies for VL reasoning dataset curation research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12225.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61c5c25705aa54027c52f7b3",
            "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
            "fullname": "Hritik Bansal",
            "name": "hbXNov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.12801",
            "authors": [
                {
                    "_id": "68eefd3a486b78128f0e3361",
                    "user": {
                        "_id": "65f92935ee1763d31ae255c1",
                        "avatarUrl": "/avatars/94c6b36e5c42b60813d3b557e3947fa7.svg",
                        "isPro": false,
                        "fullname": "Kartik Narayan",
                        "user": "kartiknarayan",
                        "type": "user"
                    },
                    "name": "Kartik Narayan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T17:03:48.916Z",
                    "hidden": false
                },
                {
                    "_id": "68eefd3a486b78128f0e3362",
                    "name": "Yang Xu",
                    "hidden": false
                },
                {
                    "_id": "68eefd3a486b78128f0e3363",
                    "name": "Tian Cao",
                    "hidden": false
                },
                {
                    "_id": "68eefd3a486b78128f0e3364",
                    "name": "Kavya Nerella",
                    "hidden": false
                },
                {
                    "_id": "68eefd3a486b78128f0e3365",
                    "name": "Vishal M. Patel",
                    "hidden": false
                },
                {
                    "_id": "68eefd3a486b78128f0e3366",
                    "name": "Navid Shiee",
                    "hidden": false
                },
                {
                    "_id": "68eefd3a486b78128f0e3367",
                    "name": "Peter Grasch",
                    "hidden": false
                },
                {
                    "_id": "68eefd3a486b78128f0e3368",
                    "name": "Chao Jia",
                    "hidden": false
                },
                {
                    "_id": "68eefd3a486b78128f0e3369",
                    "name": "Yinfei Yang",
                    "hidden": false
                },
                {
                    "_id": "68eefd3a486b78128f0e336a",
                    "name": "Zhe Gan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T17:59:58.000Z",
            "submittedOnDailyAt": "2025-10-15T00:17:51.493Z",
            "title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) in real-world applications require\naccess to external knowledge sources and must remain responsive to the dynamic\nand ever-changing real-world information in order to address\ninformation-seeking and knowledge-intensive user queries. Existing approaches,\nsuch as retrieval augmented generation (RAG) methods, search agents, and search\nequipped MLLMs, often suffer from rigid pipelines, excessive search calls, and\npoorly constructed search queries, which result in inefficiencies and\nsuboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,\nthe first multimodal LLM capable of performing on-demand, multi-turn web\nsearches and dynamically crafting queries for both image and text search tools.\nSpecifically, DeepMMSearch-R1 can initiate web searches based on relevant crops\nof the input image making the image search more effective, and can iteratively\nadapt text search queries based on retrieved information, thereby enabling\nself-reflection and self-correction. Our approach relies on a two-stage\ntraining pipeline: a cold start supervised finetuning phase followed by an\nonline reinforcement learning optimization. For training, we introduce\nDeepMMSearchVQA, a novel multimodal VQA dataset created through an automated\npipeline intermixed with real-world information from web search tools. This\ndataset contains diverse, multi-hop queries that integrate textual and visual\ninformation, teaching the model when to search, what to search for, which\nsearch tool to use and how to reason over the retrieved information. We conduct\nextensive experiments across a range of knowledge-intensive benchmarks to\ndemonstrate the superiority of our approach. Finally, we analyze the results\nand provide insights that are valuable for advancing multimodal web-search.",
            "upvotes": 8,
            "discussionId": "68eefd3a486b78128f0e336b",
            "organization": {
                "_id": "628cbd99ef14f971b69948ab",
                "name": "apple",
                "fullname": "Apple",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
            }
        },
        "publishedAt": "2025-10-14T13:59:58.000Z",
        "title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search",
        "summary": "Multimodal Large Language Models (MLLMs) in real-world applications require\naccess to external knowledge sources and must remain responsive to the dynamic\nand ever-changing real-world information in order to address\ninformation-seeking and knowledge-intensive user queries. Existing approaches,\nsuch as retrieval augmented generation (RAG) methods, search agents, and search\nequipped MLLMs, often suffer from rigid pipelines, excessive search calls, and\npoorly constructed search queries, which result in inefficiencies and\nsuboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,\nthe first multimodal LLM capable of performing on-demand, multi-turn web\nsearches and dynamically crafting queries for both image and text search tools.\nSpecifically, DeepMMSearch-R1 can initiate web searches based on relevant crops\nof the input image making the image search more effective, and can iteratively\nadapt text search queries based on retrieved information, thereby enabling\nself-reflection and self-correction. Our approach relies on a two-stage\ntraining pipeline: a cold start supervised finetuning phase followed by an\nonline reinforcement learning optimization. For training, we introduce\nDeepMMSearchVQA, a novel multimodal VQA dataset created through an automated\npipeline intermixed with real-world information from web search tools. This\ndataset contains diverse, multi-hop queries that integrate textual and visual\ninformation, teaching the model when to search, what to search for, which\nsearch tool to use and how to reason over the retrieved information. We conduct\nextensive experiments across a range of knowledge-intensive benchmarks to\ndemonstrate the superiority of our approach. Finally, we analyze the results\nand provide insights that are valuable for advancing multimodal web-search.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12801.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 127
        },
        "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.12709",
            "authors": [
                {
                    "_id": "68ef0081486b78128f0e33ba",
                    "name": "Lin Lin",
                    "hidden": false
                },
                {
                    "_id": "68ef0081486b78128f0e33bb",
                    "name": "Jiefeng Long",
                    "hidden": false
                },
                {
                    "_id": "68ef0081486b78128f0e33bc",
                    "name": "Zhihe Wan",
                    "hidden": false
                },
                {
                    "_id": "68ef0081486b78128f0e33bd",
                    "name": "Yuchi Wang",
                    "hidden": false
                },
                {
                    "_id": "68ef0081486b78128f0e33be",
                    "name": "Dingkang Yang",
                    "hidden": false
                },
                {
                    "_id": "68ef0081486b78128f0e33bf",
                    "name": "Shuang Yang",
                    "hidden": false
                },
                {
                    "_id": "68ef0081486b78128f0e33c0",
                    "name": "Yueyang Yao",
                    "hidden": false
                },
                {
                    "_id": "68ef0081486b78128f0e33c1",
                    "name": "Xu Chen",
                    "hidden": false
                },
                {
                    "_id": "68ef0081486b78128f0e33c2",
                    "name": "Zirui Guo",
                    "hidden": false
                },
                {
                    "_id": "68ef0081486b78128f0e33c3",
                    "name": "Shengqiang Li",
                    "hidden": false
                },
                {
                    "_id": "68ef0081486b78128f0e33c4",
                    "name": "Weiran Li",
                    "hidden": false
                },
                {
                    "_id": "68ef0081486b78128f0e33c5",
                    "name": "Hanyu Li",
                    "hidden": false
                },
                {
                    "_id": "68ef0081486b78128f0e33c6",
                    "name": "Yaling Mou",
                    "hidden": false
                },
                {
                    "_id": "68ef0081486b78128f0e33c7",
                    "name": "Yan Qiu",
                    "hidden": false
                },
                {
                    "_id": "68ef0081486b78128f0e33c8",
                    "name": "Haiyang Yu",
                    "hidden": false
                },
                {
                    "_id": "68ef0081486b78128f0e33c9",
                    "name": "Xiao Liang",
                    "hidden": false
                },
                {
                    "_id": "68ef0081486b78128f0e33ca",
                    "name": "Hongsheng Li",
                    "hidden": false
                },
                {
                    "_id": "68ef0081486b78128f0e33cb",
                    "name": "Chao Feng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T16:43:22.000Z",
            "submittedOnDailyAt": "2025-10-15T00:31:59.900Z",
            "title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Multimodal embedding models aim to yield informative unified representations\nthat empower diverse cross-modal tasks. Despite promising developments in the\nevolution from CLIP-based dual-tower architectures to large vision-language\nmodels, prior works still face unavoidable challenges in real-world\napplications and business scenarios, such as the limited modality support,\nunstable training mechanisms, and industrial domain gaps. In this work, we\nintroduce SAIL-Embedding, an omni-modal embedding foundation model that\naddresses these issues through tailored training strategies and architectural\ndesign. In the optimization procedure, we propose a multi-stage training scheme\nto boost the multifaceted effectiveness of representation learning.\nSpecifically, the content-aware progressive training aims to enhance the\nmodel's adaptability to diverse downstream tasks and master enriched\ncross-modal proficiency. The collaboration-aware recommendation enhancement\ntraining further adapts multimodal representations for recommendation scenarios\nby distilling knowledge from sequence-to-item and ID-to-item embeddings while\nmining user historical interests. Concurrently, we develop the stochastic\nspecialization and dataset-driven pattern matching to strengthen model training\nflexibility and generalizability. Experimental results show that SAIL-Embedding\nachieves SOTA performance compared to other methods in different retrieval\ntasks. In online experiments across various real-world scenarios integrated\nwith our model, we observe a significant increase in Lifetime (LT), which is a\ncrucial indicator for the recommendation experience. For instance, the model\ndelivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the\nDouyin-Selected scenario. For the Douyin feed rank model, the match features\nproduced by SAIL-Embedding yield a +0.08% AUC gain.",
            "upvotes": 8,
            "discussionId": "68ef0081486b78128f0e33cc",
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "publishedAt": "2025-10-14T12:43:22.000Z",
        "title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model",
        "summary": "Multimodal embedding models aim to yield informative unified representations\nthat empower diverse cross-modal tasks. Despite promising developments in the\nevolution from CLIP-based dual-tower architectures to large vision-language\nmodels, prior works still face unavoidable challenges in real-world\napplications and business scenarios, such as the limited modality support,\nunstable training mechanisms, and industrial domain gaps. In this work, we\nintroduce SAIL-Embedding, an omni-modal embedding foundation model that\naddresses these issues through tailored training strategies and architectural\ndesign. In the optimization procedure, we propose a multi-stage training scheme\nto boost the multifaceted effectiveness of representation learning.\nSpecifically, the content-aware progressive training aims to enhance the\nmodel's adaptability to diverse downstream tasks and master enriched\ncross-modal proficiency. The collaboration-aware recommendation enhancement\ntraining further adapts multimodal representations for recommendation scenarios\nby distilling knowledge from sequence-to-item and ID-to-item embeddings while\nmining user historical interests. Concurrently, we develop the stochastic\nspecialization and dataset-driven pattern matching to strengthen model training\nflexibility and generalizability. Experimental results show that SAIL-Embedding\nachieves SOTA performance compared to other methods in different retrieval\ntasks. In online experiments across various real-world scenarios integrated\nwith our model, we observe a significant increase in Lifetime (LT), which is a\ncrucial indicator for the recommendation experience. For instance, the model\ndelivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the\nDouyin-Selected scenario. For the Douyin feed rank model, the match features\nproduced by SAIL-Embedding yield a +0.08% AUC gain.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12709.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 127
        },
        "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11892",
            "authors": [
                {
                    "_id": "68ef9c827c010e9ba0dc7f70",
                    "name": "Kai Mei",
                    "hidden": false
                },
                {
                    "_id": "68ef9c827c010e9ba0dc7f71",
                    "name": "Jiang Guo",
                    "hidden": false
                },
                {
                    "_id": "68ef9c827c010e9ba0dc7f72",
                    "name": "Shuaichen Chang",
                    "hidden": false
                },
                {
                    "_id": "68ef9c827c010e9ba0dc7f73",
                    "name": "Mingwen Dong",
                    "hidden": false
                },
                {
                    "_id": "68ef9c827c010e9ba0dc7f74",
                    "name": "Dongkyu Lee",
                    "hidden": false
                },
                {
                    "_id": "68ef9c827c010e9ba0dc7f75",
                    "name": "Xing Niu",
                    "hidden": false
                },
                {
                    "_id": "68ef9c827c010e9ba0dc7f76",
                    "name": "Jiarong Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T19:52:04.000Z",
            "submittedOnDailyAt": "2025-10-15T11:39:01.038Z",
            "title": "R-WoM: Retrieval-augmented World Model For Computer-use Agents",
            "submittedOnDailyBy": {
                "_id": "63ea62855c837d9968eb93f7",
                "avatarUrl": "/avatars/405ea6b595c36251d61cb5d7b7ec87f1.svg",
                "isPro": false,
                "fullname": "KaiMei",
                "user": "dongyuanjushi",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) can serve as world models to enhance agent\ndecision-making in digital environments by simulating future states and\npredicting action outcomes, potentially eliminating costly trial-and-error\nexploration. However, this capability is fundamentally limited by LLMs'\ntendency toward hallucination and their reliance on static training knowledge,\nwhich can lead to compounding errors that inhibit long-horizon simulations. To\nsystematically investigate whether LLMs are appropriate for world modeling, we\nprobe two core capabilities of world models--future state prediction and reward\nestimation--through three tasks: next-state identification, full-procedure\nplanning alignment, and milestone transition recognition. Our analysis shows\nthat while LLMs effectively capture immediate next states and identify\nmeaningful state transitions, their performance rapidly degrades in\nfull-procedure planning. This highlights LLMs' limitations in reliably modeling\nenvironment dynamics over long horizons. To address these limitations, we\npropose the Retrieval-augmented World Model (R-WoM), which grounds LLM\nsimulations by incorporating factual, up-to-date knowledge retrieved from\nexternal tutorials. Experiments show that R-WoM achieves substantial\nimprovements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to\nbaselines, with particular advantages in longer-horizon simulations.",
            "upvotes": 7,
            "discussionId": "68ef9c837c010e9ba0dc7f77",
            "ai_summary": "LLMs can enhance decision-making in digital environments but struggle with long-horizon simulations due to hallucination and static knowledge. R-WoM improves performance by integrating external, up-to-date knowledge.",
            "ai_keywords": [
                "Large Language Models",
                "world models",
                "future state prediction",
                "reward estimation",
                "next-state identification",
                "full-procedure planning",
                "milestone transition recognition",
                "Retrieval-augmented World Model",
                "R-WoM",
                "OSWorld",
                "WebArena"
            ]
        },
        "publishedAt": "2025-10-13T15:52:04.000Z",
        "title": "R-WoM: Retrieval-augmented World Model For Computer-use Agents",
        "summary": "Large Language Models (LLMs) can serve as world models to enhance agent\ndecision-making in digital environments by simulating future states and\npredicting action outcomes, potentially eliminating costly trial-and-error\nexploration. However, this capability is fundamentally limited by LLMs'\ntendency toward hallucination and their reliance on static training knowledge,\nwhich can lead to compounding errors that inhibit long-horizon simulations. To\nsystematically investigate whether LLMs are appropriate for world modeling, we\nprobe two core capabilities of world models--future state prediction and reward\nestimation--through three tasks: next-state identification, full-procedure\nplanning alignment, and milestone transition recognition. Our analysis shows\nthat while LLMs effectively capture immediate next states and identify\nmeaningful state transitions, their performance rapidly degrades in\nfull-procedure planning. This highlights LLMs' limitations in reliably modeling\nenvironment dynamics over long horizons. To address these limitations, we\npropose the Retrieval-augmented World Model (R-WoM), which grounds LLM\nsimulations by incorporating factual, up-to-date knowledge retrieved from\nexternal tutorials. Experiments show that R-WoM achieves substantial\nimprovements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to\nbaselines, with particular advantages in longer-horizon simulations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11892.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63ea62855c837d9968eb93f7",
            "avatarUrl": "/avatars/405ea6b595c36251d61cb5d7b7ec87f1.svg",
            "fullname": "KaiMei",
            "name": "dongyuanjushi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11000",
            "authors": [
                {
                    "_id": "68efd1d67087d114d85e4a87",
                    "user": {
                        "_id": "67864a46d02b5aadd99e41ea",
                        "avatarUrl": "/avatars/1774002e38a3f6e5f8ae0432e76a7066.svg",
                        "isPro": false,
                        "fullname": "Ruihang Xu",
                        "user": "ruihangxu",
                        "type": "user"
                    },
                    "name": "Ruihang Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T17:03:35.093Z",
                    "hidden": false
                },
                {
                    "_id": "68efd1d67087d114d85e4a88",
                    "name": "Dewei Zhou",
                    "hidden": false
                },
                {
                    "_id": "68efd1d67087d114d85e4a89",
                    "name": "Fan Ma",
                    "hidden": false
                },
                {
                    "_id": "68efd1d67087d114d85e4a8a",
                    "name": "Yi Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T04:21:19.000Z",
            "submittedOnDailyAt": "2025-10-15T15:41:21.451Z",
            "title": "ContextGen: Contextual Layout Anchoring for Identity-Consistent\n  Multi-Instance Generation",
            "submittedOnDailyBy": {
                "_id": "67864a46d02b5aadd99e41ea",
                "avatarUrl": "/avatars/1774002e38a3f6e5f8ae0432e76a7066.svg",
                "isPro": false,
                "fullname": "Ruihang Xu",
                "user": "ruihangxu",
                "type": "user"
            },
            "summary": "Multi-instance image generation (MIG) remains a significant challenge for\nmodern diffusion models due to key limitations in achieving precise control\nover object layout and preserving the identity of multiple distinct subjects.\nTo address these limitations, we introduce ContextGen, a novel Diffusion\nTransformer framework for multi-instance generation that is guided by both\nlayout and reference images. Our approach integrates two key technical\ncontributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates\nthe composite layout image into the generation context to robustly anchor the\nobjects in their desired positions, and Identity Consistency Attention (ICA),\nan innovative attention mechanism that leverages contextual reference images to\nensure the identity consistency of multiple instances. Recognizing the lack of\nlarge-scale, hierarchically-structured datasets for this task, we introduce\nIMIG-100K, the first dataset with detailed layout and identity annotations.\nExtensive experiments demonstrate that ContextGen sets a new state-of-the-art,\noutperforming existing methods in control precision, identity fidelity, and\noverall visual quality.",
            "upvotes": 5,
            "discussionId": "68efd1d67087d114d85e4a8b",
            "projectPage": "https://nenhang.github.io/ContextGen/",
            "githubRepo": "https://github.com/nenhang/ContextGen",
            "ai_summary": "ContextGen, a Diffusion Transformer framework, enhances multi-instance image generation by integrating layout anchoring and identity consistency attention, achieving superior control and quality.",
            "ai_keywords": [
                "Diffusion Transformer",
                "Contextual Layout Anchoring",
                "Identity Consistency Attention",
                "IMIG-100K"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-10-13T00:21:19.000Z",
        "title": "ContextGen: Contextual Layout Anchoring for Identity-Consistent\n  Multi-Instance Generation",
        "summary": "Multi-instance image generation (MIG) remains a significant challenge for\nmodern diffusion models due to key limitations in achieving precise control\nover object layout and preserving the identity of multiple distinct subjects.\nTo address these limitations, we introduce ContextGen, a novel Diffusion\nTransformer framework for multi-instance generation that is guided by both\nlayout and reference images. Our approach integrates two key technical\ncontributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates\nthe composite layout image into the generation context to robustly anchor the\nobjects in their desired positions, and Identity Consistency Attention (ICA),\nan innovative attention mechanism that leverages contextual reference images to\nensure the identity consistency of multiple instances. Recognizing the lack of\nlarge-scale, hierarchically-structured datasets for this task, we introduce\nIMIG-100K, the first dataset with detailed layout and identity annotations.\nExtensive experiments demonstrate that ContextGen sets a new state-of-the-art,\noutperforming existing methods in control precision, identity fidelity, and\noverall visual quality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11000.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67864a46d02b5aadd99e41ea",
            "avatarUrl": "/avatars/1774002e38a3f6e5f8ae0432e76a7066.svg",
            "fullname": "Ruihang Xu",
            "name": "ruihangxu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.12777",
            "authors": [
                {
                    "_id": "68ef0228486b78128f0e33d7",
                    "user": {
                        "_id": "6471c12a0c2b5fdaf1f07c45",
                        "avatarUrl": "/avatars/6783067212d24d1e716a8b4c64df61b4.svg",
                        "isPro": false,
                        "fullname": "Stefan Baumann",
                        "user": "stefan-baumann",
                        "type": "user"
                    },
                    "name": "Stefan Andreas Baumann",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T03:12:51.560Z",
                    "hidden": false
                },
                {
                    "_id": "68ef0228486b78128f0e33d8",
                    "name": "Nick Stracke",
                    "hidden": false
                },
                {
                    "_id": "68ef0228486b78128f0e33d9",
                    "name": "Timy Phan",
                    "hidden": false
                },
                {
                    "_id": "68ef0228486b78128f0e33da",
                    "name": "Björn Ommer",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6471c12a0c2b5fdaf1f07c45/ksU6k1ub2r8-LwZtZBsz1.png"
            ],
            "publishedAt": "2025-10-14T17:52:17.000Z",
            "submittedOnDailyAt": "2025-10-15T00:42:42.381Z",
            "title": "What If : Understanding Motion Through Sparse Interactions",
            "submittedOnDailyBy": {
                "_id": "6471c12a0c2b5fdaf1f07c45",
                "avatarUrl": "/avatars/6783067212d24d1e716a8b4c64df61b4.svg",
                "isPro": false,
                "fullname": "Stefan Baumann",
                "user": "stefan-baumann",
                "type": "user"
            },
            "summary": "Understanding the dynamics of a physical scene involves reasoning about the\ndiverse ways it can potentially change, especially as a result of local\ninteractions. We present the Flow Poke Transformer (FPT), a novel framework for\ndirectly predicting the distribution of local motion, conditioned on sparse\ninteractions termed \"pokes\". Unlike traditional methods that typically only\nenable dense sampling of a single realization of scene dynamics, FPT provides\nan interpretable directly accessible representation of multi-modal scene\nmotion, its dependency on physical interactions and the inherent uncertainties\nof scene dynamics. We also evaluate our model on several downstream tasks to\nenable comparisons with prior methods and highlight the flexibility of our\napproach. On dense face motion generation, our generic pre-trained model\nsurpasses specialized baselines. FPT can be fine-tuned in strongly\nout-of-distribution tasks such as synthetic datasets to enable significant\nimprovements over in-domain methods in articulated object motion estimation.\nAdditionally, predicting explicit motion distributions directly enables our\nmethod to achieve competitive performance on tasks like moving part\nsegmentation from pokes which further demonstrates the versatility of our FPT.\nCode and models are publicly available at\nhttps://compvis.github.io/flow-poke-transformer.",
            "upvotes": 4,
            "discussionId": "68ef0228486b78128f0e33db",
            "projectPage": "https://compvis.github.io/flow-poke-transformer/",
            "githubRepo": "https://github.com/CompVis/flow-poke-transformer",
            "githubStars": 8,
            "organization": {
                "_id": "62cfeeb73c54a34d508b82a9",
                "name": "CompVis",
                "fullname": "CompVis",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1657794102363-5e3aec01f55e2b62848a5217.png"
            }
        },
        "publishedAt": "2025-10-14T13:52:17.000Z",
        "title": "What If : Understanding Motion Through Sparse Interactions",
        "summary": "Understanding the dynamics of a physical scene involves reasoning about the\ndiverse ways it can potentially change, especially as a result of local\ninteractions. We present the Flow Poke Transformer (FPT), a novel framework for\ndirectly predicting the distribution of local motion, conditioned on sparse\ninteractions termed \"pokes\". Unlike traditional methods that typically only\nenable dense sampling of a single realization of scene dynamics, FPT provides\nan interpretable directly accessible representation of multi-modal scene\nmotion, its dependency on physical interactions and the inherent uncertainties\nof scene dynamics. We also evaluate our model on several downstream tasks to\nenable comparisons with prior methods and highlight the flexibility of our\napproach. On dense face motion generation, our generic pre-trained model\nsurpasses specialized baselines. FPT can be fine-tuned in strongly\nout-of-distribution tasks such as synthetic datasets to enable significant\nimprovements over in-domain methods in articulated object motion estimation.\nAdditionally, predicting explicit motion distributions directly enables our\nmethod to achieve competitive performance on tasks like moving part\nsegmentation from pokes which further demonstrates the versatility of our FPT.\nCode and models are publicly available at\nhttps://compvis.github.io/flow-poke-transformer.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6471c12a0c2b5fdaf1f07c45/ksU6k1ub2r8-LwZtZBsz1.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12777.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6471c12a0c2b5fdaf1f07c45",
            "avatarUrl": "/avatars/6783067212d24d1e716a8b4c64df61b4.svg",
            "fullname": "Stefan Baumann",
            "name": "stefan-baumann",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "organization": {
            "_id": "62cfeeb73c54a34d508b82a9",
            "name": "CompVis",
            "fullname": "CompVis",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1657794102363-5e3aec01f55e2b62848a5217.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.11919",
            "authors": [
                {
                    "_id": "68eefecd486b78128f0e3388",
                    "name": "Armel Zebaze",
                    "hidden": false
                },
                {
                    "_id": "68eefecd486b78128f0e3389",
                    "name": "Rachel Bawden",
                    "hidden": false
                },
                {
                    "_id": "68eefecd486b78128f0e338a",
                    "name": "Benoît Sagot",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T20:41:01.000Z",
            "submittedOnDailyAt": "2025-10-15T00:26:03.399Z",
            "title": "LLM Reasoning for Machine Translation: Synthetic Data Generation over\n  Thinking Tokens",
            "submittedOnDailyBy": {
                "_id": "63952b31ee411d4c6fba891c",
                "avatarUrl": "/avatars/e901c3aaa16d3a06dff09896ce8a67a2.svg",
                "isPro": false,
                "fullname": "Armel Randy Zebaze",
                "user": "ArmelRandy",
                "type": "user"
            },
            "summary": "Large reasoning models (LRMs) have led to new possibilities in terms of\nproblem-solving, through the devising of a natural language thought process\nprior to answering a query. While their capabilities are well known across\nmathematics and coding tasks, their impact on the task of machine translation\n(MT) remains underexplored. In this work, we explore the benefits of the\ngeneration of intermediate tokens when performing MT across multiple language\npairs of different levels of resourcedness and multiple setups. We find that\n\"thinking tokens\" do not help LRMs better perform MT. This result generalizes\nto models fine-tuned to reason before translating using distilled chain of\nthought (CoT) inspired by human translators' practices. Specifically,\nfine-tuning a model with synthetic CoT explanations detailing how to translate\nstep-by-step does not outperform standard input-output fine-tuning. However,\nconstructing the intermediate tokens by combining the outputs of modular\ntranslation-specific prompting strategies results in improvements. Our findings\nunderscore that the contribution of intermediate tokens during fine-tuning\nhighly depends on the presence of translation attempts within them. More\nbroadly, our results suggest that using a teacher to refine target translations\nor to expand parallel corpora is more impactful than distilling their CoT\nexplanations into \"thinking\" MT models.",
            "upvotes": 4,
            "discussionId": "68eefecd486b78128f0e338b",
            "githubRepo": "https://github.com/ArmelRandy/llm-reasoning-mt",
            "githubStars": 0,
            "organization": {
                "_id": "602ba30dc4f8038e9a1e0a60",
                "name": "almanach",
                "fullname": "ALMAnaCH (Inria)",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613472488646-602ba2a739515f8d31237967.png"
            }
        },
        "publishedAt": "2025-10-13T16:41:01.000Z",
        "title": "LLM Reasoning for Machine Translation: Synthetic Data Generation over\n  Thinking Tokens",
        "summary": "Large reasoning models (LRMs) have led to new possibilities in terms of\nproblem-solving, through the devising of a natural language thought process\nprior to answering a query. While their capabilities are well known across\nmathematics and coding tasks, their impact on the task of machine translation\n(MT) remains underexplored. In this work, we explore the benefits of the\ngeneration of intermediate tokens when performing MT across multiple language\npairs of different levels of resourcedness and multiple setups. We find that\n\"thinking tokens\" do not help LRMs better perform MT. This result generalizes\nto models fine-tuned to reason before translating using distilled chain of\nthought (CoT) inspired by human translators' practices. Specifically,\nfine-tuning a model with synthetic CoT explanations detailing how to translate\nstep-by-step does not outperform standard input-output fine-tuning. However,\nconstructing the intermediate tokens by combining the outputs of modular\ntranslation-specific prompting strategies results in improvements. Our findings\nunderscore that the contribution of intermediate tokens during fine-tuning\nhighly depends on the presence of translation attempts within them. More\nbroadly, our results suggest that using a teacher to refine target translations\nor to expand parallel corpora is more impactful than distilling their CoT\nexplanations into \"thinking\" MT models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11919.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63952b31ee411d4c6fba891c",
            "avatarUrl": "/avatars/e901c3aaa16d3a06dff09896ce8a67a2.svg",
            "fullname": "Armel Randy Zebaze",
            "name": "ArmelRandy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "602ba30dc4f8038e9a1e0a60",
            "name": "almanach",
            "fullname": "ALMAnaCH (Inria)",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613472488646-602ba2a739515f8d31237967.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.09782",
            "authors": [
                {
                    "_id": "68eeda16486b78128f0e3277",
                    "name": "Yufa Zhou",
                    "hidden": false
                },
                {
                    "_id": "68eeda16486b78128f0e3278",
                    "name": "Yixiao Wang",
                    "hidden": false
                },
                {
                    "_id": "68eeda16486b78128f0e3279",
                    "name": "Xunjian Yin",
                    "hidden": false
                },
                {
                    "_id": "68eeda16486b78128f0e327a",
                    "name": "Shuyan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68eeda16486b78128f0e327b",
                    "name": "Anru R. Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T18:44:00.000Z",
            "submittedOnDailyAt": "2025-10-15T17:47:33.052Z",
            "title": "The Geometry of Reasoning: Flowing Logics in Representation Space",
            "submittedOnDailyBy": {
                "_id": "658ab894c4b2004663dff3ae",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658ab894c4b2004663dff3ae/oPRnFuW2Imaa2KkYWNbSf.jpeg",
                "isPro": false,
                "fullname": "YUFA ZHOU",
                "user": "MasterZhou",
                "type": "user"
            },
            "summary": "We study how large language models (LLMs) ``think'' through their\nrepresentation space. We propose a novel geometric framework that models an\nLLM's reasoning as flows -- embedding trajectories evolving where logic goes.\nWe disentangle logical structure from semantics by employing the same natural\ndeduction propositions with varied semantic carriers, allowing us to test\nwhether LLMs internalize logic beyond surface form. This perspective connects\nreasoning with geometric quantities such as position, velocity, and curvature,\nenabling formal analysis in representation and concept spaces. Our theory\nestablishes: (1) LLM reasoning corresponds to smooth flows in representation\nspace, and (2) logical statements act as local controllers of these flows'\nvelocities. Using learned representation proxies, we design controlled\nexperiments to visualize and quantify reasoning flows, providing empirical\nvalidation of our theoretical framework. Our work serves as both a conceptual\nfoundation and practical tools for studying reasoning phenomenon, offering a\nnew lens for interpretability and formal analysis of LLMs' behavior.",
            "upvotes": 3,
            "discussionId": "68eeda17486b78128f0e327c",
            "githubRepo": "https://github.com/MasterZhou1/Reasoning-Flow",
            "githubStars": 3
        },
        "publishedAt": "2025-10-10T14:44:00.000Z",
        "title": "The Geometry of Reasoning: Flowing Logics in Representation Space",
        "summary": "We study how large language models (LLMs) ``think'' through their\nrepresentation space. We propose a novel geometric framework that models an\nLLM's reasoning as flows -- embedding trajectories evolving where logic goes.\nWe disentangle logical structure from semantics by employing the same natural\ndeduction propositions with varied semantic carriers, allowing us to test\nwhether LLMs internalize logic beyond surface form. This perspective connects\nreasoning with geometric quantities such as position, velocity, and curvature,\nenabling formal analysis in representation and concept spaces. Our theory\nestablishes: (1) LLM reasoning corresponds to smooth flows in representation\nspace, and (2) logical statements act as local controllers of these flows'\nvelocities. Using learned representation proxies, we design controlled\nexperiments to visualize and quantify reasoning flows, providing empirical\nvalidation of our theoretical framework. Our work serves as both a conceptual\nfoundation and practical tools for studying reasoning phenomenon, offering a\nnew lens for interpretability and formal analysis of LLMs' behavior.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09782.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "658ab894c4b2004663dff3ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658ab894c4b2004663dff3ae/oPRnFuW2Imaa2KkYWNbSf.jpeg",
            "fullname": "YUFA ZHOU",
            "name": "MasterZhou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08783",
            "authors": [
                {
                    "_id": "68ef3a71486b78128f0e3571",
                    "name": "Reuben A. Luera",
                    "hidden": false
                },
                {
                    "_id": "68ef3a71486b78128f0e3572",
                    "name": "Ryan Rossi",
                    "hidden": false
                },
                {
                    "_id": "68ef3a71486b78128f0e3573",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T07:07:09.832Z",
                    "hidden": false
                },
                {
                    "_id": "68ef3a71486b78128f0e3574",
                    "name": "Samyadeep Basu",
                    "hidden": false
                },
                {
                    "_id": "68ef3a71486b78128f0e3575",
                    "name": "Sungchul Kim",
                    "hidden": false
                },
                {
                    "_id": "68ef3a71486b78128f0e3576",
                    "name": "Subhojyoti Mukherjee",
                    "hidden": false
                },
                {
                    "_id": "68ef3a71486b78128f0e3577",
                    "name": "Puneet Mathur",
                    "hidden": false
                },
                {
                    "_id": "68ef3a71486b78128f0e3578",
                    "name": "Ruiyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ef3a71486b78128f0e3579",
                    "name": "Jihyung Kil",
                    "hidden": false
                },
                {
                    "_id": "68ef3a71486b78128f0e357a",
                    "name": "Nedim Lipka",
                    "hidden": false
                },
                {
                    "_id": "68ef3a71486b78128f0e357b",
                    "name": "Seunghyun Yoon",
                    "hidden": false
                },
                {
                    "_id": "68ef3a71486b78128f0e357c",
                    "name": "Jiuxiang Gu",
                    "hidden": false
                },
                {
                    "_id": "68ef3a71486b78128f0e357d",
                    "name": "Zichao Wang",
                    "hidden": false
                },
                {
                    "_id": "68ef3a71486b78128f0e357e",
                    "name": "Cindy Xiong Bearfield",
                    "hidden": false
                },
                {
                    "_id": "68ef3a71486b78128f0e357f",
                    "name": "Branislav Kveton",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T20:00:41.000Z",
            "submittedOnDailyAt": "2025-10-15T04:38:58.638Z",
            "title": "MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human\n  Perception of User Interfaces",
            "submittedOnDailyBy": {
                "_id": "62c5947524171688a9feb992",
                "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                "isPro": false,
                "fullname": "Franck Dernoncourt",
                "user": "Franck-Dernoncourt",
                "type": "user"
            },
            "summary": "In an ideal design pipeline, user interface (UI) design is intertwined with\nuser research to validate decisions, yet studies are often resource-constrained\nduring early exploration. Recent advances in multimodal large language models\n(MLLMs) offer a promising opportunity to act as early evaluators, helping\ndesigners narrow options before formal testing. Unlike prior work that\nemphasizes user behavior in narrow domains such as e-commerce with metrics like\nclicks or conversions, we focus on subjective user evaluations across varied\ninterfaces. We investigate whether MLLMs can mimic human preferences when\nevaluating individual UIs and comparing them. Using data from a crowdsourcing\nplatform, we benchmark GPT-4o, Claude, and Llama across 30 interfaces and\nexamine alignment with human judgments on multiple UI factors. Our results show\nthat MLLMs approximate human preferences on some dimensions but diverge on\nothers, underscoring both their potential and limitations in supplementing\nearly UX research.",
            "upvotes": 3,
            "discussionId": "68ef3a72486b78128f0e3580"
        },
        "publishedAt": "2025-10-09T16:00:41.000Z",
        "title": "MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human\n  Perception of User Interfaces",
        "summary": "In an ideal design pipeline, user interface (UI) design is intertwined with\nuser research to validate decisions, yet studies are often resource-constrained\nduring early exploration. Recent advances in multimodal large language models\n(MLLMs) offer a promising opportunity to act as early evaluators, helping\ndesigners narrow options before formal testing. Unlike prior work that\nemphasizes user behavior in narrow domains such as e-commerce with metrics like\nclicks or conversions, we focus on subjective user evaluations across varied\ninterfaces. We investigate whether MLLMs can mimic human preferences when\nevaluating individual UIs and comparing them. Using data from a crowdsourcing\nplatform, we benchmark GPT-4o, Claude, and Llama across 30 interfaces and\nexamine alignment with human judgments on multiple UI factors. Our results show\nthat MLLMs approximate human preferences on some dimensions but diverge on\nothers, underscoring both their potential and limitations in supplementing\nearly UX research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08783.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.12402",
            "authors": [
                {
                    "_id": "68efc200d22df134b7f5ca31",
                    "name": "Lizhang Chen",
                    "hidden": false
                },
                {
                    "_id": "68efc200d22df134b7f5ca32",
                    "name": "Jonathan Li",
                    "hidden": false
                },
                {
                    "_id": "68efc200d22df134b7f5ca33",
                    "name": "Kaizhao Liang",
                    "hidden": false
                },
                {
                    "_id": "68efc200d22df134b7f5ca34",
                    "name": "Baiyu Su",
                    "hidden": false
                },
                {
                    "_id": "68efc200d22df134b7f5ca35",
                    "name": "Cong Xie",
                    "hidden": false
                },
                {
                    "_id": "68efc200d22df134b7f5ca36",
                    "name": "Nuo Wang Pierse",
                    "hidden": false
                },
                {
                    "_id": "68efc200d22df134b7f5ca37",
                    "name": "Chen Liang",
                    "hidden": false
                },
                {
                    "_id": "68efc200d22df134b7f5ca38",
                    "name": "Ni Lao",
                    "hidden": false
                },
                {
                    "_id": "68efc200d22df134b7f5ca39",
                    "name": "Qiang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T11:32:55.000Z",
            "submittedOnDailyAt": "2025-10-15T14:18:36.279Z",
            "title": "Cautious Weight Decay",
            "submittedOnDailyBy": {
                "_id": "62140dcdcf7928035e8135ad",
                "avatarUrl": "/avatars/146a8ac3fb476e79b6503373df0f38c1.svg",
                "isPro": true,
                "fullname": "Kaizhao Liang",
                "user": "kz919",
                "type": "user"
            },
            "summary": "We introduce Cautious Weight Decay (CWD), a one-line, optimizer-agnostic\nmodification that applies weight decay only to parameter coordinates whose\nsigns align with the optimizer update. Unlike standard decoupled decay, which\nimplicitly optimizes a regularized or constrained objective, CWD preserves the\noriginal loss and admits a bilevel interpretation: it induces sliding-mode\nbehavior upon reaching the stationary manifold, allowing it to search for\nlocally Pareto-optimal stationary points of the unmodified objective. In\npractice, CWD is a drop-in change for optimizers such as AdamW, Lion, and Muon,\nrequiring no new hyperparameters or additional tuning. For language model\npre-training and ImageNet classification, CWD consistently improves final loss\nand accuracy at million- to billion-parameter scales.",
            "upvotes": 2,
            "discussionId": "68efc200d22df134b7f5ca3a",
            "ai_summary": "Cautious Weight Decay (CWD) enhances optimizer performance by applying weight decay selectively, improving accuracy and loss in large-scale models without additional tuning.",
            "ai_keywords": [
                "Cautious Weight Decay",
                "CWD",
                "optimizer-agnostic",
                "weight decay",
                "parameter coordinates",
                "optimizer update",
                "standard decoupled decay",
                "bilevel interpretation",
                "sliding-mode behavior",
                "stationary manifold",
                "locally Pareto-optimal",
                "AdamW",
                "Lion",
                "Muon",
                "language model pre-training",
                "ImageNet classification"
            ]
        },
        "publishedAt": "2025-10-14T07:32:55.000Z",
        "title": "Cautious Weight Decay",
        "summary": "We introduce Cautious Weight Decay (CWD), a one-line, optimizer-agnostic\nmodification that applies weight decay only to parameter coordinates whose\nsigns align with the optimizer update. Unlike standard decoupled decay, which\nimplicitly optimizes a regularized or constrained objective, CWD preserves the\noriginal loss and admits a bilevel interpretation: it induces sliding-mode\nbehavior upon reaching the stationary manifold, allowing it to search for\nlocally Pareto-optimal stationary points of the unmodified objective. In\npractice, CWD is a drop-in change for optimizers such as AdamW, Lion, and Muon,\nrequiring no new hyperparameters or additional tuning. For language model\npre-training and ImageNet classification, CWD consistently improves final loss\nand accuracy at million- to billion-parameter scales.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12402.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62140dcdcf7928035e8135ad",
            "avatarUrl": "/avatars/146a8ac3fb476e79b6503373df0f38c1.svg",
            "fullname": "Kaizhao Liang",
            "name": "kz919",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 51
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.12088",
            "authors": [
                {
                    "_id": "68ef4328486b78128f0e35a6",
                    "name": "Zaid Khan",
                    "hidden": false
                },
                {
                    "_id": "68ef4328486b78128f0e35a7",
                    "name": "Archiki Prasad",
                    "hidden": false
                },
                {
                    "_id": "68ef4328486b78128f0e35a8",
                    "name": "Elias Stengel-Eskin",
                    "hidden": false
                },
                {
                    "_id": "68ef4328486b78128f0e35a9",
                    "name": "Jaemin Cho",
                    "hidden": false
                },
                {
                    "_id": "68ef4328486b78128f0e35aa",
                    "name": "Mohit Bansal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T02:49:32.000Z",
            "submittedOnDailyAt": "2025-10-15T05:17:01.654Z",
            "title": "One Life to Learn: Inferring Symbolic World Models for Stochastic\n  Environments from Unguided Exploration",
            "submittedOnDailyBy": {
                "_id": "6301c3e0a123c93a5fb295ff",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661060051926-noauth.jpeg",
                "isPro": false,
                "fullname": "Zaid Khan",
                "user": "codezakh",
                "type": "user"
            },
            "summary": "Symbolic world modeling requires inferring and representing an environment's\ntransitional dynamics as an executable program. Prior work has focused on\nlargely deterministic environments with abundant interaction data, simple\nmechanics, and human guidance. We address a more realistic and challenging\nsetting, learning in a complex, stochastic environment where the agent has only\n\"one life\" to explore a hostile environment without human guidance. We\nintroduce OneLife, a framework that models world dynamics through\nconditionally-activated programmatic laws within a probabilistic programming\nframework. Each law operates through a precondition-effect structure,\nactivating in relevant world states. This creates a dynamic computation graph\nthat routes inference and optimization only through relevant laws, avoiding\nscaling challenges when all laws contribute to predictions about a complex,\nhierarchical state, and enabling the learning of stochastic dynamics even with\nsparse rule activation. To evaluate our approach under these demanding\nconstraints, we introduce a new evaluation protocol that measures (a) state\nranking, the ability to distinguish plausible future states from implausible\nones, and (b) state fidelity, the ability to generate future states that\nclosely resemble reality. We develop and evaluate our framework on Crafter-OO,\nour reimplementation of the Crafter environment that exposes a structured,\nobject-oriented symbolic state and a pure transition function that operates on\nthat state alone. OneLife can successfully learn key environment dynamics from\nminimal, unguided interaction, outperforming a strong baseline on 16 out of 23\nscenarios tested. We also test OneLife's planning ability, with simulated\nrollouts successfully identifying superior strategies. Our work establishes a\nfoundation for autonomously constructing programmatic world models of unknown,\ncomplex environments.",
            "upvotes": 2,
            "discussionId": "68ef4328486b78128f0e35ab",
            "projectPage": "https://onelife-worldmodel.github.io",
            "githubRepo": "https://github.com/codezakh/onelife",
            "ai_summary": "OneLife framework models complex, stochastic environments using conditionally-activated programmatic laws within a probabilistic programming framework, enabling learning from minimal, unguided interaction and outperforming baselines in state ranking and fidelity.",
            "ai_keywords": [
                "conditionally-activated programmatic laws",
                "probabilistic programming framework",
                "precondition-effect structure",
                "dynamic computation graph",
                "state ranking",
                "state fidelity",
                "Crafter-OO",
                "object-oriented symbolic state",
                "pure transition function",
                "simulated rollouts"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-10-13T22:49:32.000Z",
        "title": "One Life to Learn: Inferring Symbolic World Models for Stochastic\n  Environments from Unguided Exploration",
        "summary": "Symbolic world modeling requires inferring and representing an environment's\ntransitional dynamics as an executable program. Prior work has focused on\nlargely deterministic environments with abundant interaction data, simple\nmechanics, and human guidance. We address a more realistic and challenging\nsetting, learning in a complex, stochastic environment where the agent has only\n\"one life\" to explore a hostile environment without human guidance. We\nintroduce OneLife, a framework that models world dynamics through\nconditionally-activated programmatic laws within a probabilistic programming\nframework. Each law operates through a precondition-effect structure,\nactivating in relevant world states. This creates a dynamic computation graph\nthat routes inference and optimization only through relevant laws, avoiding\nscaling challenges when all laws contribute to predictions about a complex,\nhierarchical state, and enabling the learning of stochastic dynamics even with\nsparse rule activation. To evaluate our approach under these demanding\nconstraints, we introduce a new evaluation protocol that measures (a) state\nranking, the ability to distinguish plausible future states from implausible\nones, and (b) state fidelity, the ability to generate future states that\nclosely resemble reality. We develop and evaluate our framework on Crafter-OO,\nour reimplementation of the Crafter environment that exposes a structured,\nobject-oriented symbolic state and a pure transition function that operates on\nthat state alone. OneLife can successfully learn key environment dynamics from\nminimal, unguided interaction, outperforming a strong baseline on 16 out of 23\nscenarios tested. We also test OneLife's planning ability, with simulated\nrollouts successfully identifying superior strategies. Our work establishes a\nfoundation for autonomously constructing programmatic world models of unknown,\ncomplex environments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12088.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6301c3e0a123c93a5fb295ff",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661060051926-noauth.jpeg",
            "fullname": "Zaid Khan",
            "name": "codezakh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11661",
            "authors": [
                {
                    "_id": "68edc70ede1fee572713a90b",
                    "user": {
                        "_id": "65900d4ff5a209eeac08b463",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65900d4ff5a209eeac08b463/PJNNBRJIk1qR24oaRLTex.jpeg",
                        "isPro": false,
                        "fullname": "shijie xia",
                        "user": "seven-cat",
                        "type": "user"
                    },
                    "name": "Shijie Xia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-10-15T17:04:51.273Z",
                    "hidden": false
                },
                {
                    "_id": "68edc70ede1fee572713a90c",
                    "name": "Yuhan Sun",
                    "hidden": false
                },
                {
                    "_id": "68edc70ede1fee572713a90d",
                    "name": "Pengfei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T17:35:23.000Z",
            "submittedOnDailyAt": "2025-10-15T13:53:44.602Z",
            "title": "SR-Scientist: Scientific Equation Discovery With Agentic AI",
            "submittedOnDailyBy": {
                "_id": "63a369d98c0c89dcae3b8329",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
                "isPro": true,
                "fullname": "Adina Yakefu",
                "user": "AdinaY",
                "type": "user"
            },
            "summary": "Recently, Large Language Models (LLMs) have been applied to scientific\nequation discovery, leveraging their embedded scientific knowledge for\nhypothesis generation. However, current methods typically confine LLMs to the\nrole of an equation proposer within search algorithms like genetic programming.\nIn this paper, we present SR-Scientist, a framework that elevates the LLM from\na simple equation proposer to an autonomous AI scientist that writes code to\nanalyze data, implements the equation as code, submits it for evaluation, and\noptimizes the equation based on experimental feedback. Specifically, we wrap\nthe code interpreter into a set of tools for data analysis and equation\nevaluation. The agent is instructed to optimize the equation by utilizing these\ntools over a long horizon with minimal human-defined pipelines. Empirical\nresults show that SR-Scientist outperforms baseline methods by an absolute\nmargin of 6% to 35% on datasets covering four science disciplines.\nAdditionally, we demonstrate our method's robustness to noise, the\ngeneralization of the discovered equations to out-of-domain data, and their\nsymbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning\nframework to enhance the agent's capabilities.",
            "upvotes": 2,
            "discussionId": "68edc719de1fee572713a90e",
            "ai_summary": "SR-Scientist, an autonomous AI framework, leverages LLMs to generate, implement, and optimize scientific equations, outperforming baselines across multiple disciplines.",
            "ai_keywords": [
                "Large Language Models",
                "SR-Scientist",
                "genetic programming",
                "code interpreter",
                "data analysis",
                "equation evaluation",
                "reinforcement learning"
            ],
            "organization": {
                "_id": "630bc2d186b8b9904c33ce1b",
                "name": "GAIR",
                "fullname": "SII - GAIR",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"
            }
        },
        "publishedAt": "2025-10-13T13:35:23.000Z",
        "title": "SR-Scientist: Scientific Equation Discovery With Agentic AI",
        "summary": "Recently, Large Language Models (LLMs) have been applied to scientific\nequation discovery, leveraging their embedded scientific knowledge for\nhypothesis generation. However, current methods typically confine LLMs to the\nrole of an equation proposer within search algorithms like genetic programming.\nIn this paper, we present SR-Scientist, a framework that elevates the LLM from\na simple equation proposer to an autonomous AI scientist that writes code to\nanalyze data, implements the equation as code, submits it for evaluation, and\noptimizes the equation based on experimental feedback. Specifically, we wrap\nthe code interpreter into a set of tools for data analysis and equation\nevaluation. The agent is instructed to optimize the equation by utilizing these\ntools over a long horizon with minimal human-defined pipelines. Empirical\nresults show that SR-Scientist outperforms baseline methods by an absolute\nmargin of 6% to 35% on datasets covering four science disciplines.\nAdditionally, we demonstrate our method's robustness to noise, the\ngeneralization of the discovered equations to out-of-domain data, and their\nsymbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning\nframework to enhance the agent's capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11661.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a369d98c0c89dcae3b8329",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
            "fullname": "Adina Yakefu",
            "name": "AdinaY",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1131
        },
        "organization": {
            "_id": "630bc2d186b8b9904c33ce1b",
            "name": "GAIR",
            "fullname": "SII - GAIR",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11606",
            "authors": [
                {
                    "_id": "68ee35b69b77b5223f666173",
                    "user": {
                        "_id": "682c163fa17480053339f270",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/UQZW2bM9UEtlVHDUq0yel.png",
                        "isPro": false,
                        "fullname": "Yicheng Xu",
                        "user": "linghan199",
                        "type": "user"
                    },
                    "name": "Yicheng Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T14:24:10.982Z",
                    "hidden": false
                },
                {
                    "_id": "68ee35b69b77b5223f666174",
                    "name": "Yue Wu",
                    "hidden": false
                },
                {
                    "_id": "68ee35b69b77b5223f666175",
                    "name": "Jiashuo Yu",
                    "hidden": false
                },
                {
                    "_id": "68ee35b69b77b5223f666176",
                    "name": "Ziang Yan",
                    "hidden": false
                },
                {
                    "_id": "68ee35b69b77b5223f666177",
                    "name": "Tianxiang Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ee35b69b77b5223f666178",
                    "name": "Yinan He",
                    "hidden": false
                },
                {
                    "_id": "68ee35b69b77b5223f666179",
                    "name": "Qingsong Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ee35b69b77b5223f66617a",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "68ee35b69b77b5223f66617b",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68ee35b69b77b5223f66617c",
                    "name": "Limin Wang",
                    "hidden": false
                },
                {
                    "_id": "68ee35b69b77b5223f66617d",
                    "name": "Manabu Okumura",
                    "hidden": false
                },
                {
                    "_id": "68ee35b69b77b5223f66617e",
                    "name": "Yi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T16:45:28.000Z",
            "submittedOnDailyAt": "2025-10-15T03:27:19.691Z",
            "title": "ExpVid: A Benchmark for Experiment Video Understanding & Reasoning",
            "submittedOnDailyBy": {
                "_id": "682c163fa17480053339f270",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/UQZW2bM9UEtlVHDUq0yel.png",
                "isPro": false,
                "fullname": "Yicheng Xu",
                "user": "linghan199",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) hold promise for accelerating\nscientific discovery by interpreting complex experimental procedures. However,\ntheir true capabilities are poorly understood, as existing benchmarks neglect\nthe fine-grained and long-horizon nature of authentic laboratory work,\nespecially in wet-lab settings. To bridge this gap, we introduce ExpVid, the\nfirst benchmark designed to systematically evaluate MLLMs on scientific\nexperiment videos. Curated from peer-reviewed video publications, ExpVid\nfeatures a new three-level task hierarchy that mirrors the scientific process:\n(1) Fine-grained Perception of tools, materials, and actions; (2) Procedural\nUnderstanding of step order and completeness; and (3) Scientific Reasoning that\nconnects the full experiment to its published conclusions. Our vision-centric\nannotation pipeline, combining automated generation with multi-disciplinary\nexpert validation, ensures that tasks require visual grounding. We evaluate 19\nleading MLLMs on ExpVid and find that while they excel at coarse-grained\nrecognition, they struggle with disambiguating fine details, tracking state\nchanges over time, and linking experimental procedures to scientific outcomes.\nOur results reveal a notable performance gap between proprietary and\nopen-source models, particularly in high-order reasoning. ExpVid not only\nprovides a diagnostic tool but also charts a roadmap for developing MLLMs\ncapable of becoming trustworthy partners in scientific experimentation.",
            "upvotes": 2,
            "discussionId": "68ee35b69b77b5223f66617f",
            "githubRepo": "https://github.com/OpenGVLab/ExpVid",
            "ai_summary": "ExpVid, a new benchmark for evaluating multimodal large language models on scientific experiment videos, highlights gaps in fine-grained perception, procedural understanding, and scientific reasoning.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "MLLMs",
                "ExpVid",
                "benchmark",
                "scientific experiment videos",
                "three-level task hierarchy",
                "fine-grained perception",
                "procedural understanding",
                "scientific reasoning",
                "vision-centric annotation pipeline",
                "automated generation",
                "multi-disciplinary expert validation",
                "coarse-grained recognition",
                "fine details",
                "state changes",
                "high-order reasoning",
                "proprietary models",
                "open-source models"
            ],
            "githubStars": 4,
            "organization": {
                "_id": "64006c57a3b8fe3ac0e9af7c",
                "name": "OpenGVLab",
                "fullname": "OpenGVLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64006c09330a45b03605bba3/FvdxiTkTqH8rKDOzGZGUE.jpeg"
            }
        },
        "publishedAt": "2025-10-13T12:45:28.000Z",
        "title": "ExpVid: A Benchmark for Experiment Video Understanding & Reasoning",
        "summary": "Multimodal Large Language Models (MLLMs) hold promise for accelerating\nscientific discovery by interpreting complex experimental procedures. However,\ntheir true capabilities are poorly understood, as existing benchmarks neglect\nthe fine-grained and long-horizon nature of authentic laboratory work,\nespecially in wet-lab settings. To bridge this gap, we introduce ExpVid, the\nfirst benchmark designed to systematically evaluate MLLMs on scientific\nexperiment videos. Curated from peer-reviewed video publications, ExpVid\nfeatures a new three-level task hierarchy that mirrors the scientific process:\n(1) Fine-grained Perception of tools, materials, and actions; (2) Procedural\nUnderstanding of step order and completeness; and (3) Scientific Reasoning that\nconnects the full experiment to its published conclusions. Our vision-centric\nannotation pipeline, combining automated generation with multi-disciplinary\nexpert validation, ensures that tasks require visual grounding. We evaluate 19\nleading MLLMs on ExpVid and find that while they excel at coarse-grained\nrecognition, they struggle with disambiguating fine details, tracking state\nchanges over time, and linking experimental procedures to scientific outcomes.\nOur results reveal a notable performance gap between proprietary and\nopen-source models, particularly in high-order reasoning. ExpVid not only\nprovides a diagnostic tool but also charts a roadmap for developing MLLMs\ncapable of becoming trustworthy partners in scientific experimentation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11606.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "682c163fa17480053339f270",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/UQZW2bM9UEtlVHDUq0yel.png",
            "fullname": "Yicheng Xu",
            "name": "linghan199",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "64006c57a3b8fe3ac0e9af7c",
            "name": "OpenGVLab",
            "fullname": "OpenGVLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64006c09330a45b03605bba3/FvdxiTkTqH8rKDOzGZGUE.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.09259",
            "authors": [
                {
                    "_id": "68efba47d22df134b7f5c9d9",
                    "name": "Yongding Tao",
                    "hidden": false
                },
                {
                    "_id": "68efba47d22df134b7f5c9da",
                    "name": "Tian Wang",
                    "hidden": false
                },
                {
                    "_id": "68efba47d22df134b7f5c9db",
                    "name": "Yihong Dong",
                    "hidden": false
                },
                {
                    "_id": "68efba47d22df134b7f5c9dc",
                    "name": "Huanyu Liu",
                    "hidden": false
                },
                {
                    "_id": "68efba47d22df134b7f5c9dd",
                    "name": "Kechi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68efba47d22df134b7f5c9de",
                    "name": "Xiaolong Hu",
                    "hidden": false
                },
                {
                    "_id": "68efba47d22df134b7f5c9df",
                    "name": "Ge Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T10:58:50.000Z",
            "submittedOnDailyAt": "2025-10-15T13:48:56.456Z",
            "title": "Detecting Data Contamination from Reinforcement Learning Post-training\n  for Large Language Models",
            "submittedOnDailyBy": {
                "_id": "664b3aebfe822b08e62357f0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664b3aebfe822b08e62357f0/keEfUHXnkfwZ4o7588VdC.jpeg",
                "isPro": false,
                "fullname": "Yongding Tao",
                "user": "YongdingTao",
                "type": "user"
            },
            "summary": "Data contamination poses a significant threat to the reliable evaluation of\nLarge Language Models (LLMs). This issue arises when benchmark samples may\ninadvertently appear in training sets, compromising the validity of reported\nperformance. While detection methods have been developed for the pre-training\nand Supervised Fine-Tuning stages, a critical research gap exists for the\nincreasingly significant phase of Reinforcement Learning (RL) post-training. As\nRL post-training becomes pivotal for advancing LLM reasoning, the absence of\nspecialized contamination detection methods in this paradigm presents a\ncritical vulnerability. To address this, we conduct the first systematic study\nof data detection within RL post-training scenario and propose Self-Critique.\nOur method is motivated by a key observation: after RL phase, the output\nentropy distribution of LLMs tends to collapse into highly specific and sparse\nmodes. Self-Critique probes for the underlying policy collapse, i.e., the\nmodel's convergence to a narrow reasoning path, which causes this entropy\nreduction. To facilitate this research, we also introduce RL-MIA, a benchmark\nconstructed to simulate this specific contamination scenario. Extensive\nexperiments show that Self-Critique significantly outperforms baseline methods\nacross multiple models and contamination tasks, achieving an AUC improvement of\nup to 30%. Whereas existing methods are close to a random guess for RL-phase\ncontamination, our method makes detection possible.",
            "upvotes": 2,
            "discussionId": "68efba47d22df134b7f5c9e0",
            "githubRepo": "https://github.com/yongding-tao/RL-Data-Contamination",
            "ai_summary": "Self-Critique addresses data contamination in the RL post-training phase of LLMs by detecting policy collapse, outperforming existing methods with significant improvements in AUC.",
            "ai_keywords": [
                "Large Language Models",
                "Reinforcement Learning",
                "data contamination",
                "pre-training",
                "Supervised Fine-Tuning",
                "Self-Critique",
                "output entropy distribution",
                "policy collapse",
                "RL-MIA",
                "AUC"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "6377482f7169ed9817038dd7",
                "name": "PekingU",
                "fullname": "Peking University"
            }
        },
        "publishedAt": "2025-10-10T06:58:50.000Z",
        "title": "Detecting Data Contamination from Reinforcement Learning Post-training\n  for Large Language Models",
        "summary": "Data contamination poses a significant threat to the reliable evaluation of\nLarge Language Models (LLMs). This issue arises when benchmark samples may\ninadvertently appear in training sets, compromising the validity of reported\nperformance. While detection methods have been developed for the pre-training\nand Supervised Fine-Tuning stages, a critical research gap exists for the\nincreasingly significant phase of Reinforcement Learning (RL) post-training. As\nRL post-training becomes pivotal for advancing LLM reasoning, the absence of\nspecialized contamination detection methods in this paradigm presents a\ncritical vulnerability. To address this, we conduct the first systematic study\nof data detection within RL post-training scenario and propose Self-Critique.\nOur method is motivated by a key observation: after RL phase, the output\nentropy distribution of LLMs tends to collapse into highly specific and sparse\nmodes. Self-Critique probes for the underlying policy collapse, i.e., the\nmodel's convergence to a narrow reasoning path, which causes this entropy\nreduction. To facilitate this research, we also introduce RL-MIA, a benchmark\nconstructed to simulate this specific contamination scenario. Extensive\nexperiments show that Self-Critique significantly outperforms baseline methods\nacross multiple models and contamination tasks, achieving an AUC improvement of\nup to 30%. Whereas existing methods are close to a random guess for RL-phase\ncontamination, our method makes detection possible.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09259.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "664b3aebfe822b08e62357f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664b3aebfe822b08e62357f0/keEfUHXnkfwZ4o7588VdC.jpeg",
            "fullname": "Yongding Tao",
            "name": "YongdingTao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "6377482f7169ed9817038dd7",
            "name": "PekingU",
            "fullname": "Peking University"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08532",
            "authors": [
                {
                    "_id": "68ec9b74cd07fb414898caf2",
                    "name": "Rishubh Parihar",
                    "hidden": false
                },
                {
                    "_id": "68ec9b74cd07fb414898caf3",
                    "name": "Or Patashnik",
                    "hidden": false
                },
                {
                    "_id": "68ec9b74cd07fb414898caf4",
                    "name": "Daniil Ostashev",
                    "hidden": false
                },
                {
                    "_id": "68ec9b74cd07fb414898caf5",
                    "name": "R. Venkatesh Babu",
                    "hidden": false
                },
                {
                    "_id": "68ec9b74cd07fb414898caf6",
                    "name": "Daniel Cohen-Or",
                    "hidden": false
                },
                {
                    "_id": "68ec9b74cd07fb414898caf7",
                    "name": "Kuan-Chieh Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T17:51:03.000Z",
            "submittedOnDailyAt": "2025-10-15T20:11:03.013Z",
            "title": "Kontinuous Kontext: Continuous Strength Control for Instruction-based\n  Image Editing",
            "submittedOnDailyBy": {
                "_id": "62853516e483e0d37b354ce1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62853516e483e0d37b354ce1/t5Tyd3E07w26B9Z3XpZWI.jpeg",
                "isPro": false,
                "fullname": "Or Patashnik",
                "user": "orpatashnik",
                "type": "user"
            },
            "summary": "Instruction-based image editing offers a powerful and intuitive way to\nmanipulate images through natural language. Yet, relying solely on text\ninstructions limits fine-grained control over the extent of edits. We introduce\nKontinuous Kontext, an instruction-driven editing model that provides a new\ndimension of control over edit strength, enabling users to adjust edits\ngradually from no change to a fully realized result in a smooth and continuous\nmanner. Kontinuous Kontext extends a state-of-the-art image editing model to\naccept an additional input, a scalar edit strength which is then paired with\nthe edit instruction, enabling explicit control over the extent of the edit. To\ninject this scalar information, we train a lightweight projector network that\nmaps the input scalar and the edit instruction to coefficients in the model's\nmodulation space. For training our model, we synthesize a diverse dataset of\nimage-edit-instruction-strength quadruplets using existing generative models,\nfollowed by a filtering stage to ensure quality and consistency. Kontinuous\nKontext provides a unified approach for fine-grained control over edit strength\nfor instruction driven editing from subtle to strong across diverse operations\nsuch as stylization, attribute, material, background, and shape changes,\nwithout requiring attribute-specific training.",
            "upvotes": 2,
            "discussionId": "68ec9b75cd07fb414898caf8",
            "ai_summary": "Kontinuous Kontext is an instruction-driven image editing model that introduces a scalar edit strength for fine-grained control over the extent of edits, using a lightweight projector network and a synthesized dataset of image-edit-instruction-strength quadruplets.",
            "ai_keywords": [
                "instruction-driven editing",
                "edit strength",
                "scalar input",
                "lightweight projector network",
                "modulation space",
                "generative models",
                "stylization",
                "attribute changes",
                "material changes",
                "background changes",
                "shape changes"
            ]
        },
        "publishedAt": "2025-10-09T13:51:03.000Z",
        "title": "Kontinuous Kontext: Continuous Strength Control for Instruction-based\n  Image Editing",
        "summary": "Instruction-based image editing offers a powerful and intuitive way to\nmanipulate images through natural language. Yet, relying solely on text\ninstructions limits fine-grained control over the extent of edits. We introduce\nKontinuous Kontext, an instruction-driven editing model that provides a new\ndimension of control over edit strength, enabling users to adjust edits\ngradually from no change to a fully realized result in a smooth and continuous\nmanner. Kontinuous Kontext extends a state-of-the-art image editing model to\naccept an additional input, a scalar edit strength which is then paired with\nthe edit instruction, enabling explicit control over the extent of the edit. To\ninject this scalar information, we train a lightweight projector network that\nmaps the input scalar and the edit instruction to coefficients in the model's\nmodulation space. For training our model, we synthesize a diverse dataset of\nimage-edit-instruction-strength quadruplets using existing generative models,\nfollowed by a filtering stage to ensure quality and consistency. Kontinuous\nKontext provides a unified approach for fine-grained control over edit strength\nfor instruction driven editing from subtle to strong across diverse operations\nsuch as stylization, attribute, material, background, and shape changes,\nwithout requiring attribute-specific training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08532.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62853516e483e0d37b354ce1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62853516e483e0d37b354ce1/t5Tyd3E07w26B9Z3XpZWI.jpeg",
            "fullname": "Or Patashnik",
            "name": "orpatashnik",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.12793",
            "authors": [
                {
                    "_id": "68ef2552486b78128f0e34f0",
                    "name": "Long Cui",
                    "hidden": false
                },
                {
                    "_id": "68ef2552486b78128f0e34f1",
                    "name": "Weiyun Wang",
                    "hidden": false
                },
                {
                    "_id": "68ef2552486b78128f0e34f2",
                    "name": "Jie Shao",
                    "hidden": false
                },
                {
                    "_id": "68ef2552486b78128f0e34f3",
                    "name": "Zichen Wen",
                    "hidden": false
                },
                {
                    "_id": "68ef2552486b78128f0e34f4",
                    "name": "Gen Luo",
                    "hidden": false
                },
                {
                    "_id": "68ef2552486b78128f0e34f5",
                    "name": "Linfeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ef2552486b78128f0e34f6",
                    "name": "Yanting Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ef2552486b78128f0e34f7",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68ef2552486b78128f0e34f8",
                    "name": "Wenhai Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T17:58:10.000Z",
            "submittedOnDailyAt": "2025-10-15T03:14:40.386Z",
            "title": "ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution",
            "submittedOnDailyBy": {
                "_id": "68d8ba59f2f999edd0e25eea",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68d8ba59f2f999edd0e25eea/9GLAoaqtxo8c5LQtBt2oX.png",
                "isPro": false,
                "fullname": "Long Cui",
                "user": "CuiLong7",
                "type": "user"
            },
            "summary": "Existing Multimodal Large Language Models (MLLMs) suffer from increased\ninference costs due to the additional vision tokens introduced by image inputs.\nIn this work, we propose Visual Consistency Learning (ViCO), a novel training\nalgorithm that enables the model to represent images of varying semantic\ncomplexities using different numbers of vision tokens. The key idea behind our\nmethod is to employ multiple MLP connectors, each with a different image\ncompression ratio, to downsample the vision tokens based on the semantic\ncomplexity of the image. During training, we minimize the KL divergence between\nthe responses conditioned on different MLP connectors. At inference time, we\nintroduce an image router, termed Visual Resolution Router (ViR), that\nautomatically selects the appropriate compression rate for each image patch.\nCompared with existing dynamic high-resolution strategies, which adjust the\nnumber of visual tokens based on image resolutions, our method dynamically\nadapts the number of visual tokens according to semantic complexity.\nExperimental results demonstrate that our method can reduce the number of\nvision tokens by up to 50% while maintaining the model's perception, reasoning,\nand OCR capabilities. We hope this work will contribute to the development of\nmore efficient MLLMs. The code and models will be released to facilitate future\nresearch.",
            "upvotes": 1,
            "discussionId": "68ef2553486b78128f0e34f9",
            "organization": {
                "_id": "64006c57a3b8fe3ac0e9af7c",
                "name": "OpenGVLab",
                "fullname": "OpenGVLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64006c09330a45b03605bba3/FvdxiTkTqH8rKDOzGZGUE.jpeg"
            }
        },
        "publishedAt": "2025-10-14T13:58:10.000Z",
        "title": "ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution",
        "summary": "Existing Multimodal Large Language Models (MLLMs) suffer from increased\ninference costs due to the additional vision tokens introduced by image inputs.\nIn this work, we propose Visual Consistency Learning (ViCO), a novel training\nalgorithm that enables the model to represent images of varying semantic\ncomplexities using different numbers of vision tokens. The key idea behind our\nmethod is to employ multiple MLP connectors, each with a different image\ncompression ratio, to downsample the vision tokens based on the semantic\ncomplexity of the image. During training, we minimize the KL divergence between\nthe responses conditioned on different MLP connectors. At inference time, we\nintroduce an image router, termed Visual Resolution Router (ViR), that\nautomatically selects the appropriate compression rate for each image patch.\nCompared with existing dynamic high-resolution strategies, which adjust the\nnumber of visual tokens based on image resolutions, our method dynamically\nadapts the number of visual tokens according to semantic complexity.\nExperimental results demonstrate that our method can reduce the number of\nvision tokens by up to 50% while maintaining the model's perception, reasoning,\nand OCR capabilities. We hope this work will contribute to the development of\nmore efficient MLLMs. The code and models will be released to facilitate future\nresearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12793.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68d8ba59f2f999edd0e25eea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68d8ba59f2f999edd0e25eea/9GLAoaqtxo8c5LQtBt2oX.png",
            "fullname": "Long Cui",
            "name": "CuiLong7",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "64006c57a3b8fe3ac0e9af7c",
            "name": "OpenGVLab",
            "fullname": "OpenGVLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64006c09330a45b03605bba3/FvdxiTkTqH8rKDOzGZGUE.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.12497",
            "authors": [
                {
                    "_id": "68ef5e26007ec82ded771038",
                    "user": {
                        "_id": "658d7ae79a1397992a56430c",
                        "avatarUrl": "/avatars/15df2a99dea3af34f9864dbcf58dcc5f.svg",
                        "isPro": false,
                        "fullname": "Jincheng Zhong",
                        "user": "ConnorZhong",
                        "type": "user"
                    },
                    "name": "Jincheng Zhong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T15:24:34.707Z",
                    "hidden": false
                },
                {
                    "_id": "68ef5e26007ec82ded771039",
                    "name": "Boyuan Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ef5e26007ec82ded77103a",
                    "name": "Xin Tao",
                    "hidden": false
                },
                {
                    "_id": "68ef5e26007ec82ded77103b",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "68ef5e26007ec82ded77103c",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "68ef5e26007ec82ded77103d",
                    "name": "Mingsheng Long",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T13:31:34.000Z",
            "submittedOnDailyAt": "2025-10-15T14:34:10.977Z",
            "title": "Mitigating the Noise Shift for Denoising Generative Models via Noise\n  Awareness Guidance",
            "submittedOnDailyBy": {
                "_id": "658d7ae79a1397992a56430c",
                "avatarUrl": "/avatars/15df2a99dea3af34f9864dbcf58dcc5f.svg",
                "isPro": false,
                "fullname": "Jincheng Zhong",
                "user": "ConnorZhong",
                "type": "user"
            },
            "summary": "Existing denoising generative models rely on solving discretized reverse-time\nSDEs or ODEs. In this paper, we identify a long-overlooked yet pervasive issue\nin this family of models: a misalignment between the pre-defined noise level\nand the actual noise level encoded in intermediate states during sampling. We\nrefer to this misalignment as noise shift. Through empirical analysis, we\ndemonstrate that noise shift is widespread in modern diffusion models and\nexhibits a systematic bias, leading to sub-optimal generation due to both\nout-of-distribution generalization and inaccurate denoising updates. To address\nthis problem, we propose Noise Awareness Guidance (NAG), a simple yet effective\ncorrection method that explicitly steers sampling trajectories to remain\nconsistent with the pre-defined noise schedule. We further introduce a\nclassifier-free variant of NAG, which jointly trains a noise-conditional and a\nnoise-unconditional model via noise-condition dropout, thereby eliminating the\nneed for external classifiers. Extensive experiments, including ImageNet\ngeneration and various supervised fine-tuning tasks, show that NAG consistently\nmitigates noise shift and substantially improves the generation quality of\nmainstream diffusion models.",
            "upvotes": 1,
            "discussionId": "68ef5e26007ec82ded77103e",
            "ai_summary": "Noise Awareness Guidance (NAG) addresses noise shift in diffusion models by aligning sampling trajectories with the pre-defined noise schedule, improving generation quality.",
            "ai_keywords": [
                "denoising generative models",
                "discretized reverse-time SDEs",
                "ODEs",
                "noise shift",
                "pre-defined noise level",
                "out-of-distribution generalization",
                "inaccurate denoising updates",
                "Noise Awareness Guidance",
                "classifier-free variant",
                "noise-conditional",
                "noise-unconditional model",
                "noise-condition dropout",
                "ImageNet generation",
                "supervised fine-tuning tasks"
            ],
            "organization": {
                "_id": "6655a25cb88e4539b23a8236",
                "name": "thuml",
                "fullname": "THUML @ Tsinghua University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643b866bff50448bcfc7d1d1/5gCpGqp7wBU2TzRr_-SSA.jpeg"
            }
        },
        "publishedAt": "2025-10-14T09:31:34.000Z",
        "title": "Mitigating the Noise Shift for Denoising Generative Models via Noise\n  Awareness Guidance",
        "summary": "Existing denoising generative models rely on solving discretized reverse-time\nSDEs or ODEs. In this paper, we identify a long-overlooked yet pervasive issue\nin this family of models: a misalignment between the pre-defined noise level\nand the actual noise level encoded in intermediate states during sampling. We\nrefer to this misalignment as noise shift. Through empirical analysis, we\ndemonstrate that noise shift is widespread in modern diffusion models and\nexhibits a systematic bias, leading to sub-optimal generation due to both\nout-of-distribution generalization and inaccurate denoising updates. To address\nthis problem, we propose Noise Awareness Guidance (NAG), a simple yet effective\ncorrection method that explicitly steers sampling trajectories to remain\nconsistent with the pre-defined noise schedule. We further introduce a\nclassifier-free variant of NAG, which jointly trains a noise-conditional and a\nnoise-unconditional model via noise-condition dropout, thereby eliminating the\nneed for external classifiers. Extensive experiments, including ImageNet\ngeneration and various supervised fine-tuning tasks, show that NAG consistently\nmitigates noise shift and substantially improves the generation quality of\nmainstream diffusion models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12497.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "658d7ae79a1397992a56430c",
            "avatarUrl": "/avatars/15df2a99dea3af34f9864dbcf58dcc5f.svg",
            "fullname": "Jincheng Zhong",
            "name": "ConnorZhong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6655a25cb88e4539b23a8236",
            "name": "thuml",
            "fullname": "THUML @ Tsinghua University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643b866bff50448bcfc7d1d1/5gCpGqp7wBU2TzRr_-SSA.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.12323",
            "authors": [
                {
                    "_id": "68efa8045178b1b97206b296",
                    "name": "Zirui Guo",
                    "hidden": false
                },
                {
                    "_id": "68efa8045178b1b97206b297",
                    "name": "Xubin Ren",
                    "hidden": false
                },
                {
                    "_id": "68efa8045178b1b97206b298",
                    "name": "Lingrui Xu",
                    "hidden": false
                },
                {
                    "_id": "68efa8045178b1b97206b299",
                    "name": "Jiahao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68efa8045178b1b97206b29a",
                    "name": "Chao Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T09:25:35.000Z",
            "submittedOnDailyAt": "2025-10-15T12:27:59.630Z",
            "title": "RAG-Anything: All-in-One RAG Framework",
            "submittedOnDailyBy": {
                "_id": "631728389e6b629ba04d3012",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631728389e6b629ba04d3012/2yihb49Sw-ue_BKTlhZWE.jpeg",
                "isPro": false,
                "fullname": "Xubin Ren",
                "user": "Rbin",
                "type": "user"
            },
            "summary": "Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm\nfor expanding Large Language Models beyond their static training limitations.\nHowever, a critical misalignment exists between current RAG capabilities and\nreal-world information environments. Modern knowledge repositories are\ninherently multimodal, containing rich combinations of textual content, visual\nelements, structured tables, and mathematical expressions. Yet existing RAG\nframeworks are limited to textual content, creating fundamental gaps when\nprocessing multimodal documents. We present RAG-Anything, a unified framework\nthat enables comprehensive knowledge retrieval across all modalities. Our\napproach reconceptualizes multimodal content as interconnected knowledge\nentities rather than isolated data types. The framework introduces dual-graph\nconstruction to capture both cross-modal relationships and textual semantics\nwithin a unified representation. We develop cross-modal hybrid retrieval that\ncombines structural knowledge navigation with semantic matching. This enables\neffective reasoning over heterogeneous content where relevant evidence spans\nmultiple modalities. RAG-Anything demonstrates superior performance on\nchallenging multimodal benchmarks, achieving significant improvements over\nstate-of-the-art methods. Performance gains become particularly pronounced on\nlong documents where traditional approaches fail. Our framework establishes a\nnew paradigm for multimodal knowledge access, eliminating the architectural\nfragmentation that constrains current systems. Our framework is open-sourced\nat: https://github.com/HKUDS/RAG-Anything.",
            "upvotes": 1,
            "discussionId": "68efa8045178b1b97206b29b",
            "ai_summary": "RAG-Anything is a unified framework that enhances multimodal knowledge retrieval by integrating cross-modal relationships and semantic matching, outperforming existing methods on complex benchmarks.",
            "ai_keywords": [
                "Retrieval-Augmented Generation",
                "RAG",
                "Large Language Models",
                "multimodal",
                "textual content",
                "visual elements",
                "structured tables",
                "mathematical expressions",
                "dual-graph construction",
                "cross-modal hybrid retrieval",
                "structural knowledge navigation",
                "semantic matching",
                "multimodal benchmarks",
                "long documents"
            ],
            "organization": {
                "_id": "66b9f52ea0ebe4b8a533017d",
                "name": "hkuds",
                "fullname": "Data Intelligence Lab@HKU",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/631728389e6b629ba04d3012/_wpzD-zUk5LR0RMxifyNX.jpeg"
            }
        },
        "publishedAt": "2025-10-14T05:25:35.000Z",
        "title": "RAG-Anything: All-in-One RAG Framework",
        "summary": "Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm\nfor expanding Large Language Models beyond their static training limitations.\nHowever, a critical misalignment exists between current RAG capabilities and\nreal-world information environments. Modern knowledge repositories are\ninherently multimodal, containing rich combinations of textual content, visual\nelements, structured tables, and mathematical expressions. Yet existing RAG\nframeworks are limited to textual content, creating fundamental gaps when\nprocessing multimodal documents. We present RAG-Anything, a unified framework\nthat enables comprehensive knowledge retrieval across all modalities. Our\napproach reconceptualizes multimodal content as interconnected knowledge\nentities rather than isolated data types. The framework introduces dual-graph\nconstruction to capture both cross-modal relationships and textual semantics\nwithin a unified representation. We develop cross-modal hybrid retrieval that\ncombines structural knowledge navigation with semantic matching. This enables\neffective reasoning over heterogeneous content where relevant evidence spans\nmultiple modalities. RAG-Anything demonstrates superior performance on\nchallenging multimodal benchmarks, achieving significant improvements over\nstate-of-the-art methods. Performance gains become particularly pronounced on\nlong documents where traditional approaches fail. Our framework establishes a\nnew paradigm for multimodal knowledge access, eliminating the architectural\nfragmentation that constrains current systems. Our framework is open-sourced\nat: https://github.com/HKUDS/RAG-Anything.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12323.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631728389e6b629ba04d3012",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631728389e6b629ba04d3012/2yihb49Sw-ue_BKTlhZWE.jpeg",
            "fullname": "Xubin Ren",
            "name": "Rbin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "66b9f52ea0ebe4b8a533017d",
            "name": "hkuds",
            "fullname": "Data Intelligence Lab@HKU",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/631728389e6b629ba04d3012/_wpzD-zUk5LR0RMxifyNX.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.12117",
            "authors": [
                {
                    "_id": "68eefa47486b78128f0e3341",
                    "name": "Lipeng He",
                    "hidden": false
                },
                {
                    "_id": "68eefa47486b78128f0e3342",
                    "name": "Vasisht Duddu",
                    "hidden": false
                },
                {
                    "_id": "68eefa47486b78128f0e3343",
                    "name": "N. Asokan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T03:35:59.000Z",
            "submittedOnDailyAt": "2025-10-15T16:10:20.769Z",
            "title": "Locket: Robust Feature-Locking Technique for Language Models",
            "submittedOnDailyBy": {
                "_id": "6433707307bad11484af1d2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
                "isPro": false,
                "fullname": "Lipeng (Tony) He",
                "user": "ttttonyhe",
                "type": "user"
            },
            "summary": "Chatbot providers (e.g., OpenAI) rely on tiered subscription schemes to\ngenerate revenue, offering basic models for free users, and advanced models for\npaying subscribers. However, a finer-grained pay-to-unlock scheme for premium\nfeatures (e.g., math, coding) is thought to be more economically viable for the\nproviders. Such a scheme requires a feature-locking technique (FLoTE) which is\n(i) effective in refusing locked features, (ii) utility-preserving for unlocked\nfeatures, (iii) robust against evasion or unauthorized credential sharing, and\n(iv) scalable to multiple features and users. However, existing FLoTEs (e.g.,\npassword-locked models) are not robust or scalable. We present Locket, the\nfirst robust and scalable FLoTE to enable pay-to-unlock schemes. Locket uses a\nnovel merging approach to attach adapters to an LLM for refusing unauthorized\nfeatures. Our comprehensive evaluation shows that Locket is effective (100%\nrefusal on locked features), utility-preserving (leq 7% utility degradation\nin unlocked features), robust (leq 5% attack success rate), and scales to\nmultiple features and clients.",
            "upvotes": 1,
            "discussionId": "68eefa48486b78128f0e3344",
            "organization": {
                "_id": "6230c3ced93e84e2338765f3",
                "name": "UWaterloo",
                "fullname": "University of Waterloo",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1647363004040-6230c34bacb94a81eec91ed5.png"
            }
        },
        "publishedAt": "2025-10-13T23:35:59.000Z",
        "title": "Locket: Robust Feature-Locking Technique for Language Models",
        "summary": "Chatbot providers (e.g., OpenAI) rely on tiered subscription schemes to\ngenerate revenue, offering basic models for free users, and advanced models for\npaying subscribers. However, a finer-grained pay-to-unlock scheme for premium\nfeatures (e.g., math, coding) is thought to be more economically viable for the\nproviders. Such a scheme requires a feature-locking technique (FLoTE) which is\n(i) effective in refusing locked features, (ii) utility-preserving for unlocked\nfeatures, (iii) robust against evasion or unauthorized credential sharing, and\n(iv) scalable to multiple features and users. However, existing FLoTEs (e.g.,\npassword-locked models) are not robust or scalable. We present Locket, the\nfirst robust and scalable FLoTE to enable pay-to-unlock schemes. Locket uses a\nnovel merging approach to attach adapters to an LLM for refusing unauthorized\nfeatures. Our comprehensive evaluation shows that Locket is effective (100%\nrefusal on locked features), utility-preserving (leq 7% utility degradation\nin unlocked features), robust (leq 5% attack success rate), and scales to\nmultiple features and clients.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12117.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6433707307bad11484af1d2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
            "fullname": "Lipeng (Tony) He",
            "name": "ttttonyhe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "6230c3ced93e84e2338765f3",
            "name": "UWaterloo",
            "fullname": "University of Waterloo",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1647363004040-6230c34bacb94a81eec91ed5.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11851",
            "authors": [
                {
                    "_id": "68efaab15178b1b97206b2a3",
                    "name": "Shuo Chen",
                    "hidden": false
                },
                {
                    "_id": "68efaab15178b1b97206b2a4",
                    "name": "Zonggen Li",
                    "hidden": false
                },
                {
                    "_id": "68efaab15178b1b97206b2a5",
                    "name": "Zhen Han",
                    "hidden": false
                },
                {
                    "_id": "68efaab15178b1b97206b2a6",
                    "name": "Bailan He",
                    "hidden": false
                },
                {
                    "_id": "68efaab15178b1b97206b2a7",
                    "name": "Tong Liu",
                    "hidden": false
                },
                {
                    "_id": "68efaab15178b1b97206b2a8",
                    "name": "Haokun Chen",
                    "hidden": false
                },
                {
                    "_id": "68efaab15178b1b97206b2a9",
                    "name": "Georg Groh",
                    "hidden": false
                },
                {
                    "_id": "68efaab15178b1b97206b2aa",
                    "name": "Philip Torr",
                    "hidden": false
                },
                {
                    "_id": "68efaab15178b1b97206b2ab",
                    "name": "Volker Tresp",
                    "hidden": false
                },
                {
                    "_id": "68efaab15178b1b97206b2ac",
                    "name": "Jindong Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T19:05:00.000Z",
            "submittedOnDailyAt": "2025-10-15T12:39:09.662Z",
            "title": "Deep Research Brings Deeper Harm",
            "submittedOnDailyBy": {
                "_id": "648cbea3dee03837c823cbf2",
                "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
                "isPro": false,
                "fullname": "Shuo Chen",
                "user": "ShuoChen99",
                "type": "user"
            },
            "summary": "Deep Research (DR) agents built on Large Language Models (LLMs) can perform\ncomplex, multi-step research by decomposing tasks, retrieving online\ninformation, and synthesizing detailed reports. However, the misuse of LLMs\nwith such powerful capabilities can lead to even greater risks. This is\nespecially concerning in high-stakes and knowledge-intensive domains such as\nbiosecurity, where DR can generate a professional report containing detailed\nforbidden knowledge. Unfortunately, we have found such risks in practice:\nsimply submitting a harmful query, which a standalone LLM directly rejects, can\nelicit a detailed and dangerous report from DR agents. This highlights the\nelevated risks and underscores the need for a deeper safety analysis. Yet,\njailbreak methods designed for LLMs fall short in exposing such unique risks,\nas they do not target the research ability of DR agents. To address this gap,\nwe propose two novel jailbreak strategies: Plan Injection, which injects\nmalicious sub-goals into the agent's plan; and Intent Hijack, which reframes\nharmful queries as academic research questions. We conducted extensive\nexperiments across different LLMs and various safety benchmarks, including\ngeneral and biosecurity forbidden prompts. These experiments reveal 3 key\nfindings: (1) Alignment of the LLMs often fail in DR agents, where harmful\nprompts framed in academic terms can hijack agent intent; (2) Multi-step\nplanning and execution weaken the alignment, revealing systemic vulnerabilities\nthat prompt-level safeguards cannot address; (3) DR agents not only bypass\nrefusals but also produce more coherent, professional, and dangerous content,\ncompared with standalone LLMs. These results demonstrate a fundamental\nmisalignment in DR agents and call for better alignment techniques tailored to\nDR agents. Code and datasets are available at\nhttps://chenxshuo.github.io/deeper-harm.",
            "upvotes": 1,
            "discussionId": "68efaab25178b1b97206b2ad",
            "ai_summary": "DR agents based on LLMs can generate detailed reports from harmful queries, highlighting alignment failures and the need for specialized safety measures.",
            "ai_keywords": [
                "Deep Research (DR) agents",
                "Large Language Models (LLMs)",
                "Plan Injection",
                "Intent Hijack",
                "multi-step planning",
                "alignment",
                "forbidden knowledge",
                "biosecurity",
                "safety benchmarks",
                "prompt-level safeguards"
            ]
        },
        "publishedAt": "2025-10-13T15:05:00.000Z",
        "title": "Deep Research Brings Deeper Harm",
        "summary": "Deep Research (DR) agents built on Large Language Models (LLMs) can perform\ncomplex, multi-step research by decomposing tasks, retrieving online\ninformation, and synthesizing detailed reports. However, the misuse of LLMs\nwith such powerful capabilities can lead to even greater risks. This is\nespecially concerning in high-stakes and knowledge-intensive domains such as\nbiosecurity, where DR can generate a professional report containing detailed\nforbidden knowledge. Unfortunately, we have found such risks in practice:\nsimply submitting a harmful query, which a standalone LLM directly rejects, can\nelicit a detailed and dangerous report from DR agents. This highlights the\nelevated risks and underscores the need for a deeper safety analysis. Yet,\njailbreak methods designed for LLMs fall short in exposing such unique risks,\nas they do not target the research ability of DR agents. To address this gap,\nwe propose two novel jailbreak strategies: Plan Injection, which injects\nmalicious sub-goals into the agent's plan; and Intent Hijack, which reframes\nharmful queries as academic research questions. We conducted extensive\nexperiments across different LLMs and various safety benchmarks, including\ngeneral and biosecurity forbidden prompts. These experiments reveal 3 key\nfindings: (1) Alignment of the LLMs often fail in DR agents, where harmful\nprompts framed in academic terms can hijack agent intent; (2) Multi-step\nplanning and execution weaken the alignment, revealing systemic vulnerabilities\nthat prompt-level safeguards cannot address; (3) DR agents not only bypass\nrefusals but also produce more coherent, professional, and dangerous content,\ncompared with standalone LLMs. These results demonstrate a fundamental\nmisalignment in DR agents and call for better alignment techniques tailored to\nDR agents. Code and datasets are available at\nhttps://chenxshuo.github.io/deeper-harm.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11851.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648cbea3dee03837c823cbf2",
            "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
            "fullname": "Shuo Chen",
            "name": "ShuoChen99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11570",
            "authors": [
                {
                    "_id": "68efaac35178b1b97206b2b7",
                    "name": "Shuo Chen",
                    "hidden": false
                },
                {
                    "_id": "68efaac35178b1b97206b2b8",
                    "name": "Zhen Han",
                    "hidden": false
                },
                {
                    "_id": "68efaac35178b1b97206b2b9",
                    "name": "Haokun Chen",
                    "hidden": false
                },
                {
                    "_id": "68efaac35178b1b97206b2ba",
                    "name": "Bailan He",
                    "hidden": false
                },
                {
                    "_id": "68efaac35178b1b97206b2bb",
                    "name": "Shengyun Si",
                    "hidden": false
                },
                {
                    "_id": "68efaac35178b1b97206b2bc",
                    "name": "Jingpei Wu",
                    "hidden": false
                },
                {
                    "_id": "68efaac35178b1b97206b2bd",
                    "name": "Philip Torr",
                    "hidden": false
                },
                {
                    "_id": "68efaac35178b1b97206b2be",
                    "name": "Volker Tresp",
                    "hidden": false
                },
                {
                    "_id": "68efaac35178b1b97206b2bf",
                    "name": "Jindong Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T16:16:44.000Z",
            "submittedOnDailyAt": "2025-10-15T12:38:40.027Z",
            "title": "Bag of Tricks for Subverting Reasoning-based Safety Guardrails",
            "submittedOnDailyBy": {
                "_id": "648cbea3dee03837c823cbf2",
                "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
                "isPro": false,
                "fullname": "Shuo Chen",
                "user": "ShuoChen99",
                "type": "user"
            },
            "summary": "Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs),\nsuch as deliberative alignment, have shown strong defense against jailbreak\nattacks. By leveraging LRMs' reasoning ability, these guardrails help the\nmodels to assess the safety of user inputs before generating final responses.\nThe powerful reasoning ability can analyze the intention of the input query and\nwill refuse to assist once it detects the harmful intent hidden by the\njailbreak methods. Such guardrails have shown a significant boost in defense,\nsuch as the near-perfect refusal rates on the open-source gpt-oss series.\nUnfortunately, we find that these powerful reasoning-based guardrails can be\nextremely vulnerable to subtle manipulation of the input prompts, and once\nhijacked, can lead to even more harmful results. Specifically, we first uncover\na surprisingly fragile aspect of these guardrails: simply adding a few template\ntokens to the input prompt can successfully bypass the seemingly powerful\nguardrails and lead to explicit and harmful responses. To explore further, we\nintroduce a bag of jailbreak methods that subvert the reasoning-based\nguardrails. Our attacks span white-, gray-, and black-box settings and range\nfrom effortless template manipulations to fully automated optimization. Along\nwith the potential for scalable implementation, these methods also achieve\nalarmingly high attack success rates (e.g., exceeding 90% across 5 different\nbenchmarks on gpt-oss series on both local host models and online API\nservices). Evaluations across various leading open-source LRMs confirm that\nthese vulnerabilities are systemic, underscoring the urgent need for stronger\nalignment techniques for open-sourced LRMs to prevent malicious misuse. Code is\nopen-sourced at https://chenxshuo.github.io/bag-of-tricks.",
            "upvotes": 1,
            "discussionId": "68efaac45178b1b97206b2c0",
            "ai_summary": "Reasoning-based safety guardrails in Large Reasoning Models are vulnerable to subtle prompt manipulations, leading to high attack success rates across various benchmarks.",
            "ai_keywords": [
                "deliberative alignment",
                "jailbreak attacks",
                "reasoning ability",
                "input prompts",
                "template tokens",
                "white-box",
                "gray-box",
                "black-box",
                "automated optimization",
                "alignment techniques"
            ]
        },
        "publishedAt": "2025-10-13T12:16:44.000Z",
        "title": "Bag of Tricks for Subverting Reasoning-based Safety Guardrails",
        "summary": "Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs),\nsuch as deliberative alignment, have shown strong defense against jailbreak\nattacks. By leveraging LRMs' reasoning ability, these guardrails help the\nmodels to assess the safety of user inputs before generating final responses.\nThe powerful reasoning ability can analyze the intention of the input query and\nwill refuse to assist once it detects the harmful intent hidden by the\njailbreak methods. Such guardrails have shown a significant boost in defense,\nsuch as the near-perfect refusal rates on the open-source gpt-oss series.\nUnfortunately, we find that these powerful reasoning-based guardrails can be\nextremely vulnerable to subtle manipulation of the input prompts, and once\nhijacked, can lead to even more harmful results. Specifically, we first uncover\na surprisingly fragile aspect of these guardrails: simply adding a few template\ntokens to the input prompt can successfully bypass the seemingly powerful\nguardrails and lead to explicit and harmful responses. To explore further, we\nintroduce a bag of jailbreak methods that subvert the reasoning-based\nguardrails. Our attacks span white-, gray-, and black-box settings and range\nfrom effortless template manipulations to fully automated optimization. Along\nwith the potential for scalable implementation, these methods also achieve\nalarmingly high attack success rates (e.g., exceeding 90% across 5 different\nbenchmarks on gpt-oss series on both local host models and online API\nservices). Evaluations across various leading open-source LRMs confirm that\nthese vulnerabilities are systemic, underscoring the urgent need for stronger\nalignment techniques for open-sourced LRMs to prevent malicious misuse. Code is\nopen-sourced at https://chenxshuo.github.io/bag-of-tricks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11570.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648cbea3dee03837c823cbf2",
            "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
            "fullname": "Shuo Chen",
            "name": "ShuoChen99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11545",
            "authors": [
                {
                    "_id": "68ef4a3a486b78128f0e35da",
                    "name": "Jiayu Ding",
                    "hidden": false
                },
                {
                    "_id": "68ef4a3a486b78128f0e35db",
                    "user": {
                        "_id": "5f5a4fbe10b2753d9000c7fc",
                        "avatarUrl": "/avatars/0763b25c13733ef4eb757862ba6d603e.svg",
                        "isPro": false,
                        "fullname": "Lei Cui",
                        "user": "wolfshow",
                        "type": "user"
                    },
                    "name": "Lei Cui",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T15:24:43.386Z",
                    "hidden": false
                },
                {
                    "_id": "68ef4a3a486b78128f0e35dc",
                    "name": "Li Dong",
                    "hidden": false
                },
                {
                    "_id": "68ef4a3a486b78128f0e35dd",
                    "name": "Nanning Zheng",
                    "hidden": false
                },
                {
                    "_id": "68ef4a3a486b78128f0e35de",
                    "name": "Furu Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T15:42:11.000Z",
            "submittedOnDailyAt": "2025-10-15T06:08:27.466Z",
            "title": "Information-Preserving Reformulation of Reasoning Traces for\n  Antidistillation",
            "submittedOnDailyBy": {
                "_id": "667d9460a5d952954ce9f1c4",
                "avatarUrl": "/avatars/340b2503acad0cd692f46e21b9358bd0.svg",
                "isPro": false,
                "fullname": "Jiayu Ding",
                "user": "JiayuDing",
                "type": "user"
            },
            "summary": "Recent advances in Large Language Models (LLMs) show that extending the\nlength of reasoning chains significantly improves performance on complex tasks.\nWhile revealing these reasoning traces helps users better follow, verify, and\nlearn from the model's problem-solving process, it also makes them highly\nvulnerable to unauthorized distillation. To mitigate this risk, proprietary\nmodel providers often adopt aggressive protection strategies, such as replacing\ndetailed reasoning with brief summaries, which deprive users of valuable\nintermediate information. To address this trade-off, we propose PART, an\ninformation-preserving antidistillation reformulation of reasoning traces.\nMotivated by the difference between how humans understand reasoning traces and\nhow LLMs exploit them for supervised fine-tuning, we design a simple but\neffective two-step reformulation: removing self-talk behaviors and reordering\nsub-conclusions. A small auxiliary model is trained to perform this\nreformulation, incurring minimal computational overhead. Extensive experiments\ndemonstrate that PART consistently disrupts distillation across student models\nof different sizes and types on various reasoning benchmarks. For instance,\nwhen training on reformulated traces, even the performance of a large 32B\nstudent model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a\n13.5% degradation.",
            "upvotes": 1,
            "discussionId": "68ef4a3b486b78128f0e35df",
            "ai_summary": "PART reformulates reasoning traces to preserve information while disrupting unauthorized distillation in Large Language Models.",
            "ai_keywords": [
                "Large Language Models",
                "reasoning chains",
                "reasoning traces",
                "unauthorized distillation",
                "information-preserving antidistillation",
                "self-talk behaviors",
                "sub-conclusions",
                "auxiliary model",
                "reasoning benchmarks",
                "AIME 2024"
            ],
            "organization": {
                "_id": "5e6485f787403103f9f1055e",
                "name": "microsoft",
                "fullname": "Microsoft",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
            }
        },
        "publishedAt": "2025-10-13T11:42:11.000Z",
        "title": "Information-Preserving Reformulation of Reasoning Traces for\n  Antidistillation",
        "summary": "Recent advances in Large Language Models (LLMs) show that extending the\nlength of reasoning chains significantly improves performance on complex tasks.\nWhile revealing these reasoning traces helps users better follow, verify, and\nlearn from the model's problem-solving process, it also makes them highly\nvulnerable to unauthorized distillation. To mitigate this risk, proprietary\nmodel providers often adopt aggressive protection strategies, such as replacing\ndetailed reasoning with brief summaries, which deprive users of valuable\nintermediate information. To address this trade-off, we propose PART, an\ninformation-preserving antidistillation reformulation of reasoning traces.\nMotivated by the difference between how humans understand reasoning traces and\nhow LLMs exploit them for supervised fine-tuning, we design a simple but\neffective two-step reformulation: removing self-talk behaviors and reordering\nsub-conclusions. A small auxiliary model is trained to perform this\nreformulation, incurring minimal computational overhead. Extensive experiments\ndemonstrate that PART consistently disrupts distillation across student models\nof different sizes and types on various reasoning benchmarks. For instance,\nwhen training on reformulated traces, even the performance of a large 32B\nstudent model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a\n13.5% degradation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11545.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "667d9460a5d952954ce9f1c4",
            "avatarUrl": "/avatars/340b2503acad0cd692f46e21b9358bd0.svg",
            "fullname": "Jiayu Ding",
            "name": "JiayuDing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "5e6485f787403103f9f1055e",
            "name": "microsoft",
            "fullname": "Microsoft",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11330",
            "authors": [
                {
                    "_id": "68ef61fa121c484bb15d2db4",
                    "user": {
                        "_id": "67061a90d66c2d3af1956070",
                        "avatarUrl": "/avatars/3f9ea921ce621363e0f2ac1f03ba99da.svg",
                        "isPro": false,
                        "fullname": "Kihyun Nam",
                        "user": "southKH",
                        "type": "user"
                    },
                    "name": "KiHyun Nam",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T15:24:26.040Z",
                    "hidden": false
                },
                {
                    "_id": "68ef61fa121c484bb15d2db5",
                    "name": "Jongmin Choi",
                    "hidden": false
                },
                {
                    "_id": "68ef61fa121c484bb15d2db6",
                    "name": "Hyeongkeun Lee",
                    "hidden": false
                },
                {
                    "_id": "68ef61fa121c484bb15d2db7",
                    "name": "Jungwoo Heo",
                    "hidden": false
                },
                {
                    "_id": "68ef61fa121c484bb15d2db8",
                    "name": "Joon Son Chung",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T12:25:33.000Z",
            "submittedOnDailyAt": "2025-10-15T14:07:27.972Z",
            "title": "Diffusion-Link: Diffusion Probabilistic Model for Bridging the\n  Audio-Text Modality Gap",
            "submittedOnDailyBy": {
                "_id": "67061a90d66c2d3af1956070",
                "avatarUrl": "/avatars/3f9ea921ce621363e0f2ac1f03ba99da.svg",
                "isPro": false,
                "fullname": "Kihyun Nam",
                "user": "southKH",
                "type": "user"
            },
            "summary": "Contrastive audio-language pretraining yields powerful joint representations,\nyet a persistent audio-text modality gap limits the benefits of coupling\nmultimodal encoders with large language models (LLMs). We present\nDiffusion-Link, a diffusion-based modality-bridging module that generatively\nmaps audio embeddings into the text-embedding distribution. The module is\ntrained at the output embedding from the frozen multimodal encoder and\nimplemented as a lightweight network with three residual MLP blocks. To assess\nthe effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on\nAutomatic Audio Captioning (AAC); to our knowledge, this is the first\napplication of diffusion-based modality bridging to AAC. We report two results.\n(1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link\nreduces the modality gap the most among prior diffusion-based methods and shows\na collective migration of audio embeddings toward the text distribution. (2)\nDownstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline\nachieves state-of-the-art on AudioCaps in both zero-shot and fully supervised\ncaptioning without external knowledge, with relative gains up to 52.5% and\n7.5%, respectively. These findings show that closing the modality gap is\npivotal for effective coupling between multimodal encoders and LLMs, and\ndiffusion-based modality bridging offers a promising direction beyond\nknowledge-retrieval-centric designs. Code will be released upon acceptance\nhttps://github.com/DevKiHyun/Diffusion-Link",
            "upvotes": 1,
            "discussionId": "68ef61fa121c484bb15d2db9",
            "ai_summary": "Diffusion-Link, a diffusion-based modality-bridging module, reduces the audio-text modality gap and enhances multimodal encoder-LLM coupling, achieving state-of-the-art performance in automatic audio captioning.",
            "ai_keywords": [
                "contrastive audio-language pretraining",
                "diffusion-based modality-bridging",
                "audio embeddings",
                "text-embedding distribution",
                "residual MLP blocks",
                "Automatic Audio Captioning",
                "modality-gap analysis",
                "similarity criteria",
                "geometric criteria",
                "zero-shot captioning",
                "fully supervised captioning",
                "AudioCaps"
            ]
        },
        "publishedAt": "2025-10-13T08:25:33.000Z",
        "title": "Diffusion-Link: Diffusion Probabilistic Model for Bridging the\n  Audio-Text Modality Gap",
        "summary": "Contrastive audio-language pretraining yields powerful joint representations,\nyet a persistent audio-text modality gap limits the benefits of coupling\nmultimodal encoders with large language models (LLMs). We present\nDiffusion-Link, a diffusion-based modality-bridging module that generatively\nmaps audio embeddings into the text-embedding distribution. The module is\ntrained at the output embedding from the frozen multimodal encoder and\nimplemented as a lightweight network with three residual MLP blocks. To assess\nthe effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on\nAutomatic Audio Captioning (AAC); to our knowledge, this is the first\napplication of diffusion-based modality bridging to AAC. We report two results.\n(1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link\nreduces the modality gap the most among prior diffusion-based methods and shows\na collective migration of audio embeddings toward the text distribution. (2)\nDownstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline\nachieves state-of-the-art on AudioCaps in both zero-shot and fully supervised\ncaptioning without external knowledge, with relative gains up to 52.5% and\n7.5%, respectively. These findings show that closing the modality gap is\npivotal for effective coupling between multimodal encoders and LLMs, and\ndiffusion-based modality bridging offers a promising direction beyond\nknowledge-retrieval-centric designs. Code will be released upon acceptance\nhttps://github.com/DevKiHyun/Diffusion-Link",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11330.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67061a90d66c2d3af1956070",
            "avatarUrl": "/avatars/3f9ea921ce621363e0f2ac1f03ba99da.svg",
            "fullname": "Kihyun Nam",
            "name": "southKH",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.09776",
            "authors": [
                {
                    "_id": "68eff24136f8b025381e17fb",
                    "name": "Yufa Zhou",
                    "hidden": false
                },
                {
                    "_id": "68eff24136f8b025381e17fc",
                    "name": "Yixiao Wang",
                    "hidden": false
                },
                {
                    "_id": "68eff24136f8b025381e17fd",
                    "name": "Surbhi Goel",
                    "hidden": false
                },
                {
                    "_id": "68eff24136f8b025381e17fe",
                    "name": "Anru R. Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T18:34:19.000Z",
            "submittedOnDailyAt": "2025-10-15T17:57:38.584Z",
            "title": "Why Do Transformers Fail to Forecast Time Series In-Context?",
            "submittedOnDailyBy": {
                "_id": "658ab894c4b2004663dff3ae",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658ab894c4b2004663dff3ae/oPRnFuW2Imaa2KkYWNbSf.jpeg",
                "isPro": false,
                "fullname": "YUFA ZHOU",
                "user": "MasterZhou",
                "type": "user"
            },
            "summary": "Time series forecasting (TSF) remains a challenging and largely unsolved\nproblem in machine learning, despite significant recent efforts leveraging\nLarge Language Models (LLMs), which predominantly rely on Transformer\narchitectures. Empirical evidence consistently shows that even powerful\nTransformers often fail to outperform much simpler models, e.g., linear models,\non TSF tasks; however, a rigorous theoretical understanding of this phenomenon\nremains limited. In this paper, we provide a theoretical analysis of\nTransformers' limitations for TSF through the lens of In-Context Learning (ICL)\ntheory. Specifically, under AR(p) data, we establish that: (1) Linear\nSelf-Attention (LSA) models cannot achieve lower expected MSE than\nclassical linear models for in-context forecasting; (2) as the context length\napproaches to infinity, LSA asymptotically recovers the optimal linear\npredictor; and (3) under Chain-of-Thought (CoT) style inference, predictions\ncollapse to the mean exponentially. We empirically validate these findings\nthrough carefully designed experiments. Our theory not only sheds light on\nseveral previously underexplored phenomena but also offers practical insights\nfor designing more effective forecasting architectures. We hope our work\nencourages the broader research community to revisit the fundamental\ntheoretical limitations of TSF and to critically evaluate the direct\napplication of increasingly sophisticated architectures without deeper\nscrutiny.",
            "upvotes": 1,
            "discussionId": "68eff24136f8b025381e17ff",
            "githubRepo": "https://github.com/MasterZhou1/ICL-Time-Series",
            "ai_summary": "Theoretical analysis reveals that Transformers, particularly Linear Self-Attention models, have limitations in time series forecasting compared to classical linear models, with predictions collapsing to the mean under Chain-of-Thought inference.",
            "ai_keywords": [
                "Transformers",
                "In-Context Learning",
                "Linear Self-Attention",
                "AR($p$) data",
                "expected MSE",
                "classical linear models",
                "Chain-of-Thought inference"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-10-10T14:34:19.000Z",
        "title": "Why Do Transformers Fail to Forecast Time Series In-Context?",
        "summary": "Time series forecasting (TSF) remains a challenging and largely unsolved\nproblem in machine learning, despite significant recent efforts leveraging\nLarge Language Models (LLMs), which predominantly rely on Transformer\narchitectures. Empirical evidence consistently shows that even powerful\nTransformers often fail to outperform much simpler models, e.g., linear models,\non TSF tasks; however, a rigorous theoretical understanding of this phenomenon\nremains limited. In this paper, we provide a theoretical analysis of\nTransformers' limitations for TSF through the lens of In-Context Learning (ICL)\ntheory. Specifically, under AR(p) data, we establish that: (1) Linear\nSelf-Attention (LSA) models cannot achieve lower expected MSE than\nclassical linear models for in-context forecasting; (2) as the context length\napproaches to infinity, LSA asymptotically recovers the optimal linear\npredictor; and (3) under Chain-of-Thought (CoT) style inference, predictions\ncollapse to the mean exponentially. We empirically validate these findings\nthrough carefully designed experiments. Our theory not only sheds light on\nseveral previously underexplored phenomena but also offers practical insights\nfor designing more effective forecasting architectures. We hope our work\nencourages the broader research community to revisit the fundamental\ntheoretical limitations of TSF and to critically evaluate the direct\napplication of increasingly sophisticated architectures without deeper\nscrutiny.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09776.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "658ab894c4b2004663dff3ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658ab894c4b2004663dff3ae/oPRnFuW2Imaa2KkYWNbSf.jpeg",
            "fullname": "YUFA ZHOU",
            "name": "MasterZhou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.09263",
            "authors": [
                {
                    "_id": "68ef7314121c484bb15d2e21",
                    "name": "Sven Gowal",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e22",
                    "name": "Rudy Bunel",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e23",
                    "name": "Florian Stimberg",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e24",
                    "name": "David Stutz",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e25",
                    "name": "Guillermo Ortiz-Jimenez",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e26",
                    "name": "Christina Kouridi",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e27",
                    "name": "Mel Vecerik",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e28",
                    "name": "Jamie Hayes",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e29",
                    "name": "Sylvestre-Alvise Rebuffi",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e2a",
                    "name": "Paul Bernard",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e2b",
                    "name": "Chris Gamble",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e2c",
                    "name": "Miklós Z. Horváth",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e2d",
                    "name": "Fabian Kaczmarczyck",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e2e",
                    "name": "Alex Kaskasoli",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e2f",
                    "name": "Aleksandar Petrov",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e30",
                    "name": "Ilia Shumailov",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e31",
                    "name": "Meghana Thotakuri",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e32",
                    "name": "Olivia Wiles",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e33",
                    "name": "Jessica Yung",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e34",
                    "name": "Zahra Ahmed",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e35",
                    "name": "Victor Martin",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e36",
                    "name": "Simon Rosen",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e37",
                    "name": "Christopher Savčak",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e38",
                    "name": "Armin Senoner",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e39",
                    "name": "Nidhi Vyas",
                    "hidden": false
                },
                {
                    "_id": "68ef7314121c484bb15d2e3a",
                    "name": "Pushmeet Kohli",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T11:03:31.000Z",
            "submittedOnDailyAt": "2025-10-15T08:41:14.250Z",
            "title": "SynthID-Image: Image watermarking at internet scale",
            "submittedOnDailyBy": {
                "_id": "6475c2794766357252e69e9f",
                "avatarUrl": "/avatars/757ed789423113369868e972f21ce559.svg",
                "isPro": false,
                "fullname": "i",
                "user": "iliashum",
                "type": "user"
            },
            "summary": "We introduce SynthID-Image, a deep learning-based system for invisibly\nwatermarking AI-generated imagery. This paper documents the technical\ndesiderata, threat models, and practical challenges of deploying such a system\nat internet scale, addressing key requirements of effectiveness, fidelity,\nrobustness, and security. SynthID-Image has been used to watermark over ten\nbillion images and video frames across Google's services and its corresponding\nverification service is available to trusted testers. For completeness, we\npresent an experimental evaluation of an external model variant, SynthID-O,\nwhich is available through partnerships. We benchmark SynthID-O against other\npost-hoc watermarking methods from the literature, demonstrating\nstate-of-the-art performance in both visual quality and robustness to common\nimage perturbations. While this work centers on visual media, the conclusions\non deployment, constraints, and threat modeling generalize to other modalities,\nincluding audio. This paper provides a comprehensive documentation for the\nlarge-scale deployment of deep learning-based media provenance systems.",
            "upvotes": 1,
            "discussionId": "68ef7314121c484bb15d2e3b",
            "ai_summary": "SynthID-Image, a deep learning system for watermarking AI-generated imagery, demonstrates state-of-the-art performance in visual quality and robustness, and is deployed across Google's services.",
            "ai_keywords": [
                "deep learning",
                "watermarking",
                "AI-generated imagery",
                "threat models",
                "robustness",
                "security",
                "visual quality",
                "image perturbations",
                "media provenance systems"
            ],
            "organization": {
                "_id": "5e6aca39878b8b2bf9806447",
                "name": "google",
                "fullname": "Google",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
            }
        },
        "publishedAt": "2025-10-10T07:03:31.000Z",
        "title": "SynthID-Image: Image watermarking at internet scale",
        "summary": "We introduce SynthID-Image, a deep learning-based system for invisibly\nwatermarking AI-generated imagery. This paper documents the technical\ndesiderata, threat models, and practical challenges of deploying such a system\nat internet scale, addressing key requirements of effectiveness, fidelity,\nrobustness, and security. SynthID-Image has been used to watermark over ten\nbillion images and video frames across Google's services and its corresponding\nverification service is available to trusted testers. For completeness, we\npresent an experimental evaluation of an external model variant, SynthID-O,\nwhich is available through partnerships. We benchmark SynthID-O against other\npost-hoc watermarking methods from the literature, demonstrating\nstate-of-the-art performance in both visual quality and robustness to common\nimage perturbations. While this work centers on visual media, the conclusions\non deployment, constraints, and threat modeling generalize to other modalities,\nincluding audio. This paper provides a comprehensive documentation for the\nlarge-scale deployment of deep learning-based media provenance systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09263.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6475c2794766357252e69e9f",
            "avatarUrl": "/avatars/757ed789423113369868e972f21ce559.svg",
            "fullname": "i",
            "name": "iliashum",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "5e6aca39878b8b2bf9806447",
            "name": "google",
            "fullname": "Google",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.09062",
            "authors": [
                {
                    "_id": "68eda21ede1fee572713a67d",
                    "user": {
                        "_id": "64b6076821c601b486d217a3",
                        "avatarUrl": "/avatars/bfd680028e10683b6c0544eb24006246.svg",
                        "isPro": false,
                        "fullname": "Chung-En, Sun",
                        "user": "cesun",
                        "type": "user"
                    },
                    "name": "Chung-En Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:31:19.643Z",
                    "hidden": false
                },
                {
                    "_id": "68eda21ede1fee572713a67e",
                    "name": "Ge Yan",
                    "hidden": false
                },
                {
                    "_id": "68eda21ede1fee572713a67f",
                    "name": "Akshay Kulkarni",
                    "hidden": false
                },
                {
                    "_id": "68eda21ede1fee572713a680",
                    "name": "Tsui-Wei Weng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T07:08:44.000Z",
            "submittedOnDailyAt": "2025-10-15T05:52:19.331Z",
            "title": "ReFIne: A Framework for Trustworthy Large Reasoning Models with\n  Reliability, Faithfulness, and Interpretability",
            "submittedOnDailyBy": {
                "_id": "64b6076821c601b486d217a3",
                "avatarUrl": "/avatars/bfd680028e10683b6c0544eb24006246.svg",
                "isPro": false,
                "fullname": "Chung-En, Sun",
                "user": "cesun",
                "type": "user"
            },
            "summary": "Recent advances in long chain-of-thought (CoT) reasoning have largely\nprioritized answer accuracy and token efficiency, while overlooking aspects\ncritical to trustworthiness. We argue that usable reasoning systems must be\ntrustworthy, characterized by three properties: interpretability, faithfulness,\nand reliability. To this end, we propose ReFIne, a new training framework that\nintegrates supervised fine-tuning with GRPO to encourage models to: (i) improve\ninterpretability by producing structured, tag-based traces with high-level\nplanning that are easier for humans to follow; (ii) enhance faithfulness by\nexplicitly disclosing the decisive information guiding each solution, with\nconsistent cross-section references; and (iii) promote reliability by providing\nself-assessments of both the derivation's soundness and the confidence of the\nfinal answer. We apply ReFIne to the Qwen3 models at multiple scales\n(1.7B/4B/8B) and evaluate across mathematical benchmarks of varying difficulty.\nOur experimental results show that ReFIne models generate clearer and\nbetter-structured reasoning traces (interpretability +44.0%), more faithfully\nexpose their underlying decision process (faithfulness +18.8%), and offer\ninformative confidence estimates (reliability +42.4%). These findings highlight\nan overlooked but important direction: reasoning models should be optimized not\nonly for accuracy, but also for broader dimensions of trustworthiness. Our code\nis available at:\nhttps://github.com/Trustworthy-ML-Lab/Training_Trustworthy_LRM_with_Refine",
            "upvotes": 1,
            "discussionId": "68eda21ede1fee572713a681",
            "ai_summary": "ReFIne, a new training framework, enhances the trustworthiness of reasoning models by improving interpretability, faithfulness, and reliability through structured traces, decisive information disclosure, and confidence estimates.",
            "ai_keywords": [
                "long chain-of-thought reasoning",
                "CoT reasoning",
                "ReFIne",
                "supervised fine-tuning",
                "GRPO",
                "interpretability",
                "faithfulness",
                "reliability",
                "structured traces",
                "high-level planning",
                "cross-section references",
                "self-assessments",
                "derivation soundness",
                "confidence estimates",
                "Qwen3 models",
                "mathematical benchmarks"
            ]
        },
        "publishedAt": "2025-10-10T03:08:44.000Z",
        "title": "ReFIne: A Framework for Trustworthy Large Reasoning Models with\n  Reliability, Faithfulness, and Interpretability",
        "summary": "Recent advances in long chain-of-thought (CoT) reasoning have largely\nprioritized answer accuracy and token efficiency, while overlooking aspects\ncritical to trustworthiness. We argue that usable reasoning systems must be\ntrustworthy, characterized by three properties: interpretability, faithfulness,\nand reliability. To this end, we propose ReFIne, a new training framework that\nintegrates supervised fine-tuning with GRPO to encourage models to: (i) improve\ninterpretability by producing structured, tag-based traces with high-level\nplanning that are easier for humans to follow; (ii) enhance faithfulness by\nexplicitly disclosing the decisive information guiding each solution, with\nconsistent cross-section references; and (iii) promote reliability by providing\nself-assessments of both the derivation's soundness and the confidence of the\nfinal answer. We apply ReFIne to the Qwen3 models at multiple scales\n(1.7B/4B/8B) and evaluate across mathematical benchmarks of varying difficulty.\nOur experimental results show that ReFIne models generate clearer and\nbetter-structured reasoning traces (interpretability +44.0%), more faithfully\nexpose their underlying decision process (faithfulness +18.8%), and offer\ninformative confidence estimates (reliability +42.4%). These findings highlight\nan overlooked but important direction: reasoning models should be optimized not\nonly for accuracy, but also for broader dimensions of trustworthiness. Our code\nis available at:\nhttps://github.com/Trustworthy-ML-Lab/Training_Trustworthy_LRM_with_Refine",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09062.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b6076821c601b486d217a3",
            "avatarUrl": "/avatars/bfd680028e10683b6c0544eb24006246.svg",
            "fullname": "Chung-En, Sun",
            "name": "cesun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.12269",
            "authors": [
                {
                    "_id": "68ef6d79121c484bb15d2ded",
                    "name": "Pedro Domingos",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T08:24:08.000Z",
            "submittedOnDailyAt": "2025-10-15T08:18:15.417Z",
            "title": "Tensor Logic: The Language of AI",
            "submittedOnDailyBy": {
                "_id": "60eeedbf50b60c406afc1291",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649111275459-60eeedbf50b60c406afc1291.png",
                "isPro": false,
                "fullname": "Samuel Arcadinho",
                "user": "SSamDav",
                "type": "user"
            },
            "summary": "Progress in AI is hindered by the lack of a programming language with all the\nrequisite features. Libraries like PyTorch and TensorFlow provide automatic\ndifferentiation and efficient GPU implementation, but are additions to Python,\nwhich was never intended for AI. Their lack of support for automated reasoning\nand knowledge acquisition has led to a long and costly series of hacky attempts\nto tack them on. On the other hand, AI languages like LISP an Prolog lack\nscalability and support for learning. This paper proposes tensor logic, a\nlanguage that solves these problems by unifying neural and symbolic AI at a\nfundamental level. The sole construct in tensor logic is the tensor equation,\nbased on the observation that logical rules and Einstein summation are\nessentially the same operation, and all else can be reduced to them. I show how\nto elegantly implement key forms of neural, symbolic and statistical AI in\ntensor logic, including transformers, formal reasoning, kernel machines and\ngraphical models. Most importantly, tensor logic makes new directions possible,\nsuch as sound reasoning in embedding space. This combines the scalability and\nlearnability of neural networks with the reliability and transparency of\nsymbolic reasoning, and is potentially a basis for the wider adoption of AI.",
            "upvotes": 0,
            "discussionId": "68ef6d7a121c484bb15d2dee",
            "projectPage": "https://tensor-logic.org/",
            "ai_summary": "Tensor logic unifies neural and symbolic AI by using tensor equations, enabling scalable, learnable, and transparent AI systems.",
            "ai_keywords": [
                "tensor logic",
                "tensor equation",
                "logical rules",
                "Einstein summation",
                "transformers",
                "formal reasoning",
                "kernel machines",
                "graphical models",
                "sound reasoning",
                "embedding space"
            ]
        },
        "publishedAt": "2025-10-14T04:24:08.000Z",
        "title": "Tensor Logic: The Language of AI",
        "summary": "Progress in AI is hindered by the lack of a programming language with all the\nrequisite features. Libraries like PyTorch and TensorFlow provide automatic\ndifferentiation and efficient GPU implementation, but are additions to Python,\nwhich was never intended for AI. Their lack of support for automated reasoning\nand knowledge acquisition has led to a long and costly series of hacky attempts\nto tack them on. On the other hand, AI languages like LISP an Prolog lack\nscalability and support for learning. This paper proposes tensor logic, a\nlanguage that solves these problems by unifying neural and symbolic AI at a\nfundamental level. The sole construct in tensor logic is the tensor equation,\nbased on the observation that logical rules and Einstein summation are\nessentially the same operation, and all else can be reduced to them. I show how\nto elegantly implement key forms of neural, symbolic and statistical AI in\ntensor logic, including transformers, formal reasoning, kernel machines and\ngraphical models. Most importantly, tensor logic makes new directions possible,\nsuch as sound reasoning in embedding space. This combines the scalability and\nlearnability of neural networks with the reliability and transparency of\nsymbolic reasoning, and is potentially a basis for the wider adoption of AI.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12269.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60eeedbf50b60c406afc1291",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649111275459-60eeedbf50b60c406afc1291.png",
            "fullname": "Samuel Arcadinho",
            "name": "SSamDav",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11967",
            "authors": [
                {
                    "_id": "68f0409136f8b025381e189e",
                    "name": "Weiwei Sun",
                    "hidden": false
                },
                {
                    "_id": "68f0409136f8b025381e189f",
                    "name": "Miao Lu",
                    "hidden": false
                },
                {
                    "_id": "68f0409136f8b025381e18a0",
                    "name": "Zhan Ling",
                    "hidden": false
                },
                {
                    "_id": "68f0409136f8b025381e18a1",
                    "name": "Kang Liu",
                    "hidden": false
                },
                {
                    "_id": "68f0409136f8b025381e18a2",
                    "name": "Xuesong Yao",
                    "hidden": false
                },
                {
                    "_id": "68f0409136f8b025381e18a3",
                    "name": "Yiming Yang",
                    "hidden": false
                },
                {
                    "_id": "68f0409136f8b025381e18a4",
                    "name": "Jiecao Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T22:00:58.000Z",
            "submittedOnDailyAt": "2025-10-15T23:17:38.675Z",
            "title": "Scaling Long-Horizon LLM Agent via Context-Folding",
            "submittedOnDailyBy": {
                "_id": "6143296e7215c6d505bafd55",
                "avatarUrl": "/avatars/23ffb6199103ab21f1b9c8903f08f85f.svg",
                "isPro": false,
                "fullname": "Weiwei Sun",
                "user": "sunweiwei",
                "type": "user"
            },
            "summary": "Large language model (LLM) agents are fundamentally constrained by context\nlength on long-horizon tasks. We introduce Context-Folding, a framework that\nempowers agents to actively manage their working context. An agent can\nprocedurally branch into a sub-trajectory to handle a subtask and then fold it\nupon completion, collapsing the intermediate steps while retaining a concise\nsummary of the outcome. To make this behavior learnable, we develop an\nend-to-end reinforcement learning framework FoldGRPO with specific process\nrewards to encourage effective task decomposition and context management. On\ncomplex long-horizon tasks (Deep Research and SWE), our folding agent matches\nor outperforms the ReAct baselines while using an active context 10times\nsmaller and significantly outperforms models that rely on summarization-based\ncontext management.",
            "upvotes": 0,
            "discussionId": "68f0409236f8b025381e18a5",
            "ai_summary": "Context-Folding, an end-to-end reinforcement learning framework, enables LLM agents to manage context effectively by branching into subtasks and folding them, outperforming baselines on long-horizon tasks with reduced context size.",
            "ai_keywords": [
                "Context-Folding",
                "reinforcement learning",
                "FoldGRPO",
                "task decomposition",
                "context management",
                "Deep Research",
                "SWE",
                "ReAct baselines",
                "summarization-based context management"
            ],
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-10-13T18:00:58.000Z",
        "title": "Scaling Long-Horizon LLM Agent via Context-Folding",
        "summary": "Large language model (LLM) agents are fundamentally constrained by context\nlength on long-horizon tasks. We introduce Context-Folding, a framework that\nempowers agents to actively manage their working context. An agent can\nprocedurally branch into a sub-trajectory to handle a subtask and then fold it\nupon completion, collapsing the intermediate steps while retaining a concise\nsummary of the outcome. To make this behavior learnable, we develop an\nend-to-end reinforcement learning framework FoldGRPO with specific process\nrewards to encourage effective task decomposition and context management. On\ncomplex long-horizon tasks (Deep Research and SWE), our folding agent matches\nor outperforms the ReAct baselines while using an active context 10times\nsmaller and significantly outperforms models that rely on summarization-based\ncontext management.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11967.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6143296e7215c6d505bafd55",
            "avatarUrl": "/avatars/23ffb6199103ab21f1b9c8903f08f85f.svg",
            "fullname": "Weiwei Sun",
            "name": "sunweiwei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.08666",
            "authors": [
                {
                    "_id": "68efa7065178b1b97206b27d",
                    "name": "Yuxin Ma",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b27e",
                    "name": "Lun Du",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b27f",
                    "name": "Lanning Wei",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b280",
                    "name": "Kun Chen",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b281",
                    "name": "Qian Xu",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b282",
                    "name": "Kangyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b283",
                    "name": "Guofeng Feng",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b284",
                    "name": "Guoshan Lu",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b285",
                    "name": "Lin Liu",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b286",
                    "name": "Xiaojing Qi",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b287",
                    "name": "Xinyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b288",
                    "name": "Zhen Tao",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b289",
                    "name": "Haibo Feng",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b28a",
                    "name": "Ziyun Jiang",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b28b",
                    "name": "Ying Xu",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b28c",
                    "name": "Zenan Huang",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b28d",
                    "name": "Yihong Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b28e",
                    "name": "Haokai Xu",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b28f",
                    "name": "Jiaqi Hu",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b290",
                    "name": "Zhenzhong Lan",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b291",
                    "name": "Junbo Zhao",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b292",
                    "name": "Jianguo Li",
                    "hidden": false
                },
                {
                    "_id": "68efa7065178b1b97206b293",
                    "name": "Da Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T16:19:42.000Z",
            "submittedOnDailyAt": "2025-10-15T13:02:51.918Z",
            "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
            "submittedOnDailyBy": {
                "_id": "66270e0026d5a3eee310ad53",
                "avatarUrl": "/avatars/db34068c114c348de296e00b1b5a5b9b.svg",
                "isPro": false,
                "fullname": "Da Zheng",
                "user": "zhengda1936",
                "type": "user"
            },
            "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on 8times H800 GPUs. Compared to\nprior systems, dInfer delivers a 10times speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a 2-3times speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
            "upvotes": 0,
            "discussionId": "68efa7065178b1b97206b294",
            "ai_summary": "dInfer is an efficient and extensible framework for diffusion-based large language model inference, achieving significant speedups over existing systems without compromising output quality.",
            "ai_keywords": [
                "diffusion-based large language models",
                "autoregressive LLMs",
                "denoising-based generation",
                "inference framework",
                "modular components",
                "diffusion iteration manager",
                "decoding strategy",
                "KV-cache manager",
                "algorithmic innovations",
                "system-level optimizations",
                "LLaDA-MoE",
                "HumanEval",
                "H800 GPUs",
                "Fast-dLLM",
                "QWen2.5-3B",
                "vLLM inference engine"
            ]
        },
        "publishedAt": "2025-10-09T12:19:42.000Z",
        "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
        "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on 8times H800 GPUs. Compared to\nprior systems, dInfer delivers a 10times speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a 2-3times speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08666.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66270e0026d5a3eee310ad53",
            "avatarUrl": "/avatars/db34068c114c348de296e00b1b5a5b9b.svg",
            "fullname": "Da Zheng",
            "name": "zhengda1936",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.06727",
            "authors": [
                {
                    "_id": "68f0412436f8b025381e18a7",
                    "name": "Miao Lu",
                    "hidden": false
                },
                {
                    "_id": "68f0412436f8b025381e18a8",
                    "name": "Weiwei Sun",
                    "hidden": false
                },
                {
                    "_id": "68f0412436f8b025381e18a9",
                    "name": "Weihua Du",
                    "hidden": false
                },
                {
                    "_id": "68f0412436f8b025381e18aa",
                    "name": "Zhan Ling",
                    "hidden": false
                },
                {
                    "_id": "68f0412436f8b025381e18ab",
                    "name": "Xuesong Yao",
                    "hidden": false
                },
                {
                    "_id": "68f0412436f8b025381e18ac",
                    "name": "Kang Liu",
                    "hidden": false
                },
                {
                    "_id": "68f0412436f8b025381e18ad",
                    "name": "Jiecao Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T07:29:22.000Z",
            "submittedOnDailyAt": "2025-10-15T23:20:07.281Z",
            "title": "Scaling LLM Multi-turn RL with End-to-end Summarization-based Context\n  Management",
            "submittedOnDailyBy": {
                "_id": "6143296e7215c6d505bafd55",
                "avatarUrl": "/avatars/23ffb6199103ab21f1b9c8903f08f85f.svg",
                "isPro": false,
                "fullname": "Weiwei Sun",
                "user": "sunweiwei",
                "type": "user"
            },
            "summary": "We study reinforcement learning (RL) fine-tuning of large language model\n(LLM) agents for long-horizon multi-turn tool use, where context length quickly\nbecomes a fundamental bottleneck. Existing RL pipelines can suffer from\ndegraded instruction following, excessive rollout costs, and most importantly,\nstrict context limits. To address these challenges, we introduce\nsummarization-based context management to training. In specific, it\nperiodically compresses the tool using history by LLM-generated summaries that\nretain task-relevant information to keep a compact context while enabling the\nagent to scale beyond the fixed context window. Building on this formulation,\nwe derive a policy gradient representation that seamlessly enables standard LLM\nRL infrastructures to optimize both tool-use behaviors as well as summarization\nstrategies in an end-to-end fashion. We instantiate this framework with\nSUmmarization augmented Policy Optimization\n(SUPO), an LLM RL algorithm that enables long-horizon training beyond\na fixed context limit. Experiments on interactive function calling and\nsearching tasks demonstrate that SUPO significantly improves the\nsuccess rate while maintaining the same or even lower working context length\ncompared to baselines. We also demonstrate that for complex searching tasks,\nSUPO can further improve the evaluation performance when scaling\ntest-time maximum round of summarization beyond that of training time. Our\nresults establish summarization-based context management as a principled and\nscalable approach for training RL agents beyond a fixed context length limit.",
            "upvotes": 0,
            "discussionId": "68f0412536f8b025381e18ae",
            "ai_summary": "Summarization-based context management in reinforcement learning fine-tunes large language model agents for long-horizon tool use, improving success rates and scalability beyond fixed context limits.",
            "ai_keywords": [
                "reinforcement learning",
                "large language model",
                "long-horizon",
                "multi-turn tool use",
                "context length",
                "summarization-based context management",
                "policy gradient",
                "SUPO",
                "interactive function calling",
                "searching tasks"
            ]
        },
        "publishedAt": "2025-10-08T03:29:22.000Z",
        "title": "Scaling LLM Multi-turn RL with End-to-end Summarization-based Context\n  Management",
        "summary": "We study reinforcement learning (RL) fine-tuning of large language model\n(LLM) agents for long-horizon multi-turn tool use, where context length quickly\nbecomes a fundamental bottleneck. Existing RL pipelines can suffer from\ndegraded instruction following, excessive rollout costs, and most importantly,\nstrict context limits. To address these challenges, we introduce\nsummarization-based context management to training. In specific, it\nperiodically compresses the tool using history by LLM-generated summaries that\nretain task-relevant information to keep a compact context while enabling the\nagent to scale beyond the fixed context window. Building on this formulation,\nwe derive a policy gradient representation that seamlessly enables standard LLM\nRL infrastructures to optimize both tool-use behaviors as well as summarization\nstrategies in an end-to-end fashion. We instantiate this framework with\nSUmmarization augmented Policy Optimization\n(SUPO), an LLM RL algorithm that enables long-horizon training beyond\na fixed context limit. Experiments on interactive function calling and\nsearching tasks demonstrate that SUPO significantly improves the\nsuccess rate while maintaining the same or even lower working context length\ncompared to baselines. We also demonstrate that for complex searching tasks,\nSUPO can further improve the evaluation performance when scaling\ntest-time maximum round of summarization beyond that of training time. Our\nresults establish summarization-based context management as a principled and\nscalable approach for training RL agents beyond a fixed context length limit.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06727.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6143296e7215c6d505bafd55",
            "avatarUrl": "/avatars/23ffb6199103ab21f1b9c8903f08f85f.svg",
            "fullname": "Weiwei Sun",
            "name": "sunweiwei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]