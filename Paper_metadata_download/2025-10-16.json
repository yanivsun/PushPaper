[
    {
        "paper": {
            "id": "2510.13344",
            "authors": [
                {
                    "_id": "68f03fba36f8b025381e188c",
                    "name": "Zhenyu Liu",
                    "hidden": false
                },
                {
                    "_id": "68f03fba36f8b025381e188d",
                    "name": "Yunxin Li",
                    "hidden": false
                },
                {
                    "_id": "68f03fba36f8b025381e188e",
                    "name": "Xuanyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f03fba36f8b025381e188f",
                    "name": "Qixun Teng",
                    "hidden": false
                },
                {
                    "_id": "68f03fba36f8b025381e1890",
                    "name": "Shenyuan Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f03fba36f8b025381e1891",
                    "name": "Xinyu Chen",
                    "hidden": false
                },
                {
                    "_id": "68f03fba36f8b025381e1892",
                    "user": {
                        "_id": "652fb8bcc9dd2692a25ef2e3",
                        "avatarUrl": "/avatars/461e6cc1c3441cde18192b080b0b8576.svg",
                        "isPro": false,
                        "fullname": "Haoyuan Shi",
                        "user": "MrSunshy",
                        "type": "user"
                    },
                    "name": "Haoyuan Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:39:35.490Z",
                    "hidden": false
                },
                {
                    "_id": "68f03fba36f8b025381e1893",
                    "name": "Jinchao Li",
                    "hidden": false
                },
                {
                    "_id": "68f03fba36f8b025381e1894",
                    "name": "Qi Wang",
                    "hidden": false
                },
                {
                    "_id": "68f03fba36f8b025381e1895",
                    "name": "Haolan Chen",
                    "hidden": false
                },
                {
                    "_id": "68f03fba36f8b025381e1896",
                    "name": "Fanbo Meng",
                    "hidden": false
                },
                {
                    "_id": "68f03fba36f8b025381e1897",
                    "name": "Mingjun Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f03fba36f8b025381e1898",
                    "name": "Yu Xu",
                    "hidden": false
                },
                {
                    "_id": "68f03fba36f8b025381e1899",
                    "name": "Yancheng He",
                    "hidden": false
                },
                {
                    "_id": "68f03fba36f8b025381e189a",
                    "name": "Baotian Hu",
                    "hidden": false
                },
                {
                    "_id": "68f03fba36f8b025381e189b",
                    "name": "Min Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T09:30:25.000Z",
            "submittedOnDailyAt": "2025-10-16T01:03:52.180Z",
            "title": "UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity\n  MoE",
            "submittedOnDailyBy": {
                "_id": "64380ae1819f3ab20d17431b",
                "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
                "isPro": false,
                "fullname": "ZhenyuLiu",
                "user": "foggyforest",
                "type": "user"
            },
            "summary": "Recent advances in unified multimodal models indicate a clear trend towards\ncomprehensive content generation. However, the auditory domain remains a\nsignificant challenge, with music and speech often developed in isolation,\nhindering progress towards universal audio synthesis. This separation stems\nfrom inherent task conflicts and severe data imbalances, which impede the\ndevelopment of a truly unified audio generation model. To address this\nchallenge, we propose UniMoE-Audio, a unified speech and music generation model\nwithin a novel Dynamic-Capacity Mixture-of-Experts (MoE) framework.\nArchitecturally, UniMoE-Audio introduces a Top-P routing strategy for dynamic\nexpert number allocation, and a hybrid expert design comprising routed experts\nfor domain-specific knowledge, shared experts for domain-agnostic features, and\nnull experts for adaptive computation skipping. To tackle data imbalance, we\nintroduce a three-stage training curriculum: 1) Independent Specialist Training\nleverages original datasets to instill domain-specific knowledge into each\n\"proto-expert\" without interference; 2) MoE Integration and Warmup incorporates\nthese specialists into the UniMoE-Audio architecture, warming up the gate\nmodule and shared expert using a subset of balanced dataset; and 3) Synergistic\nJoint Training trains the entire model end-to-end on the fully balanced\ndataset, fostering enhanced cross-domain synergy. Extensive experiments show\nthat UniMoE-Audio not only achieves state-of-the-art performance on major\nspeech and music generation benchmarks, but also demonstrates superior\nsynergistic learning, mitigating the performance degradation typically seen in\nnaive joint training. Our findings highlight the substantial potential of\nspecialized MoE architecture and curated training strategies in advancing the\nfield of universal audio generation. Homepage:\nhttps://mukioxun.github.io/Uni-MoE-site/home.html",
            "upvotes": 57,
            "discussionId": "68f03fba36f8b025381e189c",
            "projectPage": "https://mukioxun.github.io/Uni-MoE-site/home.html",
            "githubRepo": "https://github.com/HITsz-TMG/Uni-MoE/tree/master/UniMoE-Audio",
            "ai_summary": "UniMoE-Audio, a unified speech and music generation model using a Dynamic-Capacity Mixture-of-Experts framework, addresses data imbalance and task conflicts, achieving state-of-the-art performance and enhanced cross-domain synergy.",
            "ai_keywords": [
                "Dynamic-Capacity Mixture-of-Experts",
                "MoE",
                "Top-P routing strategy",
                "hybrid expert design",
                "routed experts",
                "shared experts",
                "null experts",
                "Independent Specialist Training",
                "MoE Integration and Warmup",
                "Synergistic Joint Training",
                "universal audio generation"
            ],
            "githubStars": 780,
            "organization": {
                "_id": "629867d7f2bf8bd3e468706e",
                "name": "HIT-TMG",
                "fullname": "HITsz-Text and Multimodal Generative Intelligence Group(TMG)",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658735061824-62986540f2bf8bd3e468622a.png"
            }
        },
        "publishedAt": "2025-10-15T05:30:25.000Z",
        "title": "UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity\n  MoE",
        "summary": "Recent advances in unified multimodal models indicate a clear trend towards\ncomprehensive content generation. However, the auditory domain remains a\nsignificant challenge, with music and speech often developed in isolation,\nhindering progress towards universal audio synthesis. This separation stems\nfrom inherent task conflicts and severe data imbalances, which impede the\ndevelopment of a truly unified audio generation model. To address this\nchallenge, we propose UniMoE-Audio, a unified speech and music generation model\nwithin a novel Dynamic-Capacity Mixture-of-Experts (MoE) framework.\nArchitecturally, UniMoE-Audio introduces a Top-P routing strategy for dynamic\nexpert number allocation, and a hybrid expert design comprising routed experts\nfor domain-specific knowledge, shared experts for domain-agnostic features, and\nnull experts for adaptive computation skipping. To tackle data imbalance, we\nintroduce a three-stage training curriculum: 1) Independent Specialist Training\nleverages original datasets to instill domain-specific knowledge into each\n\"proto-expert\" without interference; 2) MoE Integration and Warmup incorporates\nthese specialists into the UniMoE-Audio architecture, warming up the gate\nmodule and shared expert using a subset of balanced dataset; and 3) Synergistic\nJoint Training trains the entire model end-to-end on the fully balanced\ndataset, fostering enhanced cross-domain synergy. Extensive experiments show\nthat UniMoE-Audio not only achieves state-of-the-art performance on major\nspeech and music generation benchmarks, but also demonstrates superior\nsynergistic learning, mitigating the performance degradation typically seen in\nnaive joint training. Our findings highlight the substantial potential of\nspecialized MoE architecture and curated training strategies in advancing the\nfield of universal audio generation. Homepage:\nhttps://mukioxun.github.io/Uni-MoE-site/home.html",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13344.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64380ae1819f3ab20d17431b",
            "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
            "fullname": "ZhenyuLiu",
            "name": "foggyforest",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "629867d7f2bf8bd3e468706e",
            "name": "HIT-TMG",
            "fullname": "HITsz-Text and Multimodal Generative Intelligence Group(TMG)",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658735061824-62986540f2bf8bd3e468622a.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13678",
            "authors": [
                {
                    "_id": "68f051ed36f8b025381e1902",
                    "name": "Xinyang Li",
                    "hidden": false
                },
                {
                    "_id": "68f051ed36f8b025381e1903",
                    "name": "Tengfei Wang",
                    "hidden": false
                },
                {
                    "_id": "68f051ed36f8b025381e1904",
                    "user": {
                        "_id": "64b4b415a15d33a1bcc6ba6a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4b415a15d33a1bcc6ba6a/vqxjy7NFiRJMK9gdtOaz_.jpeg",
                        "isPro": false,
                        "fullname": "Zixiao Gu",
                        "user": "nightkiller",
                        "type": "user"
                    },
                    "name": "Zixiao Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:39:05.791Z",
                    "hidden": false
                },
                {
                    "_id": "68f051ed36f8b025381e1905",
                    "name": "Shengchuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f051ed36f8b025381e1906",
                    "name": "Chunchao Guo",
                    "hidden": false
                },
                {
                    "_id": "68f051ed36f8b025381e1907",
                    "name": "Liujuan Cao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/629631565de6e0eb3292afed/WhnRuWrdLfTMHJ2hXMj_P.mp4"
            ],
            "publishedAt": "2025-10-15T15:35:48.000Z",
            "submittedOnDailyAt": "2025-10-16T00:36:24.608Z",
            "title": "FlashWorld: High-quality 3D Scene Generation within Seconds",
            "submittedOnDailyBy": {
                "_id": "629631565de6e0eb3292afed",
                "avatarUrl": "/avatars/0ae1080eebce0f747f650bfc292c46ca.svg",
                "isPro": false,
                "fullname": "Xinyang Li",
                "user": "imlixinyang",
                "type": "user"
            },
            "summary": "We propose FlashWorld, a generative model that produces 3D scenes from a\nsingle image or text prompt in seconds, 10~100times faster than previous\nworks while possessing superior rendering quality. Our approach shifts from the\nconventional multi-view-oriented (MV-oriented) paradigm, which generates\nmulti-view images for subsequent 3D reconstruction, to a 3D-oriented approach\nwhere the model directly produces 3D Gaussian representations during multi-view\ngeneration. While ensuring 3D consistency, 3D-oriented method typically suffers\npoor visual quality. FlashWorld includes a dual-mode pre-training phase\nfollowed by a cross-mode post-training phase, effectively integrating the\nstrengths of both paradigms. Specifically, leveraging the prior from a video\ndiffusion model, we first pre-train a dual-mode multi-view diffusion model,\nwhich jointly supports MV-oriented and 3D-oriented generation modes. To bridge\nthe quality gap in 3D-oriented generation, we further propose a cross-mode\npost-training distillation by matching distribution from consistent 3D-oriented\nmode to high-quality MV-oriented mode. This not only enhances visual quality\nwhile maintaining 3D consistency, but also reduces the required denoising steps\nfor inference. Also, we propose a strategy to leverage massive single-view\nimages and text prompts during this process to enhance the model's\ngeneralization to out-of-distribution inputs. Extensive experiments demonstrate\nthe superiority and efficiency of our method.",
            "upvotes": 55,
            "discussionId": "68f051ed36f8b025381e1908",
            "projectPage": "https://imlixinyang.github.io/FlashWorld-Project-Page/",
            "githubRepo": "https://github.com/imlixinyang/FlashWorld",
            "ai_summary": "FlashWorld generates 3D scenes from single images or text prompts quickly and with high quality by combining MV-oriented and 3D-oriented generation methods.",
            "ai_keywords": [
                "generative model",
                "3D scenes",
                "single image",
                "text prompt",
                "3D Gaussian representations",
                "MV-oriented paradigm",
                "3D-oriented approach",
                "dual-mode pre-training",
                "cross-mode post-training",
                "video diffusion model",
                "multi-view diffusion model",
                "cross-mode post-training distillation",
                "denoising steps",
                "single-view images",
                "out-of-distribution inputs"
            ],
            "githubStars": 149
        },
        "publishedAt": "2025-10-15T11:35:48.000Z",
        "title": "FlashWorld: High-quality 3D Scene Generation within Seconds",
        "summary": "We propose FlashWorld, a generative model that produces 3D scenes from a\nsingle image or text prompt in seconds, 10~100times faster than previous\nworks while possessing superior rendering quality. Our approach shifts from the\nconventional multi-view-oriented (MV-oriented) paradigm, which generates\nmulti-view images for subsequent 3D reconstruction, to a 3D-oriented approach\nwhere the model directly produces 3D Gaussian representations during multi-view\ngeneration. While ensuring 3D consistency, 3D-oriented method typically suffers\npoor visual quality. FlashWorld includes a dual-mode pre-training phase\nfollowed by a cross-mode post-training phase, effectively integrating the\nstrengths of both paradigms. Specifically, leveraging the prior from a video\ndiffusion model, we first pre-train a dual-mode multi-view diffusion model,\nwhich jointly supports MV-oriented and 3D-oriented generation modes. To bridge\nthe quality gap in 3D-oriented generation, we further propose a cross-mode\npost-training distillation by matching distribution from consistent 3D-oriented\nmode to high-quality MV-oriented mode. This not only enhances visual quality\nwhile maintaining 3D consistency, but also reduces the required denoising steps\nfor inference. Also, we propose a strategy to leverage massive single-view\nimages and text prompts during this process to enhance the model's\ngeneralization to out-of-distribution inputs. Extensive experiments demonstrate\nthe superiority and efficiency of our method.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/629631565de6e0eb3292afed/WhnRuWrdLfTMHJ2hXMj_P.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13678.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "629631565de6e0eb3292afed",
            "avatarUrl": "/avatars/0ae1080eebce0f747f650bfc292c46ca.svg",
            "fullname": "Xinyang Li",
            "name": "imlixinyang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13554",
            "authors": [
                {
                    "_id": "68f04b7636f8b025381e18bf",
                    "user": {
                        "_id": "6570587b44d0620d221c722b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/T-B6q6sqhWAyXM7Hk_qWp.jpeg",
                        "isPro": false,
                        "fullname": "Yang Li (SJTU & SII)",
                        "user": "yangcole",
                        "type": "user"
                    },
                    "name": "Yang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T10:38:35.110Z",
                    "hidden": false
                },
                {
                    "_id": "68f04b7636f8b025381e18c0",
                    "name": "Zhichen Dong",
                    "hidden": false
                },
                {
                    "_id": "68f04b7636f8b025381e18c1",
                    "name": "Yuhan Sun",
                    "hidden": false
                },
                {
                    "_id": "68f04b7636f8b025381e18c2",
                    "name": "Weixun Wang",
                    "hidden": false
                },
                {
                    "_id": "68f04b7636f8b025381e18c3",
                    "name": "Shaopan Xiong",
                    "hidden": false
                },
                {
                    "_id": "68f04b7636f8b025381e18c4",
                    "user": {
                        "_id": "6731715cacb365a5ccd14258",
                        "avatarUrl": "/avatars/919bd7f51b0b7ac52e55846f8d78c293.svg",
                        "isPro": false,
                        "fullname": "YIJIALUO",
                        "user": "YIJIALUO",
                        "type": "user"
                    },
                    "name": "Yijia Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T10:38:37.247Z",
                    "hidden": false
                },
                {
                    "_id": "68f04b7636f8b025381e18c5",
                    "name": "Jiashun Liu",
                    "hidden": false
                },
                {
                    "_id": "68f04b7636f8b025381e18c6",
                    "user": {
                        "_id": "66825d697b0920f40d16c74d",
                        "avatarUrl": "/avatars/0048c8865a8de94218cde580fe0f8bc9.svg",
                        "isPro": false,
                        "fullname": "Han Lu",
                        "user": "SJTULH",
                        "type": "user"
                    },
                    "name": "Han Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:39:32.455Z",
                    "hidden": false
                },
                {
                    "_id": "68f04b7636f8b025381e18c7",
                    "name": "Jiamang Wang",
                    "hidden": false
                },
                {
                    "_id": "68f04b7636f8b025381e18c8",
                    "name": "Wenbo Su",
                    "hidden": false
                },
                {
                    "_id": "68f04b7636f8b025381e18c9",
                    "name": "Bo Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f04b7636f8b025381e18ca",
                    "name": "Junchi Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T13:49:51.000Z",
            "submittedOnDailyAt": "2025-10-16T01:50:37.335Z",
            "title": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm\n  Enables Fine-Grained Policy Optimization",
            "submittedOnDailyBy": {
                "_id": "6570587b44d0620d221c722b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/T-B6q6sqhWAyXM7Hk_qWp.jpeg",
                "isPro": false,
                "fullname": "Yang Li (SJTU & SII)",
                "user": "yangcole",
                "type": "user"
            },
            "summary": "The reasoning pattern of Large language models (LLMs) remains opaque, and\nReinforcement learning (RL) typically applies uniform credit across an entire\ngeneration, blurring the distinction between pivotal and routine steps. This\nwork positions attention as a privileged substrate that renders the internal\nlogic of LLMs legible, not merely as a byproduct of computation, but as a\nmechanistic blueprint of reasoning itself. We first distinguish attention heads\nbetween locally and globally focused information processing and reveal that\nlocally focused heads produce a sawtooth pattern near the diagonal indicating\nphrasal chunks, while globally focused heads expose tokens that exert broad\ndownstream influence over future tokens. We formalize these with two metrics:\n1) Windowed Average Attention Distance, which measures the extent of backward\nattention within a clipped window; 2) Future Attention Influence, which\nquantifies a token's global importance as the average attention it receives\nfrom subsequent tokens. Taken together, these signals reveal a recurring\npreplan-and-anchor mechanism, where the model first performs a long-range\ncontextual reference to generate an introductory token, which is immediately\nfollowed by or coincides with a semantic anchor token that organizes subsequent\nreasoning. Leveraging these insights, we introduce three novel RL strategies\nthat dynamically perform targeted credit assignment to critical nodes (preplan\ntokens, anchor tokens, and their temporal coupling) and show consistent\nperformance gains across various reasoning tasks. By aligning optimization with\nthe model's intrinsic reasoning rhythm, we aim to transform opaque optimization\ninto an actionable structure-aware process, hoping to offer a potential step\ntoward more transparent and effective optimization of LLM reasoning.",
            "upvotes": 45,
            "discussionId": "68f04b7636f8b025381e18cb",
            "ai_summary": "Attention mechanisms in LLMs are analyzed to reveal reasoning patterns, leading to novel RL strategies that improve performance by focusing on critical tokens.",
            "ai_keywords": [
                "Large language models",
                "Reinforcement learning",
                "attention heads",
                "locally focused",
                "globally focused",
                "Windowed Average Attention Distance",
                "Future Attention Influence",
                "preplan tokens",
                "anchor tokens",
                "targeted credit assignment"
            ],
            "organization": {
                "_id": "64488b334988ee01f2a8d856",
                "name": "alibaba-inc",
                "fullname": "alibaba-inc",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
            }
        },
        "publishedAt": "2025-10-15T09:49:51.000Z",
        "title": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm\n  Enables Fine-Grained Policy Optimization",
        "summary": "The reasoning pattern of Large language models (LLMs) remains opaque, and\nReinforcement learning (RL) typically applies uniform credit across an entire\ngeneration, blurring the distinction between pivotal and routine steps. This\nwork positions attention as a privileged substrate that renders the internal\nlogic of LLMs legible, not merely as a byproduct of computation, but as a\nmechanistic blueprint of reasoning itself. We first distinguish attention heads\nbetween locally and globally focused information processing and reveal that\nlocally focused heads produce a sawtooth pattern near the diagonal indicating\nphrasal chunks, while globally focused heads expose tokens that exert broad\ndownstream influence over future tokens. We formalize these with two metrics:\n1) Windowed Average Attention Distance, which measures the extent of backward\nattention within a clipped window; 2) Future Attention Influence, which\nquantifies a token's global importance as the average attention it receives\nfrom subsequent tokens. Taken together, these signals reveal a recurring\npreplan-and-anchor mechanism, where the model first performs a long-range\ncontextual reference to generate an introductory token, which is immediately\nfollowed by or coincides with a semantic anchor token that organizes subsequent\nreasoning. Leveraging these insights, we introduce three novel RL strategies\nthat dynamically perform targeted credit assignment to critical nodes (preplan\ntokens, anchor tokens, and their temporal coupling) and show consistent\nperformance gains across various reasoning tasks. By aligning optimization with\nthe model's intrinsic reasoning rhythm, we aim to transform opaque optimization\ninto an actionable structure-aware process, hoping to offer a potential step\ntoward more transparent and effective optimization of LLM reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13554.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6570587b44d0620d221c722b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/T-B6q6sqhWAyXM7Hk_qWp.jpeg",
            "fullname": "Yang Li (SJTU & SII)",
            "name": "yangcole",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "64488b334988ee01f2a8d856",
            "name": "alibaba-inc",
            "fullname": "alibaba-inc",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.13626",
            "authors": [
                {
                    "_id": "68f05beb36f8b025381e1995",
                    "name": "Senyu Fei",
                    "hidden": false
                },
                {
                    "_id": "68f05beb36f8b025381e1996",
                    "user": {
                        "_id": "64c3c631e77ea9f28111172a",
                        "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
                        "isPro": false,
                        "fullname": "Siyin Wang",
                        "user": "sinwang",
                        "type": "user"
                    },
                    "name": "Siyin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:38:33.758Z",
                    "hidden": false
                },
                {
                    "_id": "68f05beb36f8b025381e1997",
                    "name": "Junhao Shi",
                    "hidden": false
                },
                {
                    "_id": "68f05beb36f8b025381e1998",
                    "name": "Zihao Dai",
                    "hidden": false
                },
                {
                    "_id": "68f05beb36f8b025381e1999",
                    "name": "Jikun Cai",
                    "hidden": false
                },
                {
                    "_id": "68f05beb36f8b025381e199a",
                    "name": "Pengfang Qian",
                    "hidden": false
                },
                {
                    "_id": "68f05beb36f8b025381e199b",
                    "name": "Li Ji",
                    "hidden": false
                },
                {
                    "_id": "68f05beb36f8b025381e199c",
                    "name": "Xinzhe He",
                    "hidden": false
                },
                {
                    "_id": "68f05beb36f8b025381e199d",
                    "name": "Shiduo Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f05beb36f8b025381e199e",
                    "name": "Zhaoye Fei",
                    "hidden": false
                },
                {
                    "_id": "68f05beb36f8b025381e199f",
                    "name": "Jinlan Fu",
                    "hidden": false
                },
                {
                    "_id": "68f05beb36f8b025381e19a0",
                    "name": "Jingjing Gong",
                    "hidden": false
                },
                {
                    "_id": "68f05beb36f8b025381e19a1",
                    "name": "Xipeng Qiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T14:51:36.000Z",
            "submittedOnDailyAt": "2025-10-16T01:20:05.695Z",
            "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action\n  Models",
            "submittedOnDailyBy": {
                "_id": "64c3c631e77ea9f28111172a",
                "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
                "isPro": false,
                "fullname": "Siyin Wang",
                "user": "sinwang",
                "type": "user"
            },
            "summary": "Visual-Language-Action (VLA) models report impressive success rates on\nrobotic manipulation benchmarks, yet these results may mask fundamental\nweaknesses in robustness. We perform a systematic vulnerability analysis by\nintroducing controlled perturbations across seven dimensions: objects layout,\ncamera viewpoints, robot initial states, language instructions, light\nconditions, background textures and sensor noise. We comprehensively analyzed\nmultiple state-of-the-art models and revealed consistent brittleness beneath\napparent competence. Our analysis exposes critical weaknesses: models exhibit\nextreme sensitivity to perturbation factors, including camera viewpoints and\nrobot initial states, with performance dropping from 95% to below 30% under\nmodest perturbations. Surprisingly, models are largely insensitive to language\nvariations, with further experiments revealing that models tend to ignore\nlanguage instructions completely. Our findings challenge the assumption that\nhigh benchmark scores equate to true competency and highlight the need for\nevaluation practices that assess reliability under realistic variation.",
            "upvotes": 38,
            "discussionId": "68f05beb36f8b025381e19a2",
            "projectPage": "https://sylvestf.github.io/LIBERO-plus/",
            "githubRepo": "https://github.com/sylvestf/LIBERO-plus",
            "ai_summary": "State-of-the-art Visual-Language-Action models show high benchmark scores but are brittle to various perturbations, particularly in camera viewpoints and robot initial states, and often ignore language instructions.",
            "ai_keywords": [
                "Visual-Language-Action models",
                "vulnerability analysis",
                "perturbations",
                "objects layout",
                "camera viewpoints",
                "robot initial states",
                "language instructions",
                "light conditions",
                "background textures",
                "sensor noise",
                "brittleness",
                "reliability"
            ],
            "githubStars": 27,
            "organization": {
                "_id": "613b0dee83ec35d460684607",
                "name": "fnlp",
                "fullname": "OpenMOSS (SII, Fudan NLP)",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/xM_PjniEZ9fmDKtJN7PAG.png"
            }
        },
        "publishedAt": "2025-10-15T10:51:36.000Z",
        "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action\n  Models",
        "summary": "Visual-Language-Action (VLA) models report impressive success rates on\nrobotic manipulation benchmarks, yet these results may mask fundamental\nweaknesses in robustness. We perform a systematic vulnerability analysis by\nintroducing controlled perturbations across seven dimensions: objects layout,\ncamera viewpoints, robot initial states, language instructions, light\nconditions, background textures and sensor noise. We comprehensively analyzed\nmultiple state-of-the-art models and revealed consistent brittleness beneath\napparent competence. Our analysis exposes critical weaknesses: models exhibit\nextreme sensitivity to perturbation factors, including camera viewpoints and\nrobot initial states, with performance dropping from 95% to below 30% under\nmodest perturbations. Surprisingly, models are largely insensitive to language\nvariations, with further experiments revealing that models tend to ignore\nlanguage instructions completely. Our findings challenge the assumption that\nhigh benchmark scores equate to true competency and highlight the need for\nevaluation practices that assess reliability under realistic variation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13626.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "64c3c631e77ea9f28111172a",
            "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
            "fullname": "Siyin Wang",
            "name": "sinwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "613b0dee83ec35d460684607",
            "name": "fnlp",
            "fullname": "OpenMOSS (SII, Fudan NLP)",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/xM_PjniEZ9fmDKtJN7PAG.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.13795",
            "authors": [
                {
                    "_id": "68f05e7636f8b025381e19af",
                    "user": {
                        "_id": "63b2efb5922f26a27e76381c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b2efb5922f26a27e76381c/zOQAt_xywiY8eTvvQOrmQ.png",
                        "isPro": false,
                        "fullname": "Yi Zhang",
                        "user": "uyzhang",
                        "type": "user"
                    },
                    "name": "Yi Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:35:15.644Z",
                    "hidden": false
                },
                {
                    "_id": "68f05e7636f8b025381e19b0",
                    "name": "Bolin Ni",
                    "hidden": false
                },
                {
                    "_id": "68f05e7636f8b025381e19b1",
                    "name": "Xin-Sheng Chen",
                    "hidden": false
                },
                {
                    "_id": "68f05e7636f8b025381e19b2",
                    "name": "Heng-Rui Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f05e7636f8b025381e19b3",
                    "name": "Yongming Rao",
                    "hidden": false
                },
                {
                    "_id": "68f05e7636f8b025381e19b4",
                    "name": "Houwen Peng",
                    "hidden": false
                },
                {
                    "_id": "68f05e7636f8b025381e19b5",
                    "name": "Qinglin Lu",
                    "hidden": false
                },
                {
                    "_id": "68f05e7636f8b025381e19b6",
                    "name": "Han Hu",
                    "hidden": false
                },
                {
                    "_id": "68f05e7636f8b025381e19b7",
                    "user": {
                        "_id": "62145614b670cb63a38075ba",
                        "avatarUrl": "/avatars/5e33debde75ae6c87640f63c48c560c6.svg",
                        "isPro": false,
                        "fullname": "MenghaoGuo",
                        "user": "MenghaoGuo",
                        "type": "user"
                    },
                    "name": "Meng-Hao Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:38:31.512Z",
                    "hidden": false
                },
                {
                    "_id": "68f05e7636f8b025381e19b8",
                    "name": "Shi-Min Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T17:52:59.000Z",
            "submittedOnDailyAt": "2025-10-16T01:26:28.869Z",
            "title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully\n  Open MLLMs",
            "submittedOnDailyBy": {
                "_id": "6571b51fd5c6a6d3b0ba68ad",
                "avatarUrl": "/avatars/0ccd8fe8de857753b534356a90eb10f0.svg",
                "isPro": false,
                "fullname": "gmh",
                "user": "menghao22",
                "type": "user"
            },
            "summary": "Fully open multimodal large language models (MLLMs) currently lag behind\nproprietary counterparts, primarily due to a significant gap in data quality\nfor supervised fine-tuning (SFT). Existing open-source datasets are often\nplagued by widespread noise and a critical deficit in complex reasoning data,\nsuch as Chain-of-Thought (CoT), which hinders the development of advanced model\ncapabilities. Addressing these challenges, our work makes three primary\ncontributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising\napproximately 15 million QA pairs, processed through multiple cleaning\ntechniques and enhanced with a novel dual-level (short and long) CoT enrichment\nstrategy. Second, we introduce HoneyPipe, the data curation pipeline, and its\nunderlying framework DataStudio, providing the community with a transparent and\nadaptable methodology for data curation that moves beyond static dataset\nreleases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B\nmodel on Honey-Data-15M. Experiments show that Bee-8B establishes a new\nstate-of-the-art (SOTA) for fully open MLLMs, achieving performance that is\ncompetitive with, and in some cases surpasses, recent semi-open models such as\nInternVL3.5-8B. Our work delivers to the community a suite of foundational\nresources, including: the Honey-Data-15M corpus; the full-stack suite\ncomprising HoneyPipe and DataStudio; training recipes; an evaluation harness;\nand the model weights. This effort demonstrates that a principled focus on data\nquality is a key pathway to developing fully open MLLMs that are highly\ncompetitive with their semi-open counterparts.",
            "upvotes": 36,
            "discussionId": "68f05e7736f8b025381e19b9",
            "projectPage": "https://open-bee.github.io/",
            "ai_summary": "A new dataset and pipeline for data curation improve the performance of fully open multimodal large language models, achieving state-of-the-art results competitive with semi-open models.",
            "ai_keywords": [
                "multimodal large language models",
                "supervised fine-tuning",
                "Chain-of-Thought",
                "Honey-Data-15M",
                "HoneyPipe",
                "DataStudio",
                "Bee-8B",
                "state-of-the-art"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "68e8c7928bba74b5f4ab0299",
                "name": "Open-Bee",
                "fullname": "Open-Bee",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b2efb5922f26a27e76381c/PU469_PkFTS-Gs40EDprp.png"
            }
        },
        "publishedAt": "2025-10-15T13:52:59.000Z",
        "title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully\n  Open MLLMs",
        "summary": "Fully open multimodal large language models (MLLMs) currently lag behind\nproprietary counterparts, primarily due to a significant gap in data quality\nfor supervised fine-tuning (SFT). Existing open-source datasets are often\nplagued by widespread noise and a critical deficit in complex reasoning data,\nsuch as Chain-of-Thought (CoT), which hinders the development of advanced model\ncapabilities. Addressing these challenges, our work makes three primary\ncontributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising\napproximately 15 million QA pairs, processed through multiple cleaning\ntechniques and enhanced with a novel dual-level (short and long) CoT enrichment\nstrategy. Second, we introduce HoneyPipe, the data curation pipeline, and its\nunderlying framework DataStudio, providing the community with a transparent and\nadaptable methodology for data curation that moves beyond static dataset\nreleases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B\nmodel on Honey-Data-15M. Experiments show that Bee-8B establishes a new\nstate-of-the-art (SOTA) for fully open MLLMs, achieving performance that is\ncompetitive with, and in some cases surpasses, recent semi-open models such as\nInternVL3.5-8B. Our work delivers to the community a suite of foundational\nresources, including: the Honey-Data-15M corpus; the full-stack suite\ncomprising HoneyPipe and DataStudio; training recipes; an evaluation harness;\nand the model weights. This effort demonstrates that a principled focus on data\nquality is a key pathway to developing fully open MLLMs that are highly\ncompetitive with their semi-open counterparts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13795.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6571b51fd5c6a6d3b0ba68ad",
            "avatarUrl": "/avatars/0ccd8fe8de857753b534356a90eb10f0.svg",
            "fullname": "gmh",
            "name": "menghao22",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "68e8c7928bba74b5f4ab0299",
            "name": "Open-Bee",
            "fullname": "Open-Bee",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b2efb5922f26a27e76381c/PU469_PkFTS-Gs40EDprp.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13809",
            "authors": [
                {
                    "_id": "68f04e9936f8b025381e18d2",
                    "user": {
                        "_id": "64775b6ce5ddd7d2a16fadee",
                        "avatarUrl": "/avatars/19af37ffd7626ebc58b387a34a8f98d7.svg",
                        "isPro": false,
                        "fullname": "Sihui Ji",
                        "user": "zjuJish",
                        "type": "user"
                    },
                    "name": "Sihui Ji",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:39:18.148Z",
                    "hidden": false
                },
                {
                    "_id": "68f04e9936f8b025381e18d3",
                    "name": "Xi Chen",
                    "hidden": false
                },
                {
                    "_id": "68f04e9936f8b025381e18d4",
                    "name": "Xin Tao",
                    "hidden": false
                },
                {
                    "_id": "68f04e9936f8b025381e18d5",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "68f04e9936f8b025381e18d6",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T17:59:59.000Z",
            "submittedOnDailyAt": "2025-10-16T00:17:16.195Z",
            "title": "PhysMaster: Mastering Physical Representation for Video Generation via\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Video generation models nowadays are capable of generating visually realistic\nvideos, but often fail to adhere to physical laws, limiting their ability to\ngenerate physically plausible videos and serve as ''world models''. To address\nthis issue, we propose PhysMaster, which captures physical knowledge as a\nrepresentation for guiding video generation models to enhance their\nphysics-awareness. Specifically, PhysMaster is based on the image-to-video task\nwhere the model is expected to predict physically plausible dynamics from the\ninput image. Since the input image provides physical priors like relative\npositions and potential interactions of objects in the scenario, we devise\nPhysEncoder to encode physical information from it as an extra condition to\ninject physical knowledge into the video generation process. The lack of proper\nsupervision on the model's physical performance beyond mere appearance\nmotivates PhysEncoder to apply reinforcement learning with human feedback to\nphysical representation learning, which leverages feedback from generation\nmodels to optimize physical representations with Direct Preference Optimization\n(DPO) in an end-to-end manner. PhysMaster provides a feasible solution for\nimproving physics-awareness of PhysEncoder and thus of video generation,\nproving its ability on a simple proxy task and generalizability to wide-ranging\nphysical scenarios. This implies that our PhysMaster, which unifies solutions\nfor various physical processes via representation learning in the reinforcement\nlearning paradigm, can act as a generic and plug-in solution for physics-aware\nvideo generation and broader applications.",
            "upvotes": 31,
            "discussionId": "68f04e9936f8b025381e18d7",
            "projectPage": "https://sihuiji.github.io/PhysMaster-Page/",
            "githubRepo": "https://github.com/KwaiVGI/PhysMaster",
            "ai_summary": "PhysMaster enhances video generation by integrating physical knowledge through PhysEncoder, using reinforcement learning and Direct Preference Optimization to improve physics-awareness.",
            "ai_keywords": [
                "PhysMaster",
                "PhysEncoder",
                "reinforcement learning",
                "Direct Preference Optimization",
                "physics-aware video generation"
            ],
            "githubStars": 19
        },
        "publishedAt": "2025-10-15T13:59:59.000Z",
        "title": "PhysMaster: Mastering Physical Representation for Video Generation via\n  Reinforcement Learning",
        "summary": "Video generation models nowadays are capable of generating visually realistic\nvideos, but often fail to adhere to physical laws, limiting their ability to\ngenerate physically plausible videos and serve as ''world models''. To address\nthis issue, we propose PhysMaster, which captures physical knowledge as a\nrepresentation for guiding video generation models to enhance their\nphysics-awareness. Specifically, PhysMaster is based on the image-to-video task\nwhere the model is expected to predict physically plausible dynamics from the\ninput image. Since the input image provides physical priors like relative\npositions and potential interactions of objects in the scenario, we devise\nPhysEncoder to encode physical information from it as an extra condition to\ninject physical knowledge into the video generation process. The lack of proper\nsupervision on the model's physical performance beyond mere appearance\nmotivates PhysEncoder to apply reinforcement learning with human feedback to\nphysical representation learning, which leverages feedback from generation\nmodels to optimize physical representations with Direct Preference Optimization\n(DPO) in an end-to-end manner. PhysMaster provides a feasible solution for\nimproving physics-awareness of PhysEncoder and thus of video generation,\nproving its ability on a simple proxy task and generalizability to wide-ranging\nphysical scenarios. This implies that our PhysMaster, which unifies solutions\nfor various physical processes via representation learning in the reinforcement\nlearning paradigm, can act as a generic and plug-in solution for physics-aware\nvideo generation and broader applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13809.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 127
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13747",
            "authors": [
                {
                    "_id": "68f0571d36f8b025381e1957",
                    "name": "Wenwen Tong",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e1958",
                    "user": {
                        "_id": "662f58bc5cf9dca4a2b225ca",
                        "avatarUrl": "/avatars/4b4f989621ddf2882b3dcdc43de4daf0.svg",
                        "isPro": false,
                        "fullname": "Hewei Guo",
                        "user": "heweiguo",
                        "type": "user"
                    },
                    "name": "Hewei Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:38:36.074Z",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e1959",
                    "name": "Dongchuan Ran",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e195a",
                    "user": {
                        "_id": "65531be65d55ccb20e97f34a",
                        "avatarUrl": "/avatars/ba919227d8c0b3532b62bb3a2d2d6ff3.svg",
                        "isPro": false,
                        "fullname": "Chan",
                        "user": "Chris2me",
                        "type": "user"
                    },
                    "name": "Jiangnan Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T07:25:38.654Z",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e195b",
                    "name": "Jiefan Lu",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e195c",
                    "name": "Kaibin Wang",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e195d",
                    "name": "Keqiang Li",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e195e",
                    "name": "Xiaoxu Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e195f",
                    "name": "Jiakui Li",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e1960",
                    "name": "Kehan Li",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e1961",
                    "name": "Xueheng Li",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e1962",
                    "name": "Lumin Li",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e1963",
                    "name": "Chenxu Guo",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e1964",
                    "name": "Jiasheng Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e1965",
                    "name": "Jiandong Chen",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e1966",
                    "name": "Xianye Wu",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e1967",
                    "user": {
                        "_id": "66bc7862aa7cdcb1c31a1efb",
                        "avatarUrl": "/avatars/4c2ab907247fe071ff5cdd71c404ca7c.svg",
                        "isPro": false,
                        "fullname": "wang jiahao",
                        "user": "datamonkey",
                        "type": "user"
                    },
                    "name": "Jiahao Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T11:40:32.492Z",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e1968",
                    "name": "Silei Wu",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e1969",
                    "name": "Lei Chen",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e196a",
                    "name": "Hanming Deng",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e196b",
                    "name": "Yuxuan Song",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e196c",
                    "name": "Dinghao Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e196d",
                    "name": "Guiping Zhong",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e196e",
                    "name": "Ken Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e196f",
                    "name": "Shiyin Kang",
                    "hidden": false
                },
                {
                    "_id": "68f0571d36f8b025381e1970",
                    "name": "Lewei Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T16:52:48.000Z",
            "submittedOnDailyAt": "2025-10-16T01:06:55.300Z",
            "title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn\n  Dialogue",
            "submittedOnDailyBy": {
                "_id": "647d4f1236e109abce409c3b",
                "avatarUrl": "/avatars/d166f5f8be666e96b522a0a0effd21c4.svg",
                "isPro": false,
                "fullname": "Wenwen Tong",
                "user": "tongww",
                "type": "user"
            },
            "summary": "We introduce InteractiveOmni, a unified and open-source omni-modal large\nlanguage model for audio-visual multi-turn interaction, ranging from 4B to 8B\nparameters, designed to lead the field of lightweight models by offering\ncomprehensive omni-modal understanding and speech generation capabilities. To\nachieve this, we integrate the vision encoder, audio encoder, large language\nmodel, and speech decoder into a unified model for understanding and generation\ntasks. We design a multi-stage training strategy to ensure robust cross-modal\ncapabilities, including pre-training for omni-modal understanding, followed by\npost-training with speech conversation and audio-visual interaction. To enable\nhuman-like long-term conversational ability, we meticulously curate a\nmulti-turn training dataset that enhances the model's ability to handle complex\nand multi-turn interactions. To effectively evaluate the multi-turn memory and\nspeech interaction capabilities, we construct the multi-modal multi-turn memory\nbenchmark and the multi-turn speech interaction benchmark. Experiments\ndemonstrate that InteractiveOmni significantly outperforms leading open-source\nmodels and provides a more intelligent multi-turn audio-visual experience,\nparticularly in its long-term memory capabilities. Notably, InteractiveOmni-4B\nis comparable to the much larger model like Qwen2.5-Omni-7B on general\nbenchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B\nwhile utilizing only 50% of the model size. Achieving state-of-the-art results\nagainst similarly sized models across image, audio, video understanding, and\nspeech generation tasks, InteractiveOmni is an accessible, open-source\nfoundation for next-generation intelligent interactive systems.",
            "upvotes": 28,
            "discussionId": "68f0571d36f8b025381e1971",
            "ai_summary": "InteractiveOmni is a unified omni-modal large language model for audio-visual multi-turn interactions, offering comprehensive understanding and speech generation capabilities with efficient parameter usage.",
            "ai_keywords": [
                "omni-modal large language model",
                "audio-visual multi-turn interaction",
                "vision encoder",
                "audio encoder",
                "large language model",
                "speech decoder",
                "multi-stage training strategy",
                "omni-modal understanding",
                "speech conversation",
                "audio-visual interaction",
                "multi-turn training dataset",
                "multi-modal multi-turn memory benchmark",
                "multi-turn speech interaction benchmark",
                "long-term memory capabilities",
                "Qwen2.5-Omni-7B",
                "InteractiveOmni-4B",
                "InteractiveOmni-8B",
                "image understanding",
                "audio understanding",
                "video understanding",
                "speech generation"
            ]
        },
        "publishedAt": "2025-10-15T12:52:48.000Z",
        "title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn\n  Dialogue",
        "summary": "We introduce InteractiveOmni, a unified and open-source omni-modal large\nlanguage model for audio-visual multi-turn interaction, ranging from 4B to 8B\nparameters, designed to lead the field of lightweight models by offering\ncomprehensive omni-modal understanding and speech generation capabilities. To\nachieve this, we integrate the vision encoder, audio encoder, large language\nmodel, and speech decoder into a unified model for understanding and generation\ntasks. We design a multi-stage training strategy to ensure robust cross-modal\ncapabilities, including pre-training for omni-modal understanding, followed by\npost-training with speech conversation and audio-visual interaction. To enable\nhuman-like long-term conversational ability, we meticulously curate a\nmulti-turn training dataset that enhances the model's ability to handle complex\nand multi-turn interactions. To effectively evaluate the multi-turn memory and\nspeech interaction capabilities, we construct the multi-modal multi-turn memory\nbenchmark and the multi-turn speech interaction benchmark. Experiments\ndemonstrate that InteractiveOmni significantly outperforms leading open-source\nmodels and provides a more intelligent multi-turn audio-visual experience,\nparticularly in its long-term memory capabilities. Notably, InteractiveOmni-4B\nis comparable to the much larger model like Qwen2.5-Omni-7B on general\nbenchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B\nwhile utilizing only 50% of the model size. Achieving state-of-the-art results\nagainst similarly sized models across image, audio, video understanding, and\nspeech generation tasks, InteractiveOmni is an accessible, open-source\nfoundation for next-generation intelligent interactive systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13747.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647d4f1236e109abce409c3b",
            "avatarUrl": "/avatars/d166f5f8be666e96b522a0a0effd21c4.svg",
            "fullname": "Wenwen Tong",
            "name": "tongww",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13802",
            "authors": [
                {
                    "_id": "68f04f2d36f8b025381e18e3",
                    "user": {
                        "_id": "64b6e2abd3711dd619e476fc",
                        "avatarUrl": "/avatars/430f53c44d12c578a5128e42ada62527.svg",
                        "isPro": false,
                        "fullname": "Xinhang Liu",
                        "user": "xinhang0111",
                        "type": "user"
                    },
                    "name": "Xinhang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:39:12.666Z",
                    "hidden": false
                },
                {
                    "_id": "68f04f2d36f8b025381e18e4",
                    "user": {
                        "_id": "6688a8f30bf195d6e53ac28d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6688a8f30bf195d6e53ac28d/5izbVmdCWWwA1wBjcxZPB.jpeg",
                        "isPro": true,
                        "fullname": "Yuxi Xiao",
                        "user": "Yuxihenry",
                        "type": "user"
                    },
                    "name": "Yuxi Xiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T07:25:42.979Z",
                    "hidden": false
                },
                {
                    "_id": "68f04f2d36f8b025381e18e5",
                    "user": {
                        "_id": "653862bdbe39573b3b247b44",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653862bdbe39573b3b247b44/oMqOC4USbQlPX5HFLDXt6.jpeg",
                        "isPro": false,
                        "fullname": "Donny Chen",
                        "user": "donydchen",
                        "type": "user"
                    },
                    "name": "Donny Y. Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:39:10.430Z",
                    "hidden": false
                },
                {
                    "_id": "68f04f2d36f8b025381e18e6",
                    "name": "Jiashi Feng",
                    "hidden": false
                },
                {
                    "_id": "68f04f2d36f8b025381e18e7",
                    "name": "Yu-Wing Tai",
                    "hidden": false
                },
                {
                    "_id": "68f04f2d36f8b025381e18e8",
                    "name": "Chi-Keung Tang",
                    "hidden": false
                },
                {
                    "_id": "68f04f2d36f8b025381e18e9",
                    "name": "Bingyi Kang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T17:59:04.000Z",
            "submittedOnDailyAt": "2025-10-16T00:19:47.936Z",
            "title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Effective spatio-temporal representation is fundamental to modeling,\nunderstanding, and predicting dynamics in videos. The atomic unit of a video,\nthe pixel, traces a continuous 3D trajectory over time, serving as the\nprimitive element of dynamics. Based on this principle, we propose representing\nany video as a Trajectory Field: a dense mapping that assigns a continuous 3D\ntrajectory function of time to each pixel in every frame. With this\nrepresentation, we introduce Trace Anything, a neural network that predicts the\nentire trajectory field in a single feed-forward pass. Specifically, for each\npixel in each frame, our model predicts a set of control points that\nparameterizes a trajectory (i.e., a B-spline), yielding its 3D position at\narbitrary query time instants. We trained the Trace Anything model on\nlarge-scale 4D data, including data from our new platform, and our experiments\ndemonstrate that: (i) Trace Anything achieves state-of-the-art performance on\nour new benchmark for trajectory field estimation and performs competitively on\nestablished point-tracking benchmarks; (ii) it offers significant efficiency\ngains thanks to its one-pass paradigm, without requiring iterative optimization\nor auxiliary estimators; and (iii) it exhibits emergent abilities, including\ngoal-conditioned manipulation, motion forecasting, and spatio-temporal fusion.\nProject page: https://trace-anything.github.io/.",
            "upvotes": 24,
            "discussionId": "68f04f2d36f8b025381e18ea",
            "projectPage": "https://trace-anything.github.io/",
            "githubRepo": "https://github.com/ByteDance-Seed/TraceAnything",
            "ai_summary": "Trace Anything, a neural network, predicts video trajectories in a single pass, achieving state-of-the-art performance and demonstrating efficiency and emergent abilities like motion forecasting.",
            "ai_keywords": [
                "Trajectory Field",
                "Trace Anything",
                "B-spline",
                "trajectory field estimation",
                "point-tracking benchmarks",
                "goal-conditioned manipulation",
                "motion forecasting",
                "spatio-temporal fusion"
            ],
            "githubStars": 119,
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-10-15T13:59:04.000Z",
        "title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields",
        "summary": "Effective spatio-temporal representation is fundamental to modeling,\nunderstanding, and predicting dynamics in videos. The atomic unit of a video,\nthe pixel, traces a continuous 3D trajectory over time, serving as the\nprimitive element of dynamics. Based on this principle, we propose representing\nany video as a Trajectory Field: a dense mapping that assigns a continuous 3D\ntrajectory function of time to each pixel in every frame. With this\nrepresentation, we introduce Trace Anything, a neural network that predicts the\nentire trajectory field in a single feed-forward pass. Specifically, for each\npixel in each frame, our model predicts a set of control points that\nparameterizes a trajectory (i.e., a B-spline), yielding its 3D position at\narbitrary query time instants. We trained the Trace Anything model on\nlarge-scale 4D data, including data from our new platform, and our experiments\ndemonstrate that: (i) Trace Anything achieves state-of-the-art performance on\nour new benchmark for trajectory field estimation and performs competitively on\nestablished point-tracking benchmarks; (ii) it offers significant efficiency\ngains thanks to its one-pass paradigm, without requiring iterative optimization\nor auxiliary estimators; and (iii) it exhibits emergent abilities, including\ngoal-conditioned manipulation, motion forecasting, and spatio-temporal fusion.\nProject page: https://trace-anything.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13802.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 127
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.04767",
            "authors": [
                {
                    "_id": "68f056d836f8b025381e194a",
                    "name": "Wonjun Kang",
                    "hidden": false
                },
                {
                    "_id": "68f056d836f8b025381e194b",
                    "name": "Kevin Galim",
                    "hidden": false
                },
                {
                    "_id": "68f056d836f8b025381e194c",
                    "name": "Seunghyuk Oh",
                    "hidden": false
                },
                {
                    "_id": "68f056d836f8b025381e194d",
                    "user": {
                        "_id": "61ad9c3300d01045fca0ad64",
                        "avatarUrl": "/avatars/04c53c2d68d80db1053e5ebadbda5592.svg",
                        "isPro": false,
                        "fullname": "Min Jae Lee",
                        "user": "mjbooo",
                        "type": "user"
                    },
                    "name": "Minjae Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:38:41.418Z",
                    "hidden": false
                },
                {
                    "_id": "68f056d836f8b025381e194e",
                    "name": "Yuchen Zeng",
                    "hidden": false
                },
                {
                    "_id": "68f056d836f8b025381e194f",
                    "name": "Shuibai Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f056d836f8b025381e1950",
                    "name": "Coleman Hooper",
                    "hidden": false
                },
                {
                    "_id": "68f056d836f8b025381e1951",
                    "name": "Yuezhou Hu",
                    "hidden": false
                },
                {
                    "_id": "68f056d836f8b025381e1952",
                    "name": "Hyung Il Koo",
                    "hidden": false
                },
                {
                    "_id": "68f056d836f8b025381e1953",
                    "name": "Nam Ik Cho",
                    "hidden": false
                },
                {
                    "_id": "68f056d836f8b025381e1954",
                    "name": "Kangwook Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-06T12:41:31.000Z",
            "submittedOnDailyAt": "2025-10-16T01:00:13.316Z",
            "title": "ParallelBench: Understanding the Trade-offs of Parallel Decoding in\n  Diffusion LLMs",
            "submittedOnDailyBy": {
                "_id": "64ae35dc00781825350e880b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae35dc00781825350e880b/VuON41yUDzNDACbvWAVhz.jpeg",
                "isPro": false,
                "fullname": "Seunghyuk Oh",
                "user": "JakeOh",
                "type": "user"
            },
            "summary": "While most autoregressive LLMs are constrained to one-by-one decoding,\ndiffusion LLMs (dLLMs) have attracted growing interest for their potential to\ndramatically accelerate inference through parallel decoding. Despite this\npromise, the conditional independence assumption in dLLMs causes parallel\ndecoding to ignore token dependencies, inevitably degrading generation quality\nwhen these dependencies are strong. However, existing works largely overlook\nthese inherent challenges, and evaluations on standard benchmarks (e.g., math\nand coding) are not sufficient to capture the quality degradation caused by\nparallel decoding. To address this gap, we first provide an\ninformation-theoretic analysis of parallel decoding. We then conduct case\nstudies on analytically tractable synthetic list operations from both data\ndistribution and decoding strategy perspectives, offering quantitative insights\nthat highlight the fundamental limitations of parallel decoding. Building on\nthese insights, we propose ParallelBench, the first benchmark specifically\ndesigned for dLLMs, featuring realistic tasks that are trivial for humans and\nautoregressive LLMs yet exceptionally challenging for dLLMs under parallel\ndecoding. Using ParallelBench, we systematically analyze both dLLMs and\nautoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can\nsuffer dramatic quality degradation in real-world scenarios, and (ii) current\nparallel decoding strategies struggle to adapt their degree of parallelism\nbased on task difficulty, thus failing to achieve meaningful speedup without\ncompromising quality. Our findings underscore the pressing need for innovative\ndecoding methods that can overcome the current speed-quality trade-off. We\nrelease our benchmark to help accelerate the development of truly efficient\ndLLMs.",
            "upvotes": 24,
            "discussionId": "68f056d936f8b025381e1955",
            "projectPage": "https://parallelbench.github.io",
            "githubRepo": "https://github.com/furiosa-ai/ParallelBench",
            "ai_summary": "Parallel decoding in diffusion LLMs degrades generation quality due to ignored token dependencies, highlighting the need for new decoding methods and benchmarks.",
            "ai_keywords": [
                "autoregressive LLMs",
                "diffusion LLMs",
                "dLLMs",
                "parallel decoding",
                "conditional independence assumption",
                "information-theoretic analysis",
                "synthetic list operations",
                "ParallelBench",
                "real-world scenarios",
                "speed-quality trade-off"
            ],
            "githubStars": 8,
            "organization": {
                "_id": "6213a1dcb670cb63a38074a1",
                "name": "furiosa-ai",
                "fullname": "FuriosaAI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620b6dc29412b0861cb2474a/Bl7ua2mXSFxk9rVo8vMA8.png"
            }
        },
        "publishedAt": "2025-10-06T08:41:31.000Z",
        "title": "ParallelBench: Understanding the Trade-offs of Parallel Decoding in\n  Diffusion LLMs",
        "summary": "While most autoregressive LLMs are constrained to one-by-one decoding,\ndiffusion LLMs (dLLMs) have attracted growing interest for their potential to\ndramatically accelerate inference through parallel decoding. Despite this\npromise, the conditional independence assumption in dLLMs causes parallel\ndecoding to ignore token dependencies, inevitably degrading generation quality\nwhen these dependencies are strong. However, existing works largely overlook\nthese inherent challenges, and evaluations on standard benchmarks (e.g., math\nand coding) are not sufficient to capture the quality degradation caused by\nparallel decoding. To address this gap, we first provide an\ninformation-theoretic analysis of parallel decoding. We then conduct case\nstudies on analytically tractable synthetic list operations from both data\ndistribution and decoding strategy perspectives, offering quantitative insights\nthat highlight the fundamental limitations of parallel decoding. Building on\nthese insights, we propose ParallelBench, the first benchmark specifically\ndesigned for dLLMs, featuring realistic tasks that are trivial for humans and\nautoregressive LLMs yet exceptionally challenging for dLLMs under parallel\ndecoding. Using ParallelBench, we systematically analyze both dLLMs and\nautoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can\nsuffer dramatic quality degradation in real-world scenarios, and (ii) current\nparallel decoding strategies struggle to adapt their degree of parallelism\nbased on task difficulty, thus failing to achieve meaningful speedup without\ncompromising quality. Our findings underscore the pressing need for innovative\ndecoding methods that can overcome the current speed-quality trade-off. We\nrelease our benchmark to help accelerate the development of truly efficient\ndLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04767.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ae35dc00781825350e880b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae35dc00781825350e880b/VuON41yUDzNDACbvWAVhz.jpeg",
            "fullname": "Seunghyuk Oh",
            "name": "JakeOh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "6213a1dcb670cb63a38074a1",
            "name": "furiosa-ai",
            "fullname": "FuriosaAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620b6dc29412b0861cb2474a/Bl7ua2mXSFxk9rVo8vMA8.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07944",
            "authors": [
                {
                    "_id": "68f05c6336f8b025381e19a4",
                    "name": "Tianrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f05c6336f8b025381e19a5",
                    "name": "Yichen Liu",
                    "hidden": false
                },
                {
                    "_id": "68f05c6336f8b025381e19a6",
                    "name": "Zilin Guo",
                    "hidden": false
                },
                {
                    "_id": "68f05c6336f8b025381e19a7",
                    "name": "Yuxin Guo",
                    "hidden": false
                },
                {
                    "_id": "68f05c6336f8b025381e19a8",
                    "name": "Jingcheng Ni",
                    "hidden": false
                },
                {
                    "_id": "68f05c6336f8b025381e19a9",
                    "name": "Chenjing Ding",
                    "hidden": false
                },
                {
                    "_id": "68f05c6336f8b025381e19aa",
                    "name": "Dan Xu",
                    "hidden": false
                },
                {
                    "_id": "68f05c6336f8b025381e19ab",
                    "name": "Lewei Lu",
                    "hidden": false
                },
                {
                    "_id": "68f05c6336f8b025381e19ac",
                    "name": "Zehuan Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T08:41:58.000Z",
            "submittedOnDailyAt": "2025-10-16T01:16:54.120Z",
            "title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal\n  Reconstruction Model for Autonomous Driving",
            "submittedOnDailyBy": {
                "_id": "6572dcc6bbd6664053b1fa6b",
                "avatarUrl": "/avatars/aba29efd00bc41f14ce422f7807cd2c3.svg",
                "isPro": false,
                "fullname": "Liu Yichen",
                "user": "lyclyc52",
                "type": "user"
            },
            "summary": "Generative models have been widely applied to world modeling for environment\nsimulation and future state prediction. With advancements in autonomous\ndriving, there is a growing demand not only for high-fidelity video generation\nunder various controls, but also for producing diverse and meaningful\ninformation such as depth estimation. To address this, we propose CVD-STORM, a\ncross-view video diffusion model utilizing a spatial-temporal reconstruction\nVariational Autoencoder (VAE) that generates long-term, multi-view videos with\n4D reconstruction capabilities under various control inputs. Our approach first\nfine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its\nability to encode 3D structures and temporal dynamics. Subsequently, we\nintegrate this VAE into the video diffusion process to significantly improve\ngeneration quality. Experimental results demonstrate that our model achieves\nsubstantial improvements in both FID and FVD metrics. Additionally, the\njointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic\nscenes, providing valuable geometric information for comprehensive scene\nunderstanding.",
            "upvotes": 23,
            "discussionId": "68f05c6336f8b025381e19ad",
            "projectPage": "https://sensetime-fvg.github.io/CVD-STORM/",
            "ai_summary": "CVD-STORM, a cross-view video diffusion model with a spatial-temporal reconstruction VAE, enhances video generation quality and provides depth estimation for dynamic scenes.",
            "ai_keywords": [
                "cross-view video diffusion model",
                "spatial-temporal reconstruction",
                "Variational Autoencoder (VAE)",
                "4D reconstruction",
                "FID",
                "FVD",
                "Gaussian Splatting Decoder"
            ]
        },
        "publishedAt": "2025-10-09T04:41:58.000Z",
        "title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal\n  Reconstruction Model for Autonomous Driving",
        "summary": "Generative models have been widely applied to world modeling for environment\nsimulation and future state prediction. With advancements in autonomous\ndriving, there is a growing demand not only for high-fidelity video generation\nunder various controls, but also for producing diverse and meaningful\ninformation such as depth estimation. To address this, we propose CVD-STORM, a\ncross-view video diffusion model utilizing a spatial-temporal reconstruction\nVariational Autoencoder (VAE) that generates long-term, multi-view videos with\n4D reconstruction capabilities under various control inputs. Our approach first\nfine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its\nability to encode 3D structures and temporal dynamics. Subsequently, we\nintegrate this VAE into the video diffusion process to significantly improve\ngeneration quality. Experimental results demonstrate that our model achieves\nsubstantial improvements in both FID and FVD metrics. Additionally, the\njointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic\nscenes, providing valuable geometric information for comprehensive scene\nunderstanding.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07944.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6572dcc6bbd6664053b1fa6b",
            "avatarUrl": "/avatars/aba29efd00bc41f14ce422f7807cd2c3.svg",
            "fullname": "Liu Yichen",
            "name": "lyclyc52",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13804",
            "authors": [
                {
                    "_id": "68f04ef636f8b025381e18d9",
                    "user": {
                        "_id": "653e5d31ffd60206c8b64bb5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653e5d31ffd60206c8b64bb5/bgztraPC27L6culMlJw4s.png",
                        "isPro": false,
                        "fullname": "Xinchen Zhang",
                        "user": "comin",
                        "type": "user"
                    },
                    "name": "Xinchen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:39:15.323Z",
                    "hidden": false
                },
                {
                    "_id": "68f04ef636f8b025381e18da",
                    "name": "Xiaoying Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f04ef636f8b025381e18db",
                    "name": "Youbin Wu",
                    "hidden": false
                },
                {
                    "_id": "68f04ef636f8b025381e18dc",
                    "name": "Yanbin Cao",
                    "hidden": false
                },
                {
                    "_id": "68f04ef636f8b025381e18dd",
                    "name": "Renrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f04ef636f8b025381e18de",
                    "name": "Ruihang Chu",
                    "hidden": false
                },
                {
                    "_id": "68f04ef636f8b025381e18df",
                    "name": "Ling Yang",
                    "hidden": false
                },
                {
                    "_id": "68f04ef636f8b025381e18e0",
                    "name": "Yujiu Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T17:59:24.000Z",
            "submittedOnDailyAt": "2025-10-16T00:18:57.663Z",
            "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce Generative Universal Verifier, a novel concept and plugin\ndesigned for next-generation multimodal reasoning in vision-language models and\nunified multimodal models, providing the fundamental capability of reflection\nand refinement on visual outcomes during the reasoning and generation process.\nThis work makes three main contributions: (1) We build ViVerBench, a\ncomprehensive benchmark spanning 16 categories of critical tasks for evaluating\nvisual outcomes in multimodal reasoning. Results show that existing VLMs\nconsistently underperform across these tasks, underscoring a substantial gap\nfrom human-level capability in reliable visual verification. (2) We design two\nautomated pipelines to construct large-scale visual verification data and train\nOmniVerifier-7B, the first omni-capable generative verifier trained for\nuniversal visual verification and achieves notable gains on ViVerBench(+8.3).\nThrough training, we identify three atomic capabilities in visual verification\nand demonstrate how they generalize and interact synergistically. (3) We\npropose OmniVerifier-TTS, a sequential test-time scaling paradigm that\nleverages the universal verifier to bridge image generation and editing within\nunified models, enhancing the upper bound of generative ability through\niterative fine-grained optimization. Beyond generation, we extend universal\nverifier to broader world-modeling interleaved reasoning scenarios.\nEmpirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),\nand GenEval++(+4.3), outperforming existing parallel test-time scaling methods,\nsuch as Best-of-N. By endowing multimodal reasoning with reliable visual\nverification, OmniVerifier advances both reliable reflection during generation\nand scalable test-time refinement, marking a step toward more trustworthy and\ncontrollable next-generation reasoning systems.",
            "upvotes": 22,
            "discussionId": "68f04ef636f8b025381e18e1",
            "projectPage": "https://omniverifier.github.io/",
            "githubRepo": "https://github.com/Cominclip/OmniVerifier",
            "ai_summary": "Generative Universal Verifier enhances multimodal reasoning by providing reliable visual verification through ViVerBench, OmniVerifier-7B, and OmniVerifier-TTS, improving generation and refinement capabilities.",
            "ai_keywords": [
                "Generative Universal Verifier",
                "ViVerBench",
                "OmniVerifier-7B",
                "OmniVerifier-TTS",
                "multimodal reasoning",
                "vision-language models",
                "visual verification",
                "atomic capabilities",
                "sequential test-time scaling",
                "T2I-ReasonBench",
                "GenEval++"
            ],
            "githubStars": 13,
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-10-15T13:59:24.000Z",
        "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
        "summary": "We introduce Generative Universal Verifier, a novel concept and plugin\ndesigned for next-generation multimodal reasoning in vision-language models and\nunified multimodal models, providing the fundamental capability of reflection\nand refinement on visual outcomes during the reasoning and generation process.\nThis work makes three main contributions: (1) We build ViVerBench, a\ncomprehensive benchmark spanning 16 categories of critical tasks for evaluating\nvisual outcomes in multimodal reasoning. Results show that existing VLMs\nconsistently underperform across these tasks, underscoring a substantial gap\nfrom human-level capability in reliable visual verification. (2) We design two\nautomated pipelines to construct large-scale visual verification data and train\nOmniVerifier-7B, the first omni-capable generative verifier trained for\nuniversal visual verification and achieves notable gains on ViVerBench(+8.3).\nThrough training, we identify three atomic capabilities in visual verification\nand demonstrate how they generalize and interact synergistically. (3) We\npropose OmniVerifier-TTS, a sequential test-time scaling paradigm that\nleverages the universal verifier to bridge image generation and editing within\nunified models, enhancing the upper bound of generative ability through\niterative fine-grained optimization. Beyond generation, we extend universal\nverifier to broader world-modeling interleaved reasoning scenarios.\nEmpirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),\nand GenEval++(+4.3), outperforming existing parallel test-time scaling methods,\nsuch as Best-of-N. By endowing multimodal reasoning with reliable visual\nverification, OmniVerifier advances both reliable reflection during generation\nand scalable test-time refinement, marking a step toward more trustworthy and\ncontrollable next-generation reasoning systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13804.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 127
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11062",
            "authors": [
                {
                    "_id": "68f089fb36f8b025381e1a3b",
                    "name": "Yujie Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f089fb36f8b025381e1a3c",
                    "user": {
                        "_id": "6301d6455e305a35cb0846a7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
                        "isPro": true,
                        "fullname": "Lanxiang Hu",
                        "user": "Snyhlxde",
                        "type": "user"
                    },
                    "name": "Lanxiang Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:35:06.135Z",
                    "hidden": false
                },
                {
                    "_id": "68f089fb36f8b025381e1a3d",
                    "name": "Yang Wang",
                    "hidden": false
                },
                {
                    "_id": "68f089fb36f8b025381e1a3e",
                    "name": "Minmin Hou",
                    "hidden": false
                },
                {
                    "_id": "68f089fb36f8b025381e1a3f",
                    "name": "Hao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f089fb36f8b025381e1a40",
                    "name": "Ke Ding",
                    "hidden": false
                },
                {
                    "_id": "68f089fb36f8b025381e1a41",
                    "name": "Jishen Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T06:55:09.000Z",
            "submittedOnDailyAt": "2025-10-16T04:31:16.464Z",
            "title": "Stronger Together: On-Policy Reinforcement Learning for Collaborative\n  LLMs",
            "submittedOnDailyBy": {
                "_id": "6301d6455e305a35cb0846a7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
                "isPro": true,
                "fullname": "Lanxiang Hu",
                "user": "Snyhlxde",
                "type": "user"
            },
            "summary": "Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to\nenhance the agentic capabilities of large language models (LLMs). MAS improves\ntask performance through role-based orchestration, while RL uses environmental\nrewards to learn stronger policies, such as GRPO-style optimization. However,\napplying on-policy RL to MAS remains underexplored and presents unique\nchallenges. Algorithmically, standard GRPO grouping assumptions break down\nbecause prompts vary by role and by turn. System-wise, the training stack must\nsupport MAS-workflow rollouts and on-policy updates for both single-policy and\nmulti-policy models.\n  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL\nalgorithm tailored to MAS and (ii) a training system that supports both single-\nand multi-policy regimes. Across game, planning, coding, and math tasks,\nAT-GRPO delivers substantial gains. On long-horizon planning, it increases\naccuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5\npercent. It also improves reasoning performance, with average gains of 3.87 to\n7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and\nenvironments are available at: https://github.com/pettingllms-ai/PettingLLMs.",
            "upvotes": 18,
            "discussionId": "68f089fb36f8b025381e1a42",
            "projectPage": "https://pettingllms-ai.github.io/",
            "githubRepo": "https://github.com/pettingllms-ai/PettingLLMs",
            "ai_summary": "AT-GRPO, a tailored RL algorithm for multi-agent systems, significantly enhances performance across various tasks by addressing unique challenges in on-policy RL.",
            "ai_keywords": [
                "multi-agent systems",
                "reinforcement learning",
                "large language models",
                "role-based orchestration",
                "GRPO-style optimization",
                "on-policy RL",
                "agent-wise grouped RL",
                "turn-wise grouped RL",
                "single-policy",
                "multi-policy",
                "long-horizon planning",
                "reasoning performance",
                "coding tasks",
                "math tasks"
            ],
            "githubStars": 9
        },
        "publishedAt": "2025-10-13T02:55:09.000Z",
        "title": "Stronger Together: On-Policy Reinforcement Learning for Collaborative\n  LLMs",
        "summary": "Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to\nenhance the agentic capabilities of large language models (LLMs). MAS improves\ntask performance through role-based orchestration, while RL uses environmental\nrewards to learn stronger policies, such as GRPO-style optimization. However,\napplying on-policy RL to MAS remains underexplored and presents unique\nchallenges. Algorithmically, standard GRPO grouping assumptions break down\nbecause prompts vary by role and by turn. System-wise, the training stack must\nsupport MAS-workflow rollouts and on-policy updates for both single-policy and\nmulti-policy models.\n  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL\nalgorithm tailored to MAS and (ii) a training system that supports both single-\nand multi-policy regimes. Across game, planning, coding, and math tasks,\nAT-GRPO delivers substantial gains. On long-horizon planning, it increases\naccuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5\npercent. It also improves reasoning performance, with average gains of 3.87 to\n7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and\nenvironments are available at: https://github.com/pettingllms-ai/PettingLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11062.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6301d6455e305a35cb0846a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
            "fullname": "Lanxiang Hu",
            "name": "Snyhlxde",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.13800",
            "authors": [
                {
                    "_id": "68f09ba636f8b025381e1a7d",
                    "name": "Yiming Chen",
                    "hidden": false
                },
                {
                    "_id": "68f09ba636f8b025381e1a7e",
                    "name": "Zekun Qi",
                    "hidden": false
                },
                {
                    "_id": "68f09ba636f8b025381e1a7f",
                    "name": "Wenyao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f09ba636f8b025381e1a80",
                    "name": "Xin Jin",
                    "hidden": false
                },
                {
                    "_id": "68f09ba636f8b025381e1a81",
                    "name": "Li Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f09ba636f8b025381e1a82",
                    "name": "Peidong Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T17:58:08.000Z",
            "submittedOnDailyAt": "2025-10-16T05:48:15.437Z",
            "title": "Reasoning in Space via Grounding in the World",
            "submittedOnDailyBy": {
                "_id": "63c3e8abc7d7f4c63a515a02",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
                "isPro": false,
                "fullname": "Zekun Qi",
                "user": "qizekun",
                "type": "user"
            },
            "summary": "In this paper, we claim that 3D visual grounding is the cornerstone of\nspatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to\nexplore the effective spatial representations that bridge the gap between them.\nExisting 3D LLMs suffer from the absence of a unified 3D representation capable\nof jointly capturing semantic and geometric information. This deficiency is\nmanifested either in poor performance on grounding or in an excessive reliance\non external modules, ultimately hindering the seamless integration of grounding\nand spatial reasoning. To address this, we propose a simple yet effective\ndual-path pooling mechanism that tightly aligns geometric features with both\nsemantic and positional cues, constructing a unified image patch-based 3D\nrepresentation that encapsulates all essential information without increasing\nthe number of input tokens. Leveraging this holistic representation,\nGS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely\nwithout external modules while delivering performance comparable to\nstate-of-the-art models, establishing a unified and self-contained framework\nfor 3D spatial reasoning. To further bridge grounding and spatial reasoning, we\nintroduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is\nmeticulously curated to include both 3D bounding box annotations for objects\nreferenced in reasoning questions and step-by-step reasoning paths that\nintegrate grounding as a core component of the problem-solving process.\nExtensive experiments demonstrate that GS-Reasoner achieves impressive results\non 3D visual grounding, which in turn significantly enhances its spatial\nreasoning capabilities, leading to state-of-the-art performance.",
            "upvotes": 13,
            "discussionId": "68f09ba636f8b025381e1a83",
            "projectPage": "https://yiming-cc.github.io/gs-reasoner/",
            "githubRepo": "https://github.com/WU-CVGL/GS-Reasoner",
            "ai_summary": "GS-Reasoner, a 3D LLM with a dual-path pooling mechanism, achieves autoregressive grounding and state-of-the-art spatial reasoning without external modules.",
            "ai_keywords": [
                "3D visual grounding",
                "spatial reasoning",
                "Grounded-Spatial Reasoner (GS-Reasoner)",
                "3D LLMs",
                "unified 3D representation",
                "dual-path pooling mechanism",
                "geometric features",
                "semantic cues",
                "positional cues",
                "image patch-based representation",
                "autoregressive grounding",
                "Grounded Chain-of-Thought (GCoT) dataset",
                "3D bounding box annotations",
                "step-by-step reasoning paths"
            ],
            "githubStars": 16
        },
        "publishedAt": "2025-10-15T13:58:08.000Z",
        "title": "Reasoning in Space via Grounding in the World",
        "summary": "In this paper, we claim that 3D visual grounding is the cornerstone of\nspatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to\nexplore the effective spatial representations that bridge the gap between them.\nExisting 3D LLMs suffer from the absence of a unified 3D representation capable\nof jointly capturing semantic and geometric information. This deficiency is\nmanifested either in poor performance on grounding or in an excessive reliance\non external modules, ultimately hindering the seamless integration of grounding\nand spatial reasoning. To address this, we propose a simple yet effective\ndual-path pooling mechanism that tightly aligns geometric features with both\nsemantic and positional cues, constructing a unified image patch-based 3D\nrepresentation that encapsulates all essential information without increasing\nthe number of input tokens. Leveraging this holistic representation,\nGS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely\nwithout external modules while delivering performance comparable to\nstate-of-the-art models, establishing a unified and self-contained framework\nfor 3D spatial reasoning. To further bridge grounding and spatial reasoning, we\nintroduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is\nmeticulously curated to include both 3D bounding box annotations for objects\nreferenced in reasoning questions and step-by-step reasoning paths that\nintegrate grounding as a core component of the problem-solving process.\nExtensive experiments demonstrate that GS-Reasoner achieves impressive results\non 3D visual grounding, which in turn significantly enhances its spatial\nreasoning capabilities, leading to state-of-the-art performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13800.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63c3e8abc7d7f4c63a515a02",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
            "fullname": "Zekun Qi",
            "name": "qizekun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13778",
            "authors": [
                {
                    "_id": "68f0545236f8b025381e191a",
                    "user": {
                        "_id": "676243ab3c9c66292d2b3267",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jeIGuv4xLGY_buc-BKYuF.png",
                        "isPro": false,
                        "fullname": "xinyi chen",
                        "user": "quasdo",
                        "type": "user"
                    },
                    "name": "Xinyi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T10:38:30.945Z",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e191b",
                    "name": "Yilun Chen",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e191c",
                    "name": "Yanwei Fu",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e191d",
                    "name": "Ning Gao",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e191e",
                    "name": "Jiaya Jia",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e191f",
                    "user": {
                        "_id": "66608add236f958513d21d2e",
                        "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg",
                        "isPro": false,
                        "fullname": "Weiyang Jin",
                        "user": "Wayne-King",
                        "type": "user"
                    },
                    "name": "Weiyang Jin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:38:55.583Z",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e1920",
                    "name": "Hao Li",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e1921",
                    "name": "Yao Mu",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e1922",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e1923",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e1924",
                    "name": "Yang Tian",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e1925",
                    "name": "Bin Wang",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e1926",
                    "name": "Bolun Wang",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e1927",
                    "name": "Fangjing Wang",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e1928",
                    "name": "Hanqing Wang",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e1929",
                    "name": "Tai Wang",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e192a",
                    "user": {
                        "_id": "65537770c9e9343421eb40a1",
                        "avatarUrl": "/avatars/682458ecbdd69421c4adb741b31de8d7.svg",
                        "isPro": false,
                        "fullname": "wang",
                        "user": "wz7in",
                        "type": "user"
                    },
                    "name": "Ziqin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:38:51.687Z",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e192b",
                    "name": "Xueyuan Wei",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e192c",
                    "name": "Chao Wu",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e192d",
                    "name": "Shuai Yang",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e192e",
                    "name": "Jinhui Ye",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e192f",
                    "name": "Junqiu Yu",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e1930",
                    "name": "Jia Zeng",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e1931",
                    "name": "Jingjing Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e1932",
                    "name": "Jinyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e1933",
                    "name": "Shi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e1934",
                    "name": "Feng Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e1935",
                    "name": "Bowen Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f0545236f8b025381e1936",
                    "user": {
                        "_id": "645b11ed5196a37266166d63",
                        "avatarUrl": "/avatars/87ee10835aeda3f358e7d7d16900a324.svg",
                        "isPro": false,
                        "fullname": "zhuyangkun",
                        "user": "tenstep",
                        "type": "user"
                    },
                    "name": "Yangkun Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T07:25:41.023Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T17:30:05.000Z",
            "submittedOnDailyAt": "2025-10-16T00:42:25.171Z",
            "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for\n  Generalist Robot Policy",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and\nrobot control that advances instruction-following robots toward scalable,\ngeneral-purpose intelligence. Its core idea is spatially guided\nvision-language-action training, where spatial grounding serves as the critical\nlink between instructions and robot actions. InternVLA-M1 employs a two-stage\npipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning\ndata to determine ``where to act'' by aligning instructions with visual,\nembodiment-agnostic positions, and (ii) spatially guided action post-training\nto decide ``how to act'' by generating embodiment-aware actions through\nplug-and-play spatial prompting. This spatially guided training recipe yields\nconsistent gains: InternVLA-M1 outperforms its variant without spatial guidance\nby +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO\nFranka, while demonstrating stronger spatial reasoning capability in box,\npoint, and trace prediction. To further scale instruction following, we built a\nsimulation engine to collect 244K generalizable pick-and-place episodes,\nenabling a 6.2% average improvement across 200 tasks and 3K+ objects. In\nreal-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with\nsynthetic co-training, achieved +20.6% on unseen objects and novel\nconfigurations. Moreover, in long-horizon reasoning-intensive scenarios, it\nsurpassed existing works by over 10%. These results highlight spatially guided\ntraining as a unifying principle for scalable and resilient generalist robots.\nCode and models are available at\nhttps://github.com/InternRobotics/InternVLA-M1.",
            "upvotes": 13,
            "discussionId": "68f0545236f8b025381e1937",
            "projectPage": "https://internrobotics.github.io/internvla-m1.github.io/",
            "githubRepo": "https://github.com/InternRobotics/InternVLA-M1",
            "ai_summary": "A unified framework for spatial grounding and robot control, InternVLA-M1, enhances instruction-following robots through spatially guided vision-language-action training, achieving significant improvements across various tasks and simulations.",
            "ai_keywords": [
                "spatial grounding",
                "vision-language-action training",
                "spatial reasoning",
                "spatial prompting",
                "SimplerEnv",
                "WidowX",
                "LIBERO Franka",
                "pick-and-place",
                "long-horizon reasoning"
            ],
            "githubStars": 159
        },
        "publishedAt": "2025-10-15T13:30:05.000Z",
        "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for\n  Generalist Robot Policy",
        "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and\nrobot control that advances instruction-following robots toward scalable,\ngeneral-purpose intelligence. Its core idea is spatially guided\nvision-language-action training, where spatial grounding serves as the critical\nlink between instructions and robot actions. InternVLA-M1 employs a two-stage\npipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning\ndata to determine ``where to act'' by aligning instructions with visual,\nembodiment-agnostic positions, and (ii) spatially guided action post-training\nto decide ``how to act'' by generating embodiment-aware actions through\nplug-and-play spatial prompting. This spatially guided training recipe yields\nconsistent gains: InternVLA-M1 outperforms its variant without spatial guidance\nby +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO\nFranka, while demonstrating stronger spatial reasoning capability in box,\npoint, and trace prediction. To further scale instruction following, we built a\nsimulation engine to collect 244K generalizable pick-and-place episodes,\nenabling a 6.2% average improvement across 200 tasks and 3K+ objects. In\nreal-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with\nsynthetic co-training, achieved +20.6% on unseen objects and novel\nconfigurations. Moreover, in long-horizon reasoning-intensive scenarios, it\nsurpassed existing works by over 10%. These results highlight spatially guided\ntraining as a unifying principle for scalable and resilient generalist robots.\nCode and models are available at\nhttps://github.com/InternRobotics/InternVLA-M1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13778.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 127
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13621",
            "authors": [
                {
                    "_id": "68f0515b36f8b025381e18f5",
                    "name": "Yuexing Hao",
                    "hidden": false
                },
                {
                    "_id": "68f0515b36f8b025381e18f6",
                    "name": "Yue Huang",
                    "hidden": false
                },
                {
                    "_id": "68f0515b36f8b025381e18f7",
                    "name": "Haoran Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f0515b36f8b025381e18f8",
                    "name": "Chenyang Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f0515b36f8b025381e18f9",
                    "name": "Zhenwen Liang",
                    "hidden": false
                },
                {
                    "_id": "68f0515b36f8b025381e18fa",
                    "name": "Paul Pu Liang",
                    "hidden": false
                },
                {
                    "_id": "68f0515b36f8b025381e18fb",
                    "name": "Yue Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f0515b36f8b025381e18fc",
                    "name": "Lichao Sun",
                    "hidden": false
                },
                {
                    "_id": "68f0515b36f8b025381e18fd",
                    "name": "Saleh Kalantari",
                    "hidden": false
                },
                {
                    "_id": "68f0515b36f8b025381e18fe",
                    "name": "Xiangliang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f0515b36f8b025381e18ff",
                    "name": "Marzyeh Ghassemi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T14:50:45.000Z",
            "submittedOnDailyAt": "2025-10-16T00:29:52.071Z",
            "title": "The Role of Computing Resources in Publishing Foundation Model Research",
            "submittedOnDailyBy": {
                "_id": "639d94ab7145123e0d44e48a",
                "avatarUrl": "/avatars/5bb6a65b306d1383c4a8bcd9334b470a.svg",
                "isPro": false,
                "fullname": "Yue Huang",
                "user": "HowieHwong",
                "type": "user"
            },
            "summary": "Cutting-edge research in Artificial Intelligence (AI) requires considerable\nresources, including Graphics Processing Units (GPUs), data, and human\nresources. In this paper, we evaluate of the relationship between these\nresources and the scientific advancement of foundation models (FM). We reviewed\n6517 FM papers published between 2022 to 2024, and surveyed 229 first-authors\nto the impact of computing resources on scientific output. We find that\nincreased computing is correlated with national funding allocations and\ncitations, but our findings don't observe the strong correlations with research\nenvironment (academic or industrial), domain, or study methodology. We advise\nthat individuals and institutions focus on creating shared and affordable\ncomputing opportunities to lower the entry barrier for under-resourced\nresearchers. These steps can help expand participation in FM research, foster\ndiversity of ideas and contributors, and sustain innovation and progress in AI.\nThe data will be available at: https://mit-calc.csail.mit.edu/",
            "upvotes": 13,
            "discussionId": "68f0515b36f8b025381e1900",
            "ai_summary": "Increased computing resources are correlated with national funding and citations in foundation model research, but not with research environment, domain, or methodology.",
            "ai_keywords": [
                "foundation models",
                "computing resources",
                "national funding",
                "citations",
                "research environment",
                "domain",
                "study methodology",
                "shared computing",
                "affordable computing",
                "under-resourced researchers",
                "diversity of ideas",
                "innovation",
                "progress in AI"
            ]
        },
        "publishedAt": "2025-10-15T10:50:45.000Z",
        "title": "The Role of Computing Resources in Publishing Foundation Model Research",
        "summary": "Cutting-edge research in Artificial Intelligence (AI) requires considerable\nresources, including Graphics Processing Units (GPUs), data, and human\nresources. In this paper, we evaluate of the relationship between these\nresources and the scientific advancement of foundation models (FM). We reviewed\n6517 FM papers published between 2022 to 2024, and surveyed 229 first-authors\nto the impact of computing resources on scientific output. We find that\nincreased computing is correlated with national funding allocations and\ncitations, but our findings don't observe the strong correlations with research\nenvironment (academic or industrial), domain, or study methodology. We advise\nthat individuals and institutions focus on creating shared and affordable\ncomputing opportunities to lower the entry barrier for under-resourced\nresearchers. These steps can help expand participation in FM research, foster\ndiversity of ideas and contributors, and sustain innovation and progress in AI.\nThe data will be available at: https://mit-calc.csail.mit.edu/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13621.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "639d94ab7145123e0d44e48a",
            "avatarUrl": "/avatars/5bb6a65b306d1383c4a8bcd9334b470a.svg",
            "fullname": "Yue Huang",
            "name": "HowieHwong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13515",
            "authors": [
                {
                    "_id": "68f052c836f8b025381e190a",
                    "user": {
                        "_id": "641030c77a15af878ae5bd8f",
                        "avatarUrl": "/avatars/8a5037edf55c78ebc317c8b191343671.svg",
                        "isPro": false,
                        "fullname": "TianchengGu",
                        "user": "TianchengGu",
                        "type": "user"
                    },
                    "name": "Tiancheng Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T11:40:30.471Z",
                    "hidden": false
                },
                {
                    "_id": "68f052c836f8b025381e190b",
                    "user": {
                        "_id": "63e202f352b7578dba448ab5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
                        "isPro": false,
                        "fullname": "Yang",
                        "user": "Kaichengalex",
                        "type": "user"
                    },
                    "name": "Kaicheng Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:39:01.782Z",
                    "hidden": false
                },
                {
                    "_id": "68f052c836f8b025381e190c",
                    "name": "Kaichen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f052c836f8b025381e190d",
                    "name": "Xiang An",
                    "hidden": false
                },
                {
                    "_id": "68f052c836f8b025381e190e",
                    "name": "Ziyong Feng",
                    "hidden": false
                },
                {
                    "_id": "68f052c836f8b025381e190f",
                    "name": "Yueyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f052c836f8b025381e1910",
                    "name": "Weidong Cai",
                    "hidden": false
                },
                {
                    "_id": "68f052c836f8b025381e1911",
                    "name": "Jiankang Deng",
                    "hidden": false
                },
                {
                    "_id": "68f052c836f8b025381e1912",
                    "name": "Lidong Bing",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T13:07:00.000Z",
            "submittedOnDailyAt": "2025-10-16T00:36:48.179Z",
            "title": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
            "submittedOnDailyBy": {
                "_id": "63e202f352b7578dba448ab5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
                "isPro": false,
                "fullname": "Yang",
                "user": "Kaichengalex",
                "type": "user"
            },
            "summary": "Universal multimodal embedding models are foundational to various tasks.\nExisting approaches typically employ in-batch negative mining by measuring the\nsimilarity of query-candidate pairs. However, these methods often struggle to\ncapture subtle semantic differences among candidates and lack diversity in\nnegative samples. Moreover, the embeddings exhibit limited discriminative\nability in distinguishing false and hard negatives. In this paper, we leverage\nthe advanced understanding capabilities of MLLMs to enhance representation\nlearning and present a novel Universal Multimodal Embedding (UniME-V2) model.\nOur approach first constructs a potential hard negative set through global\nretrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes\nMLLMs to assess the semantic alignment of query-candidate pairs and generate\nsoft semantic matching scores. These scores serve as a foundation for hard\nnegative mining, mitigating the impact of false negatives and enabling the\nidentification of diverse, high-quality hard negatives. Furthermore, the\nsemantic matching scores are used as soft labels to mitigate the rigid\none-to-one mapping constraint. By aligning the similarity matrix with the soft\nsemantic matching score matrix, the model learns semantic distinctions among\ncandidates, significantly enhancing its discriminative capacity. To further\nimprove performance, we propose UniME-V2-Reranker, a reranking model trained on\nour mined hard negatives through a joint pairwise and listwise optimization\napproach. We conduct comprehensive experiments on the MMEB benchmark and\nmultiple retrieval tasks, demonstrating that our method achieves\nstate-of-the-art performance on average across all tasks.",
            "upvotes": 10,
            "discussionId": "68f052c836f8b025381e1913",
            "projectPage": "https://garygutc.github.io/UniME-v2/",
            "githubRepo": "https://github.com/GaryGuTC/UniME-v2",
            "ai_summary": "A novel Universal Multimodal Embedding (UniME-V2) model uses MLLMs to enhance representation learning by identifying diverse, high-quality hard negatives and improving discriminative capacity through soft semantic matching scores.",
            "ai_keywords": [
                "Universal Multimodal Embedding",
                "UniME-V2",
                "MLLMs",
                "in-batch negative mining",
                "global retrieval",
                "hard negative mining",
                "semantic alignment",
                "soft semantic matching scores",
                "soft labels",
                "joint pairwise and listwise optimization",
                "MMEB benchmark",
                "retrieval tasks"
            ],
            "githubStars": 24
        },
        "publishedAt": "2025-10-15T09:07:00.000Z",
        "title": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
        "summary": "Universal multimodal embedding models are foundational to various tasks.\nExisting approaches typically employ in-batch negative mining by measuring the\nsimilarity of query-candidate pairs. However, these methods often struggle to\ncapture subtle semantic differences among candidates and lack diversity in\nnegative samples. Moreover, the embeddings exhibit limited discriminative\nability in distinguishing false and hard negatives. In this paper, we leverage\nthe advanced understanding capabilities of MLLMs to enhance representation\nlearning and present a novel Universal Multimodal Embedding (UniME-V2) model.\nOur approach first constructs a potential hard negative set through global\nretrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes\nMLLMs to assess the semantic alignment of query-candidate pairs and generate\nsoft semantic matching scores. These scores serve as a foundation for hard\nnegative mining, mitigating the impact of false negatives and enabling the\nidentification of diverse, high-quality hard negatives. Furthermore, the\nsemantic matching scores are used as soft labels to mitigate the rigid\none-to-one mapping constraint. By aligning the similarity matrix with the soft\nsemantic matching score matrix, the model learns semantic distinctions among\ncandidates, significantly enhancing its discriminative capacity. To further\nimprove performance, we propose UniME-V2-Reranker, a reranking model trained on\nour mined hard negatives through a joint pairwise and listwise optimization\napproach. We conduct comprehensive experiments on the MMEB benchmark and\nmultiple retrieval tasks, demonstrating that our method achieves\nstate-of-the-art performance on average across all tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13515.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63e202f352b7578dba448ab5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
            "fullname": "Yang",
            "name": "Kaichengalex",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.11438",
            "authors": [
                {
                    "_id": "68eea08a510125ca000ccf59",
                    "name": "Yujiang Wu",
                    "hidden": false
                },
                {
                    "_id": "68eea08a510125ca000ccf5a",
                    "user": {
                        "_id": "64311e8edd466752c73ab69a",
                        "avatarUrl": "/avatars/34f559399e57534ee9883029c8b91ccf.svg",
                        "isPro": false,
                        "fullname": "Shanshan Zhong",
                        "user": "zhongshsh",
                        "type": "user"
                    },
                    "name": "Shanshan Zhong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T03:13:18.432Z",
                    "hidden": false
                },
                {
                    "_id": "68eea08a510125ca000ccf5b",
                    "name": "Yubin Kim",
                    "hidden": false
                },
                {
                    "_id": "68eea08a510125ca000ccf5c",
                    "name": "Chenyan Xiong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T14:10:26.000Z",
            "submittedOnDailyAt": "2025-10-16T13:37:30.305Z",
            "title": "What Generative Search Engines Like and How to Optimize Web Content\n  Cooperatively",
            "submittedOnDailyBy": {
                "_id": "64311e8edd466752c73ab69a",
                "avatarUrl": "/avatars/34f559399e57534ee9883029c8b91ccf.svg",
                "isPro": false,
                "fullname": "Shanshan Zhong",
                "user": "zhongshsh",
                "type": "user"
            },
            "summary": "By employing large language models (LLMs) to retrieve documents and generate\nnatural language responses, Generative Engines, such as Google AI overview and\nChatGPT, provide significantly enhanced user experiences and have rapidly\nbecome the new form of search. Their rapid adoption also drives the needs of\nGenerative Engine Optimization (GEO), as content providers are eager to gain\nmore traction from them. In this paper, we introduce AutoGEO, a framework to\nautomatically learn generative engine preferences when using retrieved contents\nfor response generation, and rewrite web contents for more such traction.\nAutoGEO first prompts frontier LLMs to explain generative engine preferences\nand extract meaningful preference rules from these explanations. Then it uses\npreference rules as context engineering for AutoGEO_API, a\nprompt-based GEO system, and as rule-based rewards to train\nAutoGEO_Mini, a cost-effective GEO model. Experiments on the standard\nGEO-Bench and two newly constructed benchmarks using real user queries\ndemonstrate the effectiveness of AutoGEO in enhancing content traction while\npreserving search utility. Analyses confirm the learned rules' robustness and\nabilities to capture unique preferences in variant domains, and AutoGEO\nsystems' ability to embed them in content optimization. The code is released at\nhttps://github.com/cxcscmu/AutoGEO.",
            "upvotes": 10,
            "discussionId": "68eea08b510125ca000ccf5d",
            "githubRepo": "https://github.com/cxcscmu/AutoGEO",
            "ai_summary": "AutoGEO, a framework for optimizing generative engines, learns and applies preference rules to enhance content traction and search utility using large language models.",
            "ai_keywords": [
                "large language models",
                "Generative Engines",
                "Generative Engine Optimization",
                "AutoGEO",
                "GEO-Bench",
                "prompt-based GEO system",
                "rule-based rewards",
                "content optimization"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-10-13T10:10:26.000Z",
        "title": "What Generative Search Engines Like and How to Optimize Web Content\n  Cooperatively",
        "summary": "By employing large language models (LLMs) to retrieve documents and generate\nnatural language responses, Generative Engines, such as Google AI overview and\nChatGPT, provide significantly enhanced user experiences and have rapidly\nbecome the new form of search. Their rapid adoption also drives the needs of\nGenerative Engine Optimization (GEO), as content providers are eager to gain\nmore traction from them. In this paper, we introduce AutoGEO, a framework to\nautomatically learn generative engine preferences when using retrieved contents\nfor response generation, and rewrite web contents for more such traction.\nAutoGEO first prompts frontier LLMs to explain generative engine preferences\nand extract meaningful preference rules from these explanations. Then it uses\npreference rules as context engineering for AutoGEO_API, a\nprompt-based GEO system, and as rule-based rewards to train\nAutoGEO_Mini, a cost-effective GEO model. Experiments on the standard\nGEO-Bench and two newly constructed benchmarks using real user queries\ndemonstrate the effectiveness of AutoGEO in enhancing content traction while\npreserving search utility. Analyses confirm the learned rules' robustness and\nabilities to capture unique preferences in variant domains, and AutoGEO\nsystems' ability to embed them in content optimization. The code is released at\nhttps://github.com/cxcscmu/AutoGEO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11438.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64311e8edd466752c73ab69a",
            "avatarUrl": "/avatars/34f559399e57534ee9883029c8b91ccf.svg",
            "fullname": "Shanshan Zhong",
            "name": "zhongshsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.13786",
            "authors": [
                {
                    "_id": "68f0e30636f8b025381e1b7e",
                    "name": "Devvrit Khatri",
                    "hidden": false
                },
                {
                    "_id": "68f0e30636f8b025381e1b7f",
                    "name": "Lovish Madaan",
                    "hidden": false
                },
                {
                    "_id": "68f0e30636f8b025381e1b80",
                    "name": "Rishabh Tiwari",
                    "hidden": false
                },
                {
                    "_id": "68f0e30636f8b025381e1b81",
                    "name": "Rachit Bansal",
                    "hidden": false
                },
                {
                    "_id": "68f0e30636f8b025381e1b82",
                    "name": "Sai Surya Duvvuri",
                    "hidden": false
                },
                {
                    "_id": "68f0e30636f8b025381e1b83",
                    "name": "Manzil Zaheer",
                    "hidden": false
                },
                {
                    "_id": "68f0e30636f8b025381e1b84",
                    "name": "Inderjit S. Dhillon",
                    "hidden": false
                },
                {
                    "_id": "68f0e30636f8b025381e1b85",
                    "name": "David Brandfonbrener",
                    "hidden": false
                },
                {
                    "_id": "68f0e30636f8b025381e1b86",
                    "name": "Rishabh Agarwal",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/5f1158120c833276f61f1a84/1GwLDsIbjzAmKdUuh1soU.png"
            ],
            "publishedAt": "2025-10-15T17:43:03.000Z",
            "submittedOnDailyAt": "2025-10-16T10:51:29.724Z",
            "title": "The Art of Scaling Reinforcement Learning Compute for LLMs",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": true,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) has become central to training large language\nmodels (LLMs), yet the field lacks predictive scaling methodologies comparable\nto those established for pre-training. Despite rapidly rising compute budgets,\nthere is no principled understanding of how to evaluate algorithmic\nimprovements for scaling RL compute. We present the first large-scale\nsystematic study, amounting to more than 400,000 GPU-hours, that defines a\nprincipled framework for analyzing and predicting RL scaling in LLMs. We fit\nsigmoidal compute-performance curves for RL training and ablate a wide range of\ncommon design choices to analyze their effects on asymptotic performance and\ncompute efficiency. We observe: (1) Not all recipes yield similar asymptotic\nperformance, (2) Details such as loss aggregation, normalization, curriculum,\nand off-policy algorithm primarily modulate compute efficiency without\nmaterially shifting the asymptote, and (3) Stable, scalable recipes follow\npredictable scaling trajectories, enabling extrapolation from smaller-scale\nruns. Combining these insights, we propose a best-practice recipe, ScaleRL, and\ndemonstrate its effectiveness by successfully scaling and predicting validation\nperformance on a single RL run scaled up to 100,000 GPU-hours. Our work\nprovides both a scientific framework for analyzing scaling in RL and a\npractical recipe that brings RL training closer to the predictability long\nachieved in pre-training.",
            "upvotes": 9,
            "discussionId": "68f0e30736f8b025381e1b87",
            "ai_summary": "A systematic study defines a framework for analyzing and predicting reinforcement learning scaling in large language models, identifying key design choices that affect compute efficiency and proposing a best-practice recipe.",
            "ai_keywords": [
                "reinforcement learning",
                "large language models",
                "sigmoidal compute-performance curves",
                "loss aggregation",
                "normalization",
                "curriculum",
                "off-policy algorithm",
                "asymptotic performance",
                "compute efficiency",
                "scaling trajectories",
                "ScaleRL"
            ],
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "publishedAt": "2025-10-15T13:43:03.000Z",
        "title": "The Art of Scaling Reinforcement Learning Compute for LLMs",
        "summary": "Reinforcement learning (RL) has become central to training large language\nmodels (LLMs), yet the field lacks predictive scaling methodologies comparable\nto those established for pre-training. Despite rapidly rising compute budgets,\nthere is no principled understanding of how to evaluate algorithmic\nimprovements for scaling RL compute. We present the first large-scale\nsystematic study, amounting to more than 400,000 GPU-hours, that defines a\nprincipled framework for analyzing and predicting RL scaling in LLMs. We fit\nsigmoidal compute-performance curves for RL training and ablate a wide range of\ncommon design choices to analyze their effects on asymptotic performance and\ncompute efficiency. We observe: (1) Not all recipes yield similar asymptotic\nperformance, (2) Details such as loss aggregation, normalization, curriculum,\nand off-policy algorithm primarily modulate compute efficiency without\nmaterially shifting the asymptote, and (3) Stable, scalable recipes follow\npredictable scaling trajectories, enabling extrapolation from smaller-scale\nruns. Combining these insights, we propose a best-practice recipe, ScaleRL, and\ndemonstrate its effectiveness by successfully scaling and predicting validation\nperformance on a single RL run scaled up to 100,000 GPU-hours. Our work\nprovides both a scientific framework for analyzing scaling in RL and a\npractical recipe that brings RL training closer to the predictability long\nachieved in pre-training.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/5f1158120c833276f61f1a84/1GwLDsIbjzAmKdUuh1soU.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13786.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 994
        },
        "organization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13759",
            "authors": [
                {
                    "_id": "68f05f6336f8b025381e19c3",
                    "user": {
                        "_id": "647993d9f966f086918da59e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647993d9f966f086918da59e/NDxz3PEpo3srZQNhwT7Qf.jpeg",
                        "isPro": false,
                        "fullname": "kzou",
                        "user": "jackyhate",
                        "type": "user"
                    },
                    "name": "Kai Zou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:35:13.604Z",
                    "hidden": false
                },
                {
                    "_id": "68f05f6336f8b025381e19c4",
                    "name": "Ziqi Huang",
                    "hidden": false
                },
                {
                    "_id": "68f05f6336f8b025381e19c5",
                    "name": "Yuhao Dong",
                    "hidden": false
                },
                {
                    "_id": "68f05f6336f8b025381e19c6",
                    "name": "Shulin Tian",
                    "hidden": false
                },
                {
                    "_id": "68f05f6336f8b025381e19c7",
                    "name": "Dian Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f05f6336f8b025381e19c8",
                    "name": "Hongbo Liu",
                    "hidden": false
                },
                {
                    "_id": "68f05f6336f8b025381e19c9",
                    "name": "Jingwen He",
                    "hidden": false
                },
                {
                    "_id": "68f05f6336f8b025381e19ca",
                    "name": "Bin Liu",
                    "hidden": false
                },
                {
                    "_id": "68f05f6336f8b025381e19cb",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68f05f6336f8b025381e19cc",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T17:10:35.000Z",
            "submittedOnDailyAt": "2025-10-16T01:31:54.776Z",
            "title": "Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark",
            "submittedOnDailyBy": {
                "_id": "647993d9f966f086918da59e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647993d9f966f086918da59e/NDxz3PEpo3srZQNhwT7Qf.jpeg",
                "isPro": false,
                "fullname": "kzou",
                "user": "jackyhate",
                "type": "user"
            },
            "summary": "Unified multimodal models aim to jointly enable visual understanding and\ngeneration, yet current benchmarks rarely examine their true integration.\nExisting evaluations either treat the two abilities in isolation or overlook\ntasks that inherently couple them. To address this gap, we present Uni-MMMU, a\ncomprehensive and discipline-aware benchmark that systematically unfolds the\nbidirectional synergy between generation and understanding across eight\nreasoning-centric domains, including science, coding, mathematics, and puzzles.\nEach task is bidirectionally coupled, demanding models to (i) leverage\nconceptual understanding to guide precise visual synthesis, or (ii) utilize\ngeneration as a cognitive scaffold for analytical reasoning. Uni-MMMU\nincorporates verifiable intermediate reasoning steps, unique ground truths, and\na reproducible scoring protocol for both textual and visual outputs. Through\nextensive evaluation of state-of-the-art unified, generation-only, and\nunderstanding-only models, we reveal substantial performance disparities and\ncross-modal dependencies, offering new insights into when and how these\nabilities reinforce one another, and establishing a reliable foundation for\nadvancing unified models.",
            "upvotes": 9,
            "discussionId": "68f05f6436f8b025381e19cd",
            "projectPage": "https://vchitect.github.io/Uni-MMMU-Project/",
            "githubRepo": "https://github.com/vchitect/Uni-MMMU",
            "ai_summary": "Uni-MMMU is a benchmark that evaluates the bidirectional synergy between visual understanding and generation across multiple domains, providing insights into their integration and performance.",
            "ai_keywords": [
                "unified multimodal models",
                "visual understanding",
                "visual generation",
                "bidirectional synergy",
                "reasoning-centric domains",
                "conceptual understanding",
                "precise visual synthesis",
                "cognitive scaffold",
                "analytical reasoning",
                "verifiable intermediate reasoning steps",
                "unique ground truths",
                "reproducible scoring protocol",
                "unified models",
                "generation-only models",
                "understanding-only models",
                "cross-modal dependencies"
            ],
            "githubStars": 9,
            "organization": {
                "_id": "6567494b05fb89fb7e0a38da",
                "name": "Vchitect",
                "fullname": "Vchitect",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63201256c6b20f03c829c4b8/-wsKa-mCdj-vuoD5u9N1C.jpeg"
            }
        },
        "publishedAt": "2025-10-15T13:10:35.000Z",
        "title": "Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark",
        "summary": "Unified multimodal models aim to jointly enable visual understanding and\ngeneration, yet current benchmarks rarely examine their true integration.\nExisting evaluations either treat the two abilities in isolation or overlook\ntasks that inherently couple them. To address this gap, we present Uni-MMMU, a\ncomprehensive and discipline-aware benchmark that systematically unfolds the\nbidirectional synergy between generation and understanding across eight\nreasoning-centric domains, including science, coding, mathematics, and puzzles.\nEach task is bidirectionally coupled, demanding models to (i) leverage\nconceptual understanding to guide precise visual synthesis, or (ii) utilize\ngeneration as a cognitive scaffold for analytical reasoning. Uni-MMMU\nincorporates verifiable intermediate reasoning steps, unique ground truths, and\na reproducible scoring protocol for both textual and visual outputs. Through\nextensive evaluation of state-of-the-art unified, generation-only, and\nunderstanding-only models, we reveal substantial performance disparities and\ncross-modal dependencies, offering new insights into when and how these\nabilities reinforce one another, and establishing a reliable foundation for\nadvancing unified models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13759.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647993d9f966f086918da59e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647993d9f966f086918da59e/NDxz3PEpo3srZQNhwT7Qf.jpeg",
            "fullname": "kzou",
            "name": "jackyhate",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "organization": {
            "_id": "6567494b05fb89fb7e0a38da",
            "name": "Vchitect",
            "fullname": "Vchitect",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63201256c6b20f03c829c4b8/-wsKa-mCdj-vuoD5u9N1C.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.13282",
            "authors": [
                {
                    "_id": "68f0754736f8b025381e19f9",
                    "user": {
                        "_id": "64ccd5cc4726a3f833831087",
                        "avatarUrl": "/avatars/6364b1ebb1d06c68245f4c786fb07ee9.svg",
                        "isPro": false,
                        "fullname": "Hu",
                        "user": "Jiakui",
                        "type": "user"
                    },
                    "name": "JiaKui Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T11:40:34.324Z",
                    "hidden": false
                },
                {
                    "_id": "68f0754736f8b025381e19fa",
                    "name": "Zhengjian Yao",
                    "hidden": false
                },
                {
                    "_id": "68f0754736f8b025381e19fb",
                    "user": {
                        "_id": "68776bf57ef0f82c9b71e166",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GYOfXhxWN0Xg1hyGOKDpA.png",
                        "isPro": false,
                        "fullname": "Lujia Jin",
                        "user": "jinlujia",
                        "type": "user"
                    },
                    "name": "Lujia Jin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T10:38:28.848Z",
                    "hidden": false
                },
                {
                    "_id": "68f0754736f8b025381e19fc",
                    "name": "Yinghao Chen",
                    "hidden": false
                },
                {
                    "_id": "68f0754736f8b025381e19fd",
                    "name": "Yanye Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T08:30:15.000Z",
            "submittedOnDailyAt": "2025-10-16T03:08:58.013Z",
            "title": "Universal Image Restoration Pre-training via Masked Degradation\n  Classification",
            "submittedOnDailyBy": {
                "_id": "64ccd5cc4726a3f833831087",
                "avatarUrl": "/avatars/6364b1ebb1d06c68245f4c786fb07ee9.svg",
                "isPro": false,
                "fullname": "Hu",
                "user": "Jiakui",
                "type": "user"
            },
            "summary": "This study introduces a Masked Degradation Classification Pre-Training method\n(MaskDCPT), designed to facilitate the classification of degradation types in\ninput images, leading to comprehensive image restoration pre-training. Unlike\nconventional pre-training methods, MaskDCPT uses the degradation type of the\nimage as an extremely weak supervision, while simultaneously leveraging the\nimage reconstruction to enhance performance and robustness. MaskDCPT includes\nan encoder and two decoders: the encoder extracts features from the masked\nlow-quality input image. The classification decoder uses these features to\nidentify the degradation type, whereas the reconstruction decoder aims to\nreconstruct a corresponding high-quality image. This design allows the\npre-training to benefit from both masked image modeling and contrastive\nlearning, resulting in a generalized representation suited for restoration\ntasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained\nencoder can be used to address universal image restoration and achieve\noutstanding performance. Implementing MaskDCPT significantly improves\nperformance for both convolution neural networks (CNNs) and Transformers, with\na minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and\na 34.8% reduction in PIQE compared to baseline in real-world degradation\nscenarios. It also emergences strong generalization to previously unseen\ndegradation types and levels. In addition, we curate and release the UIR-2.5M\ndataset, which includes 2.5 million paired restoration samples across 19\ndegradation types and over 200 degradation levels, incorporating both synthetic\nand real-world data. The dataset, source code, and models are available at\nhttps://github.com/MILab-PKU/MaskDCPT.",
            "upvotes": 9,
            "discussionId": "68f0754736f8b025381e19fe",
            "projectPage": "https://github.com/MILab-PKU/MaskDCPT",
            "githubRepo": "https://github.com/MILab-PKU/MaskDCPT",
            "ai_summary": "A Masked Degradation Classification Pre-Training method enhances image restoration by using degradation type classification and image reconstruction, improving performance across CNNs and Transformers.",
            "ai_keywords": [
                "Masked Degradation Classification Pre-Training",
                "MaskDCPT",
                "encoder",
                "decoders",
                "masked image modeling",
                "contrastive learning",
                "PSNR",
                "PIQE",
                "UIR-2.5M dataset"
            ],
            "githubStars": 8,
            "organization": {
                "_id": "61dcd8e344f59573371b5cb6",
                "name": "PekingUniversity",
                "fullname": "Peking University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
            }
        },
        "publishedAt": "2025-10-15T04:30:15.000Z",
        "title": "Universal Image Restoration Pre-training via Masked Degradation\n  Classification",
        "summary": "This study introduces a Masked Degradation Classification Pre-Training method\n(MaskDCPT), designed to facilitate the classification of degradation types in\ninput images, leading to comprehensive image restoration pre-training. Unlike\nconventional pre-training methods, MaskDCPT uses the degradation type of the\nimage as an extremely weak supervision, while simultaneously leveraging the\nimage reconstruction to enhance performance and robustness. MaskDCPT includes\nan encoder and two decoders: the encoder extracts features from the masked\nlow-quality input image. The classification decoder uses these features to\nidentify the degradation type, whereas the reconstruction decoder aims to\nreconstruct a corresponding high-quality image. This design allows the\npre-training to benefit from both masked image modeling and contrastive\nlearning, resulting in a generalized representation suited for restoration\ntasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained\nencoder can be used to address universal image restoration and achieve\noutstanding performance. Implementing MaskDCPT significantly improves\nperformance for both convolution neural networks (CNNs) and Transformers, with\na minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and\na 34.8% reduction in PIQE compared to baseline in real-world degradation\nscenarios. It also emergences strong generalization to previously unseen\ndegradation types and levels. In addition, we curate and release the UIR-2.5M\ndataset, which includes 2.5 million paired restoration samples across 19\ndegradation types and over 200 degradation levels, incorporating both synthetic\nand real-world data. The dataset, source code, and models are available at\nhttps://github.com/MILab-PKU/MaskDCPT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13282.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ccd5cc4726a3f833831087",
            "avatarUrl": "/avatars/6364b1ebb1d06c68245f4c786fb07ee9.svg",
            "fullname": "Hu",
            "name": "Jiakui",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.10274",
            "authors": [
                {
                    "_id": "68f0669336f8b025381e19cf",
                    "user": {
                        "_id": "64c0afc06b2f05ae642e1918",
                        "avatarUrl": "/avatars/70f9a87d123ba65a5f931db028bb095b.svg",
                        "isPro": false,
                        "fullname": "Jinliang Zheng",
                        "user": "2toINF",
                        "type": "user"
                    },
                    "name": "Jinliang Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:35:11.003Z",
                    "hidden": false
                },
                {
                    "_id": "68f0669336f8b025381e19d0",
                    "name": "Jianxiong Li",
                    "hidden": false
                },
                {
                    "_id": "68f0669336f8b025381e19d1",
                    "name": "Zhihao Wang",
                    "hidden": false
                },
                {
                    "_id": "68f0669336f8b025381e19d2",
                    "name": "Dongxiu Liu",
                    "hidden": false
                },
                {
                    "_id": "68f0669336f8b025381e19d3",
                    "name": "Xirui Kang",
                    "hidden": false
                },
                {
                    "_id": "68f0669336f8b025381e19d4",
                    "name": "Yuchun Feng",
                    "hidden": false
                },
                {
                    "_id": "68f0669336f8b025381e19d5",
                    "name": "Yinan Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f0669336f8b025381e19d6",
                    "name": "Jiayin Zou",
                    "hidden": false
                },
                {
                    "_id": "68f0669336f8b025381e19d7",
                    "name": "Yilun Chen",
                    "hidden": false
                },
                {
                    "_id": "68f0669336f8b025381e19d8",
                    "name": "Jia Zeng",
                    "hidden": false
                },
                {
                    "_id": "68f0669336f8b025381e19d9",
                    "name": "Ya-Qin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f0669336f8b025381e19da",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                },
                {
                    "_id": "68f0669336f8b025381e19db",
                    "name": "Jingjing Liu",
                    "hidden": false
                },
                {
                    "_id": "68f0669336f8b025381e19dc",
                    "name": "Tai Wang",
                    "hidden": false
                },
                {
                    "_id": "68f0669336f8b025381e19dd",
                    "name": "Xianyuan Zhan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-11T16:20:17.000Z",
            "submittedOnDailyAt": "2025-10-16T02:08:26.258Z",
            "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment\n  Vision-Language-Action Model",
            "submittedOnDailyBy": {
                "_id": "64c0afc06b2f05ae642e1918",
                "avatarUrl": "/avatars/70f9a87d123ba65a5f931db028bb095b.svg",
                "isPro": false,
                "fullname": "Jinliang Zheng",
                "user": "2toINF",
                "type": "user"
            },
            "summary": "Successful generalist Vision-Language-Action (VLA) models rely on effective\ntraining across diverse robotic platforms with large-scale, cross-embodiment,\nheterogeneous datasets. To facilitate and leverage the heterogeneity in rich,\ndiverse robotic data sources, we propose a novel Soft Prompt approach with\nminimally added parameters, by infusing prompt learning concepts into\ncross-embodiment robot learning and introducing separate sets of learnable\nembeddings for each distinct data source. These embeddings serve as\nembodiment-specific prompts, which in unity empower VLA models with effective\nexploitation of varying cross-embodiment features. Our new X-VLA, a neat\nflow-matching-based VLA architecture, relies exclusively on soft-prompted\nstandard Transformer encoders, enjoying both scalability and simplicity.\nEvaluated across 6 simulations as well as 3 real-world robots, our 0.9B\ninstantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep\nof benchmarks, demonstrating superior results on a wide axes of capabilities,\nfrom flexible dexterity to quick adaptation across embodiments, environments,\nand tasks. Website: https://thu-air-dream.github.io/X-VLA/",
            "upvotes": 9,
            "discussionId": "68f0669336f8b025381e19de",
            "projectPage": "https://thu-air-dream.github.io/X-VLA/",
            "githubRepo": "https://github.com/2toinf/X-VLA.git",
            "ai_summary": "A novel Soft Prompt approach enhances Vision-Language-Action models by using learnable embeddings for diverse robotic data, enabling superior performance across simulations and real-world robots.",
            "ai_keywords": [
                "Soft Prompt",
                "prompt learning",
                "cross-embodiment",
                "learnable embeddings",
                "embodiment-specific prompts",
                "flow-matching-based",
                "Transformer encoders",
                "X-VLA",
                "VLA architecture",
                "flexible dexterity",
                "quick adaptation"
            ],
            "githubStars": 32
        },
        "publishedAt": "2025-10-11T12:20:17.000Z",
        "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment\n  Vision-Language-Action Model",
        "summary": "Successful generalist Vision-Language-Action (VLA) models rely on effective\ntraining across diverse robotic platforms with large-scale, cross-embodiment,\nheterogeneous datasets. To facilitate and leverage the heterogeneity in rich,\ndiverse robotic data sources, we propose a novel Soft Prompt approach with\nminimally added parameters, by infusing prompt learning concepts into\ncross-embodiment robot learning and introducing separate sets of learnable\nembeddings for each distinct data source. These embeddings serve as\nembodiment-specific prompts, which in unity empower VLA models with effective\nexploitation of varying cross-embodiment features. Our new X-VLA, a neat\nflow-matching-based VLA architecture, relies exclusively on soft-prompted\nstandard Transformer encoders, enjoying both scalability and simplicity.\nEvaluated across 6 simulations as well as 3 real-world robots, our 0.9B\ninstantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep\nof benchmarks, demonstrating superior results on a wide axes of capabilities,\nfrom flexible dexterity to quick adaptation across embodiments, environments,\nand tasks. Website: https://thu-air-dream.github.io/X-VLA/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10274.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c0afc06b2f05ae642e1918",
            "avatarUrl": "/avatars/70f9a87d123ba65a5f931db028bb095b.svg",
            "fullname": "Jinliang Zheng",
            "name": "2toINF",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.10977",
            "authors": [
                {
                    "_id": "68ef0dce486b78128f0e342f",
                    "user": {
                        "_id": "6621cea88850e38ffbb1854f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621cea88850e38ffbb1854f/LeytEEjSwnnqB-zFN1Tgt.jpeg",
                        "isPro": false,
                        "fullname": "Taki WU",
                        "user": "taki555",
                        "type": "user"
                    },
                    "name": "Taiqiang Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:41:49.305Z",
                    "hidden": false
                },
                {
                    "_id": "68ef0dce486b78128f0e3430",
                    "name": "Runming Yang",
                    "hidden": false
                },
                {
                    "_id": "68ef0dce486b78128f0e3431",
                    "name": "Tao Liu",
                    "hidden": false
                },
                {
                    "_id": "68ef0dce486b78128f0e3432",
                    "name": "Jiahao Wang",
                    "hidden": false
                },
                {
                    "_id": "68ef0dce486b78128f0e3433",
                    "name": "Ngai Wong",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6621cea88850e38ffbb1854f/PzaCBVbRjJxzByjU92Cma.png"
            ],
            "publishedAt": "2025-10-13T03:30:01.000Z",
            "submittedOnDailyAt": "2025-10-16T00:46:01.871Z",
            "title": "Revisiting Model Interpolation for Efficient Reasoning",
            "submittedOnDailyBy": {
                "_id": "6621cea88850e38ffbb1854f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621cea88850e38ffbb1854f/LeytEEjSwnnqB-zFN1Tgt.jpeg",
                "isPro": false,
                "fullname": "Taki WU",
                "user": "taki555",
                "type": "user"
            },
            "summary": "Model merging, typically on Instruct and Thinking models, has shown\nremarkable performance for efficient reasoning. In this paper, we\nsystematically revisit the simplest merging method that interpolates two\nweights directly. Particularly, we observe that model interpolation follows a\nthree-stage evolutionary paradigm with distinct behaviors on the reasoning\ntrajectory. These dynamics provide a principled guide for navigating the\nperformance-cost trade-off. Empirical results demonstrate that a strategically\ninterpolated model surprisingly surpasses sophisticated model merging baselines\non both efficiency and effectiveness. We further validate our findings with\nextensive ablation studies on model layers, modules, and decoding strategies.\nUltimately, this work demystifies model interpolation and offers a practical\nframework for crafting models with precisely targeted reasoning capabilities.\nCode is available at https://github.com/wutaiqiang/MI{Github}.",
            "upvotes": 8,
            "discussionId": "68ef0dcf486b78128f0e3434",
            "githubRepo": "https://github.com/wutaiqiang/MI",
            "githubStars": 2,
            "organization": {
                "_id": "67ea9ecfc234715db8dbf339",
                "name": "hkuhk",
                "fullname": "The University of Hong Kong",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
            }
        },
        "publishedAt": "2025-10-12T23:30:01.000Z",
        "title": "Revisiting Model Interpolation for Efficient Reasoning",
        "summary": "Model merging, typically on Instruct and Thinking models, has shown\nremarkable performance for efficient reasoning. In this paper, we\nsystematically revisit the simplest merging method that interpolates two\nweights directly. Particularly, we observe that model interpolation follows a\nthree-stage evolutionary paradigm with distinct behaviors on the reasoning\ntrajectory. These dynamics provide a principled guide for navigating the\nperformance-cost trade-off. Empirical results demonstrate that a strategically\ninterpolated model surprisingly surpasses sophisticated model merging baselines\non both efficiency and effectiveness. We further validate our findings with\nextensive ablation studies on model layers, modules, and decoding strategies.\nUltimately, this work demystifies model interpolation and offers a practical\nframework for crafting models with precisely targeted reasoning capabilities.\nCode is available at https://github.com/wutaiqiang/MI{Github}.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6621cea88850e38ffbb1854f/PzaCBVbRjJxzByjU92Cma.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10977.png",
        "numComments": 6,
        "submittedBy": {
            "_id": "6621cea88850e38ffbb1854f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621cea88850e38ffbb1854f/LeytEEjSwnnqB-zFN1Tgt.jpeg",
            "fullname": "Taki WU",
            "name": "taki555",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "67ea9ecfc234715db8dbf339",
            "name": "hkuhk",
            "fullname": "The University of Hong Kong",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.10921",
            "authors": [
                {
                    "_id": "68edbbe5de1fee572713a84e",
                    "name": "Chunyu Xie",
                    "hidden": false
                },
                {
                    "_id": "68edbbe5de1fee572713a84f",
                    "name": "Bin Wang",
                    "hidden": false
                },
                {
                    "_id": "68edbbe5de1fee572713a850",
                    "name": "Fanjing Kong",
                    "hidden": false
                },
                {
                    "_id": "68edbbe5de1fee572713a851",
                    "name": "Jincheng Li",
                    "hidden": false
                },
                {
                    "_id": "68edbbe5de1fee572713a852",
                    "name": "Dawei Liang",
                    "hidden": false
                },
                {
                    "_id": "68edbbe5de1fee572713a853",
                    "name": "Ji Ao",
                    "hidden": false
                },
                {
                    "_id": "68edbbe5de1fee572713a854",
                    "name": "Dawei Leng",
                    "hidden": false
                },
                {
                    "_id": "68edbbe5de1fee572713a855",
                    "name": "Yuhui Yin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T02:32:07.000Z",
            "submittedOnDailyAt": "2025-10-16T01:57:29.152Z",
            "title": "FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model",
            "submittedOnDailyBy": {
                "_id": "649935abbe8fd92c27ab1ed8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
                "isPro": false,
                "fullname": "David Leon",
                "user": "DavidLeon",
                "type": "user"
            },
            "summary": "Fine-grained vision-language understanding requires precise alignment between\nvisual content and linguistic descriptions, a capability that remains limited\nin current models, particularly in non-English settings. While models like CLIP\nperform well on global alignment, they often struggle to capture fine-grained\ndetails in object attributes, spatial relations, and linguistic expressions,\nwith limited support for bilingual comprehension. To address these challenges,\nwe introduce FG-CLIP 2, a bilingual vision-language model designed to advance\nfine-grained alignment for both English and Chinese. Our approach leverages\nrich fine-grained supervision, including region-text matching and long-caption\nmodeling, alongside multiple discriminative objectives. We further introduce\nthe Textual Intra-modal Contrastive (TIC) loss to better distinguish\nsemantically similar captions. Trained on a carefully curated mixture of\nlarge-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual\nperformance. To enable rigorous evaluation, we present a new benchmark for\nChinese multimodal understanding, featuring long-caption retrieval and bounding\nbox classification. Extensive experiments on 29 datasets across 8 tasks show\nthat FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results\nin both languages. We release the model, code, and benchmark to facilitate\nfuture research on bilingual fine-grained alignment.",
            "upvotes": 8,
            "discussionId": "68edbbe5de1fee572713a856",
            "projectPage": "https://360cvgroup.github.io/FG-CLIP/",
            "ai_summary": "FG-CLIP 2, a bilingual vision-language model, enhances fine-grained alignment for English and Chinese through rich supervision and a new TIC loss, achieving state-of-the-art performance across multiple datasets and tasks.",
            "ai_keywords": [
                "CLIP",
                "FG-CLIP 2",
                "fine-grained supervision",
                "region-text matching",
                "long-caption modeling",
                "multiple discriminative objectives",
                "Textual Intra-modal Contrastive (TIC) loss",
                "bilingual performance",
                "long-caption retrieval",
                "bounding box classification"
            ]
        },
        "publishedAt": "2025-10-12T22:32:07.000Z",
        "title": "FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model",
        "summary": "Fine-grained vision-language understanding requires precise alignment between\nvisual content and linguistic descriptions, a capability that remains limited\nin current models, particularly in non-English settings. While models like CLIP\nperform well on global alignment, they often struggle to capture fine-grained\ndetails in object attributes, spatial relations, and linguistic expressions,\nwith limited support for bilingual comprehension. To address these challenges,\nwe introduce FG-CLIP 2, a bilingual vision-language model designed to advance\nfine-grained alignment for both English and Chinese. Our approach leverages\nrich fine-grained supervision, including region-text matching and long-caption\nmodeling, alongside multiple discriminative objectives. We further introduce\nthe Textual Intra-modal Contrastive (TIC) loss to better distinguish\nsemantically similar captions. Trained on a carefully curated mixture of\nlarge-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual\nperformance. To enable rigorous evaluation, we present a new benchmark for\nChinese multimodal understanding, featuring long-caption retrieval and bounding\nbox classification. Extensive experiments on 29 datasets across 8 tasks show\nthat FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results\nin both languages. We release the model, code, and benchmark to facilitate\nfuture research on bilingual fine-grained alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10921.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649935abbe8fd92c27ab1ed8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
            "fullname": "David Leon",
            "name": "DavidLeon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11958",
            "authors": [
                {
                    "_id": "68f0539136f8b025381e1915",
                    "name": "Xuan Luo",
                    "hidden": false
                },
                {
                    "_id": "68f0539136f8b025381e1916",
                    "user": {
                        "_id": "63d34004b734eaa4d4faeccf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
                        "isPro": false,
                        "fullname": "Weizhi Wang",
                        "user": "weizhiwang",
                        "type": "user"
                    },
                    "name": "Weizhi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:38:58.035Z",
                    "hidden": false
                },
                {
                    "_id": "68f0539136f8b025381e1917",
                    "name": "Xifeng Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T21:42:37.000Z",
            "submittedOnDailyAt": "2025-10-16T00:39:07.024Z",
            "title": "Direct Multi-Token Decoding",
            "submittedOnDailyBy": {
                "_id": "63d34004b734eaa4d4faeccf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
                "isPro": false,
                "fullname": "Weizhi Wang",
                "user": "weizhiwang",
                "type": "user"
            },
            "summary": "Decoder-only transformers have become the standard architecture for large\nlanguage models (LLMs) due to their strong performance. Recent studies suggest\nthat, in pre-trained LLMs, early, middle, and late layers may serve distinct\nroles: Early layers focus on understanding the input context, middle layers\nhandle task-specific processing, and late layers convert abstract\nrepresentations into output tokens. We hypothesize that once representations\nhave been processed by the early and middle layers, the resulting hidden states\nmay encapsulate sufficient information to support the generation of multiple\ntokens using only the late layers, eliminating the need to repeatedly traverse\nthe early and middle layers. We refer to this inference paradigm as Direct\nMulti-Token Decoding (DMTD). Unlike speculative decoding, our method introduces\nno additional parameters, auxiliary routines, or post-generation verification.\nDespite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model\nhas already demonstrated promising results, achieving up to a 2x speedup with\nonly minor performance loss. Moreover, as shown in our scaling analysis, its\nperformance is expected to further improve with larger training datasets.",
            "upvotes": 5,
            "discussionId": "68f0539136f8b025381e1918",
            "ai_summary": "Direct Multi-Token Decoding (DMTD) accelerates large language model inference by using only late layers for token generation, achieving significant speedup with minimal performance loss.",
            "ai_keywords": [
                "decoder-only transformers",
                "large language models (LLMs)",
                "early layers",
                "middle layers",
                "late layers",
                "hidden states",
                "Direct Multi-Token Decoding (DMTD)",
                "speculative decoding",
                "fine-tuned",
                "Qwen3-4B",
                "scaling analysis"
            ]
        },
        "publishedAt": "2025-10-13T17:42:37.000Z",
        "title": "Direct Multi-Token Decoding",
        "summary": "Decoder-only transformers have become the standard architecture for large\nlanguage models (LLMs) due to their strong performance. Recent studies suggest\nthat, in pre-trained LLMs, early, middle, and late layers may serve distinct\nroles: Early layers focus on understanding the input context, middle layers\nhandle task-specific processing, and late layers convert abstract\nrepresentations into output tokens. We hypothesize that once representations\nhave been processed by the early and middle layers, the resulting hidden states\nmay encapsulate sufficient information to support the generation of multiple\ntokens using only the late layers, eliminating the need to repeatedly traverse\nthe early and middle layers. We refer to this inference paradigm as Direct\nMulti-Token Decoding (DMTD). Unlike speculative decoding, our method introduces\nno additional parameters, auxiliary routines, or post-generation verification.\nDespite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model\nhas already demonstrated promising results, achieving up to a 2x speedup with\nonly minor performance loss. Moreover, as shown in our scaling analysis, its\nperformance is expected to further improve with larger training datasets.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11958.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63d34004b734eaa4d4faeccf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
            "fullname": "Weizhi Wang",
            "name": "weizhiwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.13744",
            "authors": [
                {
                    "_id": "68f0495d36f8b025381e18b7",
                    "name": "Shrey Pandit",
                    "hidden": false
                },
                {
                    "_id": "68f0495d36f8b025381e18b8",
                    "name": "Austin Xu",
                    "hidden": false
                },
                {
                    "_id": "68f0495d36f8b025381e18b9",
                    "name": "Xuan-Phi Nguyen",
                    "hidden": false
                },
                {
                    "_id": "68f0495d36f8b025381e18ba",
                    "name": "Yifei Ming",
                    "hidden": false
                },
                {
                    "_id": "68f0495d36f8b025381e18bb",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "68f0495d36f8b025381e18bc",
                    "name": "Shafiq Joty",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T16:50:54.000Z",
            "submittedOnDailyAt": "2025-10-16T00:20:50.470Z",
            "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier\n  Math",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Large language model (LLM)-based reasoning systems have recently achieved\ngold medal-level performance in the IMO 2025 competition, writing mathematical\nproofs where, to receive full credit, each step must be not only correct but\nalso sufficiently supported. To train LLM-based reasoners in such challenging,\nopen-ended settings, strong verifiers capable of catching step-level mistakes\nare necessary prerequisites. We introduce Hard2Verify, a human-annotated,\nstep-level verification benchmark produced with over 500 hours of human labor.\nHard2Verify is designed to rigorously assess step-level verifiers at the\nfrontier: Verifiers must provide step-level annotations or identify the first\nerror in responses generated by frontier LLMs for very recent, challenging, and\nopen-ended math questions. We evaluate 29 generative critics and process reward\nmodels, demonstrating that, beyond a few standouts, open-source verifiers lag\nclosed source models. We subsequently analyze what drives poor performance in\nstep-level verification, the impacts of scaling verifier compute, as well as\nfundamental questions such as self-verification and verification-generation\ndynamics.",
            "upvotes": 4,
            "discussionId": "68f0495d36f8b025381e18bd",
            "githubRepo": "https://github.com/SalesforceAIResearch/Hard2Verify",
            "ai_summary": "Hard2Verify, a human-annotated benchmark, evaluates step-level verifiers for LLM-based mathematical reasoning systems, highlighting the challenges and performance gaps between open-source and closed-source models.",
            "ai_keywords": [
                "Large language model",
                "LLM-based reasoning systems",
                "IMO 2025",
                "mathematical proofs",
                "step-level verification",
                "Hard2Verify",
                "generative critics",
                "process reward models",
                "self-verification",
                "verification-generation dynamics"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "5f6d64475e78cc6b0ed31e4c",
                "name": "Salesforce",
                "fullname": "Salesforce",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
            }
        },
        "publishedAt": "2025-10-15T12:50:54.000Z",
        "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier\n  Math",
        "summary": "Large language model (LLM)-based reasoning systems have recently achieved\ngold medal-level performance in the IMO 2025 competition, writing mathematical\nproofs where, to receive full credit, each step must be not only correct but\nalso sufficiently supported. To train LLM-based reasoners in such challenging,\nopen-ended settings, strong verifiers capable of catching step-level mistakes\nare necessary prerequisites. We introduce Hard2Verify, a human-annotated,\nstep-level verification benchmark produced with over 500 hours of human labor.\nHard2Verify is designed to rigorously assess step-level verifiers at the\nfrontier: Verifiers must provide step-level annotations or identify the first\nerror in responses generated by frontier LLMs for very recent, challenging, and\nopen-ended math questions. We evaluate 29 generative critics and process reward\nmodels, demonstrating that, beyond a few standouts, open-source verifiers lag\nclosed source models. We subsequently analyze what drives poor performance in\nstep-level verification, the impacts of scaling verifier compute, as well as\nfundamental questions such as self-verification and verification-generation\ndynamics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13744.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 127
        },
        "organization": {
            "_id": "5f6d64475e78cc6b0ed31e4c",
            "name": "Salesforce",
            "fullname": "Salesforce",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13602",
            "authors": [
                {
                    "_id": "68f05b2436f8b025381e198f",
                    "user": {
                        "_id": "65394f942ddab2beff1cc967",
                        "avatarUrl": "/avatars/bdd5b47618758f6de080a51834a17655.svg",
                        "isPro": false,
                        "fullname": "Shawn Huang",
                        "user": "hyx21",
                        "type": "user"
                    },
                    "name": "Yuxiang Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T07:12:24.514Z",
                    "hidden": false
                },
                {
                    "_id": "68f05b2436f8b025381e1990",
                    "name": "Chaojun Xiao",
                    "hidden": false
                },
                {
                    "_id": "68f05b2436f8b025381e1991",
                    "name": "Xu Han",
                    "hidden": false
                },
                {
                    "_id": "68f05b2436f8b025381e1992",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T14:33:16.000Z",
            "submittedOnDailyAt": "2025-10-16T01:11:14.938Z",
            "title": "NOSA: Native and Offloadable Sparse Attention",
            "submittedOnDailyBy": {
                "_id": "65394f942ddab2beff1cc967",
                "avatarUrl": "/avatars/bdd5b47618758f6de080a51834a17655.svg",
                "isPro": false,
                "fullname": "Shawn Huang",
                "user": "hyx21",
                "type": "user"
            },
            "summary": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2).",
            "upvotes": 4,
            "discussionId": "68f05b2436f8b025381e1993",
            "ai_summary": "NOSA, a trainable sparse attention framework, enhances decoding throughput by enabling efficient KV cache offloading without compromising performance.",
            "ai_keywords": [
                "sparse attention",
                "key-value cache",
                "token selection",
                "query-aware",
                "query-agnostic",
                "NOSA",
                "decoding throughput",
                "pretrain",
                "InfLLM-V2"
            ]
        },
        "publishedAt": "2025-10-15T10:33:16.000Z",
        "title": "NOSA: Native and Offloadable Sparse Attention",
        "summary": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13602.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65394f942ddab2beff1cc967",
            "avatarUrl": "/avatars/bdd5b47618758f6de080a51834a17655.svg",
            "fullname": "Shawn Huang",
            "name": "hyx21",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.12560",
            "authors": [
                {
                    "_id": "68ef1c02486b78128f0e348c",
                    "user": {
                        "_id": "655601f1ae085c2ba7a22b95",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4UmxFrc_TEiXcnm3RewZM.jpeg",
                        "isPro": false,
                        "fullname": "Xiaoji Zheng",
                        "user": "Student-Xiaoji",
                        "type": "user"
                    },
                    "name": "Xiaoji Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T07:08:32.045Z",
                    "hidden": false
                },
                {
                    "_id": "68ef1c02486b78128f0e348d",
                    "name": "Ziyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "68ef1c02486b78128f0e348e",
                    "name": "Yanhao Chen",
                    "hidden": false
                },
                {
                    "_id": "68ef1c02486b78128f0e348f",
                    "name": "Yuhang Peng",
                    "hidden": false
                },
                {
                    "_id": "68ef1c02486b78128f0e3490",
                    "name": "Yuanrong Tang",
                    "hidden": false
                },
                {
                    "_id": "68ef1c02486b78128f0e3491",
                    "name": "Gengyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68ef1c02486b78128f0e3492",
                    "name": "Bokui Chen",
                    "hidden": false
                },
                {
                    "_id": "68ef1c02486b78128f0e3493",
                    "name": "Jiangtao Gong",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/655601f1ae085c2ba7a22b95/6S7ap5ao7OKmqif62lOAh.png"
            ],
            "publishedAt": "2025-10-14T14:21:52.000Z",
            "submittedOnDailyAt": "2025-10-16T00:53:23.602Z",
            "title": "CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in\n  Latent World Models for Autonomous Driving",
            "submittedOnDailyBy": {
                "_id": "655601f1ae085c2ba7a22b95",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4UmxFrc_TEiXcnm3RewZM.jpeg",
                "isPro": false,
                "fullname": "Xiaoji Zheng",
                "user": "Student-Xiaoji",
                "type": "user"
            },
            "summary": "End-to-end autonomous driving models trained solely with imitation learning\n(IL) often suffer from poor generalization. In contrast, reinforcement learning\n(RL) promotes exploration through reward maximization but faces challenges such\nas sample inefficiency and unstable convergence. A natural solution is to\ncombine IL and RL. Moving beyond the conventional two-stage paradigm (IL\npretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive\ndual-policy framework that enables IL and RL agents to interact during\ntraining. CoIRL-AD introduces a competition-based mechanism that facilitates\nknowledge exchange while preventing gradient conflicts. Experiments on the\nnuScenes dataset show an 18% reduction in collision rate compared to baselines,\nalong with stronger generalization and improved performance on long-tail\nscenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.",
            "upvotes": 4,
            "discussionId": "68ef1c02486b78128f0e3494",
            "projectPage": "https://seu-zxj.github.io/CoIRL-AD/",
            "githubRepo": "https://github.com/SEU-zxj/CoIRL-AD",
            "githubStars": 10,
            "organization": {
                "_id": "628735cbc83a2d6ab8d14a66",
                "name": "Tsinghua",
                "fullname": "Tsinghua University"
            }
        },
        "publishedAt": "2025-10-14T10:21:52.000Z",
        "title": "CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in\n  Latent World Models for Autonomous Driving",
        "summary": "End-to-end autonomous driving models trained solely with imitation learning\n(IL) often suffer from poor generalization. In contrast, reinforcement learning\n(RL) promotes exploration through reward maximization but faces challenges such\nas sample inefficiency and unstable convergence. A natural solution is to\ncombine IL and RL. Moving beyond the conventional two-stage paradigm (IL\npretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive\ndual-policy framework that enables IL and RL agents to interact during\ntraining. CoIRL-AD introduces a competition-based mechanism that facilitates\nknowledge exchange while preventing gradient conflicts. Experiments on the\nnuScenes dataset show an 18% reduction in collision rate compared to baselines,\nalong with stronger generalization and improved performance on long-tail\nscenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/655601f1ae085c2ba7a22b95/6S7ap5ao7OKmqif62lOAh.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12560.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655601f1ae085c2ba7a22b95",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4UmxFrc_TEiXcnm3RewZM.jpeg",
            "fullname": "Xiaoji Zheng",
            "name": "Student-Xiaoji",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "628735cbc83a2d6ab8d14a66",
            "name": "Tsinghua",
            "fullname": "Tsinghua University"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.10611",
            "authors": [
                {
                    "_id": "68ef4bbb486b78128f0e35e1",
                    "name": "Heng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ef4bbb486b78128f0e35e2",
                    "user": {
                        "_id": "645b0c3ec35da9c7afd95421",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
                        "isPro": false,
                        "fullname": "Yuling",
                        "user": "YerbaPage",
                        "type": "user"
                    },
                    "name": "Yuling Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T15:24:40.752Z",
                    "hidden": false
                },
                {
                    "_id": "68ef4bbb486b78128f0e35e3",
                    "name": "Xiaodong Gu",
                    "hidden": false
                },
                {
                    "_id": "68ef4bbb486b78128f0e35e4",
                    "name": "Zijian Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ef4bbb486b78128f0e35e5",
                    "name": "Haochen You",
                    "hidden": false
                },
                {
                    "_id": "68ef4bbb486b78128f0e35e6",
                    "name": "Lubin Gan",
                    "hidden": false
                },
                {
                    "_id": "68ef4bbb486b78128f0e35e7",
                    "name": "Yilei Yuan",
                    "hidden": false
                },
                {
                    "_id": "68ef4bbb486b78128f0e35e8",
                    "name": "Jin Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-12T13:47:42.000Z",
            "submittedOnDailyAt": "2025-10-16T00:46:55.714Z",
            "title": "HyperAgent: Leveraging Hypergraphs for Topology Optimization in\n  Multi-Agent Communication",
            "submittedOnDailyBy": {
                "_id": "645b0c3ec35da9c7afd95421",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
                "isPro": false,
                "fullname": "Yuling",
                "user": "YerbaPage",
                "type": "user"
            },
            "summary": "Recent advances in large language model-powered multi-agent systems have\ndemonstrated remarkable collective intelligence through effective\ncommunication. However, existing approaches face two primary challenges: (i)\nIneffective group collaboration modeling, as they rely on pairwise\nedge representations in graph structures, limiting their ability to capture\nrelationships among multiple agents; and (ii) Limited task-adaptiveness\nin communication topology design, leading to excessive communication cost for\nsimple tasks and insufficient coordination for complex scenarios. These issues\nrestrict the scalability and practical deployment of adaptive collaboration\nframeworks. To address these challenges, we propose HyperAgent, a\nhypergraph-based framework that optimizes communication topologies and\neffectively captures group collaboration patterns using direct hyperedge\nrepresentations. Unlike edge-based approaches, HyperAgent uses hyperedges to\nlink multiple agents within the same subtask and employs hypergraph\nconvolutional layers to achieve one-step information aggregation in\ncollaboration groups. Additionally, it incorporates a variational autoencoder\nframework with sparsity regularization to dynamically adjust hypergraph\ntopologies based on task complexity. Experiments highlight the superiority of\nHyperAgent in both performance and efficiency. For instance, on GSM8K,\nHyperAgent achieves 95.07\\% accuracy while reducing token consumption by\n25.33\\%, demonstrating the potential of hypergraph-based optimization for\nmulti-agent communication.",
            "upvotes": 4,
            "discussionId": "68ef4bbc486b78128f0e35e9",
            "ai_summary": "HyperAgent, a hypergraph-based framework, optimizes communication topologies and captures group collaboration patterns, improving performance and efficiency in multi-agent systems.",
            "ai_keywords": [
                "hypergraph-based framework",
                "hyperedges",
                "hypergraph convolutional layers",
                "variational autoencoder",
                "sparsity regularization",
                "GSM8K",
                "token consumption"
            ]
        },
        "publishedAt": "2025-10-12T09:47:42.000Z",
        "title": "HyperAgent: Leveraging Hypergraphs for Topology Optimization in\n  Multi-Agent Communication",
        "summary": "Recent advances in large language model-powered multi-agent systems have\ndemonstrated remarkable collective intelligence through effective\ncommunication. However, existing approaches face two primary challenges: (i)\nIneffective group collaboration modeling, as they rely on pairwise\nedge representations in graph structures, limiting their ability to capture\nrelationships among multiple agents; and (ii) Limited task-adaptiveness\nin communication topology design, leading to excessive communication cost for\nsimple tasks and insufficient coordination for complex scenarios. These issues\nrestrict the scalability and practical deployment of adaptive collaboration\nframeworks. To address these challenges, we propose HyperAgent, a\nhypergraph-based framework that optimizes communication topologies and\neffectively captures group collaboration patterns using direct hyperedge\nrepresentations. Unlike edge-based approaches, HyperAgent uses hyperedges to\nlink multiple agents within the same subtask and employs hypergraph\nconvolutional layers to achieve one-step information aggregation in\ncollaboration groups. Additionally, it incorporates a variational autoencoder\nframework with sparsity regularization to dynamically adjust hypergraph\ntopologies based on task complexity. Experiments highlight the superiority of\nHyperAgent in both performance and efficiency. For instance, on GSM8K,\nHyperAgent achieves 95.07\\% accuracy while reducing token consumption by\n25.33\\%, demonstrating the potential of hypergraph-based optimization for\nmulti-agent communication.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10611.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645b0c3ec35da9c7afd95421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
            "fullname": "Yuling",
            "name": "YerbaPage",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 190
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.12866",
            "authors": [
                {
                    "_id": "68f127eadf97aa3a74268631",
                    "name": "Dantong Niu",
                    "hidden": false
                },
                {
                    "_id": "68f127eadf97aa3a74268632",
                    "name": "Yuvan Sharma",
                    "hidden": false
                },
                {
                    "_id": "68f127eadf97aa3a74268633",
                    "name": "Baifeng Shi",
                    "hidden": false
                },
                {
                    "_id": "68f127eadf97aa3a74268634",
                    "name": "Rachel Ding",
                    "hidden": false
                },
                {
                    "_id": "68f127eadf97aa3a74268635",
                    "name": "Matteo Gioia",
                    "hidden": false
                },
                {
                    "_id": "68f127eadf97aa3a74268636",
                    "name": "Haoru Xue",
                    "hidden": false
                },
                {
                    "_id": "68f127eadf97aa3a74268637",
                    "name": "Henry Tsai",
                    "hidden": false
                },
                {
                    "_id": "68f127eadf97aa3a74268638",
                    "name": "Konstantinos Kallidromitis",
                    "hidden": false
                },
                {
                    "_id": "68f127eadf97aa3a74268639",
                    "name": "Anirudh Pai",
                    "hidden": false
                },
                {
                    "_id": "68f127eadf97aa3a7426863a",
                    "name": "Shankar Shastry",
                    "hidden": false
                },
                {
                    "_id": "68f127eadf97aa3a7426863b",
                    "name": "Trevor Darrell",
                    "hidden": false
                },
                {
                    "_id": "68f127eadf97aa3a7426863c",
                    "name": "Jitendra Malik",
                    "hidden": false
                },
                {
                    "_id": "68f127eadf97aa3a7426863d",
                    "name": "Roei Herzig",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T17:56:10.000Z",
            "submittedOnDailyAt": "2025-10-16T15:45:25.761Z",
            "title": "Learning to Grasp Anything by Playing with Random Toys",
            "submittedOnDailyBy": {
                "_id": "667c5764186b27ef806636d3",
                "avatarUrl": "/avatars/5c08f0109bc0e350624112c0aff544f6.svg",
                "isPro": false,
                "fullname": "Roei Herzig",
                "user": "roeiherz",
                "type": "user"
            },
            "summary": "Robotic manipulation policies often struggle to generalize to novel objects,\nlimiting their real-world utility. In contrast, cognitive science suggests that\nchildren develop generalizable dexterous manipulation skills by mastering a\nsmall set of simple toys and then applying that knowledge to more complex\nitems. Inspired by this, we study if similar generalization capabilities can\nalso be achieved by robots. Our results indicate robots can learn generalizable\ngrasping using randomly assembled objects that are composed from just four\nshape primitives: spheres, cuboids, cylinders, and rings. We show that training\non these \"toys\" enables robust generalization to real-world objects, yielding\nstrong zero-shot performance. Crucially, we find the key to this generalization\nis an object-centric visual representation induced by our proposed detection\npooling mechanism. Evaluated in both simulation and on physical robots, our\nmodel achieves a 67% real-world grasping success rate on the YCB dataset,\noutperforming state-of-the-art approaches that rely on substantially more\nin-domain data. We further study how zero-shot generalization performance\nscales by varying the number and diversity of training toys and the\ndemonstrations per toy. We believe this work offers a promising path to\nscalable and generalizable learning in robotic manipulation. Demonstration\nvideos, code, checkpoints and our dataset are available on our project page:\nhttps://lego-grasp.github.io/ .",
            "upvotes": 3,
            "discussionId": "68f127ebdf97aa3a7426863e",
            "ai_summary": "Robots can achieve generalizable grasping skills by learning from a small set of simple objects, using an object-centric visual representation, which outperforms state-of-the-art methods with less data.",
            "ai_keywords": [
                "object-centric visual representation",
                "detection pooling mechanism",
                "zero-shot performance",
                "YCB dataset",
                "robotic manipulation",
                "generalizable grasping"
            ],
            "organization": {
                "_id": "61f20a9ce108f2cba2dc0730",
                "name": "Berkeley",
                "fullname": "UC Berkeley",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"
            }
        },
        "publishedAt": "2025-10-14T13:56:10.000Z",
        "title": "Learning to Grasp Anything by Playing with Random Toys",
        "summary": "Robotic manipulation policies often struggle to generalize to novel objects,\nlimiting their real-world utility. In contrast, cognitive science suggests that\nchildren develop generalizable dexterous manipulation skills by mastering a\nsmall set of simple toys and then applying that knowledge to more complex\nitems. Inspired by this, we study if similar generalization capabilities can\nalso be achieved by robots. Our results indicate robots can learn generalizable\ngrasping using randomly assembled objects that are composed from just four\nshape primitives: spheres, cuboids, cylinders, and rings. We show that training\non these \"toys\" enables robust generalization to real-world objects, yielding\nstrong zero-shot performance. Crucially, we find the key to this generalization\nis an object-centric visual representation induced by our proposed detection\npooling mechanism. Evaluated in both simulation and on physical robots, our\nmodel achieves a 67% real-world grasping success rate on the YCB dataset,\noutperforming state-of-the-art approaches that rely on substantially more\nin-domain data. We further study how zero-shot generalization performance\nscales by varying the number and diversity of training toys and the\ndemonstrations per toy. We believe this work offers a promising path to\nscalable and generalizable learning in robotic manipulation. Demonstration\nvideos, code, checkpoints and our dataset are available on our project page:\nhttps://lego-grasp.github.io/ .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12866.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "667c5764186b27ef806636d3",
            "avatarUrl": "/avatars/5c08f0109bc0e350624112c0aff544f6.svg",
            "fullname": "Roei Herzig",
            "name": "roeiherz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "61f20a9ce108f2cba2dc0730",
            "name": "Berkeley",
            "fullname": "UC Berkeley",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13940",
            "authors": [
                {
                    "_id": "68f191746e0bef323a68fbb5",
                    "name": "Zhen Yang",
                    "hidden": false
                },
                {
                    "_id": "68f191746e0bef323a68fbb6",
                    "name": "Mingyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f191746e0bef323a68fbb7",
                    "name": "Feng Chen",
                    "hidden": false
                },
                {
                    "_id": "68f191746e0bef323a68fbb8",
                    "name": "Ganggui Ding",
                    "hidden": false
                },
                {
                    "_id": "68f191746e0bef323a68fbb9",
                    "name": "Liang Hou",
                    "hidden": false
                },
                {
                    "_id": "68f191746e0bef323a68fbba",
                    "name": "Xin Tao",
                    "hidden": false
                },
                {
                    "_id": "68f191746e0bef323a68fbbb",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "68f191746e0bef323a68fbbc",
                    "name": "Ying-Cong Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T17:59:45.000Z",
            "submittedOnDailyAt": "2025-10-16T23:18:36.686Z",
            "title": "Less is More: Improving LLM Reasoning with Minimal Test-Time\n  Intervention",
            "submittedOnDailyBy": {
                "_id": "63f58403fcf95ecac2b33d78",
                "avatarUrl": "/avatars/a77ea80784896502ae1cfa086a78ce66.svg",
                "isPro": false,
                "fullname": "Zhen Yang",
                "user": "YZCS",
                "type": "user"
            },
            "summary": "Recent progress in large language models (LLMs) has focused on test-time\nscaling to improve reasoning via increased inference computation, but often at\nthe cost of efficiency. We revisit test-time behavior and uncover a simple yet\nunderexplored phenomenon: reasoning uncertainty is highly localized-only a\nsmall subset of high-entropy tokens dominantly affects output correctness.\nMotivated by this, we propose Minimal Test-Time Intervention (MTI), a\ntraining-free framework that enhances reasoning accuracy and stability with\nminimal overhead. MTI includes: (i) Selective CFG intervention, applying\nclassifier-free guidance only at uncertain positions; and (ii) Lightweight\nnegative-prompt guidance, reusing the main model's KV cache to approximate\nunconditional decoding efficiently. MTI yields consistent gains across general,\ncoding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for\nQwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining\nhighly efficient.",
            "upvotes": 2,
            "discussionId": "68f191746e0bef323a68fbbd",
            "githubRepo": "https://github.com/EnVision-Research/MTI",
            "ai_summary": "Minimal Test-Time Intervention (MTI) enhances reasoning accuracy and stability in large language models with minimal overhead by selectively applying classifier-free guidance and lightweight negative-prompt guidance.",
            "ai_keywords": [
                "large language models",
                "test-time scaling",
                "reasoning uncertainty",
                "high-entropy tokens",
                "Minimal Test-Time Intervention",
                "Selective CFG intervention",
                "classifier-free guidance",
                "Lightweight negative-prompt guidance",
                "KV cache",
                "unconditional decoding",
                "Qwen3-8B-Base",
                "Qwen3-32B-Reasoning",
                "AIME2024"
            ],
            "githubStars": 6,
            "organization": {
                "_id": "65ad19cac14c3cf579ad9b68",
                "name": "HKUSTGZ",
                "fullname": "HKUSTGZ"
            }
        },
        "publishedAt": "2025-10-15T13:59:45.000Z",
        "title": "Less is More: Improving LLM Reasoning with Minimal Test-Time\n  Intervention",
        "summary": "Recent progress in large language models (LLMs) has focused on test-time\nscaling to improve reasoning via increased inference computation, but often at\nthe cost of efficiency. We revisit test-time behavior and uncover a simple yet\nunderexplored phenomenon: reasoning uncertainty is highly localized-only a\nsmall subset of high-entropy tokens dominantly affects output correctness.\nMotivated by this, we propose Minimal Test-Time Intervention (MTI), a\ntraining-free framework that enhances reasoning accuracy and stability with\nminimal overhead. MTI includes: (i) Selective CFG intervention, applying\nclassifier-free guidance only at uncertain positions; and (ii) Lightweight\nnegative-prompt guidance, reusing the main model's KV cache to approximate\nunconditional decoding efficiently. MTI yields consistent gains across general,\ncoding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for\nQwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining\nhighly efficient.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13940.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63f58403fcf95ecac2b33d78",
            "avatarUrl": "/avatars/a77ea80784896502ae1cfa086a78ce66.svg",
            "fullname": "Zhen Yang",
            "name": "YZCS",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "65ad19cac14c3cf579ad9b68",
            "name": "HKUSTGZ",
            "fullname": "HKUSTGZ"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.12831",
            "authors": [
                {
                    "_id": "68f0566536f8b025381e1939",
                    "user": {
                        "_id": "60ed74f536ceac2554083559",
                        "avatarUrl": "/avatars/28c184ef76f719a720d933d05afb5800.svg",
                        "isPro": false,
                        "fullname": "taicheng guo",
                        "user": "taicheng",
                        "type": "user"
                    },
                    "name": "Taicheng Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:38:48.648Z",
                    "hidden": false
                },
                {
                    "_id": "68f0566536f8b025381e193a",
                    "name": "Hai Wang",
                    "hidden": false
                },
                {
                    "_id": "68f0566536f8b025381e193b",
                    "name": "ChaoChun Liu",
                    "hidden": false
                },
                {
                    "_id": "68f0566536f8b025381e193c",
                    "name": "Mohsen Golalikhani",
                    "hidden": false
                },
                {
                    "_id": "68f0566536f8b025381e193d",
                    "name": "Xin Chen",
                    "hidden": false
                },
                {
                    "_id": "68f0566536f8b025381e193e",
                    "name": "Xiangliang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f0566536f8b025381e193f",
                    "name": "Chandan K. Reddy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-12T16:12:05.000Z",
            "submittedOnDailyAt": "2025-10-16T00:53:22.241Z",
            "title": "MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic\n  Training",
            "submittedOnDailyBy": {
                "_id": "60ed74f536ceac2554083559",
                "avatarUrl": "/avatars/28c184ef76f719a720d933d05afb5800.svg",
                "isPro": false,
                "fullname": "taicheng guo",
                "user": "taicheng",
                "type": "user"
            },
            "summary": "Multi-turn Text-to-SQL aims to translate a user's conversational utterances\ninto executable SQL while preserving dialogue coherence and grounding to the\ntarget schema. However, most existing systems only regard this task as a simple\ntext translation task and follow a short-horizon paradigm, generating a query\nper turn without execution, explicit verification, and refinement, which leads\nto non-executable or incoherent outputs. We present MTSQL-R1, an agentic\ntraining framework for long-horizon multi-turn Text-to-SQL. We cast the task as\na Markov Decision Process (MDP) in which an agent interacts with (i) a database\nfor execution feedback and (ii) a persistent dialogue memory for coherence\nverification, performing an iterative propose to execute -> verify -> refine\ncycle until all checks pass. Experiments on COSQL and SPARC demonstrate that\nMTSQL-R1 consistently outperforms strong baselines, highlighting the importance\nof environment-driven verification and memory-guided refinement for\nconversational semantic parsing. Full recipes (including code, trained models,\nlogs, reasoning trajectories, etc.) will be released after the internal review\nto contribute to community research.",
            "upvotes": 2,
            "discussionId": "68f0566536f8b025381e1940",
            "githubRepo": "https://github.com/taichengguo/MTSQL-R1",
            "ai_summary": "MTSQL-R1, an agentic training framework, improves multi-turn Text-to-SQL by treating it as an MDP with iterative propose-execute-verify-refine cycles, enhancing coherence and execution.",
            "ai_keywords": [
                "Multi-turn Text-to-SQL",
                "Markov Decision Process (MDP)",
                "dialogue coherence",
                "dialogue memory",
                "iterative propose-execute-verify-refine cycle",
                "conversational semantic parsing"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "5ffdfbadbba2ae614d771970",
                "name": "amazon",
                "fullname": "Amazon",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
            }
        },
        "publishedAt": "2025-10-12T12:12:05.000Z",
        "title": "MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic\n  Training",
        "summary": "Multi-turn Text-to-SQL aims to translate a user's conversational utterances\ninto executable SQL while preserving dialogue coherence and grounding to the\ntarget schema. However, most existing systems only regard this task as a simple\ntext translation task and follow a short-horizon paradigm, generating a query\nper turn without execution, explicit verification, and refinement, which leads\nto non-executable or incoherent outputs. We present MTSQL-R1, an agentic\ntraining framework for long-horizon multi-turn Text-to-SQL. We cast the task as\na Markov Decision Process (MDP) in which an agent interacts with (i) a database\nfor execution feedback and (ii) a persistent dialogue memory for coherence\nverification, performing an iterative propose to execute -> verify -> refine\ncycle until all checks pass. Experiments on COSQL and SPARC demonstrate that\nMTSQL-R1 consistently outperforms strong baselines, highlighting the importance\nof environment-driven verification and memory-guided refinement for\nconversational semantic parsing. Full recipes (including code, trained models,\nlogs, reasoning trajectories, etc.) will be released after the internal review\nto contribute to community research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12831.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60ed74f536ceac2554083559",
            "avatarUrl": "/avatars/28c184ef76f719a720d933d05afb5800.svg",
            "fullname": "taicheng guo",
            "name": "taicheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "organization": {
            "_id": "5ffdfbadbba2ae614d771970",
            "name": "amazon",
            "fullname": "Amazon",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.10581",
            "authors": [
                {
                    "_id": "68efc102d22df134b7f5ca27",
                    "name": "Heng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68efc102d22df134b7f5ca28",
                    "user": {
                        "_id": "645b0c3ec35da9c7afd95421",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
                        "isPro": false,
                        "fullname": "Yuling",
                        "user": "YerbaPage",
                        "type": "user"
                    },
                    "name": "Yuling Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-15T16:55:17.600Z",
                    "hidden": false
                },
                {
                    "_id": "68efc102d22df134b7f5ca29",
                    "name": "Xiaodong Gu",
                    "hidden": false
                },
                {
                    "_id": "68efc102d22df134b7f5ca2a",
                    "name": "Haochen You",
                    "hidden": false
                },
                {
                    "_id": "68efc102d22df134b7f5ca2b",
                    "name": "Zijian Zhang",
                    "hidden": false
                },
                {
                    "_id": "68efc102d22df134b7f5ca2c",
                    "name": "Lubin Gan",
                    "hidden": false
                },
                {
                    "_id": "68efc102d22df134b7f5ca2d",
                    "name": "Yilei Yuan",
                    "hidden": false
                },
                {
                    "_id": "68efc102d22df134b7f5ca2e",
                    "name": "Jin Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-12T12:55:42.000Z",
            "submittedOnDailyAt": "2025-10-16T00:47:00.305Z",
            "title": "GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust\n  Multi-Turn Deep Search",
            "submittedOnDailyBy": {
                "_id": "645b0c3ec35da9c7afd95421",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
                "isPro": false,
                "fullname": "Yuling",
                "user": "YerbaPage",
                "type": "user"
            },
            "summary": "Multi-agent systems powered by Large Language Models excel at complex tasks\nthrough coordinated collaboration, yet they face high failure rates in\nmulti-turn deep search scenarios. Existing temporal attribution methods\nstruggle to accurately diagnose root causes, particularly when errors propagate\nacross multiple agents. Attempts to automate failure attribution by analyzing\naction sequences remain ineffective due to their inability to account for\ninformation dependencies that span agents. This paper identifies two core\nchallenges: (i) distinguishing symptoms from root causes in multi-agent\nerror propagation, and (ii) tracing information dependencies beyond\ntemporal order. To address these issues, we introduce GraphTracer, a\nframework that redefines failure attribution through information flow analysis.\nGraphTracer constructs Information Dependency Graphs (IDGs) to explicitly\ncapture how agents reference and build on prior outputs. It localizes root\ncauses by tracing through these dependency structures instead of relying on\ntemporal sequences. GraphTracer also uses graph-aware synthetic data generation\nto target critical nodes, creating realistic failure scenarios. Evaluations on\nthe Who\\&When benchmark and integration into production systems demonstrate\nthat GraphTracer-8B achieves up to 18.18\\% higher attribution accuracy compared\nto state-of-the-art models and enables 4.8\\% to 14.2\\% performance improvements\nin deployed multi-agent frameworks, establishing a robust solution for\nmulti-agent system debugging.",
            "upvotes": 2,
            "discussionId": "68efc102d22df134b7f5ca2f",
            "ai_summary": "GraphTracer addresses multi-agent system failure attribution by constructing Information Dependency Graphs to trace information flow and improve debugging accuracy.",
            "ai_keywords": [
                "multi-agent systems",
                "Large Language Models",
                "deep search scenarios",
                "temporal attribution",
                "information dependencies",
                "GraphTracer",
                "Information Dependency Graphs",
                "failure attribution",
                "synthetic data generation",
                "Who&When benchmark"
            ]
        },
        "publishedAt": "2025-10-12T08:55:42.000Z",
        "title": "GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust\n  Multi-Turn Deep Search",
        "summary": "Multi-agent systems powered by Large Language Models excel at complex tasks\nthrough coordinated collaboration, yet they face high failure rates in\nmulti-turn deep search scenarios. Existing temporal attribution methods\nstruggle to accurately diagnose root causes, particularly when errors propagate\nacross multiple agents. Attempts to automate failure attribution by analyzing\naction sequences remain ineffective due to their inability to account for\ninformation dependencies that span agents. This paper identifies two core\nchallenges: (i) distinguishing symptoms from root causes in multi-agent\nerror propagation, and (ii) tracing information dependencies beyond\ntemporal order. To address these issues, we introduce GraphTracer, a\nframework that redefines failure attribution through information flow analysis.\nGraphTracer constructs Information Dependency Graphs (IDGs) to explicitly\ncapture how agents reference and build on prior outputs. It localizes root\ncauses by tracing through these dependency structures instead of relying on\ntemporal sequences. GraphTracer also uses graph-aware synthetic data generation\nto target critical nodes, creating realistic failure scenarios. Evaluations on\nthe Who\\&When benchmark and integration into production systems demonstrate\nthat GraphTracer-8B achieves up to 18.18\\% higher attribution accuracy compared\nto state-of-the-art models and enables 4.8\\% to 14.2\\% performance improvements\nin deployed multi-agent frameworks, establishing a robust solution for\nmulti-agent system debugging.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10581.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645b0c3ec35da9c7afd95421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
            "fullname": "Yuling",
            "name": "YerbaPage",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 190
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.13714",
            "authors": [
                {
                    "_id": "68f0e78436f8b025381e1b89",
                    "name": "Dan Jacobellis",
                    "hidden": false
                },
                {
                    "_id": "68f0e78436f8b025381e1b8a",
                    "name": "Mateen Ulhaq",
                    "hidden": false
                },
                {
                    "_id": "68f0e78436f8b025381e1b8b",
                    "name": "Fabien Racap",
                    "hidden": false
                },
                {
                    "_id": "68f0e78436f8b025381e1b8c",
                    "name": "Hyomin Choi",
                    "hidden": false
                },
                {
                    "_id": "68f0e78436f8b025381e1b8d",
                    "name": "Neeraja J. Yadwadkar",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63213080d2d45f3151837eba/J8x_N9A8rBtrKBzaqXlxk.png"
            ],
            "publishedAt": "2025-10-15T16:13:44.000Z",
            "submittedOnDailyAt": "2025-10-16T11:28:45.624Z",
            "title": "Dedelayed: Deleting remote inference delay via on-device correction",
            "submittedOnDailyBy": {
                "_id": "63213080d2d45f3151837eba",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63213080d2d45f3151837eba/aBhKfY-0gZhKGQmb_Gwi2.png",
                "isPro": true,
                "fullname": "Dan Jacobellis",
                "user": "danjacobellis",
                "type": "user"
            },
            "summary": "Remote inference allows lightweight devices to leverage powerful cloud\nmodels. However, communication network latency makes predictions stale and\nunsuitable for real-time tasks. To address this, we introduce Dedelayed, a\ndelay-corrective method that mitigates arbitrary remote inference delays,\nallowing the local device to produce low-latency outputs in real time. Our\nmethod employs a lightweight local model that processes the current frame and\nfuses in features that a heavyweight remote model computes from past frames. On\nvideo from the BDD100K driving dataset, Dedelayed improves semantic\nsegmentation accuracy over the stronger of the local-only and remote-only\nbaselines across all realistic communication network delays beyond 33 ms.\nWithout incurring additional delay, it improves accuracy by 6.4 mIoU compared\nto fully local inference and 9.8 mIoU compared to remote inference, for a\nround-trip delay of 100 ms. The advantage grows under longer delays and\nhigher-motion scenes, as delay-mitigated split inference sustains accuracy more\neffectively, providing clear advantages for real-time tasks that must remain\naligned with the current world state.",
            "upvotes": 1,
            "discussionId": "68f0e78536f8b025381e1b8e",
            "ai_summary": "Dedelayed, a delay-corrective method, improves real-time semantic segmentation accuracy by fusing local and remote model outputs, mitigating communication network latency.",
            "ai_keywords": [
                "remote inference",
                "lightweight devices",
                "cloud models",
                "communication network latency",
                "delay-corrective method",
                "local model",
                "remote model",
                "BDD100K driving dataset",
                "semantic segmentation accuracy",
                "mIoU",
                "round-trip delay",
                "high-motion scenes",
                "split inference"
            ]
        },
        "publishedAt": "2025-10-15T12:13:44.000Z",
        "title": "Dedelayed: Deleting remote inference delay via on-device correction",
        "summary": "Remote inference allows lightweight devices to leverage powerful cloud\nmodels. However, communication network latency makes predictions stale and\nunsuitable for real-time tasks. To address this, we introduce Dedelayed, a\ndelay-corrective method that mitigates arbitrary remote inference delays,\nallowing the local device to produce low-latency outputs in real time. Our\nmethod employs a lightweight local model that processes the current frame and\nfuses in features that a heavyweight remote model computes from past frames. On\nvideo from the BDD100K driving dataset, Dedelayed improves semantic\nsegmentation accuracy over the stronger of the local-only and remote-only\nbaselines across all realistic communication network delays beyond 33 ms.\nWithout incurring additional delay, it improves accuracy by 6.4 mIoU compared\nto fully local inference and 9.8 mIoU compared to remote inference, for a\nround-trip delay of 100 ms. The advantage grows under longer delays and\nhigher-motion scenes, as delay-mitigated split inference sustains accuracy more\neffectively, providing clear advantages for real-time tasks that must remain\naligned with the current world state.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63213080d2d45f3151837eba/J8x_N9A8rBtrKBzaqXlxk.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13714.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63213080d2d45f3151837eba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63213080d2d45f3151837eba/aBhKfY-0gZhKGQmb_Gwi2.png",
            "fullname": "Dan Jacobellis",
            "name": "danjacobellis",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13586",
            "authors": [
                {
                    "_id": "68f04f9036f8b025381e18ec",
                    "user": {
                        "_id": "652a5d23b20d500447f7e80c",
                        "avatarUrl": "/avatars/eb555e48d36829aeaadc0bb97187df81.svg",
                        "isPro": false,
                        "fullname": "Pasin Buakhaw",
                        "user": "PasinB",
                        "type": "user"
                    },
                    "name": "Pasin Buakhaw",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T10:38:33.227Z",
                    "hidden": false
                },
                {
                    "_id": "68f04f9036f8b025381e18ed",
                    "user": {
                        "_id": "63a83c5432ed73936eb8363e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a83c5432ed73936eb8363e/8iJN61whs49R0bccdL7b5.jpeg",
                        "isPro": false,
                        "fullname": "kun kerdthaisong",
                        "user": "augustus2011",
                        "type": "user"
                    },
                    "name": "Kun Kerdthaisong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T06:39:08.330Z",
                    "hidden": false
                },
                {
                    "_id": "68f04f9036f8b025381e18ee",
                    "name": "Phuree Phenhiran",
                    "hidden": false
                },
                {
                    "_id": "68f04f9036f8b025381e18ef",
                    "name": "Pitikorn Khlaisamniang",
                    "hidden": false
                },
                {
                    "_id": "68f04f9036f8b025381e18f0",
                    "name": "Supasate Vorathammathorn",
                    "hidden": false
                },
                {
                    "_id": "68f04f9036f8b025381e18f1",
                    "name": "Piyalitt Ittichaiwong",
                    "hidden": false
                },
                {
                    "_id": "68f04f9036f8b025381e18f2",
                    "name": "Nutchanon Yongsatianchot",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T14:17:23.000Z",
            "submittedOnDailyAt": "2025-10-16T00:21:39.147Z",
            "title": "Deflanderization for Game Dialogue: Balancing Character Authenticity\n  with Task Execution in LLM-based NPCs",
            "submittedOnDailyBy": {
                "_id": "63a83c5432ed73936eb8363e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a83c5432ed73936eb8363e/8iJN61whs49R0bccdL7b5.jpeg",
                "isPro": false,
                "fullname": "kun kerdthaisong",
                "user": "augustus2011",
                "type": "user"
            },
            "summary": "The emergence of large language models (LLMs) has opened new opportunities\nfor cre- ating dynamic non-player characters (NPCs) in gaming environments,\nenabling both func- tional task execution and persona-consistent dialogue\ngeneration. In this paper, we (Tu_Character_lab) report our participation in\nthe Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which\neval- uates agents across three tracks: task-oriented dialogue, context-aware\ndialogue, and their integration. Our approach combines two complementary\nstrategies: (i) lightweight prompting techniques in the API track, including a\nDeflanderization prompting method to suppress excessive role-play and improve\ntask fidelity, and (ii) fine-tuned large models in the GPU track, leveraging\nQwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our\nbest submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on\nTask 3 (GPU track).",
            "upvotes": 1,
            "discussionId": "68f04f9036f8b025381e18f3",
            "githubRepo": "https://github.com/Augustus2011/sony-cpdc-2025-starter-kit",
            "ai_summary": "Participants in the CPDC 2025 used lightweight prompting and fine-tuned large models to achieve high rankings in task-oriented and context-aware dialogue challenges.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "non-player characters",
                "NPCs",
                "Commonsense Persona-Grounded Dialogue Challenge",
                "CPDC",
                "task-oriented dialogue",
                "context-aware dialogue",
                "lightweight prompting",
                "Deflanderization",
                "fine-tuned models",
                "Qwen3-14B",
                "supervised fine-tuning",
                "SFT",
                "Low-Rank Adaptation",
                "LoRA"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "67aaba5d8d478dcb4b2f4281",
                "name": "Character-lab",
                "fullname": "Character-lab"
            }
        },
        "publishedAt": "2025-10-15T10:17:23.000Z",
        "title": "Deflanderization for Game Dialogue: Balancing Character Authenticity\n  with Task Execution in LLM-based NPCs",
        "summary": "The emergence of large language models (LLMs) has opened new opportunities\nfor cre- ating dynamic non-player characters (NPCs) in gaming environments,\nenabling both func- tional task execution and persona-consistent dialogue\ngeneration. In this paper, we (Tu_Character_lab) report our participation in\nthe Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which\neval- uates agents across three tracks: task-oriented dialogue, context-aware\ndialogue, and their integration. Our approach combines two complementary\nstrategies: (i) lightweight prompting techniques in the API track, including a\nDeflanderization prompting method to suppress excessive role-play and improve\ntask fidelity, and (ii) fine-tuned large models in the GPU track, leveraging\nQwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our\nbest submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on\nTask 3 (GPU track).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13586.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a83c5432ed73936eb8363e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a83c5432ed73936eb8363e/8iJN61whs49R0bccdL7b5.jpeg",
            "fullname": "kun kerdthaisong",
            "name": "augustus2011",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "67aaba5d8d478dcb4b2f4281",
            "name": "Character-lab",
            "fullname": "Character-lab"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.13255",
            "authors": [
                {
                    "_id": "68f0c8c136f8b025381e1b23",
                    "user": {
                        "_id": "66a7267a474c222eb649bdbc",
                        "avatarUrl": "/avatars/83771cce0b91a5d88707d5fee3b3b88b.svg",
                        "isPro": false,
                        "fullname": "Jingmin",
                        "user": "GarfieldX",
                        "type": "user"
                    },
                    "name": "Jingmin An",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T10:38:17.629Z",
                    "hidden": false
                },
                {
                    "_id": "68f0c8c136f8b025381e1b24",
                    "name": "Yilong Song",
                    "hidden": false
                },
                {
                    "_id": "68f0c8c136f8b025381e1b25",
                    "name": "Ruolin Yang",
                    "hidden": false
                },
                {
                    "_id": "68f0c8c136f8b025381e1b26",
                    "name": "Nai Ding",
                    "hidden": false
                },
                {
                    "_id": "68f0c8c136f8b025381e1b27",
                    "name": "Lingxi Lu",
                    "hidden": false
                },
                {
                    "_id": "68f0c8c136f8b025381e1b28",
                    "name": "Yuxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68f0c8c136f8b025381e1b29",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "68f0c8c136f8b025381e1b2a",
                    "name": "Chu Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68f0c8c136f8b025381e1b2b",
                    "name": "Qian Wang",
                    "hidden": false
                },
                {
                    "_id": "68f0c8c136f8b025381e1b2c",
                    "name": "Fang Fang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T08:04:49.000Z",
            "submittedOnDailyAt": "2025-10-16T09:22:02.944Z",
            "title": "Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to\n  Investigate Syntactic Structure Representations in Large Language Models and\n  the Human Brain",
            "submittedOnDailyBy": {
                "_id": "66a7267a474c222eb649bdbc",
                "avatarUrl": "/avatars/83771cce0b91a5d88707d5fee3b3b88b.svg",
                "isPro": false,
                "fullname": "Jingmin",
                "user": "GarfieldX",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) demonstrate human-level or even superior\nlanguage abilities, effectively modeling syntactic structures, yet the specific\ncomputational modules responsible remain unclear. A key question is whether LLM\nbehavioral capabilities stem from mechanisms akin to those in the human brain.\nTo address these questions, we introduce the Hierarchical Frequency Tagging\nProbe (HFTP), a tool that utilizes frequency-domain analysis to identify\nneuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP)\nneurons) and cortical regions (via intracranial recordings) encoding syntactic\nstructures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama\n2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human\nbrain relies on distinct cortical regions for different syntactic levels.\nRepresentational similarity analysis reveals a stronger alignment between LLM\nrepresentations and the left hemisphere of the brain (dominant in language\nprocessing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows\ngreater brain similarity than Gemma, while Llama 3.1 shows less alignment with\nthe brain compared to Llama 2. These findings offer new insights into the\ninterpretability of LLM behavioral improvements, raising questions about\nwhether these advancements are driven by human-like or non-human-like\nmechanisms, and establish HFTP as a valuable tool bridging computational\nlinguistics and cognitive neuroscience. This project is available at\nhttps://github.com/LilTiger/HFTP.",
            "upvotes": 1,
            "discussionId": "68f0c8c236f8b025381e1b2d",
            "githubRepo": "https://github.com/LilTiger/HFTP",
            "ai_summary": "Hierarchical Frequency Tagging Probe (HFTP) identifies neuron-wise components in LLMs and cortical regions encoding syntactic structures, revealing differences in how LLMs and the human brain process syntax.",
            "ai_keywords": [
                "Hierarchical Frequency Tagging Probe",
                "HFTP",
                "frequency-domain analysis",
                "Multilayer Perceptron",
                "MLP",
                "representational similarity analysis",
                "left hemisphere",
                "language processing"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-10-15T04:04:49.000Z",
        "title": "Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to\n  Investigate Syntactic Structure Representations in Large Language Models and\n  the Human Brain",
        "summary": "Large Language Models (LLMs) demonstrate human-level or even superior\nlanguage abilities, effectively modeling syntactic structures, yet the specific\ncomputational modules responsible remain unclear. A key question is whether LLM\nbehavioral capabilities stem from mechanisms akin to those in the human brain.\nTo address these questions, we introduce the Hierarchical Frequency Tagging\nProbe (HFTP), a tool that utilizes frequency-domain analysis to identify\nneuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP)\nneurons) and cortical regions (via intracranial recordings) encoding syntactic\nstructures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama\n2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human\nbrain relies on distinct cortical regions for different syntactic levels.\nRepresentational similarity analysis reveals a stronger alignment between LLM\nrepresentations and the left hemisphere of the brain (dominant in language\nprocessing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows\ngreater brain similarity than Gemma, while Llama 3.1 shows less alignment with\nthe brain compared to Llama 2. These findings offer new insights into the\ninterpretability of LLM behavioral improvements, raising questions about\nwhether these advancements are driven by human-like or non-human-like\nmechanisms, and establish HFTP as a valuable tool bridging computational\nlinguistics and cognitive neuroscience. This project is available at\nhttps://github.com/LilTiger/HFTP.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13255.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66a7267a474c222eb649bdbc",
            "avatarUrl": "/avatars/83771cce0b91a5d88707d5fee3b3b88b.svg",
            "fullname": "Jingmin",
            "name": "GarfieldX",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.12872",
            "authors": [
                {
                    "_id": "68f0a09c36f8b025381e1aa0",
                    "user": {
                        "_id": "6597f70d260bcd5fb5529613",
                        "avatarUrl": "/avatars/8fd7a210f3cb0de1229b49a3cd1ad8a0.svg",
                        "isPro": false,
                        "fullname": "Hancheng Ye",
                        "user": "HankYe",
                        "type": "user"
                    },
                    "name": "Hancheng Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-16T10:38:24.967Z",
                    "hidden": false
                },
                {
                    "_id": "68f0a09c36f8b025381e1aa1",
                    "name": "Zhengqi Gao",
                    "hidden": false
                },
                {
                    "_id": "68f0a09c36f8b025381e1aa2",
                    "name": "Mingyuan Ma",
                    "hidden": false
                },
                {
                    "_id": "68f0a09c36f8b025381e1aa3",
                    "name": "Qinsi Wang",
                    "hidden": false
                },
                {
                    "_id": "68f0a09c36f8b025381e1aa4",
                    "name": "Yuzhe Fu",
                    "hidden": false
                },
                {
                    "_id": "68f0a09c36f8b025381e1aa5",
                    "name": "Ming-Yu Chung",
                    "hidden": false
                },
                {
                    "_id": "68f0a09c36f8b025381e1aa6",
                    "name": "Yueqian Lin",
                    "hidden": false
                },
                {
                    "_id": "68f0a09c36f8b025381e1aa7",
                    "name": "Zhijian Liu",
                    "hidden": false
                },
                {
                    "_id": "68f0a09c36f8b025381e1aa8",
                    "name": "Jianyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f0a09c36f8b025381e1aa9",
                    "name": "Danyang Zhuo",
                    "hidden": false
                },
                {
                    "_id": "68f0a09c36f8b025381e1aaa",
                    "name": "Yiran Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6597f70d260bcd5fb5529613/iTO2hySSdCYKajlTpsas7.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/6597f70d260bcd5fb5529613/tRrjLPPATAHKQb9b8HUtn.jpeg"
            ],
            "publishedAt": "2025-10-14T18:00:01.000Z",
            "submittedOnDailyAt": "2025-10-16T23:38:42.053Z",
            "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems",
            "submittedOnDailyBy": {
                "_id": "6597f70d260bcd5fb5529613",
                "avatarUrl": "/avatars/8fd7a210f3cb0de1229b49a3cd1ad8a0.svg",
                "isPro": false,
                "fullname": "Hancheng Ye",
                "user": "HankYe",
                "type": "user"
            },
            "summary": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms.",
            "upvotes": 1,
            "discussionId": "68f0a09c36f8b025381e1aab",
            "ai_summary": "KVCOMM is a training-free framework that enhances multi-agent LLM systems by efficiently reusing KV-caches and aligning cache offsets, achieving significant speedups without quality loss.",
            "ai_keywords": [
                "multi-agent large language model",
                "key-value caching",
                "KVCOMM",
                "prefilling",
                "cache offsets",
                "anchor pool",
                "retrieval-augmented generation",
                "math reasoning",
                "collaborative coding tasks",
                "speedup",
                "TTFT"
            ],
            "organization": {
                "_id": "67b08968f632e29b93b22deb",
                "name": "DukeCEICenter",
                "fullname": "Duke Center for Computational Evolutionary Intelligence (CEI)",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63291bf2ce50f12537506bf2/inRsMa0-N2NIfSdfq0reW.png"
            }
        },
        "publishedAt": "2025-10-14T14:00:01.000Z",
        "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems",
        "summary": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6597f70d260bcd5fb5529613/iTO2hySSdCYKajlTpsas7.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/6597f70d260bcd5fb5529613/tRrjLPPATAHKQb9b8HUtn.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12872.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6597f70d260bcd5fb5529613",
            "avatarUrl": "/avatars/8fd7a210f3cb0de1229b49a3cd1ad8a0.svg",
            "fullname": "Hancheng Ye",
            "name": "HankYe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "67b08968f632e29b93b22deb",
            "name": "DukeCEICenter",
            "fullname": "Duke Center for Computational Evolutionary Intelligence (CEI)",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63291bf2ce50f12537506bf2/inRsMa0-N2NIfSdfq0reW.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.11715",
            "authors": [
                {
                    "_id": "68f0973e36f8b025381e1a6d",
                    "name": "Ayush Shrivastava",
                    "hidden": false
                },
                {
                    "_id": "68f0973e36f8b025381e1a6e",
                    "name": "Sanyam Mehta",
                    "hidden": false
                },
                {
                    "_id": "68f0973e36f8b025381e1a6f",
                    "name": "Daniel Geng",
                    "hidden": false
                },
                {
                    "_id": "68f0973e36f8b025381e1a70",
                    "name": "Andrew Owens",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66f41c37d8de0f55059d3317/r0h0OPhM6EP_K1lCFgbHS.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/66f41c37d8de0f55059d3317/WA_D_4fbsbGOHgDY55cH8.mp4"
            ],
            "publishedAt": "2025-10-13T17:59:46.000Z",
            "submittedOnDailyAt": "2025-10-16T05:34:34.768Z",
            "title": "Point Prompting: Counterfactual Tracking with Video Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "66f41c37d8de0f55059d3317",
                "avatarUrl": "/avatars/a730ff35ea1da9a2da79aa56617e82d8.svg",
                "isPro": false,
                "fullname": "Ayush Shrivastava",
                "user": "ayshrv",
                "type": "user"
            },
            "summary": "Trackers and video generators solve closely related problems: the former\nanalyze motion, while the latter synthesize it. We show that this connection\nenables pretrained video diffusion models to perform zero-shot point tracking\nby simply prompting them to visually mark points as they move over time. We\nplace a distinctively colored marker at the query point, then regenerate the\nrest of the video from an intermediate noise level. This propagates the marker\nacross frames, tracing the point's trajectory. To ensure that the marker\nremains visible in this counterfactual generation, despite such markers being\nunlikely in natural videos, we use the unedited initial frame as a negative\nprompt. Through experiments with multiple image-conditioned video diffusion\nmodels, we find that these \"emergent\" tracks outperform those of prior\nzero-shot methods and persist through occlusions, often obtaining performance\nthat is competitive with specialized self-supervised models.",
            "upvotes": 1,
            "discussionId": "68f0973e36f8b025381e1a71",
            "projectPage": "https://point-prompting.github.io/",
            "ai_summary": "Pretrained video diffusion models can perform zero-shot point tracking by visually marking points and regenerating video frames, outperforming prior methods and handling occlusions.",
            "ai_keywords": [
                "video diffusion models",
                "zero-shot point tracking",
                "image-conditioned video diffusion models",
                "emergent tracks",
                "self-supervised models"
            ]
        },
        "publishedAt": "2025-10-13T13:59:46.000Z",
        "title": "Point Prompting: Counterfactual Tracking with Video Diffusion Models",
        "summary": "Trackers and video generators solve closely related problems: the former\nanalyze motion, while the latter synthesize it. We show that this connection\nenables pretrained video diffusion models to perform zero-shot point tracking\nby simply prompting them to visually mark points as they move over time. We\nplace a distinctively colored marker at the query point, then regenerate the\nrest of the video from an intermediate noise level. This propagates the marker\nacross frames, tracing the point's trajectory. To ensure that the marker\nremains visible in this counterfactual generation, despite such markers being\nunlikely in natural videos, we use the unedited initial frame as a negative\nprompt. Through experiments with multiple image-conditioned video diffusion\nmodels, we find that these \"emergent\" tracks outperform those of prior\nzero-shot methods and persist through occlusions, often obtaining performance\nthat is competitive with specialized self-supervised models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66f41c37d8de0f55059d3317/r0h0OPhM6EP_K1lCFgbHS.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/66f41c37d8de0f55059d3317/WA_D_4fbsbGOHgDY55cH8.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11715.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66f41c37d8de0f55059d3317",
            "avatarUrl": "/avatars/a730ff35ea1da9a2da79aa56617e82d8.svg",
            "fullname": "Ayush Shrivastava",
            "name": "ayshrv",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11653",
            "authors": [
                {
                    "_id": "68edca69de1fee572713a921",
                    "user": {
                        "_id": "6698325e08250aac9f076f43",
                        "avatarUrl": "/avatars/f4a17acaf62ecf20e9ef54b166a891a9.svg",
                        "isPro": false,
                        "fullname": "Prasanna Mayilvahanan",
                        "user": "prasannamayil",
                        "type": "user"
                    },
                    "name": "Prasanna Mayilvahanan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T07:29:53.814Z",
                    "hidden": false
                },
                {
                    "_id": "68edca69de1fee572713a922",
                    "name": "Ricardo Dominguez-Olmedo",
                    "hidden": false
                },
                {
                    "_id": "68edca69de1fee572713a923",
                    "name": "Thaddus Wiedemer",
                    "hidden": false
                },
                {
                    "_id": "68edca69de1fee572713a924",
                    "name": "Wieland Brendel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T17:30:54.000Z",
            "submittedOnDailyAt": "2025-10-16T09:30:27.389Z",
            "title": "MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model",
            "submittedOnDailyBy": {
                "_id": "6698325e08250aac9f076f43",
                "avatarUrl": "/avatars/f4a17acaf62ecf20e9ef54b166a891a9.svg",
                "isPro": false,
                "fullname": "Prasanna Mayilvahanan",
                "user": "prasannamayil",
                "type": "user"
            },
            "summary": "With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL)\nmethods has emerged that seem to unlock stronger mathematical reasoning.\nHowever, a closer look at the open-source ecosystem reveals a critical\nlimitation: with sufficiently many draws (e.g., pass@1024), many\nexisting base models already solve nearly all questions on widely used math\nbenchmarks such as MATH-500 and AIME 2024. This suggests that the RL\nfine-tuning methods prevalent in the LLM reasoning literature largely sharpen\nexisting solution modes rather than discovering entirely new ones. Such\nsharpening stands in contrast to the broader promise of RL: to foster\nexploration and to acquire new skills. To move beyond this plateau, we\nintroduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat\ncommon open-source models of up to 8B parameters even under large sampling\nbudgets. Improving performance on our benchmark via RL requires methods that\nlearn to reason in ways that go beyond base model capabilities in repeated\nsampling. Since the problems are drawn from subsets of DAPO-Math-17K and\nDeepScaleR datasets, they remain topically equivalent to standard high-school\nmath. Validating our premise, RL fine-tuned models such as\nNemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform\npoorly on MATH-B at pass@1024, showing how existing approaches fall\nshort on tackling harder instances. We hope MATH-B will catalyze\nexploration-driven RL approaches that elicit deeper reasoning capabilities. We\nrelease MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond.",
            "upvotes": 1,
            "discussionId": "68edca69de1fee572713a925",
            "ai_summary": "MATH-Beyond is a benchmark designed to challenge existing reinforcement learning methods by requiring deeper reasoning capabilities beyond current model capabilities.",
            "ai_keywords": [
                "reinforcement learning",
                "RL",
                "fine-tuning",
                "MATH-500",
                "AIME 2024",
                "MATH-Beyond",
                "MATH-B",
                "DAPO-Math-17K",
                "DeepScaleR",
                "Nemotron-Research-Reasoning-Qwen-1.5B",
                "DeepScaleR-1.5B-Preview"
            ]
        },
        "publishedAt": "2025-10-13T13:30:54.000Z",
        "title": "MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model",
        "summary": "With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL)\nmethods has emerged that seem to unlock stronger mathematical reasoning.\nHowever, a closer look at the open-source ecosystem reveals a critical\nlimitation: with sufficiently many draws (e.g., pass@1024), many\nexisting base models already solve nearly all questions on widely used math\nbenchmarks such as MATH-500 and AIME 2024. This suggests that the RL\nfine-tuning methods prevalent in the LLM reasoning literature largely sharpen\nexisting solution modes rather than discovering entirely new ones. Such\nsharpening stands in contrast to the broader promise of RL: to foster\nexploration and to acquire new skills. To move beyond this plateau, we\nintroduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat\ncommon open-source models of up to 8B parameters even under large sampling\nbudgets. Improving performance on our benchmark via RL requires methods that\nlearn to reason in ways that go beyond base model capabilities in repeated\nsampling. Since the problems are drawn from subsets of DAPO-Math-17K and\nDeepScaleR datasets, they remain topically equivalent to standard high-school\nmath. Validating our premise, RL fine-tuned models such as\nNemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform\npoorly on MATH-B at pass@1024, showing how existing approaches fall\nshort on tackling harder instances. We hope MATH-B will catalyze\nexploration-driven RL approaches that elicit deeper reasoning capabilities. We\nrelease MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11653.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6698325e08250aac9f076f43",
            "avatarUrl": "/avatars/f4a17acaf62ecf20e9ef54b166a891a9.svg",
            "fullname": "Prasanna Mayilvahanan",
            "name": "prasannamayil",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.11170",
            "authors": [
                {
                    "_id": "68f0d8cc36f8b025381e1b5a",
                    "name": "Daniel Scalena",
                    "hidden": false
                },
                {
                    "_id": "68f0d8cc36f8b025381e1b5b",
                    "name": "Leonidas Zotos",
                    "hidden": false
                },
                {
                    "_id": "68f0d8cc36f8b025381e1b5c",
                    "name": "Elisabetta Fersini",
                    "hidden": false
                },
                {
                    "_id": "68f0d8cc36f8b025381e1b5d",
                    "name": "Malvina Nissim",
                    "hidden": false
                },
                {
                    "_id": "68f0d8cc36f8b025381e1b5e",
                    "name": "Ahmet stn",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/633e9ae8630ed415eb38073c/kiWtrSN8j9nVLHw1A5kfF.png"
            ],
            "publishedAt": "2025-10-13T09:04:28.000Z",
            "submittedOnDailyAt": "2025-10-16T10:11:59.881Z",
            "title": "EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling",
            "submittedOnDailyBy": {
                "_id": "633e9ae8630ed415eb38073c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633e9ae8630ed415eb38073c/FtSb_ybm08qBR1liISKQ0.jpeg",
                "isPro": false,
                "fullname": "Daniel Scalena",
                "user": "DanielSc4",
                "type": "user"
            },
            "summary": "With the rise of reasoning language models and test-time scaling methods as a\nparadigm for improving model performance, substantial computation is often\nrequired to generate multiple candidate sequences from the same prompt. This\nenables exploration of different reasoning paths toward the correct solution,\nhowever, allocates the same compute budget for each prompt. Grounded on the\nassumption that different prompts carry different degrees of complexity, and\nthus different computation needs, we propose EAGer, a training-free generation\nmethod that leverages model uncertainty through token-wise entropy distribution\nto reduce redundant computation and concurrently improve overall performance.\nEAGer allows branching to multiple reasoning paths only in the presence of\nhigh-entropy tokens, and then reallocates the saved compute budget to the\ninstances where exploration of alternative paths is most needed. We find that\nacross multiple open-source models on complex reasoning benchmarks such as AIME\n2025, EAGer can reallocate the budget without accessing target labels,\nachieving the best efficiency-performance trade-off in terms of reasoning\nlength and Pass@k. When target labels are accessible, EAGer generates up to 65%\nfewer tokens (hence saving compute) and achieves up to 37% improvement in\nPass@k compared to the Full Parallel Sampling.",
            "upvotes": 1,
            "discussionId": "68f0d8cd36f8b025381e1b5f",
            "githubRepo": "https://github.com/DanielSc4/EAGer",
            "ai_summary": "EAGer, a training-free method, uses token-wise entropy to optimize computational resources and improve performance on complex reasoning tasks.",
            "ai_keywords": [
                "reasoning language models",
                "test-time scaling",
                "model uncertainty",
                "token-wise entropy",
                "high-entropy tokens",
                "reasoning length",
                "Pass@k",
                "Full Parallel Sampling"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "640ca1c93623f6a56ddab373",
                "name": "CohereLabs",
                "fullname": "Cohere Labs",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678549441248-5e70f6048ce3c604d78fe133.png"
            }
        },
        "publishedAt": "2025-10-13T05:04:28.000Z",
        "title": "EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling",
        "summary": "With the rise of reasoning language models and test-time scaling methods as a\nparadigm for improving model performance, substantial computation is often\nrequired to generate multiple candidate sequences from the same prompt. This\nenables exploration of different reasoning paths toward the correct solution,\nhowever, allocates the same compute budget for each prompt. Grounded on the\nassumption that different prompts carry different degrees of complexity, and\nthus different computation needs, we propose EAGer, a training-free generation\nmethod that leverages model uncertainty through token-wise entropy distribution\nto reduce redundant computation and concurrently improve overall performance.\nEAGer allows branching to multiple reasoning paths only in the presence of\nhigh-entropy tokens, and then reallocates the saved compute budget to the\ninstances where exploration of alternative paths is most needed. We find that\nacross multiple open-source models on complex reasoning benchmarks such as AIME\n2025, EAGer can reallocate the budget without accessing target labels,\nachieving the best efficiency-performance trade-off in terms of reasoning\nlength and Pass@k. When target labels are accessible, EAGer generates up to 65%\nfewer tokens (hence saving compute) and achieves up to 37% improvement in\nPass@k compared to the Full Parallel Sampling.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/633e9ae8630ed415eb38073c/kiWtrSN8j9nVLHw1A5kfF.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11170.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "633e9ae8630ed415eb38073c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633e9ae8630ed415eb38073c/FtSb_ybm08qBR1liISKQ0.jpeg",
            "fullname": "Daniel Scalena",
            "name": "DanielSc4",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "640ca1c93623f6a56ddab373",
            "name": "CohereLabs",
            "fullname": "Cohere Labs",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678549441248-5e70f6048ce3c604d78fe133.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.09913",
            "authors": [
                {
                    "_id": "68f1666e6e0bef323a68fb28",
                    "name": "Shangbin Feng",
                    "hidden": false
                },
                {
                    "_id": "68f1666e6e0bef323a68fb29",
                    "name": "Wenhao Yu",
                    "hidden": false
                },
                {
                    "_id": "68f1666e6e0bef323a68fb2a",
                    "name": "Yike Wang",
                    "hidden": false
                },
                {
                    "_id": "68f1666e6e0bef323a68fb2b",
                    "name": "Hongming Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1666e6e0bef323a68fb2c",
                    "name": "Yulia Tsvetkov",
                    "hidden": false
                },
                {
                    "_id": "68f1666e6e0bef323a68fb2d",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T23:12:20.000Z",
            "submittedOnDailyAt": "2025-10-16T20:11:19.782Z",
            "title": "Don't Throw Away Your Pretrained Model",
            "submittedOnDailyBy": {
                "_id": "5feab3a28a3201f8e554c969",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660795228685-5feab3a28a3201f8e554c969.png",
                "isPro": false,
                "fullname": "Wenhao Yu",
                "user": "wyu1",
                "type": "user"
            },
            "summary": "Alignment training has tradeoffs: it helps language models (LMs) gain in\nreasoning and instruction following but might lose out on skills such as\ncreativity and calibration, where unaligned base models are better at. We aim\nto make the best of both worlds through model collaboration, where different\nmodels in the training pipeline collaborate and complement each other. Since LM\nresponses feature interleaving skills that favor different models, we propose\nSwitch Generation, where pretrained and aligned model versions take turns to\n``speak'' in a response sequence. Specifically, we train a switcher LM by\nlearning from outcomes of choosing different models to generate the next\nsegment across diverse queries and contexts. At inference time, the switcher LM\nguides different model checkpoints to dynamically generate the next segment\nwhere their strengths are most needed. Extensive experiments with 8 model\ncollaboration baselines and 18 datasets show that 1) model collaboration\nconsistently outperforms individual models on 16 out of 18 tasks, and 2) Switch\nGeneration further outperforms baselines by 12.9% on average. Further analysis\nreveals that Switch Generation discovers compositional skills to solve problems\nwhere individual models struggle and generalizes to unseen models and tasks,\nreusing and repurposing by-products in expensive model training pipelines that\nare otherwise discarded.",
            "upvotes": 1,
            "discussionId": "68f1666f6e0bef323a68fb2e",
            "githubRepo": "https://github.com/BunsenFeng/switch_generation",
            "ai_summary": "Model collaboration and Switch Generation improve language model performance by leveraging the strengths of different models in a dynamic and complementary manner.",
            "ai_keywords": [
                "alignment training",
                "language models",
                "reasoning",
                "instruction following",
                "creativity",
                "calibration",
                "model collaboration",
                "Switch Generation",
                "pretrained models",
                "aligned models",
                "switcher LM",
                "inference time",
                "compositional skills",
                "unseen models",
                "tasks",
                "expensive model training pipelines"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-10-10T19:12:20.000Z",
        "title": "Don't Throw Away Your Pretrained Model",
        "summary": "Alignment training has tradeoffs: it helps language models (LMs) gain in\nreasoning and instruction following but might lose out on skills such as\ncreativity and calibration, where unaligned base models are better at. We aim\nto make the best of both worlds through model collaboration, where different\nmodels in the training pipeline collaborate and complement each other. Since LM\nresponses feature interleaving skills that favor different models, we propose\nSwitch Generation, where pretrained and aligned model versions take turns to\n``speak'' in a response sequence. Specifically, we train a switcher LM by\nlearning from outcomes of choosing different models to generate the next\nsegment across diverse queries and contexts. At inference time, the switcher LM\nguides different model checkpoints to dynamically generate the next segment\nwhere their strengths are most needed. Extensive experiments with 8 model\ncollaboration baselines and 18 datasets show that 1) model collaboration\nconsistently outperforms individual models on 16 out of 18 tasks, and 2) Switch\nGeneration further outperforms baselines by 12.9% on average. Further analysis\nreveals that Switch Generation discovers compositional skills to solve problems\nwhere individual models struggle and generalizes to unseen models and tasks,\nreusing and repurposing by-products in expensive model training pipelines that\nare otherwise discarded.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09913.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5feab3a28a3201f8e554c969",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660795228685-5feab3a28a3201f8e554c969.png",
            "fullname": "Wenhao Yu",
            "name": "wyu1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.10930",
            "authors": [
                {
                    "_id": "68f1084cdf97aa3a742685af",
                    "name": "Katherine M. Collins",
                    "hidden": false
                },
                {
                    "_id": "68f1084cdf97aa3a742685b0",
                    "name": "Cedegao E. Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1084cdf97aa3a742685b1",
                    "name": "Graham Todd",
                    "hidden": false
                },
                {
                    "_id": "68f1084cdf97aa3a742685b2",
                    "name": "Lance Ying",
                    "hidden": false
                },
                {
                    "_id": "68f1084cdf97aa3a742685b3",
                    "name": "Mauricio Barba da Costa",
                    "hidden": false
                },
                {
                    "_id": "68f1084cdf97aa3a742685b4",
                    "name": "Ryan Liu",
                    "hidden": false
                },
                {
                    "_id": "68f1084cdf97aa3a742685b5",
                    "name": "Prafull Sharma",
                    "hidden": false
                },
                {
                    "_id": "68f1084cdf97aa3a742685b6",
                    "name": "Adrian Weller",
                    "hidden": false
                },
                {
                    "_id": "68f1084cdf97aa3a742685b7",
                    "name": "Ionatan Kuperwajs",
                    "hidden": false
                },
                {
                    "_id": "68f1084cdf97aa3a742685b8",
                    "name": "Lionel Wong",
                    "hidden": false
                },
                {
                    "_id": "68f1084cdf97aa3a742685b9",
                    "name": "Joshua B. Tenenbaum",
                    "hidden": false
                },
                {
                    "_id": "68f1084cdf97aa3a742685ba",
                    "name": "Thomas L. Griffiths",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T02:45:37.000Z",
            "submittedOnDailyAt": "2025-10-16T13:31:13.750Z",
            "title": "Evaluating Language Models' Evaluations of Games",
            "submittedOnDailyBy": {
                "_id": "64514cd06db3be7ee57b6f34",
                "avatarUrl": "/avatars/3276eeb344f57601e2a2199dd090935b.svg",
                "isPro": false,
                "fullname": "Katie Collins",
                "user": "kmcollins",
                "type": "user"
            },
            "summary": "Reasoning is not just about solving problems -- it is also about evaluating\nwhich problems are worth solving at all. Evaluations of artificial intelligence\n(AI) systems primarily focused on problem solving, historically by studying how\nmodels play games such as chess and Go. In this paper, we advocate for a new\nparadigm that assesses AI systems' evaluation of games. First, we introduce a\nformalism for evaluating such evaluations. We then leverage a large-scale\ndataset of over 100 novel board games and over 450 human judgments to compare\nevaluations produced by modern language and reasoning models against those of\npeople and symbolic computational agents. We consider two kinds of evaluative\nqueries: assessing the payoff (or fairness) and the funness of games. These\nqueries span two dimensions relevant to the design of evaluations of AI\nevaluations: how complex a query is to compute and how difficult a query is to\nquantify. Our results show that reasoning models are generally more aligned to\npeople in their evaluations of games than non-reasoning language models.\nHowever, we observe a non-monotonic relationship: as models get closer to\ngame-theoretic optimal, their fit to human data weakens. We also observe more\n\"jaggedness\" across models for assessing funness, in line with the greater\ndifficulty of quantifying this query. Across queries and games, reasoning\nmodels show highly variable and unpredictable resource usage when assessing\nqueries, pointing to the importance of imbuing more resource-rational\nmeta-reasoning in language and reasoning models.",
            "upvotes": 0,
            "discussionId": "68f1084cdf97aa3a742685bb",
            "ai_summary": "Modern reasoning models are more aligned with human evaluations of games than non-reasoning models, but their performance can degrade as they approach game-theoretic optimality, especially for subjective assessments like funness.",
            "ai_keywords": [
                "formalism",
                "board games",
                "human judgments",
                "evaluative queries",
                "payoff",
                "fairness",
                "funness",
                "game-theoretic optimal",
                "resource-rational meta-reasoning"
            ]
        },
        "publishedAt": "2025-10-12T22:45:37.000Z",
        "title": "Evaluating Language Models' Evaluations of Games",
        "summary": "Reasoning is not just about solving problems -- it is also about evaluating\nwhich problems are worth solving at all. Evaluations of artificial intelligence\n(AI) systems primarily focused on problem solving, historically by studying how\nmodels play games such as chess and Go. In this paper, we advocate for a new\nparadigm that assesses AI systems' evaluation of games. First, we introduce a\nformalism for evaluating such evaluations. We then leverage a large-scale\ndataset of over 100 novel board games and over 450 human judgments to compare\nevaluations produced by modern language and reasoning models against those of\npeople and symbolic computational agents. We consider two kinds of evaluative\nqueries: assessing the payoff (or fairness) and the funness of games. These\nqueries span two dimensions relevant to the design of evaluations of AI\nevaluations: how complex a query is to compute and how difficult a query is to\nquantify. Our results show that reasoning models are generally more aligned to\npeople in their evaluations of games than non-reasoning language models.\nHowever, we observe a non-monotonic relationship: as models get closer to\ngame-theoretic optimal, their fit to human data weakens. We also observe more\n\"jaggedness\" across models for assessing funness, in line with the greater\ndifficulty of quantifying this query. Across queries and games, reasoning\nmodels show highly variable and unpredictable resource usage when assessing\nqueries, pointing to the importance of imbuing more resource-rational\nmeta-reasoning in language and reasoning models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10930.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64514cd06db3be7ee57b6f34",
            "avatarUrl": "/avatars/3276eeb344f57601e2a2199dd090935b.svg",
            "fullname": "Katie Collins",
            "name": "kmcollins",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.10494",
            "authors": [
                {
                    "_id": "68f11b81df97aa3a74268614",
                    "name": "Martina G. Vilas",
                    "hidden": false
                },
                {
                    "_id": "68f11b81df97aa3a74268615",
                    "name": "Safoora Yousefi",
                    "hidden": false
                },
                {
                    "_id": "68f11b81df97aa3a74268616",
                    "name": "Besmira Nushi",
                    "hidden": false
                },
                {
                    "_id": "68f11b81df97aa3a74268617",
                    "name": "Eric Horvitz",
                    "hidden": false
                },
                {
                    "_id": "68f11b81df97aa3a74268618",
                    "name": "Vidhisha Balachandran",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62cec94de1d5ee194e604177/bAUj6dXfD_BMQHQGiNJsD.png",
                "https://cdn-uploads.huggingface.co/production/uploads/62cec94de1d5ee194e604177/-C1ysaYlvzjJpsqz3et5R.png"
            ],
            "publishedAt": "2025-10-12T08:03:56.000Z",
            "submittedOnDailyAt": "2025-10-16T15:15:06.210Z",
            "title": "Tracing the Traces: Latent Temporal Signals for Efficient and Accurate\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "62cec94de1d5ee194e604177",
                "avatarUrl": "/avatars/85551a83b430a43af9b12ce09814b16f.svg",
                "isPro": false,
                "fullname": "Martina Vilas",
                "user": "martinagvilas",
                "type": "user"
            },
            "summary": "Reasoning models improve their problem-solving ability through inference-time\nscaling, allocating more compute via longer token budgets. Identifying which\nreasoning traces are likely to succeed remains a key opportunity: reliably\npredicting productive paths can substantially reduce wasted computation and\nimprove overall efficiency. We introduce Latent-Trajectory signals that\ncharacterize the temporal evolution of a model's internal representations\nduring the generation of intermediate reasoning tokens. By measuring the\noverall change in latent representations between the start and end of\nreasoning, the change accumulated across intermediate steps, and the extent to\nwhich these changes advance toward the final state, we show that these signals\npredict solution accuracy more reliably than both cross-layer metrics and\noutput-based confidence measures. When used to guide answer selection across\nmultiple sampled generations, Latent-Trajectory signals make test-time scaling\nmore effective and efficient than majority voting, reducing token usage by up\nto 70% while preserving and even improving accuracy by 2.6% on average.\nMoreover, these predictive signals often emerge early in the reasoning trace,\nenabling early selection and allocation of compute to the most promising\ncandidates. Our findings contribute not only practical strategies for\ninference-time efficiency, but also a deeper interpretability perspective on\nhow reasoning processes are represented and differentiated in latent space.",
            "upvotes": 0,
            "discussionId": "68f11b81df97aa3a74268619",
            "ai_summary": "Latent-Trajectory signals improve inference-time efficiency by predicting productive reasoning paths, reducing token usage and enhancing accuracy.",
            "ai_keywords": [
                "Latent-Trajectory signals",
                "internal representations",
                "reasoning tokens",
                "cross-layer metrics",
                "output-based confidence measures",
                "test-time scaling",
                "majority voting",
                "inference-time efficiency",
                "latent space"
            ],
            "organization": {
                "_id": "68151d0f51add3813f3f7d1b",
                "name": "MicrosoftResearch",
                "fullname": "Microsoft Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
            }
        },
        "publishedAt": "2025-10-12T04:03:56.000Z",
        "title": "Tracing the Traces: Latent Temporal Signals for Efficient and Accurate\n  Reasoning",
        "summary": "Reasoning models improve their problem-solving ability through inference-time\nscaling, allocating more compute via longer token budgets. Identifying which\nreasoning traces are likely to succeed remains a key opportunity: reliably\npredicting productive paths can substantially reduce wasted computation and\nimprove overall efficiency. We introduce Latent-Trajectory signals that\ncharacterize the temporal evolution of a model's internal representations\nduring the generation of intermediate reasoning tokens. By measuring the\noverall change in latent representations between the start and end of\nreasoning, the change accumulated across intermediate steps, and the extent to\nwhich these changes advance toward the final state, we show that these signals\npredict solution accuracy more reliably than both cross-layer metrics and\noutput-based confidence measures. When used to guide answer selection across\nmultiple sampled generations, Latent-Trajectory signals make test-time scaling\nmore effective and efficient than majority voting, reducing token usage by up\nto 70% while preserving and even improving accuracy by 2.6% on average.\nMoreover, these predictive signals often emerge early in the reasoning trace,\nenabling early selection and allocation of compute to the most promising\ncandidates. Our findings contribute not only practical strategies for\ninference-time efficiency, but also a deeper interpretability perspective on\nhow reasoning processes are represented and differentiated in latent space.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62cec94de1d5ee194e604177/bAUj6dXfD_BMQHQGiNJsD.png",
            "https://cdn-uploads.huggingface.co/production/uploads/62cec94de1d5ee194e604177/-C1ysaYlvzjJpsqz3et5R.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10494.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62cec94de1d5ee194e604177",
            "avatarUrl": "/avatars/85551a83b430a43af9b12ce09814b16f.svg",
            "fullname": "Martina Vilas",
            "name": "martinagvilas",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "68151d0f51add3813f3f7d1b",
            "name": "MicrosoftResearch",
            "fullname": "Microsoft Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07414",
            "authors": [
                {
                    "_id": "68f1154bdf97aa3a74268605",
                    "name": "Mufei Li",
                    "hidden": false
                },
                {
                    "_id": "68f1154bdf97aa3a74268606",
                    "name": "Dongqi Fu",
                    "hidden": false
                },
                {
                    "_id": "68f1154bdf97aa3a74268607",
                    "name": "Limei Wang",
                    "hidden": false
                },
                {
                    "_id": "68f1154bdf97aa3a74268608",
                    "name": "Si Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1154bdf97aa3a74268609",
                    "name": "Hanqing Zeng",
                    "hidden": false
                },
                {
                    "_id": "68f1154bdf97aa3a7426860a",
                    "name": "Kaan Sancak",
                    "hidden": false
                },
                {
                    "_id": "68f1154bdf97aa3a7426860b",
                    "name": "Ruizhong Qiu",
                    "hidden": false
                },
                {
                    "_id": "68f1154bdf97aa3a7426860c",
                    "name": "Haoyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68f1154bdf97aa3a7426860d",
                    "name": "Xiaoxin He",
                    "hidden": false
                },
                {
                    "_id": "68f1154bdf97aa3a7426860e",
                    "name": "Xavier Bresson",
                    "hidden": false
                },
                {
                    "_id": "68f1154bdf97aa3a7426860f",
                    "name": "Yinglong Xia",
                    "hidden": false
                },
                {
                    "_id": "68f1154bdf97aa3a74268610",
                    "name": "Chonglin Sun",
                    "hidden": false
                },
                {
                    "_id": "68f1154bdf97aa3a74268611",
                    "name": "Pan Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T18:12:10.000Z",
            "submittedOnDailyAt": "2025-10-16T14:30:42.555Z",
            "title": "Haystack Engineering: Context Engineering for Heterogeneous and Agentic\n  Long-Context Evaluation",
            "submittedOnDailyBy": {
                "_id": "667113dbef4c0c97967ceff3",
                "avatarUrl": "/avatars/366c8ef61e4cecbbd5c5e9fae01f1855.svg",
                "isPro": true,
                "fullname": "Mufei Li",
                "user": "ml1996",
                "type": "user"
            },
            "summary": "Modern long-context large language models (LLMs) perform well on synthetic\n\"needle-in-a-haystack\" (NIAH) benchmarks, but such tests overlook how noisy\ncontexts arise from biased retrieval and agentic workflows. We argue that\nhaystack engineering is necessary to construct noisy long contexts that\nfaithfully capture key real-world factors -- distraction from heterogeneous\nbiased retrievers and cascading errors in agentic workflows -- to test models'\nlong-context robustness. We instantiate it through HaystackCraft, a new NIAH\nbenchmark built on the full English Wikipedia hyperlink network with multi-hop\nquestions. HaystackCraft evaluates how heterogeneous retrieval strategies\n(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,\nhaystack ordering, and downstream LLM performance. HaystackCraft further\nextends NIAH to dynamic, LLM-dependent settings that simulate agentic\noperations, where models refine queries, reflect on their past reasonings, and\ndecide when to stop. Experiments with 15 long-context models show that (1)\nwhile stronger dense retrievers can introduce more challenging distractors,\ngraph-based reranking simultaneously improves retrieval effectiveness and\nmitigates more harmful distractors; (2) in agentic tests, even advanced models\nlike Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated\ndistractors or struggle to perform early stops. These results highlight\npersistent challenges in agentic long-context reasoning and establish\nHaystackCraft as a valuable testbed for future progress.",
            "upvotes": 0,
            "discussionId": "68f1154cdf97aa3a74268612",
            "githubRepo": "https://github.com/Graph-COM/HaystackCraft",
            "ai_summary": "HaystackCraft, a new benchmark using Wikipedia, evaluates long-context LLM robustness by simulating noisy retrieval and agentic workflows, revealing challenges in handling distractors and cascading errors.",
            "ai_keywords": [
                "long-context large language models",
                "needle-in-a-haystack benchmarks",
                "haystack engineering",
                "heterogeneous biased retrievers",
                "cascading errors",
                "agentic workflows",
                "HaystackCraft",
                "Wikipedia hyperlink network",
                "multi-hop questions",
                "sparse retrievers",
                "dense retrievers",
                "hybrid retrievers",
                "graph-based retrievers",
                "reranking",
                "retrieval effectiveness",
                "agentic operations",
                "self-generated distractors",
                "early stops",
                "Gemini 2.5 Pro",
                "GPT-5"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-10-08T14:12:10.000Z",
        "title": "Haystack Engineering: Context Engineering for Heterogeneous and Agentic\n  Long-Context Evaluation",
        "summary": "Modern long-context large language models (LLMs) perform well on synthetic\n\"needle-in-a-haystack\" (NIAH) benchmarks, but such tests overlook how noisy\ncontexts arise from biased retrieval and agentic workflows. We argue that\nhaystack engineering is necessary to construct noisy long contexts that\nfaithfully capture key real-world factors -- distraction from heterogeneous\nbiased retrievers and cascading errors in agentic workflows -- to test models'\nlong-context robustness. We instantiate it through HaystackCraft, a new NIAH\nbenchmark built on the full English Wikipedia hyperlink network with multi-hop\nquestions. HaystackCraft evaluates how heterogeneous retrieval strategies\n(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,\nhaystack ordering, and downstream LLM performance. HaystackCraft further\nextends NIAH to dynamic, LLM-dependent settings that simulate agentic\noperations, where models refine queries, reflect on their past reasonings, and\ndecide when to stop. Experiments with 15 long-context models show that (1)\nwhile stronger dense retrievers can introduce more challenging distractors,\ngraph-based reranking simultaneously improves retrieval effectiveness and\nmitigates more harmful distractors; (2) in agentic tests, even advanced models\nlike Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated\ndistractors or struggle to perform early stops. These results highlight\npersistent challenges in agentic long-context reasoning and establish\nHaystackCraft as a valuable testbed for future progress.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07414.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "667113dbef4c0c97967ceff3",
            "avatarUrl": "/avatars/366c8ef61e4cecbbd5c5e9fae01f1855.svg",
            "fullname": "Mufei Li",
            "name": "ml1996",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]