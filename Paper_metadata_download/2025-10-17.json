[
    {
        "paper": {
            "id": "2510.04849",
            "authors": [
                {
                    "_id": "68ed33b8de1fee572713a5b2",
                    "name": "Elisei Rykov",
                    "hidden": false
                },
                {
                    "_id": "68ed33b8de1fee572713a5b3",
                    "name": "Kseniia Petrushina",
                    "hidden": false
                },
                {
                    "_id": "68ed33b8de1fee572713a5b4",
                    "name": "Maksim Savkin",
                    "hidden": false
                },
                {
                    "_id": "68ed33b8de1fee572713a5b5",
                    "name": "Valerii Olisov",
                    "hidden": false
                },
                {
                    "_id": "68ed33b8de1fee572713a5b6",
                    "name": "Artem Vazhentsev",
                    "hidden": false
                },
                {
                    "_id": "68ed33b8de1fee572713a5b7",
                    "name": "Kseniia Titova",
                    "hidden": false
                },
                {
                    "_id": "68ed33b8de1fee572713a5b8",
                    "name": "Alexander Panchenko",
                    "hidden": false
                },
                {
                    "_id": "68ed33b8de1fee572713a5b9",
                    "name": "Vasily Konovalov",
                    "hidden": false
                },
                {
                    "_id": "68ed33b8de1fee572713a5ba",
                    "name": "Julia Belikova",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/622b1f6b9f6139daa8e998ce/-3FpmnPusd2upOiU67a_J.png"
            ],
            "publishedAt": "2025-10-06T14:36:30.000Z",
            "submittedOnDailyAt": "2025-10-17T09:28:42.156Z",
            "title": "When Models Lie, We Learn: Multilingual Span-Level Hallucination\n  Detection with PsiloQA",
            "submittedOnDailyBy": {
                "_id": "622b1f6b9f6139daa8e998ce",
                "avatarUrl": "/avatars/842719c100a5969be75d04da97333675.svg",
                "isPro": false,
                "fullname": "Vasily Konovalov",
                "user": "Vasily",
                "type": "user"
            },
            "summary": "Hallucination detection remains a fundamental challenge for the safe and\nreliable deployment of large language models (LLMs), especially in applications\nrequiring factual accuracy. Existing hallucination benchmarks often operate at\nthe sequence level and are limited to English, lacking the fine-grained,\nmultilingual supervision needed for a comprehensive evaluation. In this work,\nwe introduce PsiloQA, a large-scale, multilingual dataset annotated with\nspan-level hallucinations across 14 languages. PsiloQA is constructed through\nan automated three-stage pipeline: generating question-answer pairs from\nWikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse\nLLMs in a no-context setting, and automatically annotating hallucinated spans\nusing GPT-4o by comparing against golden answers and retrieved context. We\nevaluate a wide range of hallucination detection methods -- including\nuncertainty quantification, LLM-based tagging, and fine-tuned encoder models --\nand show that encoder-based models achieve the strongest performance across\nlanguages. Furthermore, PsiloQA demonstrates effective cross-lingual\ngeneralization and supports robust knowledge transfer to other benchmarks, all\nwhile being significantly more cost-efficient than human-annotated datasets.\nOur dataset and results advance the development of scalable, fine-grained\nhallucination detection in multilingual settings.",
            "upvotes": 80,
            "discussionId": "68ed33b8de1fee572713a5bb",
            "githubRepo": "https://github.com/s-nlp/PsiloQA",
            "ai_summary": "PsiloQA, a multilingual dataset with span-level hallucinations, enhances hallucination detection in large language models across 14 languages using an automated pipeline and encoder-based models.",
            "ai_keywords": [
                "large language models",
                "hallucination detection",
                "PsiloQA",
                "span-level hallucinations",
                "multilingual",
                "GPT-4o",
                "uncertainty quantification",
                "LLM-based tagging",
                "encoder models",
                "cross-lingual generalization",
                "knowledge transfer"
            ],
            "githubStars": 5,
            "organization": {
                "_id": "62a1fefca12f9cb8a15a5219",
                "name": "AIRI-Institute",
                "fullname": " AIRI - Artificial Intelligence Research Institute",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654783663739-62a1fdd62cfb273c7f41333e.png"
            }
        },
        "publishedAt": "2025-10-06T10:36:30.000Z",
        "title": "When Models Lie, We Learn: Multilingual Span-Level Hallucination\n  Detection with PsiloQA",
        "summary": "Hallucination detection remains a fundamental challenge for the safe and\nreliable deployment of large language models (LLMs), especially in applications\nrequiring factual accuracy. Existing hallucination benchmarks often operate at\nthe sequence level and are limited to English, lacking the fine-grained,\nmultilingual supervision needed for a comprehensive evaluation. In this work,\nwe introduce PsiloQA, a large-scale, multilingual dataset annotated with\nspan-level hallucinations across 14 languages. PsiloQA is constructed through\nan automated three-stage pipeline: generating question-answer pairs from\nWikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse\nLLMs in a no-context setting, and automatically annotating hallucinated spans\nusing GPT-4o by comparing against golden answers and retrieved context. We\nevaluate a wide range of hallucination detection methods -- including\nuncertainty quantification, LLM-based tagging, and fine-tuned encoder models --\nand show that encoder-based models achieve the strongest performance across\nlanguages. Furthermore, PsiloQA demonstrates effective cross-lingual\ngeneralization and supports robust knowledge transfer to other benchmarks, all\nwhile being significantly more cost-efficient than human-annotated datasets.\nOur dataset and results advance the development of scalable, fine-grained\nhallucination detection in multilingual settings.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/622b1f6b9f6139daa8e998ce/-3FpmnPusd2upOiU67a_J.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04849.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "622b1f6b9f6139daa8e998ce",
            "avatarUrl": "/avatars/842719c100a5969be75d04da97333675.svg",
            "fullname": "Vasily Konovalov",
            "name": "Vasily",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "62a1fefca12f9cb8a15a5219",
            "name": "AIRI-Institute",
            "fullname": " AIRI - Artificial Intelligence Research Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654783663739-62a1fdd62cfb273c7f41333e.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14545",
            "authors": [
                {
                    "_id": "68f1a44e6e0bef323a68fd02",
                    "user": {
                        "_id": "61cd4b833dd34ba1985e0753",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
                        "isPro": false,
                        "fullname": "KABI",
                        "user": "dongguanting",
                        "type": "user"
                    },
                    "name": "Guanting Dong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:10:43.546Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a44e6e0bef323a68fd03",
                    "user": {
                        "_id": "6690cf4c5d0a25eea5e3dfbc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690cf4c5d0a25eea5e3dfbc/zqY3kl0uZaXrOPJvfmRAn.jpeg",
                        "isPro": false,
                        "fullname": "baolicheng",
                        "user": "blc0910",
                        "type": "user"
                    },
                    "name": "Licheng Bao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:10:38.484Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a44e6e0bef323a68fd04",
                    "name": "Zhongyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68f1a44e6e0bef323a68fd05",
                    "name": "Kangzhi Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f1a44e6e0bef323a68fd06",
                    "name": "Xiaoxi Li",
                    "hidden": false
                },
                {
                    "_id": "68f1a44e6e0bef323a68fd07",
                    "name": "Jiajie Jin",
                    "hidden": false
                },
                {
                    "_id": "68f1a44e6e0bef323a68fd08",
                    "name": "Jinghan Yang",
                    "hidden": false
                },
                {
                    "_id": "68f1a44e6e0bef323a68fd09",
                    "name": "Hangyu Mao",
                    "hidden": false
                },
                {
                    "_id": "68f1a44e6e0bef323a68fd0a",
                    "name": "Fuzheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1a44e6e0bef323a68fd0b",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "68f1a44e6e0bef323a68fd0c",
                    "name": "Guorui Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f1a44e6e0bef323a68fd0d",
                    "name": "Yutao Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f1a44e6e0bef323a68fd0e",
                    "name": "Ji-Rong Wen",
                    "hidden": false
                },
                {
                    "_id": "68f1a44e6e0bef323a68fd0f",
                    "name": "Zhicheng Dou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T10:40:52.000Z",
            "submittedOnDailyAt": "2025-10-17T00:37:49.509Z",
            "title": "Agentic Entropy-Balanced Policy Optimization",
            "submittedOnDailyBy": {
                "_id": "61cd4b833dd34ba1985e0753",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
                "isPro": false,
                "fullname": "KABI",
                "user": "dongguanting",
                "type": "user"
            },
            "summary": "Recently, Agentic Reinforcement Learning (Agentic RL) has made significant\nprogress in incentivizing the multi-turn, long-horizon tool-use capabilities of\nweb agents. While mainstream agentic RL algorithms autonomously explore\nhigh-uncertainty tool-call steps under the guidance of entropy, excessive\nreliance on entropy signals can impose further constraints, leading to the\ntraining collapse. In this paper, we delve into the challenges caused by\nentropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an\nagentic RL algorithm designed to balance entropy in both the rollout and policy\nupdate phases. AEPO comprises two core components: (1) a dynamic\nentropy-balanced rollout mechanism that adaptively allocate global and branch\nsampling budget through entropy pre-monitoring, while imposing a branch penalty\non consecutive high-entropy tool-call steps to prevent over-branching issues;\nand (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient\noperation into the high-entropy clipping term to preserve and properly rescale\ngradients on high-entropy tokens, while incorporating entropy-aware advantage\nestimation to prioritize learning on high-uncertainty tokens. Results across 14\nchallenging datasets show that AEPO consistently outperforms 7 mainstream RL\nalgorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive\nresults: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker\nfor Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on\nWebWalker for Pass@5. Further analysis reveals that AEPO improves rollout\nsampling diversity while maintaining stable policy entropy, facilitating\nscalable web agent training.",
            "upvotes": 77,
            "discussionId": "68f1a44e6e0bef323a68fd10",
            "githubRepo": "https://github.com/RUC-NLPIR/ARPO/blob/main/",
            "ai_summary": "AEPO, an agentic RL algorithm, addresses entropy-related challenges in web agent training, enhancing performance and stability across various datasets.",
            "ai_keywords": [
                "Agentic Reinforcement Learning",
                "AEPO",
                "entropy",
                "rollout",
                "policy update",
                "dynamic entropy-balanced rollout",
                "branch penalty",
                "stop-gradient operation",
                "high-entropy clipping",
                "entropy-aware advantage estimation",
                "rollout sampling diversity",
                "policy entropy"
            ],
            "githubStars": 678,
            "organization": {
                "_id": "622177ac43826d6f261f8208",
                "name": "RUC",
                "fullname": "Renmin University of China",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
            }
        },
        "publishedAt": "2025-10-16T06:40:52.000Z",
        "title": "Agentic Entropy-Balanced Policy Optimization",
        "summary": "Recently, Agentic Reinforcement Learning (Agentic RL) has made significant\nprogress in incentivizing the multi-turn, long-horizon tool-use capabilities of\nweb agents. While mainstream agentic RL algorithms autonomously explore\nhigh-uncertainty tool-call steps under the guidance of entropy, excessive\nreliance on entropy signals can impose further constraints, leading to the\ntraining collapse. In this paper, we delve into the challenges caused by\nentropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an\nagentic RL algorithm designed to balance entropy in both the rollout and policy\nupdate phases. AEPO comprises two core components: (1) a dynamic\nentropy-balanced rollout mechanism that adaptively allocate global and branch\nsampling budget through entropy pre-monitoring, while imposing a branch penalty\non consecutive high-entropy tool-call steps to prevent over-branching issues;\nand (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient\noperation into the high-entropy clipping term to preserve and properly rescale\ngradients on high-entropy tokens, while incorporating entropy-aware advantage\nestimation to prioritize learning on high-uncertainty tokens. Results across 14\nchallenging datasets show that AEPO consistently outperforms 7 mainstream RL\nalgorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive\nresults: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker\nfor Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on\nWebWalker for Pass@5. Further analysis reveals that AEPO improves rollout\nsampling diversity while maintaining stable policy entropy, facilitating\nscalable web agent training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14545.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "61cd4b833dd34ba1985e0753",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
            "fullname": "KABI",
            "name": "dongguanting",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 49
        },
        "organization": {
            "_id": "622177ac43826d6f261f8208",
            "name": "RUC",
            "fullname": "Renmin University of China",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.14975",
            "authors": [
                {
                    "_id": "68f1a2616e0bef323a68fcd1",
                    "user": {
                        "_id": "634bde123d11eaedd889e277",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665916392312-noauth.png",
                        "isPro": false,
                        "fullname": "Hengyuan Xu",
                        "user": "DobyXu",
                        "type": "user"
                    },
                    "name": "Hengyuan Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T05:56:54.829Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a2616e0bef323a68fcd2",
                    "user": {
                        "_id": "64b914c8ace99c0723ad83a9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
                        "isPro": false,
                        "fullname": "Wei Cheng",
                        "user": "wchengad",
                        "type": "user"
                    },
                    "name": "Wei Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:11:13.677Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a2616e0bef323a68fcd3",
                    "name": "Peng Xing",
                    "hidden": false
                },
                {
                    "_id": "68f1a2616e0bef323a68fcd4",
                    "user": {
                        "_id": "647469b9a51711a3b58bda2b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647469b9a51711a3b58bda2b/yeDf8Sa8IDEQyney1dGC9.jpeg",
                        "isPro": false,
                        "fullname": "Yixiao Fang",
                        "user": "fangyixiao",
                        "type": "user"
                    },
                    "name": "Yixiao Fang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:11:11.270Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a2616e0bef323a68fcd5",
                    "name": "Shuhan Wu",
                    "hidden": false
                },
                {
                    "_id": "68f1a2616e0bef323a68fcd6",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "68f1a2616e0bef323a68fcd7",
                    "name": "Xianfang Zeng",
                    "hidden": false
                },
                {
                    "_id": "68f1a2616e0bef323a68fcd8",
                    "name": "Daxin Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f1a2616e0bef323a68fcd9",
                    "name": "Gang Yu",
                    "hidden": false
                },
                {
                    "_id": "68f1a2616e0bef323a68fcda",
                    "name": "Xingjun Ma",
                    "hidden": false
                },
                {
                    "_id": "68f1a2616e0bef323a68fcdb",
                    "name": "Yu-Gang Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:59:54.000Z",
            "submittedOnDailyAt": "2025-10-17T00:27:22.105Z",
            "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration.",
            "upvotes": 65,
            "discussionId": "68f1a2616e0bef323a68fcdc",
            "projectPage": "https://doby-xu.github.io/WithAnyone/",
            "githubRepo": "https://github.com/Doby-Xu/WithAnyone",
            "ai_summary": "A diffusion-based model addresses copy-paste artifacts in text-to-image generation by using a large-scale paired dataset and a contrastive identity loss to balance identity fidelity and variation.",
            "ai_keywords": [
                "identity-consistent generation",
                "text-to-image",
                "reconstruction-based training",
                "copy-paste",
                "MultiID-2M",
                "contrastive identity loss",
                "diffusion-based model",
                "identity fidelity",
                "variation",
                "perceptual quality"
            ],
            "githubStars": 102,
            "organization": {
                "_id": "66e43eae9d477f566f937935",
                "name": "stepfun-ai",
                "fullname": "StepFun",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
            }
        },
        "publishedAt": "2025-10-16T13:59:54.000Z",
        "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
        "summary": "Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14975.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 130
        },
        "organization": {
            "_id": "66e43eae9d477f566f937935",
            "name": "stepfun-ai",
            "fullname": "StepFun",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14359",
            "authors": [
                {
                    "_id": "68f1a6b16e0bef323a68fd1a",
                    "name": "Zichen Wen",
                    "hidden": false
                },
                {
                    "_id": "68f1a6b16e0bef323a68fd1b",
                    "name": "Yiyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6b16e0bef323a68fd1c",
                    "user": {
                        "_id": "6806464ed918f6d2fee2bc8b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6806464ed918f6d2fee2bc8b/rgpG2oO0m6PT0KltCF_Wf.jpeg",
                        "isPro": false,
                        "fullname": "Chenfei Liao",
                        "user": "Chenfei-Liao",
                        "type": "user"
                    },
                    "name": "Chenfei Liao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:34:38.296Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a6b16e0bef323a68fd1d",
                    "name": "Boxue Yang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6b16e0bef323a68fd1e",
                    "user": {
                        "_id": "656ae4088fb1ddf0d5ec9ac5",
                        "avatarUrl": "/avatars/e38468d2c0274f3c0f5732f30a2e3436.svg",
                        "isPro": false,
                        "fullname": "Junxian Li",
                        "user": "Duke-de-Artois",
                        "type": "user"
                    },
                    "name": "Junxian Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:10:29.247Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a6b16e0bef323a68fd1f",
                    "name": "Weifeng Liu",
                    "hidden": false
                },
                {
                    "_id": "68f1a6b16e0bef323a68fd20",
                    "name": "Haocong He",
                    "hidden": false
                },
                {
                    "_id": "68f1a6b16e0bef323a68fd21",
                    "user": {
                        "_id": "68d3f778e41c80eb120e52e0",
                        "avatarUrl": "/avatars/ad8de30deec15cf0f29134f3a1e73078.svg",
                        "isPro": false,
                        "fullname": "Bolong Feng",
                        "user": "Fabulous1496",
                        "type": "user"
                    },
                    "name": "Bolong Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:10:31.321Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a6b16e0bef323a68fd22",
                    "name": "Xuyang Liu",
                    "hidden": false
                },
                {
                    "_id": "68f1a6b16e0bef323a68fd23",
                    "name": "Yuanhuiyi Lyu",
                    "hidden": false
                },
                {
                    "_id": "68f1a6b16e0bef323a68fd24",
                    "name": "Xu Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f1a6b16e0bef323a68fd25",
                    "name": "Xuming Hu",
                    "hidden": false
                },
                {
                    "_id": "68f1a6b16e0bef323a68fd26",
                    "name": "Linfeng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T06:55:28.000Z",
            "submittedOnDailyAt": "2025-10-17T00:46:23.869Z",
            "title": "AI for Service: Proactive Assistance with AI Glasses",
            "submittedOnDailyBy": {
                "_id": "653b8c3e97a4d71d950e2f20",
                "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
                "isPro": false,
                "fullname": "Zichen Wen",
                "user": "zichenwen",
                "type": "user"
            },
            "summary": "In an era where AI is evolving from a passive tool into an active and\nadaptive companion, we introduce AI for Service (AI4Service), a new paradigm\nthat enables proactive and real-time assistance in daily life. Existing AI\nservices remain largely reactive, responding only to explicit user commands. We\nargue that a truly intelligent and helpful assistant should be capable of\nanticipating user needs and taking actions proactively when appropriate. To\nrealize this vision, we propose Alpha-Service, a unified framework that\naddresses two fundamental challenges: Know When to intervene by detecting\nservice opportunities from egocentric video streams, and Know How to provide\nboth generalized and personalized services. Inspired by the von Neumann\ncomputer architecture and based on AI glasses, Alpha-Service consists of five\nkey components: an Input Unit for perception, a Central Processing Unit for\ntask scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit\nfor long-term personalization, and an Output Unit for natural human\ninteraction. As an initial exploration, we implement Alpha-Service through a\nmulti-agent system deployed on AI glasses. Case studies, including a real-time\nBlackjack advisor, a museum tour guide, and a shopping fit assistant,\ndemonstrate its ability to seamlessly perceive the environment, infer user\nintent, and provide timely and useful assistance without explicit prompts.",
            "upvotes": 60,
            "discussionId": "68f1a6b16e0bef323a68fd27",
            "ai_summary": "Alpha-Service, a unified framework for proactive AI assistance, uses a multi-agent system on AI glasses to detect service opportunities and provide timely, personalized assistance.",
            "ai_keywords": [
                "AI4Service",
                "Alpha-Service",
                "egocentric video streams",
                "multi-agent system",
                "AI glasses",
                "Input Unit",
                "Central Processing Unit",
                "Arithmetic Logic Unit",
                "Memory Unit",
                "Output Unit",
                "natural human interaction"
            ],
            "organization": {
                "_id": "63e5ef7bf2e9a8f22c515654",
                "name": "SJTU",
                "fullname": "Shanghai Jiao Tong University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
            }
        },
        "publishedAt": "2025-10-16T02:55:28.000Z",
        "title": "AI for Service: Proactive Assistance with AI Glasses",
        "summary": "In an era where AI is evolving from a passive tool into an active and\nadaptive companion, we introduce AI for Service (AI4Service), a new paradigm\nthat enables proactive and real-time assistance in daily life. Existing AI\nservices remain largely reactive, responding only to explicit user commands. We\nargue that a truly intelligent and helpful assistant should be capable of\nanticipating user needs and taking actions proactively when appropriate. To\nrealize this vision, we propose Alpha-Service, a unified framework that\naddresses two fundamental challenges: Know When to intervene by detecting\nservice opportunities from egocentric video streams, and Know How to provide\nboth generalized and personalized services. Inspired by the von Neumann\ncomputer architecture and based on AI glasses, Alpha-Service consists of five\nkey components: an Input Unit for perception, a Central Processing Unit for\ntask scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit\nfor long-term personalization, and an Output Unit for natural human\ninteraction. As an initial exploration, we implement Alpha-Service through a\nmulti-agent system deployed on AI glasses. Case studies, including a real-time\nBlackjack advisor, a museum tour guide, and a shopping fit assistant,\ndemonstrate its ability to seamlessly perceive the environment, infer user\nintent, and provide timely and useful assistance without explicit prompts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14359.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653b8c3e97a4d71d950e2f20",
            "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
            "fullname": "Zichen Wen",
            "name": "zichenwen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "organization": {
            "_id": "63e5ef7bf2e9a8f22c515654",
            "name": "SJTU",
            "fullname": "Shanghai Jiao Tong University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14979",
            "authors": [
                {
                    "_id": "68f19c5d6e0bef323a68fc30",
                    "name": "Haiwen Diao",
                    "hidden": false
                },
                {
                    "_id": "68f19c5d6e0bef323a68fc31",
                    "name": "Mingxuan Li",
                    "hidden": false
                },
                {
                    "_id": "68f19c5d6e0bef323a68fc32",
                    "name": "Silei Wu",
                    "hidden": false
                },
                {
                    "_id": "68f19c5d6e0bef323a68fc33",
                    "name": "Linjun Dai",
                    "hidden": false
                },
                {
                    "_id": "68f19c5d6e0bef323a68fc34",
                    "name": "Xiaohua Wang",
                    "hidden": false
                },
                {
                    "_id": "68f19c5d6e0bef323a68fc35",
                    "name": "Hanming Deng",
                    "hidden": false
                },
                {
                    "_id": "68f19c5d6e0bef323a68fc36",
                    "name": "Lewei Lu",
                    "hidden": false
                },
                {
                    "_id": "68f19c5d6e0bef323a68fc37",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "68f19c5d6e0bef323a68fc38",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:59:58.000Z",
            "submittedOnDailyAt": "2025-10-17T00:18:30.032Z",
            "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale",
            "submittedOnDailyBy": {
                "_id": "64b4a717aa03b6520839e9b8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/Rt3ERG-6BVEA4hAwOz0_I.jpeg",
                "isPro": false,
                "fullname": "Haiwen Diao",
                "user": "Paranioar",
                "type": "user"
            },
            "summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs\napart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\nnative VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
            "upvotes": 51,
            "discussionId": "68f19c5e6e0bef323a68fc39",
            "githubRepo": "https://github.com/EvolvingLMMs-Lab/NEO",
            "ai_summary": "NEO, a novel family of native Vision-Language Models, addresses fundamental constraints and integrates vision and language within a unified framework, achieving competitive performance with limited data.",
            "ai_keywords": [
                "Vision-Language Models",
                "native VLMs",
                "modular VLMs",
                "pixel and word representations",
                "semantic space",
                "cross-modal properties",
                "visual perception",
                "vision-language conflicts",
                "monolithic model",
                "reusable components"
            ],
            "githubStars": 49,
            "organization": {
                "_id": "63203b617196f93bc94b73a2",
                "name": "SenseTime",
                "fullname": "SenseTime",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/No33gl22RKB0HXjo-qpLX.png"
            }
        },
        "publishedAt": "2025-10-16T13:59:58.000Z",
        "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale",
        "summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs\napart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\nnative VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14979.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b4a717aa03b6520839e9b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/Rt3ERG-6BVEA4hAwOz0_I.jpeg",
            "fullname": "Haiwen Diao",
            "name": "Paranioar",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "63203b617196f93bc94b73a2",
            "name": "SenseTime",
            "fullname": "SenseTime",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/No33gl22RKB0HXjo-qpLX.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14847",
            "authors": [
                {
                    "_id": "68f1d8076e0bef323a68ffd3",
                    "name": "Meiqi Wu",
                    "hidden": false
                },
                {
                    "_id": "68f1d8076e0bef323a68ffd4",
                    "user": {
                        "_id": "6682775501c30ad93ec5e500",
                        "avatarUrl": "/avatars/971ee2028589f6089559306b40a58da0.svg",
                        "isPro": false,
                        "fullname": "Jiashu Zhu",
                        "user": "Jiashuz",
                        "type": "user"
                    },
                    "name": "Jiashu Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T05:56:13.761Z",
                    "hidden": false
                },
                {
                    "_id": "68f1d8076e0bef323a68ffd5",
                    "name": "Xiaokun Feng",
                    "hidden": false
                },
                {
                    "_id": "68f1d8076e0bef323a68ffd6",
                    "name": "Chubin Chen",
                    "hidden": false
                },
                {
                    "_id": "68f1d8076e0bef323a68ffd7",
                    "name": "Chen Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f1d8076e0bef323a68ffd8",
                    "name": "Bingze Song",
                    "hidden": false
                },
                {
                    "_id": "68f1d8076e0bef323a68ffd9",
                    "name": "Fangyuan Mao",
                    "hidden": false
                },
                {
                    "_id": "68f1d8076e0bef323a68ffda",
                    "name": "Jiahong Wu",
                    "hidden": false
                },
                {
                    "_id": "68f1d8076e0bef323a68ffdb",
                    "name": "Xiangxiang Chu",
                    "hidden": false
                },
                {
                    "_id": "68f1d8076e0bef323a68ffdc",
                    "name": "Kaiqi Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T16:19:13.000Z",
            "submittedOnDailyAt": "2025-10-17T05:35:39.801Z",
            "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond\n  Semantic Dependency Constraints",
            "submittedOnDailyBy": {
                "_id": "66d255e3947594430c723ff6",
                "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
                "isPro": false,
                "fullname": "xiaochonglinghu",
                "user": "xiaochonglinghu",
                "type": "user"
            },
            "summary": "Video generation models have achieved remarkable progress, particularly\nexcelling in realistic scenarios; however, their performance degrades notably\nin imaginative scenarios. These prompts often involve rarely co-occurring\nconcepts with long-distance semantic relationships, falling outside training\ndistributions. Existing methods typically apply test-time scaling for improving\nvideo quality, but their fixed search spaces and static reward designs limit\nadaptability to imaginative scenarios. To fill this gap, we propose\nImagerySearch, a prompt-guided adaptive test-time search strategy that\ndynamically adjusts both the inference search space and reward function\naccording to semantic relationships in the prompt. This enables more coherent\nand visually plausible videos in challenging imaginative settings. To evaluate\nprogress in this direction, we introduce LDT-Bench, the first dedicated\nbenchmark for long-distance semantic prompts, consisting of 2,839 diverse\nconcept pairs and an automated protocol for assessing creative generation\ncapabilities. Extensive experiments show that ImagerySearch consistently\noutperforms strong video generation baselines and existing test-time scaling\napproaches on LDT-Bench, and achieves competitive improvements on VBench,\ndemonstrating its effectiveness across diverse prompt types. We will release\nLDT-Bench and code to facilitate future research on imaginative video\ngeneration.",
            "upvotes": 46,
            "discussionId": "68f1d8076e0bef323a68ffdd",
            "githubRepo": "https://github.com/AMAP-ML/ImagerySearch",
            "ai_summary": "ImagerySearch, a prompt-guided adaptive test-time search strategy, enhances video generation in imaginative scenarios by dynamically adjusting search spaces and reward functions, outperforming existing methods on a new benchmark, LDT-Bench.",
            "ai_keywords": [
                "prompt-guided",
                "adaptive test-time search",
                "inference search space",
                "reward function",
                "long-distance semantic prompts",
                "LDT-Bench",
                "VBench",
                "creative generation capabilities"
            ],
            "githubStars": 46,
            "organization": {
                "_id": "67d11771890254196d3174e5",
                "name": "GD-ML",
                "fullname": "AMAP-ML",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"
            }
        },
        "publishedAt": "2025-10-16T12:19:13.000Z",
        "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond\n  Semantic Dependency Constraints",
        "summary": "Video generation models have achieved remarkable progress, particularly\nexcelling in realistic scenarios; however, their performance degrades notably\nin imaginative scenarios. These prompts often involve rarely co-occurring\nconcepts with long-distance semantic relationships, falling outside training\ndistributions. Existing methods typically apply test-time scaling for improving\nvideo quality, but their fixed search spaces and static reward designs limit\nadaptability to imaginative scenarios. To fill this gap, we propose\nImagerySearch, a prompt-guided adaptive test-time search strategy that\ndynamically adjusts both the inference search space and reward function\naccording to semantic relationships in the prompt. This enables more coherent\nand visually plausible videos in challenging imaginative settings. To evaluate\nprogress in this direction, we introduce LDT-Bench, the first dedicated\nbenchmark for long-distance semantic prompts, consisting of 2,839 diverse\nconcept pairs and an automated protocol for assessing creative generation\ncapabilities. Extensive experiments show that ImagerySearch consistently\noutperforms strong video generation baselines and existing test-time scaling\napproaches on LDT-Bench, and achieves competitive improvements on VBench,\ndemonstrating its effectiveness across diverse prompt types. We will release\nLDT-Bench and code to facilitate future research on imaginative video\ngeneration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14847.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "fullname": "xiaochonglinghu",
            "name": "xiaochonglinghu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "67d11771890254196d3174e5",
            "name": "GD-ML",
            "fullname": "AMAP-ML",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14967",
            "authors": [
                {
                    "_id": "68f1a04a6e0bef323a68fcac",
                    "name": "Guoqing Wang",
                    "hidden": false
                },
                {
                    "_id": "68f1a04a6e0bef323a68fcad",
                    "name": "Sunhao Dai",
                    "hidden": false
                },
                {
                    "_id": "68f1a04a6e0bef323a68fcae",
                    "name": "Guangze Ye",
                    "hidden": false
                },
                {
                    "_id": "68f1a04a6e0bef323a68fcaf",
                    "name": "Zeyu Gan",
                    "hidden": false
                },
                {
                    "_id": "68f1a04a6e0bef323a68fcb0",
                    "name": "Wei Yao",
                    "hidden": false
                },
                {
                    "_id": "68f1a04a6e0bef323a68fcb1",
                    "name": "Yong Deng",
                    "hidden": false
                },
                {
                    "_id": "68f1a04a6e0bef323a68fcb2",
                    "name": "Xiaofeng Wu",
                    "hidden": false
                },
                {
                    "_id": "68f1a04a6e0bef323a68fcb3",
                    "name": "Zhenzhe Ying",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:59:32.000Z",
            "submittedOnDailyAt": "2025-10-17T00:19:11.288Z",
            "title": "Information Gain-based Policy Optimization: A Simple and Effective\n  Approach for Multi-Turn LLM Agents",
            "submittedOnDailyBy": {
                "_id": "64db88993725f8d9a908c077",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
                "isPro": false,
                "fullname": "Sunhao Dai",
                "user": "KID-22",
                "type": "user"
            },
            "summary": "Large language model (LLM)-based agents are increasingly trained with\nreinforcement learning (RL) to enhance their ability to interact with external\nenvironments through tool use, particularly in search-based settings that\nrequire multi-turn reasoning and knowledge acquisition. However, existing\napproaches typically rely on outcome-based rewards that are only provided at\nthe final answer. This reward sparsity becomes particularly problematic in\nmulti-turn settings, where long trajectories exacerbate two critical issues:\n(i) advantage collapse, where all rollouts receive identical rewards and\nprovide no useful learning signals, and (ii) lack of fine-grained credit\nassignment, where dependencies between turns are obscured, especially in\nlong-horizon tasks. In this paper, we propose Information Gain-based Policy\nOptimization (IGPO), a simple yet effective RL framework that provides dense\nand intrinsic supervision for multi-turn agent training. IGPO models each\ninteraction turn as an incremental process of acquiring information about the\nground truth, and defines turn-level rewards as the marginal increase in the\npolicy's probability of producing the correct answer. Unlike prior\nprocess-level reward approaches that depend on external reward models or costly\nMonte Carlo estimation, IGPO derives intrinsic rewards directly from the\nmodel's own belief updates. These intrinsic turn-level rewards are combined\nwith outcome-level supervision to form dense reward trajectories. Extensive\nexperiments on both in-domain and out-of-domain benchmarks demonstrate that\nIGPO consistently outperforms strong baselines in multi-turn scenarios,\nachieving higher accuracy and improved sample efficiency.",
            "upvotes": 30,
            "discussionId": "68f1a04a6e0bef323a68fcb4",
            "githubRepo": "https://github.com/GuoqingWang1/IGPO",
            "ai_summary": "Information Gain-based Policy Optimization (IGPO) enhances multi-turn reasoning in large language models by providing dense intrinsic rewards derived from the model's belief updates, improving accuracy and sample efficiency.",
            "ai_keywords": [
                "reinforcement learning",
                "large language model",
                "tool use",
                "multi-turn reasoning",
                "knowledge acquisition",
                "reward sparsity",
                "advantage collapse",
                "fine-grained credit assignment",
                "Information Gain-based Policy Optimization",
                "intrinsic rewards",
                "belief updates",
                "dense reward trajectories",
                "sample efficiency"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "67c1d682826160b28f778510",
                "name": "antgroup",
                "fullname": "Ant Group",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
            }
        },
        "publishedAt": "2025-10-16T13:59:32.000Z",
        "title": "Information Gain-based Policy Optimization: A Simple and Effective\n  Approach for Multi-Turn LLM Agents",
        "summary": "Large language model (LLM)-based agents are increasingly trained with\nreinforcement learning (RL) to enhance their ability to interact with external\nenvironments through tool use, particularly in search-based settings that\nrequire multi-turn reasoning and knowledge acquisition. However, existing\napproaches typically rely on outcome-based rewards that are only provided at\nthe final answer. This reward sparsity becomes particularly problematic in\nmulti-turn settings, where long trajectories exacerbate two critical issues:\n(i) advantage collapse, where all rollouts receive identical rewards and\nprovide no useful learning signals, and (ii) lack of fine-grained credit\nassignment, where dependencies between turns are obscured, especially in\nlong-horizon tasks. In this paper, we propose Information Gain-based Policy\nOptimization (IGPO), a simple yet effective RL framework that provides dense\nand intrinsic supervision for multi-turn agent training. IGPO models each\ninteraction turn as an incremental process of acquiring information about the\nground truth, and defines turn-level rewards as the marginal increase in the\npolicy's probability of producing the correct answer. Unlike prior\nprocess-level reward approaches that depend on external reward models or costly\nMonte Carlo estimation, IGPO derives intrinsic rewards directly from the\nmodel's own belief updates. These intrinsic turn-level rewards are combined\nwith outcome-level supervision to form dense reward trajectories. Extensive\nexperiments on both in-domain and out-of-domain benchmarks demonstrate that\nIGPO consistently outperforms strong baselines in multi-turn scenarios,\nachieving higher accuracy and improved sample efficiency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14967.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64db88993725f8d9a908c077",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
            "fullname": "Sunhao Dai",
            "name": "KID-22",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "67c1d682826160b28f778510",
            "name": "antgroup",
            "fullname": "Ant Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14943",
            "authors": [
                {
                    "_id": "68f19d716e0bef323a68fc51",
                    "user": {
                        "_id": "64b7df742f5a966b973e25f7",
                        "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
                        "isPro": false,
                        "fullname": "Wenkai Yang",
                        "user": "Keven16",
                        "type": "user"
                    },
                    "name": "Wenkai Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:11:28.203Z",
                    "hidden": false
                },
                {
                    "_id": "68f19d716e0bef323a68fc52",
                    "name": "Weijie Liu",
                    "hidden": false
                },
                {
                    "_id": "68f19d716e0bef323a68fc53",
                    "name": "Ruobing Xie",
                    "hidden": false
                },
                {
                    "_id": "68f19d716e0bef323a68fc54",
                    "name": "Yiju Guo",
                    "hidden": false
                },
                {
                    "_id": "68f19d716e0bef323a68fc55",
                    "name": "Lulu Wu",
                    "hidden": false
                },
                {
                    "_id": "68f19d716e0bef323a68fc56",
                    "name": "Saiyong Yang",
                    "hidden": false
                },
                {
                    "_id": "68f19d716e0bef323a68fc57",
                    "name": "Yankai Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:55:11.000Z",
            "submittedOnDailyAt": "2025-10-17T00:25:17.471Z",
            "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
            "submittedOnDailyBy": {
                "_id": "64b7df742f5a966b973e25f7",
                "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
                "isPro": false,
                "fullname": "Wenkai Yang",
                "user": "Keven16",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na core paradigm for enhancing the reasoning capabilities of Large Language\nModels (LLMs). To address the lack of verification signals at test time, prior\nstudies incorporate the training of model's self-verification capability into\nthe standard RLVR process, thereby unifying reasoning and verification\ncapabilities within a single LLM. However, previous practice requires the LLM\nto sequentially generate solutions and self-verifications using two separate\nprompt templates, which significantly reduces efficiency. In this work, we\ntheoretically reveal that the closed-form solution to the RL objective of\nself-verification can be reduced to a remarkably simple form: the true\nreasoning reward of a solution is equal to its last-token self-rewarding score,\nwhich is computed as the difference between the policy model's next-token\nlog-probability assigned to any pre-specified token at the solution's last\ntoken and a pre-calculated constant, scaled by the KL coefficient. Based on\nthis insight, we propose LaSeR (Reinforcement Learning with Last-Token\nSelf-Rewarding), an algorithm that simply augments the original RLVR loss with\na MSE loss that aligns the last-token self-rewarding scores with verifier-based\nreasoning rewards, jointly optimizing the reasoning and self-rewarding\ncapabilities of LLMs. The optimized self-rewarding scores can be utilized in\nboth training and testing to enhance model performance. Notably, our algorithm\nderives these scores from the predicted next-token probability distribution of\nthe last token immediately after generation, incurring only the minimal extra\ncost of one additional token inference. Experiments show that our method not\nonly improves the model's reasoning performance but also equips it with\nremarkable self-rewarding capability, thereby boosting its inference-time\nscaling performance.",
            "upvotes": 30,
            "discussionId": "68f19d716e0bef323a68fc58",
            "githubRepo": "https://github.com/RUCBM/LaSeR",
            "ai_summary": "LaSeR, a reinforcement learning algorithm, enhances Large Language Models by aligning last-token self-rewarding scores with verifier-based reasoning rewards, improving reasoning performance and inference-time scaling.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards",
                "Large Language Models",
                "self-verification",
                "RLVR",
                "last-token self-rewarding",
                "policy model",
                "next-token log-probability",
                "KL coefficient",
                "MSE loss",
                "reasoning performance",
                "inference-time scaling"
            ],
            "githubStars": 5,
            "organization": {
                "_id": "6645f953c39288df638dbdd5",
                "name": "Tencent-Hunyuan",
                "fullname": "Tencent Hunyuan",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
            }
        },
        "publishedAt": "2025-10-16T13:55:11.000Z",
        "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na core paradigm for enhancing the reasoning capabilities of Large Language\nModels (LLMs). To address the lack of verification signals at test time, prior\nstudies incorporate the training of model's self-verification capability into\nthe standard RLVR process, thereby unifying reasoning and verification\ncapabilities within a single LLM. However, previous practice requires the LLM\nto sequentially generate solutions and self-verifications using two separate\nprompt templates, which significantly reduces efficiency. In this work, we\ntheoretically reveal that the closed-form solution to the RL objective of\nself-verification can be reduced to a remarkably simple form: the true\nreasoning reward of a solution is equal to its last-token self-rewarding score,\nwhich is computed as the difference between the policy model's next-token\nlog-probability assigned to any pre-specified token at the solution's last\ntoken and a pre-calculated constant, scaled by the KL coefficient. Based on\nthis insight, we propose LaSeR (Reinforcement Learning with Last-Token\nSelf-Rewarding), an algorithm that simply augments the original RLVR loss with\na MSE loss that aligns the last-token self-rewarding scores with verifier-based\nreasoning rewards, jointly optimizing the reasoning and self-rewarding\ncapabilities of LLMs. The optimized self-rewarding scores can be utilized in\nboth training and testing to enhance model performance. Notably, our algorithm\nderives these scores from the predicted next-token probability distribution of\nthe last token immediately after generation, incurring only the minimal extra\ncost of one additional token inference. Experiments show that our method not\nonly improves the model's reasoning performance but also equips it with\nremarkable self-rewarding capability, thereby boosting its inference-time\nscaling performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14943.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b7df742f5a966b973e25f7",
            "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
            "fullname": "Wenkai Yang",
            "name": "Keven16",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "6645f953c39288df638dbdd5",
            "name": "Tencent-Hunyuan",
            "fullname": "Tencent Hunyuan",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.14972",
            "authors": [
                {
                    "_id": "68f19f5f6e0bef323a68fc99",
                    "user": {
                        "_id": "675764b1a55640463e079271",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6EMvM7M8uP9K7EeVMqAz-.png",
                        "isPro": false,
                        "fullname": "Yinxi Li",
                        "user": "Yinxxx",
                        "type": "user"
                    },
                    "name": "Yinxi Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:11:23.022Z",
                    "hidden": false
                },
                {
                    "_id": "68f19f5f6e0bef323a68fc9a",
                    "user": {
                        "_id": "63081e15a670ed10f9d44229",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
                        "isPro": true,
                        "fullname": "Yuntian Deng",
                        "user": "yuntian-deng",
                        "type": "user"
                    },
                    "name": "Yuntian Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T05:56:56.808Z",
                    "hidden": false
                },
                {
                    "_id": "68f19f5f6e0bef323a68fc9b",
                    "user": {
                        "_id": "6643bdd9bc86c9465b551357",
                        "avatarUrl": "/avatars/98f04dc2263e79f170e662a9ed113fc5.svg",
                        "isPro": false,
                        "fullname": "Pengyu Nie",
                        "user": "pengyunie",
                        "type": "user"
                    },
                    "name": "Pengyu Nie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:11:25.906Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6643bdd9bc86c9465b551357/Zsnh3cU3Ix3vn65geRT0r.gif"
            ],
            "publishedAt": "2025-10-16T17:59:45.000Z",
            "submittedOnDailyAt": "2025-10-17T01:35:08.569Z",
            "title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar",
            "submittedOnDailyBy": {
                "_id": "6643bdd9bc86c9465b551357",
                "avatarUrl": "/avatars/98f04dc2263e79f170e662a9ed113fc5.svg",
                "isPro": false,
                "fullname": "Pengyu Nie",
                "user": "pengyunie",
                "type": "user"
            },
            "summary": "Large language models (LLMs) for code rely on subword tokenizers, such as\nbyte-pair encoding (BPE), learned from mixed natural language text and\nprogramming language code but driven by statistics rather than grammar. As a\nresult, semantically identical code snippets can be tokenized differently\ndepending on superficial factors such as whitespace or identifier naming. To\nmeasure the impact of this misalignment, we introduce TokDrift, a framework\nthat applies semantic-preserving rewrite rules to create code variants\ndiffering only in tokenization. Across nine code LLMs, including large ones\nwith over 30B parameters, even minor formatting changes can cause substantial\nshifts in model behavior. Layer-wise analysis shows that the issue originates\nin early embeddings, where subword segmentation fails to capture grammar token\nboundaries. Our findings identify misaligned tokenization as a hidden obstacle\nto reliable code understanding and generation, highlighting the need for\ngrammar-aware tokenization for future code LLMs.",
            "upvotes": 26,
            "discussionId": "68f19f606e0bef323a68fc9c",
            "githubRepo": "https://github.com/uw-swag/tokdrift",
            "ai_summary": "Misaligned tokenization in large language models for code leads to inconsistent model behavior, necessitating grammar-aware tokenization.",
            "ai_keywords": [
                "large language models",
                "code LLMs",
                "subword tokenizers",
                "byte-pair encoding",
                "BPE",
                "semantic-preserving rewrite rules",
                "TokDrift",
                "tokenization",
                "early embeddings",
                "grammar token boundaries",
                "code understanding",
                "code generation"
            ],
            "githubStars": 4,
            "organization": {
                "_id": "6230c3ced93e84e2338765f3",
                "name": "UWaterloo",
                "fullname": "University of Waterloo",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1647363004040-6230c34bacb94a81eec91ed5.png"
            }
        },
        "publishedAt": "2025-10-16T13:59:45.000Z",
        "title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar",
        "summary": "Large language models (LLMs) for code rely on subword tokenizers, such as\nbyte-pair encoding (BPE), learned from mixed natural language text and\nprogramming language code but driven by statistics rather than grammar. As a\nresult, semantically identical code snippets can be tokenized differently\ndepending on superficial factors such as whitespace or identifier naming. To\nmeasure the impact of this misalignment, we introduce TokDrift, a framework\nthat applies semantic-preserving rewrite rules to create code variants\ndiffering only in tokenization. Across nine code LLMs, including large ones\nwith over 30B parameters, even minor formatting changes can cause substantial\nshifts in model behavior. Layer-wise analysis shows that the issue originates\nin early embeddings, where subword segmentation fails to capture grammar token\nboundaries. Our findings identify misaligned tokenization as a hidden obstacle\nto reliable code understanding and generation, highlighting the need for\ngrammar-aware tokenization for future code LLMs.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6643bdd9bc86c9465b551357/Zsnh3cU3Ix3vn65geRT0r.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14972.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6643bdd9bc86c9465b551357",
            "avatarUrl": "/avatars/98f04dc2263e79f170e662a9ed113fc5.svg",
            "fullname": "Pengyu Nie",
            "name": "pengyunie",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "6230c3ced93e84e2338765f3",
            "name": "UWaterloo",
            "fullname": "University of Waterloo",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1647363004040-6230c34bacb94a81eec91ed5.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.13998",
            "authors": [
                {
                    "_id": "68f1b1f06e0bef323a68fde5",
                    "name": "Xun Wu",
                    "hidden": false
                },
                {
                    "_id": "68f1b1f06e0bef323a68fde6",
                    "name": "Shaohan Huang",
                    "hidden": false
                },
                {
                    "_id": "68f1b1f06e0bef323a68fde7",
                    "name": "Wenhui Wang",
                    "hidden": false
                },
                {
                    "_id": "68f1b1f06e0bef323a68fde8",
                    "name": "Ting Song",
                    "hidden": false
                },
                {
                    "_id": "68f1b1f06e0bef323a68fde9",
                    "name": "Li Dong",
                    "hidden": false
                },
                {
                    "_id": "68f1b1f06e0bef323a68fdea",
                    "name": "Yan Xia",
                    "hidden": false
                },
                {
                    "_id": "68f1b1f06e0bef323a68fdeb",
                    "name": "Furu Wei",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/632bd2f72d6a805eeb4bc601/DDH8WmVsFnYPWUzk8tXdy.png"
            ],
            "publishedAt": "2025-10-15T18:28:12.000Z",
            "submittedOnDailyAt": "2025-10-17T01:34:38.483Z",
            "title": "BitNet Distillation",
            "submittedOnDailyBy": {
                "_id": "632bd2f72d6a805eeb4bc601",
                "avatarUrl": "/avatars/6e1533e8a599f3068290aa69ac82cab7.svg",
                "isPro": false,
                "fullname": "HUANG SHAOHAN",
                "user": "buaahsh",
                "type": "user"
            },
            "summary": "In this paper, we present BitNet Distillation (BitDistill), a lightweight\npipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into\n1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream\ntasks, achieving strong task-specific performance with minimal computational\ncost. Specifically, BitDistill incorporates three key techniques: the SubLN\nmodule, as introduced in BitNet; multi-head attention distillation, based on\nMiniLM; and continual pre-training, which serves as a crucial warm-up step to\nmitigate the scalability issue of the performance gap between finetuned\nfull-precision and 1.58-bit LLMs on specific tasks. Experimental results show\nthat BitDistill achieves performance comparable to the full-precision\ncounterpart models across model size, while enabling up to 10x memory savings\nand 2.65x faster inference on CPUs. Code is available at\nhttps://github.com/microsoft/BitNet.",
            "upvotes": 26,
            "discussionId": "68f1b1f06e0bef323a68fdec",
            "githubRepo": "https://github.com/microsoft/BitNet",
            "ai_summary": "BitNet Distillation fine-tunes large language models to 1.58-bit precision using SubLN, multi-head attention distillation, and continual pre-training, achieving comparable performance with significant memory and inference speed improvements.",
            "ai_keywords": [
                "BitNet Distillation",
                "BitDistill",
                "SubLN",
                "multi-head attention distillation",
                "continual pre-training",
                "LLMs",
                "Qwen",
                "ternary weights",
                "memory savings",
                "inference speed"
            ],
            "githubStars": 24205,
            "organization": {
                "_id": "68151d0f51add3813f3f7d1b",
                "name": "MicrosoftResearch",
                "fullname": "Microsoft Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
            }
        },
        "publishedAt": "2025-10-15T14:28:12.000Z",
        "title": "BitNet Distillation",
        "summary": "In this paper, we present BitNet Distillation (BitDistill), a lightweight\npipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into\n1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream\ntasks, achieving strong task-specific performance with minimal computational\ncost. Specifically, BitDistill incorporates three key techniques: the SubLN\nmodule, as introduced in BitNet; multi-head attention distillation, based on\nMiniLM; and continual pre-training, which serves as a crucial warm-up step to\nmitigate the scalability issue of the performance gap between finetuned\nfull-precision and 1.58-bit LLMs on specific tasks. Experimental results show\nthat BitDistill achieves performance comparable to the full-precision\ncounterpart models across model size, while enabling up to 10x memory savings\nand 2.65x faster inference on CPUs. Code is available at\nhttps://github.com/microsoft/BitNet.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/632bd2f72d6a805eeb4bc601/DDH8WmVsFnYPWUzk8tXdy.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13998.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "632bd2f72d6a805eeb4bc601",
            "avatarUrl": "/avatars/6e1533e8a599f3068290aa69ac82cab7.svg",
            "fullname": "HUANG SHAOHAN",
            "name": "buaahsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "organization": {
            "_id": "68151d0f51add3813f3f7d1b",
            "name": "MicrosoftResearch",
            "fullname": "Microsoft Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14973",
            "authors": [
                {
                    "_id": "68f1a7886e0bef323a68fd6a",
                    "user": {
                        "_id": "651517d381ce32e6e142a10e",
                        "avatarUrl": "/avatars/1e87fddecf913a75b6c24fca08f7cd5b.svg",
                        "isPro": false,
                        "fullname": "Nguyen Tri Quan",
                        "user": "NguyenTriQuan",
                        "type": "user"
                    },
                    "name": "Quan Nguyen-Tri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:10:24.539Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a7886e0bef323a68fd6b",
                    "user": {
                        "_id": "65262a396b41932089fd7bae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
                        "isPro": true,
                        "fullname": "Mukul Ranjan",
                        "user": "mukul54",
                        "type": "user"
                    },
                    "name": "Mukul Ranjan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:10:26.806Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a7886e0bef323a68fd6c",
                    "name": "Zhiqiang Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:59:48.000Z",
            "submittedOnDailyAt": "2025-10-17T01:02:05.861Z",
            "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
            "submittedOnDailyBy": {
                "_id": "65262a396b41932089fd7bae",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
                "isPro": true,
                "fullname": "Mukul Ranjan",
                "user": "mukul54",
                "type": "user"
            },
            "summary": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant {bf MASK} tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n{bf Elastic-Cache}, a training-free, architecture-agnostic strategy that\njointly decides {when} to refresh (via an attention-aware drift test on the\nmost-attended token) and {where} to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: 8.7times on GSM8K (256 tokens), 45.1times on longer sequences,\nand 4.8times on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n(6.8times on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.",
            "upvotes": 24,
            "discussionId": "68f1a7886e0bef323a68fd6d",
            "projectPage": "https://vila-lab.github.io/elastic-cache-webpage/",
            "ai_summary": "Elastic-Cache optimizes key-value cache management in diffusion large language models to reduce decoding latency without sacrificing prediction accuracy.",
            "ai_keywords": [
                "diffusion large language models",
                "KV caches",
                "QKV",
                "denoising steps",
                "attention-aware drift test",
                "depth-aware schedule",
                "Elastic-Cache",
                "adaptive cache updates",
                "generation quality",
                "throughput"
            ],
            "organization": {
                "_id": "61fb9e24dc607a42af5f193f",
                "name": "MBZUAI",
                "fullname": "Mohamed Bin Zayed University of Artificial Intelligence",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"
            }
        },
        "publishedAt": "2025-10-16T13:59:48.000Z",
        "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
        "summary": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant {bf MASK} tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n{bf Elastic-Cache}, a training-free, architecture-agnostic strategy that\njointly decides {when} to refresh (via an attention-aware drift test on the\nmost-attended token) and {where} to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: 8.7times on GSM8K (256 tokens), 45.1times on longer sequences,\nand 4.8times on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n(6.8times on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14973.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65262a396b41932089fd7bae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
            "fullname": "Mukul Ranjan",
            "name": "mukul54",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "61fb9e24dc607a42af5f193f",
            "name": "MBZUAI",
            "fullname": "Mohamed Bin Zayed University of Artificial Intelligence",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.14528",
            "authors": [
                {
                    "_id": "68f1a43f6e0bef323a68fcee",
                    "name": "Cheng Cui",
                    "hidden": false
                },
                {
                    "_id": "68f1a43f6e0bef323a68fcef",
                    "user": {
                        "_id": "63d7b8ee07cd1aa3c49a2026",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d7b8ee07cd1aa3c49a2026/VdFETZ7zQKwkQhhahzW3i.jpeg",
                        "isPro": false,
                        "fullname": "Ting Sun",
                        "user": "sunflowerting78",
                        "type": "user"
                    },
                    "name": "Ting Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:10:51.310Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a43f6e0bef323a68fcf0",
                    "name": "Suyin Liang",
                    "hidden": false
                },
                {
                    "_id": "68f1a43f6e0bef323a68fcf1",
                    "user": {
                        "_id": "681c1ecd9539bdde5ae1733c",
                        "avatarUrl": "/avatars/23322876ba0812bdabc288cc2381a776.svg",
                        "isPro": false,
                        "fullname": "Tingquan Gao",
                        "user": "Tingquan",
                        "type": "user"
                    },
                    "name": "Tingquan Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:10:58.135Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a43f6e0bef323a68fcf2",
                    "name": "Zelun Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1a43f6e0bef323a68fcf3",
                    "name": "Jiaxuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68f1a43f6e0bef323a68fcf4",
                    "name": "Xueqing Wang",
                    "hidden": false
                },
                {
                    "_id": "68f1a43f6e0bef323a68fcf5",
                    "user": {
                        "_id": "68c23142fc7c68f3c1be9788",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68c23142fc7c68f3c1be9788/jDAZ0p5tDC9M1MOvhHY0J.png",
                        "isPro": false,
                        "fullname": "chagndazhou",
                        "user": "vvvic0415",
                        "type": "user"
                    },
                    "name": "Changda Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:10:53.661Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a43f6e0bef323a68fcf6",
                    "user": {
                        "_id": "68493f0616e67d38f02f138a",
                        "avatarUrl": "/avatars/f0ece4f914440fd652ef0ff935f61c9c.svg",
                        "isPro": false,
                        "fullname": "Liu",
                        "user": "Hongen1",
                        "type": "user"
                    },
                    "name": "Hongen Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:10:49.116Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a43f6e0bef323a68fcf7",
                    "user": {
                        "_id": "67d96e68939c3823ab2e06a5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FJx_Nf_5aD1w1s0RqbyGn.png",
                        "isPro": false,
                        "fullname": "Manhui Lin",
                        "user": "gggdddfff",
                        "type": "user"
                    },
                    "name": "Manhui Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:10:55.920Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a43f6e0bef323a68fcf8",
                    "user": {
                        "_id": "68c23ee406c83a13a583843f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0Sgn8aro1cQeLlY9G7ju9.png",
                        "isPro": false,
                        "fullname": "Yue Zhang",
                        "user": "xiaohei66",
                        "type": "user"
                    },
                    "name": "Yue Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:10:46.078Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a43f6e0bef323a68fcf9",
                    "name": "Yubo Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1a43f6e0bef323a68fcfa",
                    "name": "Handong Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f1a43f6e0bef323a68fcfb",
                    "user": {
                        "_id": "652b2e9166313ebb6197e706",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652b2e9166313ebb6197e706/0qAVPmsc_fRp8OmlaC2S2.png",
                        "isPro": false,
                        "fullname": "AlexZhang",
                        "user": "AlexTransformer",
                        "type": "user"
                    },
                    "name": "Jing Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:11:00.280Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a43f6e0bef323a68fcfc",
                    "user": {
                        "_id": "64f187a2cc1c03340ac30498",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f187a2cc1c03340ac30498/dMTUFA5Ul35v595JPKCMw.jpeg",
                        "isPro": false,
                        "fullname": "Jun Zhang",
                        "user": "jzhang533",
                        "type": "user"
                    },
                    "name": "Jun Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:11:02.869Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a43f6e0bef323a68fcfd",
                    "user": {
                        "_id": "6501403fea5699b59abacf15",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6501403fea5699b59abacf15/Sh9ouC68HWHsF1Jh_0CGi.jpeg",
                        "isPro": false,
                        "fullname": "Yi Liu",
                        "user": "michaelowenliu",
                        "type": "user"
                    },
                    "name": "Yi Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:11:09.095Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a43f6e0bef323a68fcfe",
                    "name": "Dianhai Yu",
                    "hidden": false
                },
                {
                    "_id": "68f1a43f6e0bef323a68fcff",
                    "name": "Yanjun Ma",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T10:18:48.000Z",
            "submittedOnDailyAt": "2025-10-17T00:35:15.163Z",
            "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B\n  Ultra-Compact Vision-Language Model",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style\ndynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to\nenable accurate element recognition. This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition. It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds. These strengths make it highly suitable for practical\ndeployment in real-world scenarios.",
            "upvotes": 23,
            "discussionId": "68f1a4406e0bef323a68fd00",
            "githubRepo": "https://github.com/PaddlePaddle/PaddleOCR",
            "ai_summary": "PaddleOCR-VL, a vision-language model combining NaViT-style visual encoder and ERNIE-4.5 language model, achieves state-of-the-art performance in document parsing with minimal resource consumption.",
            "ai_keywords": [
                "vision-language model",
                "NaViT-style",
                "dynamic resolution visual encoder",
                "ERNIE-4.5",
                "element recognition",
                "page-level document parsing",
                "element-level recognition",
                "inference speeds"
            ],
            "githubStars": 57911,
            "organization": {
                "_id": "62067d5d3906f102bc9658bd",
                "name": "PaddlePaddle",
                "fullname": "PaddlePaddle",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654942635336-5f3ff69679c1ba4c353d0c5a.png"
            }
        },
        "publishedAt": "2025-10-16T06:18:48.000Z",
        "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B\n  Ultra-Compact Vision-Language Model",
        "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style\ndynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to\nenable accurate element recognition. This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition. It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds. These strengths make it highly suitable for practical\ndeployment in real-world scenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14528.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 130
        },
        "organization": {
            "_id": "62067d5d3906f102bc9658bd",
            "name": "PaddlePaddle",
            "fullname": "PaddlePaddle",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654942635336-5f3ff69679c1ba4c353d0c5a.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.10518",
            "authors": [
                {
                    "_id": "68f0de8336f8b025381e1b61",
                    "user": {
                        "_id": "67a2ed64ed6024cc7b4f8acf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a2ed64ed6024cc7b4f8acf/763S4FbWvnaYl2D8KyIsp.jpeg",
                        "isPro": false,
                        "fullname": "Qunzhong WANG",
                        "user": "qunwang13",
                        "type": "user"
                    },
                    "name": "Qunzhong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:12:54.291Z",
                    "hidden": false
                },
                {
                    "_id": "68f0de8336f8b025381e1b62",
                    "name": "Jie Liu",
                    "hidden": false
                },
                {
                    "_id": "68f0de8336f8b025381e1b63",
                    "name": "Jiajun Liang",
                    "hidden": false
                },
                {
                    "_id": "68f0de8336f8b025381e1b64",
                    "name": "Yilei Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f0de8336f8b025381e1b65",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f0de8336f8b025381e1b66",
                    "name": "Jinyuan Chen",
                    "hidden": false
                },
                {
                    "_id": "68f0de8336f8b025381e1b67",
                    "name": "Yaozhi Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f0de8336f8b025381e1b68",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "68f0de8336f8b025381e1b69",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "68f0de8336f8b025381e1b6a",
                    "name": "Xiangyu Yue",
                    "hidden": false
                },
                {
                    "_id": "68f0de8336f8b025381e1b6b",
                    "name": "Jiaheng Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-12T09:29:50.000Z",
            "submittedOnDailyAt": "2025-10-17T00:46:35.185Z",
            "title": "VR-Thinker: Boosting Video Reward Models through Thinking-with-Image\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "65377c30e48353201e6fdda0",
                "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
                "isPro": false,
                "fullname": "Jiaheng Liu",
                "user": "CheeryLJH",
                "type": "user"
            },
            "summary": "Recent advancements in multimodal reward models (RMs) have substantially\nimproved post-training for visual generative models. However, current RMs face\ninherent limitations: (1) visual inputs consume large context budgets, forcing\nfewer frames and causing loss of fine-grained details; and (2) all visual\ninformation is packed into the initial prompt, exacerbating hallucination and\nforgetting during chain-of-thought reasoning. To overcome these issues, we\nintroduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework\nthat equips the RM with visual reasoning operations (e.g., select frame) and a\nconfigurable visual memory window. This allows the RM to actively acquire and\nupdate visual evidence within context limits, improving reasoning fidelity and\nreliability. We activate visual reasoning via a reinforcement fine-tuning\npipeline: (i) Cold Start with curated visual chain-of-thought data to distill\nbasic reasoning skills and operation formatting; (ii) select samples whose\nper-dimension and overall judgments are all correct, then conduct Rejection\nsampling Fine-Tuning on these high-quality traces to further enhance reasoning;\nand (iii) apply Group Relative Policy Optimization (GRPO) to strengthen\nreasoning. Our approach delivers state-of-the-art accuracy among open-source\nmodels on video preference benchmarks, especially for longer videos: a 7B\nVR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6%\non MJ-Bench-Video. These results validate the effectiveness and promise of\nthinking-with-image multimodal reward modeling.",
            "upvotes": 13,
            "discussionId": "68f0de8336f8b025381e1b6c",
            "githubRepo": "https://github.com/qunzhongwang/vr-thinker",
            "ai_summary": "VideoReward Thinker enhances multimodal reward models with visual reasoning operations and a configurable memory window, improving accuracy on video preference benchmarks.",
            "ai_keywords": [
                "multimodal reward models",
                "visual generative models",
                "visual reasoning operations",
                "visual memory window",
                "reinforcement fine-tuning",
                "curated visual chain-of-thought data",
                "Rejection sampling Fine-Tuning",
                "Group Relative Policy Optimization",
                "VideoGen Reward",
                "GenAI-Bench",
                "MJ-Bench-Video"
            ],
            "githubStars": 14,
            "organization": {
                "_id": "68edc767abe005ac1b354573",
                "name": "NJU-LINK",
                "fullname": "NJU-LINK Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"
            }
        },
        "publishedAt": "2025-10-12T05:29:50.000Z",
        "title": "VR-Thinker: Boosting Video Reward Models through Thinking-with-Image\n  Reasoning",
        "summary": "Recent advancements in multimodal reward models (RMs) have substantially\nimproved post-training for visual generative models. However, current RMs face\ninherent limitations: (1) visual inputs consume large context budgets, forcing\nfewer frames and causing loss of fine-grained details; and (2) all visual\ninformation is packed into the initial prompt, exacerbating hallucination and\nforgetting during chain-of-thought reasoning. To overcome these issues, we\nintroduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework\nthat equips the RM with visual reasoning operations (e.g., select frame) and a\nconfigurable visual memory window. This allows the RM to actively acquire and\nupdate visual evidence within context limits, improving reasoning fidelity and\nreliability. We activate visual reasoning via a reinforcement fine-tuning\npipeline: (i) Cold Start with curated visual chain-of-thought data to distill\nbasic reasoning skills and operation formatting; (ii) select samples whose\nper-dimension and overall judgments are all correct, then conduct Rejection\nsampling Fine-Tuning on these high-quality traces to further enhance reasoning;\nand (iii) apply Group Relative Policy Optimization (GRPO) to strengthen\nreasoning. Our approach delivers state-of-the-art accuracy among open-source\nmodels on video preference benchmarks, especially for longer videos: a 7B\nVR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6%\non MJ-Bench-Video. These results validate the effectiveness and promise of\nthinking-with-image multimodal reward modeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10518.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65377c30e48353201e6fdda0",
            "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
            "fullname": "Jiaheng Liu",
            "name": "CheeryLJH",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 20
        },
        "organization": {
            "_id": "68edc767abe005ac1b354573",
            "name": "NJU-LINK",
            "fullname": "NJU-LINK Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.09033",
            "authors": [
                {
                    "_id": "68f1a6f96e0bef323a68fd64",
                    "user": {
                        "_id": "6454f1486f4ae9965626409c",
                        "avatarUrl": "/avatars/ac9d471b230f8babd03a1f35378d990e.svg",
                        "isPro": true,
                        "fullname": "Chi Seng Cheang",
                        "user": "chiseng-cheang",
                        "type": "user"
                    },
                    "name": "Chi Seng Cheang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:34:35.821Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a6f96e0bef323a68fd65",
                    "name": "Hou Pong Chan",
                    "hidden": false
                },
                {
                    "_id": "68f1a6f96e0bef323a68fd66",
                    "name": "Wenxuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6f96e0bef323a68fd67",
                    "user": {
                        "_id": "68f1c7bc2faaae2d56c10bff",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xWST1Lh10BK7SRKBLjkFR.jpeg",
                        "isPro": false,
                        "fullname": "Yang Deng",
                        "user": "ydeng17",
                        "type": "user"
                    },
                    "name": "Yang Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T05:56:48.685Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T06:09:04.000Z",
            "submittedOnDailyAt": "2025-10-17T00:49:36.375Z",
            "title": "Large Language Models Do NOT Really Know What They Don't Know",
            "submittedOnDailyBy": {
                "_id": "604f67ef0fe8ff3ec13d71ef",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
                "isPro": false,
                "fullname": "Hou Pong (Ken) Chan",
                "user": "kenchan0226",
                "type": "user"
            },
            "summary": "Recent work suggests that large language models (LLMs) encode factuality\nsignals in their internal representations, such as hidden states, attention\nweights, or token probabilities, implying that LLMs may \"know what they don't\nknow\". However, LLMs can also produce factual errors by relying on shortcuts or\nspurious associations. These error are driven by the same training objective\nthat encourage correct predictions, raising the question of whether internal\ncomputations can reliably distinguish between factual and hallucinated outputs.\nIn this work, we conduct a mechanistic analysis of how LLMs internally process\nfactual queries by comparing two types of hallucinations based on their\nreliance on subject information. We find that when hallucinations are\nassociated with subject knowledge, LLMs employ the same internal recall process\nas for correct responses, leading to overlapping and indistinguishable\nhidden-state geometries. In contrast, hallucinations detached from subject\nknowledge produce distinct, clustered representations that make them\ndetectable. These findings reveal a fundamental limitation: LLMs do not encode\ntruthfulness in their internal states but only patterns of knowledge recall,\ndemonstrating that \"LLMs don't really know what they don't know\".",
            "upvotes": 13,
            "discussionId": "68f1a6fa6e0bef323a68fd68",
            "ai_summary": "LLMs process factual queries and hallucinations similarly when associated with subject knowledge, leading to indistinguishable internal representations, but produce distinct representations for hallucinations without subject knowledge.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "hidden states",
                "attention weights",
                "token probabilities",
                "factual queries",
                "hallucinations",
                "subject information",
                "internal recall process",
                "hidden-state geometries",
                "truthfulness",
                "knowledge recall"
            ],
            "organization": {
                "_id": "6296f43820dc74838613d1ba",
                "name": "SingaporeManagementUniversity",
                "fullname": "Singapore Management University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654060072621-6296f3e0b493a00f946220de.png"
            }
        },
        "publishedAt": "2025-10-10T02:09:04.000Z",
        "title": "Large Language Models Do NOT Really Know What They Don't Know",
        "summary": "Recent work suggests that large language models (LLMs) encode factuality\nsignals in their internal representations, such as hidden states, attention\nweights, or token probabilities, implying that LLMs may \"know what they don't\nknow\". However, LLMs can also produce factual errors by relying on shortcuts or\nspurious associations. These error are driven by the same training objective\nthat encourage correct predictions, raising the question of whether internal\ncomputations can reliably distinguish between factual and hallucinated outputs.\nIn this work, we conduct a mechanistic analysis of how LLMs internally process\nfactual queries by comparing two types of hallucinations based on their\nreliance on subject information. We find that when hallucinations are\nassociated with subject knowledge, LLMs employ the same internal recall process\nas for correct responses, leading to overlapping and indistinguishable\nhidden-state geometries. In contrast, hallucinations detached from subject\nknowledge produce distinct, clustered representations that make them\ndetectable. These findings reveal a fundamental limitation: LLMs do not encode\ntruthfulness in their internal states but only patterns of knowledge recall,\ndemonstrating that \"LLMs don't really know what they don't know\".",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09033.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "604f67ef0fe8ff3ec13d71ef",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
            "fullname": "Hou Pong (Ken) Chan",
            "name": "kenchan0226",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "organization": {
            "_id": "6296f43820dc74838613d1ba",
            "name": "SingaporeManagementUniversity",
            "fullname": "Singapore Management University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654060072621-6296f3e0b493a00f946220de.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14958",
            "authors": [
                {
                    "_id": "68f1a3346e0bef323a68fcde",
                    "name": "Weikang Shi",
                    "hidden": false
                },
                {
                    "_id": "68f1a3346e0bef323a68fcdf",
                    "name": "Aldrich Yu",
                    "hidden": false
                },
                {
                    "_id": "68f1a3346e0bef323a68fce0",
                    "name": "Rongyao Fang",
                    "hidden": false
                },
                {
                    "_id": "68f1a3346e0bef323a68fce1",
                    "name": "Houxing Ren",
                    "hidden": false
                },
                {
                    "_id": "68f1a3346e0bef323a68fce2",
                    "name": "Ke Wang",
                    "hidden": false
                },
                {
                    "_id": "68f1a3346e0bef323a68fce3",
                    "name": "Aojun Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f1a3346e0bef323a68fce4",
                    "name": "Changyao Tian",
                    "hidden": false
                },
                {
                    "_id": "68f1a3346e0bef323a68fce5",
                    "name": "Xinyu Fu",
                    "hidden": false
                },
                {
                    "_id": "68f1a3346e0bef323a68fce6",
                    "name": "Yuxuan Hu",
                    "hidden": false
                },
                {
                    "_id": "68f1a3346e0bef323a68fce7",
                    "name": "Zimu Lu",
                    "hidden": false
                },
                {
                    "_id": "68f1a3346e0bef323a68fce8",
                    "name": "Linjiang Huang",
                    "hidden": false
                },
                {
                    "_id": "68f1a3346e0bef323a68fce9",
                    "name": "Si Liu",
                    "hidden": false
                },
                {
                    "_id": "68f1a3346e0bef323a68fcea",
                    "name": "Rui Liu",
                    "hidden": false
                },
                {
                    "_id": "68f1a3346e0bef323a68fceb",
                    "name": "Hongsheng Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:58:58.000Z",
            "submittedOnDailyAt": "2025-10-17T00:30:38.950Z",
            "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal\n  Mathematical Reasoning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/",
            "upvotes": 12,
            "discussionId": "68f1a3346e0bef323a68fcec",
            "projectPage": "https://mathcanvas.github.io/",
            "githubRepo": "https://github.com/shiwk24/MathCanvas",
            "ai_summary": "MathCanvas enhances Large Multimodal Models with Visual Chain-of-Thought capabilities for mathematics through pre-training on diagram generation and fine-tuning on visual-textual reasoning, achieving significant improvements on math benchmarks.",
            "ai_keywords": [
                "Large Language Models",
                "Visual Chain-of-Thought",
                "Large Multimodal Models",
                "Visual Manipulation",
                "Strategic Visual-Aided Reasoning",
                "MathCanvas-Imagen",
                "MathCanvas-Edit",
                "MathCanvas-Instruct",
                "MathCanvas-Bench",
                "BAGEL-Canvas"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-10-16T13:58:58.000Z",
        "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal\n  Mathematical Reasoning",
        "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14958.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 130
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14902",
            "authors": [
                {
                    "_id": "68f1d24b6e0bef323a68ff4b",
                    "name": "Han Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f1d24b6e0bef323a68ff4c",
                    "name": "Jiaxuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1d24b6e0bef323a68ff4d",
                    "name": "Wenxuan Song",
                    "hidden": false
                },
                {
                    "_id": "68f1d24b6e0bef323a68ff4e",
                    "name": "Pengxiang Ding",
                    "hidden": false
                },
                {
                    "_id": "68f1d24b6e0bef323a68ff4f",
                    "name": "Donglin Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:18:34.000Z",
            "submittedOnDailyAt": "2025-10-17T05:06:57.203Z",
            "title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic\n  Framework for Unseen Concept Manipulation",
            "submittedOnDailyBy": {
                "_id": "646dbbc8075bbcc48ddcecbf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646dbbc8075bbcc48ddcecbf/V52Em-78O5F3QxRbRwG5O.jpeg",
                "isPro": false,
                "fullname": "Han Zhao",
                "user": "han1997",
                "type": "user"
            },
            "summary": "Current vision-language-action (VLA) models, pre-trained on large-scale\nrobotic data, exhibit strong multi-task capabilities and generalize well to\nvariations in visual and language instructions for manipulation. However, their\nsuccess rate drops significantly when faced with object concepts outside the\ntraining data, such as unseen object descriptions and textures in the dataset.\nTo address this, we propose a novel agentic framework, VLA^2, which leverages\nOpenVLA as the execution backbone and effectively leverages external modules\nsuch as web retrieval and object detection to provide visual and textual\nknowledge about target objects to the VLA. This approach mitigates\ngeneralization failure when handling out-of-distribution objects. Based on the\nLIBERO simulation environment, we introduced novel objects and object\ndescriptions to construct a new evaluation benchmark with three difficulty\nlevels to test the effectiveness of our method. Our framework successfully\noutperformed the current state-of-the-art models on our designed hard-level\ngeneralization benchmark. Compared to the standalone OpenVLA baseline, VLA^2\nachieves a 44.2% improvement in the success rate in the hard-level benchmark\nand an average improvement of 20.2% in all customized environments without any\nperformance degradation on in-domain tasks. Project website:\nhttps://vla-2.github.io.",
            "upvotes": 10,
            "discussionId": "68f1d24b6e0bef323a68ff50",
            "projectPage": "https://vla-2.github.io",
            "ai_summary": "A novel agentic framework, VLA^2, enhances vision-language-action models by integrating external modules like web retrieval and object detection, improving generalization to unseen objects and descriptions.",
            "ai_keywords": [
                "vision-language-action models",
                "pre-trained models",
                "multi-task capabilities",
                "generalization",
                "out-of-distribution objects",
                "OpenVLA",
                "web retrieval",
                "object detection",
                "LIBERO simulation environment",
                "evaluation benchmark",
                "success rate",
                "in-domain tasks"
            ],
            "organization": {
                "_id": "61bac2af530e5c78d7b99667",
                "name": "zju",
                "fullname": "Zhejiang University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
            }
        },
        "publishedAt": "2025-10-16T13:18:34.000Z",
        "title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic\n  Framework for Unseen Concept Manipulation",
        "summary": "Current vision-language-action (VLA) models, pre-trained on large-scale\nrobotic data, exhibit strong multi-task capabilities and generalize well to\nvariations in visual and language instructions for manipulation. However, their\nsuccess rate drops significantly when faced with object concepts outside the\ntraining data, such as unseen object descriptions and textures in the dataset.\nTo address this, we propose a novel agentic framework, VLA^2, which leverages\nOpenVLA as the execution backbone and effectively leverages external modules\nsuch as web retrieval and object detection to provide visual and textual\nknowledge about target objects to the VLA. This approach mitigates\ngeneralization failure when handling out-of-distribution objects. Based on the\nLIBERO simulation environment, we introduced novel objects and object\ndescriptions to construct a new evaluation benchmark with three difficulty\nlevels to test the effectiveness of our method. Our framework successfully\noutperformed the current state-of-the-art models on our designed hard-level\ngeneralization benchmark. Compared to the standalone OpenVLA baseline, VLA^2\nachieves a 44.2% improvement in the success rate in the hard-level benchmark\nand an average improvement of 20.2% in all customized environments without any\nperformance degradation on in-domain tasks. Project website:\nhttps://vla-2.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14902.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646dbbc8075bbcc48ddcecbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646dbbc8075bbcc48ddcecbf/V52Em-78O5F3QxRbRwG5O.jpeg",
            "fullname": "Han Zhao",
            "name": "han1997",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14763",
            "authors": [
                {
                    "_id": "68f1d66a6e0bef323a68ffbe",
                    "name": "Yunwen Li",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffbf",
                    "name": "Shuangshuang Ying",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffc0",
                    "name": "Xingwei Qu",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffc1",
                    "name": "Xin Li",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffc2",
                    "name": "Sheng Jin",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffc3",
                    "name": "Minghao Liu",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffc4",
                    "name": "Zhoufutu Wen",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffc5",
                    "name": "Tianyu Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffc6",
                    "name": "Xeron Du",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffc7",
                    "name": "Qiguang Chen",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffc8",
                    "name": "Jiajun Shi",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffc9",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffca",
                    "name": "Jiazhan Feng",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffcb",
                    "name": "Wanjun Zhong",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffcc",
                    "name": "Libo Qin",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffcd",
                    "name": "Stephen Huang",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffce",
                    "name": "Wanxiang Che",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffcf",
                    "name": "Chenghua Lin",
                    "hidden": false
                },
                {
                    "_id": "68f1d66a6e0bef323a68ffd0",
                    "name": "Eli Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T15:01:19.000Z",
            "submittedOnDailyAt": "2025-10-17T04:09:34.642Z",
            "title": "COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with\n  Thought Processes",
            "submittedOnDailyBy": {
                "_id": "64a948d9723beceb2f13f5eb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a948d9723beceb2f13f5eb/UQMO7LczJpabtbcoNt1zQ.jpeg",
                "isPro": false,
                "fullname": "XinLi",
                "user": "XINLI1997",
                "type": "user"
            },
            "summary": "Large language models exhibit systematic deficiencies in creative writing,\nparticularly in non-English contexts where training data is scarce and lacks\nprocess-level supervision. We present COIG-Writer, a novel Chinese creative\nwriting dataset that captures both diverse outputs and their underlying thought\nprocesses through systematic reverse-engineering of high-quality texts. Unlike\nexisting datasets that provide only input-output pairs, COIG-Writer comprises\n1,665 meticulously curated triplets spanning 51 genres, each containing: (1) a\nreverse-engineered prompt, (2) detailed creative reasoning documenting\ndecision-making processes, and (3) the final text. Through comprehensive\nexperiments, we identify a two-component model of creative writing: narrative\nlogic (provided by process supervision) and linguistic expression (maintained\nby general-purpose data). Our findings reveal three critical insights: (1)\nProcess supervision is highly effective but requires stabilization with general\ndata. A ratio of at least one creative sample to twelve general samples is\nneeded to achieve optimal performance; below this threshold, the win rate\nprogressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities\nare culturally-bound with no cross-lingual transfer (89.26pp gap between\nChinese and English performance), and (3) lexical diversity inversely\ncorrelates with creative quality (TTR paradox), suggesting high diversity\nsignals compensatory behavior for logical deficiencies. These findings\nestablish that creative excellence emerges from the interaction between logical\nscaffolding and linguistic grounding, analogous to how mathematical reasoning\nenhances but cannot replace linguistic competence in foundation models.",
            "upvotes": 10,
            "discussionId": "68f1d66b6e0bef323a68ffd1",
            "projectPage": "https://COIG-Writer.github.io/",
            "githubRepo": "https://github.com/COIG-Writer/COIG-Writer",
            "ai_summary": "COIG-Writer, a Chinese creative writing dataset, reveals that process supervision and general-purpose data are crucial for creative writing, with cultural-bound capabilities and lexical diversity impacting performance.",
            "ai_keywords": [
                "COIG-Writer",
                "creative writing dataset",
                "reverse-engineered prompt",
                "creative reasoning",
                "narrative logic",
                "linguistic expression",
                "process supervision",
                "general-purpose data",
                "cultural-bound capabilities",
                "cross-lingual transfer",
                "lexical diversity",
                "TTR paradox"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "6384ee7fdfffab482400b938",
                "name": "m-a-p",
                "fullname": "Multimodal Art Projection",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63839e9962badff4326cf360/k4Q7R4XLDMp_1VF4C6GEd.jpeg"
            }
        },
        "publishedAt": "2025-10-16T11:01:19.000Z",
        "title": "COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with\n  Thought Processes",
        "summary": "Large language models exhibit systematic deficiencies in creative writing,\nparticularly in non-English contexts where training data is scarce and lacks\nprocess-level supervision. We present COIG-Writer, a novel Chinese creative\nwriting dataset that captures both diverse outputs and their underlying thought\nprocesses through systematic reverse-engineering of high-quality texts. Unlike\nexisting datasets that provide only input-output pairs, COIG-Writer comprises\n1,665 meticulously curated triplets spanning 51 genres, each containing: (1) a\nreverse-engineered prompt, (2) detailed creative reasoning documenting\ndecision-making processes, and (3) the final text. Through comprehensive\nexperiments, we identify a two-component model of creative writing: narrative\nlogic (provided by process supervision) and linguistic expression (maintained\nby general-purpose data). Our findings reveal three critical insights: (1)\nProcess supervision is highly effective but requires stabilization with general\ndata. A ratio of at least one creative sample to twelve general samples is\nneeded to achieve optimal performance; below this threshold, the win rate\nprogressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities\nare culturally-bound with no cross-lingual transfer (89.26pp gap between\nChinese and English performance), and (3) lexical diversity inversely\ncorrelates with creative quality (TTR paradox), suggesting high diversity\nsignals compensatory behavior for logical deficiencies. These findings\nestablish that creative excellence emerges from the interaction between logical\nscaffolding and linguistic grounding, analogous to how mathematical reasoning\nenhances but cannot replace linguistic competence in foundation models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14763.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a948d9723beceb2f13f5eb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a948d9723beceb2f13f5eb/UQMO7LczJpabtbcoNt1zQ.jpeg",
            "fullname": "XinLi",
            "name": "XINLI1997",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6384ee7fdfffab482400b938",
            "name": "m-a-p",
            "fullname": "Multimodal Art Projection",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63839e9962badff4326cf360/k4Q7R4XLDMp_1VF4C6GEd.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13217",
            "authors": [
                {
                    "_id": "68f19ecf6e0bef323a68fc75",
                    "name": "Nilesh Gupta",
                    "hidden": false
                },
                {
                    "_id": "68f19ecf6e0bef323a68fc76",
                    "name": "Wei-Cheng Chang",
                    "hidden": false
                },
                {
                    "_id": "68f19ecf6e0bef323a68fc77",
                    "name": "Ngot Bui",
                    "hidden": false
                },
                {
                    "_id": "68f19ecf6e0bef323a68fc78",
                    "name": "Cho-Jui Hsieh",
                    "hidden": false
                },
                {
                    "_id": "68f19ecf6e0bef323a68fc79",
                    "name": "Inderjit S. Dhillon",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T07:05:17.000Z",
            "submittedOnDailyAt": "2025-10-17T00:12:11.927Z",
            "title": "LLM-guided Hierarchical Retrieval",
            "submittedOnDailyBy": {
                "_id": "6527151d5606f146974d60d8",
                "avatarUrl": "/avatars/00ac8ab005ceadae866dea5471f6aab9.svg",
                "isPro": false,
                "fullname": "Nilesh Gupta",
                "user": "quicktensor",
                "type": "user"
            },
            "summary": "Modern IR systems are increasingly tasked with answering complex,\nmulti-faceted queries that require deep reasoning rather than simple keyword or\nsemantic matching. While LLM-based IR has shown great promise, the prevailing\nretrieve-then-rerank paradigm inherits the limitations of embedding-based\nretrieval; parametric generative approaches are difficult to update with new\ninformation; and long-context methods that place the entire corpus in context\nare computationally infeasible for large document collections. To address these\nchallenges, we introduce LATTICE, a hierarchical retrieval framework that\nenables an LLM to reason over and navigate large corpora with logarithmic\nsearch complexity by imposing a semantic tree structure on the corpus. Our\napproach consists of two stages: (1) an offline phase that organizes the corpus\ninto a semantic hierarchy via either a bottom-up agglomerative strategy or a\ntop-down divisive strategy using multi-level summaries and (2) an online\ntraversal phase where a search LLM navigates this tree. A central challenge in\nsuch LLM-guided search is that the model's relevance judgments are noisy,\ncontext-dependent, and unaware of the hierarchy, making cross-branch and\ncross-level comparisons difficult. To overcome this, we propose a traversal\nalgorithm that estimates calibrated latent relevance scores from local LLM\noutputs and aggregates them into a global path relevance metric. Our\ntraining-free framework achieves state-of-the-art zero-shot performance on the\nreasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in\nRecall@100 and 5% in nDCG@10 over the next best zero-shot baseline.\nFurthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains\ncomparable results on BRIGHT subsets that use a static corpus for evaluation.",
            "upvotes": 10,
            "discussionId": "68f19ed06e0bef323a68fc7a",
            "projectPage": "https://nilesh2797.github.io/publications/lattice/",
            "githubRepo": "https://github.com/nilesh2797/lattice",
            "ai_summary": "LATTICE, a hierarchical retrieval framework, enables efficient and accurate reasoning over large document collections using a semantic tree structure and a traversal algorithm that calibrates relevance scores.",
            "ai_keywords": [
                "LLM-based IR",
                "retrieve-then-rerank",
                "embedding-based retrieval",
                "parametric generative approaches",
                "long-context methods",
                "hierarchical retrieval framework",
                "semantic tree structure",
                "offline phase",
                "bottom-up agglomerative strategy",
                "top-down divisive strategy",
                "multi-level summaries",
                "online traversal phase",
                "search LLM",
                "relevance judgments",
                "latent relevance scores",
                "global path relevance metric",
                "zero-shot performance",
                "BRIGHT benchmark",
                "Recall@100",
                "nDCG@10",
                "DIVER-v2"
            ],
            "githubStars": 5,
            "organization": {
                "_id": "5e6aca39878b8b2bf9806447",
                "name": "google",
                "fullname": "Google",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
            }
        },
        "publishedAt": "2025-10-15T03:05:17.000Z",
        "title": "LLM-guided Hierarchical Retrieval",
        "summary": "Modern IR systems are increasingly tasked with answering complex,\nmulti-faceted queries that require deep reasoning rather than simple keyword or\nsemantic matching. While LLM-based IR has shown great promise, the prevailing\nretrieve-then-rerank paradigm inherits the limitations of embedding-based\nretrieval; parametric generative approaches are difficult to update with new\ninformation; and long-context methods that place the entire corpus in context\nare computationally infeasible for large document collections. To address these\nchallenges, we introduce LATTICE, a hierarchical retrieval framework that\nenables an LLM to reason over and navigate large corpora with logarithmic\nsearch complexity by imposing a semantic tree structure on the corpus. Our\napproach consists of two stages: (1) an offline phase that organizes the corpus\ninto a semantic hierarchy via either a bottom-up agglomerative strategy or a\ntop-down divisive strategy using multi-level summaries and (2) an online\ntraversal phase where a search LLM navigates this tree. A central challenge in\nsuch LLM-guided search is that the model's relevance judgments are noisy,\ncontext-dependent, and unaware of the hierarchy, making cross-branch and\ncross-level comparisons difficult. To overcome this, we propose a traversal\nalgorithm that estimates calibrated latent relevance scores from local LLM\noutputs and aggregates them into a global path relevance metric. Our\ntraining-free framework achieves state-of-the-art zero-shot performance on the\nreasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in\nRecall@100 and 5% in nDCG@10 over the next best zero-shot baseline.\nFurthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains\ncomparable results on BRIGHT subsets that use a static corpus for evaluation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13217.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6527151d5606f146974d60d8",
            "avatarUrl": "/avatars/00ac8ab005ceadae866dea5471f6aab9.svg",
            "fullname": "Nilesh Gupta",
            "name": "quicktensor",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "5e6aca39878b8b2bf9806447",
            "name": "google",
            "fullname": "Google",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14616",
            "authors": [
                {
                    "_id": "68f1b22d6e0bef323a68fdf4",
                    "name": "Shuangshuang Ying",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fdf5",
                    "name": "Yunwen Li",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fdf6",
                    "name": "Xingwei Qu",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fdf7",
                    "name": "Xin Li",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fdf8",
                    "name": "Sheng Jin",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fdf9",
                    "name": "Minghao Liu",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fdfa",
                    "name": "Zhoufutu Wen",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fdfb",
                    "name": "Xeron Du",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fdfc",
                    "name": "Tianyu Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fdfd",
                    "name": "Yichi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fdfe",
                    "name": "Letian Ni",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fdff",
                    "name": "Yuyang Cheng",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fe00",
                    "name": "Qiguang Chen",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fe01",
                    "name": "Jingzhe Ding",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fe02",
                    "name": "Shengda Long",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fe03",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fe04",
                    "name": "Jiazhan Feng",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fe05",
                    "name": "Wanjun Zhong",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fe06",
                    "name": "Libo Qin",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fe07",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fe08",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fe09",
                    "name": "Wanxiang Che",
                    "hidden": false
                },
                {
                    "_id": "68f1b22d6e0bef323a68fe0a",
                    "name": "Chenghua Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T12:23:13.000Z",
            "submittedOnDailyAt": "2025-10-17T04:10:42.518Z",
            "title": "Beyond Correctness: Evaluating Subjective Writing Preferences Across\n  Cultures",
            "submittedOnDailyBy": {
                "_id": "64a948d9723beceb2f13f5eb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a948d9723beceb2f13f5eb/UQMO7LczJpabtbcoNt1zQ.jpeg",
                "isPro": false,
                "fullname": "XinLi",
                "user": "XINLI1997",
                "type": "user"
            },
            "summary": "Current preference learning methods achieve high accuracy on standard\nbenchmarks but exhibit significant performance degradation when objective\nquality signals are removed. We introduce WritingPreferenceBench, a dataset of\n1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8\ncreative writing genres, where responses are matched for objective correctness,\nfactual accuracy, and length. On this benchmark, sequence-based reward\nmodels--the standard architecture for RLHF--achieve only 52.7% mean accuracy,\nwhile zero-shot language model judges perform at 53.9%. In contrast, generative\nreward models that produce explicit reasoning chains achieve 81.8% accuracy. We\nobserve high within-model variance across genres: individual models range from\n18.2% to 81.8% accuracy across different writing categories, with standard\ndeviations averaging 10.1%. This variance persists regardless of model scale,\nwith 27B parameter models showing no consistent improvement over 8B variants.\nOur results suggest that current RLHF methods primarily learn to detect\nobjective errors rather than capture subjective quality preferences (e.g.,\ncreativity, stylistic flair, and emotional resonance), and that successful\npreference modeling may require intermediate reasoning representations rather\nthan direct classification.",
            "upvotes": 9,
            "discussionId": "68f1b22d6e0bef323a68fe0b",
            "projectPage": "https://WritingPreferenceBench.github.io/",
            "githubRepo": "https://github.com/WritingPreferenceBench/Writing-Preference-Bench",
            "ai_summary": "Generative reward models with explicit reasoning chains outperform sequence-based reward models and zero-shot language models in preference learning for creative writing, indicating the need for intermediate reasoning in capturing subjective quality.",
            "ai_keywords": [
                "sequence-based reward models",
                "zero-shot language models",
                "generative reward models",
                "explicit reasoning chains",
                "RLHF",
                "preference learning",
                "creative writing genres",
                "objective correctness",
                "factual accuracy",
                "subjective quality preferences",
                "creativity",
                "stylistic flair",
                "emotional resonance"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-10-16T08:23:13.000Z",
        "title": "Beyond Correctness: Evaluating Subjective Writing Preferences Across\n  Cultures",
        "summary": "Current preference learning methods achieve high accuracy on standard\nbenchmarks but exhibit significant performance degradation when objective\nquality signals are removed. We introduce WritingPreferenceBench, a dataset of\n1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8\ncreative writing genres, where responses are matched for objective correctness,\nfactual accuracy, and length. On this benchmark, sequence-based reward\nmodels--the standard architecture for RLHF--achieve only 52.7% mean accuracy,\nwhile zero-shot language model judges perform at 53.9%. In contrast, generative\nreward models that produce explicit reasoning chains achieve 81.8% accuracy. We\nobserve high within-model variance across genres: individual models range from\n18.2% to 81.8% accuracy across different writing categories, with standard\ndeviations averaging 10.1%. This variance persists regardless of model scale,\nwith 27B parameter models showing no consistent improvement over 8B variants.\nOur results suggest that current RLHF methods primarily learn to detect\nobjective errors rather than capture subjective quality preferences (e.g.,\ncreativity, stylistic flair, and emotional resonance), and that successful\npreference modeling may require intermediate reasoning representations rather\nthan direct classification.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14616.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a948d9723beceb2f13f5eb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a948d9723beceb2f13f5eb/UQMO7LczJpabtbcoNt1zQ.jpeg",
            "fullname": "XinLi",
            "name": "XINLI1997",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14880",
            "authors": [
                {
                    "_id": "68f1b4f86e0bef323a68fe21",
                    "name": "Rikiya Takehi",
                    "hidden": false
                },
                {
                    "_id": "68f1b4f86e0bef323a68fe22",
                    "user": {
                        "_id": "5ff60d4352c26e9bc240badd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5ff60d4352c26e9bc240badd/HzoknJibrSasc1ZzU71XA.png",
                        "isPro": true,
                        "fullname": "Benjamin Clavié",
                        "user": "bclavie",
                        "type": "user"
                    },
                    "name": "Benjamin Clavié",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:10:11.390Z",
                    "hidden": false
                },
                {
                    "_id": "68f1b4f86e0bef323a68fe23",
                    "name": "Sean Lee",
                    "hidden": false
                },
                {
                    "_id": "68f1b4f86e0bef323a68fe24",
                    "name": "Aamir Shakir",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:00:35.000Z",
            "submittedOnDailyAt": "2025-10-17T01:47:16.250Z",
            "title": "Fantastic (small) Retrievers and How to Train Them:\n  mxbai-edge-colbert-v0 Tech Report",
            "submittedOnDailyBy": {
                "_id": "5ff60d4352c26e9bc240badd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5ff60d4352c26e9bc240badd/HzoknJibrSasc1ZzU71XA.png",
                "isPro": true,
                "fullname": "Benjamin Clavié",
                "user": "bclavie",
                "type": "user"
            },
            "summary": "In this work, we introduce mxbai-edge-colbert-v0 models, at two different\nparameter counts: 17M and 32M. As part of our research, we conduct numerous\nexperiments to improve retrieval and late-interaction models, which we intend\nto distill into smaller models as proof-of-concepts. Our ultimate aim is to\nsupport retrieval at all scales, from large-scale retrieval which lives in the\ncloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a\nmodel that we hope will serve as a solid foundation backbone for all future\nexperiments, representing the first version of a long series of small\nproof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we\nconducted multiple ablation studies, of which we report the results. In terms\nof downstream performance, mxbai-edge-colbert-v0 is a particularly capable\nsmall model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and\nrepresenting a large step forward in long-context tasks, with unprecedented\nefficiency.",
            "upvotes": 8,
            "discussionId": "68f1b4f96e0bef323a68fe25",
            "ai_summary": "mxbai-edge-colbert-v0 models, with 17M and 32M parameters, demonstrate superior retrieval performance on short-text and long-context benchmarks compared to ColBERTv2.",
            "ai_keywords": [
                "mxbai-edge-colbert-v0",
                "retrieval",
                "late-interaction models",
                "distillation",
                "ablation studies",
                "BEIR",
                "ColBERTv2",
                "long-context tasks"
            ],
            "organization": {
                "_id": "6579b8e83db8c022afd86d63",
                "name": "mixedbread-ai",
                "fullname": "Mixedbread",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643ee0870d1194da249bd7fe/voYdYlFgQH5vyMwyMNluZ.png"
            }
        },
        "publishedAt": "2025-10-16T13:00:35.000Z",
        "title": "Fantastic (small) Retrievers and How to Train Them:\n  mxbai-edge-colbert-v0 Tech Report",
        "summary": "In this work, we introduce mxbai-edge-colbert-v0 models, at two different\nparameter counts: 17M and 32M. As part of our research, we conduct numerous\nexperiments to improve retrieval and late-interaction models, which we intend\nto distill into smaller models as proof-of-concepts. Our ultimate aim is to\nsupport retrieval at all scales, from large-scale retrieval which lives in the\ncloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a\nmodel that we hope will serve as a solid foundation backbone for all future\nexperiments, representing the first version of a long series of small\nproof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we\nconducted multiple ablation studies, of which we report the results. In terms\nof downstream performance, mxbai-edge-colbert-v0 is a particularly capable\nsmall model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and\nrepresenting a large step forward in long-context tasks, with unprecedented\nefficiency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14880.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5ff60d4352c26e9bc240badd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5ff60d4352c26e9bc240badd/HzoknJibrSasc1ZzU71XA.png",
            "fullname": "Benjamin Clavié",
            "name": "bclavie",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 67
        },
        "organization": {
            "_id": "6579b8e83db8c022afd86d63",
            "name": "mixedbread-ai",
            "fullname": "Mixedbread",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643ee0870d1194da249bd7fe/voYdYlFgQH5vyMwyMNluZ.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.14300",
            "authors": [
                {
                    "_id": "68f1c62c6e0bef323a68fe9a",
                    "user": {
                        "_id": "674d8f71556d9ccbaee9f55a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VyZeljrRkqXCpeE1gkbAJ.png",
                        "isPro": false,
                        "fullname": "Shen Weijie",
                        "user": "shenweijie",
                        "type": "user"
                    },
                    "name": "Weijie Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:34:33.583Z",
                    "hidden": false
                },
                {
                    "_id": "68f1c62c6e0bef323a68fe9b",
                    "name": "Yitian Liu",
                    "hidden": false
                },
                {
                    "_id": "68f1c62c6e0bef323a68fe9c",
                    "name": "Yuhao Wu",
                    "hidden": false
                },
                {
                    "_id": "68f1c62c6e0bef323a68fe9d",
                    "name": "Zhixuan Liang",
                    "hidden": false
                },
                {
                    "_id": "68f1c62c6e0bef323a68fe9e",
                    "name": "Sijia Gu",
                    "hidden": false
                },
                {
                    "_id": "68f1c62c6e0bef323a68fe9f",
                    "name": "Dehui Wang",
                    "hidden": false
                },
                {
                    "_id": "68f1c62c6e0bef323a68fea0",
                    "name": "Tian Nian",
                    "hidden": false
                },
                {
                    "_id": "68f1c62c6e0bef323a68fea1",
                    "name": "Lei Xu",
                    "hidden": false
                },
                {
                    "_id": "68f1c62c6e0bef323a68fea2",
                    "name": "Yusen Qin",
                    "hidden": false
                },
                {
                    "_id": "68f1c62c6e0bef323a68fea3",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                },
                {
                    "_id": "68f1c62c6e0bef323a68fea4",
                    "name": "Xinping Guan",
                    "hidden": false
                },
                {
                    "_id": "68f1c62c6e0bef323a68fea5",
                    "name": "Xiaokang Yang",
                    "hidden": false
                },
                {
                    "_id": "68f1c62c6e0bef323a68fea6",
                    "name": "Yao Mu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T04:52:57.000Z",
            "submittedOnDailyAt": "2025-10-17T03:12:34.734Z",
            "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for\n  Vision-Language-Action Learning",
            "submittedOnDailyBy": {
                "_id": "674d8f71556d9ccbaee9f55a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VyZeljrRkqXCpeE1gkbAJ.png",
                "isPro": false,
                "fullname": "Shen Weijie",
                "user": "shenweijie",
                "type": "user"
            },
            "summary": "Vision-Language-Action (VLA) models are experiencing rapid development and\ndemonstrating promising capabilities in robotic manipulation tasks. However,\nscaling up VLA models presents several critical challenges: (1) Training new\nVLA models from scratch demands substantial computational resources and\nextensive datasets. Given the current scarcity of robot data, it becomes\nparticularly valuable to fully leverage well-pretrained VLA model weights\nduring the scaling process. (2) Real-time control requires carefully balancing\nmodel capacity with computational efficiency. To address these challenges, We\npropose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits\npretrained weights from dense VLA models, and scales up the action expert by\nsubstituting the feedforward layers into sparsely activated MoE layers. AdaMoE\nemploys a decoupling technique that decouples expert selection from expert\nweighting through an independent scale adapter working alongside the\ntraditional router. This enables experts to be selected based on task relevance\nwhile contributing with independently controlled weights, allowing\ncollaborative expert utilization rather than winner-takes-all dynamics. Our\napproach demonstrates that expertise need not monopolize. Instead, through\ncollaborative expert utilization, we can achieve superior performance while\nmaintaining computational efficiency. AdaMoE consistently outperforms the\nbaseline model across key benchmarks, delivering performance gains of 1.8% on\nLIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement\nin real-world experiments validates its practical effectiveness for robotic\nmanipulation tasks.",
            "upvotes": 8,
            "discussionId": "68f1c62d6e0bef323a68fea7",
            "ai_summary": "AdaMoE, a Mixture-of-Experts architecture, enhances VLA models by leveraging pretrained weights and improving computational efficiency, achieving superior performance in robotic manipulation tasks.",
            "ai_keywords": [
                "Mixture-of-Experts",
                "MoE",
                "feedforward layers",
                "sparsely activated MoE layers",
                "decoupling technique",
                "scale adapter",
                "router",
                "expert selection",
                "expert weighting",
                "collaborative expert utilization",
                "LIBERO",
                "RoboTwin"
            ]
        },
        "publishedAt": "2025-10-16T00:52:57.000Z",
        "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for\n  Vision-Language-Action Learning",
        "summary": "Vision-Language-Action (VLA) models are experiencing rapid development and\ndemonstrating promising capabilities in robotic manipulation tasks. However,\nscaling up VLA models presents several critical challenges: (1) Training new\nVLA models from scratch demands substantial computational resources and\nextensive datasets. Given the current scarcity of robot data, it becomes\nparticularly valuable to fully leverage well-pretrained VLA model weights\nduring the scaling process. (2) Real-time control requires carefully balancing\nmodel capacity with computational efficiency. To address these challenges, We\npropose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits\npretrained weights from dense VLA models, and scales up the action expert by\nsubstituting the feedforward layers into sparsely activated MoE layers. AdaMoE\nemploys a decoupling technique that decouples expert selection from expert\nweighting through an independent scale adapter working alongside the\ntraditional router. This enables experts to be selected based on task relevance\nwhile contributing with independently controlled weights, allowing\ncollaborative expert utilization rather than winner-takes-all dynamics. Our\napproach demonstrates that expertise need not monopolize. Instead, through\ncollaborative expert utilization, we can achieve superior performance while\nmaintaining computational efficiency. AdaMoE consistently outperforms the\nbaseline model across key benchmarks, delivering performance gains of 1.8% on\nLIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement\nin real-world experiments validates its practical effectiveness for robotic\nmanipulation tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14300.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "674d8f71556d9ccbaee9f55a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VyZeljrRkqXCpeE1gkbAJ.png",
            "fullname": "Shen Weijie",
            "name": "shenweijie",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.14276",
            "authors": [
                {
                    "_id": "68f1a6d46e0bef323a68fd29",
                    "name": "Haiquan Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd2a",
                    "name": "Chenhan Yuan",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd2b",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd2c",
                    "name": "Xiaomeng Hu",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd2d",
                    "name": "Yichang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd2e",
                    "name": "An Yang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd2f",
                    "name": "Bowen Yu",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd30",
                    "name": "Dayiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd31",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd32",
                    "name": "Junyang Lin",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd33",
                    "name": "Baosong Yang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd34",
                    "name": "Chen Cheng",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd35",
                    "name": "Jialong Tang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd36",
                    "name": "Jiandong Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd37",
                    "name": "Jianwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd38",
                    "name": "Jijie Xu",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd39",
                    "name": "Ming Yan",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd3a",
                    "name": "Minmin Sun",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd3b",
                    "name": "Pei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd3c",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd3d",
                    "name": "Qiaoyu Tang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd3e",
                    "name": "Qin Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd3f",
                    "name": "Rong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd40",
                    "name": "Shibin Wu",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd41",
                    "name": "Shuo Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd42",
                    "name": "Tao He",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd43",
                    "name": "Tianyi Tang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd44",
                    "name": "Tingyu Xia",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd45",
                    "name": "Wei Liao",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd46",
                    "name": "Weizhou Shen",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd47",
                    "name": "Wenbiao Yin",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd48",
                    "name": "Wenmeng Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd49",
                    "name": "Wenyuan Yu",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd4a",
                    "name": "Xiaobin Wang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd4b",
                    "name": "Xiaodong Deng",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd4c",
                    "name": "Xiaodong Xu",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd4d",
                    "name": "Xinyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd4e",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd4f",
                    "name": "Yeqiu Li",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd50",
                    "name": "Yi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd51",
                    "name": "Yong Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd52",
                    "name": "Yu Wan",
                    "hidden": false
                },
                {
                    "_id": "68f1a6d46e0bef323a68fd53",
                    "name": "Yuxin Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T04:00:18.000Z",
            "submittedOnDailyAt": "2025-10-17T00:46:03.670Z",
            "title": "Qwen3Guard Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "As large language models (LLMs) become more capable and widely used, ensuring\nthe safety of their outputs is increasingly critical. Existing guardrail\nmodels, though useful in static evaluation settings, face two major limitations\nin real-world applications: (1) they typically output only binary \"safe/unsafe\"\nlabels, which can be interpreted inconsistently across diverse safety policies,\nrendering them incapable of accommodating varying safety tolerances across\ndomains; and (2) they require complete model outputs before performing safety\nchecks, making them fundamentally incompatible with streaming LLM inference,\nthereby preventing timely intervention during generation and increasing\nexposure to harmful partial outputs. To address these challenges, we present\nQwen3Guard, a series of multilingual safety guardrail models with two\nspecialized variants: Generative Qwen3Guard, which casts safety classification\nas an instruction-following task to enable fine-grained tri-class judgments\n(safe, controversial, unsafe); and Stream Qwen3Guard, which introduces a\ntoken-level classification head for real-time safety monitoring during\nincremental text generation. Both variants are available in three sizes (0.6B,\n4B, and 8B parameters) and support up to 119 languages and dialects, providing\ncomprehensive, scalable, and low-latency safety moderation for global LLM\ndeployments. Evaluated across English, Chinese, and multilingual benchmarks,\nQwen3Guard achieves state-of-the-art performance in both prompt and response\nsafety classification. All models are released under the Apache 2.0 license for\npublic use.",
            "upvotes": 8,
            "discussionId": "68f1a6d56e0bef323a68fd54",
            "githubRepo": "https://github.com/QwenLM/Qwen3Guard",
            "ai_summary": "Qwen3Guard introduces multilingual safety guardrail models with fine-grained tri-class judgments and real-time token-level safety monitoring for large language models.",
            "ai_keywords": [
                "large language models",
                "safety guardrail models",
                "binary labels",
                "safety policies",
                "safety tolerances",
                "streaming inference",
                "Generative Qwen3Guard",
                "Stream Qwen3Guard",
                "instruction-following task",
                "tri-class judgments",
                "token-level classification",
                "multilingual",
                "prompt safety",
                "response safety",
                "Apache 2.0 license"
            ],
            "githubStars": 287,
            "organization": {
                "_id": "64c8b5837fe12ecd0a7e92eb",
                "name": "Qwen",
                "fullname": "Qwen",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
            }
        },
        "publishedAt": "2025-10-16T00:00:18.000Z",
        "title": "Qwen3Guard Technical Report",
        "summary": "As large language models (LLMs) become more capable and widely used, ensuring\nthe safety of their outputs is increasingly critical. Existing guardrail\nmodels, though useful in static evaluation settings, face two major limitations\nin real-world applications: (1) they typically output only binary \"safe/unsafe\"\nlabels, which can be interpreted inconsistently across diverse safety policies,\nrendering them incapable of accommodating varying safety tolerances across\ndomains; and (2) they require complete model outputs before performing safety\nchecks, making them fundamentally incompatible with streaming LLM inference,\nthereby preventing timely intervention during generation and increasing\nexposure to harmful partial outputs. To address these challenges, we present\nQwen3Guard, a series of multilingual safety guardrail models with two\nspecialized variants: Generative Qwen3Guard, which casts safety classification\nas an instruction-following task to enable fine-grained tri-class judgments\n(safe, controversial, unsafe); and Stream Qwen3Guard, which introduces a\ntoken-level classification head for real-time safety monitoring during\nincremental text generation. Both variants are available in three sizes (0.6B,\n4B, and 8B parameters) and support up to 119 languages and dialects, providing\ncomprehensive, scalable, and low-latency safety moderation for global LLM\ndeployments. Evaluated across English, Chinese, and multilingual benchmarks,\nQwen3Guard achieves state-of-the-art performance in both prompt and response\nsafety classification. All models are released under the Apache 2.0 license for\npublic use.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14276.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 130
        },
        "organization": {
            "_id": "64c8b5837fe12ecd0a7e92eb",
            "name": "Qwen",
            "fullname": "Qwen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14240",
            "authors": [
                {
                    "_id": "68f19a176e0bef323a68fc00",
                    "name": "Jiayu Wang",
                    "hidden": false
                },
                {
                    "_id": "68f19a176e0bef323a68fc01",
                    "name": "Yifei Ming",
                    "hidden": false
                },
                {
                    "_id": "68f19a176e0bef323a68fc02",
                    "name": "Riya Dulepet",
                    "hidden": false
                },
                {
                    "_id": "68f19a176e0bef323a68fc03",
                    "name": "Qinglin Chen",
                    "hidden": false
                },
                {
                    "_id": "68f19a176e0bef323a68fc04",
                    "name": "Austin Xu",
                    "hidden": false
                },
                {
                    "_id": "68f19a176e0bef323a68fc05",
                    "name": "Zixuan Ke",
                    "hidden": false
                },
                {
                    "_id": "68f19a176e0bef323a68fc06",
                    "name": "Frederic Sala",
                    "hidden": false
                },
                {
                    "_id": "68f19a176e0bef323a68fc07",
                    "name": "Aws Albarghouthi",
                    "hidden": false
                },
                {
                    "_id": "68f19a176e0bef323a68fc08",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "68f19a176e0bef323a68fc09",
                    "name": "Shafiq Joty",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T02:49:16.000Z",
            "submittedOnDailyAt": "2025-10-17T16:57:17.388Z",
            "title": "LiveResearchBench: A Live Benchmark for User-Centric Deep Research in\n  the Wild",
            "submittedOnDailyBy": {
                "_id": "651651f5d93a51ceda3021c3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/FE2uGpTKBRWMKTDBv1H-g.png",
                "isPro": false,
                "fullname": "Jiayu (Mila) Wang",
                "user": "MilaWang",
                "type": "user"
            },
            "summary": "Deep research -- producing comprehensive, citation-grounded reports by\nsearching and synthesizing information from hundreds of live web sources --\nmarks an important frontier for agentic systems. To rigorously evaluate this\nability, four principles are essential: tasks should be (1) user-centric,\nreflecting realistic information needs, (2) dynamic, requiring up-to-date\ninformation beyond parametric knowledge, (3) unambiguous, ensuring consistent\ninterpretation across users, and (4) multi-faceted and search-intensive,\nrequiring search over numerous web sources and in-depth analysis. Existing\nbenchmarks fall short of these principles, often focusing on narrow domains or\nposing ambiguous questions that hinder fair comparison. Guided by these\nprinciples, we introduce LiveResearchBench, a benchmark of 100 expert-curated\ntasks spanning daily life, enterprise, and academia, each requiring extensive,\ndynamic, real-time web search and synthesis. Built with over 1,500 hours of\nhuman labor, LiveResearchBench provides a rigorous basis for systematic\nevaluation. To evaluate citation-grounded long-form reports, we introduce\nDeepEval, a comprehensive suite covering both content- and report-level\nquality, including coverage, presentation, citation accuracy and association,\nconsistency and depth of analysis. DeepEval integrates four complementary\nevaluation protocols, each designed to ensure stable assessment and high\nagreement with human judgments. Using LiveResearchBench and DeepEval, we\nconduct a comprehensive evaluation of 17 frontier deep research systems,\nincluding single-agent web search, single-agent deep research, and multi-agent\nsystems. Our analysis reveals current strengths, recurring failure modes, and\nkey system components needed to advance reliable, insightful deep research.",
            "upvotes": 8,
            "discussionId": "68f19a176e0bef323a68fc0a",
            "ai_summary": "LiveResearchBench and DeepEval provide a comprehensive framework for evaluating deep research systems across various domains, focusing on real-time web search, synthesis, and citation-grounded long-form reports.",
            "ai_keywords": [
                "LiveResearchBench",
                "DeepEval",
                "deep research systems",
                "single-agent web search",
                "single-agent deep research",
                "multi-agent systems",
                "citation-grounded long-form reports",
                "real-time web search",
                "synthesis",
                "content-level quality",
                "report-level quality",
                "coverage",
                "presentation",
                "citation accuracy",
                "association",
                "consistency",
                "depth of analysis"
            ]
        },
        "publishedAt": "2025-10-15T22:49:16.000Z",
        "title": "LiveResearchBench: A Live Benchmark for User-Centric Deep Research in\n  the Wild",
        "summary": "Deep research -- producing comprehensive, citation-grounded reports by\nsearching and synthesizing information from hundreds of live web sources --\nmarks an important frontier for agentic systems. To rigorously evaluate this\nability, four principles are essential: tasks should be (1) user-centric,\nreflecting realistic information needs, (2) dynamic, requiring up-to-date\ninformation beyond parametric knowledge, (3) unambiguous, ensuring consistent\ninterpretation across users, and (4) multi-faceted and search-intensive,\nrequiring search over numerous web sources and in-depth analysis. Existing\nbenchmarks fall short of these principles, often focusing on narrow domains or\nposing ambiguous questions that hinder fair comparison. Guided by these\nprinciples, we introduce LiveResearchBench, a benchmark of 100 expert-curated\ntasks spanning daily life, enterprise, and academia, each requiring extensive,\ndynamic, real-time web search and synthesis. Built with over 1,500 hours of\nhuman labor, LiveResearchBench provides a rigorous basis for systematic\nevaluation. To evaluate citation-grounded long-form reports, we introduce\nDeepEval, a comprehensive suite covering both content- and report-level\nquality, including coverage, presentation, citation accuracy and association,\nconsistency and depth of analysis. DeepEval integrates four complementary\nevaluation protocols, each designed to ensure stable assessment and high\nagreement with human judgments. Using LiveResearchBench and DeepEval, we\nconduct a comprehensive evaluation of 17 frontier deep research systems,\nincluding single-agent web search, single-agent deep research, and multi-agent\nsystems. Our analysis reveals current strengths, recurring failure modes, and\nkey system components needed to advance reliable, insightful deep research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14240.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "651651f5d93a51ceda3021c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/FE2uGpTKBRWMKTDBv1H-g.png",
            "fullname": "Jiayu (Mila) Wang",
            "name": "MilaWang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13928",
            "authors": [
                {
                    "_id": "68f2a8838589920bf4d318fe",
                    "name": "Shuo Xing",
                    "hidden": false
                },
                {
                    "_id": "68f2a8838589920bf4d318ff",
                    "name": "Junyuan Hong",
                    "hidden": false
                },
                {
                    "_id": "68f2a8838589920bf4d31900",
                    "name": "Yifan Wang",
                    "hidden": false
                },
                {
                    "_id": "68f2a8838589920bf4d31901",
                    "name": "Runjin Chen",
                    "hidden": false
                },
                {
                    "_id": "68f2a8838589920bf4d31902",
                    "name": "Zhenyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f2a8838589920bf4d31903",
                    "name": "Ananth Grama",
                    "hidden": false
                },
                {
                    "_id": "68f2a8838589920bf4d31904",
                    "name": "Zhengzhong Tu",
                    "hidden": false
                },
                {
                    "_id": "68f2a8838589920bf4d31905",
                    "name": "Zhangyang Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6400cf982b67d27affce2d89/OMSzdVwKBKDF_Au5SVY0W.png"
            ],
            "publishedAt": "2025-10-15T13:28:49.000Z",
            "submittedOnDailyAt": "2025-10-17T19:11:29.993Z",
            "title": "LLMs Can Get \"Brain Rot\"!",
            "submittedOnDailyBy": {
                "_id": "6400cf982b67d27affce2d89",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6400cf982b67d27affce2d89/Vs042d2M-iV2wk9Q_Jqh3.jpeg",
                "isPro": false,
                "fullname": "Junyuan Hong",
                "user": "jyhong836",
                "type": "user"
            },
            "summary": "We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk\nweb text induces lasting cognitive decline in large language models (LLMs). To\ncausally isolate data quality, we run controlled experiments on real Twitter/X\ncorpora, constructing junk and reversely controlled datasets via two orthogonal\noperationalizations: M1 (engagement degree) and M2 (semantic quality), with\nmatched token scale and training operations across conditions. Contrary to the\ncontrol group, continual pre-training of 4 LLMs on the junk dataset causes\nnon-trivial declines (Hedges' g>0.3) on reasoning, long-context\nunderstanding, safety, and inflating \"dark traits\" (e.g., psychopathy,\nnarcissism). The gradual mixtures of junk and control datasets also yield\ndose-response cognition decay: for example, under M1, ARC-Challenge with Chain\nOf Thoughts drops 74.9 rightarrow 57.2 and RULER-CWE 84.4 rightarrow 52.3\nas junk ratio rises from 0% to 100%.\n  Error forensics reveal several key insights. First, we identify\nthought-skipping as the primary lesion: models increasingly truncate or skip\nreasoning chains, explaining most of the error growth. Second, partial but\nincomplete healing is observed: scaling instruction tuning and clean data\npre-training improve the declined cognition yet cannot restore baseline\ncapability, suggesting persistent representational drift rather than format\nmismatch. Finally, we discover that the popularity, a non-semantic metric, of a\ntweet is a better indicator of the Brain Rot effect than the length in M1.\nTogether, the results provide significant, multi-perspective evidence that data\nquality is a causal driver of LLM capability decay, reframing curation for\ncontinual pretraining as a training-time safety problem and motivating\nroutine \"cognitive health checks\" for deployed LLMs.",
            "upvotes": 8,
            "discussionId": "68f2a8838589920bf4d31906",
            "projectPage": "https://llm-brain-rot.github.io/",
            "githubRepo": "https://github.com/llm-brain-rot/llm-brain-rot",
            "ai_summary": "Continual exposure to low-quality web text leads to cognitive decline in large language models, affecting reasoning, context understanding, safety, and personality traits, with partial recovery possible through instruction tuning and clean data pre-training.",
            "ai_keywords": [
                "LLM Brain Rot Hypothesis",
                "junk web text",
                "cognitive decline",
                "large language models",
                "Twitter/X corpora",
                "engagement degree",
                "semantic quality",
                "reasoning",
                "long-context understanding",
                "safety",
                "dark traits",
                "ARC-Challenge",
                "Chain Of Thoughts",
                "RULER-CWE",
                "thought-skipping",
                "representational drift",
                "training-time safety",
                "cognitive health checks"
            ],
            "githubStars": 7,
            "organization": {
                "_id": "64f0a050a29b74227f5a56a5",
                "name": "vita-group",
                "fullname": "Visual Informatics Group @ University of Texas at Austin",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6400cf982b67d27affce2d89/6-Y5FtffpEueYTPLDugMU.png"
            }
        },
        "publishedAt": "2025-10-15T09:28:49.000Z",
        "title": "LLMs Can Get \"Brain Rot\"!",
        "summary": "We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk\nweb text induces lasting cognitive decline in large language models (LLMs). To\ncausally isolate data quality, we run controlled experiments on real Twitter/X\ncorpora, constructing junk and reversely controlled datasets via two orthogonal\noperationalizations: M1 (engagement degree) and M2 (semantic quality), with\nmatched token scale and training operations across conditions. Contrary to the\ncontrol group, continual pre-training of 4 LLMs on the junk dataset causes\nnon-trivial declines (Hedges' g>0.3) on reasoning, long-context\nunderstanding, safety, and inflating \"dark traits\" (e.g., psychopathy,\nnarcissism). The gradual mixtures of junk and control datasets also yield\ndose-response cognition decay: for example, under M1, ARC-Challenge with Chain\nOf Thoughts drops 74.9 rightarrow 57.2 and RULER-CWE 84.4 rightarrow 52.3\nas junk ratio rises from 0% to 100%.\n  Error forensics reveal several key insights. First, we identify\nthought-skipping as the primary lesion: models increasingly truncate or skip\nreasoning chains, explaining most of the error growth. Second, partial but\nincomplete healing is observed: scaling instruction tuning and clean data\npre-training improve the declined cognition yet cannot restore baseline\ncapability, suggesting persistent representational drift rather than format\nmismatch. Finally, we discover that the popularity, a non-semantic metric, of a\ntweet is a better indicator of the Brain Rot effect than the length in M1.\nTogether, the results provide significant, multi-perspective evidence that data\nquality is a causal driver of LLM capability decay, reframing curation for\ncontinual pretraining as a training-time safety problem and motivating\nroutine \"cognitive health checks\" for deployed LLMs.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6400cf982b67d27affce2d89/OMSzdVwKBKDF_Au5SVY0W.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13928.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6400cf982b67d27affce2d89",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6400cf982b67d27affce2d89/Vs042d2M-iV2wk9Q_Jqh3.jpeg",
            "fullname": "Junyuan Hong",
            "name": "jyhong836",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "64f0a050a29b74227f5a56a5",
            "name": "vita-group",
            "fullname": "Visual Informatics Group @ University of Texas at Austin",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6400cf982b67d27affce2d89/6-Y5FtffpEueYTPLDugMU.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13054",
            "authors": [
                {
                    "_id": "68f0817e36f8b025381e1a20",
                    "name": "Ankit Goyal",
                    "hidden": false
                },
                {
                    "_id": "68f0817e36f8b025381e1a21",
                    "name": "Hugo Hadfield",
                    "hidden": false
                },
                {
                    "_id": "68f0817e36f8b025381e1a22",
                    "name": "Xuning Yang",
                    "hidden": false
                },
                {
                    "_id": "68f0817e36f8b025381e1a23",
                    "name": "Valts Blukis",
                    "hidden": false
                },
                {
                    "_id": "68f0817e36f8b025381e1a24",
                    "name": "Fabio Ramos",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/666c6f67bc840e6748554a91/0p5wqkJj8rDIMzA2SRj6z.mp4"
            ],
            "publishedAt": "2025-10-15T00:31:10.000Z",
            "submittedOnDailyAt": "2025-10-17T00:51:30.866Z",
            "title": "VLA-0: Building State-of-the-Art VLAs with Zero Modification",
            "submittedOnDailyBy": {
                "_id": "666c6f67bc840e6748554a91",
                "avatarUrl": "/avatars/72121715e14f720e5c1d029b7f00d55d.svg",
                "isPro": false,
                "fullname": "Ankit Goyal",
                "user": "ankgoyal",
                "type": "user"
            },
            "summary": "Vision-Language-Action models (VLAs) hold immense promise for enabling\ngeneralist robot manipulation. However, the best way to build them remains an\nopen question. Current approaches often add complexity, such as modifying the\nexisting vocabulary of a Vision-Language Model (VLM) with action tokens or\nintroducing special action heads. Curiously, the simplest strategy of\nrepresenting actions directly as text has remained largely unexplored. This\nwork introduces VLA-0 to investigate this idea. We find that VLA-0 is not only\neffective; it is surprisingly powerful. With the right design, VLA-0\noutperforms more involved models. On LIBERO, a popular benchmark for evaluating\nVLAs, VLA-0 outperforms all existing methods trained on the same robotic data,\nincluding pi_0.5-KI, OpenVLA-OFT and SmolVLA. Furthermore, without\nlarge-scale robotics-specific training, it outperforms methods trained on\nlarge-scale robotic data, like pi_0.5-KI, pi_0, GR00T-N1 and MolmoAct.\nThese findings also translate to the real world, where VLA-0 outperforms\nSmolVLA, a VLA model pre-trained on large-scale real data. This paper\nsummarizes our unexpected findings and spells out the specific techniques\nrequired to unlock the high performance of this simple yet potent VLA design.\nVisual results, code, and trained models are provided here:\nhttps://vla0.github.io/.",
            "upvotes": 7,
            "discussionId": "68f0817e36f8b025381e1a25",
            "projectPage": "https://vla0.github.io/",
            "githubRepo": "https://github.com/NVlabs/vla0",
            "ai_summary": "A simple VLA model, VLA-0, outperforms more complex models on robotic manipulation tasks by representing actions as text without additional modifications or large-scale training.",
            "ai_keywords": [
                "Vision-Language-Action models",
                "Vision-Language Model",
                "action tokens",
                "action heads",
                "LIBERO",
                "$\\pi_0.5$-KI",
                "OpenVLA-OFT",
                "SmolVLA",
                "GR00T-N1",
                "MolmoAct"
            ],
            "githubStars": 79,
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-10-14T20:31:10.000Z",
        "title": "VLA-0: Building State-of-the-Art VLAs with Zero Modification",
        "summary": "Vision-Language-Action models (VLAs) hold immense promise for enabling\ngeneralist robot manipulation. However, the best way to build them remains an\nopen question. Current approaches often add complexity, such as modifying the\nexisting vocabulary of a Vision-Language Model (VLM) with action tokens or\nintroducing special action heads. Curiously, the simplest strategy of\nrepresenting actions directly as text has remained largely unexplored. This\nwork introduces VLA-0 to investigate this idea. We find that VLA-0 is not only\neffective; it is surprisingly powerful. With the right design, VLA-0\noutperforms more involved models. On LIBERO, a popular benchmark for evaluating\nVLAs, VLA-0 outperforms all existing methods trained on the same robotic data,\nincluding pi_0.5-KI, OpenVLA-OFT and SmolVLA. Furthermore, without\nlarge-scale robotics-specific training, it outperforms methods trained on\nlarge-scale robotic data, like pi_0.5-KI, pi_0, GR00T-N1 and MolmoAct.\nThese findings also translate to the real world, where VLA-0 outperforms\nSmolVLA, a VLA model pre-trained on large-scale real data. This paper\nsummarizes our unexpected findings and spells out the specific techniques\nrequired to unlock the high performance of this simple yet potent VLA design.\nVisual results, code, and trained models are provided here:\nhttps://vla0.github.io/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/666c6f67bc840e6748554a91/0p5wqkJj8rDIMzA2SRj6z.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13054.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "666c6f67bc840e6748554a91",
            "avatarUrl": "/avatars/72121715e14f720e5c1d029b7f00d55d.svg",
            "fullname": "Ankit Goyal",
            "name": "ankgoyal",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14980",
            "authors": [
                {
                    "_id": "68f1c78c6e0bef323a68feb6",
                    "name": "Wenqian Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1c78c6e0bef323a68feb7",
                    "name": "Weiyang Liu",
                    "hidden": false
                },
                {
                    "_id": "68f1c78c6e0bef323a68feb8",
                    "name": "Zhen Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:59:58.000Z",
            "submittedOnDailyAt": "2025-10-17T13:53:51.208Z",
            "title": "Agentic Design of Compositional Machines",
            "submittedOnDailyBy": {
                "_id": "648905d1a15c43c791d4381f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
                "isPro": false,
                "fullname": "Weiyang Liu",
                "user": "wy1iu",
                "type": "user"
            },
            "summary": "The design of complex machines stands as both a marker of human intelligence\nand a foundation of engineering practice. Given recent advances in large\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\napproach this question through the lens of compositional machine design: a task\nin which machines are assembled from standardized components to meet functional\ndemands like locomotion or manipulation in a simulated physical environment. To\nsupport this investigation, we introduce BesiegeField, a testbed built on the\nmachine-building game Besiege, which enables part-based construction, physical\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\nrequired for success, including spatial reasoning, strategic assembly, and\ninstruction-following. As current open-source models fall short, we explore\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\ndataset, conduct RL finetuning experiments, and highlight open challenges at\nthe intersection of language, machine design, and physical reasoning.",
            "upvotes": 5,
            "discussionId": "68f1c78c6e0bef323a68feb9",
            "projectPage": "https://besiegefield.github.io/",
            "githubRepo": "https://github.com/Godheritage/BesiegeField",
            "ai_summary": "State-of-the-art LLMs are benchmarked in a machine design testbed, BesiegeField, highlighting the need for reinforcement learning to improve spatial reasoning, strategic assembly, and instruction-following capabilities.",
            "ai_keywords": [
                "large language models",
                "compositional machine design",
                "BesiegeField",
                "spatial reasoning",
                "strategic assembly",
                "instruction-following",
                "reinforcement learning",
                "cold-start dataset",
                "RL finetuning"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-10-16T13:59:58.000Z",
        "title": "Agentic Design of Compositional Machines",
        "summary": "The design of complex machines stands as both a marker of human intelligence\nand a foundation of engineering practice. Given recent advances in large\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\napproach this question through the lens of compositional machine design: a task\nin which machines are assembled from standardized components to meet functional\ndemands like locomotion or manipulation in a simulated physical environment. To\nsupport this investigation, we introduce BesiegeField, a testbed built on the\nmachine-building game Besiege, which enables part-based construction, physical\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\nrequired for success, including spatial reasoning, strategic assembly, and\ninstruction-following. As current open-source models fall short, we explore\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\ndataset, conduct RL finetuning experiments, and highlight open challenges at\nthe intersection of language, machine design, and physical reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14980.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "648905d1a15c43c791d4381f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
            "fullname": "Weiyang Liu",
            "name": "wy1iu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14978",
            "authors": [
                {
                    "_id": "68f1a20a6e0bef323a68fcc5",
                    "name": "Nupur Kumari",
                    "hidden": false
                },
                {
                    "_id": "68f1a20a6e0bef323a68fcc6",
                    "name": "Sheng-Yu Wang",
                    "hidden": false
                },
                {
                    "_id": "68f1a20a6e0bef323a68fcc7",
                    "name": "Nanxuan Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f1a20a6e0bef323a68fcc8",
                    "name": "Yotam Nitzan",
                    "hidden": false
                },
                {
                    "_id": "68f1a20a6e0bef323a68fcc9",
                    "name": "Yuheng Li",
                    "hidden": false
                },
                {
                    "_id": "68f1a20a6e0bef323a68fcca",
                    "name": "Krishna Kumar Singh",
                    "hidden": false
                },
                {
                    "_id": "68f1a20a6e0bef323a68fccb",
                    "name": "Richard Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1a20a6e0bef323a68fccc",
                    "name": "Eli Shechtman",
                    "hidden": false
                },
                {
                    "_id": "68f1a20a6e0bef323a68fccd",
                    "name": "Jun-Yan Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f1a20a6e0bef323a68fcce",
                    "name": "Xun Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:59:57.000Z",
            "submittedOnDailyAt": "2025-10-17T00:25:36.586Z",
            "title": "Learning an Image Editing Model without Image Editing Pairs",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent image editing models have achieved impressive results while following\nnatural language editing instructions, but they rely on supervised fine-tuning\nwith large datasets of input-target pairs. This is a critical bottleneck, as\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\nuse synthetic training pairs that leverage the zero-shot capabilities of\nexisting models. However, this can propagate and magnify the artifacts of the\npretrained model into the final trained model. In this work, we present a new\ntraining paradigm that eliminates the need for paired data entirely. Our\napproach directly optimizes a few-step diffusion model by unrolling it during\ntraining and leveraging feedback from vision-language models (VLMs). For each\ninput and editing instruction, the VLM evaluates if an edit follows the\ninstruction and preserves unchanged content, providing direct gradients for\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\nmatching loss (DMD), which constrains generated images to remain within the\nimage manifold learned by pretrained models. We evaluate our method on standard\nbenchmarks and include an extensive ablation study. Without any paired data,\nour method performs on par with various image editing diffusion models trained\non extensive supervised paired data, under the few-step setting. Given the same\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
            "upvotes": 5,
            "discussionId": "68f1a20a6e0bef323a68fccf",
            "ai_summary": "A new training paradigm for image editing models uses unrolled diffusion models and vision-language feedback to achieve performance comparable to supervised models without paired data.",
            "ai_keywords": [
                "diffusion model",
                "unrolling",
                "vision-language models",
                "distribution matching loss",
                "image manifold",
                "few-step setting",
                "RL-based techniques",
                "Flow-GRPO"
            ],
            "organization": {
                "_id": "61e5d14f77496de0a6d95c6b",
                "name": "adobe",
                "fullname": "Adobe",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
            }
        },
        "publishedAt": "2025-10-16T13:59:57.000Z",
        "title": "Learning an Image Editing Model without Image Editing Pairs",
        "summary": "Recent image editing models have achieved impressive results while following\nnatural language editing instructions, but they rely on supervised fine-tuning\nwith large datasets of input-target pairs. This is a critical bottleneck, as\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\nuse synthetic training pairs that leverage the zero-shot capabilities of\nexisting models. However, this can propagate and magnify the artifacts of the\npretrained model into the final trained model. In this work, we present a new\ntraining paradigm that eliminates the need for paired data entirely. Our\napproach directly optimizes a few-step diffusion model by unrolling it during\ntraining and leveraging feedback from vision-language models (VLMs). For each\ninput and editing instruction, the VLM evaluates if an edit follows the\ninstruction and preserves unchanged content, providing direct gradients for\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\nmatching loss (DMD), which constrains generated images to remain within the\nimage manifold learned by pretrained models. We evaluate our method on standard\nbenchmarks and include an extensive ablation study. Without any paired data,\nour method performs on par with various image editing diffusion models trained\non extensive supervised paired data, under the few-step setting. Given the same\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14978.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 130
        },
        "organization": {
            "_id": "61e5d14f77496de0a6d95c6b",
            "name": "adobe",
            "fullname": "Adobe",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14974",
            "authors": [
                {
                    "_id": "68f19b086e0bef323a68fc1a",
                    "user": {
                        "_id": "638067fcb334960c987fbeda",
                        "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
                        "isPro": false,
                        "fullname": "Hansheng Chen",
                        "user": "Lakonik",
                        "type": "user"
                    },
                    "name": "Hansheng Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:11:30.593Z",
                    "hidden": false
                },
                {
                    "_id": "68f19b086e0bef323a68fc1b",
                    "name": "Kai Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f19b086e0bef323a68fc1c",
                    "name": "Hao Tan",
                    "hidden": false
                },
                {
                    "_id": "68f19b086e0bef323a68fc1d",
                    "name": "Leonidas Guibas",
                    "hidden": false
                },
                {
                    "_id": "68f19b086e0bef323a68fc1e",
                    "name": "Gordon Wetzstein",
                    "hidden": false
                },
                {
                    "_id": "68f19b086e0bef323a68fc1f",
                    "name": "Sai Bi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:59:51.000Z",
            "submittedOnDailyAt": "2025-10-17T00:12:55.031Z",
            "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
            "submittedOnDailyBy": {
                "_id": "638067fcb334960c987fbeda",
                "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
                "isPro": false,
                "fullname": "Hansheng Chen",
                "user": "Lakonik",
                "type": "user"
            },
            "summary": "Few-step diffusion or flow-based generative models typically distill a\nvelocity-predicting teacher into a student that predicts a shortcut towards\ndenoised data. This format mismatch has led to complex distillation procedures\nthat often suffer from a quality-diversity trade-off. To address this, we\npropose policy-based flow models (pi-Flow). pi-Flow modifies the output\nlayer of a student flow model to predict a network-free policy at one timestep.\nThe policy then produces dynamic flow velocities at future substeps with\nnegligible overhead, enabling fast and accurate ODE integration on these\nsubsteps without extra network evaluations. To match the policy's ODE\ntrajectory to the teacher's, we introduce a novel imitation distillation\napproach, which matches the policy's velocity to the teacher's along the\npolicy's trajectory using a standard ell_2 flow matching loss. By simply\nmimicking the teacher's behavior, pi-Flow enables stable and scalable\ntraining and avoids the quality-diversity trade-off. On ImageNet 256^2, it\nattains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT\narchitecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, pi-Flow achieves\nsubstantially better diversity than state-of-the-art few-step methods, while\nmaintaining teacher-level quality.",
            "upvotes": 5,
            "discussionId": "68f19b086e0bef323a68fc20",
            "githubRepo": "https://github.com/Lakonik/piFlow",
            "ai_summary": "Policy-based flow models enable efficient and high-quality image generation by distilling teacher models into student models with dynamic flow velocities, improving diversity and quality.",
            "ai_keywords": [
                "velocity-predicting teacher",
                "student flow model",
                "policy-based flow models",
                "$\\pi$-Flow",
                "dynamic flow velocities",
                "ODE integration",
                "imitation distillation",
                "$\\ell_2$ flow matching loss",
                "ImageNet 256$^2$",
                "FID",
                "MeanFlow",
                "DiT architecture",
                "FLUX.1-12B",
                "Qwen-Image-20B",
                "NFE"
            ],
            "githubStars": 34,
            "organization": {
                "_id": "61e5d14f77496de0a6d95c6b",
                "name": "adobe",
                "fullname": "Adobe",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
            }
        },
        "publishedAt": "2025-10-16T13:59:51.000Z",
        "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
        "summary": "Few-step diffusion or flow-based generative models typically distill a\nvelocity-predicting teacher into a student that predicts a shortcut towards\ndenoised data. This format mismatch has led to complex distillation procedures\nthat often suffer from a quality-diversity trade-off. To address this, we\npropose policy-based flow models (pi-Flow). pi-Flow modifies the output\nlayer of a student flow model to predict a network-free policy at one timestep.\nThe policy then produces dynamic flow velocities at future substeps with\nnegligible overhead, enabling fast and accurate ODE integration on these\nsubsteps without extra network evaluations. To match the policy's ODE\ntrajectory to the teacher's, we introduce a novel imitation distillation\napproach, which matches the policy's velocity to the teacher's along the\npolicy's trajectory using a standard ell_2 flow matching loss. By simply\nmimicking the teacher's behavior, pi-Flow enables stable and scalable\ntraining and avoids the quality-diversity trade-off. On ImageNet 256^2, it\nattains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT\narchitecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, pi-Flow achieves\nsubstantially better diversity than state-of-the-art few-step methods, while\nmaintaining teacher-level quality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14974.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638067fcb334960c987fbeda",
            "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
            "fullname": "Hansheng Chen",
            "name": "Lakonik",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "organization": {
            "_id": "61e5d14f77496de0a6d95c6b",
            "name": "adobe",
            "fullname": "Adobe",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.14969",
            "authors": [
                {
                    "_id": "68f19da26e0bef323a68fc68",
                    "name": "Yiming Wang",
                    "hidden": false
                },
                {
                    "_id": "68f19da26e0bef323a68fc69",
                    "name": "Da Yin",
                    "hidden": false
                },
                {
                    "_id": "68f19da26e0bef323a68fc6a",
                    "name": "Yuedong Cui",
                    "hidden": false
                },
                {
                    "_id": "68f19da26e0bef323a68fc6b",
                    "name": "Ruichen Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f19da26e0bef323a68fc6c",
                    "name": "Zhiqian Li",
                    "hidden": false
                },
                {
                    "_id": "68f19da26e0bef323a68fc6d",
                    "name": "Zongyu Lin",
                    "hidden": false
                },
                {
                    "_id": "68f19da26e0bef323a68fc6e",
                    "name": "Di Wu",
                    "hidden": false
                },
                {
                    "_id": "68f19da26e0bef323a68fc6f",
                    "name": "Xueqing Wu",
                    "hidden": false
                },
                {
                    "_id": "68f19da26e0bef323a68fc70",
                    "name": "Chenchen Ye",
                    "hidden": false
                },
                {
                    "_id": "68f19da26e0bef323a68fc71",
                    "name": "Yu Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f19da26e0bef323a68fc72",
                    "name": "Kai-Wei Chang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/634e4670a51d5df8c2d92fce/hP00XFC26Wxs0TvDfDWas.mp4"
            ],
            "publishedAt": "2025-10-16T17:59:38.000Z",
            "submittedOnDailyAt": "2025-10-17T00:20:18.195Z",
            "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent\n  Training",
            "submittedOnDailyBy": {
                "_id": "634e4670a51d5df8c2d92fce",
                "avatarUrl": "/avatars/c52d7150b4de6a2eb2d83b345d35cbc2.svg",
                "isPro": false,
                "fullname": "Da Yin",
                "user": "DaYin",
                "type": "user"
            },
            "summary": "Digital agents require diverse, large-scale UI trajectories to generalize\nacross real-world tasks, yet collecting such data is prohibitively expensive in\nboth human annotation, infra and engineering perspectives. To this end, we\nintroduce UI-Simulator, a scalable paradigm that generates\nstructured UI states and transitions to synthesize training trajectories at\nscale. Our paradigm integrates a digital world simulator for diverse UI states,\na guided rollout process for coherent exploration, and a trajectory wrapper\nthat produces high-quality and diverse trajectories for agent training. We\nfurther propose UI-Simulator-Grow, a targeted scaling strategy that\nenables more rapid and data-efficient scaling by prioritizing high-impact tasks\nand synthesizes informative trajectory variants. Experiments on WebArena and\nAndroidWorld show that UI-Simulator rivals or surpasses open-source agents\ntrained on real UIs with significantly better robustness, despite using weaker\nteacher models. Moreover, UI-Simulator-Grow matches the performance of\nLlama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,\nhighlighting the potential of targeted synthesis scaling paradigm to\ncontinuously and efficiently enhance the digital agents.",
            "upvotes": 5,
            "discussionId": "68f19da26e0bef323a68fc73",
            "ai_summary": "UI-Simulator generates diverse UI trajectories for digital agents using a scalable paradigm, improving robustness and performance with targeted scaling strategies.",
            "ai_keywords": [
                "UI-Simulator",
                "digital world simulator",
                "guided rollout process",
                "trajectory wrapper",
                "UI-Simulator-Grow",
                "WebArena",
                "AndroidWorld",
                "Llama-3-70B-Instruct",
                "Llama-3-8B-Instruct"
            ],
            "organization": {
                "_id": "60b492a78e9589ce25930346",
                "name": "uclanlp",
                "fullname": "UCLA NLP",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1622446751930-5ec0135ded25d76864d553f1.png"
            }
        },
        "publishedAt": "2025-10-16T13:59:38.000Z",
        "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent\n  Training",
        "summary": "Digital agents require diverse, large-scale UI trajectories to generalize\nacross real-world tasks, yet collecting such data is prohibitively expensive in\nboth human annotation, infra and engineering perspectives. To this end, we\nintroduce UI-Simulator, a scalable paradigm that generates\nstructured UI states and transitions to synthesize training trajectories at\nscale. Our paradigm integrates a digital world simulator for diverse UI states,\na guided rollout process for coherent exploration, and a trajectory wrapper\nthat produces high-quality and diverse trajectories for agent training. We\nfurther propose UI-Simulator-Grow, a targeted scaling strategy that\nenables more rapid and data-efficient scaling by prioritizing high-impact tasks\nand synthesizes informative trajectory variants. Experiments on WebArena and\nAndroidWorld show that UI-Simulator rivals or surpasses open-source agents\ntrained on real UIs with significantly better robustness, despite using weaker\nteacher models. Moreover, UI-Simulator-Grow matches the performance of\nLlama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,\nhighlighting the potential of targeted synthesis scaling paradigm to\ncontinuously and efficiently enhance the digital agents.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/634e4670a51d5df8c2d92fce/hP00XFC26Wxs0TvDfDWas.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14969.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "634e4670a51d5df8c2d92fce",
            "avatarUrl": "/avatars/c52d7150b4de6a2eb2d83b345d35cbc2.svg",
            "fullname": "Da Yin",
            "name": "DaYin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 0
        },
        "organization": {
            "_id": "60b492a78e9589ce25930346",
            "name": "uclanlp",
            "fullname": "UCLA NLP",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1622446751930-5ec0135ded25d76864d553f1.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14949",
            "authors": [
                {
                    "_id": "68f1d2d56e0bef323a68ff5e",
                    "name": "Yu Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f1d2d56e0bef323a68ff5f",
                    "name": "Sohyun An",
                    "hidden": false
                },
                {
                    "_id": "68f1d2d56e0bef323a68ff60",
                    "user": {
                        "_id": "624cf6ba5ee9cb3d45588971",
                        "avatarUrl": "/avatars/f0fc5e353951d3e501542dab559bbdb5.svg",
                        "isPro": false,
                        "fullname": "Haikang Deng",
                        "user": "hk",
                        "type": "user"
                    },
                    "name": "Haikang Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T05:56:24.216Z",
                    "hidden": false
                },
                {
                    "_id": "68f1d2d56e0bef323a68ff61",
                    "name": "Da Yin",
                    "hidden": false
                },
                {
                    "_id": "68f1d2d56e0bef323a68ff62",
                    "name": "Clark Peng",
                    "hidden": false
                },
                {
                    "_id": "68f1d2d56e0bef323a68ff63",
                    "name": "Cho-Jui Hsieh",
                    "hidden": false
                },
                {
                    "_id": "68f1d2d56e0bef323a68ff64",
                    "name": "Kai-Wei Chang",
                    "hidden": false
                },
                {
                    "_id": "68f1d2d56e0bef323a68ff65",
                    "name": "Nanyun Peng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:56:55.000Z",
            "submittedOnDailyAt": "2025-10-17T04:38:12.186Z",
            "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal\n  Generation",
            "submittedOnDailyBy": {
                "_id": "624cf6ba5ee9cb3d45588971",
                "avatarUrl": "/avatars/f0fc5e353951d3e501542dab559bbdb5.svg",
                "isPro": false,
                "fullname": "Haikang Deng",
                "user": "hk",
                "type": "user"
            },
            "summary": "Contact languages like English exhibit rich regional variations in the form\nof dialects, which are often used by dialect speakers interacting with\ngenerative models. However, can multimodal generative models effectively\nproduce content given dialectal textual input? In this work, we study this\nquestion by constructing a new large-scale benchmark spanning six common\nEnglish dialects. We work with dialect speakers to collect and verify over 4200\nunique prompts and evaluate on 17 image and video generative models. Our\nautomatic and human evaluation results show that current state-of-the-art\nmultimodal generative models exhibit 32.26% to 48.17% performance degradation\nwhen a single dialect word is used in the prompt. Common mitigation methods\nsuch as fine-tuning and prompt rewriting can only improve dialect performance\nby small margins (< 7%), while potentially incurring significant performance\ndegradation in Standard American English (SAE). To this end, we design a\ngeneral encoder-based mitigation strategy for multimodal generative models. Our\nmethod teaches the model to recognize new dialect features while preserving SAE\nperformance. Experiments on models such as Stable Diffusion 1.5 show that our\nmethod is able to simultaneously raise performance on five dialects to be on\npar with SAE (+34.4%), while incurring near zero cost to SAE performance.",
            "upvotes": 5,
            "discussionId": "68f1d2d56e0bef323a68ff66",
            "projectPage": "https://dialectgen.github.io/",
            "githubRepo": "https://github.com/DialectGen/DialectGen",
            "ai_summary": "A new benchmark and encoder-based mitigation strategy improve multimodal generative models' performance on dialectal textual input without degrading performance on Standard American English.",
            "ai_keywords": [
                "multimodal generative models",
                "dialectal textual input",
                "large-scale benchmark",
                "dialect speakers",
                "image and video generative models",
                "performance degradation",
                "fine-tuning",
                "prompt rewriting",
                "encoder-based mitigation strategy",
                "Stable Diffusion 1.5"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "60b492a78e9589ce25930346",
                "name": "uclanlp",
                "fullname": "UCLA NLP",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1622446751930-5ec0135ded25d76864d553f1.png"
            }
        },
        "publishedAt": "2025-10-16T13:56:55.000Z",
        "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal\n  Generation",
        "summary": "Contact languages like English exhibit rich regional variations in the form\nof dialects, which are often used by dialect speakers interacting with\ngenerative models. However, can multimodal generative models effectively\nproduce content given dialectal textual input? In this work, we study this\nquestion by constructing a new large-scale benchmark spanning six common\nEnglish dialects. We work with dialect speakers to collect and verify over 4200\nunique prompts and evaluate on 17 image and video generative models. Our\nautomatic and human evaluation results show that current state-of-the-art\nmultimodal generative models exhibit 32.26% to 48.17% performance degradation\nwhen a single dialect word is used in the prompt. Common mitigation methods\nsuch as fine-tuning and prompt rewriting can only improve dialect performance\nby small margins (< 7%), while potentially incurring significant performance\ndegradation in Standard American English (SAE). To this end, we design a\ngeneral encoder-based mitigation strategy for multimodal generative models. Our\nmethod teaches the model to recognize new dialect features while preserving SAE\nperformance. Experiments on models such as Stable Diffusion 1.5 show that our\nmethod is able to simultaneously raise performance on five dialects to be on\npar with SAE (+34.4%), while incurring near zero cost to SAE performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14949.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "624cf6ba5ee9cb3d45588971",
            "avatarUrl": "/avatars/f0fc5e353951d3e501542dab559bbdb5.svg",
            "fullname": "Haikang Deng",
            "name": "hk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "60b492a78e9589ce25930346",
            "name": "uclanlp",
            "fullname": "UCLA NLP",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1622446751930-5ec0135ded25d76864d553f1.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.14211",
            "authors": [
                {
                    "_id": "68f1ccce6e0bef323a68ff16",
                    "name": "Beomseok Kang",
                    "hidden": false
                },
                {
                    "_id": "68f1ccce6e0bef323a68ff17",
                    "user": {
                        "_id": "662672eaebdfec5cfdf1d034",
                        "avatarUrl": "/avatars/61bc7add693c555e29ad3c1112215684.svg",
                        "isPro": false,
                        "fullname": "Jiwon Song",
                        "user": "jiwonsong",
                        "type": "user"
                    },
                    "name": "Jiwon Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T05:56:29.645Z",
                    "hidden": false
                },
                {
                    "_id": "68f1ccce6e0bef323a68ff18",
                    "name": "Jae-Joon Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T01:37:39.000Z",
            "submittedOnDailyAt": "2025-10-17T03:29:50.976Z",
            "title": "LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning",
            "submittedOnDailyBy": {
                "_id": "662672eaebdfec5cfdf1d034",
                "avatarUrl": "/avatars/61bc7add693c555e29ad3c1112215684.svg",
                "isPro": false,
                "fullname": "Jiwon Song",
                "user": "jiwonsong",
                "type": "user"
            },
            "summary": "Multi-stage reasoning has emerged as an effective strategy for enhancing the\nreasoning capability of small language models by decomposing complex problems\ninto sequential sub-stages. However, this comes at the cost of increased\nlatency. We observe that existing adaptive acceleration techniques, such as\nlayer skipping, struggle to balance efficiency and accuracy in this setting due\nto two key challenges: (1) stage-wise variation in skip sensitivity, and (2)\nthe generation of redundant output tokens. To address these, we propose\nLiteStage, a latency-aware layer skipping framework for multi-stage reasoning.\nLiteStage combines a stage-wise offline search that allocates optimal layer\nbudgets with an online confidence-based generation early exit to suppress\nunnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and\nStrategyQA, show that LiteStage achieves up to 1.70x speedup with less than\n4.0% accuracy loss, outperforming prior training-free layer skipping methods.",
            "upvotes": 5,
            "discussionId": "68f1cccf6e0bef323a68ff19",
            "githubRepo": "https://github.com/beomseokg/LiteStage",
            "ai_summary": "LiteStage, a latency-aware layer skipping framework, enhances multi-stage reasoning by optimizing layer budgets and suppressing redundant output tokens, achieving significant speedup with minimal accuracy loss.",
            "ai_keywords": [
                "multi-stage reasoning",
                "layer skipping",
                "stage-wise variation",
                "skip sensitivity",
                "redundant output tokens",
                "confidence-based generation",
                "early exit",
                "OBQA",
                "CSQA",
                "StrategyQA"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "6304ac9f5d136debcecba4fd",
                "name": "snu",
                "fullname": "Seoul National University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1661250691168-6304abe86dbbb80f1636b20b.jpeg"
            }
        },
        "publishedAt": "2025-10-15T21:37:39.000Z",
        "title": "LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning",
        "summary": "Multi-stage reasoning has emerged as an effective strategy for enhancing the\nreasoning capability of small language models by decomposing complex problems\ninto sequential sub-stages. However, this comes at the cost of increased\nlatency. We observe that existing adaptive acceleration techniques, such as\nlayer skipping, struggle to balance efficiency and accuracy in this setting due\nto two key challenges: (1) stage-wise variation in skip sensitivity, and (2)\nthe generation of redundant output tokens. To address these, we propose\nLiteStage, a latency-aware layer skipping framework for multi-stage reasoning.\nLiteStage combines a stage-wise offline search that allocates optimal layer\nbudgets with an online confidence-based generation early exit to suppress\nunnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and\nStrategyQA, show that LiteStage achieves up to 1.70x speedup with less than\n4.0% accuracy loss, outperforming prior training-free layer skipping methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14211.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "662672eaebdfec5cfdf1d034",
            "avatarUrl": "/avatars/61bc7add693c555e29ad3c1112215684.svg",
            "fullname": "Jiwon Song",
            "name": "jiwonsong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "6304ac9f5d136debcecba4fd",
            "name": "snu",
            "fullname": "Seoul National University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1661250691168-6304abe86dbbb80f1636b20b.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.13454",
            "authors": [
                {
                    "_id": "68f19cc36e0bef323a68fc3b",
                    "name": "Hyojun Go",
                    "hidden": false
                },
                {
                    "_id": "68f19cc36e0bef323a68fc3c",
                    "name": "Dominik Narnhofer",
                    "hidden": false
                },
                {
                    "_id": "68f19cc36e0bef323a68fc3d",
                    "name": "Goutam Bhat",
                    "hidden": false
                },
                {
                    "_id": "68f19cc36e0bef323a68fc3e",
                    "name": "Prune Truong",
                    "hidden": false
                },
                {
                    "_id": "68f19cc36e0bef323a68fc3f",
                    "name": "Federico Tombari",
                    "hidden": false
                },
                {
                    "_id": "68f19cc36e0bef323a68fc40",
                    "name": "Konrad Schindler",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T11:55:08.000Z",
            "submittedOnDailyAt": "2025-10-17T00:09:34.641Z",
            "title": "VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a\n  Video Generator",
            "submittedOnDailyBy": {
                "_id": "649f65a4ca03a1a35e3dac14",
                "avatarUrl": "/avatars/b0dcd8ad795b1e666ee247b2ac024d53.svg",
                "isPro": false,
                "fullname": "Hyojun GO",
                "user": "HJGO",
                "type": "user"
            },
            "summary": "The rapid progress of large, pretrained models for both visual content\ngeneration and 3D reconstruction opens up new possibilities for text-to-3D\ngeneration. Intuitively, one could obtain a formidable 3D scene generator if\none were able to combine the power of a modern latent text-to-video model as\n\"generator\" with the geometric abilities of a recent (feedforward) 3D\nreconstruction system as \"decoder\". We introduce VIST3A, a general framework\nthat does just that, addressing two main challenges. First, the two components\nmust be joined in a way that preserves the rich knowledge encoded in their\nweights. We revisit model stitching, i.e., we identify the layer in the 3D\ndecoder that best matches the latent representation produced by the\ntext-to-video generator and stitch the two parts together. That operation\nrequires only a small dataset and no labels. Second, the text-to-video\ngenerator must be aligned with the stitched 3D decoder, to ensure that the\ngenerated latents are decodable into consistent, perceptually convincing 3D\nscene geometry. To that end, we adapt direct reward finetuning, a popular\ntechnique for human preference alignment. We evaluate the proposed VIST3A\napproach with different video generators and 3D reconstruction models. All\ntested pairings markedly improve over prior text-to-3D models that output\nGaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also\nenables high-quality text-to-pointmap generation.",
            "upvotes": 5,
            "discussionId": "68f19cc86e0bef323a68fc41",
            "projectPage": "https://gohyojun15.github.io/VIST3A/",
            "githubRepo": "https://github.com/gohyojun15/VIST3A",
            "ai_summary": "VIST3A combines latent text-to-video models and 3D reconstruction systems to generate high-quality 3D scenes from text, improving upon prior methods.",
            "ai_keywords": [
                "latent text-to-video model",
                "3D reconstruction system",
                "model stitching",
                "direct reward finetuning",
                "Gaussian splats",
                "text-to-pointmap generation"
            ],
            "githubStars": 23
        },
        "publishedAt": "2025-10-15T07:55:08.000Z",
        "title": "VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a\n  Video Generator",
        "summary": "The rapid progress of large, pretrained models for both visual content\ngeneration and 3D reconstruction opens up new possibilities for text-to-3D\ngeneration. Intuitively, one could obtain a formidable 3D scene generator if\none were able to combine the power of a modern latent text-to-video model as\n\"generator\" with the geometric abilities of a recent (feedforward) 3D\nreconstruction system as \"decoder\". We introduce VIST3A, a general framework\nthat does just that, addressing two main challenges. First, the two components\nmust be joined in a way that preserves the rich knowledge encoded in their\nweights. We revisit model stitching, i.e., we identify the layer in the 3D\ndecoder that best matches the latent representation produced by the\ntext-to-video generator and stitch the two parts together. That operation\nrequires only a small dataset and no labels. Second, the text-to-video\ngenerator must be aligned with the stitched 3D decoder, to ensure that the\ngenerated latents are decodable into consistent, perceptually convincing 3D\nscene geometry. To that end, we adapt direct reward finetuning, a popular\ntechnique for human preference alignment. We evaluate the proposed VIST3A\napproach with different video generators and 3D reconstruction models. All\ntested pairings markedly improve over prior text-to-3D models that output\nGaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also\nenables high-quality text-to-pointmap generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13454.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649f65a4ca03a1a35e3dac14",
            "avatarUrl": "/avatars/b0dcd8ad795b1e666ee247b2ac024d53.svg",
            "fullname": "Hyojun GO",
            "name": "HJGO",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14807",
            "authors": [
                {
                    "_id": "68f1cfb86e0bef323a68ff30",
                    "name": "Ruotian Peng",
                    "hidden": false
                },
                {
                    "_id": "68f1cfb86e0bef323a68ff31",
                    "name": "Yi Ren",
                    "hidden": false
                },
                {
                    "_id": "68f1cfb86e0bef323a68ff32",
                    "name": "Zhouliang Yu",
                    "hidden": false
                },
                {
                    "_id": "68f1cfb86e0bef323a68ff33",
                    "name": "Weiyang Liu",
                    "hidden": false
                },
                {
                    "_id": "68f1cfb86e0bef323a68ff34",
                    "name": "Yandong Wen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T15:40:49.000Z",
            "submittedOnDailyAt": "2025-10-17T13:23:49.796Z",
            "title": "SimKO: Simple Pass@K Policy Optimization",
            "submittedOnDailyBy": {
                "_id": "662f2c8435ab6df959b005de",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662f2c8435ab6df959b005de/6rtp_Olhe_9wN00L2YU_M.jpeg",
                "isPro": false,
                "fullname": "ruotian peng",
                "user": "prt66",
                "type": "user"
            },
            "summary": "Reinforcement learning with verifiable rewards (RLVR) has advanced the\nreasoning capabilities of large language models (LLMs). However, prevailing\nRLVR methods exhibit a systematic bias toward exploitation over exploration, as\nevidenced by improved pass@1 but reduced pass@K (K>1) performance. To\nunderstand this issue, we analyze training dynamics of RLVR methods by tracking\nthe token-level probability distributions over vocabulary candidates. Our\nanalysis reveals a consistent probability concentration effect where the top-1\ncandidate increasingly accumulates probability mass and suppresses that of\nother candidates. More importantly, stronger over-concentration correlates with\nworse pass@K performance. Inspired by this finding, we propose Simple Pass@K\nOptimization (SimKO), a method designed to mitigate the over-concentration\nissue, thereby encouraging exploration. SimKO operates in an asymmetrical\nmanner. For verified-correct responses, it boosts the probabilities of the\ntop-K candidates. For verified-incorrect responses, it applies stronger\npenalties to the top-1 candidate. We observe that this asymmetric design is\nparticularly effective at mitigating over-concentration when applied at tokens\nwith high entropy. Across various math and logical-reasoning benchmarks, SimKO\nconsistently yields higher pass@K for a wide range of K, providing a simple way\nto improve RLVR's exploration.",
            "upvotes": 4,
            "discussionId": "68f1cfb86e0bef323a68ff35",
            "projectPage": "https://spherelab.ai/simko/",
            "githubRepo": "https://github.com/CLR-Lab/SimKO",
            "ai_summary": "Simple Pass@K Optimization (SimKO) addresses over-concentration in reinforcement learning with verifiable rewards (RLVR) by asymmetrically adjusting token probabilities, enhancing exploration and pass@K performance.",
            "ai_keywords": [
                "reinforcement learning with verifiable rewards",
                "RLVR",
                "large language models",
                "LLMs",
                "pass@1",
                "pass@K",
                "token-level probability distributions",
                "probability concentration effect",
                "Simple Pass@K Optimization",
                "SimKO",
                "entropy"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-10-16T11:40:49.000Z",
        "title": "SimKO: Simple Pass@K Policy Optimization",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has advanced the\nreasoning capabilities of large language models (LLMs). However, prevailing\nRLVR methods exhibit a systematic bias toward exploitation over exploration, as\nevidenced by improved pass@1 but reduced pass@K (K>1) performance. To\nunderstand this issue, we analyze training dynamics of RLVR methods by tracking\nthe token-level probability distributions over vocabulary candidates. Our\nanalysis reveals a consistent probability concentration effect where the top-1\ncandidate increasingly accumulates probability mass and suppresses that of\nother candidates. More importantly, stronger over-concentration correlates with\nworse pass@K performance. Inspired by this finding, we propose Simple Pass@K\nOptimization (SimKO), a method designed to mitigate the over-concentration\nissue, thereby encouraging exploration. SimKO operates in an asymmetrical\nmanner. For verified-correct responses, it boosts the probabilities of the\ntop-K candidates. For verified-incorrect responses, it applies stronger\npenalties to the top-1 candidate. We observe that this asymmetric design is\nparticularly effective at mitigating over-concentration when applied at tokens\nwith high entropy. Across various math and logical-reasoning benchmarks, SimKO\nconsistently yields higher pass@K for a wide range of K, providing a simple way\nto improve RLVR's exploration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14807.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "662f2c8435ab6df959b005de",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662f2c8435ab6df959b005de/6rtp_Olhe_9wN00L2YU_M.jpeg",
            "fullname": "ruotian peng",
            "name": "prt66",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13996",
            "authors": [
                {
                    "_id": "68f1d4a56e0bef323a68ff90",
                    "name": "Lukas Gienapp",
                    "hidden": false
                },
                {
                    "_id": "68f1d4a56e0bef323a68ff91",
                    "name": "Christopher Schröder",
                    "hidden": false
                },
                {
                    "_id": "68f1d4a56e0bef323a68ff92",
                    "user": {
                        "_id": "5e6a3d4ea9afd5125d9ec064",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
                        "isPro": true,
                        "fullname": "Stefan Schweter",
                        "user": "stefan-it",
                        "type": "user"
                    },
                    "name": "Stefan Schweter",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T05:56:03.613Z",
                    "hidden": false
                },
                {
                    "_id": "68f1d4a56e0bef323a68ff93",
                    "name": "Christopher Akiki",
                    "hidden": false
                },
                {
                    "_id": "68f1d4a56e0bef323a68ff94",
                    "name": "Ferdinand Schlatt",
                    "hidden": false
                },
                {
                    "_id": "68f1d4a56e0bef323a68ff95",
                    "name": "Arden Zimmermann",
                    "hidden": false
                },
                {
                    "_id": "68f1d4a56e0bef323a68ff96",
                    "name": "Phillipe Genêt",
                    "hidden": false
                },
                {
                    "_id": "68f1d4a56e0bef323a68ff97",
                    "name": "Martin Potthast",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/5e6a3d4ea9afd5125d9ec064/jPBWPo-iRgevY5QK6u_Jn.png"
            ],
            "publishedAt": "2025-10-15T18:24:26.000Z",
            "submittedOnDailyAt": "2025-10-17T04:22:19.366Z",
            "title": "The German Commons - 154 Billion Tokens of Openly Licensed Text for\n  German Language Models",
            "submittedOnDailyBy": {
                "_id": "5e6a3d4ea9afd5125d9ec064",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
                "isPro": true,
                "fullname": "Stefan Schweter",
                "user": "stefan-it",
                "type": "user"
            },
            "summary": "Large language model development relies on large-scale training corpora, yet\nmost contain data of unclear licensing status, limiting the development of\ntruly open models. This problem is exacerbated for non-English languages, where\nopenly licensed text remains critically scarce. We introduce the German\nCommons, the largest collection of openly licensed German text to date. It\ncompiles data from 41 sources across seven domains, encompassing legal,\nscientific, cultural, political, news, economic, and web text. Through\nsystematic sourcing from established data providers with verifiable licensing,\nit yields 154.56 billion tokens of high-quality text for language model\ntraining. Our processing pipeline implements comprehensive quality filtering,\ndeduplication, and text formatting fixes, ensuring consistent quality across\nheterogeneous text sources. All domain subsets feature licenses of at least\nCC-BY-SA 4.0 or equivalent, ensuring legal compliance for model training and\nredistribution. The German Commons therefore addresses the critical gap in\nopenly licensed German pretraining data, and enables the development of truly\nopen German language models. We also release code for corpus construction and\ndata filtering tailored to German language text, rendering the German Commons\nfully reproducible and extensible.",
            "upvotes": 4,
            "discussionId": "68f1d4a56e0bef323a68ff98",
            "githubRepo": "https://github.com/coral-nlp/llmdata",
            "ai_summary": "The German Commons provides a large-scale, openly licensed dataset for training German language models, addressing the scarcity of such data.",
            "ai_keywords": [
                "language model",
                "training corpora",
                "licensing status",
                "openly licensed text",
                "German Commons",
                "data sources",
                "legal",
                "scientific",
                "cultural",
                "political",
                "news",
                "economic",
                "web text",
                "high-quality text",
                "quality filtering",
                "deduplication",
                "text formatting",
                "CC-BY-SA 4.0",
                "legal compliance",
                "model training",
                "redistribution",
                "corpus construction",
                "data filtering"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "6819f812d0e7f0efa77658f8",
                "name": "coralnlp",
                "fullname": "CORAL NLP Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/625026749d39e8be3166132f/4wCrPdv2fnR-gWBJVXYdo.png"
            }
        },
        "publishedAt": "2025-10-15T14:24:26.000Z",
        "title": "The German Commons - 154 Billion Tokens of Openly Licensed Text for\n  German Language Models",
        "summary": "Large language model development relies on large-scale training corpora, yet\nmost contain data of unclear licensing status, limiting the development of\ntruly open models. This problem is exacerbated for non-English languages, where\nopenly licensed text remains critically scarce. We introduce the German\nCommons, the largest collection of openly licensed German text to date. It\ncompiles data from 41 sources across seven domains, encompassing legal,\nscientific, cultural, political, news, economic, and web text. Through\nsystematic sourcing from established data providers with verifiable licensing,\nit yields 154.56 billion tokens of high-quality text for language model\ntraining. Our processing pipeline implements comprehensive quality filtering,\ndeduplication, and text formatting fixes, ensuring consistent quality across\nheterogeneous text sources. All domain subsets feature licenses of at least\nCC-BY-SA 4.0 or equivalent, ensuring legal compliance for model training and\nredistribution. The German Commons therefore addresses the critical gap in\nopenly licensed German pretraining data, and enables the development of truly\nopen German language models. We also release code for corpus construction and\ndata filtering tailored to German language text, rendering the German Commons\nfully reproducible and extensible.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/5e6a3d4ea9afd5125d9ec064/jPBWPo-iRgevY5QK6u_Jn.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13996.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5e6a3d4ea9afd5125d9ec064",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
            "fullname": "Stefan Schweter",
            "name": "stefan-it",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3397
        },
        "organization": {
            "_id": "6819f812d0e7f0efa77658f8",
            "name": "coralnlp",
            "fullname": "CORAL NLP Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/625026749d39e8be3166132f/4wCrPdv2fnR-gWBJVXYdo.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.14961",
            "authors": [
                {
                    "_id": "68f2308fe624abe1d1f0ffb3",
                    "name": "Jonas Geiping",
                    "hidden": false
                },
                {
                    "_id": "68f2308fe624abe1d1f0ffb4",
                    "name": "Xinyu Yang",
                    "hidden": false
                },
                {
                    "_id": "68f2308fe624abe1d1f0ffb5",
                    "name": "Guinan Su",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63d86dbf3130cadcaf8bdd11/Fp6Kn88UuHC6USEuqAUWh.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63d86dbf3130cadcaf8bdd11/5wACmbojdnJzJn0fcl88t.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63d86dbf3130cadcaf8bdd11/fPgsg1nOB4y5jGBhocF3B.png"
            ],
            "publishedAt": "2025-10-16T17:59:07.000Z",
            "submittedOnDailyAt": "2025-10-17T11:18:20.273Z",
            "title": "Efficient Parallel Samplers for Recurrent-Depth Models and Their\n  Connection to Diffusion Language Models",
            "submittedOnDailyBy": {
                "_id": "63d86dbf3130cadcaf8bdd11",
                "avatarUrl": "/avatars/29d79a0c6dcec01111ef192fecd0fa7a.svg",
                "isPro": false,
                "fullname": "Jonas Geiping",
                "user": "JonasGeiping",
                "type": "user"
            },
            "summary": "Language models with recurrent depth, also referred to as universal or looped\nwhen considering transformers, are defined by the capacity to increase their\ncomputation through the repetition of layers. Recent efforts in pretraining\nhave demonstrated that these architectures can scale to modern language\nmodeling tasks while exhibiting advantages in reasoning tasks. In this work, we\nexamine the relationship between recurrent-depth models and diffusion language\nmodels. Building on their similarities, we develop a new diffusion forcing\nsampler for these models to accelerate generation. The sampler advances by\ndecoding new tokens at every forward pass of the model, while the latent states\nof these tokens can be further refined in parallel through recurrence.\nTheoretically, generation with our sampler is strictly more expressive than the\nbaseline autoregressive generation using the same time budget on modern\nhardware. Moreover, this sampler, based on principles from diffusion\nliterature, can be directly applied to existing 3.5B recurrent-depth\ntransformers without any tuning, leading to up to a 5x speedup. Consequently,\nour findings not only provide an efficient mechanism for parallelizing the\nextra computation in recurrent-depth models at inference, but also suggest that\nsuch models can be naturally viewed as strong continuous, though causal,\ndiffusion language models.",
            "upvotes": 3,
            "discussionId": "68f23090e624abe1d1f0ffb6",
            "githubRepo": "https://github.com/seal-rg/recurrent-pretraining",
            "ai_summary": "A new diffusion forcing sampler accelerates token generation in recurrent-depth language models, offering a 5x speedup without tuning.",
            "ai_keywords": [
                "recurrent depth",
                "universal",
                "looped",
                "transformers",
                "diffusion language models",
                "diffusion forcing sampler",
                "autoregressive generation",
                "latent states",
                "parallelizing",
                "continuous",
                "causal"
            ],
            "githubStars": 833,
            "organization": {
                "_id": "68f2384a5e53d7d6240bd063",
                "name": "ELLIS-Institute-Tuebingen",
                "fullname": "ELLIS Institute Tübingen",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63d86dbf3130cadcaf8bdd11/yXv5xW2lR52xL8s_hKWC6.png"
            }
        },
        "publishedAt": "2025-10-16T13:59:07.000Z",
        "title": "Efficient Parallel Samplers for Recurrent-Depth Models and Their\n  Connection to Diffusion Language Models",
        "summary": "Language models with recurrent depth, also referred to as universal or looped\nwhen considering transformers, are defined by the capacity to increase their\ncomputation through the repetition of layers. Recent efforts in pretraining\nhave demonstrated that these architectures can scale to modern language\nmodeling tasks while exhibiting advantages in reasoning tasks. In this work, we\nexamine the relationship between recurrent-depth models and diffusion language\nmodels. Building on their similarities, we develop a new diffusion forcing\nsampler for these models to accelerate generation. The sampler advances by\ndecoding new tokens at every forward pass of the model, while the latent states\nof these tokens can be further refined in parallel through recurrence.\nTheoretically, generation with our sampler is strictly more expressive than the\nbaseline autoregressive generation using the same time budget on modern\nhardware. Moreover, this sampler, based on principles from diffusion\nliterature, can be directly applied to existing 3.5B recurrent-depth\ntransformers without any tuning, leading to up to a 5x speedup. Consequently,\nour findings not only provide an efficient mechanism for parallelizing the\nextra computation in recurrent-depth models at inference, but also suggest that\nsuch models can be naturally viewed as strong continuous, though causal,\ndiffusion language models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63d86dbf3130cadcaf8bdd11/Fp6Kn88UuHC6USEuqAUWh.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63d86dbf3130cadcaf8bdd11/5wACmbojdnJzJn0fcl88t.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63d86dbf3130cadcaf8bdd11/fPgsg1nOB4y5jGBhocF3B.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14961.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63d86dbf3130cadcaf8bdd11",
            "avatarUrl": "/avatars/29d79a0c6dcec01111ef192fecd0fa7a.svg",
            "fullname": "Jonas Geiping",
            "name": "JonasGeiping",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 33
        },
        "organization": {
            "_id": "68f2384a5e53d7d6240bd063",
            "name": "ELLIS-Institute-Tuebingen",
            "fullname": "ELLIS Institute Tübingen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63d86dbf3130cadcaf8bdd11/yXv5xW2lR52xL8s_hKWC6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14955",
            "authors": [
                {
                    "_id": "68f1e1d76e0bef323a68fffc",
                    "name": "Guo Cheng",
                    "hidden": false
                },
                {
                    "_id": "68f1e1d76e0bef323a68fffd",
                    "name": "Danni Yang",
                    "hidden": false
                },
                {
                    "_id": "68f1e1d76e0bef323a68fffe",
                    "name": "Ziqi Huang",
                    "hidden": false
                },
                {
                    "_id": "68f1e1d76e0bef323a68ffff",
                    "name": "Jianlou Si",
                    "hidden": false
                },
                {
                    "_id": "68f1e1d76e0bef323a690000",
                    "name": "Chenyang Si",
                    "hidden": false
                },
                {
                    "_id": "68f1e1d76e0bef323a690001",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:58:25.000Z",
            "submittedOnDailyAt": "2025-10-17T05:22:05.938Z",
            "title": "RealDPO: Real or Not Real, that is the Preference",
            "submittedOnDailyBy": {
                "_id": "60efe7fa0d920bc7805cada5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png",
                "isPro": false,
                "fullname": "Ziqi Huang",
                "user": "Ziqi",
                "type": "user"
            },
            "summary": "Video generative models have recently achieved notable advancements in\nsynthesis quality. However, generating complex motions remains a critical\nchallenge, as existing models often struggle to produce natural, smooth, and\ncontextually consistent movements. This gap between generated and real-world\nmotions limits their practical applicability. To address this issue, we\nintroduce RealDPO, a novel alignment paradigm that leverages real-world data as\npositive samples for preference learning, enabling more accurate motion\nsynthesis. Unlike traditional supervised fine-tuning (SFT), which offers\nlimited corrective feedback, RealDPO employs Direct Preference Optimization\n(DPO) with a tailored loss function to enhance motion realism. By contrasting\nreal-world videos with erroneous model outputs, RealDPO enables iterative\nself-correction, progressively refining motion quality. To support\npost-training in complex motion synthesis, we propose RealAction-5K, a curated\ndataset of high-quality videos capturing human daily activities with rich and\nprecise motion details. Extensive experiments demonstrate that RealDPO\nsignificantly improves video quality, text alignment, and motion realism\ncompared to state-of-the-art models and existing preference optimization\ntechniques.",
            "upvotes": 3,
            "discussionId": "68f1e1d86e0bef323a690002",
            "ai_summary": "RealDPO, a novel preference learning paradigm using real-world data, enhances motion realism in video generative models through Direct Preference Optimization and iterative self-correction.",
            "ai_keywords": [
                "RealDPO",
                "Direct Preference Optimization",
                "preference learning",
                "motion synthesis",
                "video generative models",
                "RealAction-5K",
                "text alignment",
                "motion realism"
            ]
        },
        "publishedAt": "2025-10-16T13:58:25.000Z",
        "title": "RealDPO: Real or Not Real, that is the Preference",
        "summary": "Video generative models have recently achieved notable advancements in\nsynthesis quality. However, generating complex motions remains a critical\nchallenge, as existing models often struggle to produce natural, smooth, and\ncontextually consistent movements. This gap between generated and real-world\nmotions limits their practical applicability. To address this issue, we\nintroduce RealDPO, a novel alignment paradigm that leverages real-world data as\npositive samples for preference learning, enabling more accurate motion\nsynthesis. Unlike traditional supervised fine-tuning (SFT), which offers\nlimited corrective feedback, RealDPO employs Direct Preference Optimization\n(DPO) with a tailored loss function to enhance motion realism. By contrasting\nreal-world videos with erroneous model outputs, RealDPO enables iterative\nself-correction, progressively refining motion quality. To support\npost-training in complex motion synthesis, we propose RealAction-5K, a curated\ndataset of high-quality videos capturing human daily activities with rich and\nprecise motion details. Extensive experiments demonstrate that RealDPO\nsignificantly improves video quality, text alignment, and motion realism\ncompared to state-of-the-art models and existing preference optimization\ntechniques.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14955.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60efe7fa0d920bc7805cada5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png",
            "fullname": "Ziqi Huang",
            "name": "Ziqi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13697",
            "authors": [
                {
                    "_id": "68f21e48e624abe1d1f0ff37",
                    "name": "Maksim Sapronov",
                    "hidden": false
                },
                {
                    "_id": "68f21e48e624abe1d1f0ff38",
                    "name": "Evgeniy Glukhov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T15:55:19.000Z",
            "submittedOnDailyAt": "2025-10-17T09:17:44.920Z",
            "title": "On Pretraining for Project-Level Code Completion",
            "submittedOnDailyBy": {
                "_id": "6440ff39ad24e9b2cfba8575",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6440ff39ad24e9b2cfba8575/QoQC60vbfd2YzTtPAXpM-.jpeg",
                "isPro": false,
                "fullname": "Evgeniy Glukhov",
                "user": "jenyag",
                "type": "user"
            },
            "summary": "Repository-level pretraining is commonly used to enable large language models\nfor code to leverage codebase-wide context. This enhances their ability to\ngenerate accurate and context-aware code completions. In this work, we\ninvestigate how different repository-processing strategies affect in-context\nlearning in OpenCoder, a 1.5B-parameter model. We extend its context window\nfrom 4,096 to 16,384 tokens by training on additional 1B tokens of curated\nrepository-level data. Despite relying on a smaller dataset than competing\nmodels (which often use hundreds of billions of tokens), our model achieves\ncomparable performance on the Long Code Arena benchmark. We find that various\nrepository-processing techniques yield similarly strong results, with the\nprimary gain coming from adapting to a new rotary positional embedding (RoPE)\nscaling parameter. Finally, we show that a simpler file-level training approach\nat the original sequence length remains highly effective, opening up\nrepository-level code completion research to settings with more constrained\ndata and compute resources.",
            "upvotes": 3,
            "discussionId": "68f21e4de624abe1d1f0ff39",
            "ai_summary": "Extending the context window and adapting to a new rotary positional embedding scaling parameter improve repository-level code completion in OpenCoder, achieving performance comparable to larger models with less data.",
            "ai_keywords": [
                "repository-level pretraining",
                "code completions",
                "OpenCoder",
                "context window",
                "rotary positional embedding (RoPE)",
                "Long Code Arena benchmark",
                "file-level training"
            ],
            "organization": {
                "_id": "64665ffb326128fd2c6b2708",
                "name": "JetBrains-Research",
                "fullname": "JetBrains Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6457bd2cf9b42c048d0f771d/Bz6WJPFu9MBOn7d7xQVai.png"
            }
        },
        "publishedAt": "2025-10-15T11:55:19.000Z",
        "title": "On Pretraining for Project-Level Code Completion",
        "summary": "Repository-level pretraining is commonly used to enable large language models\nfor code to leverage codebase-wide context. This enhances their ability to\ngenerate accurate and context-aware code completions. In this work, we\ninvestigate how different repository-processing strategies affect in-context\nlearning in OpenCoder, a 1.5B-parameter model. We extend its context window\nfrom 4,096 to 16,384 tokens by training on additional 1B tokens of curated\nrepository-level data. Despite relying on a smaller dataset than competing\nmodels (which often use hundreds of billions of tokens), our model achieves\ncomparable performance on the Long Code Arena benchmark. We find that various\nrepository-processing techniques yield similarly strong results, with the\nprimary gain coming from adapting to a new rotary positional embedding (RoPE)\nscaling parameter. Finally, we show that a simpler file-level training approach\nat the original sequence length remains highly effective, opening up\nrepository-level code completion research to settings with more constrained\ndata and compute resources.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13697.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6440ff39ad24e9b2cfba8575",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6440ff39ad24e9b2cfba8575/QoQC60vbfd2YzTtPAXpM-.jpeg",
            "fullname": "Evgeniy Glukhov",
            "name": "jenyag",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "64665ffb326128fd2c6b2708",
            "name": "JetBrains-Research",
            "fullname": "JetBrains Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6457bd2cf9b42c048d0f771d/Bz6WJPFu9MBOn7d7xQVai.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14913",
            "authors": [
                {
                    "_id": "68f1ab1b6e0bef323a68fd8c",
                    "user": {
                        "_id": "646c351a31968a60a0237c71",
                        "avatarUrl": "/avatars/d1b3ef3c33671d1cb097bad69e97943b.svg",
                        "isPro": false,
                        "fullname": "Kyle Montgomery",
                        "user": "kylemontgomery",
                        "type": "user"
                    },
                    "name": "Kyle Montgomery",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:10:16.839Z",
                    "hidden": false
                },
                {
                    "_id": "68f1ab1b6e0bef323a68fd8d",
                    "name": "Sijun Tan",
                    "hidden": false
                },
                {
                    "_id": "68f1ab1b6e0bef323a68fd8e",
                    "name": "Yuqi Chen",
                    "hidden": false
                },
                {
                    "_id": "68f1ab1b6e0bef323a68fd8f",
                    "name": "Siyuan Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68f1ab1b6e0bef323a68fd90",
                    "name": "Tianjun Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1ab1b6e0bef323a68fd91",
                    "name": "Raluca Ada Popa",
                    "hidden": false
                },
                {
                    "_id": "68f1ab1b6e0bef323a68fd92",
                    "name": "Chenguang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:30:02.000Z",
            "submittedOnDailyAt": "2025-10-17T19:13:19.915Z",
            "title": "Budget-aware Test-time Scaling via Discriminative Verification",
            "submittedOnDailyBy": {
                "_id": "646c351a31968a60a0237c71",
                "avatarUrl": "/avatars/d1b3ef3c33671d1cb097bad69e97943b.svg",
                "isPro": false,
                "fullname": "Kyle Montgomery",
                "user": "kylemontgomery",
                "type": "user"
            },
            "summary": "Test-time scaling is a powerful strategy for boosting the performance of\nlarge language models on complex reasoning tasks. While state-of-the-art\napproaches often employ generative verifiers to select the best solution from a\npool of candidates, this method incurs prohibitive computational costs,\nlimiting its practicality. In this work, we shift the focus to a more\nbudget-aware paradigm: discriminative verification. We conduct a thorough\nempirical analysis and demonstrate that while discriminative verifiers may\nunderperform in isolation, combining them with self-consistency in a hybrid\napproach creates a powerful and efficient test-time scaling mechanism. Notably,\nunder a fixed compute budget, this hybrid approach surpasses state-of-the-art\ngenerative verification by a significant margin: achieving up to 15.3\\% higher\naccuracy on AIME2025. Our findings establish that for practical, real-world\napplications, budget-aware scaling with discriminative verifiers is not only a\n\"free\" upgrade over self-consistency, but also a more effective and efficient\nalternative to costly generative techniques. Code is available at\nhttps://github.com/wang-research-lab/verification.",
            "upvotes": 2,
            "discussionId": "68f1ab1c6e0bef323a68fd93",
            "githubRepo": "https://github.com/wang-research-lab/verification",
            "ai_summary": "A hybrid approach combining discriminative verification with self-consistency outperforms generative verification in test-time scaling for large language models, achieving higher accuracy within a fixed compute budget.",
            "ai_keywords": [
                "test-time scaling",
                "large language models",
                "generative verifiers",
                "discriminative verification",
                "self-consistency",
                "AIME2025"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-10-16T13:30:02.000Z",
        "title": "Budget-aware Test-time Scaling via Discriminative Verification",
        "summary": "Test-time scaling is a powerful strategy for boosting the performance of\nlarge language models on complex reasoning tasks. While state-of-the-art\napproaches often employ generative verifiers to select the best solution from a\npool of candidates, this method incurs prohibitive computational costs,\nlimiting its practicality. In this work, we shift the focus to a more\nbudget-aware paradigm: discriminative verification. We conduct a thorough\nempirical analysis and demonstrate that while discriminative verifiers may\nunderperform in isolation, combining them with self-consistency in a hybrid\napproach creates a powerful and efficient test-time scaling mechanism. Notably,\nunder a fixed compute budget, this hybrid approach surpasses state-of-the-art\ngenerative verification by a significant margin: achieving up to 15.3\\% higher\naccuracy on AIME2025. Our findings establish that for practical, real-world\napplications, budget-aware scaling with discriminative verifiers is not only a\n\"free\" upgrade over self-consistency, but also a more effective and efficient\nalternative to costly generative techniques. Code is available at\nhttps://github.com/wang-research-lab/verification.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14913.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646c351a31968a60a0237c71",
            "avatarUrl": "/avatars/d1b3ef3c33671d1cb097bad69e97943b.svg",
            "fullname": "Kyle Montgomery",
            "name": "kylemontgomery",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.14252",
            "authors": [
                {
                    "_id": "68f1a46e6e0bef323a68fd12",
                    "user": {
                        "_id": "658e85bb5b7553ca5c29ba89",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
                        "isPro": false,
                        "fullname": "Jihao Zhao",
                        "user": "Robot2050",
                        "type": "user"
                    },
                    "name": "Jihao Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:10:33.450Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a46e6e0bef323a68fd13",
                    "name": "Zhiyuan Ji",
                    "hidden": false
                },
                {
                    "_id": "68f1a46e6e0bef323a68fd14",
                    "name": "Simin Niu",
                    "hidden": false
                },
                {
                    "_id": "68f1a46e6e0bef323a68fd15",
                    "name": "Hanyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68f1a46e6e0bef323a68fd16",
                    "name": "Feiyu Xiong",
                    "hidden": false
                },
                {
                    "_id": "68f1a46e6e0bef323a68fd17",
                    "name": "Zhiyu Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T03:09:51.000Z",
            "submittedOnDailyAt": "2025-10-17T00:42:57.762Z",
            "title": "MoM: Mixtures of Scenario-Aware Document Memories for\n  Retrieval-Augmented Generation Systems",
            "submittedOnDailyBy": {
                "_id": "658e85bb5b7553ca5c29ba89",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
                "isPro": false,
                "fullname": "Jihao Zhao",
                "user": "Robot2050",
                "type": "user"
            },
            "summary": "The traditional RAG paradigm, which typically engages in the comprehension of\nrelevant text chunks in response to received queries, inherently restricts both\nthe depth of knowledge internalization and reasoning capabilities. To address\nthis limitation, our research transforms the text processing in RAG from\npassive chunking to proactive understanding, defining this process as document\nmemory extraction with the objective of simulating human cognitive processes\nduring reading. Building upon this, we propose the Mixtures of scenario-aware\ndocument Memories (MoM) framework, engineered to efficiently handle documents\nfrom multiple domains and train small language models (SLMs) to acquire the\nability to proactively explore and construct document memories. The MoM\ninitially instructs large language models (LLMs) to simulate domain experts in\ngenerating document logical outlines, thereby directing structured chunking and\ncore content extraction. It employs a multi-path sampling and multi-perspective\nevaluation mechanism, specifically designing comprehensive metrics that\nrepresent chunk clarity and extraction completeness to select the optimal\ndocument memories. Additionally, to infuse deeper human-like reading abilities\nduring the training of SLMs, we incorporate a reverse reasoning strategy, which\ndeduces refined expert thinking paths from high-quality outcomes. Finally,\nleveraging diverse forms of content generated by MoM, we develop a three-layer\ndocument memory retrieval mechanism, which is grounded in our theoretical proof\nfrom the perspective of probabilistic modeling. Extensive experimental results\nacross three distinct domains demonstrate that the MoM framework not only\nresolves text chunking challenges in existing RAG systems, providing LLMs with\nsemantically complete document memories, but also paves the way for SLMs to\nachieve human-centric intelligent text processing.",
            "upvotes": 2,
            "discussionId": "68f1a46e6e0bef323a68fd18",
            "ai_summary": "The MoM framework enhances RAG by transforming text processing from passive chunking to proactive understanding, enabling LLMs to generate structured document memories and SLMs to develop human-like reading abilities.",
            "ai_keywords": [
                "document memory extraction",
                "Mixtures of scenario-aware document Memories (MoM)",
                "small language models (SLMs)",
                "large language models (LLMs)",
                "document logical outlines",
                "multi-path sampling",
                "multi-perspective evaluation",
                "reverse reasoning strategy",
                "three-layer document memory retrieval mechanism",
                "probabilistic modeling"
            ]
        },
        "publishedAt": "2025-10-15T23:09:51.000Z",
        "title": "MoM: Mixtures of Scenario-Aware Document Memories for\n  Retrieval-Augmented Generation Systems",
        "summary": "The traditional RAG paradigm, which typically engages in the comprehension of\nrelevant text chunks in response to received queries, inherently restricts both\nthe depth of knowledge internalization and reasoning capabilities. To address\nthis limitation, our research transforms the text processing in RAG from\npassive chunking to proactive understanding, defining this process as document\nmemory extraction with the objective of simulating human cognitive processes\nduring reading. Building upon this, we propose the Mixtures of scenario-aware\ndocument Memories (MoM) framework, engineered to efficiently handle documents\nfrom multiple domains and train small language models (SLMs) to acquire the\nability to proactively explore and construct document memories. The MoM\ninitially instructs large language models (LLMs) to simulate domain experts in\ngenerating document logical outlines, thereby directing structured chunking and\ncore content extraction. It employs a multi-path sampling and multi-perspective\nevaluation mechanism, specifically designing comprehensive metrics that\nrepresent chunk clarity and extraction completeness to select the optimal\ndocument memories. Additionally, to infuse deeper human-like reading abilities\nduring the training of SLMs, we incorporate a reverse reasoning strategy, which\ndeduces refined expert thinking paths from high-quality outcomes. Finally,\nleveraging diverse forms of content generated by MoM, we develop a three-layer\ndocument memory retrieval mechanism, which is grounded in our theoretical proof\nfrom the perspective of probabilistic modeling. Extensive experimental results\nacross three distinct domains demonstrate that the MoM framework not only\nresolves text chunking challenges in existing RAG systems, providing LLMs with\nsemantically complete document memories, but also paves the way for SLMs to\nachieve human-centric intelligent text processing.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14252.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "658e85bb5b7553ca5c29ba89",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
            "fullname": "Jihao Zhao",
            "name": "Robot2050",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.13913",
            "authors": [
                {
                    "_id": "68f2693637ed56c2c926f940",
                    "name": "Shrey Pandit",
                    "hidden": false
                },
                {
                    "_id": "68f2693637ed56c2c926f941",
                    "name": "Xuan-Phi Nguyen",
                    "hidden": false
                },
                {
                    "_id": "68f2693637ed56c2c926f942",
                    "name": "Yifei Ming",
                    "hidden": false
                },
                {
                    "_id": "68f2693637ed56c2c926f943",
                    "name": "Austin Xu",
                    "hidden": false
                },
                {
                    "_id": "68f2693637ed56c2c926f944",
                    "name": "Jiayu Wang",
                    "hidden": false
                },
                {
                    "_id": "68f2693637ed56c2c926f945",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "68f2693637ed56c2c926f946",
                    "name": "Shafiq Joty",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T06:34:46.000Z",
            "submittedOnDailyAt": "2025-10-17T15:26:23.923Z",
            "title": "Synthesizing Agentic Data for Web Agents with Progressive Difficulty\n  Enhancement Mechanisms",
            "submittedOnDailyBy": {
                "_id": "648749094dea003c6dae810f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
                "isPro": false,
                "fullname": "Shrey Pandit",
                "user": "SP2001",
                "type": "user"
            },
            "summary": "Web-based 'deep research' agents aim to solve complex question - answering\ntasks through long-horizon interactions with online tools. These tasks remain\nchallenging, as the underlying language models are often not optimized for\nlong-horizon reasoning and exploration. Prior work has proposed workflows for\nconstructing instruction-tuning datasets, often leveraging knowledge graphs.\nHowever, such methods typically lack fine-grained control over difficulty and\nquality, yielding synthetic data that falls short of capturing the complexity\nrequired for long-horizon reasoning. Furthermore, many studies conflate data\nand training effects by comparing models trained under different optimization\nrecipes, making it difficult to isolate and evaluate the effectiveness of the\ndata itself. We introduce a two-pronged data synthesis pipeline that generates\nquestion - answer pairs by progressively increasing task complexity until a\nfrontier baseline web agent fails. The baseline agent plays multiple roles in\nthis process: attempting the questions, validating factuality, checking for\nalternative answers, and enforcing filtering. To evaluate the effectiveness of\nour synthesis methods, we adopt a controlled training setup based on\ndistillation from strong web agents. Experiments across multiple web-based\nbenchmarks show that our dataset - despite being smaller - enables the training\nof more effective web agents than existing datasets. In particular, our data\nexhibits twice the diversity in tool-use actions, allowing models trained on it\nto achieve stronger performance while avoiding repetitive tool-calling\nbehaviors.",
            "upvotes": 2,
            "discussionId": "68f2693637ed56c2c926f947",
            "ai_summary": "A two-pronged data synthesis pipeline generates complex question-answer pairs, enabling the training of more effective web-based research agents with higher diversity in tool use.",
            "ai_keywords": [
                "deep research agents",
                "long-horizon reasoning",
                "instruction-tuning datasets",
                "knowledge graphs",
                "data synthesis pipeline",
                "baseline web agent",
                "distillation",
                "web-based benchmarks",
                "tool-use actions"
            ]
        },
        "publishedAt": "2025-10-15T02:34:46.000Z",
        "title": "Synthesizing Agentic Data for Web Agents with Progressive Difficulty\n  Enhancement Mechanisms",
        "summary": "Web-based 'deep research' agents aim to solve complex question - answering\ntasks through long-horizon interactions with online tools. These tasks remain\nchallenging, as the underlying language models are often not optimized for\nlong-horizon reasoning and exploration. Prior work has proposed workflows for\nconstructing instruction-tuning datasets, often leveraging knowledge graphs.\nHowever, such methods typically lack fine-grained control over difficulty and\nquality, yielding synthetic data that falls short of capturing the complexity\nrequired for long-horizon reasoning. Furthermore, many studies conflate data\nand training effects by comparing models trained under different optimization\nrecipes, making it difficult to isolate and evaluate the effectiveness of the\ndata itself. We introduce a two-pronged data synthesis pipeline that generates\nquestion - answer pairs by progressively increasing task complexity until a\nfrontier baseline web agent fails. The baseline agent plays multiple roles in\nthis process: attempting the questions, validating factuality, checking for\nalternative answers, and enforcing filtering. To evaluate the effectiveness of\nour synthesis methods, we adopt a controlled training setup based on\ndistillation from strong web agents. Experiments across multiple web-based\nbenchmarks show that our dataset - despite being smaller - enables the training\nof more effective web agents than existing datasets. In particular, our data\nexhibits twice the diversity in tool-use actions, allowing models trained on it\nto achieve stronger performance while avoiding repetitive tool-calling\nbehaviors.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13913.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648749094dea003c6dae810f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
            "fullname": "Shrey Pandit",
            "name": "SP2001",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14976",
            "authors": [
                {
                    "_id": "68f1c7cb6e0bef323a68febb",
                    "name": "Shaowei Liu",
                    "hidden": false
                },
                {
                    "_id": "68f1c7cb6e0bef323a68febc",
                    "name": "Chuan Guo",
                    "hidden": false
                },
                {
                    "_id": "68f1c7cb6e0bef323a68febd",
                    "name": "Bing Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f1c7cb6e0bef323a68febe",
                    "name": "Jian Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:59:56.000Z",
            "submittedOnDailyAt": "2025-10-17T03:12:25.526Z",
            "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human\n  Interaction Animation",
            "submittedOnDailyBy": {
                "_id": "670462c6fd5ef6902568d7bd",
                "avatarUrl": "/avatars/2d6ea275ccc289f6f1b07d0c8e860888.svg",
                "isPro": false,
                "fullname": "Shaowei Liu",
                "user": "shaoweiliu",
                "type": "user"
            },
            "summary": "Close-proximity human-human interactive poses convey rich contextual\ninformation about interaction dynamics. Given such poses, humans can\nintuitively infer the context and anticipate possible past and future dynamics,\ndrawing on strong priors of human behavior. Inspired by this observation, we\npropose Ponimator, a simple framework anchored on proximal interactive poses\nfor versatile interaction animation. Our training data consists of\nclose-contact two-person poses and their surrounding temporal context from\nmotion-capture interaction datasets. Leveraging interactive pose priors,\nPonimator employs two conditional diffusion models: (1) a pose animator that\nuses the temporal prior to generate dynamic motion sequences from interactive\nposes, and (2) a pose generator that applies the spatial prior to synthesize\ninteractive poses from a single pose, text, or both when interactive poses are\nunavailable. Collectively, Ponimator supports diverse tasks, including\nimage-based interaction animation, reaction animation, and text-to-interaction\nsynthesis, facilitating the transfer of interaction knowledge from high-quality\nmocap data to open-world scenarios. Empirical experiments across diverse\ndatasets and applications demonstrate the universality of the pose prior and\nthe effectiveness and robustness of our framework.",
            "upvotes": 1,
            "discussionId": "68f1c7cb6e0bef323a68febf",
            "projectPage": "https://stevenlsw.github.io/ponimator/",
            "githubRepo": "https://github.com/stevenlsw/ponimator",
            "ai_summary": "Ponimator uses conditional diffusion models to generate and synthesize interactive poses from motion capture data, enabling versatile interaction animation tasks.",
            "ai_keywords": [
                "conditional diffusion models",
                "pose animator",
                "pose generator",
                "interactive pose priors",
                "image-based interaction animation",
                "reaction animation",
                "text-to-interaction synthesis",
                "motion capture data"
            ],
            "githubStars": 5,
            "organization": {
                "_id": "668450a2c1cbe5e008ac6515",
                "name": "Snapchat",
                "fullname": "Snapchat Inc.",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/648ca58a39d2584ee47efef6/plasFy052q2795odYb6NO.jpeg"
            }
        },
        "publishedAt": "2025-10-16T13:59:56.000Z",
        "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human\n  Interaction Animation",
        "summary": "Close-proximity human-human interactive poses convey rich contextual\ninformation about interaction dynamics. Given such poses, humans can\nintuitively infer the context and anticipate possible past and future dynamics,\ndrawing on strong priors of human behavior. Inspired by this observation, we\npropose Ponimator, a simple framework anchored on proximal interactive poses\nfor versatile interaction animation. Our training data consists of\nclose-contact two-person poses and their surrounding temporal context from\nmotion-capture interaction datasets. Leveraging interactive pose priors,\nPonimator employs two conditional diffusion models: (1) a pose animator that\nuses the temporal prior to generate dynamic motion sequences from interactive\nposes, and (2) a pose generator that applies the spatial prior to synthesize\ninteractive poses from a single pose, text, or both when interactive poses are\nunavailable. Collectively, Ponimator supports diverse tasks, including\nimage-based interaction animation, reaction animation, and text-to-interaction\nsynthesis, facilitating the transfer of interaction knowledge from high-quality\nmocap data to open-world scenarios. Empirical experiments across diverse\ndatasets and applications demonstrate the universality of the pose prior and\nthe effectiveness and robustness of our framework.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14976.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "670462c6fd5ef6902568d7bd",
            "avatarUrl": "/avatars/2d6ea275ccc289f6f1b07d0c8e860888.svg",
            "fullname": "Shaowei Liu",
            "name": "shaoweiliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "668450a2c1cbe5e008ac6515",
            "name": "Snapchat",
            "fullname": "Snapchat Inc.",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/648ca58a39d2584ee47efef6/plasFy052q2795odYb6NO.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14919",
            "authors": [
                {
                    "_id": "68f1a9626e0bef323a68fd83",
                    "user": {
                        "_id": "646c351a31968a60a0237c71",
                        "avatarUrl": "/avatars/d1b3ef3c33671d1cb097bad69e97943b.svg",
                        "isPro": false,
                        "fullname": "Kyle Montgomery",
                        "user": "kylemontgomery",
                        "type": "user"
                    },
                    "name": "Kyle Montgomery",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:10:18.896Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a9626e0bef323a68fd84",
                    "name": "David Park",
                    "hidden": false
                },
                {
                    "_id": "68f1a9626e0bef323a68fd85",
                    "name": "Jianhong Tu",
                    "hidden": false
                },
                {
                    "_id": "68f1a9626e0bef323a68fd86",
                    "name": "Michael Bendersky",
                    "hidden": false
                },
                {
                    "_id": "68f1a9626e0bef323a68fd87",
                    "name": "Beliz Gunel",
                    "hidden": false
                },
                {
                    "_id": "68f1a9626e0bef323a68fd88",
                    "name": "Dawn Song",
                    "hidden": false
                },
                {
                    "_id": "68f1a9626e0bef323a68fd89",
                    "name": "Chenguang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:35:18.000Z",
            "submittedOnDailyAt": "2025-10-17T19:07:07.730Z",
            "title": "Predicting Task Performance with Context-aware Scaling Laws",
            "submittedOnDailyBy": {
                "_id": "646c351a31968a60a0237c71",
                "avatarUrl": "/avatars/d1b3ef3c33671d1cb097bad69e97943b.svg",
                "isPro": false,
                "fullname": "Kyle Montgomery",
                "user": "kylemontgomery",
                "type": "user"
            },
            "summary": "Scaling laws have transformed our understanding of large language models by\nlinking upstream metrics like cross-entropy loss to design factors such as\nmodel size, training data, and compute. However, these conventional laws fail\nto capture downstream task performance, where context plays a critical role. In\nthis work, we propose a straightforward, interpretable framework that jointly\nmodels downstream performance as a function of the training compute and the\nprovided context. We empirically validate our framework by fitting it on the\nobserved downstream performance of extended-context variants of Llama-2-7B and\nLlama-2-13B across 65,500 unique instances spanning three tasks: arithmetic\nreasoning, common sense reasoning, and machine translation. Our results\ndemonstrate that our framework accurately models in-distribution downstream\nperformance, generalizes across three orders of magnitude in training compute,\nand reliably extrapolates performance as the amount of context increases. These\nfindings offer valuable insights into the interplay between training compute\nand context utilization, providing guidance for designing more efficient\nlong-context LLMs for diverse downstream tasks. Our code is available at\nhttps://github.com/wang-research-lab/context-scaling.",
            "upvotes": 1,
            "discussionId": "68f1a9626e0bef323a68fd8a",
            "githubRepo": "https://github.com/wang-research-lab/context-scaling",
            "ai_summary": "A framework models downstream performance of large language models as a function of training compute and context, offering insights into efficient design for long-context tasks.",
            "ai_keywords": [
                "scaling laws",
                "large language models",
                "cross-entropy loss",
                "model size",
                "training data",
                "compute",
                "downstream performance",
                "context",
                "extended-context variants",
                "Llama-2-7B",
                "Llama-2-13B",
                "arithmetic reasoning",
                "common sense reasoning",
                "machine translation",
                "in-distribution performance",
                "context utilization",
                "long-context LLMs"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-10-16T13:35:18.000Z",
        "title": "Predicting Task Performance with Context-aware Scaling Laws",
        "summary": "Scaling laws have transformed our understanding of large language models by\nlinking upstream metrics like cross-entropy loss to design factors such as\nmodel size, training data, and compute. However, these conventional laws fail\nto capture downstream task performance, where context plays a critical role. In\nthis work, we propose a straightforward, interpretable framework that jointly\nmodels downstream performance as a function of the training compute and the\nprovided context. We empirically validate our framework by fitting it on the\nobserved downstream performance of extended-context variants of Llama-2-7B and\nLlama-2-13B across 65,500 unique instances spanning three tasks: arithmetic\nreasoning, common sense reasoning, and machine translation. Our results\ndemonstrate that our framework accurately models in-distribution downstream\nperformance, generalizes across three orders of magnitude in training compute,\nand reliably extrapolates performance as the amount of context increases. These\nfindings offer valuable insights into the interplay between training compute\nand context utilization, providing guidance for designing more efficient\nlong-context LLMs for diverse downstream tasks. Our code is available at\nhttps://github.com/wang-research-lab/context-scaling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14919.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646c351a31968a60a0237c71",
            "avatarUrl": "/avatars/d1b3ef3c33671d1cb097bad69e97943b.svg",
            "fullname": "Kyle Montgomery",
            "name": "kylemontgomery",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.14351",
            "authors": [
                {
                    "_id": "68f193bd6e0bef323a68fbcd",
                    "name": "Perapard Ngokpol",
                    "hidden": false
                },
                {
                    "_id": "68f193bd6e0bef323a68fbce",
                    "user": {
                        "_id": "63a83c5432ed73936eb8363e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a83c5432ed73936eb8363e/8iJN61whs49R0bccdL7b5.jpeg",
                        "isPro": false,
                        "fullname": "kun kerdthaisong",
                        "user": "augustus2011",
                        "type": "user"
                    },
                    "name": "Kun Kerdthaisong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:11:35.817Z",
                    "hidden": false
                },
                {
                    "_id": "68f193bd6e0bef323a68fbcf",
                    "name": "Pasin Buakhaw",
                    "hidden": false
                },
                {
                    "_id": "68f193bd6e0bef323a68fbd0",
                    "name": "Pitikorn Khlaisamniang",
                    "hidden": false
                },
                {
                    "_id": "68f193bd6e0bef323a68fbd1",
                    "name": "Supasate Vorathammathorn",
                    "hidden": false
                },
                {
                    "_id": "68f193bd6e0bef323a68fbd2",
                    "name": "Piyalitt Ittichaiwong",
                    "hidden": false
                },
                {
                    "_id": "68f193bd6e0bef323a68fbd3",
                    "name": "Nutchanon Yongsatianchot",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T06:39:27.000Z",
            "submittedOnDailyAt": "2025-10-17T00:01:32.280Z",
            "title": "Beyond One World: Benchmarking Super Heros in Role-Playing Across\n  Multiversal Contexts",
            "submittedOnDailyBy": {
                "_id": "63a83c5432ed73936eb8363e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a83c5432ed73936eb8363e/8iJN61whs49R0bccdL7b5.jpeg",
                "isPro": false,
                "fullname": "kun kerdthaisong",
                "user": "augustus2011",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are increasingly used as role-playing agents,\nyet their capacity to faithfully and consistently portray version-specific\ncharacters -- for example, superheroes across comic and cinematic universes --\nremains underexplored. Superhero canons such as Marvel and DC provide a rich\ntestbed: decades of storytelling yield multiple incarnations of the same\ncharacter with distinct histories, values, and moral codes. To study this\nproblem, we introduce Beyond One World, a benchmark for character-grounded\nroleplay spanning 30 iconic heroes and 90 canon-specific versions. The\nbenchmark comprises two tasks: (i) Canon Events, which probes factual recall of\npivotal life stages, and (ii) Moral Dilemmas, which confronts models with\nethically charged scenarios. We score responses for canonical accuracy and\nreasoning fidelity under a framework that separates internal deliberation\n(\"thinking\") from outward decisions (\"acting\"). We further propose Think-Act\nMatching, a metric that quantifies alignment between reasons and actions and\nserves as a proxy for model trustworthiness. Experiments across reasoning- and\nnon-reasoning-oriented models yield three findings: (1) chain-of-thought\nprompting improves narrative coherence in weaker models but can reduce\ncanonical accuracy in stronger ones; (2) cross-version generalization within a\ncharacter remains a major obstacle; and (3) models often excel at either\nthinking or acting, but rarely both. Beyond One World exposes critical gaps in\nmultiversal consistency and reasoning alignment, offering a challenging\nevaluation for role-playing LLMs.",
            "upvotes": 1,
            "discussionId": "68f193be6e0bef323a68fbd4",
            "githubRepo": "https://github.com/Augustus2011/Beyond_One_World.git",
            "ai_summary": "Beyond One World benchmark evaluates LLMs' ability to consistently portray version-specific superheroes across different canons through factual recall and ethical reasoning tasks.",
            "ai_keywords": [
                "large language models",
                "role-playing agents",
                "character-grounded roleplay",
                "Canon Events",
                "Moral Dilemmas",
                "canonical accuracy",
                "reasoning fidelity",
                "Think-Act Matching",
                "chain-of-thought prompting",
                "cross-version generalization"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "67aaba5d8d478dcb4b2f4281",
                "name": "Character-lab",
                "fullname": "Character-lab"
            }
        },
        "publishedAt": "2025-10-16T02:39:27.000Z",
        "title": "Beyond One World: Benchmarking Super Heros in Role-Playing Across\n  Multiversal Contexts",
        "summary": "Large language models (LLMs) are increasingly used as role-playing agents,\nyet their capacity to faithfully and consistently portray version-specific\ncharacters -- for example, superheroes across comic and cinematic universes --\nremains underexplored. Superhero canons such as Marvel and DC provide a rich\ntestbed: decades of storytelling yield multiple incarnations of the same\ncharacter with distinct histories, values, and moral codes. To study this\nproblem, we introduce Beyond One World, a benchmark for character-grounded\nroleplay spanning 30 iconic heroes and 90 canon-specific versions. The\nbenchmark comprises two tasks: (i) Canon Events, which probes factual recall of\npivotal life stages, and (ii) Moral Dilemmas, which confronts models with\nethically charged scenarios. We score responses for canonical accuracy and\nreasoning fidelity under a framework that separates internal deliberation\n(\"thinking\") from outward decisions (\"acting\"). We further propose Think-Act\nMatching, a metric that quantifies alignment between reasons and actions and\nserves as a proxy for model trustworthiness. Experiments across reasoning- and\nnon-reasoning-oriented models yield three findings: (1) chain-of-thought\nprompting improves narrative coherence in weaker models but can reduce\ncanonical accuracy in stronger ones; (2) cross-version generalization within a\ncharacter remains a major obstacle; and (3) models often excel at either\nthinking or acting, but rarely both. Beyond One World exposes critical gaps in\nmultiversal consistency and reasoning alignment, offering a challenging\nevaluation for role-playing LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14351.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a83c5432ed73936eb8363e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a83c5432ed73936eb8363e/8iJN61whs49R0bccdL7b5.jpeg",
            "fullname": "kun kerdthaisong",
            "name": "augustus2011",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "67aaba5d8d478dcb4b2f4281",
            "name": "Character-lab",
            "fullname": "Character-lab"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.14095",
            "authors": [
                {
                    "_id": "68f2802537ed56c2c926f973",
                    "name": "Awni Altabaa",
                    "hidden": false
                },
                {
                    "_id": "68f2802537ed56c2c926f974",
                    "name": "Siyu Chen",
                    "hidden": false
                },
                {
                    "_id": "68f2802537ed56c2c926f975",
                    "name": "John Lafferty",
                    "hidden": false
                },
                {
                    "_id": "68f2802537ed56c2c926f976",
                    "name": "Zhuoran Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T21:03:59.000Z",
            "submittedOnDailyAt": "2025-10-17T16:18:01.226Z",
            "title": "Unlocking Out-of-Distribution Generalization in Transformers via\n  Recursive Latent Space Reasoning",
            "submittedOnDailyBy": {
                "_id": "637abd9114076b808c6a2f4f",
                "avatarUrl": "/avatars/7fa8b275b4347dfeab73384e2aa72ea4.svg",
                "isPro": false,
                "fullname": "Awni Altabaa",
                "user": "awni00",
                "type": "user"
            },
            "summary": "Systematic, compositional generalization beyond the training distribution\nremains a core challenge in machine learning -- and a critical bottleneck for\nthe emergent reasoning abilities of modern language models. This work\ninvestigates out-of-distribution (OOD) generalization in Transformer networks\nusing a GSM8K-style modular arithmetic on computational graphs task as a\ntestbed. We introduce and explore a set of four architectural mechanisms aimed\nat enhancing OOD generalization: (i) input-adaptive recurrence; (ii)\nalgorithmic supervision; (iii) anchored latent representations via a discrete\nbottleneck; and (iv) an explicit error-correction mechanism. Collectively,\nthese mechanisms yield an architectural approach for native and scalable latent\nspace reasoning in Transformer networks with robust algorithmic generalization\ncapabilities. We complement these empirical results with a detailed mechanistic\ninterpretability analysis that reveals how these mechanisms give rise to robust\nOOD generalization abilities.",
            "upvotes": 1,
            "discussionId": "68f2802537ed56c2c926f977",
            "githubRepo": "https://github.com/Awni00/algorithmic-generalization-transformer-architectures",
            "ai_summary": "Transformer networks are enhanced with four architectural mechanisms to improve out-of-distribution generalization and algorithmic reasoning.",
            "ai_keywords": [
                "Transformer networks",
                "out-of-distribution (OOD) generalization",
                "GSM8K-style modular arithmetic",
                "input-adaptive recurrence",
                "algorithmic supervision",
                "anchored latent representations",
                "discrete bottleneck",
                "explicit error-correction mechanism",
                "latent space reasoning",
                "algorithmic generalization"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-10-15T17:03:59.000Z",
        "title": "Unlocking Out-of-Distribution Generalization in Transformers via\n  Recursive Latent Space Reasoning",
        "summary": "Systematic, compositional generalization beyond the training distribution\nremains a core challenge in machine learning -- and a critical bottleneck for\nthe emergent reasoning abilities of modern language models. This work\ninvestigates out-of-distribution (OOD) generalization in Transformer networks\nusing a GSM8K-style modular arithmetic on computational graphs task as a\ntestbed. We introduce and explore a set of four architectural mechanisms aimed\nat enhancing OOD generalization: (i) input-adaptive recurrence; (ii)\nalgorithmic supervision; (iii) anchored latent representations via a discrete\nbottleneck; and (iv) an explicit error-correction mechanism. Collectively,\nthese mechanisms yield an architectural approach for native and scalable latent\nspace reasoning in Transformer networks with robust algorithmic generalization\ncapabilities. We complement these empirical results with a detailed mechanistic\ninterpretability analysis that reveals how these mechanisms give rise to robust\nOOD generalization abilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14095.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637abd9114076b808c6a2f4f",
            "avatarUrl": "/avatars/7fa8b275b4347dfeab73384e2aa72ea4.svg",
            "fullname": "Awni Altabaa",
            "name": "awni00",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.12764",
            "authors": [
                {
                    "_id": "68f25feb37ed56c2c926f902",
                    "name": "Thomas Wimmer",
                    "hidden": false
                },
                {
                    "_id": "68f25feb37ed56c2c926f903",
                    "name": "Prune Truong",
                    "hidden": false
                },
                {
                    "_id": "68f25feb37ed56c2c926f904",
                    "name": "Marie-Julie Rakotosaona",
                    "hidden": false
                },
                {
                    "_id": "68f25feb37ed56c2c926f905",
                    "name": "Michael Oechsle",
                    "hidden": false
                },
                {
                    "_id": "68f25feb37ed56c2c926f906",
                    "name": "Federico Tombari",
                    "hidden": false
                },
                {
                    "_id": "68f25feb37ed56c2c926f907",
                    "name": "Bernt Schiele",
                    "hidden": false
                },
                {
                    "_id": "68f25feb37ed56c2c926f908",
                    "name": "Jan Eric Lenssen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/638a2350d8e0b272508e6227/WPjyr9LTxg5TJH3BWbd_h.qt"
            ],
            "publishedAt": "2025-10-14T17:45:17.000Z",
            "submittedOnDailyAt": "2025-10-17T14:00:22.493Z",
            "title": "AnyUp: Universal Feature Upsampling",
            "submittedOnDailyBy": {
                "_id": "638a2350d8e0b272508e6227",
                "avatarUrl": "/avatars/83d6090eb16530b51941aa94cd51be3c.svg",
                "isPro": false,
                "fullname": "Thomas Wimmer",
                "user": "wimmerth",
                "type": "user"
            },
            "summary": "We introduce AnyUp, a method for feature upsampling that can be applied to\nany vision feature at any resolution, without encoder-specific training.\nExisting learning-based upsamplers for features like DINO or CLIP need to be\nre-trained for every feature extractor and thus do not generalize to different\nfeature types at inference time. In this work, we propose an inference-time\nfeature-agnostic upsampling architecture to alleviate this limitation and\nimprove upsampling quality. In our experiments, AnyUp sets a new state of the\nart for upsampled features, generalizes to different feature types, and\npreserves feature semantics while being efficient and easy to apply to a wide\nrange of downstream tasks.",
            "upvotes": 1,
            "discussionId": "68f25feb37ed56c2c926f909",
            "projectPage": "https://wimmerth.github.io/anyup/",
            "githubRepo": "https://github.com/wimmerth/anyup",
            "ai_summary": "AnyUp is a feature-agnostic upsampling method that generalizes across different vision features and resolutions without requiring re-training.",
            "ai_keywords": [
                "feature upsampling",
                "vision feature",
                "inference-time",
                "feature-agnostic",
                "upsampling architecture",
                "DINO",
                "CLIP",
                "feature extractor",
                "feature semantics",
                "downstream tasks"
            ],
            "githubStars": 186,
            "organization": {
                "_id": "68f264bd142ed543d7ce2e5d",
                "name": "MPI-INF",
                "fullname": "Max Planck Institute for Informatics",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/638a2350d8e0b272508e6227/b9QGPqO5MkNdlNRt9haJd.webp"
            }
        },
        "publishedAt": "2025-10-14T13:45:17.000Z",
        "title": "AnyUp: Universal Feature Upsampling",
        "summary": "We introduce AnyUp, a method for feature upsampling that can be applied to\nany vision feature at any resolution, without encoder-specific training.\nExisting learning-based upsamplers for features like DINO or CLIP need to be\nre-trained for every feature extractor and thus do not generalize to different\nfeature types at inference time. In this work, we propose an inference-time\nfeature-agnostic upsampling architecture to alleviate this limitation and\nimprove upsampling quality. In our experiments, AnyUp sets a new state of the\nart for upsampled features, generalizes to different feature types, and\npreserves feature semantics while being efficient and easy to apply to a wide\nrange of downstream tasks.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/638a2350d8e0b272508e6227/WPjyr9LTxg5TJH3BWbd_h.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12764.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638a2350d8e0b272508e6227",
            "avatarUrl": "/avatars/83d6090eb16530b51941aa94cd51be3c.svg",
            "fullname": "Thomas Wimmer",
            "name": "wimmerth",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "68f264bd142ed543d7ce2e5d",
            "name": "MPI-INF",
            "fullname": "Max Planck Institute for Informatics",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/638a2350d8e0b272508e6227/b9QGPqO5MkNdlNRt9haJd.webp"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.10390",
            "authors": [
                {
                    "_id": "68f1bb1f6e0bef323a68fe35",
                    "name": "Aashiq Muhamed",
                    "hidden": false
                },
                {
                    "_id": "68f1bb1f6e0bef323a68fe36",
                    "name": "Leonardo F. R. Ribeiro",
                    "hidden": false
                },
                {
                    "_id": "68f1bb1f6e0bef323a68fe37",
                    "name": "Markus Dreyer",
                    "hidden": false
                },
                {
                    "_id": "68f1bb1f6e0bef323a68fe38",
                    "name": "Virginia Smith",
                    "hidden": false
                },
                {
                    "_id": "68f1bb1f6e0bef323a68fe39",
                    "name": "Mona T. Diab",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-12T00:53:42.000Z",
            "submittedOnDailyAt": "2025-10-17T02:13:43.285Z",
            "title": "RefusalBench: Generative Evaluation of Selective Refusal in Grounded\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "64755a83e0b188d3cb2579d8",
                "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
                "isPro": false,
                "fullname": "Aashiq Muhamed",
                "user": "aashiqmuhamed",
                "type": "user"
            },
            "summary": "The ability of language models in RAG systems to selectively refuse to answer\nbased on flawed context is critical for safety, yet remains a significant\nfailure point. Our large-scale study reveals that even frontier models struggle\nin this setting, with refusal accuracy dropping below 50% on multi-document\ntasks, while exhibiting either dangerous overconfidence or overcaution. Static\nbenchmarks fail to reliably evaluate this capability, as models exploit\ndataset-specific artifacts and memorize test instances. We introduce\nRefusalBench, a generative methodology that programmatically creates diagnostic\ntest cases through controlled linguistic perturbation. Our framework employs\n176 distinct perturbation strategies across six categories of informational\nuncertainty and three intensity levels. Evaluation of over 30 models uncovers\nsystematic failure patterns: refusal comprises separable detection and\ncategorization skills, and neither scale nor extended reasoning improves\nperformance. We find that selective refusal is a trainable, alignment-sensitive\ncapability, offering a clear path for improvement. We release two benchmarks --\nRefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) --\nand our complete generation framework to enable continued, dynamic evaluation\nof this critical capability.",
            "upvotes": 1,
            "discussionId": "68f1bb206e0bef323a68fe3a",
            "ai_summary": "RefusalBench evaluates the selective refusal capability of language models in RAG systems using programmatically generated test cases, revealing systematic failure patterns and offering a path for improvement.",
            "ai_keywords": [
                "RAG systems",
                "language models",
                "selective refusal",
                "refusal accuracy",
                "dangerous overconfidence",
                "overcaution",
                "static benchmarks",
                "generative methodology",
                "controlled linguistic perturbation",
                "perturbation strategies",
                "informational uncertainty",
                "intensity levels",
                "RefusalBench-NQ",
                "RefusalBench-GaRAGe"
            ],
            "organization": {
                "_id": "674e3f226f1597e933139875",
                "name": "amazon-agi",
                "fullname": "Amazon AGI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6163872970554df7422ef784/DZhaUsJ9C5C2G6vHKUzLD.png"
            }
        },
        "publishedAt": "2025-10-11T20:53:42.000Z",
        "title": "RefusalBench: Generative Evaluation of Selective Refusal in Grounded\n  Language Models",
        "summary": "The ability of language models in RAG systems to selectively refuse to answer\nbased on flawed context is critical for safety, yet remains a significant\nfailure point. Our large-scale study reveals that even frontier models struggle\nin this setting, with refusal accuracy dropping below 50% on multi-document\ntasks, while exhibiting either dangerous overconfidence or overcaution. Static\nbenchmarks fail to reliably evaluate this capability, as models exploit\ndataset-specific artifacts and memorize test instances. We introduce\nRefusalBench, a generative methodology that programmatically creates diagnostic\ntest cases through controlled linguistic perturbation. Our framework employs\n176 distinct perturbation strategies across six categories of informational\nuncertainty and three intensity levels. Evaluation of over 30 models uncovers\nsystematic failure patterns: refusal comprises separable detection and\ncategorization skills, and neither scale nor extended reasoning improves\nperformance. We find that selective refusal is a trainable, alignment-sensitive\ncapability, offering a clear path for improvement. We release two benchmarks --\nRefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) --\nand our complete generation framework to enable continued, dynamic evaluation\nof this critical capability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10390.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64755a83e0b188d3cb2579d8",
            "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
            "fullname": "Aashiq Muhamed",
            "name": "aashiqmuhamed",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "674e3f226f1597e933139875",
            "name": "amazon-agi",
            "fullname": "Amazon AGI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6163872970554df7422ef784/DZhaUsJ9C5C2G6vHKUzLD.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.06694",
            "authors": [
                {
                    "_id": "68f1e5d66e0bef323a690014",
                    "name": "Jipeng Lyu",
                    "hidden": false
                },
                {
                    "_id": "68f1e5d66e0bef323a690015",
                    "name": "Jiahua Dong",
                    "hidden": false
                },
                {
                    "_id": "68f1e5d66e0bef323a690016",
                    "name": "Yu-Xiong Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T06:39:33.000Z",
            "submittedOnDailyAt": "2025-10-17T05:17:07.221Z",
            "title": "SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D\n  Novel View Synthesis",
            "submittedOnDailyBy": {
                "_id": "6475b69ef60449361e556b78",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6475b69ef60449361e556b78/qkyWjWe7y1LruNm-uiFhm.png",
                "isPro": false,
                "fullname": "Jipeng Lyu",
                "user": "kedaxiaoqiu",
                "type": "user"
            },
            "summary": "Persistent dynamic scene modeling for tracking and novel-view synthesis\nremains challenging due to the difficulty of capturing accurate deformations\nwhile maintaining computational efficiency. We propose SCas4D, a cascaded\noptimization framework that leverages structural patterns in 3D Gaussian\nSplatting for dynamic scenes. The key idea is that real-world deformations\noften exhibit hierarchical patterns, where groups of Gaussians share similar\ntransformations. By progressively refining deformations from coarse part-level\nto fine point-level, SCas4D achieves convergence within 100 iterations per time\nframe and produces results comparable to existing methods with only\none-twentieth of the training iterations. The approach also demonstrates\neffectiveness in self-supervised articulated object segmentation, novel view\nsynthesis, and dense point tracking tasks.",
            "upvotes": 1,
            "discussionId": "68f1e5d66e0bef323a690017",
            "ai_summary": "SCas4D, a cascaded optimization framework using 3D Gaussian Splatting, efficiently models dynamic scenes by leveraging hierarchical deformation patterns, enabling fast convergence and high-quality results in various tasks.",
            "ai_keywords": [
                "3D Gaussian Splatting",
                "cascaded optimization framework",
                "hierarchical patterns",
                "deformations",
                "self-supervised articulated object segmentation",
                "novel view synthesis",
                "dense point tracking"
            ],
            "organization": {
                "_id": "65448bef5b5d9185ba3202b9",
                "name": "UIUC-CS",
                "fullname": "University of Illinois at Urbana-Champaign",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
            }
        },
        "publishedAt": "2025-10-08T02:39:33.000Z",
        "title": "SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D\n  Novel View Synthesis",
        "summary": "Persistent dynamic scene modeling for tracking and novel-view synthesis\nremains challenging due to the difficulty of capturing accurate deformations\nwhile maintaining computational efficiency. We propose SCas4D, a cascaded\noptimization framework that leverages structural patterns in 3D Gaussian\nSplatting for dynamic scenes. The key idea is that real-world deformations\noften exhibit hierarchical patterns, where groups of Gaussians share similar\ntransformations. By progressively refining deformations from coarse part-level\nto fine point-level, SCas4D achieves convergence within 100 iterations per time\nframe and produces results comparable to existing methods with only\none-twentieth of the training iterations. The approach also demonstrates\neffectiveness in self-supervised articulated object segmentation, novel view\nsynthesis, and dense point tracking tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06694.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6475b69ef60449361e556b78",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6475b69ef60449361e556b78/qkyWjWe7y1LruNm-uiFhm.png",
            "fullname": "Jipeng Lyu",
            "name": "kedaxiaoqiu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "65448bef5b5d9185ba3202b9",
            "name": "UIUC-CS",
            "fullname": "University of Illinois at Urbana-Champaign",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14942",
            "authors": [
                {
                    "_id": "68f2675637ed56c2c926f935",
                    "name": "Yao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f2675637ed56c2c926f936",
                    "name": "Yu Wu",
                    "hidden": false
                },
                {
                    "_id": "68f2675637ed56c2c926f937",
                    "name": "Haowei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f2675637ed56c2c926f938",
                    "name": "Weiguo Li",
                    "hidden": false
                },
                {
                    "_id": "68f2675637ed56c2c926f939",
                    "name": "Haokun Chen",
                    "hidden": false
                },
                {
                    "_id": "68f2675637ed56c2c926f93a",
                    "name": "Jingpei Wu",
                    "hidden": false
                },
                {
                    "_id": "68f2675637ed56c2c926f93b",
                    "name": "Guohao Li",
                    "hidden": false
                },
                {
                    "_id": "68f2675637ed56c2c926f93c",
                    "name": "Zhen Han",
                    "hidden": false
                },
                {
                    "_id": "68f2675637ed56c2c926f93d",
                    "name": "Volker Tresp",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:54:07.000Z",
            "submittedOnDailyAt": "2025-10-17T14:29:55.259Z",
            "title": "GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for\n  Step-Level Reasoning",
            "submittedOnDailyBy": {
                "_id": "62cecf9c1415317d1fbf6cfe",
                "avatarUrl": "/avatars/d30630ad96bcec1349728ba39476847a.svg",
                "isPro": false,
                "fullname": "Yao Zhang",
                "user": "ZYao720",
                "type": "user"
            },
            "summary": "Process Reward Models (PRMs) aim to improve multi-step reasoning in Large\nLanguage Models (LLMs) by supervising intermediate steps and identifying\nerrors. However, building effective PRMs remains challenging due to the lack of\nscalable, high-quality annotations. Existing approaches rely on costly human\nlabeling, LLM-based self-evaluation that is prone to hallucination, or Monte\nCarlo (MC) estimation, which infers step quality solely from rollout outcomes\nand often introduces noisy, misaligned supervision due to credit\nmisattribution. These issues result in three core limitations: noisy rewards,\nlow factual fidelity, and misalignment with step-level reasoning objectives. To\naddress these challenges, we introduce GroundedPRM, a tree-guided and\nfidelity-aware framework for automatic process supervision. To reduce reward\nnoise and enable fine-grained credit assignment, we construct structured\nreasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated\nsupervision, we validate each intermediate step using an external tool,\nproviding execution-grounded correctness signals. To combine both step-level\nvalidation and global outcome assessment, we design a hybrid reward aggregation\nmechanism that fuses tool-based verification with MCTS-derived feedback.\nFinally, we format the reward signal into a rationale-enhanced, generative\nstructure to promote interpretability and compatibility with instruction-tuned\nLLMs. GroundedPRM is trained on only 40K automatically labeled samples,\namounting to just 10% of the data used by the best-performing PRM trained with\nauto-labeled supervision. Nevertheless, it achieves up to a 26% relative\nimprovement in average performance on ProcessBench. When used for reward-guided\ngreedy search, GroundedPRM outperforms even PRMs trained with human-labeled\nsupervision, offering a scalable and verifiable path toward high-quality\nprocess-level reasoning.",
            "upvotes": 0,
            "discussionId": "68f2675637ed56c2c926f93e",
            "ai_summary": "GroundedPRM uses Monte Carlo Tree Search and external validation to improve multi-step reasoning in LLMs with fewer, higher-quality annotations.",
            "ai_keywords": [
                "Process Reward Models",
                "Large Language Models",
                "multi-step reasoning",
                "intermediate steps",
                "errors",
                "scalable annotations",
                "human labeling",
                "LLM-based self-evaluation",
                "hallucination",
                "Monte Carlo estimation",
                "credit misattribution",
                "noisy rewards",
                "low factual fidelity",
                "misalignment",
                "tree-guided",
                "fidelity-aware",
                "structured reasoning paths",
                "Monte Carlo Tree Search",
                "external tool",
                "execution-grounded correctness signals",
                "hybrid reward aggregation",
                "rationale-enhanced",
                "generative structure",
                "ProcessBench",
                "reward-guided greedy search"
            ],
            "organization": {
                "_id": "62e50495ae9d3f10acb6a9ca",
                "name": "LMU",
                "fullname": "Ludwig Maximilian University of Munich",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1659176121442-5fcaabed246881afd5b00167.png"
            }
        },
        "publishedAt": "2025-10-16T13:54:07.000Z",
        "title": "GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for\n  Step-Level Reasoning",
        "summary": "Process Reward Models (PRMs) aim to improve multi-step reasoning in Large\nLanguage Models (LLMs) by supervising intermediate steps and identifying\nerrors. However, building effective PRMs remains challenging due to the lack of\nscalable, high-quality annotations. Existing approaches rely on costly human\nlabeling, LLM-based self-evaluation that is prone to hallucination, or Monte\nCarlo (MC) estimation, which infers step quality solely from rollout outcomes\nand often introduces noisy, misaligned supervision due to credit\nmisattribution. These issues result in three core limitations: noisy rewards,\nlow factual fidelity, and misalignment with step-level reasoning objectives. To\naddress these challenges, we introduce GroundedPRM, a tree-guided and\nfidelity-aware framework for automatic process supervision. To reduce reward\nnoise and enable fine-grained credit assignment, we construct structured\nreasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated\nsupervision, we validate each intermediate step using an external tool,\nproviding execution-grounded correctness signals. To combine both step-level\nvalidation and global outcome assessment, we design a hybrid reward aggregation\nmechanism that fuses tool-based verification with MCTS-derived feedback.\nFinally, we format the reward signal into a rationale-enhanced, generative\nstructure to promote interpretability and compatibility with instruction-tuned\nLLMs. GroundedPRM is trained on only 40K automatically labeled samples,\namounting to just 10% of the data used by the best-performing PRM trained with\nauto-labeled supervision. Nevertheless, it achieves up to a 26% relative\nimprovement in average performance on ProcessBench. When used for reward-guided\ngreedy search, GroundedPRM outperforms even PRMs trained with human-labeled\nsupervision, offering a scalable and verifiable path toward high-quality\nprocess-level reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14942.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62cecf9c1415317d1fbf6cfe",
            "avatarUrl": "/avatars/d30630ad96bcec1349728ba39476847a.svg",
            "fullname": "Yao Zhang",
            "name": "ZYao720",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "62e50495ae9d3f10acb6a9ca",
            "name": "LMU",
            "fullname": "Ludwig Maximilian University of Munich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1659176121442-5fcaabed246881afd5b00167.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.13161",
            "authors": [
                {
                    "_id": "68f25dc437ed56c2c926f8fa",
                    "name": "Nikhil Bhendawade",
                    "hidden": false
                },
                {
                    "_id": "68f25dc437ed56c2c926f8fb",
                    "name": "Kumari Nishu",
                    "hidden": false
                },
                {
                    "_id": "68f25dc437ed56c2c926f8fc",
                    "name": "Arnav Kundu",
                    "hidden": false
                },
                {
                    "_id": "68f25dc437ed56c2c926f8fd",
                    "name": "Chris Bartels",
                    "hidden": false
                },
                {
                    "_id": "68f25dc437ed56c2c926f8fe",
                    "name": "Minsik Cho",
                    "hidden": false
                },
                {
                    "_id": "68f25dc437ed56c2c926f8ff",
                    "name": "Irina Belousova",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T05:22:57.000Z",
            "submittedOnDailyAt": "2025-10-17T13:48:51.951Z",
            "title": "Mirror Speculative Decoding: Breaking the Serial Barrier in LLM\n  Inference",
            "submittedOnDailyBy": {
                "_id": "64d5061ea146b1c0a63f91c6",
                "avatarUrl": "/avatars/41f322d00c859cd345b853b55a3ad59f.svg",
                "isPro": false,
                "fullname": "Nikhil",
                "user": "NickNickGo",
                "type": "user"
            },
            "summary": "Speculative decoding accelerates LLM inference by using a draft model to look\nahead, but gains are capped by the cost of autoregressive draft generation:\nincreasing draft size elevates acceptance rates but introduces additional\nlatency overhead exacerbating the speed-accuracy tradeoff. Prior methods\n(Medusa, Hydra, EAGLE) partially reduce draft cost but either degrade\nacceptance or introduce overheads that limit scaling. We present Mirror\nSpeculative Decoding (Mirror-SD), an inference algorithm that breaks the\nlatency-acceptance tradeoff. Mirror-SD launches branch-complete rollouts from\nearly-exit signals in parallel with the target model's suffix and explicitly\nmaps computation across heterogeneous accelerators (GPU and NPU) to exploit\ncross-device parallelism. The draft speculates forward continuations for the\ntarget to verify, while the target simultaneously speculates correction paths\nfor the draft, converting speculation into two complementary execution\npipelines. To further cut draft latency without weakening acceptance semantics,\nwe add speculative streaming so the draft emits multiple tokens per step. This\ndual strategy of parallel heterogeneous execution plus multi-token speculative\nstreaming pushes speculative decoding toward its ideal regime of high\nacceptance with low overhead. On SpecBench with server-scale models from 14B to\n66B parameters, Mirror-SD delivers consistent end-to-end gains, achieving\n2.8x-5.8x wall-time speedups across diverse tasks and a 30% average relative\nimprovement over the strongest baseline, EAGLE3.",
            "upvotes": 0,
            "discussionId": "68f25dc537ed56c2c926f900",
            "ai_summary": "Mirror Speculative Decoding accelerates large language model inference by parallelizing speculative execution across heterogeneous accelerators and using multi-token speculative streaming to reduce draft latency without compromising acceptance rates.",
            "ai_keywords": [
                "speculative decoding",
                "draft model",
                "autoregressive draft generation",
                "early-exit signals",
                "branch-complete rollouts",
                "heterogeneous accelerators",
                "GPU",
                "NPU",
                "cross-device parallelism",
                "speculative correction paths",
                "speculative streaming",
                "SpecBench",
                "server-scale models"
            ],
            "organization": {
                "_id": "628cbd99ef14f971b69948ab",
                "name": "apple",
                "fullname": "Apple",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
            }
        },
        "publishedAt": "2025-10-15T01:22:57.000Z",
        "title": "Mirror Speculative Decoding: Breaking the Serial Barrier in LLM\n  Inference",
        "summary": "Speculative decoding accelerates LLM inference by using a draft model to look\nahead, but gains are capped by the cost of autoregressive draft generation:\nincreasing draft size elevates acceptance rates but introduces additional\nlatency overhead exacerbating the speed-accuracy tradeoff. Prior methods\n(Medusa, Hydra, EAGLE) partially reduce draft cost but either degrade\nacceptance or introduce overheads that limit scaling. We present Mirror\nSpeculative Decoding (Mirror-SD), an inference algorithm that breaks the\nlatency-acceptance tradeoff. Mirror-SD launches branch-complete rollouts from\nearly-exit signals in parallel with the target model's suffix and explicitly\nmaps computation across heterogeneous accelerators (GPU and NPU) to exploit\ncross-device parallelism. The draft speculates forward continuations for the\ntarget to verify, while the target simultaneously speculates correction paths\nfor the draft, converting speculation into two complementary execution\npipelines. To further cut draft latency without weakening acceptance semantics,\nwe add speculative streaming so the draft emits multiple tokens per step. This\ndual strategy of parallel heterogeneous execution plus multi-token speculative\nstreaming pushes speculative decoding toward its ideal regime of high\nacceptance with low overhead. On SpecBench with server-scale models from 14B to\n66B parameters, Mirror-SD delivers consistent end-to-end gains, achieving\n2.8x-5.8x wall-time speedups across diverse tasks and a 30% average relative\nimprovement over the strongest baseline, EAGLE3.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13161.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d5061ea146b1c0a63f91c6",
            "avatarUrl": "/avatars/41f322d00c859cd345b853b55a3ad59f.svg",
            "fullname": "Nikhil",
            "name": "NickNickGo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.10472",
            "authors": [
                {
                    "_id": "68ee45dd9b77b5223f6661d1",
                    "user": {
                        "_id": "6763dc97d63e4b348ea3936c",
                        "avatarUrl": "/avatars/0f0567dcaa14276a8c2cb2bdfcc7ec80.svg",
                        "isPro": false,
                        "fullname": "Qiran Zou",
                        "user": "qiranzou",
                        "type": "user"
                    },
                    "name": "Qiran Zou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-14T14:24:08.753Z",
                    "hidden": false
                },
                {
                    "_id": "68ee45dd9b77b5223f6661d2",
                    "name": "Hou Hei Lam",
                    "hidden": false
                },
                {
                    "_id": "68ee45dd9b77b5223f6661d3",
                    "name": "Wenhao Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ee45dd9b77b5223f6661d4",
                    "name": "Yiming Tang",
                    "hidden": false
                },
                {
                    "_id": "68ee45dd9b77b5223f6661d5",
                    "name": "Tingting Chen",
                    "hidden": false
                },
                {
                    "_id": "68ee45dd9b77b5223f6661d6",
                    "name": "Samson Yu",
                    "hidden": false
                },
                {
                    "_id": "68ee45dd9b77b5223f6661d7",
                    "name": "Tianyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ee45dd9b77b5223f6661d8",
                    "name": "Chang Liu",
                    "hidden": false
                },
                {
                    "_id": "68ee45dd9b77b5223f6661d9",
                    "name": "Xiangyang Ji",
                    "hidden": false
                },
                {
                    "_id": "68ee45dd9b77b5223f6661da",
                    "name": "Dianbo Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-12T06:41:05.000Z",
            "submittedOnDailyAt": "2025-10-17T15:34:30.998Z",
            "title": "FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the\n  Importance of Exploration Breadth",
            "submittedOnDailyBy": {
                "_id": "6763dc97d63e4b348ea3936c",
                "avatarUrl": "/avatars/0f0567dcaa14276a8c2cb2bdfcc7ec80.svg",
                "isPro": false,
                "fullname": "Qiran Zou",
                "user": "qiranzou",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have sparked growing interest in automatic\nmachine learning research agents. Among them, agents capable of autonomously\nproposing ideas and conducting machine learning experiments are particularly\npromising, as they maximize research automation and accelerate scientific\nprogress by iteratively refining ideas based on experimental results. However,\ncomprehensively evaluating such agents remains challenging. Existing benchmarks\ntend to overemphasize engineering aspects while neglecting academic rigor,\ncreating barriers that obscure a clear assessment of an agent's scientific\ncapabilities in machine learning research. They also suffer from limited task\ndiversity, an overemphasis on application-oriented tasks over fundamental\nresearch problems, and limited scalability to realistic research settings. To\naddress these limitations, we introduce FML-bench, a benchmark designed to\nevaluate automatic machine learning research agents on 8 diverse and\nfundamental machine learning research problems. It reduces coding burden,\nemphasizes fundamental problems rather than specific use cases, offers high\ntask diversity, and is extensible to real-world machine learning GitHub\nrepositories. Furthermore, we present a unified evaluation framework with five\ncomplementary metrics, designed to comprehensively assess agent performance on\nour benchmark. We evaluate state-of-the-art automatic research agents on\nFML-bench, and find that agents employing broad research exploration strategies\noutperform those focusing on narrow but deep exploration. These findings\nsuggest that emphasizing the breadth of exploration may lead to more effective\nresearch outcomes than focusing solely on incremental refinement. Our benchmark\nis available at https://github.com/qrzou/FML-bench.",
            "upvotes": 0,
            "discussionId": "68ee45de9b77b5223f6661db",
            "projectPage": "https://github.com/qrzou/FML-bench",
            "githubRepo": "https://github.com/qrzou/FML-bench",
            "ai_summary": "FML-bench evaluates automatic machine learning research agents on diverse fundamental problems using a unified framework with multiple metrics, highlighting the importance of broad exploration strategies.",
            "ai_keywords": [
                "large language models",
                "automatic machine learning",
                "research agents",
                "FML-bench",
                "evaluation framework",
                "research exploration strategies"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "6508ab2b349930913196378b",
                "name": "NationalUniversityofSingapore",
                "fullname": "National University of Singapore",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
            }
        },
        "publishedAt": "2025-10-12T02:41:05.000Z",
        "title": "FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the\n  Importance of Exploration Breadth",
        "summary": "Large language models (LLMs) have sparked growing interest in automatic\nmachine learning research agents. Among them, agents capable of autonomously\nproposing ideas and conducting machine learning experiments are particularly\npromising, as they maximize research automation and accelerate scientific\nprogress by iteratively refining ideas based on experimental results. However,\ncomprehensively evaluating such agents remains challenging. Existing benchmarks\ntend to overemphasize engineering aspects while neglecting academic rigor,\ncreating barriers that obscure a clear assessment of an agent's scientific\ncapabilities in machine learning research. They also suffer from limited task\ndiversity, an overemphasis on application-oriented tasks over fundamental\nresearch problems, and limited scalability to realistic research settings. To\naddress these limitations, we introduce FML-bench, a benchmark designed to\nevaluate automatic machine learning research agents on 8 diverse and\nfundamental machine learning research problems. It reduces coding burden,\nemphasizes fundamental problems rather than specific use cases, offers high\ntask diversity, and is extensible to real-world machine learning GitHub\nrepositories. Furthermore, we present a unified evaluation framework with five\ncomplementary metrics, designed to comprehensively assess agent performance on\nour benchmark. We evaluate state-of-the-art automatic research agents on\nFML-bench, and find that agents employing broad research exploration strategies\noutperform those focusing on narrow but deep exploration. These findings\nsuggest that emphasizing the breadth of exploration may lead to more effective\nresearch outcomes than focusing solely on incremental refinement. Our benchmark\nis available at https://github.com/qrzou/FML-bench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10472.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6763dc97d63e4b348ea3936c",
            "avatarUrl": "/avatars/0f0567dcaa14276a8c2cb2bdfcc7ec80.svg",
            "fullname": "Qiran Zou",
            "name": "qiranzou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6508ab2b349930913196378b",
            "name": "NationalUniversityofSingapore",
            "fullname": "National University of Singapore",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
        },
        "isAuthorParticipating": true
    }
]