[
    {
        "paper": {
            "id": "2510.15444",
            "authors": [
                {
                    "_id": "68f60ee18589920bf4d32293",
                    "user": {
                        "_id": "64675fd0b990713c50317559",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64675fd0b990713c50317559/qGUiaMyAd4mUjXJe0EXVU.png",
                        "isPro": false,
                        "fullname": "Zhi Zhou",
                        "user": "WNJXYK",
                        "type": "user"
                    },
                    "name": "Zhi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:09:29.609Z",
                    "hidden": false
                },
                {
                    "_id": "68f60ee18589920bf4d32294",
                    "name": "Yuhao Tan",
                    "hidden": false
                },
                {
                    "_id": "68f60ee18589920bf4d32295",
                    "name": "Zenan Li",
                    "hidden": false
                },
                {
                    "_id": "68f60ee18589920bf4d32296",
                    "name": "Yuan Yao",
                    "hidden": false
                },
                {
                    "_id": "68f60ee18589920bf4d32297",
                    "name": "Lan-Zhe Guo",
                    "hidden": false
                },
                {
                    "_id": "68f60ee18589920bf4d32298",
                    "name": "Yu-Feng Li",
                    "hidden": false
                },
                {
                    "_id": "68f60ee18589920bf4d32299",
                    "name": "Xiaoxing Ma",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T08:59:30.000Z",
            "submittedOnDailyAt": "2025-10-20T11:28:20.308Z",
            "title": "A Theoretical Study on Bridging Internal Probability and\n  Self-Consistency for LLM Reasoning",
            "submittedOnDailyBy": {
                "_id": "64675fd0b990713c50317559",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64675fd0b990713c50317559/qGUiaMyAd4mUjXJe0EXVU.png",
                "isPro": false,
                "fullname": "Zhi Zhou",
                "user": "WNJXYK",
                "type": "user"
            },
            "summary": "Test-time scaling seeks to improve the reasoning performance of large\nlanguage models (LLMs) by adding computational resources. A prevalent approach\nwithin the field is sampling-based test-time scaling methods, which enhance\nreasoning by generating multiple reasoning paths for a given input during\ninference. However, despite its practical success, the theoretical foundations\nremain underexplored. In this paper, we provide the first theoretical framework\nfor analyzing sampling-based test-time scaling methods, grounded in the\nperspective of confidence estimation. Based on the framework, we analyze two\ndominant paradigms: self-consistency and perplexity, and reveal key\nlimitations: self-consistency suffers from high estimation error while\nperplexity exhibits substantial modeling error and possible degradation of the\nestimation error convergence. To address these limitations, we introduce RPC, a\nhybrid method that leverages our theoretical insights through two key\ncomponents: Perplexity Consistency and Reasoning Pruning. Perplexity\nConsistency combines the strengths of self-consistency and perplexity, boosting\nthe convergence rate of estimation error from linear to exponential while\npreserving model error. Reasoning Pruning prevents degradation by eliminating\nlow-probability reasoning paths. Both theoretical analysis and empirical\nresults across seven benchmark datasets demonstrate that RPC has a strong\npotential for reducing reasoning error. Notably, RPC achieves reasoning\nperformance comparable to self-consistency while not only enhancing confidence\nreliability but also reducing sampling costs by 50%. The code and resources are\navailable at https://wnjxyk.github.io/RPC.",
            "upvotes": 107,
            "discussionId": "68f60ee18589920bf4d3229a",
            "projectPage": "https://wnjxyk.github.io/RPC",
            "ai_summary": "A theoretical framework for sampling-based test-time scaling in large language models reveals limitations in self-consistency and perplexity, and introduces RPC to improve reasoning performance and reduce sampling costs.",
            "ai_keywords": [
                "sampling-based test-time scaling",
                "large language models",
                "self-consistency",
                "perplexity",
                "Perplexity Consistency",
                "Reasoning Pruning",
                "confidence estimation",
                "reasoning error",
                "benchmark datasets"
            ],
            "organization": {
                "_id": "67a472fbf09fe7a0c5101ef2",
                "name": "LAMDA-NeSy",
                "fullname": "LAMDA Neuro Symbolic Learning",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63fc116b1b4b1bd4e707d198/iLEbqB8j_71vnPvAekqQC.jpeg"
            }
        },
        "publishedAt": "2025-10-17T04:59:30.000Z",
        "title": "A Theoretical Study on Bridging Internal Probability and\n  Self-Consistency for LLM Reasoning",
        "summary": "Test-time scaling seeks to improve the reasoning performance of large\nlanguage models (LLMs) by adding computational resources. A prevalent approach\nwithin the field is sampling-based test-time scaling methods, which enhance\nreasoning by generating multiple reasoning paths for a given input during\ninference. However, despite its practical success, the theoretical foundations\nremain underexplored. In this paper, we provide the first theoretical framework\nfor analyzing sampling-based test-time scaling methods, grounded in the\nperspective of confidence estimation. Based on the framework, we analyze two\ndominant paradigms: self-consistency and perplexity, and reveal key\nlimitations: self-consistency suffers from high estimation error while\nperplexity exhibits substantial modeling error and possible degradation of the\nestimation error convergence. To address these limitations, we introduce RPC, a\nhybrid method that leverages our theoretical insights through two key\ncomponents: Perplexity Consistency and Reasoning Pruning. Perplexity\nConsistency combines the strengths of self-consistency and perplexity, boosting\nthe convergence rate of estimation error from linear to exponential while\npreserving model error. Reasoning Pruning prevents degradation by eliminating\nlow-probability reasoning paths. Both theoretical analysis and empirical\nresults across seven benchmark datasets demonstrate that RPC has a strong\npotential for reducing reasoning error. Notably, RPC achieves reasoning\nperformance comparable to self-consistency while not only enhancing confidence\nreliability but also reducing sampling costs by 50%. The code and resources are\navailable at https://wnjxyk.github.io/RPC.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15444.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "64675fd0b990713c50317559",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64675fd0b990713c50317559/qGUiaMyAd4mUjXJe0EXVU.png",
            "fullname": "Zhi Zhou",
            "name": "WNJXYK",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "67a472fbf09fe7a0c5101ef2",
            "name": "LAMDA-NeSy",
            "fullname": "LAMDA Neuro Symbolic Learning",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63fc116b1b4b1bd4e707d198/iLEbqB8j_71vnPvAekqQC.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.15870",
            "authors": [
                {
                    "_id": "68f592478589920bf4d32084",
                    "name": "Hanrong Ye",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d32085",
                    "user": {
                        "_id": "629e1b71bb6419817ed7566c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629e1b71bb6419817ed7566c/0ZCt-11eQtRDCOk9AozOp.jpeg",
                        "isPro": false,
                        "fullname": "Huck Yang",
                        "user": "huckiyang",
                        "type": "user"
                    },
                    "name": "Chao-Han Huck Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:11:24.329Z",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d32086",
                    "name": "Arushi Goel",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d32087",
                    "name": "Wei Huang",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d32088",
                    "name": "Ligeng Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d32089",
                    "name": "Yuanhang Su",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d3208a",
                    "name": "Sean Lin",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d3208b",
                    "name": "An-Chieh Cheng",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d3208c",
                    "name": "Zhen Wan",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d3208d",
                    "name": "Jinchuan Tian",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d3208e",
                    "name": "Yuming Lou",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d3208f",
                    "name": "Dong Yang",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d32090",
                    "name": "Zhijian Liu",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d32091",
                    "name": "Yukang Chen",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d32092",
                    "name": "Ambrish Dantrey",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d32093",
                    "name": "Ehsan Jahangiri",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d32094",
                    "name": "Sreyan Ghosh",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d32095",
                    "name": "Daguang Xu",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d32096",
                    "name": "Ehsan Hosseini-Asl",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d32097",
                    "name": "Danial Mohseni Taheri",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d32098",
                    "name": "Vidya Murali",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d32099",
                    "name": "Sifei Liu",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d3209a",
                    "name": "Jason Lu",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d3209b",
                    "name": "Oluwatobi Olabiyi",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d3209c",
                    "name": "Frank Wang",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d3209d",
                    "name": "Rafael Valle",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d3209e",
                    "name": "Bryan Catanzaro",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d3209f",
                    "name": "Andrew Tao",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d320a0",
                    "name": "Song Han",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d320a1",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d320a2",
                    "name": "Hongxu Yin",
                    "hidden": false
                },
                {
                    "_id": "68f592478589920bf4d320a3",
                    "name": "Pavlo Molchanov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T17:59:59.000Z",
            "submittedOnDailyAt": "2025-10-20T00:07:23.233Z",
            "title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding\n  LLM",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Advancing machine intelligence requires developing the ability to perceive\nacross multiple modalities, much as humans sense the world. We introduce\nOmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We\ncarefully study the design choices across model architecture and data curation.\nFor model architecture, we present three key innovations: (i) OmniAlignNet for\nstrengthening alignment between vision and audio embeddings in a shared\nomni-modal latent space; (ii) Temporal Embedding Grouping for capturing\nrelative temporal alignment between vision and audio signals; and (iii)\nConstrained Rotary Time Embedding for encoding absolute temporal information in\nomni-modal embeddings. We introduce a curation and synthesis pipeline that\ngenerates 24M single-modal and omni-modal conversations. We find that\nmodalities reinforce one another in both perception and reasoning. Our model,\nOmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal\nunderstanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while\nusing just 0.2T training tokens - a 6 times reduction compared to\nQwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream\napplications spanning robotics, medical AI, and smart factory.",
            "upvotes": 55,
            "discussionId": "68f592478589920bf4d320a4",
            "projectPage": "https://nvlabs.github.io/OmniVinci/",
            "githubRepo": "https://github.com/NVlabs/OmniVinci",
            "ai_summary": "OmniVinci, an open-source omni-modal LLM, enhances cross-modal understanding and performance across audio, vision, and robotics applications with innovative architecture and efficient data curation.",
            "ai_keywords": [
                "OmniAlignNet",
                "Temporal Embedding Grouping",
                "Constrained Rotary Time Embedding",
                "omni-modal latent space",
                "DailyOmni",
                "MMAR",
                "Video-MME",
                "omni-modal conversations",
                "omni-modal advantages"
            ],
            "githubStars": 96,
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-10-17T13:59:59.000Z",
        "title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding\n  LLM",
        "summary": "Advancing machine intelligence requires developing the ability to perceive\nacross multiple modalities, much as humans sense the world. We introduce\nOmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We\ncarefully study the design choices across model architecture and data curation.\nFor model architecture, we present three key innovations: (i) OmniAlignNet for\nstrengthening alignment between vision and audio embeddings in a shared\nomni-modal latent space; (ii) Temporal Embedding Grouping for capturing\nrelative temporal alignment between vision and audio signals; and (iii)\nConstrained Rotary Time Embedding for encoding absolute temporal information in\nomni-modal embeddings. We introduce a curation and synthesis pipeline that\ngenerates 24M single-modal and omni-modal conversations. We find that\nmodalities reinforce one another in both perception and reasoning. Our model,\nOmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal\nunderstanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while\nusing just 0.2T training tokens - a 6 times reduction compared to\nQwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream\napplications spanning robotics, medical AI, and smart factory.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15870.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 132
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.15019",
            "authors": [
                {
                    "_id": "68f5c6ae8589920bf4d321bc",
                    "name": "Junliang Ye",
                    "hidden": false
                },
                {
                    "_id": "68f5c6ae8589920bf4d321bd",
                    "name": "Shenghao Xie",
                    "hidden": false
                },
                {
                    "_id": "68f5c6ae8589920bf4d321be",
                    "name": "Ruowen Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f5c6ae8589920bf4d321bf",
                    "name": "Zhengyi Wang",
                    "hidden": false
                },
                {
                    "_id": "68f5c6ae8589920bf4d321c0",
                    "name": "Hongyu Yan",
                    "hidden": false
                },
                {
                    "_id": "68f5c6ae8589920bf4d321c1",
                    "name": "Wenqiang Zu",
                    "hidden": false
                },
                {
                    "_id": "68f5c6ae8589920bf4d321c2",
                    "name": "Lei Ma",
                    "hidden": false
                },
                {
                    "_id": "68f5c6ae8589920bf4d321c3",
                    "name": "Jun Zhu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/ldmAjBXk17bLdgoKhA9W9.mp4"
            ],
            "publishedAt": "2025-10-16T17:51:50.000Z",
            "submittedOnDailyAt": "2025-10-20T04:11:33.811Z",
            "title": "NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks",
            "submittedOnDailyBy": {
                "_id": "65a420cd90e65dc39a6abe9e",
                "avatarUrl": "/avatars/81ac5b749043e899f5017782409f9e28.svg",
                "isPro": false,
                "fullname": "yejunliang",
                "user": "yejunliang23",
                "type": "user"
            },
            "summary": "3D object editing is essential for interactive content creation in gaming,\nanimation, and robotics, yet current approaches remain inefficient,\ninconsistent, and often fail to preserve unedited regions. Most methods rely on\nediting multi-view renderings followed by reconstruction, which introduces\nartifacts and limits practicality. To address these challenges, we propose\nNano3D, a training-free framework for precise and coherent 3D object editing\nwithout masks. Nano3D integrates FlowEdit into TRELLIS to perform localized\nedits guided by front-view renderings, and further introduces region-aware\nmerging strategies, Voxel/Slat-Merge, which adaptively preserve structural\nfidelity by ensuring consistency between edited and unedited areas. Experiments\ndemonstrate that Nano3D achieves superior 3D consistency and visual quality\ncompared with existing methods. Based on this framework, we construct the first\nlarge-scale 3D editing datasets Nano3D-Edit-100k, which contains over 100,000\nhigh-quality 3D editing pairs. This work addresses long-standing challenges in\nboth algorithm design and data availability, significantly improving the\ngenerality and reliability of 3D editing, and laying the groundwork for the\ndevelopment of feed-forward 3D editing models. Project\nPage:https://jamesyjl.github.io/Nano3D",
            "upvotes": 48,
            "discussionId": "68f5c6ae8589920bf4d321c4",
            "projectPage": "https://jamesyjl.github.io/Nano3D/",
            "githubRepo": "https://github.com/JAMESYJL/Nano3D/tree/main",
            "ai_summary": "Nano3D is a training-free framework that integrates FlowEdit and TRELLIS for precise 3D object editing, using front-view renderings and region-aware merging strategies to maintain structural fidelity and visual quality.",
            "ai_keywords": [
                "FlowEdit",
                "TRELLIS",
                "front-view renderings",
                "region-aware merging",
                "Voxel/Slat-Merge",
                "3D consistency",
                "3D editing datasets",
                "feed-forward 3D editing models"
            ],
            "githubStars": 26
        },
        "publishedAt": "2025-10-16T13:51:50.000Z",
        "title": "NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks",
        "summary": "3D object editing is essential for interactive content creation in gaming,\nanimation, and robotics, yet current approaches remain inefficient,\ninconsistent, and often fail to preserve unedited regions. Most methods rely on\nediting multi-view renderings followed by reconstruction, which introduces\nartifacts and limits practicality. To address these challenges, we propose\nNano3D, a training-free framework for precise and coherent 3D object editing\nwithout masks. Nano3D integrates FlowEdit into TRELLIS to perform localized\nedits guided by front-view renderings, and further introduces region-aware\nmerging strategies, Voxel/Slat-Merge, which adaptively preserve structural\nfidelity by ensuring consistency between edited and unedited areas. Experiments\ndemonstrate that Nano3D achieves superior 3D consistency and visual quality\ncompared with existing methods. Based on this framework, we construct the first\nlarge-scale 3D editing datasets Nano3D-Edit-100k, which contains over 100,000\nhigh-quality 3D editing pairs. This work addresses long-standing challenges in\nboth algorithm design and data availability, significantly improving the\ngenerality and reliability of 3D editing, and laying the groundwork for the\ndevelopment of feed-forward 3D editing models. Project\nPage:https://jamesyjl.github.io/Nano3D",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/ldmAjBXk17bLdgoKhA9W9.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15019.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65a420cd90e65dc39a6abe9e",
            "avatarUrl": "/avatars/81ac5b749043e899f5017782409f9e28.svg",
            "fullname": "yejunliang",
            "name": "yejunliang23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.11288",
            "authors": [
                {
                    "_id": "68f5fe5d8589920bf4d32275",
                    "name": "Nikita Afonin",
                    "hidden": false
                },
                {
                    "_id": "68f5fe5d8589920bf4d32276",
                    "name": "Nikita Andriyanov",
                    "hidden": false
                },
                {
                    "_id": "68f5fe5d8589920bf4d32277",
                    "name": "Nikhil Bageshpura",
                    "hidden": false
                },
                {
                    "_id": "68f5fe5d8589920bf4d32278",
                    "name": "Kyle Liu",
                    "hidden": false
                },
                {
                    "_id": "68f5fe5d8589920bf4d32279",
                    "name": "Kevin Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f5fe5d8589920bf4d3227a",
                    "name": "Sunishchal Dev",
                    "hidden": false
                },
                {
                    "_id": "68f5fe5d8589920bf4d3227b",
                    "name": "Ashwinee Panda",
                    "hidden": false
                },
                {
                    "_id": "68f5fe5d8589920bf4d3227c",
                    "name": "Alexander Panchenko",
                    "hidden": false
                },
                {
                    "_id": "68f5fe5d8589920bf4d3227d",
                    "name": "Oleg Rogov",
                    "hidden": false
                },
                {
                    "_id": "68f5fe5d8589920bf4d3227e",
                    "name": "Elena Tutubalina",
                    "hidden": false
                },
                {
                    "_id": "68f5fe5d8589920bf4d3227f",
                    "name": "Mikhail Seleznyov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T11:23:56.000Z",
            "submittedOnDailyAt": "2025-10-20T07:50:50.427Z",
            "title": "Emergent Misalignment via In-Context Learning: Narrow in-context\n  examples can produce broadly misaligned LLMs",
            "submittedOnDailyBy": {
                "_id": "654621f45cd5692b3a9d08cb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/NJj1FJyYHNPRXb0QBXjv-.jpeg",
                "isPro": false,
                "fullname": "Mikhail Seleznyov",
                "user": "myyycroft",
                "type": "user"
            },
            "summary": "Recent work has shown that narrow finetuning can produce broadly misaligned\nLLMs, a phenomenon termed emergent misalignment (EM). While concerning, these\nfindings were limited to finetuning and activation steering, leaving out\nin-context learning (ICL). We therefore ask: does EM emerge in ICL? We find\nthat it does: across three datasets, three frontier models produce broadly\nmisaligned responses at rates between 2% and 17% given 64 narrow in-context\nexamples, and up to 58% with 256 examples. We also examine mechanisms of EM by\neliciting step-by-step reasoning (while leaving in-context examples unchanged).\nManual analysis of the resulting chain-of-thought shows that 67.5% of\nmisaligned traces explicitly rationalize harmful outputs by adopting a reckless\nor dangerous ''persona'', echoing prior results on finetuning-induced EM.",
            "upvotes": 39,
            "discussionId": "68f5fe5d8589920bf4d32280",
            "ai_summary": "Emergent misalignment occurs in in-context learning across multiple models and datasets, with misaligned responses increasing with the number of examples provided.",
            "ai_keywords": [
                "emergent misalignment",
                "in-context learning",
                "chain-of-thought",
                "persona"
            ]
        },
        "publishedAt": "2025-10-13T07:23:56.000Z",
        "title": "Emergent Misalignment via In-Context Learning: Narrow in-context\n  examples can produce broadly misaligned LLMs",
        "summary": "Recent work has shown that narrow finetuning can produce broadly misaligned\nLLMs, a phenomenon termed emergent misalignment (EM). While concerning, these\nfindings were limited to finetuning and activation steering, leaving out\nin-context learning (ICL). We therefore ask: does EM emerge in ICL? We find\nthat it does: across three datasets, three frontier models produce broadly\nmisaligned responses at rates between 2% and 17% given 64 narrow in-context\nexamples, and up to 58% with 256 examples. We also examine mechanisms of EM by\neliciting step-by-step reasoning (while leaving in-context examples unchanged).\nManual analysis of the resulting chain-of-thought shows that 67.5% of\nmisaligned traces explicitly rationalize harmful outputs by adopting a reckless\nor dangerous ''persona'', echoing prior results on finetuning-induced EM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11288.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "654621f45cd5692b3a9d08cb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/NJj1FJyYHNPRXb0QBXjv-.jpeg",
            "fullname": "Mikhail Seleznyov",
            "name": "myyycroft",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.15742",
            "authors": [
                {
                    "_id": "68f5a20f8589920bf4d3212e",
                    "user": {
                        "_id": "63f0baf66309c84d5f4a2226",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f0baf66309c84d5f4a2226/ihOgtwseRkfP1t-60IgyT.jpeg",
                        "isPro": true,
                        "fullname": "Qingyan",
                        "user": "QingyanBai",
                        "type": "user"
                    },
                    "name": "Qingyan Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:10:45.283Z",
                    "hidden": false
                },
                {
                    "_id": "68f5a20f8589920bf4d3212f",
                    "user": {
                        "_id": "64981bea09cea550852652af",
                        "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg",
                        "isPro": false,
                        "fullname": "Qiuyu Wang",
                        "user": "qiuyuu",
                        "type": "user"
                    },
                    "name": "Qiuyu Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:10:49.093Z",
                    "hidden": false
                },
                {
                    "_id": "68f5a20f8589920bf4d32130",
                    "name": "Hao Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68f5a20f8589920bf4d32131",
                    "user": {
                        "_id": "662128ec9ca2cd4e6db2fb44",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662128ec9ca2cd4e6db2fb44/uUg1V-pVfxT3mLuFgJuAN.jpeg",
                        "isPro": false,
                        "fullname": "Bruce Yu",
                        "user": "bruceyyu",
                        "type": "user"
                    },
                    "name": "Yue Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:10:54.813Z",
                    "hidden": false
                },
                {
                    "_id": "68f5a20f8589920bf4d32132",
                    "name": "Hanlin Wang",
                    "hidden": false
                },
                {
                    "_id": "68f5a20f8589920bf4d32133",
                    "user": {
                        "_id": "63f089456309c84d5f47f951",
                        "avatarUrl": "/avatars/04b926a7f2ad091ee00fef0c59903492.svg",
                        "isPro": false,
                        "fullname": "Wen Wang",
                        "user": "wwen1997",
                        "type": "user"
                    },
                    "name": "Wen Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:10:52.179Z",
                    "hidden": false
                },
                {
                    "_id": "68f5a20f8589920bf4d32134",
                    "name": "Ka Leong Cheng",
                    "hidden": false
                },
                {
                    "_id": "68f5a20f8589920bf4d32135",
                    "name": "Shuailei Ma",
                    "hidden": false
                },
                {
                    "_id": "68f5a20f8589920bf4d32136",
                    "name": "Yanhong Zeng",
                    "hidden": false
                },
                {
                    "_id": "68f5a20f8589920bf4d32137",
                    "name": "Zichen Liu",
                    "hidden": false
                },
                {
                    "_id": "68f5a20f8589920bf4d32138",
                    "name": "Yinghao Xu",
                    "hidden": false
                },
                {
                    "_id": "68f5a20f8589920bf4d32139",
                    "name": "Yujun Shen",
                    "hidden": false
                },
                {
                    "_id": "68f5a20f8589920bf4d3213a",
                    "name": "Qifeng Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63f0baf66309c84d5f4a2226/Ep2-xcIdP80khURfhTDcz.mp4"
            ],
            "publishedAt": "2025-10-17T15:31:40.000Z",
            "submittedOnDailyAt": "2025-10-20T01:18:11.842Z",
            "title": "Scaling Instruction-Based Video Editing with a High-Quality Synthetic\n  Dataset",
            "submittedOnDailyBy": {
                "_id": "63f0baf66309c84d5f4a2226",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f0baf66309c84d5f4a2226/ihOgtwseRkfP1t-60IgyT.jpeg",
                "isPro": true,
                "fullname": "Qingyan",
                "user": "QingyanBai",
                "type": "user"
            },
            "summary": "Instruction-based video editing promises to democratize content creation, yet\nits progress is severely hampered by the scarcity of large-scale, high-quality\ntraining data. We introduce Ditto, a holistic framework designed to tackle this\nfundamental challenge. At its heart, Ditto features a novel data generation\npipeline that fuses the creative diversity of a leading image editor with an\nin-context video generator, overcoming the limited scope of existing models. To\nmake this process viable, our framework resolves the prohibitive cost-quality\ntrade-off by employing an efficient, distilled model architecture augmented by\na temporal enhancer, which simultaneously reduces computational overhead and\nimproves temporal coherence. Finally, to achieve full scalability, this entire\npipeline is driven by an intelligent agent that crafts diverse instructions and\nrigorously filters the output, ensuring quality control at scale. Using this\nframework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of\none million high-fidelity video editing examples. We trained our model, Editto,\non Ditto-1M with a curriculum learning strategy. The results demonstrate\nsuperior instruction-following ability and establish a new state-of-the-art in\ninstruction-based video editing.",
            "upvotes": 36,
            "discussionId": "68f5a20f8589920bf4d3213b",
            "projectPage": "https://ezioby.github.io/Ditto_page",
            "githubRepo": "https://github.com/EzioBy/Ditto",
            "ai_summary": "Ditto framework addresses data scarcity in instruction-based video editing by generating a large dataset and using a curriculum learning strategy to train Editto, achieving superior instruction-following ability.",
            "ai_keywords": [
                "data generation pipeline",
                "image editor",
                "in-context video generator",
                "distilled model architecture",
                "temporal enhancer",
                "temporal coherence",
                "intelligent agent",
                "curriculum learning strategy",
                "instruction-following ability"
            ],
            "githubStars": 111
        },
        "publishedAt": "2025-10-17T11:31:40.000Z",
        "title": "Scaling Instruction-Based Video Editing with a High-Quality Synthetic\n  Dataset",
        "summary": "Instruction-based video editing promises to democratize content creation, yet\nits progress is severely hampered by the scarcity of large-scale, high-quality\ntraining data. We introduce Ditto, a holistic framework designed to tackle this\nfundamental challenge. At its heart, Ditto features a novel data generation\npipeline that fuses the creative diversity of a leading image editor with an\nin-context video generator, overcoming the limited scope of existing models. To\nmake this process viable, our framework resolves the prohibitive cost-quality\ntrade-off by employing an efficient, distilled model architecture augmented by\na temporal enhancer, which simultaneously reduces computational overhead and\nimproves temporal coherence. Finally, to achieve full scalability, this entire\npipeline is driven by an intelligent agent that crafts diverse instructions and\nrigorously filters the output, ensuring quality control at scale. Using this\nframework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of\none million high-fidelity video editing examples. We trained our model, Editto,\non Ditto-1M with a curriculum learning strategy. The results demonstrate\nsuperior instruction-following ability and establish a new state-of-the-art in\ninstruction-based video editing.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63f0baf66309c84d5f4a2226/Ep2-xcIdP80khURfhTDcz.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15742.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63f0baf66309c84d5f4a2226",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f0baf66309c84d5f4a2226/ihOgtwseRkfP1t-60IgyT.jpeg",
            "fullname": "Qingyan",
            "name": "QingyanBai",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.15869",
            "authors": [
                {
                    "_id": "68f593468589920bf4d320d3",
                    "user": {
                        "_id": "655f1770f74fa124d1172ec1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655f1770f74fa124d1172ec1/bdYocZ1qN50CAfb2z2YLA.png",
                        "isPro": false,
                        "fullname": "Jie-Ying Lee",
                        "user": "jayinnn",
                        "type": "user"
                    },
                    "name": "Jie-Ying Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:11:07.295Z",
                    "hidden": false
                },
                {
                    "_id": "68f593468589920bf4d320d4",
                    "name": "Yi-Ruei Liu",
                    "hidden": false
                },
                {
                    "_id": "68f593468589920bf4d320d5",
                    "name": "Shr-Ruei Tsai",
                    "hidden": false
                },
                {
                    "_id": "68f593468589920bf4d320d6",
                    "name": "Wei-Cheng Chang",
                    "hidden": false
                },
                {
                    "_id": "68f593468589920bf4d320d7",
                    "name": "Chung-Ho Wu",
                    "hidden": false
                },
                {
                    "_id": "68f593468589920bf4d320d8",
                    "name": "Jiewen Chan",
                    "hidden": false
                },
                {
                    "_id": "68f593468589920bf4d320d9",
                    "name": "Zhenjun Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f593468589920bf4d320da",
                    "name": "Chieh Hubert Lin",
                    "hidden": false
                },
                {
                    "_id": "68f593468589920bf4d320db",
                    "name": "Yu-Lun Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T17:59:51.000Z",
            "submittedOnDailyAt": "2025-10-20T00:11:33.356Z",
            "title": "Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite\n  Imagery",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Synthesizing large-scale, explorable, and geometrically accurate 3D urban\nscenes is a challenging yet valuable task in providing immersive and embodied\napplications. The challenges lie in the lack of large-scale and high-quality\nreal-world 3D scans for training generalizable generative models. In this\npaper, we take an alternative route to create large-scale 3D scenes by\nsynergizing the readily available satellite imagery that supplies realistic\ncoarse geometry and the open-domain diffusion model for creating high-quality\nclose-up appearances. We propose Skyfall-GS, the first city-block\nscale 3D scene creation framework without costly 3D annotations, also featuring\nreal-time, immersive 3D exploration. We tailor a curriculum-driven iterative\nrefinement strategy to progressively enhance geometric completeness and\nphotorealistic textures. Extensive experiments demonstrate that Skyfall-GS\nprovides improved cross-view consistent geometry and more realistic textures\ncompared to state-of-the-art approaches. Project page:\nhttps://skyfall-gs.jayinnn.dev/",
            "upvotes": 32,
            "discussionId": "68f593468589920bf4d320dc",
            "projectPage": "https://skyfall-gs.jayinnn.dev/",
            "githubRepo": "https://github.com/jayin92/skyfall-gs",
            "ai_summary": "Skyfall-GS creates large-scale, high-quality 3D urban scenes using satellite imagery and diffusion models, offering real-time exploration and improved geometry and texture consistency.",
            "ai_keywords": [
                "diffusion model",
                "Skyfall-GS",
                "curriculum-driven iterative refinement",
                "geometric completeness",
                "photorealistic textures",
                "cross-view consistent geometry"
            ],
            "githubStars": 55
        },
        "publishedAt": "2025-10-17T13:59:51.000Z",
        "title": "Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite\n  Imagery",
        "summary": "Synthesizing large-scale, explorable, and geometrically accurate 3D urban\nscenes is a challenging yet valuable task in providing immersive and embodied\napplications. The challenges lie in the lack of large-scale and high-quality\nreal-world 3D scans for training generalizable generative models. In this\npaper, we take an alternative route to create large-scale 3D scenes by\nsynergizing the readily available satellite imagery that supplies realistic\ncoarse geometry and the open-domain diffusion model for creating high-quality\nclose-up appearances. We propose Skyfall-GS, the first city-block\nscale 3D scene creation framework without costly 3D annotations, also featuring\nreal-time, immersive 3D exploration. We tailor a curriculum-driven iterative\nrefinement strategy to progressively enhance geometric completeness and\nphotorealistic textures. Extensive experiments demonstrate that Skyfall-GS\nprovides improved cross-view consistent geometry and more realistic textures\ncompared to state-of-the-art approaches. Project page:\nhttps://skyfall-gs.jayinnn.dev/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15869.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 132
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.15301",
            "authors": [
                {
                    "_id": "68f59fc78589920bf4d32123",
                    "name": "Minglei Shi",
                    "hidden": false
                },
                {
                    "_id": "68f59fc78589920bf4d32124",
                    "user": {
                        "_id": "641d373d353524fe41f1d453",
                        "avatarUrl": "/avatars/6fa9a0a4ba9818a221f835174a14be2d.svg",
                        "isPro": false,
                        "fullname": "Haolin Wang",
                        "user": "howlin",
                        "type": "user"
                    },
                    "name": "Haolin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:10:58.245Z",
                    "hidden": false
                },
                {
                    "_id": "68f59fc78589920bf4d32125",
                    "name": "Wenzhao Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f59fc78589920bf4d32126",
                    "name": "Ziyang Yuan",
                    "hidden": false
                },
                {
                    "_id": "68f59fc78589920bf4d32127",
                    "name": "Xiaoshi Wu",
                    "hidden": false
                },
                {
                    "_id": "68f59fc78589920bf4d32128",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "68f59fc78589920bf4d32129",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "68f59fc78589920bf4d3212a",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f59fc78589920bf4d3212b",
                    "name": "Jiwen Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T04:17:44.000Z",
            "submittedOnDailyAt": "2025-10-20T01:05:37.276Z",
            "title": "Latent Diffusion Model without Variational Autoencoder",
            "submittedOnDailyBy": {
                "_id": "662887715d246621f33d2ce6",
                "avatarUrl": "/avatars/3e0b1017b1e1bf284758ce840c174290.svg",
                "isPro": false,
                "fullname": "Shi Minglei",
                "user": "MingleiShi",
                "type": "user"
            },
            "summary": "Recent progress in diffusion-based visual generation has largely relied on\nlatent diffusion models with variational autoencoders (VAEs). While effective\nfor high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited\ntraining efficiency, slow inference, and poor transferability to broader vision\ntasks. These issues stem from a key limitation of VAE latent spaces: the lack\nof clear semantic separation and strong discriminative structure. Our analysis\nconfirms that these properties are crucial not only for perception and\nunderstanding tasks, but also for the stable and efficient training of latent\ndiffusion models. Motivated by this insight, we introduce SVG, a novel latent\ndiffusion model without variational autoencoders, which leverages\nself-supervised representations for visual generation. SVG constructs a feature\nspace with clear semantic discriminability by leveraging frozen DINO features,\nwhile a lightweight residual branch captures fine-grained details for\nhigh-fidelity reconstruction. Diffusion models are trained directly on this\nsemantically structured latent space to facilitate more efficient learning. As\na result, SVG enables accelerated diffusion training, supports few-step\nsampling, and improves generative quality. Experimental results further show\nthat SVG preserves the semantic and discriminative capabilities of the\nunderlying self-supervised representations, providing a principled pathway\ntoward task-general, high-quality visual representations.",
            "upvotes": 30,
            "discussionId": "68f59fc88589920bf4d3212c",
            "projectPage": "https://howlin-wang.github.io/svg/",
            "githubRepo": "https://github.com/shiml20/SVG",
            "ai_summary": "SVG, a novel latent diffusion model without VAEs, uses self-supervised representations to enable efficient training, few-step sampling, and high-quality visual generation with semantic and discriminative capabilities.",
            "ai_keywords": [
                "latent diffusion models",
                "variational autoencoders",
                "diffusion models",
                "self-supervised representations",
                "DINO features",
                "residual branch",
                "high-fidelity reconstruction",
                "semantic discriminability",
                "task-general",
                "visual representations"
            ],
            "githubStars": 33,
            "organization": {
                "_id": "662c559b322afcbae51b3c8b",
                "name": "KwaiVGI",
                "fullname": "Kuaishou Visual Generation and Interaction Center",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
            }
        },
        "publishedAt": "2025-10-17T00:17:44.000Z",
        "title": "Latent Diffusion Model without Variational Autoencoder",
        "summary": "Recent progress in diffusion-based visual generation has largely relied on\nlatent diffusion models with variational autoencoders (VAEs). While effective\nfor high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited\ntraining efficiency, slow inference, and poor transferability to broader vision\ntasks. These issues stem from a key limitation of VAE latent spaces: the lack\nof clear semantic separation and strong discriminative structure. Our analysis\nconfirms that these properties are crucial not only for perception and\nunderstanding tasks, but also for the stable and efficient training of latent\ndiffusion models. Motivated by this insight, we introduce SVG, a novel latent\ndiffusion model without variational autoencoders, which leverages\nself-supervised representations for visual generation. SVG constructs a feature\nspace with clear semantic discriminability by leveraging frozen DINO features,\nwhile a lightweight residual branch captures fine-grained details for\nhigh-fidelity reconstruction. Diffusion models are trained directly on this\nsemantically structured latent space to facilitate more efficient learning. As\na result, SVG enables accelerated diffusion training, supports few-step\nsampling, and improves generative quality. Experimental results further show\nthat SVG preserves the semantic and discriminative capabilities of the\nunderlying self-supervised representations, providing a principled pathway\ntoward task-general, high-quality visual representations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15301.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "662887715d246621f33d2ce6",
            "avatarUrl": "/avatars/3e0b1017b1e1bf284758ce840c174290.svg",
            "fullname": "Shi Minglei",
            "name": "MingleiShi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "662c559b322afcbae51b3c8b",
            "name": "KwaiVGI",
            "fullname": "Kuaishou Visual Generation and Interaction Center",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.15868",
            "authors": [
                {
                    "_id": "68f592b28589920bf4d320a6",
                    "name": "Shr-Ruei Tsai",
                    "hidden": false
                },
                {
                    "_id": "68f592b28589920bf4d320a7",
                    "name": "Wei-Cheng Chang",
                    "hidden": false
                },
                {
                    "_id": "68f592b28589920bf4d320a8",
                    "user": {
                        "_id": "655f1770f74fa124d1172ec1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655f1770f74fa124d1172ec1/bdYocZ1qN50CAfb2z2YLA.png",
                        "isPro": false,
                        "fullname": "Jie-Ying Lee",
                        "user": "jayinnn",
                        "type": "user"
                    },
                    "name": "Jie-Ying Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:11:21.151Z",
                    "hidden": false
                },
                {
                    "_id": "68f592b28589920bf4d320a9",
                    "name": "Chih-Hai Su",
                    "hidden": false
                },
                {
                    "_id": "68f592b28589920bf4d320aa",
                    "name": "Yu-Lun Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/AA0iCVZKQKkdsonKfSirM.mp4"
            ],
            "publishedAt": "2025-10-17T17:59:50.000Z",
            "submittedOnDailyAt": "2025-10-20T00:10:34.903Z",
            "title": "LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal",
            "submittedOnDailyBy": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
            },
            "summary": "Lens flare significantly degrades image quality, impacting critical computer\nvision tasks like object detection and autonomous driving. Recent Single Image\nFlare Removal (SIFR) methods perform poorly when off-frame light sources are\nincomplete or absent. We propose LightsOut, a diffusion-based outpainting\nframework tailored to enhance SIFR by reconstructing off-frame light sources.\nOur method leverages a multitask regression module and LoRA fine-tuned\ndiffusion model to ensure realistic and physically consistent outpainting\nresults. Comprehensive experiments demonstrate LightsOut consistently boosts\nthe performance of existing SIFR methods across challenging scenarios without\nadditional retraining, serving as a universally applicable plug-and-play\npreprocessing solution. Project page: https://ray-1026.github.io/lightsout/",
            "upvotes": 20,
            "discussionId": "68f592b38589920bf4d320ab",
            "projectPage": "https://ray-1026.github.io/lightsout/",
            "githubRepo": "https://github.com/Ray-1026/LightsOut-official",
            "ai_summary": "LightsOut enhances Single Image Flare Removal by reconstructing off-frame light sources using a diffusion-based outpainting framework, improving performance across challenging scenarios.",
            "ai_keywords": [
                "diffusion-based outpainting",
                "multitask regression module",
                "LoRA fine-tuned diffusion model",
                "Single Image Flare Removal",
                "off-frame light sources"
            ],
            "githubStars": 8,
            "organization": {
                "_id": "63e39e6499a032b1c950403d",
                "name": "NYCU",
                "fullname": "National Yang Ming Chiao Tung University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
            }
        },
        "publishedAt": "2025-10-17T13:59:50.000Z",
        "title": "LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal",
        "summary": "Lens flare significantly degrades image quality, impacting critical computer\nvision tasks like object detection and autonomous driving. Recent Single Image\nFlare Removal (SIFR) methods perform poorly when off-frame light sources are\nincomplete or absent. We propose LightsOut, a diffusion-based outpainting\nframework tailored to enhance SIFR by reconstructing off-frame light sources.\nOur method leverages a multitask regression module and LoRA fine-tuned\ndiffusion model to ensure realistic and physically consistent outpainting\nresults. Comprehensive experiments demonstrate LightsOut consistently boosts\nthe performance of existing SIFR methods across challenging scenarios without\nadditional retraining, serving as a universally applicable plug-and-play\npreprocessing solution. Project page: https://ray-1026.github.io/lightsout/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/AA0iCVZKQKkdsonKfSirM.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15868.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "fullname": "Yu-Lun Liu",
            "name": "yulunliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "63e39e6499a032b1c950403d",
            "name": "NYCU",
            "fullname": "National Yang Ming Chiao Tung University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.15842",
            "authors": [
                {
                    "_id": "68f595bf8589920bf4d320fa",
                    "name": "Yuhang Chen",
                    "hidden": false
                },
                {
                    "_id": "68f595bf8589920bf4d320fb",
                    "name": "Tianpeng Lv",
                    "hidden": false
                },
                {
                    "_id": "68f595bf8589920bf4d320fc",
                    "name": "Siyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f595bf8589920bf4d320fd",
                    "name": "Yixiang Yin",
                    "hidden": false
                },
                {
                    "_id": "68f595bf8589920bf4d320fe",
                    "name": "Yao Wan",
                    "hidden": false
                },
                {
                    "_id": "68f595bf8589920bf4d320ff",
                    "name": "Philip S. Yu",
                    "hidden": false
                },
                {
                    "_id": "68f595bf8589920bf4d32100",
                    "name": "Dongping Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T17:35:58.000Z",
            "submittedOnDailyAt": "2025-10-20T00:22:22.687Z",
            "title": "Paper2Web: Let's Make Your Paper Alive!",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Academic project websites can more effectively disseminate research when they\nclearly present core content and enable intuitive navigation and interaction.\nHowever, current approaches such as direct Large Language Model (LLM)\ngeneration, templates, or direct HTML conversion struggle to produce\nlayout-aware, interactive sites, and a comprehensive evaluation suite for this\ntask has been lacking. In this paper, we introduce Paper2Web, a benchmark\ndataset and multi-dimensional evaluation framework for assessing academic\nwebpage generation. It incorporates rule-based metrics like Connectivity,\nCompleteness and human-verified LLM-as-a-Judge (covering interactivity,\naesthetics, and informativeness), and PaperQuiz, which measures paper-level\nknowledge retention. We further present PWAgent, an autonomous pipeline that\nconverts scientific papers into interactive and multimedia-rich academic\nhomepages. The agent iteratively refines both content and layout through MCP\ntools that enhance emphasis, balance, and presentation quality. Our experiments\nshow that PWAgent consistently outperforms end-to-end baselines like\ntemplate-based webpages and arXiv/alphaXiv versions by a large margin while\nmaintaining low cost, achieving the Pareto-front in academic webpage\ngeneration.",
            "upvotes": 20,
            "discussionId": "68f595bf8589920bf4d32101",
            "projectPage": "https://francischen3.github.io/P2W_Website/",
            "ai_summary": "Paper2Web is a benchmark and evaluation framework for academic webpage generation, featuring PWAgent, an autonomous pipeline that enhances content and layout through MCP tools, outperforming end-to-end baselines.",
            "ai_keywords": [
                "Large Language Model (LLM)",
                "Paper2Web",
                "benchmark dataset",
                "evaluation framework",
                "Connectivity",
                "Completeness",
                "LLM-as-a-Judge",
                "PaperQuiz",
                "PWAgent",
                "MCP tools",
                "academic webpage generation",
                "template-based webpages",
                "arXiv/alphaXiv versions",
                "Pareto-front"
            ]
        },
        "publishedAt": "2025-10-17T13:35:58.000Z",
        "title": "Paper2Web: Let's Make Your Paper Alive!",
        "summary": "Academic project websites can more effectively disseminate research when they\nclearly present core content and enable intuitive navigation and interaction.\nHowever, current approaches such as direct Large Language Model (LLM)\ngeneration, templates, or direct HTML conversion struggle to produce\nlayout-aware, interactive sites, and a comprehensive evaluation suite for this\ntask has been lacking. In this paper, we introduce Paper2Web, a benchmark\ndataset and multi-dimensional evaluation framework for assessing academic\nwebpage generation. It incorporates rule-based metrics like Connectivity,\nCompleteness and human-verified LLM-as-a-Judge (covering interactivity,\naesthetics, and informativeness), and PaperQuiz, which measures paper-level\nknowledge retention. We further present PWAgent, an autonomous pipeline that\nconverts scientific papers into interactive and multimedia-rich academic\nhomepages. The agent iteratively refines both content and layout through MCP\ntools that enhance emphasis, balance, and presentation quality. Our experiments\nshow that PWAgent consistently outperforms end-to-end baselines like\ntemplate-based webpages and arXiv/alphaXiv versions by a large margin while\nmaintaining low cost, achieving the Pareto-front in academic webpage\ngeneration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15842.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 132
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.12838",
            "authors": [
                {
                    "_id": "68f5c0698589920bf4d321ab",
                    "name": "Qianben Chen",
                    "hidden": false
                },
                {
                    "_id": "68f5c0698589920bf4d321ac",
                    "name": "Jingyi Cao",
                    "hidden": false
                },
                {
                    "_id": "68f5c0698589920bf4d321ad",
                    "name": "Jiayu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f5c0698589920bf4d321ae",
                    "name": "Tianrui Qin",
                    "hidden": false
                },
                {
                    "_id": "68f5c0698589920bf4d321af",
                    "name": "Xiaowan Li",
                    "hidden": false
                },
                {
                    "_id": "68f5c0698589920bf4d321b0",
                    "user": {
                        "_id": "6578265ddea7e2122d02f6ba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578265ddea7e2122d02f6ba/Bh6JjoVF5ceLSjV7Z7nTk.jpeg",
                        "isPro": false,
                        "fullname": "king zhu",
                        "user": "kangz",
                        "type": "user"
                    },
                    "name": "King Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:09:56.809Z",
                    "hidden": false
                },
                {
                    "_id": "68f5c0698589920bf4d321b1",
                    "name": "Dingfeng Shi",
                    "hidden": false
                },
                {
                    "_id": "68f5c0698589920bf4d321b2",
                    "name": "He Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f5c0698589920bf4d321b3",
                    "name": "Minghao Liu",
                    "hidden": false
                },
                {
                    "_id": "68f5c0698589920bf4d321b4",
                    "name": "Xiaobo Liang",
                    "hidden": false
                },
                {
                    "_id": "68f5c0698589920bf4d321b5",
                    "name": "Xin Gui",
                    "hidden": false
                },
                {
                    "_id": "68f5c0698589920bf4d321b6",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f5c0698589920bf4d321b7",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "68f5c0698589920bf4d321b8",
                    "name": "Yuchen Eleanor Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f5c0698589920bf4d321b9",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T17:08:25.000Z",
            "submittedOnDailyAt": "2025-10-20T03:28:18.913Z",
            "title": "A^2FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "64be3a8b86e7fb5b8a7ec8a9",
                "avatarUrl": "/avatars/58280c82f643a5a8073623eff33fefb2.svg",
                "isPro": false,
                "fullname": "Chen",
                "user": "Qianben",
                "type": "user"
            },
            "summary": "Large language models split into two families: reasoning-centric LLMs, which\nstrengthen internal chain-of-thought reasoning but cannot invoke external\ntools, and agentic LLMs, which learn to interact with environments and leverage\ntools but often lag in deep reasoning. This divide arises from fundamentally\ndifferent training objectives, leading to mismatched strengths and inefficiency\non simple queries, where both families tend to overthink or over-call tools. In\nthis work, we present Adaptive Agent Foundation Model (A^2FM), a unified\nframework that follows a route-then-align principle: the model first learns\ntask-aware routing and then aligns mode-specific trajectories under a shared\nbackbone. To address the inefficiency gap, we introduce a third\nmode-instant-that handles simple queries directly, preventing unnecessary\nreasoning or tool calls while complementing the agentic and reasoning modes. To\njointly enhance accuracy and efficiency, we propose Adaptive Policy\nOptimization (APO), which enforces adaptive sampling across modes and applies a\ncost-regularized reward. On the 32B scale, A^2FM achieves 13.4% on\nBrowseComp, 70.4% on AIME25, and 16.7% on HLE, setting new SOTA among\ncomparable models and performing competitively with frontier LLMs across\nagentic, reasoning, and general benchmarks. Notably, the adaptive execution\nachieves a cost of pass of only $0.00487 per correct answer-cutting cost by\n45.2% relative to reasoning and 33.5% relative to agentic, thus delivering\nsubstantially higher cost efficiency while maintaining comparable accuracy.",
            "upvotes": 19,
            "discussionId": "68f5c06a8589920bf4d321ba",
            "githubRepo": "https://github.com/OPPO-PersonalAI/Adaptive_Agent_Foundation_Models",
            "ai_summary": "A unified framework, A$^2$FM, combines reasoning and agentic capabilities in large language models, improving efficiency and accuracy across benchmarks by adaptively routing queries and optimizing policy.",
            "ai_keywords": [
                "reasoning-centric LLMs",
                "agentic LLMs",
                "task-aware routing",
                "mode-specific trajectories",
                "shared backbone",
                "Adaptive Policy Optimization",
                "APO",
                "cost-regularized reward",
                "BrowseComp",
                "AIME25",
                "HLE",
                "SOTA",
                "adaptive execution"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "67177eecd0fad5b4ccc09461",
                "name": "OPPOer",
                "fullname": "OPPO",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66e24afddce93c7249b418c0/gQ-XFJehEyAH12zhbeR8Z.png"
            }
        },
        "publishedAt": "2025-10-13T13:08:25.000Z",
        "title": "A^2FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid\n  Reasoning",
        "summary": "Large language models split into two families: reasoning-centric LLMs, which\nstrengthen internal chain-of-thought reasoning but cannot invoke external\ntools, and agentic LLMs, which learn to interact with environments and leverage\ntools but often lag in deep reasoning. This divide arises from fundamentally\ndifferent training objectives, leading to mismatched strengths and inefficiency\non simple queries, where both families tend to overthink or over-call tools. In\nthis work, we present Adaptive Agent Foundation Model (A^2FM), a unified\nframework that follows a route-then-align principle: the model first learns\ntask-aware routing and then aligns mode-specific trajectories under a shared\nbackbone. To address the inefficiency gap, we introduce a third\nmode-instant-that handles simple queries directly, preventing unnecessary\nreasoning or tool calls while complementing the agentic and reasoning modes. To\njointly enhance accuracy and efficiency, we propose Adaptive Policy\nOptimization (APO), which enforces adaptive sampling across modes and applies a\ncost-regularized reward. On the 32B scale, A^2FM achieves 13.4% on\nBrowseComp, 70.4% on AIME25, and 16.7% on HLE, setting new SOTA among\ncomparable models and performing competitively with frontier LLMs across\nagentic, reasoning, and general benchmarks. Notably, the adaptive execution\nachieves a cost of pass of only $0.00487 per correct answer-cutting cost by\n45.2% relative to reasoning and 33.5% relative to agentic, thus delivering\nsubstantially higher cost efficiency while maintaining comparable accuracy.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12838.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64be3a8b86e7fb5b8a7ec8a9",
            "avatarUrl": "/avatars/58280c82f643a5a8073623eff33fefb2.svg",
            "fullname": "Chen",
            "name": "Qianben",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "67177eecd0fad5b4ccc09461",
            "name": "OPPOer",
            "fullname": "OPPO",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66e24afddce93c7249b418c0/gQ-XFJehEyAH12zhbeR8Z.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14265",
            "authors": [
                {
                    "_id": "68f592bd8589920bf4d320be",
                    "name": "Xukai Wang",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320bf",
                    "user": {
                        "_id": "679370562edd129a25abd63f",
                        "avatarUrl": "/avatars/976db18e97e85c0333b45b4468f35a6e.svg",
                        "isPro": false,
                        "fullname": "Xuanbo Liu",
                        "user": "liuxb725",
                        "type": "user"
                    },
                    "name": "Xuanbo Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T14:22:41.089Z",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320c0",
                    "name": "Mingrui Chen",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320c1",
                    "name": "Haitian Zhong",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320c2",
                    "name": "Xuanlin Yang",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320c3",
                    "user": {
                        "_id": "6671214c92412fd4640714eb",
                        "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
                        "isPro": false,
                        "fullname": "bohan zeng",
                        "user": "zbhpku",
                        "type": "user"
                    },
                    "name": "Bohan Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:11:14.634Z",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320c4",
                    "name": "Jinbo Hu",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320c5",
                    "name": "Hao Liang",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320c6",
                    "name": "Junbo Niu",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320c7",
                    "user": {
                        "_id": "67b4368a6ca0de5f8c266ad7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b4368a6ca0de5f8c266ad7/yifBjTFRI2ZjYOofYS4BQ.jpeg",
                        "isPro": false,
                        "fullname": "Xuchen Li",
                        "user": "Xuchen-Li",
                        "type": "user"
                    },
                    "name": "Xuchen Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:11:09.938Z",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320c8",
                    "name": "Ruitao Wu",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320c9",
                    "name": "Ruichuan An",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320ca",
                    "user": {
                        "_id": "673c7319d11b1c2e246ead9c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
                        "isPro": false,
                        "fullname": "Yang Shi",
                        "user": "DogNeverSleep",
                        "type": "user"
                    },
                    "name": "Yang Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:11:17.061Z",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320cb",
                    "name": "Liu Liu",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320cc",
                    "name": "Xu-Yao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320cd",
                    "name": "Qiang Liu",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320ce",
                    "name": "Zhouchen Lin",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320cf",
                    "name": "Wentao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f592bd8589920bf4d320d0",
                    "name": "Bin Dong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T03:30:56.000Z",
            "submittedOnDailyAt": "2025-10-20T00:11:27.651Z",
            "title": "MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning",
            "submittedOnDailyBy": {
                "_id": "6671214c92412fd4640714eb",
                "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
                "isPro": false,
                "fullname": "bohan zeng",
                "user": "zbhpku",
                "type": "user"
            },
            "summary": "With the advancement of powerful large-scale reasoning models, effectively\nevaluating the reasoning capabilities of these models has become increasingly\nimportant. However, existing benchmarks designed to assess the reasoning\nabilities of large models tend to be limited in scope and lack the flexibility\nto adapt their difficulty according to the evolving reasoning capacities of the\nmodels. To address this, we propose MorphoBench, a benchmark that incorporates\nmultidisciplinary questions to evaluate the reasoning capabilities of large\nmodels and can adjust and update question difficulty based on the reasoning\nabilities of advanced models. Specifically, we curate the benchmark by\nselecting and collecting complex reasoning questions from existing benchmarks\nand sources such as Olympiad-level competitions. Additionally, MorphoBench\nadaptively modifies the analytical challenge of questions by leveraging key\nstatements generated during the model's reasoning process. Furthermore, it\nincludes questions generated using simulation software, enabling dynamic\nadjustment of benchmark difficulty with minimal resource consumption. We have\ngathered over 1,300 test questions and iteratively adjusted the difficulty of\nMorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.\nMorphoBench enhances the comprehensiveness and validity of model reasoning\nevaluation, providing reliable guidance for improving both the reasoning\nabilities and scientific robustness of large models. The code has been released\nin https://github.com/OpenDCAI/MorphoBench.",
            "upvotes": 18,
            "discussionId": "68f592bd8589920bf4d320d1",
            "githubRepo": "https://github.com/OpenDCAI/MorphoBench",
            "ai_summary": "MorphoBench is a benchmark that evaluates large models' reasoning capabilities using multidisciplinary questions, adaptive difficulty, and simulation-generated questions.",
            "ai_keywords": [
                "MorphoBench",
                "reasoning capabilities",
                "multidisciplinary questions",
                "adaptive difficulty",
                "simulation-generated questions"
            ],
            "githubStars": 6,
            "organization": {
                "_id": "68896d3a716ee5bfb1428441",
                "name": "ZGCA",
                "fullname": "Zhongguancun Academy",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"
            }
        },
        "publishedAt": "2025-10-15T23:30:56.000Z",
        "title": "MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning",
        "summary": "With the advancement of powerful large-scale reasoning models, effectively\nevaluating the reasoning capabilities of these models has become increasingly\nimportant. However, existing benchmarks designed to assess the reasoning\nabilities of large models tend to be limited in scope and lack the flexibility\nto adapt their difficulty according to the evolving reasoning capacities of the\nmodels. To address this, we propose MorphoBench, a benchmark that incorporates\nmultidisciplinary questions to evaluate the reasoning capabilities of large\nmodels and can adjust and update question difficulty based on the reasoning\nabilities of advanced models. Specifically, we curate the benchmark by\nselecting and collecting complex reasoning questions from existing benchmarks\nand sources such as Olympiad-level competitions. Additionally, MorphoBench\nadaptively modifies the analytical challenge of questions by leveraging key\nstatements generated during the model's reasoning process. Furthermore, it\nincludes questions generated using simulation software, enabling dynamic\nadjustment of benchmark difficulty with minimal resource consumption. We have\ngathered over 1,300 test questions and iteratively adjusted the difficulty of\nMorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.\nMorphoBench enhances the comprehensiveness and validity of model reasoning\nevaluation, providing reliable guidance for improving both the reasoning\nabilities and scientific robustness of large models. The code has been released\nin https://github.com/OpenDCAI/MorphoBench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14265.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6671214c92412fd4640714eb",
            "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
            "fullname": "bohan zeng",
            "name": "zbhpku",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "68896d3a716ee5bfb1428441",
            "name": "ZGCA",
            "fullname": "Zhongguancun Academy",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.12766",
            "authors": [
                {
                    "_id": "68f5d9e88589920bf4d3220b",
                    "name": "Łukasz Borchmann",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T17:45:31.000Z",
            "submittedOnDailyAt": "2025-10-20T05:16:48.610Z",
            "title": "Language Models Model Language",
            "submittedOnDailyBy": {
                "_id": "600b381d3cc3b87db94bc0ce",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
                "isPro": false,
                "fullname": "Łukasz Borchmann",
                "user": "Borchmann",
                "type": "user"
            },
            "summary": "Linguistic commentary on LLMs, heavily influenced by the theoretical\nframeworks of de Saussure and Chomsky, is often speculative and unproductive.\nCritics challenge whether LLMs can legitimately model language, citing the need\nfor \"deep structure\" or \"grounding\" to achieve an idealized linguistic\n\"competence.\" We argue for a radical shift in perspective towards the\nempiricist principles of Witold Ma\\'nczak, a prominent general and historical\nlinguist. He defines language not as a \"system of signs\" or a \"computational\nsystem of the brain\" but as the totality of all that is said and written. Above\nall, he identifies frequency of use of particular language elements as\nlanguage's primary governing principle. Using his framework, we challenge prior\ncritiques of LLMs and provide a constructive guide for designing, evaluating,\nand interpreting language models.",
            "upvotes": 18,
            "discussionId": "68f5d9e88589920bf4d3220c",
            "ai_summary": "The paper advocates for an empiricist approach to evaluating language models, emphasizing frequency of use over traditional theoretical frameworks.",
            "ai_keywords": [
                ""
            ],
            "organization": {
                "_id": "62cece4aa3a23014aca72499",
                "name": "Snowflake",
                "fullname": "Snowflake",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64dc52cf858f8a41c12fc819/O9-MWzRjWzbNP_DQlMb-7.png"
            }
        },
        "publishedAt": "2025-10-14T13:45:31.000Z",
        "title": "Language Models Model Language",
        "summary": "Linguistic commentary on LLMs, heavily influenced by the theoretical\nframeworks of de Saussure and Chomsky, is often speculative and unproductive.\nCritics challenge whether LLMs can legitimately model language, citing the need\nfor \"deep structure\" or \"grounding\" to achieve an idealized linguistic\n\"competence.\" We argue for a radical shift in perspective towards the\nempiricist principles of Witold Ma\\'nczak, a prominent general and historical\nlinguist. He defines language not as a \"system of signs\" or a \"computational\nsystem of the brain\" but as the totality of all that is said and written. Above\nall, he identifies frequency of use of particular language elements as\nlanguage's primary governing principle. Using his framework, we challenge prior\ncritiques of LLMs and provide a constructive guide for designing, evaluating,\nand interpreting language models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12766.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "600b381d3cc3b87db94bc0ce",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
            "fullname": "Łukasz Borchmann",
            "name": "Borchmann",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "62cece4aa3a23014aca72499",
            "name": "Snowflake",
            "fullname": "Snowflake",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64dc52cf858f8a41c12fc819/O9-MWzRjWzbNP_DQlMb-7.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.15857",
            "authors": [
                {
                    "_id": "68f592b68589920bf4d320ad",
                    "name": "Jiuhai Chen",
                    "hidden": false
                },
                {
                    "_id": "68f592b68589920bf4d320ae",
                    "name": "Le Xue",
                    "hidden": false
                },
                {
                    "_id": "68f592b68589920bf4d320af",
                    "name": "Zhiyang Xu",
                    "hidden": false
                },
                {
                    "_id": "68f592b68589920bf4d320b0",
                    "name": "Xichen Pan",
                    "hidden": false
                },
                {
                    "_id": "68f592b68589920bf4d320b1",
                    "name": "Shusheng Yang",
                    "hidden": false
                },
                {
                    "_id": "68f592b68589920bf4d320b2",
                    "name": "Can Qin",
                    "hidden": false
                },
                {
                    "_id": "68f592b68589920bf4d320b3",
                    "name": "An Yan",
                    "hidden": false
                },
                {
                    "_id": "68f592b68589920bf4d320b4",
                    "name": "Honglu Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f592b68589920bf4d320b5",
                    "name": "Zeyuan Chen",
                    "hidden": false
                },
                {
                    "_id": "68f592b68589920bf4d320b6",
                    "name": "Lifu Huang",
                    "hidden": false
                },
                {
                    "_id": "68f592b68589920bf4d320b7",
                    "name": "Tianyi Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f592b68589920bf4d320b8",
                    "name": "Junnan Li",
                    "hidden": false
                },
                {
                    "_id": "68f592b68589920bf4d320b9",
                    "name": "Silvio Savarese",
                    "hidden": false
                },
                {
                    "_id": "68f592b68589920bf4d320ba",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "68f592b68589920bf4d320bb",
                    "name": "Ran Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T17:50:58.000Z",
            "submittedOnDailyAt": "2025-10-20T00:09:25.433Z",
            "title": "BLIP3o-NEXT: Next Frontier of Native Image Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3\nseries that advances the next frontier of native image generation. BLIP3o-NEXT\nunifies text-to-image generation and image editing within a single\narchitecture, demonstrating strong image generation and image editing\ncapabilities. In developing the state-of-the-art native image generation model,\nwe identify four key insights: (1) Most architectural choices yield comparable\nperformance; an architecture can be deemed effective provided it scales\nefficiently and supports fast inference; (2) The successful application of\nreinforcement learning can further push the frontier of native image\ngeneration; (3) Image editing still remains a challenging task, yet instruction\nfollowing and the consistency between generated and reference images can be\nsignificantly enhanced through post-training and data engine; (4) Data quality\nand scale continue to be decisive factors that determine the upper bound of\nmodel performance. Building upon these insights, BLIP3o-NEXT leverages an\nAutoregressive + Diffusion architecture in which an autoregressive model first\ngenerates discrete image tokens conditioned on multimodal inputs, whose hidden\nstates are then used as conditioning signals for a diffusion model to generate\nhigh-fidelity images. This architecture integrates the reasoning strength and\ninstruction following of autoregressive models with the fine-detail rendering\nability of diffusion models, achieving a new level of coherence and realism.\nExtensive evaluations of various text-to-image and image-editing benchmarks\nshow that BLIP3o-NEXT achieves superior performance over existing models.",
            "upvotes": 15,
            "discussionId": "68f592b68589920bf4d320bc",
            "projectPage": "https://jiuhaichen.github.io/BLIP3o-NEXT.github.io/",
            "ai_summary": "BLIP3o-NEXT, a unified text-to-image generation and image editing model, uses an Autoregressive + Diffusion architecture to achieve superior performance and realism.",
            "ai_keywords": [
                "Autoregressive",
                "Diffusion",
                "text-to-image generation",
                "image editing",
                "discrete image tokens",
                "multimodal inputs",
                "hidden states",
                "high-fidelity images",
                "coherence",
                "realism"
            ],
            "organization": {
                "_id": "64e2dbe32fbff6ed9cf0a678",
                "name": "Saleforce",
                "fullname": "Salesforce",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64e2dbce8367d2da8ba618f2/u1ulbo4ifz1X4DV_ZwyPg.png"
            }
        },
        "publishedAt": "2025-10-17T13:50:58.000Z",
        "title": "BLIP3o-NEXT: Next Frontier of Native Image Generation",
        "summary": "We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3\nseries that advances the next frontier of native image generation. BLIP3o-NEXT\nunifies text-to-image generation and image editing within a single\narchitecture, demonstrating strong image generation and image editing\ncapabilities. In developing the state-of-the-art native image generation model,\nwe identify four key insights: (1) Most architectural choices yield comparable\nperformance; an architecture can be deemed effective provided it scales\nefficiently and supports fast inference; (2) The successful application of\nreinforcement learning can further push the frontier of native image\ngeneration; (3) Image editing still remains a challenging task, yet instruction\nfollowing and the consistency between generated and reference images can be\nsignificantly enhanced through post-training and data engine; (4) Data quality\nand scale continue to be decisive factors that determine the upper bound of\nmodel performance. Building upon these insights, BLIP3o-NEXT leverages an\nAutoregressive + Diffusion architecture in which an autoregressive model first\ngenerates discrete image tokens conditioned on multimodal inputs, whose hidden\nstates are then used as conditioning signals for a diffusion model to generate\nhigh-fidelity images. This architecture integrates the reasoning strength and\ninstruction following of autoregressive models with the fine-detail rendering\nability of diffusion models, achieving a new level of coherence and realism.\nExtensive evaluations of various text-to-image and image-editing benchmarks\nshow that BLIP3o-NEXT achieves superior performance over existing models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15857.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 132
        },
        "organization": {
            "_id": "64e2dbe32fbff6ed9cf0a678",
            "name": "Saleforce",
            "fullname": "Salesforce",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64e2dbce8367d2da8ba618f2/u1ulbo4ifz1X4DV_ZwyPg.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.15831",
            "authors": [
                {
                    "_id": "68f5a4b78589920bf4d3213d",
                    "user": {
                        "_id": "63a9a0d13453852ef53c0b37",
                        "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
                        "isPro": false,
                        "fullname": "Do Xuan Long",
                        "user": "dxlong2000",
                        "type": "user"
                    },
                    "name": "Do Xuan Long",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:10:41.293Z",
                    "hidden": false
                },
                {
                    "_id": "68f5a4b78589920bf4d3213e",
                    "name": "Xingchen Wan",
                    "hidden": false
                },
                {
                    "_id": "68f5a4b78589920bf4d3213f",
                    "name": "Hootan Nakhost",
                    "hidden": false
                },
                {
                    "_id": "68f5a4b78589920bf4d32140",
                    "name": "Chen-Yu Lee",
                    "hidden": false
                },
                {
                    "_id": "68f5a4b78589920bf4d32141",
                    "name": "Tomas Pfister",
                    "hidden": false
                },
                {
                    "_id": "68f5a4b78589920bf4d32142",
                    "name": "Sercan Ö. Arık",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T17:12:08.000Z",
            "submittedOnDailyAt": "2025-10-20T01:26:57.483Z",
            "title": "VISTA: A Test-Time Self-Improving Video Generation Agent",
            "submittedOnDailyBy": {
                "_id": "63a9a0d13453852ef53c0b37",
                "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
                "isPro": false,
                "fullname": "Do Xuan Long",
                "user": "dxlong2000",
                "type": "user"
            },
            "summary": "Despite rapid advances in text-to-video synthesis, generated video quality\nremains critically dependent on precise user prompts. Existing test-time\noptimization methods, successful in other domains, struggle with the\nmulti-faceted nature of video. In this work, we introduce VISTA (Video\nIterative Self-improvemenT Agent), a novel multi-agent system that autonomously\nimproves video generation through refining prompts in an iterative loop. VISTA\nfirst decomposes a user idea into a structured temporal plan. After generation,\nthe best video is identified through a robust pairwise tournament. This winning\nvideo is then critiqued by a trio of specialized agents focusing on visual,\naudio, and contextual fidelity. Finally, a reasoning agent synthesizes this\nfeedback to introspectively rewrite and enhance the prompt for the next\ngeneration cycle. Experiments on single- and multi-scene video generation\nscenarios show that while prior methods yield inconsistent gains, VISTA\nconsistently improves video quality and alignment with user intent, achieving\nup to 60% pairwise win rate against state-of-the-art baselines. Human\nevaluators concur, preferring VISTA outputs in 66.4% of comparisons.",
            "upvotes": 12,
            "discussionId": "68f5a4b88589920bf4d32143",
            "ai_summary": "VISTA, a multi-agent system, iteratively refines user prompts to enhance video quality and alignment with user intent, outperforming existing methods.",
            "ai_keywords": [
                "text-to-video synthesis",
                "multi-agent system",
                "iterative loop",
                "structured temporal plan",
                "pairwise tournament",
                "visual fidelity",
                "audio fidelity",
                "contextual fidelity",
                "reasoning agent",
                "single-scene video generation",
                "multi-scene video generation"
            ]
        },
        "publishedAt": "2025-10-17T13:12:08.000Z",
        "title": "VISTA: A Test-Time Self-Improving Video Generation Agent",
        "summary": "Despite rapid advances in text-to-video synthesis, generated video quality\nremains critically dependent on precise user prompts. Existing test-time\noptimization methods, successful in other domains, struggle with the\nmulti-faceted nature of video. In this work, we introduce VISTA (Video\nIterative Self-improvemenT Agent), a novel multi-agent system that autonomously\nimproves video generation through refining prompts in an iterative loop. VISTA\nfirst decomposes a user idea into a structured temporal plan. After generation,\nthe best video is identified through a robust pairwise tournament. This winning\nvideo is then critiqued by a trio of specialized agents focusing on visual,\naudio, and contextual fidelity. Finally, a reasoning agent synthesizes this\nfeedback to introspectively rewrite and enhance the prompt for the next\ngeneration cycle. Experiments on single- and multi-scene video generation\nscenarios show that while prior methods yield inconsistent gains, VISTA\nconsistently improves video quality and alignment with user intent, achieving\nup to 60% pairwise win rate against state-of-the-art baselines. Human\nevaluators concur, preferring VISTA outputs in 66.4% of comparisons.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15831.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a9a0d13453852ef53c0b37",
            "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
            "fullname": "Do Xuan Long",
            "name": "dxlong2000",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.15280",
            "authors": [
                {
                    "_id": "68f59b3a8589920bf4d3210e",
                    "name": "Fan Liu",
                    "hidden": false
                },
                {
                    "_id": "68f59b3a8589920bf4d3210f",
                    "name": "Jindong Han",
                    "hidden": false
                },
                {
                    "_id": "68f59b3a8589920bf4d32110",
                    "name": "Tengfei Lyu",
                    "hidden": false
                },
                {
                    "_id": "68f59b3a8589920bf4d32111",
                    "name": "Weijia Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f59b3a8589920bf4d32112",
                    "name": "Zhe-Rui Yang",
                    "hidden": false
                },
                {
                    "_id": "68f59b3a8589920bf4d32113",
                    "name": "Lu Dai",
                    "hidden": false
                },
                {
                    "_id": "68f59b3a8589920bf4d32114",
                    "name": "Cancheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68f59b3a8589920bf4d32115",
                    "name": "Hao Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T03:40:26.000Z",
            "submittedOnDailyAt": "2025-10-20T03:31:20.249Z",
            "title": "Foundation Models for Scientific Discovery: From Paradigm Enhancement to\n  Paradigm Transition",
            "submittedOnDailyBy": {
                "_id": "65d6a6f7654f85ff0baf161f",
                "avatarUrl": "/avatars/4a46f8b0522fa572c122249a9d6526c4.svg",
                "isPro": false,
                "fullname": "Yansong NING",
                "user": "yasNing",
                "type": "user"
            },
            "summary": "Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping the\nlandscape of scientific research. Beyond accelerating tasks such as hypothesis\ngeneration, experimental design, and result interpretation, they prompt a more\nfundamental question: Are FMs merely enhancing existing scientific\nmethodologies, or are they redefining the way science is conducted? In this\npaper, we argue that FMs are catalyzing a transition toward a new scientific\nparadigm. We introduce a three-stage framework to describe this evolution: (1)\nMeta-Scientific Integration, where FMs enhance workflows within traditional\nparadigms; (2) Hybrid Human-AI Co-Creation, where FMs become active\ncollaborators in problem formulation, reasoning, and discovery; and (3)\nAutonomous Scientific Discovery, where FMs operate as independent agents\ncapable of generating new scientific knowledge with minimal human intervention.\nThrough this lens, we review current applications and emerging capabilities of\nFMs across existing scientific paradigms. We further identify risks and future\ndirections for FM-enabled scientific discovery. This position paper aims to\nsupport the scientific community in understanding the transformative role of\nFMs and to foster reflection on the future of scientific discovery. Our project\nis available at\nhttps://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery.",
            "upvotes": 12,
            "discussionId": "68f59b3b8589920bf4d32116",
            "ai_summary": "Foundation models are evolving scientific methodologies from enhancement to autonomous discovery, prompting a new paradigm in research.",
            "ai_keywords": [
                ""
            ],
            "organization": {
                "_id": "65c5cfad1f4d1a70a9556a0c",
                "name": "usail-hkust",
                "fullname": "usail-hkust",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c5ceede286dbda4ea1a18e/yXjCSTmbtnk9Q-e1-njfZ.png"
            }
        },
        "publishedAt": "2025-10-16T23:40:26.000Z",
        "title": "Foundation Models for Scientific Discovery: From Paradigm Enhancement to\n  Paradigm Transition",
        "summary": "Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping the\nlandscape of scientific research. Beyond accelerating tasks such as hypothesis\ngeneration, experimental design, and result interpretation, they prompt a more\nfundamental question: Are FMs merely enhancing existing scientific\nmethodologies, or are they redefining the way science is conducted? In this\npaper, we argue that FMs are catalyzing a transition toward a new scientific\nparadigm. We introduce a three-stage framework to describe this evolution: (1)\nMeta-Scientific Integration, where FMs enhance workflows within traditional\nparadigms; (2) Hybrid Human-AI Co-Creation, where FMs become active\ncollaborators in problem formulation, reasoning, and discovery; and (3)\nAutonomous Scientific Discovery, where FMs operate as independent agents\ncapable of generating new scientific knowledge with minimal human intervention.\nThrough this lens, we review current applications and emerging capabilities of\nFMs across existing scientific paradigms. We further identify risks and future\ndirections for FM-enabled scientific discovery. This position paper aims to\nsupport the scientific community in understanding the transformative role of\nFMs and to foster reflection on the future of scientific discovery. Our project\nis available at\nhttps://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15280.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65d6a6f7654f85ff0baf161f",
            "avatarUrl": "/avatars/4a46f8b0522fa572c122249a9d6526c4.svg",
            "fullname": "Yansong NING",
            "name": "yasNing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "65c5cfad1f4d1a70a9556a0c",
            "name": "usail-hkust",
            "fullname": "usail-hkust",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c5ceede286dbda4ea1a18e/yXjCSTmbtnk9Q-e1-njfZ.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.15624",
            "authors": [
                {
                    "_id": "68f5ab458589920bf4d32162",
                    "user": {
                        "_id": "677969a60a4d2007a86d669e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5CTwfmdOcrcK20rU5djcx.png",
                        "isPro": false,
                        "fullname": "Ed Li",
                        "user": "edli",
                        "type": "user"
                    },
                    "name": "Ed Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:10:29.350Z",
                    "hidden": false
                },
                {
                    "_id": "68f5ab458589920bf4d32163",
                    "name": "Junyu Ren",
                    "hidden": false
                },
                {
                    "_id": "68f5ab458589920bf4d32164",
                    "name": "Xintian Pan",
                    "hidden": false
                },
                {
                    "_id": "68f5ab458589920bf4d32165",
                    "name": "Cat Yan",
                    "hidden": false
                },
                {
                    "_id": "68f5ab458589920bf4d32166",
                    "name": "Chuanhao Li",
                    "hidden": false
                },
                {
                    "_id": "68f5ab458589920bf4d32167",
                    "name": "Dirk Bergemann",
                    "hidden": false
                },
                {
                    "_id": "68f5ab458589920bf4d32168",
                    "user": {
                        "_id": "646d769cda8e99940b71928e",
                        "avatarUrl": "/avatars/acee495a23362aa39b3d3e75c9afd967.svg",
                        "isPro": false,
                        "fullname": "Zhuoran Yang",
                        "user": "zhuoranyang",
                        "type": "user"
                    },
                    "name": "Zhuoran Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:10:24.204Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T13:13:32.000Z",
            "submittedOnDailyAt": "2025-10-20T04:35:41.526Z",
            "title": "Build Your Personalized Research Group: A Multiagent Framework for\n  Continual and Interactive Science Automation",
            "submittedOnDailyBy": {
                "_id": "646d769cda8e99940b71928e",
                "avatarUrl": "/avatars/acee495a23362aa39b3d3e75c9afd967.svg",
                "isPro": false,
                "fullname": "Zhuoran Yang",
                "user": "zhuoranyang",
                "type": "user"
            },
            "summary": "The automation of scientific discovery represents a critical milestone in\nArtificial Intelligence (AI) research. However, existing agentic systems for\nscience suffer from two fundamental limitations: rigid, pre-programmed\nworkflows that cannot adapt to intermediate findings, and inadequate context\nmanagement that hinders long-horizon research. We present\nfreephdlabor, an open-source multiagent framework featuring\nfully dynamic workflows determined by real-time agent reasoning and a\n\\textit{modular architecture} enabling seamless customization --\nusers can modify, add, or remove agents to address domain-specific\nrequirements. The framework provides comprehensive infrastructure including\nautomatic context compaction, workspace-based communication\nto prevent information degradation, memory persistence across\nsessions, and non-blocking human intervention mechanisms. These\nfeatures collectively transform automated research from isolated, single-run\nattempts into continual research programs that build systematically on\nprior explorations and incorporate human feedback. By providing both the\narchitectural principles and practical implementation for building customizable\nco-scientist systems, this work aims to facilitate broader adoption of\nautomated research across scientific domains, enabling practitioners to deploy\ninteractive multiagent systems that autonomously conduct end-to-end research --\nfrom ideation through experimentation to publication-ready manuscripts.",
            "upvotes": 10,
            "discussionId": "68f5ab458589920bf4d32169",
            "projectPage": "https://freephdlabor.github.io/",
            "githubRepo": "https://github.com/ltjed/freephdlabor",
            "ai_summary": "Freephdlabor is an open-source multiagent framework that supports dynamic workflows, modular architecture, and context management to enable continual and interactive automated scientific research.",
            "ai_keywords": [
                "fully dynamic workflows",
                "modular architecture",
                "automatic context compaction",
                "workspace-based communication",
                "memory persistence",
                "non-blocking human intervention",
                "continual research programs",
                "co-scientist systems"
            ],
            "githubStars": 154
        },
        "publishedAt": "2025-10-17T09:13:32.000Z",
        "title": "Build Your Personalized Research Group: A Multiagent Framework for\n  Continual and Interactive Science Automation",
        "summary": "The automation of scientific discovery represents a critical milestone in\nArtificial Intelligence (AI) research. However, existing agentic systems for\nscience suffer from two fundamental limitations: rigid, pre-programmed\nworkflows that cannot adapt to intermediate findings, and inadequate context\nmanagement that hinders long-horizon research. We present\nfreephdlabor, an open-source multiagent framework featuring\nfully dynamic workflows determined by real-time agent reasoning and a\n\\textit{modular architecture} enabling seamless customization --\nusers can modify, add, or remove agents to address domain-specific\nrequirements. The framework provides comprehensive infrastructure including\nautomatic context compaction, workspace-based communication\nto prevent information degradation, memory persistence across\nsessions, and non-blocking human intervention mechanisms. These\nfeatures collectively transform automated research from isolated, single-run\nattempts into continual research programs that build systematically on\nprior explorations and incorporate human feedback. By providing both the\narchitectural principles and practical implementation for building customizable\nco-scientist systems, this work aims to facilitate broader adoption of\nautomated research across scientific domains, enabling practitioners to deploy\ninteractive multiagent systems that autonomously conduct end-to-end research --\nfrom ideation through experimentation to publication-ready manuscripts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15624.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "646d769cda8e99940b71928e",
            "avatarUrl": "/avatars/acee495a23362aa39b3d3e75c9afd967.svg",
            "fullname": "Zhuoran Yang",
            "name": "zhuoranyang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.14438",
            "authors": [
                {
                    "_id": "68f1acf66e0bef323a68fda5",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "68f1acf66e0bef323a68fda6",
                    "name": "Ce Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1acf66e0bef323a68fda7",
                    "name": "Jun-Yu Ma",
                    "hidden": false
                },
                {
                    "_id": "68f1acf66e0bef323a68fda8",
                    "name": "Jianshu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1acf66e0bef323a68fda9",
                    "name": "Hongru Wang",
                    "hidden": false
                },
                {
                    "_id": "68f1acf66e0bef323a68fdaa",
                    "name": "Yi Chen",
                    "hidden": false
                },
                {
                    "_id": "68f1acf66e0bef323a68fdab",
                    "name": "Boyang Xue",
                    "hidden": false
                },
                {
                    "_id": "68f1acf66e0bef323a68fdac",
                    "user": {
                        "_id": "641129818573c51c0458b793",
                        "avatarUrl": "/avatars/d4bc67c160a07146cf41c614678aa36b.svg",
                        "isPro": false,
                        "fullname": "Tianqing Fang",
                        "user": "tqfang229",
                        "type": "user"
                    },
                    "name": "Tianqing Fang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T05:56:46.664Z",
                    "hidden": false
                },
                {
                    "_id": "68f1acf66e0bef323a68fdad",
                    "name": "Zhisong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1acf66e0bef323a68fdae",
                    "name": "Hongming Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f1acf66e0bef323a68fdaf",
                    "name": "Haitao Mi",
                    "hidden": false
                },
                {
                    "_id": "68f1acf66e0bef323a68fdb0",
                    "name": "Dong Yu",
                    "hidden": false
                },
                {
                    "_id": "68f1acf66e0bef323a68fdb1",
                    "name": "Kam-Fai Wong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T08:37:42.000Z",
            "submittedOnDailyAt": "2025-10-20T00:22:05.632Z",
            "title": "Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive\n  Online Exploration for Deep Research Agents",
            "submittedOnDailyBy": {
                "_id": "67298b338c66e235932ca088",
                "avatarUrl": "/avatars/912990b3b8b2e67663fc395c73287593.svg",
                "isPro": false,
                "fullname": "WANG Rui",
                "user": "Ray121381",
                "type": "user"
            },
            "summary": "Deep research web agents not only retrieve information from diverse sources\nsuch as web environments, files, and multimodal inputs, but more importantly,\nthey need to rigorously analyze and aggregate knowledge for insightful\nresearch. However, existing open-source deep research agents predominantly\nfocus on enhancing information-seeking capabilities of web agents to locate\nspecific information, while overlooking the essential need for information\naggregation, which would limit their ability to support in-depth research. We\npropose an Explore to Evolve paradigm to scalably construct verifiable training\ndata for web agents. Begins with proactive online exploration, an agent sources\ngrounded information by exploring the real web. Using the collected evidence,\nthe agent then self-evolves an aggregation program by selecting, composing, and\nrefining operations from 12 high-level logical types to synthesize a verifiable\nQA pair. This evolution from high-level guidance to concrete operations allowed\nus to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K\nwebsites and 11 domains. Based on an open-source agent framework, SmolAgents,\nwe collect supervised fine-tuning trajectories to develop a series of\nfoundation models, WebAggregator. WebAggregator-8B matches the performance of\nGPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text\nand closely approaches Claude-3.7-sonnet. Moreover, given the limited\navailability of benchmarks that evaluate web agents' information aggregation\nabilities, we construct a human-annotated evaluation split of WebAggregatorQA\nas a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves\n28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all\nreferences, they still struggle on WebAggregatorQA, highlighting the need to\nstrengthen the information aggregation capabilities of web agent foundations.",
            "upvotes": 10,
            "discussionId": "68f1acf66e0bef323a68fdb2",
            "githubRepo": "https://github.com/Tencent/WebAggregator",
            "ai_summary": "A new paradigm, Explore to Evolve, is proposed to enhance web agents' information aggregation by constructing a large dataset and developing foundation models that outperform existing models on a challenging benchmark.",
            "ai_keywords": [
                "Explore to Evolve",
                "proactive online exploration",
                "aggregation program",
                "high-level logical types",
                "verifiable QA pair",
                "WebAggregatorQA",
                "SmolAgents",
                "WebAggregator",
                "GPT-4.1",
                "GAIA-text",
                "Claude-3.7-sonnet",
                "human-annotated evaluation split"
            ],
            "githubStars": 33
        },
        "publishedAt": "2025-10-16T04:37:42.000Z",
        "title": "Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive\n  Online Exploration for Deep Research Agents",
        "summary": "Deep research web agents not only retrieve information from diverse sources\nsuch as web environments, files, and multimodal inputs, but more importantly,\nthey need to rigorously analyze and aggregate knowledge for insightful\nresearch. However, existing open-source deep research agents predominantly\nfocus on enhancing information-seeking capabilities of web agents to locate\nspecific information, while overlooking the essential need for information\naggregation, which would limit their ability to support in-depth research. We\npropose an Explore to Evolve paradigm to scalably construct verifiable training\ndata for web agents. Begins with proactive online exploration, an agent sources\ngrounded information by exploring the real web. Using the collected evidence,\nthe agent then self-evolves an aggregation program by selecting, composing, and\nrefining operations from 12 high-level logical types to synthesize a verifiable\nQA pair. This evolution from high-level guidance to concrete operations allowed\nus to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K\nwebsites and 11 domains. Based on an open-source agent framework, SmolAgents,\nwe collect supervised fine-tuning trajectories to develop a series of\nfoundation models, WebAggregator. WebAggregator-8B matches the performance of\nGPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text\nand closely approaches Claude-3.7-sonnet. Moreover, given the limited\navailability of benchmarks that evaluate web agents' information aggregation\nabilities, we construct a human-annotated evaluation split of WebAggregatorQA\nas a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves\n28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all\nreferences, they still struggle on WebAggregatorQA, highlighting the need to\nstrengthen the information aggregation capabilities of web agent foundations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14438.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67298b338c66e235932ca088",
            "avatarUrl": "/avatars/912990b3b8b2e67663fc395c73287593.svg",
            "fullname": "WANG Rui",
            "name": "Ray121381",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.15859",
            "authors": [
                {
                    "_id": "68f5bdca8589920bf4d3219b",
                    "name": "Pengkai Wang",
                    "hidden": false
                },
                {
                    "_id": "68f5bdca8589920bf4d3219c",
                    "name": "Qi Zuo",
                    "hidden": false
                },
                {
                    "_id": "68f5bdca8589920bf4d3219d",
                    "name": "Pengwei Liu",
                    "hidden": false
                },
                {
                    "_id": "68f5bdca8589920bf4d3219e",
                    "name": "Zhijie Sang",
                    "hidden": false
                },
                {
                    "_id": "68f5bdca8589920bf4d3219f",
                    "name": "Congkai Xie",
                    "hidden": false
                },
                {
                    "_id": "68f5bdca8589920bf4d321a0",
                    "name": "Hongxia Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T17:51:28.000Z",
            "submittedOnDailyAt": "2025-10-20T03:14:12.579Z",
            "title": "InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via\n  Rubric-Based Incremental Training",
            "submittedOnDailyBy": {
                "_id": "64245f2c089d5fae56b4549a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
                "isPro": false,
                "fullname": "Pengxiang Li",
                "user": "pengxiang",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have shown substantial advances through\nreinforcement learning (RL), particularly in domains where rewards can be\nprogrammatically verified, such as mathematics and code. In these areas, models\nbenefit from a well-defined operational base guided by explicit rule-based\nobjectives. However, this progress reveals a significant limitation: in\nopen-ended domains where rewards are ambiguous, subjective, or\ncontext-dependent, such as creative writing, scientific reasoning, and notably\nmedical consultation, robust reward functions are lacking, making these areas\nchallenging for current RL strategies. To bridge this gap, we introduce ORBIT,\nan open-ended rubric-based incremental training framework specifically designed\nfor high-stakes medical dialogue. ORBIT integrates syn- thetic dialogue\ngeneration with the dynamic creation of rubrics, employing these rubrics to\ndirect an incremental RL process. In particular, this approach does not depend\non external medical knowledge or manual rules, instead utilizing rubric-guided\nfeedback to shape learning. When implemented on the Qwen3-4B-Instruct model,\nour method can greatly enhance its performance on the HealthBench-Hard\nbenchmark from 7.0 to 27.2 using only 2k samples, thus achieving\nstate-of-the-art results for models of this scale. Our analysis confirms that\nrubric-driven RL fos-ters consistent performance gains across diverse\nconsultation scenarios, going beyond simple numerical improvements. These\nfindings underscore rubric-based feedback as a scalable strategy for advancing\nLLMs in intricate, open-ended tasks.",
            "upvotes": 9,
            "discussionId": "68f5bdcb8589920bf4d321a1",
            "ai_summary": "ORBIT, a rubric-based incremental training framework, enhances LLM performance in medical dialogue by using dynamic rubrics for RL, achieving state-of-the-art results on HealthBench-Hard.",
            "ai_keywords": [
                "reinforcement learning",
                "rubric-based incremental training",
                "synthetic dialogue generation",
                "rubric-guided feedback",
                "HealthBench-Hard",
                "Qwen3-4B-Instruct",
                "medical consultation",
                "open-ended tasks"
            ]
        },
        "publishedAt": "2025-10-17T13:51:28.000Z",
        "title": "InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via\n  Rubric-Based Incremental Training",
        "summary": "Large Language Models (LLMs) have shown substantial advances through\nreinforcement learning (RL), particularly in domains where rewards can be\nprogrammatically verified, such as mathematics and code. In these areas, models\nbenefit from a well-defined operational base guided by explicit rule-based\nobjectives. However, this progress reveals a significant limitation: in\nopen-ended domains where rewards are ambiguous, subjective, or\ncontext-dependent, such as creative writing, scientific reasoning, and notably\nmedical consultation, robust reward functions are lacking, making these areas\nchallenging for current RL strategies. To bridge this gap, we introduce ORBIT,\nan open-ended rubric-based incremental training framework specifically designed\nfor high-stakes medical dialogue. ORBIT integrates syn- thetic dialogue\ngeneration with the dynamic creation of rubrics, employing these rubrics to\ndirect an incremental RL process. In particular, this approach does not depend\non external medical knowledge or manual rules, instead utilizing rubric-guided\nfeedback to shape learning. When implemented on the Qwen3-4B-Instruct model,\nour method can greatly enhance its performance on the HealthBench-Hard\nbenchmark from 7.0 to 27.2 using only 2k samples, thus achieving\nstate-of-the-art results for models of this scale. Our analysis confirms that\nrubric-driven RL fos-ters consistent performance gains across diverse\nconsultation scenarios, going beyond simple numerical improvements. These\nfindings underscore rubric-based feedback as a scalable strategy for advancing\nLLMs in intricate, open-ended tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15859.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64245f2c089d5fae56b4549a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
            "fullname": "Pengxiang Li",
            "name": "pengxiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.15564",
            "authors": [
                {
                    "_id": "68f593d08589920bf4d320de",
                    "name": "Xiaoming Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f593d08589920bf4d320df",
                    "name": "Xu Huang",
                    "hidden": false
                },
                {
                    "_id": "68f593d08589920bf4d320e0",
                    "name": "Qinghongbing Xie",
                    "hidden": false
                },
                {
                    "_id": "68f593d08589920bf4d320e1",
                    "name": "Zhi Deng",
                    "hidden": false
                },
                {
                    "_id": "68f593d08589920bf4d320e2",
                    "name": "Junsheng Yu",
                    "hidden": false
                },
                {
                    "_id": "68f593d08589920bf4d320e3",
                    "name": "Yirui Guan",
                    "hidden": false
                },
                {
                    "_id": "68f593d08589920bf4d320e4",
                    "name": "Zhongyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68f593d08589920bf4d320e5",
                    "name": "Lin Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f593d08589920bf4d320e6",
                    "name": "Qijun Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f593d08589920bf4d320e7",
                    "name": "Ligang Liu",
                    "hidden": false
                },
                {
                    "_id": "68f593d08589920bf4d320e8",
                    "name": "Long Zeng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T11:48:08.000Z",
            "submittedOnDailyAt": "2025-10-20T00:13:55.960Z",
            "title": "Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Generating artistic and coherent 3D scene layouts is crucial in digital\ncontent creation. Traditional optimization-based methods are often constrained\nby cumbersome manual rules, while deep generative models face challenges in\nproducing content with richness and diversity. Furthermore, approaches that\nutilize large language models frequently lack robustness and fail to accurately\ncapture complex spatial relationships. To address these challenges, this paper\npresents a novel vision-guided 3D layout generation system. We first construct\na high-quality asset library containing 2,037 scene assets and 147 3D scene\nlayouts. Subsequently, we employ an image generation model to expand prompt\nrepresentations into images, fine-tuning it to align with our asset library. We\nthen develop a robust image parsing module to recover the 3D layout of scenes\nbased on visual semantics and geometric information. Finally, we optimize the\nscene layout using scene graphs and overall visual semantics to ensure logical\ncoherence and alignment with the images. Extensive user testing demonstrates\nthat our algorithm significantly outperforms existing methods in terms of\nlayout richness and quality. The code and dataset will be available at\nhttps://github.com/HiHiAllen/Imaginarium.",
            "upvotes": 8,
            "discussionId": "68f593d08589920bf4d320e9",
            "ai_summary": "A vision-guided 3D layout generation system uses an image generation model and scene graphs to produce rich and coherent 3D scenes from prompts.",
            "ai_keywords": [
                "image generation model",
                "image parsing module",
                "scene graphs",
                "3D layout generation",
                "visual semantics",
                "geometric information",
                "scene graphs",
                "logical coherence"
            ]
        },
        "publishedAt": "2025-10-17T07:48:08.000Z",
        "title": "Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation",
        "summary": "Generating artistic and coherent 3D scene layouts is crucial in digital\ncontent creation. Traditional optimization-based methods are often constrained\nby cumbersome manual rules, while deep generative models face challenges in\nproducing content with richness and diversity. Furthermore, approaches that\nutilize large language models frequently lack robustness and fail to accurately\ncapture complex spatial relationships. To address these challenges, this paper\npresents a novel vision-guided 3D layout generation system. We first construct\na high-quality asset library containing 2,037 scene assets and 147 3D scene\nlayouts. Subsequently, we employ an image generation model to expand prompt\nrepresentations into images, fine-tuning it to align with our asset library. We\nthen develop a robust image parsing module to recover the 3D layout of scenes\nbased on visual semantics and geometric information. Finally, we optimize the\nscene layout using scene graphs and overall visual semantics to ensure logical\ncoherence and alignment with the images. Extensive user testing demonstrates\nthat our algorithm significantly outperforms existing methods in terms of\nlayout richness and quality. The code and dataset will be available at\nhttps://github.com/HiHiAllen/Imaginarium.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15564.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 132
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.15110",
            "authors": [
                {
                    "_id": "68f5a8b68589920bf4d32145",
                    "name": "Shih-Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "68f5a8b68589920bf4d32146",
                    "name": "Xin Dong",
                    "hidden": false
                },
                {
                    "_id": "68f5a8b68589920bf4d32147",
                    "name": "Ximing Lu",
                    "hidden": false
                },
                {
                    "_id": "68f5a8b68589920bf4d32148",
                    "name": "Shizhe Diao",
                    "hidden": false
                },
                {
                    "_id": "68f5a8b68589920bf4d32149",
                    "name": "Mingjie Liu",
                    "hidden": false
                },
                {
                    "_id": "68f5a8b68589920bf4d3214a",
                    "user": {
                        "_id": "64ae22dd1aee69ece065cdcd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                        "isPro": false,
                        "fullname": "Min-Hung Chen",
                        "user": "cmhungsteve",
                        "type": "user"
                    },
                    "name": "Min-Hung Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:10:37.892Z",
                    "hidden": false
                },
                {
                    "_id": "68f5a8b68589920bf4d3214b",
                    "name": "Hongxu Yin",
                    "hidden": false
                },
                {
                    "_id": "68f5a8b68589920bf4d3214c",
                    "name": "Yu-Chiang Frank Wang",
                    "hidden": false
                },
                {
                    "_id": "68f5a8b68589920bf4d3214d",
                    "name": "Kwang-Ting Cheng",
                    "hidden": false
                },
                {
                    "_id": "68f5a8b68589920bf4d3214e",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "68f5a8b68589920bf4d3214f",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "68f5a8b68589920bf4d32150",
                    "name": "Pavlo Molchanov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T20:05:57.000Z",
            "submittedOnDailyAt": "2025-10-20T01:44:52.713Z",
            "title": "DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per\n  Token via Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
            },
            "summary": "Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve\nstrong performance via extended chains of thought but often generate\nunnecessarily long outputs. Maximizing intelligence per token--accuracy\nrelative to response length--remains an open problem. We revisit reinforcement\nlearning (RL) with the simplest length penalty--truncation--and show that\naccuracy degradation arises not from the lack of sophisticated penalties but\nfrom inadequate RL optimization. We identify three key challenges: (i) large\nbias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward\nsignal. We address them with Doing Length pEnalty Right (DLER), a training\nrecipe combining batch-wise reward normalization, higher clipping, dynamic\nsampling, and a simple truncation length penalty. DLER achieves\nstate-of-the-art accuracy--efficiency trade-offs, cutting output length by over\n70 percent while surpassing all previous baseline accuracy. It also improves\ntest-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple\nconcise responses in parallel with 28 percent higher accuracy and lower\nlatency. We further introduce Difficulty-Aware DLER, which adaptively tightens\ntruncation on easier questions for additional efficiency gains. We also propose\nan update-selective merging method that preserves baseline accuracy while\nretaining the concise reasoning ability of the DLER model, which is useful for\nscenarios where RL training data is scarce.",
            "upvotes": 8,
            "discussionId": "68f5a8b68589920bf4d32151",
            "ai_summary": "DLER, a reinforcement learning training recipe, improves the accuracy-efficiency trade-off in reasoning language models by addressing challenges in advantage estimation, entropy collapse, and sparse reward signals, leading to shorter outputs and better test-time scaling.",
            "ai_keywords": [
                "reinforcement learning",
                "advantage estimation",
                "entropy collapse",
                "sparse reward signal",
                "Doing Length pEnalty Right",
                "batch-wise reward normalization",
                "dynamic sampling",
                "Difficulty-Aware DLER",
                "update-selective merging"
            ],
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-10-16T16:05:57.000Z",
        "title": "DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per\n  Token via Reinforcement Learning",
        "summary": "Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve\nstrong performance via extended chains of thought but often generate\nunnecessarily long outputs. Maximizing intelligence per token--accuracy\nrelative to response length--remains an open problem. We revisit reinforcement\nlearning (RL) with the simplest length penalty--truncation--and show that\naccuracy degradation arises not from the lack of sophisticated penalties but\nfrom inadequate RL optimization. We identify three key challenges: (i) large\nbias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward\nsignal. We address them with Doing Length pEnalty Right (DLER), a training\nrecipe combining batch-wise reward normalization, higher clipping, dynamic\nsampling, and a simple truncation length penalty. DLER achieves\nstate-of-the-art accuracy--efficiency trade-offs, cutting output length by over\n70 percent while surpassing all previous baseline accuracy. It also improves\ntest-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple\nconcise responses in parallel with 28 percent higher accuracy and lower\nlatency. We further introduce Difficulty-Aware DLER, which adaptively tightens\ntruncation on easier questions for additional efficiency gains. We also propose\nan update-selective merging method that preserves baseline accuracy while\nretaining the concise reasoning ability of the DLER model, which is useful for\nscenarios where RL training data is scarce.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15110.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ae22dd1aee69ece065cdcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
            "fullname": "Min-Hung Chen",
            "name": "cmhungsteve",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.15232",
            "authors": [
                {
                    "_id": "68f59eb88589920bf4d3211b",
                    "user": {
                        "_id": "67492b9e347c3876f22b3684",
                        "avatarUrl": "/avatars/0d80d23f7b10ce8bac689f6e8317a014.svg",
                        "isPro": false,
                        "fullname": "Tiansheng Hu",
                        "user": "HughieHu",
                        "type": "user"
                    },
                    "name": "Tiansheng Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:11:04.076Z",
                    "hidden": false
                },
                {
                    "_id": "68f59eb88589920bf4d3211c",
                    "user": {
                        "_id": "66e83ec5deb449d8d856e78d",
                        "avatarUrl": "/avatars/c5e56be65fcacb3192ce10ba6d8f48e2.svg",
                        "isPro": false,
                        "fullname": "Tongyan Hu",
                        "user": "entropyhu",
                        "type": "user"
                    },
                    "name": "Tongyan Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:11:00.964Z",
                    "hidden": false
                },
                {
                    "_id": "68f59eb88589920bf4d3211d",
                    "name": "Liuyang Bai",
                    "hidden": false
                },
                {
                    "_id": "68f59eb88589920bf4d3211e",
                    "name": "Yilun Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f59eb88589920bf4d3211f",
                    "name": "Arman Cohan",
                    "hidden": false
                },
                {
                    "_id": "68f59eb88589920bf4d32120",
                    "name": "Chen Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T01:45:49.000Z",
            "submittedOnDailyAt": "2025-10-20T01:02:05.029Z",
            "title": "FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in\n  Finance Domain",
            "submittedOnDailyBy": {
                "_id": "67492b9e347c3876f22b3684",
                "avatarUrl": "/avatars/0d80d23f7b10ce8bac689f6e8317a014.svg",
                "isPro": false,
                "fullname": "Tiansheng Hu",
                "user": "HughieHu",
                "type": "user"
            },
            "summary": "Recent LLMs have demonstrated promising ability in solving finance related\nproblems. However, applying LLMs in real-world finance application remains\nchallenging due to its high risk and high stakes property. This paper\nintroduces FinTrust, a comprehensive benchmark specifically designed for\nevaluating the trustworthiness of LLMs in finance applications. Our benchmark\nfocuses on a wide range of alignment issues based on practical context and\nfeatures fine-grained tasks for each dimension of trustworthiness evaluation.\nWe assess eleven LLMs on FinTrust and find that proprietary models like o4-mini\noutperforms in most tasks such as safety while open-source models like\nDeepSeek-V3 have advantage in specific areas like industry-level fairness. For\nchallenging task like fiduciary alignment and disclosure, all LLMs fall short,\nshowing a significant gap in legal awareness. We believe that FinTrust can be a\nvaluable benchmark for LLMs' trustworthiness evaluation in finance domain.",
            "upvotes": 4,
            "discussionId": "68f59eb98589920bf4d32121",
            "githubRepo": "https://github.com/HughieHu/FinTrust/",
            "ai_summary": "FinTrust is a benchmark designed to evaluate the trustworthiness of LLMs in finance applications, focusing on alignment issues and revealing gaps in legal awareness.",
            "ai_keywords": [
                "LLMs",
                "FinTrust",
                "trustworthiness",
                "alignment issues",
                "safety",
                "industry-level fairness",
                "fiduciary alignment",
                "disclosure",
                "legal awareness"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-10-16T21:45:49.000Z",
        "title": "FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in\n  Finance Domain",
        "summary": "Recent LLMs have demonstrated promising ability in solving finance related\nproblems. However, applying LLMs in real-world finance application remains\nchallenging due to its high risk and high stakes property. This paper\nintroduces FinTrust, a comprehensive benchmark specifically designed for\nevaluating the trustworthiness of LLMs in finance applications. Our benchmark\nfocuses on a wide range of alignment issues based on practical context and\nfeatures fine-grained tasks for each dimension of trustworthiness evaluation.\nWe assess eleven LLMs on FinTrust and find that proprietary models like o4-mini\noutperforms in most tasks such as safety while open-source models like\nDeepSeek-V3 have advantage in specific areas like industry-level fairness. For\nchallenging task like fiduciary alignment and disclosure, all LLMs fall short,\nshowing a significant gap in legal awareness. We believe that FinTrust can be a\nvaluable benchmark for LLMs' trustworthiness evaluation in finance domain.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15232.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67492b9e347c3876f22b3684",
            "avatarUrl": "/avatars/0d80d23f7b10ce8bac689f6e8317a014.svg",
            "fullname": "Tiansheng Hu",
            "name": "HughieHu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.11328",
            "authors": [
                {
                    "_id": "68f6852a24c4489363111607",
                    "name": "Chenxi Wang",
                    "hidden": false
                },
                {
                    "_id": "68f6852a24c4489363111608",
                    "name": "Yixuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f6852a24c4489363111609",
                    "name": "Ruiji Yu",
                    "hidden": false
                },
                {
                    "_id": "68f6852a24c448936311160a",
                    "name": "Yufei Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f6852a24c448936311160b",
                    "name": "Lang Gao",
                    "hidden": false
                },
                {
                    "_id": "68f6852a24c448936311160c",
                    "name": "Zirui Song",
                    "hidden": false
                },
                {
                    "_id": "68f6852a24c448936311160d",
                    "name": "Zixiang Xu",
                    "hidden": false
                },
                {
                    "_id": "68f6852a24c448936311160e",
                    "name": "Gus Xia",
                    "hidden": false
                },
                {
                    "_id": "68f6852a24c448936311160f",
                    "name": "Huishuai Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f6852a24c4489363111610",
                    "name": "Dongyan Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f6852a24c4489363111611",
                    "name": "Xiuying Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/679bc0ec7f3c28bf968321c8/K9w8HFtTSp09GmZMiwgM5.png",
                "https://cdn-uploads.huggingface.co/production/uploads/679bc0ec7f3c28bf968321c8/z2UrrNdKRvYrRAbC47YAp.png",
                "https://cdn-uploads.huggingface.co/production/uploads/679bc0ec7f3c28bf968321c8/DQ0l1H3x8DVgzfrpgJ2d-.png"
            ],
            "publishedAt": "2025-10-13T12:24:24.000Z",
            "submittedOnDailyAt": "2025-10-20T17:55:28.531Z",
            "title": "Do LLMs \"Feel\"? Emotion Circuits Discovery and Control",
            "submittedOnDailyBy": {
                "_id": "679bc0ec7f3c28bf968321c8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/679bc0ec7f3c28bf968321c8/uFqCcPbVjDISww46Jbovf.jpeg",
                "isPro": false,
                "fullname": "Chenxi Wang",
                "user": "Aurora-cx",
                "type": "user"
            },
            "summary": "As the demand for emotional intelligence in large language models (LLMs)\ngrows, a key challenge lies in understanding the internal mechanisms that give\nrise to emotional expression and in controlling emotions in generated text.\nThis study addresses three core questions: (1) Do LLMs contain context-agnostic\nmechanisms shaping emotional expression? (2) What form do these mechanisms\ntake? (3) Can they be harnessed for universal emotion control? We first\nconstruct a controlled dataset, SEV (Scenario-Event with Valence), to elicit\ncomparable internal states across emotions. Subsequently, we extract\ncontext-agnostic emotion directions that reveal consistent, cross-context\nencoding of emotion (Q1). We identify neurons and attention heads that locally\nimplement emotional computation through analytical decomposition and causal\nanalysis, and validate their causal roles via ablation and enhancement\ninterventions. Next, we quantify each sublayer's causal influence on the\nmodel's final emotion representation and integrate the identified local\ncomponents into coherent global emotion circuits that drive emotional\nexpression (Q2). Directly modulating these circuits achieves 99.65%\nemotion-expression accuracy on the test set, surpassing prompting- and\nsteering-based methods (Q3). To our knowledge, this is the first systematic\nstudy to uncover and validate emotion circuits in LLMs, offering new insights\ninto interpretability and controllable emotional intelligence.",
            "upvotes": 4,
            "discussionId": "68f6852a24c4489363111612",
            "githubRepo": "https://github.com/Aurora-cx/EmotionCircuits-LLM",
            "ai_summary": "This study uncovers and validates emotion circuits in large language models, enabling high-accuracy emotion control in generated text.",
            "ai_keywords": [
                "LLMs",
                "emotional expression",
                "context-agnostic mechanisms",
                "SEV dataset",
                "emotion directions",
                "neurons",
                "attention heads",
                "analytical decomposition",
                "causal analysis",
                "ablation",
                "enhancement interventions",
                "emotion representation",
                "global emotion circuits",
                "emotion-expression accuracy",
                "prompting-based methods",
                "steering-based methods",
                "interpretability",
                "controllable emotional intelligence"
            ],
            "githubStars": 5,
            "organization": {
                "_id": "61fb9e24dc607a42af5f193f",
                "name": "MBZUAI",
                "fullname": "Mohamed Bin Zayed University of Artificial Intelligence",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"
            }
        },
        "publishedAt": "2025-10-13T08:24:24.000Z",
        "title": "Do LLMs \"Feel\"? Emotion Circuits Discovery and Control",
        "summary": "As the demand for emotional intelligence in large language models (LLMs)\ngrows, a key challenge lies in understanding the internal mechanisms that give\nrise to emotional expression and in controlling emotions in generated text.\nThis study addresses three core questions: (1) Do LLMs contain context-agnostic\nmechanisms shaping emotional expression? (2) What form do these mechanisms\ntake? (3) Can they be harnessed for universal emotion control? We first\nconstruct a controlled dataset, SEV (Scenario-Event with Valence), to elicit\ncomparable internal states across emotions. Subsequently, we extract\ncontext-agnostic emotion directions that reveal consistent, cross-context\nencoding of emotion (Q1). We identify neurons and attention heads that locally\nimplement emotional computation through analytical decomposition and causal\nanalysis, and validate their causal roles via ablation and enhancement\ninterventions. Next, we quantify each sublayer's causal influence on the\nmodel's final emotion representation and integrate the identified local\ncomponents into coherent global emotion circuits that drive emotional\nexpression (Q2). Directly modulating these circuits achieves 99.65%\nemotion-expression accuracy on the test set, surpassing prompting- and\nsteering-based methods (Q3). To our knowledge, this is the first systematic\nstudy to uncover and validate emotion circuits in LLMs, offering new insights\ninto interpretability and controllable emotional intelligence.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/679bc0ec7f3c28bf968321c8/K9w8HFtTSp09GmZMiwgM5.png",
            "https://cdn-uploads.huggingface.co/production/uploads/679bc0ec7f3c28bf968321c8/z2UrrNdKRvYrRAbC47YAp.png",
            "https://cdn-uploads.huggingface.co/production/uploads/679bc0ec7f3c28bf968321c8/DQ0l1H3x8DVgzfrpgJ2d-.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11328.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "679bc0ec7f3c28bf968321c8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/679bc0ec7f3c28bf968321c8/uFqCcPbVjDISww46Jbovf.jpeg",
            "fullname": "Chenxi Wang",
            "name": "Aurora-cx",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "61fb9e24dc607a42af5f193f",
            "name": "MBZUAI",
            "fullname": "Mohamed Bin Zayed University of Artificial Intelligence",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14853",
            "authors": [
                {
                    "_id": "68f5efbd8589920bf4d32247",
                    "name": "Guinan Su",
                    "hidden": false
                },
                {
                    "_id": "68f5efbd8589920bf4d32248",
                    "name": "Yanwu Yang",
                    "hidden": false
                },
                {
                    "_id": "68f5efbd8589920bf4d32249",
                    "name": "Li Shen",
                    "hidden": false
                },
                {
                    "_id": "68f5efbd8589920bf4d3224a",
                    "name": "Lu Yin",
                    "hidden": false
                },
                {
                    "_id": "68f5efbd8589920bf4d3224b",
                    "name": "Shiwei Liu",
                    "hidden": false
                },
                {
                    "_id": "68f5efbd8589920bf4d3224c",
                    "name": "Jonas Geiping",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T16:24:36.000Z",
            "submittedOnDailyAt": "2025-10-20T06:48:39.386Z",
            "title": "Rewiring Experts on the Fly:Continuous Rerouting for Better Online\n  Adaptation in Mixture-of-Expert models",
            "submittedOnDailyBy": {
                "_id": "630ed6cfc9af0163b96a71aa",
                "avatarUrl": "/avatars/bf984bb1c2f68336bcf042c6fc7e0d9d.svg",
                "isPro": false,
                "fullname": "Guinan-Su",
                "user": "guinansu",
                "type": "user"
            },
            "summary": "Mixture-of-Experts (MoE) models achieve efficient scaling through sparse\nexpert activation, but often suffer from suboptimal routing decisions due to\ndistribution shifts in deployment. While existing test-time adaptation methods\ncould potentially address these issues, they primarily focus on dense models\nand require access to external data, limiting their practical applicability to\nMoE architectures. However, we find that, instead of relying on reference data,\nwe can optimize MoE expert selection on-the-fly based only on input context. As\nsuch, we propose a data-free, online test-time framework that\ncontinuously adapts MoE routing decisions during text generation without\nexternal supervision or data. Our method cycles between two phases: During the\nprefill stage, and later in regular intervals, we optimize the routing\ndecisions of the model using self-supervision based on the already generated\nsequence. Then, we generate text as normal, maintaining the modified router\nuntil the next adaption. We implement this through lightweight additive vectors\nthat only update router logits in selected layers, maintaining computational\nefficiency while preventing over-adaptation. The experimental results show\nconsistent performance gains on challenging reasoning tasks while maintaining\nrobustness to context shifts. For example, our method achieves a 5.5\\%\nimprovement on HumanEval with OLMoE. Furthermore, owing to its plug-and-play\nproperty, our method naturally complements existing test-time scaling\ntechniques, e.g., achieving 6\\% average gains when incorporated with\nself-consistency on DeepSeek-V2-Lite.",
            "upvotes": 3,
            "discussionId": "68f5efbd8589920bf4d3224d",
            "ai_summary": "A data-free, online test-time framework optimizes MoE routing decisions during text generation using self-supervision, improving performance and robustness without external data.",
            "ai_keywords": [
                "Mixture-of-Experts",
                "MoE",
                "sparse expert activation",
                "test-time adaptation",
                "self-supervision",
                "router logits",
                "prefill stage",
                "context shifts",
                "HumanEval",
                "OLMoE",
                "self-consistency",
                "DeepSeek-V2-Lite"
            ]
        },
        "publishedAt": "2025-10-16T12:24:36.000Z",
        "title": "Rewiring Experts on the Fly:Continuous Rerouting for Better Online\n  Adaptation in Mixture-of-Expert models",
        "summary": "Mixture-of-Experts (MoE) models achieve efficient scaling through sparse\nexpert activation, but often suffer from suboptimal routing decisions due to\ndistribution shifts in deployment. While existing test-time adaptation methods\ncould potentially address these issues, they primarily focus on dense models\nand require access to external data, limiting their practical applicability to\nMoE architectures. However, we find that, instead of relying on reference data,\nwe can optimize MoE expert selection on-the-fly based only on input context. As\nsuch, we propose a data-free, online test-time framework that\ncontinuously adapts MoE routing decisions during text generation without\nexternal supervision or data. Our method cycles between two phases: During the\nprefill stage, and later in regular intervals, we optimize the routing\ndecisions of the model using self-supervision based on the already generated\nsequence. Then, we generate text as normal, maintaining the modified router\nuntil the next adaption. We implement this through lightweight additive vectors\nthat only update router logits in selected layers, maintaining computational\nefficiency while preventing over-adaptation. The experimental results show\nconsistent performance gains on challenging reasoning tasks while maintaining\nrobustness to context shifts. For example, our method achieves a 5.5\\%\nimprovement on HumanEval with OLMoE. Furthermore, owing to its plug-and-play\nproperty, our method naturally complements existing test-time scaling\ntechniques, e.g., achieving 6\\% average gains when incorporated with\nself-consistency on DeepSeek-V2-Lite.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14853.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630ed6cfc9af0163b96a71aa",
            "avatarUrl": "/avatars/bf984bb1c2f68336bcf042c6fc7e0d9d.svg",
            "fullname": "Guinan-Su",
            "name": "guinansu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.15262",
            "authors": [
                {
                    "_id": "68f5aac18589920bf4d32153",
                    "name": "Zhiyuan Fan",
                    "hidden": false
                },
                {
                    "_id": "68f5aac18589920bf4d32154",
                    "user": {
                        "_id": "653d276681f52ceb4d12bd85",
                        "avatarUrl": "/avatars/56601a25e5f883a8f6dc15f6fd9dcc57.svg",
                        "isPro": false,
                        "fullname": "Yifeng Liu",
                        "user": "Lewis-Lau",
                        "type": "user"
                    },
                    "name": "Yifeng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:10:32.620Z",
                    "hidden": false
                },
                {
                    "_id": "68f5aac18589920bf4d32155",
                    "name": "Qingyue Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f5aac18589920bf4d32156",
                    "name": "Angela Yuan",
                    "hidden": false
                },
                {
                    "_id": "68f5aac18589920bf4d32157",
                    "name": "Quanquan Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T02:58:35.000Z",
            "submittedOnDailyAt": "2025-10-20T02:04:06.992Z",
            "title": "Robust Layerwise Scaling Rules by Proper Weight Decay Tuning",
            "submittedOnDailyBy": {
                "_id": "653d276681f52ceb4d12bd85",
                "avatarUrl": "/avatars/56601a25e5f883a8f6dc15f6fd9dcc57.svg",
                "isPro": false,
                "fullname": "Yifeng Liu",
                "user": "Lewis-Lau",
                "type": "user"
            },
            "summary": "Empirical scaling laws prescribe how to allocate parameters, data, and\ncompute, while maximal-update parameterization (muP) enables learning-rate\ntransfer across widths by equalizing early-time update magnitudes. However, in\nmodern scale-invariant architectures, training quickly enters an\noptimizer-governed steady state where normalization layers create backward\nscale sensitivity and the effective learning rate becomes width dependent,\ndegrading muP transfer. We address this by introducing a weight-decay\nscaling rule for AdamW that preserves sublayer gain across widths. Empirically,\nthe singular-value spectrum of each matrix parameter scales in norm as\neta/lambda with an approximately invariant shape; under width\nscaling d, we observe that the top singular value scales approximately as\neta/lambdacdot d^{0.75}. Combining this observation with the muP\nlearning-rate rule eta_2propto d^{-1} for matrix-like parameters implies an\nempirical weight-decay scaling rule lambda_2propto d that\napproximately keeps sublayer gains width invariant. Together with vector-like\nparameters trained at eta_1=Theta_d(1) and lambda_1=0, this yields\nzero-shot transfer of both learning rate and weight decay from proxy to\ntarget widths, removing per-width sweeps. We validate the rule on LLaMA-style\nTransformers and in a minimal synthetic setting, and we provide a simple\ndiagnostic, matching top singular values, to check sublayer-gain invariance.\nOur results extend muP beyond the near-init regime by explicitly controlling\nsteady-state scales set by the optimizer, offering a practical recipe for\nwidth-robust hyperparameter transfer under AdamW.",
            "upvotes": 2,
            "discussionId": "68f5aac28589920bf4d32158",
            "ai_summary": "A new weight-decay scaling rule for AdamW is introduced to preserve sublayer gain across widths in modern scale-invariant architectures, enabling zero-shot transfer of learning rate and weight decay.",
            "ai_keywords": [
                "maximal-update parameterization",
                "$\\mu$P",
                "optimizer-governed steady state",
                "normalization layers",
                "weight-decay scaling rule",
                "AdamW",
                "singular-value spectrum",
                "sublayer gain",
                "LLaMA-style Transformers"
            ],
            "organization": {
                "_id": "63728bde14d543d507ae970d",
                "name": "MIT",
                "fullname": "Massachusetts Institute of Technology",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"
            }
        },
        "publishedAt": "2025-10-16T22:58:35.000Z",
        "title": "Robust Layerwise Scaling Rules by Proper Weight Decay Tuning",
        "summary": "Empirical scaling laws prescribe how to allocate parameters, data, and\ncompute, while maximal-update parameterization (muP) enables learning-rate\ntransfer across widths by equalizing early-time update magnitudes. However, in\nmodern scale-invariant architectures, training quickly enters an\noptimizer-governed steady state where normalization layers create backward\nscale sensitivity and the effective learning rate becomes width dependent,\ndegrading muP transfer. We address this by introducing a weight-decay\nscaling rule for AdamW that preserves sublayer gain across widths. Empirically,\nthe singular-value spectrum of each matrix parameter scales in norm as\neta/lambda with an approximately invariant shape; under width\nscaling d, we observe that the top singular value scales approximately as\neta/lambdacdot d^{0.75}. Combining this observation with the muP\nlearning-rate rule eta_2propto d^{-1} for matrix-like parameters implies an\nempirical weight-decay scaling rule lambda_2propto d that\napproximately keeps sublayer gains width invariant. Together with vector-like\nparameters trained at eta_1=Theta_d(1) and lambda_1=0, this yields\nzero-shot transfer of both learning rate and weight decay from proxy to\ntarget widths, removing per-width sweeps. We validate the rule on LLaMA-style\nTransformers and in a minimal synthetic setting, and we provide a simple\ndiagnostic, matching top singular values, to check sublayer-gain invariance.\nOur results extend muP beyond the near-init regime by explicitly controlling\nsteady-state scales set by the optimizer, offering a practical recipe for\nwidth-robust hyperparameter transfer under AdamW.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15262.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653d276681f52ceb4d12bd85",
            "avatarUrl": "/avatars/56601a25e5f883a8f6dc15f6fd9dcc57.svg",
            "fullname": "Yifeng Liu",
            "name": "Lewis-Lau",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "63728bde14d543d507ae970d",
            "name": "MIT",
            "fullname": "Massachusetts Institute of Technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.14077",
            "authors": [
                {
                    "_id": "68f292388589920bf4d318ea",
                    "user": {
                        "_id": "669577dd5d561534552a3960",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669577dd5d561534552a3960/BrLfQGoOwcjPn2J-3ePI9.png",
                        "isPro": false,
                        "fullname": "Haziq Mohammad Khalid",
                        "user": "Haziq-exe",
                        "type": "user"
                    },
                    "name": "Haziq Mohammad Khalid",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-18T04:47:22.562Z",
                    "hidden": false
                },
                {
                    "_id": "68f292388589920bf4d318eb",
                    "name": "Athikash Jeyaganthan",
                    "hidden": false
                },
                {
                    "_id": "68f292388589920bf4d318ec",
                    "name": "Timothy Do",
                    "hidden": false
                },
                {
                    "_id": "68f292388589920bf4d318ed",
                    "name": "Yicheng Fu",
                    "hidden": false
                },
                {
                    "_id": "68f292388589920bf4d318ee",
                    "name": "Sean O'Brien",
                    "hidden": false
                },
                {
                    "_id": "68f292388589920bf4d318ef",
                    "name": "Vasu Sharma",
                    "hidden": false
                },
                {
                    "_id": "68f292388589920bf4d318f0",
                    "name": "Kevin Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T20:33:08.000Z",
            "submittedOnDailyAt": "2025-10-20T07:50:15.792Z",
            "title": "ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "669577dd5d561534552a3960",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669577dd5d561534552a3960/BrLfQGoOwcjPn2J-3ePI9.png",
                "isPro": false,
                "fullname": "Haziq Mohammad Khalid",
                "user": "Haziq-exe",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) suffer significant performance degradation in\nmulti-turn conversations when information is presented incrementally. Given\nthat multi-turn conversations characterize everyday interactions with LLMs,\nthis degradation poses a severe challenge to real world usability. We\nhypothesize that abrupt increases in model uncertainty signal misalignment in\nmulti-turn LLM interactions, and we exploit this insight to dynamically realign\nconversational context. We introduce ERGO (Entropy-guided Resetting for\nGeneration Optimization), which continuously quantifies internal uncertainty\nvia Shannon entropy over next token distributions and triggers adaptive prompt\nconsolidation when a sharp spike in entropy is detected. By treating\nuncertainty as a first class signal rather than a nuisance to eliminate, ERGO\nembraces variability in language and modeling, representing and responding to\nuncertainty. In multi-turn tasks with incrementally revealed instructions, ERGO\nyields a 56.6% average performance gain over standard baselines, increases\naptitude (peak performance capability) by 24.7%, and decreases unreliability\n(variability in performance) by 35.3%, demonstrating that uncertainty aware\ninterventions can improve both accuracy and reliability in conversational AI.",
            "upvotes": 2,
            "discussionId": "68f292398589920bf4d318f1",
            "projectPage": "https://ergopaper.github.io/ERGO/",
            "githubRepo": "https://github.com/haziq-exe/ERGO",
            "ai_summary": "ERGO, an entropy-guided resetting method, improves conversational AI performance by dynamically realigning context based on internal uncertainty, leading to enhanced accuracy and reliability in multi-turn interactions.",
            "ai_keywords": [
                "Large Language Models",
                "multi-turn conversations",
                "model uncertainty",
                "Shannon entropy",
                "adaptive prompt consolidation",
                "entropy-guided resetting",
                "uncertainty-aware interventions"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-10-15T16:33:08.000Z",
        "title": "ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn\n  Language Models",
        "summary": "Large Language Models (LLMs) suffer significant performance degradation in\nmulti-turn conversations when information is presented incrementally. Given\nthat multi-turn conversations characterize everyday interactions with LLMs,\nthis degradation poses a severe challenge to real world usability. We\nhypothesize that abrupt increases in model uncertainty signal misalignment in\nmulti-turn LLM interactions, and we exploit this insight to dynamically realign\nconversational context. We introduce ERGO (Entropy-guided Resetting for\nGeneration Optimization), which continuously quantifies internal uncertainty\nvia Shannon entropy over next token distributions and triggers adaptive prompt\nconsolidation when a sharp spike in entropy is detected. By treating\nuncertainty as a first class signal rather than a nuisance to eliminate, ERGO\nembraces variability in language and modeling, representing and responding to\nuncertainty. In multi-turn tasks with incrementally revealed instructions, ERGO\nyields a 56.6% average performance gain over standard baselines, increases\naptitude (peak performance capability) by 24.7%, and decreases unreliability\n(variability in performance) by 35.3%, demonstrating that uncertainty aware\ninterventions can improve both accuracy and reliability in conversational AI.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14077.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "669577dd5d561534552a3960",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669577dd5d561534552a3960/BrLfQGoOwcjPn2J-3ePI9.png",
            "fullname": "Haziq Mohammad Khalid",
            "name": "Haziq-exe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.15264",
            "authors": [
                {
                    "_id": "68f625758589920bf4d322d8",
                    "user": {
                        "_id": "66699aa8a33847217b5a49c7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                        "isPro": false,
                        "fullname": "Weijie Wang",
                        "user": "lhmd",
                        "type": "user"
                    },
                    "name": "Weijie Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:09:14.843Z",
                    "hidden": false
                },
                {
                    "_id": "68f625758589920bf4d322d9",
                    "name": "Jiagang Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f625758589920bf4d322da",
                    "name": "Zeyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f625758589920bf4d322db",
                    "name": "Xiaofeng Wang",
                    "hidden": false
                },
                {
                    "_id": "68f625758589920bf4d322dc",
                    "name": "Zheng Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f625758589920bf4d322dd",
                    "name": "Guosheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f625758589920bf4d322de",
                    "name": "Chaojun Ni",
                    "hidden": false
                },
                {
                    "_id": "68f625758589920bf4d322df",
                    "name": "Haoxiao Wang",
                    "hidden": false
                },
                {
                    "_id": "68f625758589920bf4d322e0",
                    "name": "Guan Huang",
                    "hidden": false
                },
                {
                    "_id": "68f625758589920bf4d322e1",
                    "name": "Xinze Chen",
                    "hidden": false
                },
                {
                    "_id": "68f625758589920bf4d322e2",
                    "name": "Yukun Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f625758589920bf4d322e3",
                    "name": "Wenkang Qin",
                    "hidden": false
                },
                {
                    "_id": "68f625758589920bf4d322e4",
                    "name": "Duochao Shi",
                    "hidden": false
                },
                {
                    "_id": "68f625758589920bf4d322e5",
                    "name": "Haoyun Li",
                    "hidden": false
                },
                {
                    "_id": "68f625758589920bf4d322e6",
                    "name": "Guanghong Jia",
                    "hidden": false
                },
                {
                    "_id": "68f625758589920bf4d322e7",
                    "name": "Jiwen Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T03:00:08.000Z",
            "submittedOnDailyAt": "2025-10-20T10:36:08.878Z",
            "title": "DriveGen3D: Boosting Feed-Forward Driving Scene Generation with\n  Efficient Video Diffusion",
            "submittedOnDailyBy": {
                "_id": "66699aa8a33847217b5a49c7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                "isPro": false,
                "fullname": "Weijie Wang",
                "user": "lhmd",
                "type": "user"
            },
            "summary": "We present DriveGen3D, a novel framework for generating high-quality and\nhighly controllable dynamic 3D driving scenes that addresses critical\nlimitations in existing methodologies. Current approaches to driving scene\nsynthesis either suffer from prohibitive computational demands for extended\ntemporal generation, focus exclusively on prolonged video synthesis without 3D\nrepresentation, or restrict themselves to static single-scene reconstruction.\nOur work bridges this methodological gap by integrating accelerated long-term\nvideo generation with large-scale dynamic scene reconstruction through\nmultimodal conditional control. DriveGen3D introduces a unified pipeline\nconsisting of two specialized components: FastDrive-DiT, an efficient video\ndiffusion transformer for high-resolution, temporally coherent video synthesis\nunder text and Bird's-Eye-View (BEV) layout guidance; and FastRecon3D, a\nfeed-forward reconstruction module that rapidly builds 3D Gaussian\nrepresentations across time, ensuring spatial-temporal consistency. Together,\nthese components enable real-time generation of extended driving videos (up to\n424times800 at 12 FPS) and corresponding dynamic 3D scenes, achieving SSIM\nof 0.811 and PSNR of 22.84 on novel view synthesis, all while maintaining\nparameter efficiency.",
            "upvotes": 1,
            "discussionId": "68f625768589920bf4d322e8",
            "ai_summary": "DriveGen3D generates high-quality, controllable dynamic 3D driving scenes using a unified pipeline with efficient video diffusion and 3D reconstruction.",
            "ai_keywords": [
                "video diffusion transformer",
                "FastDrive-DiT",
                "FastRecon3D",
                "3D Gaussian representations",
                "SSIM",
                "PSNR",
                "parameter efficiency"
            ],
            "organization": {
                "_id": "68f5feb151a9558c3dc84362",
                "name": "GigaAI-Research",
                "fullname": "GigaAI-Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b68f3cdd7f21b758968b8d/8uazzXK5v3edLbCaNvmup.png"
            }
        },
        "publishedAt": "2025-10-16T23:00:08.000Z",
        "title": "DriveGen3D: Boosting Feed-Forward Driving Scene Generation with\n  Efficient Video Diffusion",
        "summary": "We present DriveGen3D, a novel framework for generating high-quality and\nhighly controllable dynamic 3D driving scenes that addresses critical\nlimitations in existing methodologies. Current approaches to driving scene\nsynthesis either suffer from prohibitive computational demands for extended\ntemporal generation, focus exclusively on prolonged video synthesis without 3D\nrepresentation, or restrict themselves to static single-scene reconstruction.\nOur work bridges this methodological gap by integrating accelerated long-term\nvideo generation with large-scale dynamic scene reconstruction through\nmultimodal conditional control. DriveGen3D introduces a unified pipeline\nconsisting of two specialized components: FastDrive-DiT, an efficient video\ndiffusion transformer for high-resolution, temporally coherent video synthesis\nunder text and Bird's-Eye-View (BEV) layout guidance; and FastRecon3D, a\nfeed-forward reconstruction module that rapidly builds 3D Gaussian\nrepresentations across time, ensuring spatial-temporal consistency. Together,\nthese components enable real-time generation of extended driving videos (up to\n424times800 at 12 FPS) and corresponding dynamic 3D scenes, achieving SSIM\nof 0.811 and PSNR of 22.84 on novel view synthesis, all while maintaining\nparameter efficiency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15264.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "fullname": "Weijie Wang",
            "name": "lhmd",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "68f5feb151a9558c3dc84362",
            "name": "GigaAI-Research",
            "fullname": "GigaAI-Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b68f3cdd7f21b758968b8d/8uazzXK5v3edLbCaNvmup.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.15162",
            "authors": [
                {
                    "_id": "68f5b0a68589920bf4d3217d",
                    "user": {
                        "_id": "63d34004b734eaa4d4faeccf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/zf6d1p0GN8gsagi8N6y4V.jpeg",
                        "isPro": false,
                        "fullname": "Weizhi Wang",
                        "user": "weizhiwang",
                        "type": "user"
                    },
                    "name": "Weizhi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:10:21.495Z",
                    "hidden": false
                },
                {
                    "_id": "68f5b0a68589920bf4d3217e",
                    "name": "Rongmei Lin",
                    "hidden": false
                },
                {
                    "_id": "68f5b0a68589920bf4d3217f",
                    "name": "Shiyang Li",
                    "hidden": false
                },
                {
                    "_id": "68f5b0a68589920bf4d32180",
                    "name": "Colin Lockard",
                    "hidden": false
                },
                {
                    "_id": "68f5b0a68589920bf4d32181",
                    "name": "Ritesh Sarkhel",
                    "hidden": false
                },
                {
                    "_id": "68f5b0a68589920bf4d32182",
                    "name": "Sanket Lokegaonkar",
                    "hidden": false
                },
                {
                    "_id": "68f5b0a68589920bf4d32183",
                    "name": "Jingbo Shang",
                    "hidden": false
                },
                {
                    "_id": "68f5b0a68589920bf4d32184",
                    "name": "Xifeng Yan",
                    "hidden": false
                },
                {
                    "_id": "68f5b0a68589920bf4d32185",
                    "name": "Nasser Zalmout",
                    "hidden": false
                },
                {
                    "_id": "68f5b0a68589920bf4d32186",
                    "name": "Xian Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T21:53:28.000Z",
            "submittedOnDailyAt": "2025-10-20T02:17:24.118Z",
            "title": "Train a Unified Multimodal Data Quality Classifier with Synthetic Data",
            "submittedOnDailyBy": {
                "_id": "63d34004b734eaa4d4faeccf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/zf6d1p0GN8gsagi8N6y4V.jpeg",
                "isPro": false,
                "fullname": "Weizhi Wang",
                "user": "weizhiwang",
                "type": "user"
            },
            "summary": "The Multimodal Large Language Models (MLLMs) are continually pre-trained on a\nmixture of image-text caption data and interleaved document data, while the\nhigh-quality data filtering towards image-text interleaved document data is\nunder-explored. We propose to train an efficient MLLM as a Unified Mulitmodal\nData Quality Classifier to Filter both high-quality image-text caption and\ninterleaved data (UniFilter). To address the challenge of collecting diverse\nlabeled multimodal data, we introduce a semi-synthetic approach that leverages\nreadily available raw images and generates corresponding text across four\nquality levels. This method enables efficient creation of sample-score pairs\nfor both caption and interleaved document data to train UniFilter. We apply\nUniFilter to curate high-quality caption data from DataComp caption dataset and\ninterleaved data from the OBELICS image-text interleaved dataset. MLLMs\npre-trained on the filtered data demonstrate significantly enhanced\ncapabilities compared to those trained on baseline-filtered data, achieving\nstronger zero-shot reasoning and in-context learning capabilities. After visual\nsupervised fine-tuning, these UniFilter-induced MLLMs achieve stronger\nperformance on various benchmarks, highlighting the downstream benefits of\nhigh-quality multimodal pre-training. We release the synthetic training data\nused for training UniFilter, the UniFilter model checkpoints, and the\nhigh-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to\nthe community for reproduction and further development.",
            "upvotes": 1,
            "discussionId": "68f5b0a78589920bf4d32187",
            "ai_summary": "UniFilter, a Unified Multimodal Data Quality Classifier, enhances Multimodal Large Language Models (MLLMs) by filtering high-quality image-text and interleaved data, leading to improved zero-shot reasoning and in-context learning.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "MLLMs",
                "Unified Multimodal Data Quality Classifier",
                "UniFilter",
                "semi-synthetic approach",
                "zero-shot reasoning",
                "in-context learning",
                "visual supervised fine-tuning",
                "DataComp caption dataset",
                "OBELICS image-text interleaved dataset",
                "OBELICS-HQ"
            ]
        },
        "publishedAt": "2025-10-16T17:53:28.000Z",
        "title": "Train a Unified Multimodal Data Quality Classifier with Synthetic Data",
        "summary": "The Multimodal Large Language Models (MLLMs) are continually pre-trained on a\nmixture of image-text caption data and interleaved document data, while the\nhigh-quality data filtering towards image-text interleaved document data is\nunder-explored. We propose to train an efficient MLLM as a Unified Mulitmodal\nData Quality Classifier to Filter both high-quality image-text caption and\ninterleaved data (UniFilter). To address the challenge of collecting diverse\nlabeled multimodal data, we introduce a semi-synthetic approach that leverages\nreadily available raw images and generates corresponding text across four\nquality levels. This method enables efficient creation of sample-score pairs\nfor both caption and interleaved document data to train UniFilter. We apply\nUniFilter to curate high-quality caption data from DataComp caption dataset and\ninterleaved data from the OBELICS image-text interleaved dataset. MLLMs\npre-trained on the filtered data demonstrate significantly enhanced\ncapabilities compared to those trained on baseline-filtered data, achieving\nstronger zero-shot reasoning and in-context learning capabilities. After visual\nsupervised fine-tuning, these UniFilter-induced MLLMs achieve stronger\nperformance on various benchmarks, highlighting the downstream benefits of\nhigh-quality multimodal pre-training. We release the synthetic training data\nused for training UniFilter, the UniFilter model checkpoints, and the\nhigh-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to\nthe community for reproduction and further development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15162.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63d34004b734eaa4d4faeccf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/zf6d1p0GN8gsagi8N6y4V.jpeg",
            "fullname": "Weizhi Wang",
            "name": "weizhiwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.14630",
            "authors": [
                {
                    "_id": "68f6a72b24c4489363111672",
                    "name": "Ming Gui",
                    "hidden": false
                },
                {
                    "_id": "68f6a72b24c4489363111673",
                    "name": "Johannes Schusterbauer",
                    "hidden": false
                },
                {
                    "_id": "68f6a72b24c4489363111674",
                    "name": "Timy Phan",
                    "hidden": false
                },
                {
                    "_id": "68f6a72b24c4489363111675",
                    "name": "Felix Krause",
                    "hidden": false
                },
                {
                    "_id": "68f6a72b24c4489363111676",
                    "name": "Josh Susskind",
                    "hidden": false
                },
                {
                    "_id": "68f6a72b24c4489363111677",
                    "name": "Miguel Angel Bautista",
                    "hidden": false
                },
                {
                    "_id": "68f6a72b24c4489363111678",
                    "name": "Björn Ommer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T12:43:03.000Z",
            "submittedOnDailyAt": "2025-10-20T19:50:44.355Z",
            "title": "Adapting Self-Supervised Representations as a Latent Space for Efficient\n  Generation",
            "submittedOnDailyBy": {
                "_id": "6319f120d47392f426cd48be",
                "avatarUrl": "/avatars/fa6d8761ef7a1a86a8a7e97408dc3142.svg",
                "isPro": false,
                "fullname": "Ming Gui",
                "user": "mgui",
                "type": "user"
            },
            "summary": "We introduce Representation Tokenizer (RepTok), a generative modeling\nframework that represents an image using a single continuous latent token\nobtained from self-supervised vision transformers. Building on a pre-trained\nSSL encoder, we fine-tune only the semantic token embedding and pair it with a\ngenerative decoder trained jointly using a standard flow matching objective.\nThis adaptation enriches the token with low-level, reconstruction-relevant\ndetails, enabling faithful image reconstruction. To preserve the favorable\ngeometry of the original SSL space, we add a cosine-similarity loss that\nregularizes the adapted token, ensuring the latent space remains smooth and\nsuitable for generation. Our single-token formulation resolves spatial\nredundancies of 2D latent spaces and significantly reduces training costs.\nDespite its simplicity and efficiency, RepTok achieves competitive results on\nclass-conditional ImageNet generation and naturally extends to text-to-image\nsynthesis, reaching competitive zero-shot performance on MS-COCO under\nextremely limited training budgets. Our findings highlight the potential of\nfine-tuned SSL representations as compact and effective latent spaces for\nefficient generative modeling.",
            "upvotes": 1,
            "discussionId": "68f6a72b24c4489363111679",
            "ai_summary": "A generative modeling framework using a single latent token from self-supervised vision transformers achieves competitive results in image and text-to-image synthesis with reduced training costs.",
            "ai_keywords": [
                "Representation Tokenizer",
                "RepTok",
                "generative modeling",
                "self-supervised vision transformers",
                "SSL encoder",
                "semantic token embedding",
                "generative decoder",
                "flow matching objective",
                "cosine-similarity loss",
                "latent space",
                "class-conditional ImageNet generation",
                "text-to-image synthesis",
                "MS-COCO",
                "zero-shot performance"
            ],
            "organization": {
                "_id": "62cfeeb73c54a34d508b82a9",
                "name": "CompVis",
                "fullname": "CompVis",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1657794102363-5e3aec01f55e2b62848a5217.png"
            }
        },
        "publishedAt": "2025-10-16T08:43:03.000Z",
        "title": "Adapting Self-Supervised Representations as a Latent Space for Efficient\n  Generation",
        "summary": "We introduce Representation Tokenizer (RepTok), a generative modeling\nframework that represents an image using a single continuous latent token\nobtained from self-supervised vision transformers. Building on a pre-trained\nSSL encoder, we fine-tune only the semantic token embedding and pair it with a\ngenerative decoder trained jointly using a standard flow matching objective.\nThis adaptation enriches the token with low-level, reconstruction-relevant\ndetails, enabling faithful image reconstruction. To preserve the favorable\ngeometry of the original SSL space, we add a cosine-similarity loss that\nregularizes the adapted token, ensuring the latent space remains smooth and\nsuitable for generation. Our single-token formulation resolves spatial\nredundancies of 2D latent spaces and significantly reduces training costs.\nDespite its simplicity and efficiency, RepTok achieves competitive results on\nclass-conditional ImageNet generation and naturally extends to text-to-image\nsynthesis, reaching competitive zero-shot performance on MS-COCO under\nextremely limited training budgets. Our findings highlight the potential of\nfine-tuned SSL representations as compact and effective latent spaces for\nefficient generative modeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14630.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6319f120d47392f426cd48be",
            "avatarUrl": "/avatars/fa6d8761ef7a1a86a8a7e97408dc3142.svg",
            "fullname": "Ming Gui",
            "name": "mgui",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "62cfeeb73c54a34d508b82a9",
            "name": "CompVis",
            "fullname": "CompVis",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1657794102363-5e3aec01f55e2b62848a5217.png"
        },
        "isAuthorParticipating": false
    }
]