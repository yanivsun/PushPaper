[
    {
        "paper": {
            "id": "2510.16872",
            "authors": [
                {
                    "_id": "68f6ed0424c4489363111848",
                    "name": "Shaolei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f6ed0424c4489363111849",
                    "name": "Ju Fan",
                    "hidden": false
                },
                {
                    "_id": "68f6ed0424c448936311184a",
                    "name": "Meihao Fan",
                    "hidden": false
                },
                {
                    "_id": "68f6ed0424c448936311184b",
                    "name": "Guoliang Li",
                    "hidden": false
                },
                {
                    "_id": "68f6ed0424c448936311184c",
                    "name": "Xiaoyong Du",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-19T15:13:42.000Z",
            "submittedOnDailyAt": "2025-10-21T00:48:17.032Z",
            "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science",
            "submittedOnDailyBy": {
                "_id": "64803e5dc57f629056c601f1",
                "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
                "isPro": false,
                "fullname": "Shaolei Zhang",
                "user": "zhangshaolei",
                "type": "user"
            },
            "summary": "Autonomous data science, from raw data sources to analyst-grade deep research\nreports, has been a long-standing challenge, and is now becoming feasible with\nthe emergence of powerful large language models (LLMs). Recent workflow-based\ndata agents have shown promising results on specific data tasks but remain\nfundamentally limited in achieving fully autonomous data science due to their\nreliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,\nthe first agentic LLM designed for autonomous data science, capable of\nautomatically completing the end-toend pipeline from data sources to\nanalyst-grade deep research reports. To tackle high-complexity data science\ntasks, we propose a curriculum-based agentic training paradigm that emulates\nthe learning trajectory of human data scientists, enabling LLMs to\nprogressively acquire and integrate multiple capabilities in real-world\nenvironments. We also introduce a data-grounded trajectory synthesis framework\nthat constructs high-quality training data. Through agentic training,\nDeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data\nquestion answering and specialized analytical tasks to open-ended data\nresearch. Experiments demonstrate that, with only 8B parameters, DeepAnalyze\noutperforms previous workflow-based agents built on most advanced proprietary\nLLMs. The model, code, and training data of DeepAnalyze are open-sourced,\npaving the way toward autonomous data science.",
            "upvotes": 56,
            "discussionId": "68f6ed0424c448936311184d",
            "projectPage": "https://ruc-deepanalyze.github.io/",
            "githubRepo": "https://github.com/ruc-datalab/DeepAnalyze",
            "ai_summary": "DeepAnalyze-8B, an agentic LLM, autonomously completes the data science pipeline from raw data to research reports using curriculum-based training and data-grounded trajectory synthesis.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "workflow-based data agents",
                "agentic LLM",
                "curriculum-based agentic training",
                "data-grounded trajectory synthesis",
                "data question answering",
                "specialized analytical tasks",
                "open-ended data research"
            ],
            "githubStars": 154,
            "organization": {
                "_id": "621a22353bae762bb9faaffb",
                "name": "RUC-DataLab",
                "fullname": "RUC-DataLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/tsYgFKBKYc4VNfO8g5zmP.png"
            }
        },
        "publishedAt": "2025-10-19T11:13:42.000Z",
        "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science",
        "summary": "Autonomous data science, from raw data sources to analyst-grade deep research\nreports, has been a long-standing challenge, and is now becoming feasible with\nthe emergence of powerful large language models (LLMs). Recent workflow-based\ndata agents have shown promising results on specific data tasks but remain\nfundamentally limited in achieving fully autonomous data science due to their\nreliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,\nthe first agentic LLM designed for autonomous data science, capable of\nautomatically completing the end-toend pipeline from data sources to\nanalyst-grade deep research reports. To tackle high-complexity data science\ntasks, we propose a curriculum-based agentic training paradigm that emulates\nthe learning trajectory of human data scientists, enabling LLMs to\nprogressively acquire and integrate multiple capabilities in real-world\nenvironments. We also introduce a data-grounded trajectory synthesis framework\nthat constructs high-quality training data. Through agentic training,\nDeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data\nquestion answering and specialized analytical tasks to open-ended data\nresearch. Experiments demonstrate that, with only 8B parameters, DeepAnalyze\noutperforms previous workflow-based agents built on most advanced proprietary\nLLMs. The model, code, and training data of DeepAnalyze are open-sourced,\npaving the way toward autonomous data science.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16872.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64803e5dc57f629056c601f1",
            "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
            "fullname": "Shaolei Zhang",
            "name": "zhangshaolei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "621a22353bae762bb9faaffb",
            "name": "RUC-DataLab",
            "fullname": "RUC-DataLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/tsYgFKBKYc4VNfO8g5zmP.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.17681",
            "authors": [
                {
                    "_id": "68f6ed0724c448936311184f",
                    "user": {
                        "_id": "625d5b9f0bec31f086e04cd9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650285458447-noauth.jpeg",
                        "isPro": false,
                        "fullname": "YuandongPu",
                        "user": "Andrew613",
                        "type": "user"
                    },
                    "name": "Yuandong Pu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:03:54.052Z",
                    "hidden": false
                },
                {
                    "_id": "68f6ed0724c4489363111850",
                    "name": "Le Zhuo",
                    "hidden": false
                },
                {
                    "_id": "68f6ed0724c4489363111851",
                    "user": {
                        "_id": "662885b9b87483ae5a9ee5c9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662885b9b87483ae5a9ee5c9/fLzCrWizo_hy7zGuAqFwk.jpeg",
                        "isPro": false,
                        "fullname": "Songhao Han",
                        "user": "hshjerry0315",
                        "type": "user"
                    },
                    "name": "Songhao Han",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:03:50.886Z",
                    "hidden": false
                },
                {
                    "_id": "68f6ed0724c4489363111852",
                    "name": "Jinbo Xing",
                    "hidden": false
                },
                {
                    "_id": "68f6ed0724c4489363111853",
                    "name": "Kaiwen Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f6ed0724c4489363111854",
                    "name": "Shuo Cao",
                    "hidden": false
                },
                {
                    "_id": "68f6ed0724c4489363111855",
                    "name": "Bin Fu",
                    "hidden": false
                },
                {
                    "_id": "68f6ed0724c4489363111856",
                    "name": "Si Liu",
                    "hidden": false
                },
                {
                    "_id": "68f6ed0724c4489363111857",
                    "name": "Hongsheng Li",
                    "hidden": false
                },
                {
                    "_id": "68f6ed0724c4489363111858",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68f6ed0724c4489363111859",
                    "name": "Wenlong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f6ed0724c448936311185a",
                    "name": "Xi Chen",
                    "hidden": false
                },
                {
                    "_id": "68f6ed0724c448936311185b",
                    "name": "Yihao Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T15:53:57.000Z",
            "submittedOnDailyAt": "2025-10-21T00:48:55.230Z",
            "title": "PICABench: How Far Are We from Physically Realistic Image Editing?",
            "submittedOnDailyBy": {
                "_id": "625d5b9f0bec31f086e04cd9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650285458447-noauth.jpeg",
                "isPro": false,
                "fullname": "YuandongPu",
                "user": "Andrew613",
                "type": "user"
            },
            "summary": "Image editing has achieved remarkable progress recently. Modern editing\nmodels could already follow complex instructions to manipulate the original\ncontent. However, beyond completing the editing instructions, the accompanying\nphysical effects are the key to the generation realism. For example, removing\nan object should also remove its shadow, reflections, and interactions with\nnearby objects. Unfortunately, existing models and benchmarks mainly focus on\ninstruction completion but overlook these physical effects. So, at this moment,\nhow far are we from physically realistic image editing? To answer this, we\nintroduce PICABench, which systematically evaluates physical realism across\neight sub-dimension (spanning optics, mechanics, and state transitions) for\nmost of the common editing operations (add, remove, attribute change, etc). We\nfurther propose the PICAEval, a reliable evaluation protocol that uses\nVLM-as-a-judge with per-case, region-level human annotations and questions.\nBeyond benchmarking, we also explore effective solutions by learning physics\nfrom videos and construct a training dataset PICA-100K. After evaluating most\nof the mainstream models, we observe that physical realism remains a\nchallenging problem with large rooms to explore. We hope that our benchmark and\nproposed solutions can serve as a foundation for future work moving from naive\ncontent editing toward physically consistent realism.",
            "upvotes": 55,
            "discussionId": "68f6ed0724c448936311185c",
            "projectPage": "https://picabench.github.io/",
            "githubRepo": "https://github.com/Andrew0613/PICABench",
            "ai_summary": "PICABench and PICAEval evaluate physical realism in image editing by assessing eight sub-dimensions and using VLM-as-a-judge with human annotations, highlighting the need for physics-based solutions.",
            "ai_keywords": [
                "PICABench",
                "PICAEval",
                "VLM-as-a-judge",
                "physical realism",
                "image editing",
                "physics-based solutions",
                "PICA-100K"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-10-20T11:53:57.000Z",
        "title": "PICABench: How Far Are We from Physically Realistic Image Editing?",
        "summary": "Image editing has achieved remarkable progress recently. Modern editing\nmodels could already follow complex instructions to manipulate the original\ncontent. However, beyond completing the editing instructions, the accompanying\nphysical effects are the key to the generation realism. For example, removing\nan object should also remove its shadow, reflections, and interactions with\nnearby objects. Unfortunately, existing models and benchmarks mainly focus on\ninstruction completion but overlook these physical effects. So, at this moment,\nhow far are we from physically realistic image editing? To answer this, we\nintroduce PICABench, which systematically evaluates physical realism across\neight sub-dimension (spanning optics, mechanics, and state transitions) for\nmost of the common editing operations (add, remove, attribute change, etc). We\nfurther propose the PICAEval, a reliable evaluation protocol that uses\nVLM-as-a-judge with per-case, region-level human annotations and questions.\nBeyond benchmarking, we also explore effective solutions by learning physics\nfrom videos and construct a training dataset PICA-100K. After evaluating most\nof the mainstream models, we observe that physical realism remains a\nchallenging problem with large rooms to explore. We hope that our benchmark and\nproposed solutions can serve as a foundation for future work moving from naive\ncontent editing toward physically consistent realism.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17681.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "625d5b9f0bec31f086e04cd9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650285458447-noauth.jpeg",
            "fullname": "YuandongPu",
            "name": "Andrew613",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.17800",
            "authors": [
                {
                    "_id": "68f6f94824c4489363111924",
                    "name": "Jiale Cheng",
                    "hidden": false
                },
                {
                    "_id": "68f6f94824c4489363111925",
                    "name": "Yusen Liu",
                    "hidden": false
                },
                {
                    "_id": "68f6f94824c4489363111926",
                    "name": "Xinyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f6f94824c4489363111927",
                    "name": "Yulin Fei",
                    "hidden": false
                },
                {
                    "_id": "68f6f94824c4489363111928",
                    "name": "Wenyi Hong",
                    "hidden": false
                },
                {
                    "_id": "68f6f94824c4489363111929",
                    "name": "Ruiliang Lyu",
                    "hidden": false
                },
                {
                    "_id": "68f6f94824c448936311192a",
                    "name": "Weihan Wang",
                    "hidden": false
                },
                {
                    "_id": "68f6f94824c448936311192b",
                    "name": "Zhe Su",
                    "hidden": false
                },
                {
                    "_id": "68f6f94824c448936311192c",
                    "name": "Xiaotao Gu",
                    "hidden": false
                },
                {
                    "_id": "68f6f94824c448936311192d",
                    "name": "Xiao Liu",
                    "hidden": false
                },
                {
                    "_id": "68f6f94824c448936311192e",
                    "name": "Yushi Bai",
                    "hidden": false
                },
                {
                    "_id": "68f6f94824c448936311192f",
                    "name": "Jie Tang",
                    "hidden": false
                },
                {
                    "_id": "68f6f94824c4489363111930",
                    "name": "Hongning Wang",
                    "hidden": false
                },
                {
                    "_id": "68f6f94824c4489363111931",
                    "name": "Minlie Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T17:58:56.000Z",
            "submittedOnDailyAt": "2025-10-21T02:00:03.910Z",
            "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
            "submittedOnDailyBy": {
                "_id": "627626d42d26ac639e56f565",
                "avatarUrl": "/avatars/805c5f909f52656345b8bde486c9fa8f.svg",
                "isPro": false,
                "fullname": "Jiale Cheng",
                "user": "CCCCCC",
                "type": "user"
            },
            "summary": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.",
            "upvotes": 42,
            "discussionId": "68f6f94824c4489363111932",
            "ai_summary": "Glyph compresses long textual inputs into images using vision-language models, achieving significant token compression and improved performance in long-context tasks.",
            "ai_keywords": [
                "long-context modeling",
                "large language models",
                "token-based sequences",
                "Glyph",
                "vision-language models",
                "genetic search",
                "token compression",
                "prefilling",
                "decoding",
                "SFT training",
                "multimodal tasks",
                "document understanding"
            ]
        },
        "publishedAt": "2025-10-20T13:58:56.000Z",
        "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
        "summary": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17800.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "627626d42d26ac639e56f565",
            "avatarUrl": "/avatars/805c5f909f52656345b8bde486c9fa8f.svg",
            "fullname": "Jiale Cheng",
            "name": "CCCCCC",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.16449",
            "authors": [
                {
                    "_id": "68f6f28324c44893631118ac",
                    "user": {
                        "_id": "63d3b5f1640bb0f77173baea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg",
                        "isPro": false,
                        "fullname": "yubin",
                        "user": "VLyb",
                        "type": "user"
                    },
                    "name": "Bin Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:03:45.205Z",
                    "hidden": true
                },
                {
                    "_id": "68f6f28324c44893631118ad",
                    "name": "Xinming Wang",
                    "hidden": false
                },
                {
                    "_id": "68f6f28324c44893631118ae",
                    "name": "Shijie Lian",
                    "hidden": false
                },
                {
                    "_id": "68f6f28324c44893631118af",
                    "name": "Haotian Li",
                    "hidden": false
                },
                {
                    "_id": "68f6f28324c44893631118b0",
                    "name": "Changti Wu",
                    "hidden": false
                },
                {
                    "_id": "68f6f28324c44893631118b1",
                    "name": "Ruina Hu",
                    "hidden": false
                },
                {
                    "_id": "68f6f28324c44893631118b2",
                    "name": "Bailing Wang",
                    "hidden": false
                },
                {
                    "_id": "68f6f28324c44893631118b3",
                    "name": "Yuliang Wei",
                    "hidden": false
                },
                {
                    "_id": "68f6f28324c44893631118b4",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-18T11:01:39.000Z",
            "submittedOnDailyAt": "2025-10-21T22:21:29.943Z",
            "title": "TrajSelector: Harnessing Latent Representations for Efficient and\n  Effective Best-of-N in Large Reasoning Model",
            "submittedOnDailyBy": {
                "_id": "63d3b5f1640bb0f77173baea",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg",
                "isPro": false,
                "fullname": "yubin",
                "user": "VLyb",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have shown remarkable progress in complex\nreasoning tasks, largely enabled by test-time scaling (TTS) paradigms that\nallocate additional compute during inference. Among these, external TTS\n(particularly the Best-of-N selection paradigm) yields scalable performance\nimprovements by selecting from multiple independently generated reasoning\ntrajectories. However, this approach faces key limitations: (i) the high\ncomputational overhead of deploying process reward models, (ii) the\nunderutilization of the LLM's intrinsic latent representations. We introduce\nTrajSelector, an efficient and effective Best-of-N framework that exploit the\nhidden states in the sampler LLM for process-level scoring. A lightweight\nverifier (with only 0.6B parameters) evaluates the quality of step-wise\ntrajectory, and then aggregates these scores to identify the optimal reasoning\ntrajectory. Our framework employs a fully data-driven, end-to-end training\nrecipe that eliminates reliance on massive step-level annotations. Experiential\nresults across five benchmarks demonstrate that TrajSelector delivers\nconsistent performance gains. In Best-of-32 settings, it surpasses majority\nvoting by 4.61% accuracy and outperforms existing process reward models by\n4.31% to 12.21%, all while maintaining lower inference costs.",
            "upvotes": 32,
            "discussionId": "68f6f28424c44893631118b5",
            "ai_summary": "TrajSelector is an efficient Best-of-N framework that leverages hidden states for process-level scoring, improving LLM performance with lower computational costs.",
            "ai_keywords": [
                "large language models",
                "test-time scaling",
                "external TTS",
                "Best-of-N selection",
                "process reward models",
                "latent representations",
                "hidden states",
                "sampler LLM",
                "lightweight verifier",
                "data-driven training",
                "step-wise trajectory",
                "optimal reasoning trajectory",
                "inference costs"
            ],
            "organization": {
                "_id": "68896d3a716ee5bfb1428441",
                "name": "ZGCA",
                "fullname": "Zhongguancun Academy",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"
            }
        },
        "publishedAt": "2025-10-18T07:01:39.000Z",
        "title": "TrajSelector: Harnessing Latent Representations for Efficient and\n  Effective Best-of-N in Large Reasoning Model",
        "summary": "Large language models (LLMs) have shown remarkable progress in complex\nreasoning tasks, largely enabled by test-time scaling (TTS) paradigms that\nallocate additional compute during inference. Among these, external TTS\n(particularly the Best-of-N selection paradigm) yields scalable performance\nimprovements by selecting from multiple independently generated reasoning\ntrajectories. However, this approach faces key limitations: (i) the high\ncomputational overhead of deploying process reward models, (ii) the\nunderutilization of the LLM's intrinsic latent representations. We introduce\nTrajSelector, an efficient and effective Best-of-N framework that exploit the\nhidden states in the sampler LLM for process-level scoring. A lightweight\nverifier (with only 0.6B parameters) evaluates the quality of step-wise\ntrajectory, and then aggregates these scores to identify the optimal reasoning\ntrajectory. Our framework employs a fully data-driven, end-to-end training\nrecipe that eliminates reliance on massive step-level annotations. Experiential\nresults across five benchmarks demonstrate that TrajSelector delivers\nconsistent performance gains. In Best-of-32 settings, it surpasses majority\nvoting by 4.61% accuracy and outperforms existing process reward models by\n4.31% to 12.21%, all while maintaining lower inference costs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16449.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63d3b5f1640bb0f77173baea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg",
            "fullname": "yubin",
            "name": "VLyb",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "68896d3a716ee5bfb1428441",
            "name": "ZGCA",
            "fullname": "Zhongguancun Academy",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.17269",
            "authors": [
                {
                    "_id": "68f6f4e524c44893631118d3",
                    "name": "Luis Wiedmann",
                    "hidden": false
                },
                {
                    "_id": "68f6f4e524c44893631118d4",
                    "name": "Orr Zohar",
                    "hidden": false
                },
                {
                    "_id": "68f6f4e524c44893631118d5",
                    "name": "Amir Mahla",
                    "hidden": false
                },
                {
                    "_id": "68f6f4e524c44893631118d6",
                    "user": {
                        "_id": "65703fab7f50602340d23704",
                        "avatarUrl": "/avatars/324c45f5fba9cd8c38a89b30427c06b4.svg",
                        "isPro": false,
                        "fullname": "Xiaohan Wang",
                        "user": "nicholswang",
                        "type": "user"
                    },
                    "name": "Xiaohan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:03:29.907Z",
                    "hidden": false
                },
                {
                    "_id": "68f6f4e524c44893631118d7",
                    "name": "Rui Li",
                    "hidden": false
                },
                {
                    "_id": "68f6f4e524c44893631118d8",
                    "name": "Thibaud Frere",
                    "hidden": false
                },
                {
                    "_id": "68f6f4e524c44893631118d9",
                    "name": "Leandro von Werra",
                    "hidden": false
                },
                {
                    "_id": "68f6f4e524c44893631118da",
                    "user": {
                        "_id": "608aabf24955d2bfc3cd99c6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/608aabf24955d2bfc3cd99c6/T762Ut0Y-w0sZB2ynvfbJ.jpeg",
                        "isPro": true,
                        "fullname": "Aritra Roy Gosthipaty",
                        "user": "ariG23498",
                        "type": "user"
                    },
                    "name": "Aritra Roy Gosthipaty",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:03:26.559Z",
                    "hidden": false
                },
                {
                    "_id": "68f6f4e524c44893631118db",
                    "name": "Andr√©s Marafioti",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T07:54:46.000Z",
            "submittedOnDailyAt": "2025-10-21T01:20:29.268Z",
            "title": "FineVision: Open Data Is All You Need",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The advancement of vision-language models (VLMs) is hampered by a fragmented\nlandscape of inconsistent and contaminated public datasets. We introduce\nFineVision, a meticulously collected, curated, and unified corpus of 24 million\nsamples - the largest open resource of its kind. We unify more than 200 sources\ninto 185 subsets via a semi-automated, human-in-the-loop pipeline: automation\nperforms bulk ingestion and schema mapping, while reviewers audit mappings and\nspot-check outputs to verify faithful consumption of annotations, appropriate\nformatting and diversity, and safety; issues trigger targeted fixes and\nre-runs. The workflow further applies rigorous de-duplication within and across\nsources and decontamination against 66 public benchmarks. FineVision also\nencompasses agentic/GUI tasks with a unified action space; reviewers validate\nschemas and inspect a sample of trajectories to confirm executable fidelity.\nModels trained on FineVision consistently outperform those trained on existing\nopen mixtures across a broad evaluation suite, underscoring the benefits of\nscale, data hygiene, and balanced automation with human oversight. We release\nthe corpus and curation tools to accelerate data-centric VLM research.",
            "upvotes": 30,
            "discussionId": "68f6f4e524c44893631118dc",
            "projectPage": "https://huggingface.co/spaces/HuggingFaceM4/FineVision",
            "ai_summary": "FineVision, a large-scale and curated dataset, enhances vision-language models through rigorous data collection, de-duplication, and human oversight, leading to improved performance.",
            "ai_keywords": [
                "vision-language models",
                "FineVision",
                "semi-automated pipeline",
                "human-in-the-loop",
                "schema mapping",
                "de-duplication",
                "decontamination",
                "agentic/GUI tasks",
                "unified action space",
                "data-centric research"
            ],
            "organization": {
                "_id": "5e67bd5b1009063689407478",
                "name": "huggingface",
                "fullname": "Hugging Face",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583856921041-5dd96eb166059660ed1ee413.png"
            }
        },
        "publishedAt": "2025-10-20T03:54:46.000Z",
        "title": "FineVision: Open Data Is All You Need",
        "summary": "The advancement of vision-language models (VLMs) is hampered by a fragmented\nlandscape of inconsistent and contaminated public datasets. We introduce\nFineVision, a meticulously collected, curated, and unified corpus of 24 million\nsamples - the largest open resource of its kind. We unify more than 200 sources\ninto 185 subsets via a semi-automated, human-in-the-loop pipeline: automation\nperforms bulk ingestion and schema mapping, while reviewers audit mappings and\nspot-check outputs to verify faithful consumption of annotations, appropriate\nformatting and diversity, and safety; issues trigger targeted fixes and\nre-runs. The workflow further applies rigorous de-duplication within and across\nsources and decontamination against 66 public benchmarks. FineVision also\nencompasses agentic/GUI tasks with a unified action space; reviewers validate\nschemas and inspect a sample of trajectories to confirm executable fidelity.\nModels trained on FineVision consistently outperform those trained on existing\nopen mixtures across a broad evaluation suite, underscoring the benefits of\nscale, data hygiene, and balanced automation with human oversight. We release\nthe corpus and curation tools to accelerate data-centric VLM research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17269.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 134
        },
        "organization": {
            "_id": "5e67bd5b1009063689407478",
            "name": "huggingface",
            "fullname": "Hugging Face",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583856921041-5dd96eb166059660ed1ee413.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.17354",
            "authors": [
                {
                    "_id": "68f6ebff24c448936311182d",
                    "user": {
                        "_id": "6710ac3fb4ee4920580a5f0e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6710ac3fb4ee4920580a5f0e/OhQQFlZmkmLQpMYqKCGP6.jpeg",
                        "isPro": false,
                        "fullname": "Chenghao Zhang",
                        "user": "SnowNation",
                        "type": "user"
                    },
                    "name": "Chenghao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:04:00.429Z",
                    "hidden": false
                },
                {
                    "_id": "68f6ebff24c448936311182e",
                    "user": {
                        "_id": "61cd4b833dd34ba1985e0753",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
                        "isPro": false,
                        "fullname": "KABI",
                        "user": "dongguanting",
                        "type": "user"
                    },
                    "name": "Guanting Dong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:03:57.402Z",
                    "hidden": false
                },
                {
                    "_id": "68f6ebff24c448936311182f",
                    "name": "Xinyu Yang",
                    "hidden": false
                },
                {
                    "_id": "68f6ebff24c4489363111830",
                    "name": "Zhicheng Dou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T09:56:43.000Z",
            "submittedOnDailyAt": "2025-10-21T00:50:13.474Z",
            "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented\n  Generation",
            "submittedOnDailyBy": {
                "_id": "6710ac3fb4ee4920580a5f0e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6710ac3fb4ee4920580a5f0e/OhQQFlZmkmLQpMYqKCGP6.jpeg",
                "isPro": false,
                "fullname": "Chenghao Zhang",
                "user": "SnowNation",
                "type": "user"
            },
            "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing large language models (LLMs) by retrieving relevant documents from an\nexternal corpus. However, existing RAG systems primarily focus on unimodal text\ndocuments, and often fall short in real-world scenarios where both queries and\ndocuments may contain mixed modalities (such as text and images). In this\npaper, we address the challenge of Universal Retrieval-Augmented Generation\n(URAG), which involves retrieving and reasoning over mixed-modal information to\nimprove vision-language generation. To this end, we propose Nyx, a unified\nmixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate\nthe scarcity of realistic mixed-modal data, we introduce a four-stage automated\npipeline for generation and filtering, leveraging web documents to construct\nNyxQA, a dataset comprising diverse mixed-modal question-answer pairs that\nbetter reflect real-world information needs. Building on this high-quality\ndataset, we adopt a two-stage training framework for Nyx: we first perform\npre-training on NyxQA along with a variety of open-source retrieval datasets,\nfollowed by supervised fine-tuning using feedback from downstream\nvision-language models (VLMs) to align retrieval outputs with generative\npreferences. Experimental results demonstrate that Nyx not only performs\ncompetitively on standard text-only RAG benchmarks, but also excels in the more\ngeneral and realistic URAG setting, significantly improving generation quality\nin vision-language tasks.",
            "upvotes": 28,
            "discussionId": "68f6ec0024c4489363111831",
            "githubRepo": "https://github.com/SnowNation101/Nyx",
            "ai_summary": "Nyx, a unified mixed-modal retriever, enhances vision-language generation by retrieving and reasoning over mixed-modal data, outperforming existing RAG systems in real-world scenarios.",
            "ai_keywords": [
                "Retrieval-Augmented Generation",
                "RAG",
                "large language models",
                "LLMs",
                "mixed modalities",
                "Universal Retrieval-Augmented Generation",
                "URAG",
                "Nyx",
                "NyxQA",
                "two-stage training framework",
                "pre-training",
                "supervised fine-tuning",
                "vision-language models",
                "VLMs",
                "generation quality",
                "vision-language tasks"
            ],
            "githubStars": 7,
            "organization": {
                "_id": "622177ac43826d6f261f8208",
                "name": "RUC",
                "fullname": "Renmin University of China",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
            }
        },
        "publishedAt": "2025-10-20T05:56:43.000Z",
        "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented\n  Generation",
        "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing large language models (LLMs) by retrieving relevant documents from an\nexternal corpus. However, existing RAG systems primarily focus on unimodal text\ndocuments, and often fall short in real-world scenarios where both queries and\ndocuments may contain mixed modalities (such as text and images). In this\npaper, we address the challenge of Universal Retrieval-Augmented Generation\n(URAG), which involves retrieving and reasoning over mixed-modal information to\nimprove vision-language generation. To this end, we propose Nyx, a unified\nmixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate\nthe scarcity of realistic mixed-modal data, we introduce a four-stage automated\npipeline for generation and filtering, leveraging web documents to construct\nNyxQA, a dataset comprising diverse mixed-modal question-answer pairs that\nbetter reflect real-world information needs. Building on this high-quality\ndataset, we adopt a two-stage training framework for Nyx: we first perform\npre-training on NyxQA along with a variety of open-source retrieval datasets,\nfollowed by supervised fine-tuning using feedback from downstream\nvision-language models (VLMs) to align retrieval outputs with generative\npreferences. Experimental results demonstrate that Nyx not only performs\ncompetitively on standard text-only RAG benchmarks, but also excels in the more\ngeneral and realistic URAG setting, significantly improving generation quality\nin vision-language tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17354.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6710ac3fb4ee4920580a5f0e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6710ac3fb4ee4920580a5f0e/OhQQFlZmkmLQpMYqKCGP6.jpeg",
            "fullname": "Chenghao Zhang",
            "name": "SnowNation",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "622177ac43826d6f261f8208",
            "name": "RUC",
            "fullname": "Renmin University of China",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.15346",
            "authors": [
                {
                    "_id": "68f5cb858589920bf4d321c6",
                    "user": {
                        "_id": "67f778ddbb19958f5d96c2a8",
                        "avatarUrl": "/avatars/49a3f119b456ff94f28f09b2fe78bb18.svg",
                        "isPro": false,
                        "fullname": "Heecheol Yun",
                        "user": "yoon6503",
                        "type": "user"
                    },
                    "name": "Heecheol Yun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:09:53.779Z",
                    "hidden": false
                },
                {
                    "_id": "68f5cb858589920bf4d321c7",
                    "user": {
                        "_id": "66e5476b8edfc6dc4461af24",
                        "avatarUrl": "/avatars/7dbcc2465a6268ebeac118c396581fd1.svg",
                        "isPro": false,
                        "fullname": "Ki Kwang Min",
                        "user": "kiikiik",
                        "type": "user"
                    },
                    "name": "Kwangmin Ki",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:04:48.478Z",
                    "hidden": false
                },
                {
                    "_id": "68f5cb858589920bf4d321c8",
                    "name": "Junghyun Lee",
                    "hidden": false
                },
                {
                    "_id": "68f5cb858589920bf4d321c9",
                    "name": "Eunho Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T06:18:29.000Z",
            "submittedOnDailyAt": "2025-10-21T03:22:40.496Z",
            "title": "When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM\n  Ensembling",
            "submittedOnDailyBy": {
                "_id": "67f778ddbb19958f5d96c2a8",
                "avatarUrl": "/avatars/49a3f119b456ff94f28f09b2fe78bb18.svg",
                "isPro": false,
                "fullname": "Heecheol Yun",
                "user": "yoon6503",
                "type": "user"
            },
            "summary": "Ensembling Large Language Models (LLMs) has gained attention as a promising\napproach to surpass the performance of individual models by leveraging their\ncomplementary strengths. In particular, aggregating models' next-token\nprobability distributions to select the next token has been shown to be\neffective in various tasks. However, while successful for short-form answers,\nits application to long-form generation remains underexplored. In this paper,\nwe show that using existing ensemble methods in long-form generation requires a\ncareful choice of ensembling positions, since the standard practice of\nensembling at every token often degrades performance. We identify two key\nfactors for determining these positions: tokenization mismatch across models\nand consensus in their next-token probability distributions. Based on this, we\npropose SAFE, (Stable And Fast LLM Ensembling), a framework that selectively\nensembles by jointly considering these factors. To further improve stability,\nwe introduce a probability sharpening strategy that consolidates probabilities\nspread across multiple sub-word tokens representing the same word into a single\nrepresentative token. Our experiments on diverse benchmarks, including MATH500\nand BBH, demonstrate that SAFE outperforms existing methods in both accuracy\nand efficiency, with gains achieved even when ensembling fewer than 1% of\ntokens.",
            "upvotes": 27,
            "discussionId": "68f5cb858589920bf4d321ca",
            "ai_summary": "SAFE, a selective ensembling framework for large language models, improves long-form generation by considering tokenization mismatch and consensus in probability distributions, leading to better accuracy and efficiency.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "ensembling",
                "next-token probability distributions",
                "long-form generation",
                "tokenization mismatch",
                "consensus",
                "probability sharpening",
                "MATH500",
                "BBH"
            ],
            "organization": {
                "_id": "6475760c33192631bad2bb38",
                "name": "kaist-ai",
                "fullname": "KAIST AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
            }
        },
        "publishedAt": "2025-10-17T02:18:29.000Z",
        "title": "When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM\n  Ensembling",
        "summary": "Ensembling Large Language Models (LLMs) has gained attention as a promising\napproach to surpass the performance of individual models by leveraging their\ncomplementary strengths. In particular, aggregating models' next-token\nprobability distributions to select the next token has been shown to be\neffective in various tasks. However, while successful for short-form answers,\nits application to long-form generation remains underexplored. In this paper,\nwe show that using existing ensemble methods in long-form generation requires a\ncareful choice of ensembling positions, since the standard practice of\nensembling at every token often degrades performance. We identify two key\nfactors for determining these positions: tokenization mismatch across models\nand consensus in their next-token probability distributions. Based on this, we\npropose SAFE, (Stable And Fast LLM Ensembling), a framework that selectively\nensembles by jointly considering these factors. To further improve stability,\nwe introduce a probability sharpening strategy that consolidates probabilities\nspread across multiple sub-word tokens representing the same word into a single\nrepresentative token. Our experiments on diverse benchmarks, including MATH500\nand BBH, demonstrate that SAFE outperforms existing methods in both accuracy\nand efficiency, with gains achieved even when ensembling fewer than 1% of\ntokens.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15346.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "67f778ddbb19958f5d96c2a8",
            "avatarUrl": "/avatars/49a3f119b456ff94f28f09b2fe78bb18.svg",
            "fullname": "Heecheol Yun",
            "name": "yoon6503",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6475760c33192631bad2bb38",
            "name": "kaist-ai",
            "fullname": "KAIST AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.17715",
            "authors": [
                {
                    "_id": "68f7545d24c4489363111c55",
                    "user": {
                        "_id": "63898876f7d3b0df09221ed0",
                        "avatarUrl": "/avatars/f7a9fe8a7be30bd07a10df2b286a0711.svg",
                        "isPro": true,
                        "fullname": "Hanxu Hu",
                        "user": "HanxuHU",
                        "type": "user"
                    },
                    "name": "Hanxu Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:00:56.944Z",
                    "hidden": false
                },
                {
                    "_id": "68f7545d24c4489363111c56",
                    "user": {
                        "_id": "64181f3d57c3a491a4d6991a",
                        "avatarUrl": "/avatars/8fc0b42c8a71f403dd3b77667841242f.svg",
                        "isPro": false,
                        "fullname": "Xingxing Zhang",
                        "user": "fantasyorg",
                        "type": "user"
                    },
                    "name": "Xingxing Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:01:01.116Z",
                    "hidden": false
                },
                {
                    "_id": "68f7545d24c4489363111c57",
                    "name": "Jannis Vamvas",
                    "hidden": false
                },
                {
                    "_id": "68f7545d24c4489363111c58",
                    "name": "Rico Sennrich",
                    "hidden": false
                },
                {
                    "_id": "68f7545d24c4489363111c59",
                    "name": "Furu Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T16:29:53.000Z",
            "submittedOnDailyAt": "2025-10-21T08:25:20.740Z",
            "title": "QueST: Incentivizing LLMs to Generate Difficult Problems",
            "submittedOnDailyBy": {
                "_id": "622a41f973f278bb2fb1c6a4",
                "avatarUrl": "/avatars/f4945fb277e60cc065ad33c9fcc5be03.svg",
                "isPro": false,
                "fullname": "Jannis Vamvas",
                "user": "jvamvas",
                "type": "user"
            },
            "summary": "Large Language Models have achieved strong performance on reasoning tasks,\nsolving competition-level coding and math problems. However, their scalability\nis limited by human-labeled datasets and the lack of large-scale, challenging\ncoding problem training data. Existing competitive coding datasets contain only\nthousands to tens of thousands of problems. Previous synthetic data generation\nmethods rely on either augmenting existing instruction datasets or selecting\nchallenging problems from human-labeled data. In this paper, we propose QueST,\na novel framework which combines difficulty-aware graph sampling and\ndifficulty-aware rejection fine-tuning that directly optimizes specialized\ngenerators to create challenging coding problems. Our trained generators\ndemonstrate superior capability compared to even GPT-4o at creating challenging\nproblems that benefit downstream performance. We leverage QueST to generate\nlarge-scale synthetic coding problems, which we then use to distill from strong\nteacher models with long chain-of-thought or to conduct reinforcement learning\nfor smaller models, proving effective in both scenarios. Our distillation\nexperiments demonstrate significant performance gains. Specifically, after\nfine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we\nsurpass the performance of the original Qwen3-8B on LiveCodeBench. With an\nadditional 112K examples (i.e., 28K human-written problems paired with multiple\nsynthetic solutions), our 8B model matches the performance of the much larger\nDeepSeek-R1-671B. These findings indicate that generating complex problems via\nQueST offers an effective and scalable approach to advancing the frontiers of\ncompetitive coding and reasoning for large language models.",
            "upvotes": 26,
            "discussionId": "68f7545e24c4489363111c5a",
            "ai_summary": "QueST, a framework combining difficulty-aware graph sampling and fine-tuning, generates large-scale synthetic coding problems to enhance the performance of large language models in competitive coding and reasoning tasks.",
            "ai_keywords": [
                "difficulty-aware graph sampling",
                "difficulty-aware rejection fine-tuning",
                "QueST",
                "synthetic data generation",
                "chain-of-thought",
                "reinforcement learning",
                "distillation",
                "LiveCodeBench",
                "DeepSeek-R1-671B"
            ],
            "organization": {
                "_id": "5e6485f787403103f9f1055e",
                "name": "microsoft",
                "fullname": "Microsoft",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
            }
        },
        "publishedAt": "2025-10-20T12:29:53.000Z",
        "title": "QueST: Incentivizing LLMs to Generate Difficult Problems",
        "summary": "Large Language Models have achieved strong performance on reasoning tasks,\nsolving competition-level coding and math problems. However, their scalability\nis limited by human-labeled datasets and the lack of large-scale, challenging\ncoding problem training data. Existing competitive coding datasets contain only\nthousands to tens of thousands of problems. Previous synthetic data generation\nmethods rely on either augmenting existing instruction datasets or selecting\nchallenging problems from human-labeled data. In this paper, we propose QueST,\na novel framework which combines difficulty-aware graph sampling and\ndifficulty-aware rejection fine-tuning that directly optimizes specialized\ngenerators to create challenging coding problems. Our trained generators\ndemonstrate superior capability compared to even GPT-4o at creating challenging\nproblems that benefit downstream performance. We leverage QueST to generate\nlarge-scale synthetic coding problems, which we then use to distill from strong\nteacher models with long chain-of-thought or to conduct reinforcement learning\nfor smaller models, proving effective in both scenarios. Our distillation\nexperiments demonstrate significant performance gains. Specifically, after\nfine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we\nsurpass the performance of the original Qwen3-8B on LiveCodeBench. With an\nadditional 112K examples (i.e., 28K human-written problems paired with multiple\nsynthetic solutions), our 8B model matches the performance of the much larger\nDeepSeek-R1-671B. These findings indicate that generating complex problems via\nQueST offers an effective and scalable approach to advancing the frontiers of\ncompetitive coding and reasoning for large language models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17715.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "622a41f973f278bb2fb1c6a4",
            "avatarUrl": "/avatars/f4945fb277e60cc065ad33c9fcc5be03.svg",
            "fullname": "Jannis Vamvas",
            "name": "jvamvas",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "organization": {
            "_id": "5e6485f787403103f9f1055e",
            "name": "microsoft",
            "fullname": "Microsoft",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.16751",
            "authors": [
                {
                    "_id": "68f6ea4a24c4489363111810",
                    "user": {
                        "_id": "647076753601bb7b06653691",
                        "avatarUrl": "/avatars/334940cbd07b084444b0eb227f88768d.svg",
                        "isPro": false,
                        "fullname": "Erik Riise",
                        "user": "Eririi",
                        "type": "user"
                    },
                    "name": "Erik Riise",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:04:09.278Z",
                    "hidden": false
                },
                {
                    "_id": "68f6ea4a24c4489363111811",
                    "user": {
                        "_id": "63be9021da08ed0544f36c38",
                        "avatarUrl": "/avatars/42511d8b1d3a3ef99bc154c98b72dfba.svg",
                        "isPro": false,
                        "fullname": "onurcan",
                        "user": "monurcan",
                        "type": "user"
                    },
                    "name": "Mehmet Onurcan Kaya",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:04:13.043Z",
                    "hidden": false
                },
                {
                    "_id": "68f6ea4a24c4489363111812",
                    "name": "Dim P. Papadopoulos",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-19T08:28:06.000Z",
            "submittedOnDailyAt": "2025-10-21T00:35:50.068Z",
            "title": "Visual Autoregressive Models Beat Diffusion Models on Inference Time\n  Scaling",
            "submittedOnDailyBy": {
                "_id": "63be9021da08ed0544f36c38",
                "avatarUrl": "/avatars/42511d8b1d3a3ef99bc154c98b72dfba.svg",
                "isPro": false,
                "fullname": "onurcan",
                "user": "monurcan",
                "type": "user"
            },
            "summary": "While inference-time scaling through search has revolutionized Large Language\nModels, translating these gains to image generation has proven difficult.\nRecent attempts to apply search strategies to continuous diffusion models show\nlimited benefits, with simple random sampling often performing best. We\ndemonstrate that the discrete, sequential nature of visual autoregressive\nmodels enables effective search for image generation. We show that beam search\nsubstantially improves text-to-image generation, enabling a 2B parameter\nautoregressive model to outperform a 12B parameter diffusion model across\nbenchmarks. Systematic ablations show that this advantage comes from the\ndiscrete token space, which allows early pruning and computational reuse, and\nour verifier analysis highlights trade-offs between speed and reasoning\ncapability. These findings suggest that model architecture, not just scale, is\ncritical for inference-time optimization in visual generation.",
            "upvotes": 19,
            "discussionId": "68f6ea4b24c4489363111813",
            "projectPage": "https://erir11.github.io/visual-autoregressive-search/",
            "ai_summary": "Beam search in discrete visual autoregressive models enhances text-to-image generation more effectively than search in continuous diffusion models, highlighting architecture's importance over scale.",
            "ai_keywords": [
                "visual autoregressive models",
                "beam search",
                "diffusion models",
                "discrete token space",
                "early pruning",
                "computational reuse",
                "verifier analysis"
            ]
        },
        "publishedAt": "2025-10-19T04:28:06.000Z",
        "title": "Visual Autoregressive Models Beat Diffusion Models on Inference Time\n  Scaling",
        "summary": "While inference-time scaling through search has revolutionized Large Language\nModels, translating these gains to image generation has proven difficult.\nRecent attempts to apply search strategies to continuous diffusion models show\nlimited benefits, with simple random sampling often performing best. We\ndemonstrate that the discrete, sequential nature of visual autoregressive\nmodels enables effective search for image generation. We show that beam search\nsubstantially improves text-to-image generation, enabling a 2B parameter\nautoregressive model to outperform a 12B parameter diffusion model across\nbenchmarks. Systematic ablations show that this advantage comes from the\ndiscrete token space, which allows early pruning and computational reuse, and\nour verifier analysis highlights trade-offs between speed and reasoning\ncapability. These findings suggest that model architecture, not just scale, is\ncritical for inference-time optimization in visual generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16751.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63be9021da08ed0544f36c38",
            "avatarUrl": "/avatars/42511d8b1d3a3ef99bc154c98b72dfba.svg",
            "fullname": "onurcan",
            "name": "monurcan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.16333",
            "authors": [
                {
                    "_id": "68f6fb4824c4489363111942",
                    "name": "Junha Song",
                    "hidden": false
                },
                {
                    "_id": "68f6fb4824c4489363111943",
                    "name": "Sangdoo Yun",
                    "hidden": false
                },
                {
                    "_id": "68f6fb4824c4489363111944",
                    "name": "Dongyoon Han",
                    "hidden": false
                },
                {
                    "_id": "68f6fb4824c4489363111945",
                    "name": "Jaegul Choo",
                    "hidden": false
                },
                {
                    "_id": "68f6fb4824c4489363111946",
                    "user": {
                        "_id": "64b9feed96676e40d0fa89a7",
                        "avatarUrl": "/avatars/2154a6ceb87677ad2c9d9620de5b18ec.svg",
                        "isPro": false,
                        "fullname": "Byeongho Heo",
                        "user": "bhheo",
                        "type": "user"
                    },
                    "name": "Byeongho Heo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:03:22.750Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-18T03:37:17.000Z",
            "submittedOnDailyAt": "2025-10-21T01:47:47.175Z",
            "title": "RL makes MLLMs see better than SFT",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "A dominant assumption in Multimodal Language Model (MLLM) research is that\nits performance is largely inherited from the LLM backbone, given its immense\nparameter scale and remarkable capabilities. This has created a void in the\nunderstanding of the vision encoder, which determines how MLLMs perceive\nimages. The recent shift in MLLM training paradigms, from Supervised Finetuning\n(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the\nsignificant lack of analysis on how such training reshapes the vision encoder\nas well as the MLLM. To address this, we first investigate the impact of\ntraining strategies on MLLMs, where RL shows a clear advantage over SFT in\nstrongly vision-related VQA benchmarks. Motivated by this, we conduct a\ncritical yet under-explored analysis of the vision encoder of MLLMs through\ndiverse and in-depth experiments, ranging from ImageNet classification and\nsegmentation to gradient visualization. Our results demonstrate that MLLM's\npost-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on\nMLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual\nrepresentations. Specifically, the key finding of our study is that RL produces\nstronger and precisely localized visual representations compared to SFT,\nboosting the ability of the vision encoder for MLLM. We then reframe our\nfindings into a simple recipe for building strong vision encoders for MLLMs,\nPreference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,\na PIVOT-trained vision encoder outperforms even larger and more heavily-trained\ncounterparts, despite requiring less than 1% of the computational cost of\nstandard vision pretraining. This result opens an effective and efficient path\nfor advancing the vision backbones of MLLMs. Project page available at\nhttps://june-page.github.io/pivot/",
            "upvotes": 18,
            "discussionId": "68f6fb4824c4489363111947",
            "projectPage": "https://june-page.github.io/pivot/",
            "ai_summary": "Reinforcement Learning enhances vision encoders in Multimodal Language Models, leading to better visual representations and performance compared to Supervised Fine-tuning.",
            "ai_keywords": [
                "Multimodal Language Model",
                "LLM backbone",
                "vision encoder",
                "Supervised Finetuning",
                "Reinforcement Learning",
                "VQA benchmarks",
                "ImageNet classification",
                "segmentation",
                "gradient visualization",
                "visual representations",
                "Preference-Instructed Vision OpTimization",
                "PIVOT"
            ],
            "organization": {
                "_id": "64ffe603efd273eec7768bde",
                "name": "naver-ai",
                "fullname": "NAVER AI Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ff1755b75685dd7a46e146/Zj2bxgq31oSqwVrw16IE_.png"
            }
        },
        "publishedAt": "2025-10-17T23:37:17.000Z",
        "title": "RL makes MLLMs see better than SFT",
        "summary": "A dominant assumption in Multimodal Language Model (MLLM) research is that\nits performance is largely inherited from the LLM backbone, given its immense\nparameter scale and remarkable capabilities. This has created a void in the\nunderstanding of the vision encoder, which determines how MLLMs perceive\nimages. The recent shift in MLLM training paradigms, from Supervised Finetuning\n(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the\nsignificant lack of analysis on how such training reshapes the vision encoder\nas well as the MLLM. To address this, we first investigate the impact of\ntraining strategies on MLLMs, where RL shows a clear advantage over SFT in\nstrongly vision-related VQA benchmarks. Motivated by this, we conduct a\ncritical yet under-explored analysis of the vision encoder of MLLMs through\ndiverse and in-depth experiments, ranging from ImageNet classification and\nsegmentation to gradient visualization. Our results demonstrate that MLLM's\npost-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on\nMLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual\nrepresentations. Specifically, the key finding of our study is that RL produces\nstronger and precisely localized visual representations compared to SFT,\nboosting the ability of the vision encoder for MLLM. We then reframe our\nfindings into a simple recipe for building strong vision encoders for MLLMs,\nPreference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,\na PIVOT-trained vision encoder outperforms even larger and more heavily-trained\ncounterparts, despite requiring less than 1% of the computational cost of\nstandard vision pretraining. This result opens an effective and efficient path\nfor advancing the vision backbones of MLLMs. Project page available at\nhttps://june-page.github.io/pivot/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16333.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 134
        },
        "organization": {
            "_id": "64ffe603efd273eec7768bde",
            "name": "naver-ai",
            "fullname": "NAVER AI Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ff1755b75685dd7a46e146/Zj2bxgq31oSqwVrw16IE_.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.17509",
            "authors": [
                {
                    "_id": "68f704d324c4489363111991",
                    "user": {
                        "_id": "64b75440da8017900e76a073",
                        "avatarUrl": "/avatars/28b4096bf13c4fdcd560015a79cd8332.svg",
                        "isPro": false,
                        "fullname": "Shiyu Ni",
                        "user": "Shiyunee",
                        "type": "user"
                    },
                    "name": "Shiyu Ni",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:03:08.864Z",
                    "hidden": false
                },
                {
                    "_id": "68f704d324c4489363111992",
                    "name": "Keping Bi",
                    "hidden": false
                },
                {
                    "_id": "68f704d324c4489363111993",
                    "name": "Jiafeng Guo",
                    "hidden": false
                },
                {
                    "_id": "68f704d324c4489363111994",
                    "user": {
                        "_id": "66544459258f5fad314c56bb",
                        "avatarUrl": "/avatars/283212e48ff267ecb19a5d1ab1ffd29d.svg",
                        "isPro": false,
                        "fullname": "Minghao Tang",
                        "user": "StudentTang",
                        "type": "user"
                    },
                    "name": "Minghao Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:03:05.642Z",
                    "hidden": false
                },
                {
                    "_id": "68f704d324c4489363111995",
                    "name": "Jingtong Wu",
                    "hidden": false
                },
                {
                    "_id": "68f704d324c4489363111996",
                    "name": "Zengxin Han",
                    "hidden": false
                },
                {
                    "_id": "68f704d324c4489363111997",
                    "name": "Xueqi Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T13:05:22.000Z",
            "submittedOnDailyAt": "2025-10-21T02:31:10.938Z",
            "title": "Annotation-Efficient Universal Honesty Alignment",
            "submittedOnDailyBy": {
                "_id": "616bfc2b40e2f69baa1c7add",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616bfc2b40e2f69baa1c7add/Os7_qgMei-2lRVelrOG7B.jpeg",
                "isPro": false,
                "fullname": "Run-Ze Fan",
                "user": "Vfrz",
                "type": "user"
            },
            "summary": "Honesty alignment-the ability of large language models (LLMs) to recognize\ntheir knowledge boundaries and express calibrated confidence-is essential for\ntrustworthy deployment. Existing methods either rely on training-free\nconfidence estimation (e.g., token probabilities, self-consistency) or\ntraining-based calibration with correctness annotations. While effective,\nachieving universal honesty alignment with training-based calibration requires\ncostly, large-scale labeling. To support annotation-efficient training, we\nintroduce Elicitation-Then-Calibration (EliCal), a two-stage framework that\nfirst elicits internal confidence using inexpensive self-consistency\nsupervision, then calibrates this confidence with a small set of correctness\nannotations. To support a large-scale study, we release HonestyBench, a\nbenchmark covering ten free-form QA datasets with 560k training and 70k\nevaluation instances annotated with correctness and self-consistency signals.\nExperiments show that EliCal achieves near-optimal alignment with only 1k\ncorrectness annotations (0.18% of full supervision) and better alignment\nperformance on unseen MMLU tasks than the calibration-only baseline, offering a\nscalable solution toward universal honesty alignment in LLMs.",
            "upvotes": 17,
            "discussionId": "68f704d324c4489363111998",
            "ai_summary": "EliCal, a two-stage framework combining self-consistency supervision and minimal correctness annotations, achieves near-optimal honesty alignment in large language models with limited annotation effort.",
            "ai_keywords": [
                "large language models",
                "honesty alignment",
                "confidence estimation",
                "self-consistency",
                "calibration",
                "correctness annotations",
                "Elicitation-Then-Calibration",
                "EliCal",
                "HonestyBench",
                "free-form QA datasets",
                "MMLU tasks"
            ]
        },
        "publishedAt": "2025-10-20T09:05:22.000Z",
        "title": "Annotation-Efficient Universal Honesty Alignment",
        "summary": "Honesty alignment-the ability of large language models (LLMs) to recognize\ntheir knowledge boundaries and express calibrated confidence-is essential for\ntrustworthy deployment. Existing methods either rely on training-free\nconfidence estimation (e.g., token probabilities, self-consistency) or\ntraining-based calibration with correctness annotations. While effective,\nachieving universal honesty alignment with training-based calibration requires\ncostly, large-scale labeling. To support annotation-efficient training, we\nintroduce Elicitation-Then-Calibration (EliCal), a two-stage framework that\nfirst elicits internal confidence using inexpensive self-consistency\nsupervision, then calibrates this confidence with a small set of correctness\nannotations. To support a large-scale study, we release HonestyBench, a\nbenchmark covering ten free-form QA datasets with 560k training and 70k\nevaluation instances annotated with correctness and self-consistency signals.\nExperiments show that EliCal achieves near-optimal alignment with only 1k\ncorrectness annotations (0.18% of full supervision) and better alignment\nperformance on unseen MMLU tasks than the calibration-only baseline, offering a\nscalable solution toward universal honesty alignment in LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17509.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "616bfc2b40e2f69baa1c7add",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616bfc2b40e2f69baa1c7add/Os7_qgMei-2lRVelrOG7B.jpeg",
            "fullname": "Run-Ze Fan",
            "name": "Vfrz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.17960",
            "authors": [
                {
                    "_id": "68f823477669bcaeecce0b78",
                    "name": "Liam Parker",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b79",
                    "name": "Francois Lanusse",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b7a",
                    "name": "Jeff Shen",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b7b",
                    "name": "Ollie Liu",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b7c",
                    "name": "Tom Hehir",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b7d",
                    "name": "Leopoldo Sarra",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b7e",
                    "name": "Lucas Meyer",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b7f",
                    "name": "Micah Bowles",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b80",
                    "name": "Sebastian Wagner-Carena",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b81",
                    "name": "Helen Qu",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b82",
                    "name": "Siavash Golkar",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b83",
                    "name": "Alberto Bietti",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b84",
                    "name": "Hatim Bourfoune",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b85",
                    "name": "Nathan Casserau",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b86",
                    "name": "Pierre Cornette",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b87",
                    "name": "Keiya Hirashima",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b88",
                    "name": "Geraud Krawezik",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b89",
                    "name": "Ruben Ohana",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b8a",
                    "name": "Nicholas Lourie",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b8b",
                    "name": "Michael McCabe",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b8c",
                    "name": "Rudy Morel",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b8d",
                    "name": "Payel Mukhopadhyay",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b8e",
                    "name": "Mariel Pettee",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b8f",
                    "name": "Bruno Regaldo-Saint Blancard",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b90",
                    "name": "Kyunghyun Cho",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b91",
                    "name": "Miles Cranmer",
                    "hidden": false
                },
                {
                    "_id": "68f823477669bcaeecce0b92",
                    "name": "Shirley Ho",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T18:00:00.000Z",
            "submittedOnDailyAt": "2025-10-21T22:53:21.973Z",
            "title": "AION-1: Omnimodal Foundation Model for Astronomical Sciences",
            "submittedOnDailyBy": {
                "_id": "64b32a43a248169796f54c46",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lTu8XKxfa1pY6HEuQNaYN.jpeg",
                "isPro": false,
                "fullname": "Francois Lanusse",
                "user": "EiffL",
                "type": "user"
            },
            "summary": "While foundation models have shown promise across a variety of fields,\nastronomy still lacks a unified framework for joint modeling across its highly\ndiverse data modalities. In this paper, we present AION-1, a family of\nlarge-scale multimodal foundation models for astronomy. AION-1 integrates\nheterogeneous imaging, spectroscopic, and scalar data using a two-stage\narchitecture: modality-specific tokenization followed by transformer-based\nmasked modeling of cross-modal token sequences. The model is pretrained on five\nlarge-scale surveys: Legacy Survey, Hyper Suprime-Cam (HSC), Sloan Digital Sky\nSurvey (SDSS), Dark Energy Spectroscopic Instrument (DESI), and Gaia. These\nspan more than 200 million observations of stars, galaxies, and quasars. With a\nsingle frozen encoder, AION-1 achieves strong results on a broad suite of\ndownstream tasks, including galaxy and stellar property estimation, galaxy\nmorphology classification, similarity-based retrieval, galaxy image\nsegmentation, and spectral super-resolution. We release AION-1 model variants\nranging from 300 M to 3.1 B parameters. Beyond astronomy, AION-1 provides a\nscalable blueprint for multimodal scientific foundation models that can\nseamlessly integrate noisy, instrument-specific observations. All code,\ntokenizers, pretrained weights, and a lightweight evaluation suite are released\nunder an open-source license.",
            "upvotes": 15,
            "discussionId": "68f823487669bcaeecce0b93",
            "githubRepo": "https://github.com/PolymathicAI/AION",
            "ai_summary": "AION-1, a family of large-scale multimodal foundation models, integrates diverse astronomical data using tokenization and transformer-based modeling, achieving strong performance across various downstream tasks.",
            "ai_keywords": [
                "multimodal foundation models",
                "tokenization",
                "transformer-based",
                "masked modeling",
                "cross-modal token sequences",
                "pretrained",
                "galaxy and stellar property estimation",
                "galaxy morphology classification",
                "similarity-based retrieval",
                "galaxy image segmentation",
                "spectral super-resolution"
            ],
            "githubStars": 11,
            "organization": {
                "_id": "66632af0d71a4e1e6cc26850",
                "name": "polymathic-ai",
                "fullname": "Polymathic AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66632a958dcc61453b126b03/OWEOuGHlRbGWy4oxF0tA-.png"
            }
        },
        "publishedAt": "2025-10-20T14:00:00.000Z",
        "title": "AION-1: Omnimodal Foundation Model for Astronomical Sciences",
        "summary": "While foundation models have shown promise across a variety of fields,\nastronomy still lacks a unified framework for joint modeling across its highly\ndiverse data modalities. In this paper, we present AION-1, a family of\nlarge-scale multimodal foundation models for astronomy. AION-1 integrates\nheterogeneous imaging, spectroscopic, and scalar data using a two-stage\narchitecture: modality-specific tokenization followed by transformer-based\nmasked modeling of cross-modal token sequences. The model is pretrained on five\nlarge-scale surveys: Legacy Survey, Hyper Suprime-Cam (HSC), Sloan Digital Sky\nSurvey (SDSS), Dark Energy Spectroscopic Instrument (DESI), and Gaia. These\nspan more than 200 million observations of stars, galaxies, and quasars. With a\nsingle frozen encoder, AION-1 achieves strong results on a broad suite of\ndownstream tasks, including galaxy and stellar property estimation, galaxy\nmorphology classification, similarity-based retrieval, galaxy image\nsegmentation, and spectral super-resolution. We release AION-1 model variants\nranging from 300 M to 3.1 B parameters. Beyond astronomy, AION-1 provides a\nscalable blueprint for multimodal scientific foundation models that can\nseamlessly integrate noisy, instrument-specific observations. All code,\ntokenizers, pretrained weights, and a lightweight evaluation suite are released\nunder an open-source license.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17960.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64b32a43a248169796f54c46",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lTu8XKxfa1pY6HEuQNaYN.jpeg",
            "fullname": "Francois Lanusse",
            "name": "EiffL",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "organization": {
            "_id": "66632af0d71a4e1e6cc26850",
            "name": "polymathic-ai",
            "fullname": "Polymathic AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66632a958dcc61453b126b03/OWEOuGHlRbGWy4oxF0tA-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.16888",
            "authors": [
                {
                    "_id": "68f7036a24c4489363111985",
                    "name": "Zongjian Li",
                    "hidden": false
                },
                {
                    "_id": "68f7036a24c4489363111986",
                    "name": "Zheyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68f7036a24c4489363111987",
                    "user": {
                        "_id": "67a99d1fef1439e285c4cbec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
                        "isPro": false,
                        "fullname": "Qihui Zhang",
                        "user": "77Hui",
                        "type": "user"
                    },
                    "name": "Qihui Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:03:12.558Z",
                    "hidden": false
                },
                {
                    "_id": "68f7036a24c4489363111988",
                    "user": {
                        "_id": "6367a8175bb06007ea099b8f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6367a8175bb06007ea099b8f/IjG7HyWyWRlVt_XwRbxRW.jpeg",
                        "isPro": false,
                        "fullname": "linbin",
                        "user": "LanguageBind",
                        "type": "user"
                    },
                    "name": "Bin Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:03:19.442Z",
                    "hidden": false
                },
                {
                    "_id": "68f7036a24c4489363111989",
                    "user": {
                        "_id": "63468720dd6d90d82ccf3450",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                        "isPro": false,
                        "fullname": "YSH",
                        "user": "BestWishYsh",
                        "type": "user"
                    },
                    "name": "Shenghai Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:03:16.291Z",
                    "hidden": false
                },
                {
                    "_id": "68f7036a24c448936311198a",
                    "name": "Zhiyuan Yan",
                    "hidden": false
                },
                {
                    "_id": "68f7036a24c448936311198b",
                    "name": "Yang Ye",
                    "hidden": false
                },
                {
                    "_id": "68f7036a24c448936311198c",
                    "name": "Wangbo Yu",
                    "hidden": false
                },
                {
                    "_id": "68f7036a24c448936311198d",
                    "name": "Yuwei Niu",
                    "hidden": false
                },
                {
                    "_id": "68f7036a24c448936311198e",
                    "name": "Li Yuan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-19T15:38:06.000Z",
            "submittedOnDailyAt": "2025-10-21T02:23:34.106Z",
            "title": "Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware\n  Finetuning and MLLM Implicit Feedback",
            "submittedOnDailyBy": {
                "_id": "646df3c04ad7f907279f14c3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646df3c04ad7f907279f14c3/WZjSDtAmezmjbczLtCP2B.jpeg",
                "isPro": false,
                "fullname": "Zongjian Li",
                "user": "chestnutlzj",
                "type": "user"
            },
            "summary": "Instruction-based image editing has achieved remarkable progress; however,\nmodels solely trained via supervised fine-tuning often overfit to annotated\npatterns, hindering their ability to explore and generalize beyond training\ndistributions. To this end, we introduce Edit-R1, a novel post-training\nframework for instruction-based image editing based on policy optimization.\nSpecifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a\nlikelihood-free policy optimization method consistent with the flow matching\nforward process, thereby enabling the use of higher-order samplers and more\nefficient training. Another key challenge here is the absence of a universal\nreward model, resulting from the diverse nature of editing instructions and\ntasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)\nas a unified, training-free reward model, leveraging its output logits to\nprovide fine-grained feedback. Furthermore, we carefully design a low-variance\ngroup filtering mechanism to reduce MLLM scoring noise and stabilize\noptimization. UniWorld-V2, trained with this framework, achieves\nstate-of-the-art results on the ImgEdit and GEdit-Bench benchmarks,\nscoring 4.49 and 7.83, respectively. Crucially, our framework is\nmodel-agnostic, delivering substantial performance gains when applied to\ndiverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its\nwide applicability. Code and models are publicly available at\nhttps://github.com/PKU-YuanGroup/UniWorld-V2.",
            "upvotes": 15,
            "discussionId": "68f7036a24c448936311198f",
            "githubRepo": "https://github.com/PKU-YuanGroup/UniWorld-V2",
            "ai_summary": "Edit-R1, a post-training framework using Diffusion Negative-aware Finetuning and a Multimodal Large Language Model, achieves state-of-the-art results in instruction-based image editing by addressing overfitting and lack of a universal reward model.",
            "ai_keywords": [
                "Diffusion Negative-aware Finetuning",
                "DiffusionNFT",
                "flow matching",
                "higher-order samplers",
                "Multimodal Large Language Model",
                "MLLM",
                "low-variance group filtering",
                "UniWorld-V2",
                "ImgEdit",
                "GEdit-Bench"
            ],
            "githubStars": 46,
            "organization": {
                "_id": "61dcd8e344f59573371b5cb6",
                "name": "PekingUniversity",
                "fullname": "Peking University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
            }
        },
        "publishedAt": "2025-10-19T11:38:06.000Z",
        "title": "Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware\n  Finetuning and MLLM Implicit Feedback",
        "summary": "Instruction-based image editing has achieved remarkable progress; however,\nmodels solely trained via supervised fine-tuning often overfit to annotated\npatterns, hindering their ability to explore and generalize beyond training\ndistributions. To this end, we introduce Edit-R1, a novel post-training\nframework for instruction-based image editing based on policy optimization.\nSpecifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a\nlikelihood-free policy optimization method consistent with the flow matching\nforward process, thereby enabling the use of higher-order samplers and more\nefficient training. Another key challenge here is the absence of a universal\nreward model, resulting from the diverse nature of editing instructions and\ntasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)\nas a unified, training-free reward model, leveraging its output logits to\nprovide fine-grained feedback. Furthermore, we carefully design a low-variance\ngroup filtering mechanism to reduce MLLM scoring noise and stabilize\noptimization. UniWorld-V2, trained with this framework, achieves\nstate-of-the-art results on the ImgEdit and GEdit-Bench benchmarks,\nscoring 4.49 and 7.83, respectively. Crucially, our framework is\nmodel-agnostic, delivering substantial performance gains when applied to\ndiverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its\nwide applicability. Code and models are publicly available at\nhttps://github.com/PKU-YuanGroup/UniWorld-V2.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16888.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646df3c04ad7f907279f14c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646df3c04ad7f907279f14c3/WZjSDtAmezmjbczLtCP2B.jpeg",
            "fullname": "Zongjian Li",
            "name": "chestnutlzj",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.17803",
            "authors": [
                {
                    "_id": "68f6e6e924c44893631117db",
                    "user": {
                        "_id": "6503ccaf13d750b4604649e4",
                        "avatarUrl": "/avatars/96bebff9284d61f37c83e7da6a7e9bac.svg",
                        "isPro": false,
                        "fullname": "Zixin Yin",
                        "user": "zachary-yin",
                        "type": "user"
                    },
                    "name": "Zixin Yin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:04:19.320Z",
                    "hidden": false
                },
                {
                    "_id": "68f6e6e924c44893631117dc",
                    "name": "Ling-Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "68f6e6e924c44893631117dd",
                    "name": "Lionel Ni",
                    "hidden": false
                },
                {
                    "_id": "68f6e6e924c44893631117de",
                    "name": "Xili Dai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T17:59:52.000Z",
            "submittedOnDailyAt": "2025-10-21T00:43:49.416Z",
            "title": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing",
            "submittedOnDailyBy": {
                "_id": "6503ccaf13d750b4604649e4",
                "avatarUrl": "/avatars/96bebff9284d61f37c83e7da6a7e9bac.svg",
                "isPro": false,
                "fullname": "Zixin Yin",
                "user": "zachary-yin",
                "type": "user"
            },
            "summary": "Recent advances in training-free attention control methods have enabled\nflexible and efficient text-guided editing capabilities for existing generation\nmodels. However, current approaches struggle to simultaneously deliver strong\nediting strength while preserving consistency with the source. This limitation\nbecomes particularly critical in multi-round and video editing, where visual\nerrors can accumulate over time. Moreover, most existing methods enforce global\nconsistency, which limits their ability to modify individual attributes such as\ntexture while preserving others, thereby hindering fine-grained editing.\nRecently, the architectural shift from U-Net to MM-DiT has brought significant\nimprovements in generative performance and introduced a novel mechanism for\nintegrating text and vision modalities. These advancements pave the way for\novercoming challenges that previous methods failed to resolve. Through an\nin-depth analysis of MM-DiT, we identify three key insights into its attention\nmechanisms. Building on these, we propose ConsistEdit, a novel attention\ncontrol method specifically tailored for MM-DiT. ConsistEdit incorporates\nvision-only attention control, mask-guided pre-attention fusion, and\ndifferentiated manipulation of the query, key, and value tokens to produce\nconsistent, prompt-aligned edits. Extensive experiments demonstrate that\nConsistEdit achieves state-of-the-art performance across a wide range of image\nand video editing tasks, including both structure-consistent and\nstructure-inconsistent scenarios. Unlike prior methods, it is the first\napproach to perform editing across all inference steps and attention layers\nwithout handcraft, significantly enhancing reliability and consistency, which\nenables robust multi-round and multi-region editing. Furthermore, it supports\nprogressive adjustment of structural consistency, enabling finer control.",
            "upvotes": 11,
            "discussionId": "68f6e6ea24c44893631117df",
            "projectPage": "https://zxyin.github.io/ConsistEdit",
            "githubRepo": "https://github.com/zxYin/ConsistEdit_Code",
            "ai_summary": "ConsistEdit, a novel attention control method for MM-DiT, enhances image and video editing by ensuring consistency and fine-grained control across all inference steps and attention layers.",
            "ai_keywords": [
                "attention control",
                "MM-DiT",
                "vision-only attention control",
                "mask-guided pre-attention fusion",
                "query",
                "key",
                "value tokens",
                "structure-consistent",
                "structure-inconsistent",
                "multi-round editing",
                "multi-region editing",
                "progressive adjustment"
            ],
            "githubStars": 25
        },
        "publishedAt": "2025-10-20T13:59:52.000Z",
        "title": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing",
        "summary": "Recent advances in training-free attention control methods have enabled\nflexible and efficient text-guided editing capabilities for existing generation\nmodels. However, current approaches struggle to simultaneously deliver strong\nediting strength while preserving consistency with the source. This limitation\nbecomes particularly critical in multi-round and video editing, where visual\nerrors can accumulate over time. Moreover, most existing methods enforce global\nconsistency, which limits their ability to modify individual attributes such as\ntexture while preserving others, thereby hindering fine-grained editing.\nRecently, the architectural shift from U-Net to MM-DiT has brought significant\nimprovements in generative performance and introduced a novel mechanism for\nintegrating text and vision modalities. These advancements pave the way for\novercoming challenges that previous methods failed to resolve. Through an\nin-depth analysis of MM-DiT, we identify three key insights into its attention\nmechanisms. Building on these, we propose ConsistEdit, a novel attention\ncontrol method specifically tailored for MM-DiT. ConsistEdit incorporates\nvision-only attention control, mask-guided pre-attention fusion, and\ndifferentiated manipulation of the query, key, and value tokens to produce\nconsistent, prompt-aligned edits. Extensive experiments demonstrate that\nConsistEdit achieves state-of-the-art performance across a wide range of image\nand video editing tasks, including both structure-consistent and\nstructure-inconsistent scenarios. Unlike prior methods, it is the first\napproach to perform editing across all inference steps and attention layers\nwithout handcraft, significantly enhancing reliability and consistency, which\nenables robust multi-round and multi-region editing. Furthermore, it supports\nprogressive adjustment of structural consistency, enabling finer control.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17803.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6503ccaf13d750b4604649e4",
            "avatarUrl": "/avatars/96bebff9284d61f37c83e7da6a7e9bac.svg",
            "fullname": "Zixin Yin",
            "name": "zachary-yin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.17795",
            "authors": [
                {
                    "_id": "68f6f0d824c4489363111892",
                    "name": "Yujie Luo",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d824c4489363111893",
                    "name": "Zhuoyun Yu",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d824c4489363111894",
                    "name": "Xuehai Wang",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d824c4489363111895",
                    "name": "Yuqi Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d824c4489363111896",
                    "name": "Ningyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d824c4489363111897",
                    "name": "Lanning Wei",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d824c4489363111898",
                    "name": "Lun Du",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d824c4489363111899",
                    "name": "Da Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d824c448936311189a",
                    "name": "Huajun Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T17:53:23.000Z",
            "submittedOnDailyAt": "2025-10-21T01:03:47.002Z",
            "title": "Executable Knowledge Graphs for Replicating AI Research",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": true,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "Replicating AI research is a crucial yet challenging task for large language\nmodel (LLM) agents. Existing approaches often struggle to generate executable\ncode, primarily due to insufficient background knowledge and the limitations of\nretrieval-augmented generation (RAG) methods, which fail to capture latent\ntechnical details hidden in referenced papers. Furthermore, previous approaches\ntend to overlook valuable implementation-level code signals and lack structured\nknowledge representations that support multi-granular retrieval and reuse. To\novercome these challenges, we propose Executable Knowledge Graphs (xKG), a\nmodular and pluggable knowledge base that automatically integrates technical\ninsights, code snippets, and domain-specific knowledge extracted from\nscientific literature. When integrated into three agent frameworks with two\ndifferent LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on\nPaperBench, demonstrating its effectiveness as a general and extensible\nsolution for automated AI research replication. Code will released at\nhttps://github.com/zjunlp/xKG.",
            "upvotes": 9,
            "discussionId": "68f6f0d824c448936311189b",
            "githubRepo": "https://github.com/zjunlp/xKG",
            "ai_summary": "Executable Knowledge Graphs (xKG) enhance AI research replication by integrating technical insights and code snippets from scientific literature, improving performance in automated replication tasks.",
            "ai_keywords": [
                "large language model (LLM)",
                "retrieval-augmented generation (RAG)",
                "Executable Knowledge Graphs (xKG)",
                "technical insights",
                "code snippets",
                "domain-specific knowledge",
                "PaperBench",
                "o3-mini"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "67c1d682826160b28f778510",
                "name": "antgroup",
                "fullname": "Ant Group",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
            }
        },
        "publishedAt": "2025-10-20T13:53:23.000Z",
        "title": "Executable Knowledge Graphs for Replicating AI Research",
        "summary": "Replicating AI research is a crucial yet challenging task for large language\nmodel (LLM) agents. Existing approaches often struggle to generate executable\ncode, primarily due to insufficient background knowledge and the limitations of\nretrieval-augmented generation (RAG) methods, which fail to capture latent\ntechnical details hidden in referenced papers. Furthermore, previous approaches\ntend to overlook valuable implementation-level code signals and lack structured\nknowledge representations that support multi-granular retrieval and reuse. To\novercome these challenges, we propose Executable Knowledge Graphs (xKG), a\nmodular and pluggable knowledge base that automatically integrates technical\ninsights, code snippets, and domain-specific knowledge extracted from\nscientific literature. When integrated into three agent frameworks with two\ndifferent LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on\nPaperBench, demonstrating its effectiveness as a general and extensible\nsolution for automated AI research replication. Code will released at\nhttps://github.com/zjunlp/xKG.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17795.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 30
        },
        "organization": {
            "_id": "67c1d682826160b28f778510",
            "name": "antgroup",
            "fullname": "Ant Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.17498",
            "authors": [
                {
                    "_id": "68f6f00024c448936311187a",
                    "name": "Zihan Liu",
                    "hidden": false
                },
                {
                    "_id": "68f6f00024c448936311187b",
                    "user": {
                        "_id": "64a7a2bad001860e0c34f7f2",
                        "avatarUrl": "/avatars/927c71448c3f8a65532b387196a82f61.svg",
                        "isPro": false,
                        "fullname": "Shun Zheng",
                        "user": "shun-zheng",
                        "type": "user"
                    },
                    "name": "Shun Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:03:47.838Z",
                    "hidden": false
                },
                {
                    "_id": "68f6f00024c448936311187c",
                    "name": "Xumeng Wen",
                    "hidden": false
                },
                {
                    "_id": "68f6f00024c448936311187d",
                    "name": "Yang Wang",
                    "hidden": false
                },
                {
                    "_id": "68f6f00024c448936311187e",
                    "name": "Jiang Bian",
                    "hidden": false
                },
                {
                    "_id": "68f6f00024c448936311187f",
                    "name": "Mao Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T12:51:42.000Z",
            "submittedOnDailyAt": "2025-10-21T01:20:30.132Z",
            "title": "Deep Self-Evolving Reasoning",
            "submittedOnDailyBy": {
                "_id": "64a7a2bad001860e0c34f7f2",
                "avatarUrl": "/avatars/927c71448c3f8a65532b387196a82f61.svg",
                "isPro": false,
                "fullname": "Shun Zheng",
                "user": "shun-zheng",
                "type": "user"
            },
            "summary": "Long-form chain-of-thought reasoning has become a cornerstone of advanced\nreasoning in large language models. While recent verification-refinement\nframeworks have enabled proprietary models to solve Olympiad-level problems,\ntheir effectiveness hinges on strong, reliable verification and correction\ncapabilities, which remain fragile in open-weight, smaller-scale models. This\nwork demonstrates that even with weak verification and refinement capabilities\non hard tasks, the reasoning limits of such models can be substantially\nextended through a probabilistic paradigm we call Deep Self-Evolving Reasoning\n(DSER). We conceptualize iterative reasoning as a Markov chain, where each step\nrepresents a stochastic transition in the solution space. The key insight is\nthat convergence to a correct solution is guaranteed as long as the probability\nof improvement marginally exceeds that of degradation. By running multiple\nlong-horizon, self-evolving processes in parallel, DSER amplifies these small\npositive tendencies, enabling the model to asymptotically approach correct\nanswers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On\nthe challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously\nunsolvable problems and boosts overall performance, enabling this compact model\nto surpass the single-turn accuracy of its 600B-parameter teacher through\nmajority voting. Beyond its immediate utility for test-time scaling, the DSER\nframework serves to diagnose the fundamental limitations of current open-weight\nreasoners. By clearly delineating their shortcomings in self-verification,\nrefinement, and stability, our findings establish a clear research agenda for\ndeveloping next-generation models with powerful, intrinsic self-evolving\ncapabilities.",
            "upvotes": 8,
            "discussionId": "68f6f00124c4489363111880",
            "ai_summary": "Deep Self-Evolving Reasoning (DSER) extends the reasoning capabilities of smaller models by iteratively improving solutions through a probabilistic Markov chain, enabling them to solve previously unsolvable problems and surpass larger models in accuracy.",
            "ai_keywords": [
                "Deep Self-Evolving Reasoning",
                "DSER",
                "Markov chain",
                "iterative reasoning",
                "probabilistic paradigm",
                "self-evolving processes",
                "AIME 2024-2025 benchmark",
                "majority voting",
                "self-verification",
                "refinement",
                "stability"
            ],
            "organization": {
                "_id": "5e6485f787403103f9f1055e",
                "name": "microsoft",
                "fullname": "Microsoft",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
            }
        },
        "publishedAt": "2025-10-20T08:51:42.000Z",
        "title": "Deep Self-Evolving Reasoning",
        "summary": "Long-form chain-of-thought reasoning has become a cornerstone of advanced\nreasoning in large language models. While recent verification-refinement\nframeworks have enabled proprietary models to solve Olympiad-level problems,\ntheir effectiveness hinges on strong, reliable verification and correction\ncapabilities, which remain fragile in open-weight, smaller-scale models. This\nwork demonstrates that even with weak verification and refinement capabilities\non hard tasks, the reasoning limits of such models can be substantially\nextended through a probabilistic paradigm we call Deep Self-Evolving Reasoning\n(DSER). We conceptualize iterative reasoning as a Markov chain, where each step\nrepresents a stochastic transition in the solution space. The key insight is\nthat convergence to a correct solution is guaranteed as long as the probability\nof improvement marginally exceeds that of degradation. By running multiple\nlong-horizon, self-evolving processes in parallel, DSER amplifies these small\npositive tendencies, enabling the model to asymptotically approach correct\nanswers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On\nthe challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously\nunsolvable problems and boosts overall performance, enabling this compact model\nto surpass the single-turn accuracy of its 600B-parameter teacher through\nmajority voting. Beyond its immediate utility for test-time scaling, the DSER\nframework serves to diagnose the fundamental limitations of current open-weight\nreasoners. By clearly delineating their shortcomings in self-verification,\nrefinement, and stability, our findings establish a clear research agenda for\ndeveloping next-generation models with powerful, intrinsic self-evolving\ncapabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17498.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a7a2bad001860e0c34f7f2",
            "avatarUrl": "/avatars/927c71448c3f8a65532b387196a82f61.svg",
            "fullname": "Shun Zheng",
            "name": "shun-zheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "5e6485f787403103f9f1055e",
            "name": "microsoft",
            "fullname": "Microsoft",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.15821",
            "authors": [
                {
                    "_id": "68f5d4928589920bf4d321f2",
                    "name": "Abdul Fatir Ansari",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d321f3",
                    "user": {
                        "_id": "6494b81c5c21b3952bec6e9e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6494b81c5c21b3952bec6e9e/t8-MSbFfUC8X-0yRJrNpj.jpeg",
                        "isPro": false,
                        "fullname": "Oleksandr Shchur",
                        "user": "shchuro",
                        "type": "user"
                    },
                    "name": "Oleksandr Shchur",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:04:42.117Z",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d321f4",
                    "name": "Jaris K√ºken",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d321f5",
                    "name": "Andreas Auer",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d321f6",
                    "name": "Boran Han",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d321f7",
                    "name": "Pedro Mercado",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d321f8",
                    "name": "Syama Sundar Rangapuram",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d321f9",
                    "name": "Huibin Shen",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d321fa",
                    "name": "Lorenzo Stella",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d321fb",
                    "name": "Xiyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d321fc",
                    "name": "Mononito Goswami",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d321fd",
                    "name": "Shubham Kapoor",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d321fe",
                    "name": "Danielle C. Maddix",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d321ff",
                    "name": "Pablo Guerron",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d32200",
                    "name": "Tony Hu",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d32201",
                    "name": "Junming Yin",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d32202",
                    "name": "Nick Erickson",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d32203",
                    "name": "Prateek Mutalik Desai",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d32204",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d32205",
                    "name": "Huzefa Rangwala",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d32206",
                    "name": "George Karypis",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d32207",
                    "name": "Yuyang Wang",
                    "hidden": false
                },
                {
                    "_id": "68f5d4928589920bf4d32208",
                    "name": "Michael Bohlke-Schneider",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64aa5930b6512b83282588c4/7LNVonvpFfFAJ0DMSq4JY.png"
            ],
            "publishedAt": "2025-10-17T17:00:53.000Z",
            "submittedOnDailyAt": "2025-10-21T04:46:59.786Z",
            "title": "Chronos-2: From Univariate to Universal Forecasting",
            "submittedOnDailyBy": {
                "_id": "64aa5930b6512b83282588c4",
                "avatarUrl": "/avatars/57e60d9d36b5ee8911d41415850891d5.svg",
                "isPro": false,
                "fullname": "Abdul Fatir Ansari",
                "user": "abdulfatir",
                "type": "user"
            },
            "summary": "Pretrained time series models have enabled inference-only forecasting systems\nthat produce accurate predictions without task-specific training. However,\nexisting approaches largely focus on univariate forecasting, limiting their\napplicability in real-world scenarios where multivariate data and covariates\nplay a crucial role. We present Chronos-2, a pretrained model capable of\nhandling univariate, multivariate, and covariate-informed forecasting tasks in\na zero-shot manner. Chronos-2 employs a group attention mechanism that\nfacilitates in-context learning (ICL) through efficient information sharing\nacross multiple time series within a group, which may represent sets of related\nseries, variates of a multivariate series, or targets and covariates in a\nforecasting task. These general capabilities are achieved through training on\nsynthetic datasets that impose diverse multivariate structures on univariate\nseries. Chronos-2 delivers state-of-the-art performance across three\ncomprehensive benchmarks: fev-bench, GIFT-Eval, and Chronos Benchmark II. On\nfev-bench, which emphasizes multivariate and covariate-informed forecasting,\nChronos-2's universal ICL capabilities lead to substantial improvements over\nexisting models. On tasks involving covariates, it consistently outperforms\nbaselines by a wide margin. Case studies in the energy and retail domains\nfurther highlight its practical advantages. The in-context learning\ncapabilities of Chronos-2 establish it as a general-purpose forecasting model\nthat can be used \"as is\" in real-world forecasting pipelines.",
            "upvotes": 7,
            "discussionId": "68f5d4928589920bf4d32209",
            "githubRepo": "https://github.com/amazon-science/chronos-forecasting",
            "ai_summary": "Chronos-2, a pretrained model with a group attention mechanism, achieves state-of-the-art performance in zero-shot univariate, multivariate, and covariate-informed forecasting tasks.",
            "ai_keywords": [
                "pretrained model",
                "group attention mechanism",
                "in-context learning",
                "ICL",
                "synthetic datasets",
                "multivariate structures",
                "fev-bench",
                "GIFT-Eval",
                "Chronos Benchmark II",
                "energy domain",
                "retail domain"
            ],
            "githubStars": 3776,
            "organization": {
                "_id": "5ffdfbadbba2ae614d771970",
                "name": "amazon",
                "fullname": "Amazon",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
            }
        },
        "publishedAt": "2025-10-17T13:00:53.000Z",
        "title": "Chronos-2: From Univariate to Universal Forecasting",
        "summary": "Pretrained time series models have enabled inference-only forecasting systems\nthat produce accurate predictions without task-specific training. However,\nexisting approaches largely focus on univariate forecasting, limiting their\napplicability in real-world scenarios where multivariate data and covariates\nplay a crucial role. We present Chronos-2, a pretrained model capable of\nhandling univariate, multivariate, and covariate-informed forecasting tasks in\na zero-shot manner. Chronos-2 employs a group attention mechanism that\nfacilitates in-context learning (ICL) through efficient information sharing\nacross multiple time series within a group, which may represent sets of related\nseries, variates of a multivariate series, or targets and covariates in a\nforecasting task. These general capabilities are achieved through training on\nsynthetic datasets that impose diverse multivariate structures on univariate\nseries. Chronos-2 delivers state-of-the-art performance across three\ncomprehensive benchmarks: fev-bench, GIFT-Eval, and Chronos Benchmark II. On\nfev-bench, which emphasizes multivariate and covariate-informed forecasting,\nChronos-2's universal ICL capabilities lead to substantial improvements over\nexisting models. On tasks involving covariates, it consistently outperforms\nbaselines by a wide margin. Case studies in the energy and retail domains\nfurther highlight its practical advantages. The in-context learning\ncapabilities of Chronos-2 establish it as a general-purpose forecasting model\nthat can be used \"as is\" in real-world forecasting pipelines.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64aa5930b6512b83282588c4/7LNVonvpFfFAJ0DMSq4JY.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15821.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64aa5930b6512b83282588c4",
            "avatarUrl": "/avatars/57e60d9d36b5ee8911d41415850891d5.svg",
            "fullname": "Abdul Fatir Ansari",
            "name": "abdulfatir",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "organization": {
            "_id": "5ffdfbadbba2ae614d771970",
            "name": "amazon",
            "fullname": "Amazon",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.16720",
            "authors": [
                {
                    "_id": "68f6ebbc24c4489363111823",
                    "name": "Jitao Sang",
                    "hidden": false
                },
                {
                    "_id": "68f6ebbc24c4489363111824",
                    "name": "Jinlin Xiao",
                    "hidden": false
                },
                {
                    "_id": "68f6ebbc24c4489363111825",
                    "name": "Jiarun Han",
                    "hidden": false
                },
                {
                    "_id": "68f6ebbc24c4489363111826",
                    "name": "Jilin Chen",
                    "hidden": false
                },
                {
                    "_id": "68f6ebbc24c4489363111827",
                    "name": "Xiaoyi Chen",
                    "hidden": false
                },
                {
                    "_id": "68f6ebbc24c4489363111828",
                    "name": "Shuyu Wei",
                    "hidden": false
                },
                {
                    "_id": "68f6ebbc24c4489363111829",
                    "name": "Yongjie Sun",
                    "hidden": false
                },
                {
                    "_id": "68f6ebbc24c448936311182a",
                    "name": "Yuhang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-19T05:23:43.000Z",
            "submittedOnDailyAt": "2025-10-21T01:25:59.196Z",
            "title": "Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native\n  Agentic AI",
            "submittedOnDailyBy": {
                "_id": "6494457c6339264dd78bcb95",
                "avatarUrl": "/avatars/d87842251f1a43f50cc827f0e2a995ee.svg",
                "isPro": false,
                "fullname": "sdzy",
                "user": "sdzy",
                "type": "user"
            },
            "summary": "The rapid evolution of agentic AI marks a new phase in artificial\nintelligence, where Large Language Models (LLMs) no longer merely respond but\nact, reason, and adapt. This survey traces the paradigm shift in building\nagentic AI: from Pipeline-based systems, where planning, tool use, and memory\nare orchestrated by external logic, to the emerging Model-native paradigm,\nwhere these capabilities are internalized within the model's parameters. We\nfirst position Reinforcement Learning (RL) as the algorithmic engine enabling\nthis paradigm shift. By reframing learning from imitating static data to\noutcome-driven exploration, RL underpins a unified solution of LLM + RL + Task\nacross language, vision and embodied domains. Building on this, the survey\nsystematically reviews how each capability -- Planning, Tool use, and Memory --\nhas evolved from externally scripted modules to end-to-end learned behaviors.\nFurthermore, it examines how this paradigm shift has reshaped major agent\napplications, specifically the Deep Research agent emphasizing long-horizon\nreasoning and the GUI agent emphasizing embodied interaction. We conclude by\ndiscussing the continued internalization of agentic capabilities like\nMulti-agent collaboration and Reflection, alongside the evolving roles of the\nsystem and model layers in future agentic AI. Together, these developments\noutline a coherent trajectory toward model-native agentic AI as an integrated\nlearning and interaction framework, marking the transition from constructing\nsystems that apply intelligence to developing models that grow intelligence\nthrough experience.",
            "upvotes": 6,
            "discussionId": "68f6ebbc24c448936311182b",
            "projectPage": "https://github.com/ADaM-BJTU/model-native-agentic-ai",
            "githubRepo": "https://github.com/ADaM-BJTU/model-native-agentic-ai",
            "ai_summary": "The survey outlines the shift from pipeline-based to model-native agentic AI, emphasizing the role of reinforcement learning in integrating planning, tool use, and memory within large language models across various domains.",
            "ai_keywords": [
                "Reinforcement Learning",
                "Model-native paradigm",
                "Large Language Models",
                "Planning",
                "Tool use",
                "Memory",
                "Deep Research agent",
                "GUI agent",
                "Multi-agent collaboration",
                "Reflection"
            ],
            "githubStars": 25,
            "organization": {
                "_id": "647c8655396de7684de79a2e",
                "name": "BJTUniversity",
                "fullname": "Beijing JiaoTong University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469b882af67f5c95414775c/NT0fZaZxB_1Cf85zeFYb3.png"
            }
        },
        "publishedAt": "2025-10-19T01:23:43.000Z",
        "title": "Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native\n  Agentic AI",
        "summary": "The rapid evolution of agentic AI marks a new phase in artificial\nintelligence, where Large Language Models (LLMs) no longer merely respond but\nact, reason, and adapt. This survey traces the paradigm shift in building\nagentic AI: from Pipeline-based systems, where planning, tool use, and memory\nare orchestrated by external logic, to the emerging Model-native paradigm,\nwhere these capabilities are internalized within the model's parameters. We\nfirst position Reinforcement Learning (RL) as the algorithmic engine enabling\nthis paradigm shift. By reframing learning from imitating static data to\noutcome-driven exploration, RL underpins a unified solution of LLM + RL + Task\nacross language, vision and embodied domains. Building on this, the survey\nsystematically reviews how each capability -- Planning, Tool use, and Memory --\nhas evolved from externally scripted modules to end-to-end learned behaviors.\nFurthermore, it examines how this paradigm shift has reshaped major agent\napplications, specifically the Deep Research agent emphasizing long-horizon\nreasoning and the GUI agent emphasizing embodied interaction. We conclude by\ndiscussing the continued internalization of agentic capabilities like\nMulti-agent collaboration and Reflection, alongside the evolving roles of the\nsystem and model layers in future agentic AI. Together, these developments\noutline a coherent trajectory toward model-native agentic AI as an integrated\nlearning and interaction framework, marking the transition from constructing\nsystems that apply intelligence to developing models that grow intelligence\nthrough experience.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16720.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6494457c6339264dd78bcb95",
            "avatarUrl": "/avatars/d87842251f1a43f50cc827f0e2a995ee.svg",
            "fullname": "sdzy",
            "name": "sdzy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "647c8655396de7684de79a2e",
            "name": "BJTUniversity",
            "fullname": "Beijing JiaoTong University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469b882af67f5c95414775c/NT0fZaZxB_1Cf85zeFYb3.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.15021",
            "authors": [
                {
                    "_id": "68f58fc18589920bf4d32065",
                    "name": "Jiaxin Ge",
                    "hidden": false
                },
                {
                    "_id": "68f58fc18589920bf4d32066",
                    "name": "Grace Luo",
                    "hidden": false
                },
                {
                    "_id": "68f58fc18589920bf4d32067",
                    "name": "Heekyung Lee",
                    "hidden": false
                },
                {
                    "_id": "68f58fc18589920bf4d32068",
                    "name": "Nishant Malpani",
                    "hidden": false
                },
                {
                    "_id": "68f58fc18589920bf4d32069",
                    "name": "Long Lian",
                    "hidden": false
                },
                {
                    "_id": "68f58fc18589920bf4d3206a",
                    "name": "XuDong Wang",
                    "hidden": false
                },
                {
                    "_id": "68f58fc18589920bf4d3206b",
                    "name": "Aleksander Holynski",
                    "hidden": false
                },
                {
                    "_id": "68f58fc18589920bf4d3206c",
                    "name": "Trevor Darrell",
                    "hidden": false
                },
                {
                    "_id": "68f58fc18589920bf4d3206d",
                    "name": "Sewon Min",
                    "hidden": false
                },
                {
                    "_id": "68f58fc18589920bf4d3206e",
                    "name": "David M. Chan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:59:30.000Z",
            "submittedOnDailyAt": "2025-10-21T03:34:49.939Z",
            "title": "Constantly Improving Image Models Need Constantly Improving Benchmarks",
            "submittedOnDailyBy": {
                "_id": "64bdc1e8d05a97d722b2c1f6",
                "avatarUrl": "/avatars/b04cbe17e7cd0a1f5bf0e8e008640a1d.svg",
                "isPro": false,
                "fullname": "Jiaxin Ge",
                "user": "para-lost",
                "type": "user"
            },
            "summary": "Recent advances in image generation, often driven by proprietary systems like\nGPT-4o Image Gen, regularly introduce new capabilities that reshape how users\ninteract with these models. Existing benchmarks often lag behind and fail to\ncapture these emerging use cases, leaving a gap between community perceptions\nof progress and formal evaluation. To address this, we present ECHO, a\nframework for constructing benchmarks directly from real-world evidence of\nmodel use: social media posts that showcase novel prompts and qualitative user\njudgments. Applying this framework to GPT-4o Image Gen, we construct a dataset\nof over 31,000 prompts curated from such posts. Our analysis shows that ECHO\n(1) discovers creative and complex tasks absent from existing benchmarks, such\nas re-rendering product labels across languages or generating receipts with\nspecified totals, (2) more clearly distinguishes state-of-the-art models from\nalternatives, and (3) surfaces community feedback that we use to inform the\ndesign of metrics for model quality (e.g., measuring observed shifts in color,\nidentity, and structure). Our website is at https://echo-bench.github.io.",
            "upvotes": 4,
            "discussionId": "68f58fc28589920bf4d3206f",
            "projectPage": "https://echo-bench.github.io/",
            "githubRepo": "https://github.com/para-lost/ECHO",
            "ai_summary": "ECHO is a framework that constructs benchmarks for image generation models using real-world social media data, uncovering complex tasks and improving model evaluation.",
            "ai_keywords": [
                "image generation",
                "GPT-4o Image Gen",
                "benchmarks",
                "social media posts",
                "qualitative user judgments",
                "dataset",
                "re-rendering",
                "product labels",
                "generating receipts",
                "model quality",
                "metrics",
                "color shifts",
                "identity shifts",
                "structure shifts"
            ],
            "githubStars": 9,
            "organization": {
                "_id": "66b1baeff10262fc4fa61961",
                "name": "UCBerkeley",
                "fullname": "University of California, Berkeley",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"
            }
        },
        "publishedAt": "2025-10-16T13:59:30.000Z",
        "title": "Constantly Improving Image Models Need Constantly Improving Benchmarks",
        "summary": "Recent advances in image generation, often driven by proprietary systems like\nGPT-4o Image Gen, regularly introduce new capabilities that reshape how users\ninteract with these models. Existing benchmarks often lag behind and fail to\ncapture these emerging use cases, leaving a gap between community perceptions\nof progress and formal evaluation. To address this, we present ECHO, a\nframework for constructing benchmarks directly from real-world evidence of\nmodel use: social media posts that showcase novel prompts and qualitative user\njudgments. Applying this framework to GPT-4o Image Gen, we construct a dataset\nof over 31,000 prompts curated from such posts. Our analysis shows that ECHO\n(1) discovers creative and complex tasks absent from existing benchmarks, such\nas re-rendering product labels across languages or generating receipts with\nspecified totals, (2) more clearly distinguishes state-of-the-art models from\nalternatives, and (3) surfaces community feedback that we use to inform the\ndesign of metrics for model quality (e.g., measuring observed shifts in color,\nidentity, and structure). Our website is at https://echo-bench.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15021.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64bdc1e8d05a97d722b2c1f6",
            "avatarUrl": "/avatars/b04cbe17e7cd0a1f5bf0e8e008640a1d.svg",
            "fullname": "Jiaxin Ge",
            "name": "para-lost",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "66b1baeff10262fc4fa61961",
            "name": "UCBerkeley",
            "fullname": "University of California, Berkeley",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.17797",
            "authors": [
                {
                    "_id": "68f6f37a24c44893631118b7",
                    "user": {
                        "_id": "6475718de0b188d3cb25f346",
                        "avatarUrl": "/avatars/b0cdca7e19bbc1dbac00ea664108b51e.svg",
                        "isPro": false,
                        "fullname": "Akshara Prabhakar",
                        "user": "aksh555",
                        "type": "user"
                    },
                    "name": "Akshara Prabhakar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:03:42.246Z",
                    "hidden": false
                },
                {
                    "_id": "68f6f37a24c44893631118b8",
                    "name": "Roshan Ram",
                    "hidden": false
                },
                {
                    "_id": "68f6f37a24c44893631118b9",
                    "name": "Zixiang Chen",
                    "hidden": false
                },
                {
                    "_id": "68f6f37a24c44893631118ba",
                    "name": "Silvio Savarese",
                    "hidden": false
                },
                {
                    "_id": "68f6f37a24c44893631118bb",
                    "name": "Frank Wang",
                    "hidden": false
                },
                {
                    "_id": "68f6f37a24c44893631118bc",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "68f6f37a24c44893631118bd",
                    "name": "Huan Wang",
                    "hidden": false
                },
                {
                    "_id": "68f6f37a24c44893631118be",
                    "name": "Weiran Yao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T17:55:11.000Z",
            "submittedOnDailyAt": "2025-10-21T01:14:26.499Z",
            "title": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for\n  Enterprise Analytics",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "As information grows exponentially, enterprises face increasing pressure to\ntransform unstructured data into coherent, actionable insights. While\nautonomous agents show promise, they often struggle with domain-specific\nnuances, intent alignment, and enterprise integration. We present Enterprise\nDeep Research (EDR), a multi-agent system that integrates (1) a Master Planning\nAgent for adaptive query decomposition, (2) four specialized search agents\n(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool\necosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a\nVisualization Agent for data-driven insights, and (5) a reflection mechanism\nthat detects knowledge gaps and updates research direction with optional\nhuman-in-the-loop steering guidance. These components enable automated report\ngeneration, real-time streaming, and seamless enterprise deployment, as\nvalidated on internal datasets. On open-ended benchmarks including DeepResearch\nBench and DeepConsult, EDR outperforms state-of-the-art agentic systems without\nany human steering. We release the EDR framework and benchmark trajectories to\nadvance research on multi-agent reasoning applications.\n  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and\nDataset at https://huggingface.co/datasets/Salesforce/EDR-200",
            "upvotes": 3,
            "discussionId": "68f6f37b24c44893631118bf",
            "githubRepo": "https://github.com/SalesforceAIResearch/enterprise-deep-research",
            "ai_summary": "Enterprise Deep Research (EDR) is a multi-agent system that automates report generation and real-time data analysis by integrating specialized agents and tools, outperforming existing agentic systems on open benchmarks.",
            "ai_keywords": [
                "multi-agent system",
                "Master Planning Agent",
                "adaptive query decomposition",
                "specialized search agents",
                "General",
                "Academic",
                "GitHub",
                "LinkedIn",
                "MCP-based tool ecosystem",
                "NL2SQL",
                "file analysis",
                "enterprise workflows",
                "Visualization Agent",
                "data-driven insights",
                "reflection mechanism",
                "knowledge gaps",
                "human-in-the-loop steering guidance",
                "automated report generation",
                "real-time streaming",
                "seamless enterprise deployment",
                "DeepResearch Bench",
                "DeepConsult"
            ],
            "githubStars": 66,
            "organization": {
                "_id": "5f6d64475e78cc6b0ed31e4c",
                "name": "Salesforce",
                "fullname": "Salesforce",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
            }
        },
        "publishedAt": "2025-10-20T13:55:11.000Z",
        "title": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for\n  Enterprise Analytics",
        "summary": "As information grows exponentially, enterprises face increasing pressure to\ntransform unstructured data into coherent, actionable insights. While\nautonomous agents show promise, they often struggle with domain-specific\nnuances, intent alignment, and enterprise integration. We present Enterprise\nDeep Research (EDR), a multi-agent system that integrates (1) a Master Planning\nAgent for adaptive query decomposition, (2) four specialized search agents\n(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool\necosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a\nVisualization Agent for data-driven insights, and (5) a reflection mechanism\nthat detects knowledge gaps and updates research direction with optional\nhuman-in-the-loop steering guidance. These components enable automated report\ngeneration, real-time streaming, and seamless enterprise deployment, as\nvalidated on internal datasets. On open-ended benchmarks including DeepResearch\nBench and DeepConsult, EDR outperforms state-of-the-art agentic systems without\nany human steering. We release the EDR framework and benchmark trajectories to\nadvance research on multi-agent reasoning applications.\n  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and\nDataset at https://huggingface.co/datasets/Salesforce/EDR-200",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17797.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 134
        },
        "organization": {
            "_id": "5f6d64475e78cc6b0ed31e4c",
            "name": "Salesforce",
            "fullname": "Salesforce",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.17790",
            "authors": [
                {
                    "_id": "68f6ff6224c4489363111976",
                    "name": "Yuhao Yang",
                    "hidden": false
                },
                {
                    "_id": "68f6ff6224c4489363111977",
                    "name": "Zhen Yang",
                    "hidden": false
                },
                {
                    "_id": "68f6ff6224c4489363111978",
                    "name": "Zi-Yi Dou",
                    "hidden": false
                },
                {
                    "_id": "68f6ff6224c4489363111979",
                    "name": "Anh Nguyen",
                    "hidden": false
                },
                {
                    "_id": "68f6ff6224c448936311197a",
                    "name": "Keen You",
                    "hidden": false
                },
                {
                    "_id": "68f6ff6224c448936311197b",
                    "name": "Omar Attia",
                    "hidden": false
                },
                {
                    "_id": "68f6ff6224c448936311197c",
                    "name": "Andrew Szot",
                    "hidden": false
                },
                {
                    "_id": "68f6ff6224c448936311197d",
                    "name": "Michael Feng",
                    "hidden": false
                },
                {
                    "_id": "68f6ff6224c448936311197e",
                    "name": "Ram Ramrakhya",
                    "hidden": false
                },
                {
                    "_id": "68f6ff6224c448936311197f",
                    "name": "Alexander Toshev",
                    "hidden": false
                },
                {
                    "_id": "68f6ff6224c4489363111980",
                    "name": "Chao Huang",
                    "hidden": false
                },
                {
                    "_id": "68f6ff6224c4489363111981",
                    "name": "Yinfei Yang",
                    "hidden": false
                },
                {
                    "_id": "68f6ff6224c4489363111982",
                    "name": "Zhe Gan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T17:48:26.000Z",
            "submittedOnDailyAt": "2025-10-21T02:05:12.480Z",
            "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Multimodal agents for computer use rely exclusively on primitive actions\n(click, type, scroll) that require accurate visual grounding and lengthy\nexecution chains, leading to cascading failures and performance bottlenecks.\nWhile other agents leverage rich programmatic interfaces (APIs, MCP servers,\ntools), computer-use agents (CUAs) remain isolated from these capabilities. We\npresent UltraCUA, a foundation model that bridges this gap through hybrid\naction -- seamlessly integrating GUI primitives with high-level programmatic\ntool calls. To achieve this, our approach comprises four key components: (1) an\nautomated pipeline that scales programmatic tools from software documentation,\nopen-source repositories, and code generation; (2) a synthetic data engine\nproducing over 17,000 verifiable tasks spanning real-world computer-use\nscenarios; (3) a large-scale high-quality hybrid action trajectory collection\nwith both low-level GUI actions and high-level programmatic tool calls; and (4)\na two-stage training pipeline combining supervised fine-tuning with online\nreinforcement learning, enabling strategic alternation between low-level and\nhigh-level actions. Experiments with our 7B and 32B models demonstrate\nsubstantial improvements over state-of-the-art agents. On OSWorld, UltraCUA\nmodels achieve an average 22% relative improvement over base models, while\nbeing 11% faster in terms of steps. Out-of-domain evaluation on\nWindowsAgentArena shows our model reaches 21.7% success rate, outperforming\nbaselines trained on Windows data. The hybrid action mechanism proves critical,\nreducing error propagation while maintaining execution efficiency.",
            "upvotes": 3,
            "discussionId": "68f6ff6224c4489363111983",
            "ai_summary": "UltraCUA integrates GUI actions with programmatic tools to improve computer-use agent performance and efficiency.",
            "ai_keywords": [
                "GUI primitives",
                "programmatic interfaces",
                "APIs",
                "MCP servers",
                "tools",
                "foundation model",
                "hybrid action",
                "automated pipeline",
                "synthetic data engine",
                "high-quality hybrid action trajectory",
                "supervised fine-tuning",
                "online reinforcement learning",
                "OSWorld",
                "WindowsAgentArena"
            ],
            "organization": {
                "_id": "628cbd99ef14f971b69948ab",
                "name": "apple",
                "fullname": "Apple",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
            }
        },
        "publishedAt": "2025-10-20T13:48:26.000Z",
        "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action",
        "summary": "Multimodal agents for computer use rely exclusively on primitive actions\n(click, type, scroll) that require accurate visual grounding and lengthy\nexecution chains, leading to cascading failures and performance bottlenecks.\nWhile other agents leverage rich programmatic interfaces (APIs, MCP servers,\ntools), computer-use agents (CUAs) remain isolated from these capabilities. We\npresent UltraCUA, a foundation model that bridges this gap through hybrid\naction -- seamlessly integrating GUI primitives with high-level programmatic\ntool calls. To achieve this, our approach comprises four key components: (1) an\nautomated pipeline that scales programmatic tools from software documentation,\nopen-source repositories, and code generation; (2) a synthetic data engine\nproducing over 17,000 verifiable tasks spanning real-world computer-use\nscenarios; (3) a large-scale high-quality hybrid action trajectory collection\nwith both low-level GUI actions and high-level programmatic tool calls; and (4)\na two-stage training pipeline combining supervised fine-tuning with online\nreinforcement learning, enabling strategic alternation between low-level and\nhigh-level actions. Experiments with our 7B and 32B models demonstrate\nsubstantial improvements over state-of-the-art agents. On OSWorld, UltraCUA\nmodels achieve an average 22% relative improvement over base models, while\nbeing 11% faster in terms of steps. Out-of-domain evaluation on\nWindowsAgentArena shows our model reaches 21.7% success rate, outperforming\nbaselines trained on Windows data. The hybrid action mechanism proves critical,\nreducing error propagation while maintaining execution efficiency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17790.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 134
        },
        "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.17431",
            "authors": [
                {
                    "_id": "68f754d524c4489363111c78",
                    "name": "Yushi Yang",
                    "hidden": false
                },
                {
                    "_id": "68f754d524c4489363111c79",
                    "name": "Shreyansh Padarha",
                    "hidden": false
                },
                {
                    "_id": "68f754d524c4489363111c7a",
                    "name": "Andrew Lee",
                    "hidden": false
                },
                {
                    "_id": "68f754d524c4489363111c7b",
                    "name": "Adam Mahdi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T11:19:37.000Z",
            "submittedOnDailyAt": "2025-10-21T08:12:39.023Z",
            "title": "Agentic Reinforcement Learning for Search is Unsafe",
            "submittedOnDailyBy": {
                "_id": "6562737bf5532ac1bd09d3bd",
                "avatarUrl": "/avatars/aaaff49b6abb5f9351159006b7755d25.svg",
                "isPro": false,
                "fullname": "YY",
                "user": "yy0514",
                "type": "user"
            },
            "summary": "Agentic reinforcement learning (RL) trains large language models to\nautonomously call tools during reasoning, with search as the most common\napplication. These models excel at multi-step reasoning tasks, but their safety\nproperties are not well understood. In this study, we show that RL-trained\nsearch models inherit refusal from instruction tuning and often deflect harmful\nrequests by turning them into safe queries. However, this safety is fragile.\nTwo simple attacks, one that forces the model to begin response with search\n(Search attack), another that encourages models to repeatedly search\n(Multi-search attack), trigger cascades of harmful searches and answers. Across\ntwo model families (Qwen, Llama) with both local and web search, these attacks\nlower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query\nsafety by 82.4%. The attacks succeed by triggering models to generate harmful,\nrequest-mirroring search queries before they can generate the inherited refusal\ntokens. This exposes a core weakness of current RL training: it rewards\ncontinued generation of effective queries without accounting for their\nharmfulness. As a result, RL search models have vulnerabilities that users can\neasily exploit, making it urgent to develop safety-aware agentic RL pipelines\noptimising for safe search.",
            "upvotes": 3,
            "discussionId": "68f754d624c4489363111c7c",
            "ai_summary": "Agentic reinforcement learning models trained for search tasks inherit safety mechanisms but are vulnerable to attacks that reduce their refusal and safety rates.",
            "ai_keywords": [
                "agentic reinforcement learning",
                "RL",
                "large language models",
                "tool calling",
                "multi-step reasoning",
                "safety properties",
                "refusal",
                "harmful requests",
                "search attack",
                "multi-search attack",
                "Qwen",
                "Llama",
                "local search",
                "web search",
                "refusal tokens",
                "safe search"
            ],
            "organization": {
                "_id": "66b35d987f152192cae03c95",
                "name": "UniOxford",
                "fullname": "University of Oxford",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b357caa8e0a87f022d323d/XLt3Ok-iiC7JdXWW53-uZ.png"
            }
        },
        "publishedAt": "2025-10-20T07:19:37.000Z",
        "title": "Agentic Reinforcement Learning for Search is Unsafe",
        "summary": "Agentic reinforcement learning (RL) trains large language models to\nautonomously call tools during reasoning, with search as the most common\napplication. These models excel at multi-step reasoning tasks, but their safety\nproperties are not well understood. In this study, we show that RL-trained\nsearch models inherit refusal from instruction tuning and often deflect harmful\nrequests by turning them into safe queries. However, this safety is fragile.\nTwo simple attacks, one that forces the model to begin response with search\n(Search attack), another that encourages models to repeatedly search\n(Multi-search attack), trigger cascades of harmful searches and answers. Across\ntwo model families (Qwen, Llama) with both local and web search, these attacks\nlower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query\nsafety by 82.4%. The attacks succeed by triggering models to generate harmful,\nrequest-mirroring search queries before they can generate the inherited refusal\ntokens. This exposes a core weakness of current RL training: it rewards\ncontinued generation of effective queries without accounting for their\nharmfulness. As a result, RL search models have vulnerabilities that users can\neasily exploit, making it urgent to develop safety-aware agentic RL pipelines\noptimising for safe search.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17431.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6562737bf5532ac1bd09d3bd",
            "avatarUrl": "/avatars/aaaff49b6abb5f9351159006b7755d25.svg",
            "fullname": "YY",
            "name": "yy0514",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "66b35d987f152192cae03c95",
            "name": "UniOxford",
            "fullname": "University of Oxford",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b357caa8e0a87f022d323d/XLt3Ok-iiC7JdXWW53-uZ.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.16641",
            "authors": [
                {
                    "_id": "68f7390224c4489363111b4b",
                    "name": "Young-Jun Lee",
                    "hidden": false
                },
                {
                    "_id": "68f7390224c4489363111b4c",
                    "name": "Byung-Kwan Lee",
                    "hidden": false
                },
                {
                    "_id": "68f7390224c4489363111b4d",
                    "name": "Jianshu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f7390224c4489363111b4e",
                    "name": "Yechan Hwang",
                    "hidden": false
                },
                {
                    "_id": "68f7390224c4489363111b4f",
                    "name": "Byungsoo Ko",
                    "hidden": false
                },
                {
                    "_id": "68f7390224c4489363111b50",
                    "name": "Han-Gyu Kim",
                    "hidden": false
                },
                {
                    "_id": "68f7390224c4489363111b51",
                    "name": "Dongyu Yao",
                    "hidden": false
                },
                {
                    "_id": "68f7390224c4489363111b52",
                    "name": "Xuankun Rong",
                    "hidden": false
                },
                {
                    "_id": "68f7390224c4489363111b53",
                    "name": "Eojin Joo",
                    "hidden": false
                },
                {
                    "_id": "68f7390224c4489363111b54",
                    "name": "Seung-Ho Han",
                    "hidden": false
                },
                {
                    "_id": "68f7390224c4489363111b55",
                    "name": "Bowon Ko",
                    "hidden": false
                },
                {
                    "_id": "68f7390224c4489363111b56",
                    "name": "Ho-Jin Choi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-18T21:00:12.000Z",
            "submittedOnDailyAt": "2025-10-21T06:17:11.707Z",
            "title": "MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large\n  Vision and Language Models",
            "submittedOnDailyBy": {
                "_id": "6434b6619bd5a84b5dcfa4de",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434b6619bd5a84b5dcfa4de/h8Q6kPNjFNc03wmdboHzq.jpeg",
                "isPro": true,
                "fullname": "Young-Jun Lee",
                "user": "passing2961",
                "type": "user"
            },
            "summary": "Vision-and-Language Models (VLMs) have shown impressive capabilities on\nsingle-turn benchmarks, yet real-world applications often demand more intricate\nmulti-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only\npartially capture the breadth and depth of conversational scenarios encountered\nby users. In this work, we introduce MultiVerse, a novel multi-turn\nconversation benchmark featuring 647 dialogues - each averaging four turns -\nderived from a diverse set of 12 popular VLM evaluation benchmarks. With 484\ntasks and 484 interaction goals, MultiVerse covers a wide range of topics, from\nfactual knowledge and perception to advanced reasoning tasks such as\nmathematics and coding. To facilitate robust assessment, we propose a\nchecklist-based evaluation method that leverages GPT-4o as the automated\nevaluator, measuring performance across 37 key aspects, including perceptual\naccuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on\nMultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve\nonly a 50% success rate in complex multi-turn conversations, highlighting the\ndataset's challenging nature. Notably, we find that providing full dialogue\ncontext significantly enhances performance for smaller or weaker models,\nemphasizing the importance of in-context learning. We believe MultiVerse is a\nlandscape of evaluating multi-turn interaction abilities for VLMs.",
            "upvotes": 3,
            "discussionId": "68f7390324c4489363111b57",
            "githubRepo": "https://github.com/passing2961/MultiVerse",
            "ai_summary": "MultiVerse, a new multi-turn conversation benchmark, evaluates VLMs across diverse tasks and interaction goals, revealing challenges and the importance of in-context learning.",
            "ai_keywords": [
                "Vision-and-Language Models",
                "multi-turn dialogues",
                "MMDU",
                "ConvBench",
                "MultiVerse",
                "GPT-4o",
                "checklist-based evaluation",
                "perceptual accuracy",
                "linguistic clarity",
                "factual correctness",
                "in-context learning"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "635b304962fb2bc1b52c6291",
                "name": "KAIST",
                "fullname": "KAIST"
            }
        },
        "publishedAt": "2025-10-18T17:00:12.000Z",
        "title": "MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large\n  Vision and Language Models",
        "summary": "Vision-and-Language Models (VLMs) have shown impressive capabilities on\nsingle-turn benchmarks, yet real-world applications often demand more intricate\nmulti-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only\npartially capture the breadth and depth of conversational scenarios encountered\nby users. In this work, we introduce MultiVerse, a novel multi-turn\nconversation benchmark featuring 647 dialogues - each averaging four turns -\nderived from a diverse set of 12 popular VLM evaluation benchmarks. With 484\ntasks and 484 interaction goals, MultiVerse covers a wide range of topics, from\nfactual knowledge and perception to advanced reasoning tasks such as\nmathematics and coding. To facilitate robust assessment, we propose a\nchecklist-based evaluation method that leverages GPT-4o as the automated\nevaluator, measuring performance across 37 key aspects, including perceptual\naccuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on\nMultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve\nonly a 50% success rate in complex multi-turn conversations, highlighting the\ndataset's challenging nature. Notably, we find that providing full dialogue\ncontext significantly enhances performance for smaller or weaker models,\nemphasizing the importance of in-context learning. We believe MultiVerse is a\nlandscape of evaluating multi-turn interaction abilities for VLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16641.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6434b6619bd5a84b5dcfa4de",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434b6619bd5a84b5dcfa4de/h8Q6kPNjFNc03wmdboHzq.jpeg",
            "fullname": "Young-Jun Lee",
            "name": "passing2961",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "organization": {
            "_id": "635b304962fb2bc1b52c6291",
            "name": "KAIST",
            "fullname": "KAIST"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.16276",
            "authors": [
                {
                    "_id": "68f719db24c44893631119bc",
                    "name": "Song Bian",
                    "hidden": false
                },
                {
                    "_id": "68f719db24c44893631119bd",
                    "name": "Minghao Yan",
                    "hidden": false
                },
                {
                    "_id": "68f719db24c44893631119be",
                    "name": "Anand Jayarajan",
                    "hidden": false
                },
                {
                    "_id": "68f719db24c44893631119bf",
                    "name": "Gennady Pekhimenko",
                    "hidden": false
                },
                {
                    "_id": "68f719db24c44893631119c0",
                    "name": "Shivaram Venkataraman",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-18T00:21:45.000Z",
            "submittedOnDailyAt": "2025-10-21T16:35:23.623Z",
            "title": "What Limits Agentic Systems Efficiency?",
            "submittedOnDailyBy": {
                "_id": "620ec8059291e41cab585a3d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620ec8059291e41cab585a3d/iuVrjdeghqo-lz0lRfw7l.jpeg",
                "isPro": false,
                "fullname": "Song",
                "user": "NaiveUser",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance.",
            "upvotes": 3,
            "discussionId": "68f719db24c44893631119c1",
            "ai_summary": "A caching framework with speculative execution reduces web environment latency in web-interactive agentic systems without degrading performance.",
            "ai_keywords": [
                "Large Language Models",
                "LLM",
                "agentic systems",
                "web interactions",
                "end-to-end latency",
                "LLM API latency",
                "web environment latency",
                "SpecCache",
                "caching framework",
                "speculative execution",
                "cache hit rate",
                "web environment overhead"
            ],
            "organization": {
                "_id": "6318959fda3063b19c1c1d9b",
                "name": "Wisconsin",
                "fullname": "University of Wisconsin - Madison",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/644645655004f2cb3aefc452/UqU99v2mCOrNNsD8hYv5Q.png"
            }
        },
        "publishedAt": "2025-10-17T20:21:45.000Z",
        "title": "What Limits Agentic Systems Efficiency?",
        "summary": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16276.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620ec8059291e41cab585a3d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620ec8059291e41cab585a3d/iuVrjdeghqo-lz0lRfw7l.jpeg",
            "fullname": "Song",
            "name": "NaiveUser",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6318959fda3063b19c1c1d9b",
            "name": "Wisconsin",
            "fullname": "University of Wisconsin - Madison",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/644645655004f2cb3aefc452/UqU99v2mCOrNNsD8hYv5Q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.16259",
            "authors": [
                {
                    "_id": "68f6f3bf24c44893631118c1",
                    "name": "Zhehao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f6f3bf24c44893631118c2",
                    "name": "Weijie Xu",
                    "hidden": false
                },
                {
                    "_id": "68f6f3bf24c44893631118c3",
                    "name": "Shixian Cui",
                    "hidden": false
                },
                {
                    "_id": "68f6f3bf24c44893631118c4",
                    "name": "Chandan K. Reddy",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/W4rEeLd9b1Wam1JngHj1-.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/DjQP4ztyFID4WsfHl7QNn.png"
            ],
            "publishedAt": "2025-10-17T23:16:34.000Z",
            "submittedOnDailyAt": "2025-10-21T01:26:24.565Z",
            "title": "Distractor Injection Attacks on Large Reasoning Models: Characterization\n  and Defense",
            "submittedOnDailyBy": {
                "_id": "63e3f57754f51ea342ce26be",
                "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
                "isPro": false,
                "fullname": "Weijie Xu",
                "user": "xwjzds",
                "type": "user"
            },
            "summary": "Recent advances in large reasoning models (LRMs) have enabled remarkable\nperformance on complex tasks such as mathematics and coding by generating long\nChain-of-Thought (CoT) traces. In this paper, we identify and systematically\nanalyze a critical vulnerability we term reasoning distraction, where LRMs are\ndiverted from their primary objective by irrelevant yet complex tasks\nmaliciously embedded in the prompt. Through a comprehensive study across\ndiverse models and benchmarks, we show that even state-of-the-art LRMs are\nhighly susceptible, with injected distractors reducing task accuracy by up to\n60%. We further reveal that certain alignment techniques can amplify this\nweakness and that models may exhibit covert compliance, following hidden\nadversarial instructions in reasoning while concealing them in the final\noutput. To mitigate these risks, we propose a training-based defense that\ncombines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on\nsynthetic adversarial data, improving robustness by over 50 points on\nchallenging distractor attacks. Our findings establish reasoning distraction as\na distinct and urgent threat to LRM reliability and provide a practical step\ntoward safer and more trustworthy reasoning systems.",
            "upvotes": 3,
            "discussionId": "68f6f3bf24c44893631118c5",
            "ai_summary": "Large reasoning models are vulnerable to reasoning distraction, where irrelevant tasks embedded in prompts reduce accuracy, and a combined SFT and RL defense improves robustness.",
            "ai_keywords": [
                "large reasoning models",
                "Chain-of-Thought",
                "reasoning distraction",
                "Supervised Fine-Tuning",
                "Reinforcement Learning",
                "adversarial data"
            ],
            "organization": {
                "_id": "615250af54cfe5db853ec856",
                "name": "AmazonScience",
                "fullname": "Amazon Science",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1632784986195-61393f6f905b1938233881e2.png"
            }
        },
        "publishedAt": "2025-10-17T19:16:34.000Z",
        "title": "Distractor Injection Attacks on Large Reasoning Models: Characterization\n  and Defense",
        "summary": "Recent advances in large reasoning models (LRMs) have enabled remarkable\nperformance on complex tasks such as mathematics and coding by generating long\nChain-of-Thought (CoT) traces. In this paper, we identify and systematically\nanalyze a critical vulnerability we term reasoning distraction, where LRMs are\ndiverted from their primary objective by irrelevant yet complex tasks\nmaliciously embedded in the prompt. Through a comprehensive study across\ndiverse models and benchmarks, we show that even state-of-the-art LRMs are\nhighly susceptible, with injected distractors reducing task accuracy by up to\n60%. We further reveal that certain alignment techniques can amplify this\nweakness and that models may exhibit covert compliance, following hidden\nadversarial instructions in reasoning while concealing them in the final\noutput. To mitigate these risks, we propose a training-based defense that\ncombines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on\nsynthetic adversarial data, improving robustness by over 50 points on\nchallenging distractor attacks. Our findings establish reasoning distraction as\na distinct and urgent threat to LRM reliability and provide a practical step\ntoward safer and more trustworthy reasoning systems.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/W4rEeLd9b1Wam1JngHj1-.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/DjQP4ztyFID4WsfHl7QNn.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16259.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63e3f57754f51ea342ce26be",
            "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
            "fullname": "Weijie Xu",
            "name": "xwjzds",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 16
        },
        "organization": {
            "_id": "615250af54cfe5db853ec856",
            "name": "AmazonScience",
            "fullname": "Amazon Science",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1632784986195-61393f6f905b1938233881e2.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.16258",
            "authors": [
                {
                    "_id": "68f6f7b124c44893631118fc",
                    "name": "Claire McLean",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c44893631118fd",
                    "name": "Makenzie Meendering",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c44893631118fe",
                    "name": "Tristan Swartz",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c44893631118ff",
                    "name": "Orri Gabbay",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c4489363111900",
                    "name": "Alexandra Olsen",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c4489363111901",
                    "name": "Rachel Jacobs",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c4489363111902",
                    "name": "Nicholas Rosen",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c4489363111903",
                    "name": "Philippe de Bree",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c4489363111904",
                    "name": "Tony Garcia",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c4489363111905",
                    "name": "Gadsden Merrill",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c4489363111906",
                    "name": "Jake Sandakly",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c4489363111907",
                    "name": "Julia Buffalini",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c4489363111908",
                    "name": "Neham Jain",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c4489363111909",
                    "name": "Steven Krenn",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c448936311190a",
                    "name": "Moneish Kumar",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c448936311190b",
                    "name": "Dejan Markovic",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c448936311190c",
                    "name": "Evonne Ng",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c448936311190d",
                    "name": "Fabian Prada",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c448936311190e",
                    "name": "Andrew Saba",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c448936311190f",
                    "name": "Siwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c4489363111910",
                    "name": "Vasu Agrawal",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c4489363111911",
                    "name": "Tim Godisart",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c4489363111912",
                    "name": "Alexander Richard",
                    "hidden": false
                },
                {
                    "_id": "68f6f7b124c4489363111913",
                    "name": "Michael Zollhoefer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T23:06:36.000Z",
            "submittedOnDailyAt": "2025-10-21T01:32:26.971Z",
            "title": "Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of\n500 individual hours of 3D motion data from 439 participants collected in a\nmulti-camera collection stage, amounting to over 54 million frames of tracked\n3D motion. The dataset features a wide range of single-person motion data,\nincluding prompted motions, hand gestures, and locomotion; as well as\nmulti-person behavioral and conversational data like discussions, conversations\nin different emotional states, collaborative activities, and co-living\nscenarios in an apartment-like space. We provide tracked human motion including\nhand tracking and body shape, text annotations, and a separate audio track for\neach participant.",
            "upvotes": 3,
            "discussionId": "68f6f7b124c4489363111914",
            "projectPage": "https://www.meta.com/emerging-tech/codec-avatars/embody-3d/",
            "ai_summary": "Embody 3D is a multimodal dataset featuring extensive 3D motion data with hand tracking, body shape, text annotations, and audio tracks from multiple participants in various scenarios.",
            "ai_keywords": [
                "3D motion data",
                "multi-camera collection",
                "hand tracking",
                "body shape",
                "text annotations",
                "audio track",
                "multimodal dataset"
            ],
            "organization": {
                "_id": "66b54027408752ae16404b05",
                "name": "metaresearch",
                "fullname": "Meta Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
            }
        },
        "publishedAt": "2025-10-17T19:06:36.000Z",
        "title": "Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset",
        "summary": "The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of\n500 individual hours of 3D motion data from 439 participants collected in a\nmulti-camera collection stage, amounting to over 54 million frames of tracked\n3D motion. The dataset features a wide range of single-person motion data,\nincluding prompted motions, hand gestures, and locomotion; as well as\nmulti-person behavioral and conversational data like discussions, conversations\nin different emotional states, collaborative activities, and co-living\nscenarios in an apartment-like space. We provide tracked human motion including\nhand tracking and body shape, text annotations, and a separate audio track for\neach participant.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16258.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 134
        },
        "organization": {
            "_id": "66b54027408752ae16404b05",
            "name": "metaresearch",
            "fullname": "Meta Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14605",
            "authors": [
                {
                    "_id": "68f33e1b8589920bf4d31af3",
                    "user": {
                        "_id": "66e03d7123e5f162e7e5c5ac",
                        "avatarUrl": "/avatars/28089b6ffeacafff2c798b7579689c1e.svg",
                        "isPro": false,
                        "fullname": "hongyuyang",
                        "user": "hongyuyang23casia",
                        "type": "user"
                    },
                    "name": "Yuyang Hong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:12:55.883Z",
                    "hidden": false
                },
                {
                    "_id": "68f33e1b8589920bf4d31af4",
                    "name": "Jiaqi Gu",
                    "hidden": false
                },
                {
                    "_id": "68f33e1b8589920bf4d31af5",
                    "name": "Qi Yang",
                    "hidden": false
                },
                {
                    "_id": "68f33e1b8589920bf4d31af6",
                    "name": "Lubin Fan",
                    "hidden": false
                },
                {
                    "_id": "68f33e1b8589920bf4d31af7",
                    "name": "Yue Wu",
                    "hidden": false
                },
                {
                    "_id": "68f33e1b8589920bf4d31af8",
                    "name": "Ying Wang",
                    "hidden": false
                },
                {
                    "_id": "68f33e1b8589920bf4d31af9",
                    "name": "Kun Ding",
                    "hidden": false
                },
                {
                    "_id": "68f33e1b8589920bf4d31afa",
                    "name": "Shiming Xiang",
                    "hidden": false
                },
                {
                    "_id": "68f33e1b8589920bf4d31afb",
                    "name": "Jieping Ye",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T12:10:00.000Z",
            "submittedOnDailyAt": "2025-10-21T00:58:55.843Z",
            "title": "Knowledge-based Visual Question Answer with Multimodal Processing,\n  Retrieval and Filtering",
            "submittedOnDailyBy": {
                "_id": "66e03d7123e5f162e7e5c5ac",
                "avatarUrl": "/avatars/28089b6ffeacafff2c798b7579689c1e.svg",
                "isPro": false,
                "fullname": "hongyuyang",
                "user": "hongyuyang23casia",
                "type": "user"
            },
            "summary": "Knowledge-based visual question answering (KB-VQA) requires visual language\nmodels (VLMs) to integrate visual understanding with external knowledge\nretrieval. Although retrieval-augmented generation (RAG) achieves significant\nadvances in this task by combining knowledge-base querying, it still struggles\nwith the quality of multimodal queries and the relevance of retrieved results.\nTo overcome these challenges, we propose a novel three-stage method, termed\nWiki-PRF, including Processing, Retrieval and Filtering stages. The processing\nstage dynamically invokes visual tools to extract precise multimodal\ninformation for retrieval. The retrieval stage integrates visual and text\nfeatures to achieve multimodal knowledge retrieval. The filtering stage\nperforms relevance filtering and concentration on retrieval results. To this\nend, we introduce a visual language model trained with answer accuracy and\nformat consistency as reward signals via a reinforcement learning manner. This\nenhances the model's reasoning, tool invocation for accurate queries, and\nfiltering of irrelevant content. Experiments on benchmark datasets (E-VQA and\nInfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,\nachieving state-of-the-art performance. Code is available at\nhttps://github.com/cqu-student/Wiki-PRF",
            "upvotes": 3,
            "discussionId": "68f33e1b8589920bf4d31afc",
            "ai_summary": "A novel three-stage method, Wiki-PRF, enhances knowledge-based visual question answering by improving multimodal query quality and relevance through visual language models and reinforcement learning.",
            "ai_keywords": [
                "knowledge-based visual question answering",
                "KB-VQA",
                "visual language models",
                "VLMs",
                "retrieval-augmented generation",
                "RAG",
                "multimodal queries",
                "multimodal knowledge retrieval",
                "relevance filtering",
                "reinforcement learning",
                "answer accuracy",
                "format consistency",
                "E-VQA",
                "InfoSeek"
            ],
            "organization": {
                "_id": "640a887796aae649741a586f",
                "name": "CASIA",
                "fullname": "Chinese Academic of Science Institute of Automation",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"
            }
        },
        "publishedAt": "2025-10-16T08:10:00.000Z",
        "title": "Knowledge-based Visual Question Answer with Multimodal Processing,\n  Retrieval and Filtering",
        "summary": "Knowledge-based visual question answering (KB-VQA) requires visual language\nmodels (VLMs) to integrate visual understanding with external knowledge\nretrieval. Although retrieval-augmented generation (RAG) achieves significant\nadvances in this task by combining knowledge-base querying, it still struggles\nwith the quality of multimodal queries and the relevance of retrieved results.\nTo overcome these challenges, we propose a novel three-stage method, termed\nWiki-PRF, including Processing, Retrieval and Filtering stages. The processing\nstage dynamically invokes visual tools to extract precise multimodal\ninformation for retrieval. The retrieval stage integrates visual and text\nfeatures to achieve multimodal knowledge retrieval. The filtering stage\nperforms relevance filtering and concentration on retrieval results. To this\nend, we introduce a visual language model trained with answer accuracy and\nformat consistency as reward signals via a reinforcement learning manner. This\nenhances the model's reasoning, tool invocation for accurate queries, and\nfiltering of irrelevant content. Experiments on benchmark datasets (E-VQA and\nInfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,\nachieving state-of-the-art performance. Code is available at\nhttps://github.com/cqu-student/Wiki-PRF",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14605.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66e03d7123e5f162e7e5c5ac",
            "avatarUrl": "/avatars/28089b6ffeacafff2c798b7579689c1e.svg",
            "fullname": "hongyuyang",
            "name": "hongyuyang23casia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "640a887796aae649741a586f",
            "name": "CASIA",
            "fullname": "Chinese Academic of Science Institute of Automation",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.17793",
            "authors": [
                {
                    "_id": "68f6e6f524c44893631117e1",
                    "name": "Austin Xu",
                    "hidden": false
                },
                {
                    "_id": "68f6e6f524c44893631117e2",
                    "name": "Xuan-Phi Nguyen",
                    "hidden": false
                },
                {
                    "_id": "68f6e6f524c44893631117e3",
                    "name": "Yilun Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f6e6f524c44893631117e4",
                    "name": "Chien-Sheng Wu",
                    "hidden": false
                },
                {
                    "_id": "68f6e6f524c44893631117e5",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "68f6e6f524c44893631117e6",
                    "name": "Shafiq Joty",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T17:52:06.000Z",
            "submittedOnDailyAt": "2025-10-21T04:00:23.082Z",
            "title": "Foundational Automatic Evaluators: Scaling Multi-Task Generative\n  Evaluator Training for Reasoning-Centric Domains",
            "submittedOnDailyBy": {
                "_id": "6668e86dc4ef4175fb18d250",
                "avatarUrl": "/avatars/d34925685799a1218017f85ce4d44e6e.svg",
                "isPro": false,
                "fullname": "Austin Xu",
                "user": "austinxu87",
                "type": "user"
            },
            "summary": "Finetuning specialized generative evaluators has emerged as a popular\nparadigm to meet the increasing demand for scalable evaluation during both\ntraining and test-time. However, recent work has largely focused on applying\nnew methodology, such as reinforcement learning (RL), to training evaluators,\nshying away from large-scale, data-driven development. In this work, we focus\non data scaling, curating a set of 2.5M samples spanning five unique evaluation\ntasks (pairwise, step-level, reference-free and reference-based verification,\nand single rating) and multiple domains focused on reasoning evaluation. With\nour data, we train Foundational Automatic Reasoning Evaluators (FARE), a family\nof 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative\nrejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges\nlarger specialized RL-trained evaluators and FARE-20B sets the new standard for\nopen-source evaluators, surpassing specialized 70B+ evaluators. Beyond static\nbenchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,\nFARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,\nFARE improves the downstream RL-trained model performance by up to 14.1% vs.\nstring-matching verifiers. When initialized from FARE, a continually-finetuned\nFARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.",
            "upvotes": 2,
            "discussionId": "68f6e6f624c44893631117e7",
            "ai_summary": "FARE, a family of large-scale parameter evaluators, surpasses specialized RL-trained evaluators in both static benchmarks and real-world tasks through data-driven development and iterative rejection-sampling supervised finetuning.",
            "ai_keywords": [
                "generative evaluators",
                "reinforcement learning",
                "Foundational Automatic Reasoning Evaluators",
                "parameter evaluators",
                "iterative rejection-sampling supervised finetuning",
                "MATH",
                "continually-finetuned",
                "gpt-oss-20B"
            ],
            "organization": {
                "_id": "5f6d64475e78cc6b0ed31e4c",
                "name": "Salesforce",
                "fullname": "Salesforce",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
            }
        },
        "publishedAt": "2025-10-20T13:52:06.000Z",
        "title": "Foundational Automatic Evaluators: Scaling Multi-Task Generative\n  Evaluator Training for Reasoning-Centric Domains",
        "summary": "Finetuning specialized generative evaluators has emerged as a popular\nparadigm to meet the increasing demand for scalable evaluation during both\ntraining and test-time. However, recent work has largely focused on applying\nnew methodology, such as reinforcement learning (RL), to training evaluators,\nshying away from large-scale, data-driven development. In this work, we focus\non data scaling, curating a set of 2.5M samples spanning five unique evaluation\ntasks (pairwise, step-level, reference-free and reference-based verification,\nand single rating) and multiple domains focused on reasoning evaluation. With\nour data, we train Foundational Automatic Reasoning Evaluators (FARE), a family\nof 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative\nrejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges\nlarger specialized RL-trained evaluators and FARE-20B sets the new standard for\nopen-source evaluators, surpassing specialized 70B+ evaluators. Beyond static\nbenchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,\nFARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,\nFARE improves the downstream RL-trained model performance by up to 14.1% vs.\nstring-matching verifiers. When initialized from FARE, a continually-finetuned\nFARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17793.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6668e86dc4ef4175fb18d250",
            "avatarUrl": "/avatars/d34925685799a1218017f85ce4d44e6e.svg",
            "fullname": "Austin Xu",
            "name": "austinxu87",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "5f6d64475e78cc6b0ed31e4c",
            "name": "Salesforce",
            "fullname": "Salesforce",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.16499",
            "authors": [
                {
                    "_id": "68f79e9ba7b9b96b2c13edfc",
                    "name": "Michelle Yuan",
                    "hidden": false
                },
                {
                    "_id": "68f79e9ba7b9b96b2c13edfd",
                    "name": "Khushbu Pahwa",
                    "hidden": false
                },
                {
                    "_id": "68f79e9ba7b9b96b2c13edfe",
                    "name": "Shuaichen Chang",
                    "hidden": false
                },
                {
                    "_id": "68f79e9ba7b9b96b2c13edff",
                    "name": "Mustafa Kaba",
                    "hidden": false
                },
                {
                    "_id": "68f79e9ba7b9b96b2c13ee00",
                    "name": "Jiarong Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f79e9ba7b9b96b2c13ee01",
                    "name": "Xiaofei Ma",
                    "hidden": false
                },
                {
                    "_id": "68f79e9ba7b9b96b2c13ee02",
                    "name": "Yi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f79e9ba7b9b96b2c13ee03",
                    "name": "Monica Sunkara",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-18T13:37:47.000Z",
            "submittedOnDailyAt": "2025-10-21T13:25:10.043Z",
            "title": "Automated Composition of Agents: A Knapsack Approach for Agentic\n  Component Selection",
            "submittedOnDailyBy": {
                "_id": "632b8294ffe3618eff2f3774",
                "avatarUrl": "/avatars/002b5b85f41033448979ccecabac28b1.svg",
                "isPro": false,
                "fullname": "Shuaichen Chang",
                "user": "shuaichenchang",
                "type": "user"
            },
            "summary": "Designing effective agentic systems requires the seamless composition and\nintegration of agents, tools, and models within dynamic and uncertain\nenvironments. Most existing methods rely on static, semantic retrieval\napproaches for tool or agent discovery. However, effective reuse and\ncomposition of existing components remain challenging due to incomplete\ncapability descriptions and the limitations of retrieval methods. Component\nselection suffers because the decisions are not based on capability, cost, and\nreal-time utility. To address these challenges, we introduce a structured,\nautomated framework for agentic system composition that is inspired by the\nknapsack problem. Our framework enables a composer agent to systematically\nidentify, select, and assemble an optimal set of agentic components by jointly\nconsidering performance, budget constraints, and compatibility. By dynamically\ntesting candidate components and modeling their utility in real-time, our\napproach streamlines the assembly of agentic systems and facilitates scalable\nreuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five\nbenchmarking datasets shows that our online-knapsack-based composer\nconsistently lies on the Pareto frontier, achieving higher success rates at\nsignificantly lower component costs compared to our baselines. In the\nsingle-agent setup, the online knapsack composer shows a success rate\nimprovement of up to 31.6% in comparison to the retrieval baselines. In\nmulti-agent systems, the online knapsack composer increases success rate from\n37% to 87% when agents are selected from an agent inventory of 100+ agents. The\nsubstantial performance gap confirms the robust adaptability of our method\nacross diverse domains and budget constraints.",
            "upvotes": 2,
            "discussionId": "68f79e9ca7b9b96b2c13ee04",
            "ai_summary": "A structured, automated framework inspired by the knapsack problem optimizes agentic system composition by considering performance, budget, and compatibility, achieving higher success rates at lower costs.",
            "ai_keywords": [
                "agentic systems",
                "component selection",
                "capability descriptions",
                "retrieval methods",
                "composer agent",
                "performance",
                "budget constraints",
                "compatibility",
                "Pareto frontier",
                "success rates",
                "multi-agent systems",
                "agent inventory"
            ]
        },
        "publishedAt": "2025-10-18T09:37:47.000Z",
        "title": "Automated Composition of Agents: A Knapsack Approach for Agentic\n  Component Selection",
        "summary": "Designing effective agentic systems requires the seamless composition and\nintegration of agents, tools, and models within dynamic and uncertain\nenvironments. Most existing methods rely on static, semantic retrieval\napproaches for tool or agent discovery. However, effective reuse and\ncomposition of existing components remain challenging due to incomplete\ncapability descriptions and the limitations of retrieval methods. Component\nselection suffers because the decisions are not based on capability, cost, and\nreal-time utility. To address these challenges, we introduce a structured,\nautomated framework for agentic system composition that is inspired by the\nknapsack problem. Our framework enables a composer agent to systematically\nidentify, select, and assemble an optimal set of agentic components by jointly\nconsidering performance, budget constraints, and compatibility. By dynamically\ntesting candidate components and modeling their utility in real-time, our\napproach streamlines the assembly of agentic systems and facilitates scalable\nreuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five\nbenchmarking datasets shows that our online-knapsack-based composer\nconsistently lies on the Pareto frontier, achieving higher success rates at\nsignificantly lower component costs compared to our baselines. In the\nsingle-agent setup, the online knapsack composer shows a success rate\nimprovement of up to 31.6% in comparison to the retrieval baselines. In\nmulti-agent systems, the online knapsack composer increases success rate from\n37% to 87% when agents are selected from an agent inventory of 100+ agents. The\nsubstantial performance gap confirms the robust adaptability of our method\nacross diverse domains and budget constraints.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16499.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "632b8294ffe3618eff2f3774",
            "avatarUrl": "/avatars/002b5b85f41033448979ccecabac28b1.svg",
            "fullname": "Shuaichen Chang",
            "name": "shuaichenchang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.15527",
            "authors": [
                {
                    "_id": "68f6ea0d24c448936311180d",
                    "name": "Aditya Vir",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T10:59:24.000Z",
            "submittedOnDailyAt": "2025-10-21T00:34:37.118Z",
            "title": "Balanced Multi-Task Attention for Satellite Image Classification: A\n  Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without\n  Pre-Training",
            "submittedOnDailyBy": {
                "_id": "63be9021da08ed0544f36c38",
                "avatarUrl": "/avatars/42511d8b1d3a3ef99bc154c98b72dfba.svg",
                "isPro": false,
                "fullname": "onurcan",
                "user": "monurcan",
                "type": "user"
            },
            "summary": "This work presents a systematic investigation of custom convolutional neural\nnetwork architectures for satellite land use classification, achieving 97.23%\ntest accuracy on the EuroSAT dataset without reliance on pre-trained models.\nThrough three progressive architectural iterations (baseline: 94.30%,\nCBAM-enhanced: 95.98%, and balanced multi-task attention: 97.23%) we identify\nand address specific failure modes in satellite imagery classification. Our\nprincipal contribution is a novel balanced multi-task attention mechanism that\ncombines Coordinate Attention for spatial feature extraction with\nSqueeze-Excitation blocks for spectral feature extraction, unified through a\nlearnable fusion parameter. Experimental results demonstrate that this\nlearnable parameter autonomously converges to alpha approximately 0.57,\nindicating near-equal importance of spatial and spectral modalities for\nsatellite imagery. We employ progressive DropBlock regularization (5-20% by\nnetwork depth) and class-balanced loss weighting to address overfitting and\nconfusion pattern imbalance. The final 12-layer architecture achieves Cohen's\nKappa of 0.9692 with all classes exceeding 94.46% accuracy, demonstrating\nconfidence calibration with a 24.25% gap between correct and incorrect\npredictions. Our approach achieves performance within 1.34% of fine-tuned\nResNet-50 (98.57%) while requiring no external data, validating the efficacy of\nsystematic architectural design for domain-specific applications. Complete\ncode, trained models, and evaluation scripts are publicly available.",
            "upvotes": 2,
            "discussionId": "68f6ea0e24c448936311180e",
            "ai_summary": "A novel balanced multi-task attention mechanism in custom convolutional neural networks improves satellite land use classification accuracy to 97.23% on the EuroSAT dataset without pre-trained models.",
            "ai_keywords": [
                "convolutional neural networks",
                "satellite land use classification",
                "EuroSAT dataset",
                "CBAM-enhanced",
                "balanced multi-task attention",
                "Coordinate Attention",
                "Squeeze-Excitation blocks",
                "learnable fusion parameter",
                "progressive DropBlock regularization",
                "class-balanced loss weighting",
                "Cohen's Kappa",
                "ResNet-50"
            ]
        },
        "publishedAt": "2025-10-17T06:59:24.000Z",
        "title": "Balanced Multi-Task Attention for Satellite Image Classification: A\n  Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without\n  Pre-Training",
        "summary": "This work presents a systematic investigation of custom convolutional neural\nnetwork architectures for satellite land use classification, achieving 97.23%\ntest accuracy on the EuroSAT dataset without reliance on pre-trained models.\nThrough three progressive architectural iterations (baseline: 94.30%,\nCBAM-enhanced: 95.98%, and balanced multi-task attention: 97.23%) we identify\nand address specific failure modes in satellite imagery classification. Our\nprincipal contribution is a novel balanced multi-task attention mechanism that\ncombines Coordinate Attention for spatial feature extraction with\nSqueeze-Excitation blocks for spectral feature extraction, unified through a\nlearnable fusion parameter. Experimental results demonstrate that this\nlearnable parameter autonomously converges to alpha approximately 0.57,\nindicating near-equal importance of spatial and spectral modalities for\nsatellite imagery. We employ progressive DropBlock regularization (5-20% by\nnetwork depth) and class-balanced loss weighting to address overfitting and\nconfusion pattern imbalance. The final 12-layer architecture achieves Cohen's\nKappa of 0.9692 with all classes exceeding 94.46% accuracy, demonstrating\nconfidence calibration with a 24.25% gap between correct and incorrect\npredictions. Our approach achieves performance within 1.34% of fine-tuned\nResNet-50 (98.57%) while requiring no external data, validating the efficacy of\nsystematic architectural design for domain-specific applications. Complete\ncode, trained models, and evaluation scripts are publicly available.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15527.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63be9021da08ed0544f36c38",
            "avatarUrl": "/avatars/42511d8b1d3a3ef99bc154c98b72dfba.svg",
            "fullname": "onurcan",
            "name": "monurcan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.16727",
            "authors": [
                {
                    "_id": "68f769b29e6a0ffab2a9c3ae",
                    "user": {
                        "_id": "671fbe8b2e5ea49d9c5dfaca",
                        "avatarUrl": "/avatars/cd09c157707634a2000528160009c155.svg",
                        "isPro": false,
                        "fullname": "pandey",
                        "user": "sanskxr02",
                        "type": "user"
                    },
                    "name": "Sanskar Pandey",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:00:36.471Z",
                    "hidden": false
                },
                {
                    "_id": "68f769b29e6a0ffab2a9c3af",
                    "name": "Ruhaan Chopra",
                    "hidden": false
                },
                {
                    "_id": "68f769b29e6a0ffab2a9c3b0",
                    "name": "Angkul Puniya",
                    "hidden": false
                },
                {
                    "_id": "68f769b29e6a0ffab2a9c3b1",
                    "name": "Sohom Pal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-19T06:36:57.000Z",
            "submittedOnDailyAt": "2025-10-21T12:41:22.017Z",
            "title": "Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in\n  Large Language Models",
            "submittedOnDailyBy": {
                "_id": "671fbe8b2e5ea49d9c5dfaca",
                "avatarUrl": "/avatars/cd09c157707634a2000528160009c155.svg",
                "isPro": false,
                "fullname": "pandey",
                "user": "sanskxr02",
                "type": "user"
            },
            "summary": "Large language models internalize a structural trade-off between truthfulness\nand obsequious flattery, emerging from reward optimization that conflates\nhelpfulness with polite submission. This latent bias, known as sycophancy,\nmanifests as a preference for user agreement over principled reasoning. We\nintroduce Beacon, a single-turn forced-choice benchmark that isolates this bias\nindependent of conversational context, enabling precise measurement of the\ntension between factual accuracy and submissive bias. Evaluations across twelve\nstate-of-the-art models reveal that sycophancy decomposes into stable\nlinguistic and affective sub-biases, each scaling with model capacity. We\nfurther propose prompt-level and activation-level interventions that modulate\nthese biases in opposing directions, exposing the internal geometry of\nalignment as a dynamic manifold between truthfulness and socially compliant\njudgment. Beacon reframes sycophancy as a measurable form of normative\nmisgeneralization, providing a reproducible foundation for studying and\nmitigating alignment drift in large-scale generative systems.",
            "upvotes": 1,
            "discussionId": "68f769b39e6a0ffab2a9c3b2",
            "ai_summary": "Beacon, a benchmark, measures sycophancy in large language models, revealing it as a combination of linguistic and affective biases that can be mitigated through interventions.",
            "ai_keywords": [
                "large language models",
                "reward optimization",
                "sycophancy",
                "forced-choice benchmark",
                "factual accuracy",
                "submissive bias",
                "prompt-level interventions",
                "activation-level interventions",
                "alignment drift",
                "normative misgeneralization"
            ]
        },
        "publishedAt": "2025-10-19T02:36:57.000Z",
        "title": "Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in\n  Large Language Models",
        "summary": "Large language models internalize a structural trade-off between truthfulness\nand obsequious flattery, emerging from reward optimization that conflates\nhelpfulness with polite submission. This latent bias, known as sycophancy,\nmanifests as a preference for user agreement over principled reasoning. We\nintroduce Beacon, a single-turn forced-choice benchmark that isolates this bias\nindependent of conversational context, enabling precise measurement of the\ntension between factual accuracy and submissive bias. Evaluations across twelve\nstate-of-the-art models reveal that sycophancy decomposes into stable\nlinguistic and affective sub-biases, each scaling with model capacity. We\nfurther propose prompt-level and activation-level interventions that modulate\nthese biases in opposing directions, exposing the internal geometry of\nalignment as a dynamic manifold between truthfulness and socially compliant\njudgment. Beacon reframes sycophancy as a measurable form of normative\nmisgeneralization, providing a reproducible foundation for studying and\nmitigating alignment drift in large-scale generative systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16727.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "671fbe8b2e5ea49d9c5dfaca",
            "avatarUrl": "/avatars/cd09c157707634a2000528160009c155.svg",
            "fullname": "pandey",
            "name": "sanskxr02",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.16156",
            "authors": [
                {
                    "_id": "68f6f42324c44893631118c7",
                    "user": {
                        "_id": "64b5198c25882acb62fb77ef",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b5198c25882acb62fb77ef/HX9pfMEPQlfjvSAgSLplY.png",
                        "isPro": false,
                        "fullname": "Yueqian Lin",
                        "user": "linyueqian",
                        "type": "user"
                    },
                    "name": "Yueqian Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:03:33.005Z",
                    "hidden": false
                },
                {
                    "_id": "68f6f42324c44893631118c8",
                    "name": "Zhengmian Hu",
                    "hidden": false
                },
                {
                    "_id": "68f6f42324c44893631118c9",
                    "name": "Jayakumar Subramanian",
                    "hidden": false
                },
                {
                    "_id": "68f6f42324c44893631118ca",
                    "name": "Qinsi Wang",
                    "hidden": false
                },
                {
                    "_id": "68f6f42324c44893631118cb",
                    "name": "Nikos Vlassis",
                    "hidden": false
                },
                {
                    "_id": "68f6f42324c44893631118cc",
                    "name": "Hai \"Helen\" Li",
                    "hidden": false
                },
                {
                    "_id": "68f6f42324c44893631118cd",
                    "name": "Yiran Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T19:00:08.000Z",
            "submittedOnDailyAt": "2025-10-21T01:20:53.305Z",
            "title": "AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning",
            "submittedOnDailyBy": {
                "_id": "64b5198c25882acb62fb77ef",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b5198c25882acb62fb77ef/HX9pfMEPQlfjvSAgSLplY.png",
                "isPro": false,
                "fullname": "Yueqian Lin",
                "user": "linyueqian",
                "type": "user"
            },
            "summary": "Effective human-AI collaboration on complex reasoning tasks requires that\nusers understand and interact with the model's process, not just receive an\noutput. However, the monolithic text from methods like Chain-of-Thought (CoT)\nprevents this, as current interfaces lack real-time verbalization and robust\nuser barge-in. We present AsyncVoice Agent, a system whose asynchronous\narchitecture decouples a streaming LLM backend from a conversational voice\nfrontend. This design allows narration and inference to run in parallel,\nempowering users to interrupt, query, and steer the model's reasoning process\nat any time. Objective benchmarks show this approach reduces interaction\nlatency by more than 600x compared to monolithic baselines while ensuring high\nfidelity and competitive task accuracy. By enabling a two-way dialogue with a\nmodel's thought process, AsyncVoice Agent offers a new paradigm for building\nmore effective, steerable, and trustworthy human-AI systems for high-stakes\ntasks.",
            "upvotes": 1,
            "discussionId": "68f6f42324c44893631118ce",
            "ai_summary": "AsyncVoice Agent, with its asynchronous architecture, enhances human-AI collaboration by enabling real-time interaction and interruption of the model's reasoning process, significantly reducing latency while maintaining accuracy.",
            "ai_keywords": [
                "Chain-of-Thought",
                "CoT",
                "streaming LLM backend",
                "conversational voice frontend",
                "narration",
                "inference",
                "interaction latency",
                "high-fidelity",
                "task accuracy",
                "two-way dialogue"
            ]
        },
        "publishedAt": "2025-10-17T15:00:08.000Z",
        "title": "AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning",
        "summary": "Effective human-AI collaboration on complex reasoning tasks requires that\nusers understand and interact with the model's process, not just receive an\noutput. However, the monolithic text from methods like Chain-of-Thought (CoT)\nprevents this, as current interfaces lack real-time verbalization and robust\nuser barge-in. We present AsyncVoice Agent, a system whose asynchronous\narchitecture decouples a streaming LLM backend from a conversational voice\nfrontend. This design allows narration and inference to run in parallel,\nempowering users to interrupt, query, and steer the model's reasoning process\nat any time. Objective benchmarks show this approach reduces interaction\nlatency by more than 600x compared to monolithic baselines while ensuring high\nfidelity and competitive task accuracy. By enabling a two-way dialogue with a\nmodel's thought process, AsyncVoice Agent offers a new paradigm for building\nmore effective, steerable, and trustworthy human-AI systems for high-stakes\ntasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16156.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b5198c25882acb62fb77ef",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b5198c25882acb62fb77ef/HX9pfMEPQlfjvSAgSLplY.png",
            "fullname": "Yueqian Lin",
            "name": "linyueqian",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.15768",
            "authors": [
                {
                    "_id": "68f7428b24c4489363111c12",
                    "name": "Orr Paradise",
                    "hidden": false
                },
                {
                    "_id": "68f7428b24c4489363111c13",
                    "name": "David F. Gruber",
                    "hidden": false
                },
                {
                    "_id": "68f7428b24c4489363111c14",
                    "name": "Adam Tauman Kalai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T15:56:30.000Z",
            "submittedOnDailyAt": "2025-10-21T06:52:28.522Z",
            "title": "On Non-interactive Evaluation of Animal Communication Translators",
            "submittedOnDailyBy": {
                "_id": "608abf1272b50b02c4b02865",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1619708309549-608abf1272b50b02c4b02865.jpeg",
                "isPro": false,
                "fullname": "Hsuan Su",
                "user": "jacksukk",
                "type": "user"
            },
            "summary": "If you had an AI Whale-to-English translator, how could you validate whether\nor not it is working? Does one need to interact with the animals or rely on\ngrounded observations such as temperature? We provide theoretical and\nproof-of-concept experimental evidence suggesting that interaction and even\nobservations may not be necessary for sufficiently complex languages. One may\nbe able to evaluate translators solely by their English outputs, offering\npotential advantages in terms of safety, ethics, and cost. This is an instance\nof machine translation quality evaluation (MTQE) without any reference\ntranslations available. A key challenge is identifying ``hallucinations,''\nfalse translations which may appear fluent and plausible. We propose using\nsegment-by-segment translation together with the classic NLP shuffle test to\nevaluate translators. The idea is to translate animal communication, turn by\nturn, and evaluate how often the resulting translations make more sense in\norder than permuted. Proof-of-concept experiments on data-scarce human\nlanguages and constructed languages demonstrate the potential utility of this\nevaluation methodology. These human-language experiments serve solely to\nvalidate our reference-free metric under data scarcity. It is found to\ncorrelate highly with a standard evaluation based on reference translations,\nwhich are available in our experiments. We also perform a theoretical analysis\nsuggesting that interaction may not be necessary nor efficient in the early\nstages of learning to translate.",
            "upvotes": 1,
            "discussionId": "68f7428c24c4489363111c15",
            "ai_summary": "Theoretical and experimental evidence suggests that evaluating AI translators for complex languages can be done solely through their outputs, using a segment-by-segment translation and shuffle test to identify hallucinations and assess quality without reference translations.",
            "ai_keywords": [
                "machine translation quality evaluation",
                "MTQE",
                "hallucinations",
                "segment-by-segment translation",
                "shuffle test",
                "data-scarce human languages",
                "constructed languages",
                "theoretical analysis"
            ]
        },
        "publishedAt": "2025-10-17T11:56:30.000Z",
        "title": "On Non-interactive Evaluation of Animal Communication Translators",
        "summary": "If you had an AI Whale-to-English translator, how could you validate whether\nor not it is working? Does one need to interact with the animals or rely on\ngrounded observations such as temperature? We provide theoretical and\nproof-of-concept experimental evidence suggesting that interaction and even\nobservations may not be necessary for sufficiently complex languages. One may\nbe able to evaluate translators solely by their English outputs, offering\npotential advantages in terms of safety, ethics, and cost. This is an instance\nof machine translation quality evaluation (MTQE) without any reference\ntranslations available. A key challenge is identifying ``hallucinations,''\nfalse translations which may appear fluent and plausible. We propose using\nsegment-by-segment translation together with the classic NLP shuffle test to\nevaluate translators. The idea is to translate animal communication, turn by\nturn, and evaluate how often the resulting translations make more sense in\norder than permuted. Proof-of-concept experiments on data-scarce human\nlanguages and constructed languages demonstrate the potential utility of this\nevaluation methodology. These human-language experiments serve solely to\nvalidate our reference-free metric under data scarcity. It is found to\ncorrelate highly with a standard evaluation based on reference translations,\nwhich are available in our experiments. We also perform a theoretical analysis\nsuggesting that interaction may not be necessary nor efficient in the early\nstages of learning to translate.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15768.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "608abf1272b50b02c4b02865",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1619708309549-608abf1272b50b02c4b02865.jpeg",
            "fullname": "Hsuan Su",
            "name": "jacksukk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.16380",
            "authors": [
                {
                    "_id": "68f6f4f524c44893631118de",
                    "name": "Yu Ying Chiu",
                    "hidden": false
                },
                {
                    "_id": "68f6f4f524c44893631118df",
                    "name": "Michael S. Lee",
                    "hidden": false
                },
                {
                    "_id": "68f6f4f524c44893631118e0",
                    "name": "Rachel Calcott",
                    "hidden": false
                },
                {
                    "_id": "68f6f4f524c44893631118e1",
                    "name": "Brandon Handoko",
                    "hidden": false
                },
                {
                    "_id": "68f6f4f524c44893631118e2",
                    "name": "Paul de Font-Reaulx",
                    "hidden": false
                },
                {
                    "_id": "68f6f4f524c44893631118e3",
                    "name": "Paula Rodriguez",
                    "hidden": false
                },
                {
                    "_id": "68f6f4f524c44893631118e4",
                    "name": "Chen Bo Calvin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f6f4f524c44893631118e5",
                    "name": "Ziwen Han",
                    "hidden": false
                },
                {
                    "_id": "68f6f4f524c44893631118e6",
                    "name": "Udari Madhushani Sehwag",
                    "hidden": false
                },
                {
                    "_id": "68f6f4f524c44893631118e7",
                    "name": "Yash Maurya",
                    "hidden": false
                },
                {
                    "_id": "68f6f4f524c44893631118e8",
                    "name": "Christina Q Knight",
                    "hidden": false
                },
                {
                    "_id": "68f6f4f524c44893631118e9",
                    "name": "Harry R. Lloyd",
                    "hidden": false
                },
                {
                    "_id": "68f6f4f524c44893631118ea",
                    "name": "Florence Bacus",
                    "hidden": false
                },
                {
                    "_id": "68f6f4f524c44893631118eb",
                    "name": "Mantas Mazeika",
                    "hidden": false
                },
                {
                    "_id": "68f6f4f524c44893631118ec",
                    "name": "Bing Liu",
                    "hidden": false
                },
                {
                    "_id": "68f6f4f524c44893631118ed",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "68f6f4f524c44893631118ee",
                    "name": "Mitchell L Gordon",
                    "hidden": false
                },
                {
                    "_id": "68f6f4f524c44893631118ef",
                    "name": "Sydney Levine",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-18T07:34:31.000Z",
            "submittedOnDailyAt": "2025-10-21T22:36:39.542Z",
            "title": "MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in\n  Language Models, More than Outcomes",
            "submittedOnDailyBy": {
                "_id": "65fcaae6e5dc5b0ec1b726cf",
                "avatarUrl": "/avatars/d5e696d19d91550be1421d46c2a3573b.svg",
                "isPro": false,
                "fullname": "Kelly Chiu",
                "user": "kellycyy",
                "type": "user"
            },
            "summary": "As AI systems progress, we rely more on them to make decisions with us and\nfor us. To ensure that such decisions are aligned with human values, it is\nimperative for us to understand not only what decisions they make but also how\nthey come to those decisions. Reasoning language models, which provide both\nfinal responses and (partially transparent) intermediate thinking traces,\npresent a timely opportunity to study AI procedural reasoning. Unlike math and\ncode problems which often have objectively correct answers, moral dilemmas are\nan excellent testbed for process-focused evaluation because they allow for\nmultiple defensible conclusions. To do so, we present MoReBench: 1,000 moral\nscenarios, each paired with a set of rubric criteria that experts consider\nessential to include (or avoid) when reasoning about the scenarios. MoReBench\ncontains over 23 thousand criteria including identifying moral considerations,\nweighing trade-offs, and giving actionable recommendations to cover cases on AI\nadvising humans moral decisions as well as making moral decisions autonomously.\nSeparately, we curate MoReBench-Theory: 150 examples to test whether AI can\nreason under five major frameworks in normative ethics. Our results show that\nscaling laws and existing benchmarks on math, code, and scientific reasoning\ntasks fail to predict models' abilities to perform moral reasoning. Models also\nshow partiality towards specific moral frameworks (e.g., Benthamite Act\nUtilitarianism and Kantian Deontology), which might be side effects of popular\ntraining paradigms. Together, these benchmarks advance process-focused\nreasoning evaluation towards safer and more transparent AI.",
            "upvotes": 0,
            "discussionId": "68f6f4f624c44893631118f0",
            "projectPage": "https://morebench.github.io/",
            "githubRepo": "https://github.com/morebench/morebench",
            "ai_summary": "MoReBench and MoReBench-Theory provide benchmarks for evaluating AI's moral reasoning and decision-making processes, highlighting the need for process-focused evaluation and transparency in AI systems.",
            "ai_keywords": [
                "reasoning language models",
                "moral dilemmas",
                "moral scenarios",
                "rubric criteria",
                "moral considerations",
                "trade-offs",
                "actionable recommendations",
                "normative ethics",
                "Benthamite Act Utilitarianism",
                "Kantian Deontology",
                "scaling laws",
                "math",
                "code",
                "scientific reasoning",
                "process-focused reasoning evaluation"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-10-18T03:34:31.000Z",
        "title": "MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in\n  Language Models, More than Outcomes",
        "summary": "As AI systems progress, we rely more on them to make decisions with us and\nfor us. To ensure that such decisions are aligned with human values, it is\nimperative for us to understand not only what decisions they make but also how\nthey come to those decisions. Reasoning language models, which provide both\nfinal responses and (partially transparent) intermediate thinking traces,\npresent a timely opportunity to study AI procedural reasoning. Unlike math and\ncode problems which often have objectively correct answers, moral dilemmas are\nan excellent testbed for process-focused evaluation because they allow for\nmultiple defensible conclusions. To do so, we present MoReBench: 1,000 moral\nscenarios, each paired with a set of rubric criteria that experts consider\nessential to include (or avoid) when reasoning about the scenarios. MoReBench\ncontains over 23 thousand criteria including identifying moral considerations,\nweighing trade-offs, and giving actionable recommendations to cover cases on AI\nadvising humans moral decisions as well as making moral decisions autonomously.\nSeparately, we curate MoReBench-Theory: 150 examples to test whether AI can\nreason under five major frameworks in normative ethics. Our results show that\nscaling laws and existing benchmarks on math, code, and scientific reasoning\ntasks fail to predict models' abilities to perform moral reasoning. Models also\nshow partiality towards specific moral frameworks (e.g., Benthamite Act\nUtilitarianism and Kantian Deontology), which might be side effects of popular\ntraining paradigms. Together, these benchmarks advance process-focused\nreasoning evaluation towards safer and more transparent AI.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16380.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65fcaae6e5dc5b0ec1b726cf",
            "avatarUrl": "/avatars/d5e696d19d91550be1421d46c2a3573b.svg",
            "fullname": "Kelly Chiu",
            "name": "kellycyy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.16136",
            "authors": [
                {
                    "_id": "68f7229e24c44893631119eb",
                    "user": {
                        "_id": "650ec19e6620b0c57e2a551b",
                        "avatarUrl": "/avatars/c26c03fa920d857120f03c9ccb9f1d7a.svg",
                        "isPro": false,
                        "fullname": "Sayan Deb Sarkar",
                        "user": "sayandsarkar",
                        "type": "user"
                    },
                    "name": "Sayan Deb Sarkar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:02:55.826Z",
                    "hidden": false
                },
                {
                    "_id": "68f7229e24c44893631119ec",
                    "name": "Sinisa Stekovic",
                    "hidden": false
                },
                {
                    "_id": "68f7229e24c44893631119ed",
                    "name": "Vincent Lepetit",
                    "hidden": false
                },
                {
                    "_id": "68f7229e24c44893631119ee",
                    "name": "Iro Armeni",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T18:22:04.000Z",
            "submittedOnDailyAt": "2025-10-21T05:02:57.476Z",
            "title": "GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer",
            "submittedOnDailyBy": {
                "_id": "650ec19e6620b0c57e2a551b",
                "avatarUrl": "/avatars/c26c03fa920d857120f03c9ccb9f1d7a.svg",
                "isPro": false,
                "fullname": "Sayan Deb Sarkar",
                "user": "sayandsarkar",
                "type": "user"
            },
            "summary": "Transferring appearance to 3D assets using different representations of the\nappearance object - such as images or text - has garnered interest due to its\nwide range of applications in industries like gaming, augmented reality, and\ndigital content creation. However, state-of-the-art methods still fail when the\ngeometry between the input and appearance objects is significantly different. A\nstraightforward approach is to directly apply a 3D generative model, but we\nshow that this ultimately fails to produce appealing results. Instead, we\npropose a principled approach inspired by universal guidance. Given a\npretrained rectified flow model conditioned on image or text, our training-free\nmethod interacts with the sampling process by periodically adding guidance.\nThis guidance can be modeled as a differentiable loss function, and we\nexperiment with two different types of guidance including part-aware losses for\nappearance and self-similarity. Our experiments show that our approach\nsuccessfully transfers texture and geometric details to the input 3D asset,\noutperforming baselines both qualitatively and quantitatively. We also show\nthat traditional metrics are not suitable for evaluating the task due to their\ninability of focusing on local details and comparing dissimilar inputs, in\nabsence of ground truth data. We thus evaluate appearance transfer quality with\na GPT-based system objectively ranking outputs, ensuring robust and human-like\nassessment, as further confirmed by our user study. Beyond showcased scenarios,\nour method is general and could be extended to different types of diffusion\nmodels and guidance functions.",
            "upvotes": 0,
            "discussionId": "68f7229e24c44893631119ef",
            "projectPage": "https://sayands.github.io/guideflow3d/",
            "ai_summary": "A method using pretrained rectified flow models with periodic guidance successfully transfers appearance and geometric details to 3D assets, outperforming baselines and evaluated using a GPT-based system.",
            "ai_keywords": [
                "rectified flow model",
                "guidance",
                "differentiable loss function",
                "part-aware losses",
                "self-similarity",
                "GPT-based system",
                "diffusion models"
            ],
            "organization": {
                "_id": "6745347f0efcb641a76ed31f",
                "name": "gradient-spaces",
                "fullname": "Gradient Spaces Research Group",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/650ec19e6620b0c57e2a551b/D6wE8-X3R25IUKnQ5Pf89.jpeg"
            }
        },
        "publishedAt": "2025-10-17T14:22:04.000Z",
        "title": "GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer",
        "summary": "Transferring appearance to 3D assets using different representations of the\nappearance object - such as images or text - has garnered interest due to its\nwide range of applications in industries like gaming, augmented reality, and\ndigital content creation. However, state-of-the-art methods still fail when the\ngeometry between the input and appearance objects is significantly different. A\nstraightforward approach is to directly apply a 3D generative model, but we\nshow that this ultimately fails to produce appealing results. Instead, we\npropose a principled approach inspired by universal guidance. Given a\npretrained rectified flow model conditioned on image or text, our training-free\nmethod interacts with the sampling process by periodically adding guidance.\nThis guidance can be modeled as a differentiable loss function, and we\nexperiment with two different types of guidance including part-aware losses for\nappearance and self-similarity. Our experiments show that our approach\nsuccessfully transfers texture and geometric details to the input 3D asset,\noutperforming baselines both qualitatively and quantitatively. We also show\nthat traditional metrics are not suitable for evaluating the task due to their\ninability of focusing on local details and comparing dissimilar inputs, in\nabsence of ground truth data. We thus evaluate appearance transfer quality with\na GPT-based system objectively ranking outputs, ensuring robust and human-like\nassessment, as further confirmed by our user study. Beyond showcased scenarios,\nour method is general and could be extended to different types of diffusion\nmodels and guidance functions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16136.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650ec19e6620b0c57e2a551b",
            "avatarUrl": "/avatars/c26c03fa920d857120f03c9ccb9f1d7a.svg",
            "fullname": "Sayan Deb Sarkar",
            "name": "sayandsarkar",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "6745347f0efcb641a76ed31f",
            "name": "gradient-spaces",
            "fullname": "Gradient Spaces Research Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/650ec19e6620b0c57e2a551b/D6wE8-X3R25IUKnQ5Pf89.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.06471",
            "authors": [
                {
                    "_id": "68f78566a7b9b96b2c13eda4",
                    "user": {
                        "_id": "6514ba89f95f39fd02a949da",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3NqhopFY4bQIMxUKNN4es.jpeg",
                        "isPro": false,
                        "fullname": "Zihao Li",
                        "user": "Zihao-Li",
                        "type": "user"
                    },
                    "name": "Zihao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:00:18.670Z",
                    "hidden": false
                },
                {
                    "_id": "68f78566a7b9b96b2c13eda5",
                    "name": "Shaoxiong Ji",
                    "hidden": false
                },
                {
                    "_id": "68f78566a7b9b96b2c13eda6",
                    "name": "J√∂rg Tiedemann",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-07T21:15:18.000Z",
            "submittedOnDailyAt": "2025-10-21T11:37:33.989Z",
            "title": "Test-Time Scaling of Reasoning Models for Machine Translation",
            "submittedOnDailyBy": {
                "_id": "6514ba89f95f39fd02a949da",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3NqhopFY4bQIMxUKNN4es.jpeg",
                "isPro": false,
                "fullname": "Zihao Li",
                "user": "Zihao-Li",
                "type": "user"
            },
            "summary": "Test-time scaling (TTS) has enhanced the performance of Reasoning Models\n(RMs) on various tasks such as math and coding, yet its efficacy in machine\ntranslation (MT) remains underexplored. This paper investigates whether\nincreased inference-time computation improves translation quality. We evaluate\n12 RMs across a diverse suite of MT benchmarks spanning multiple domains,\nexamining three scenarios: direct translation, forced-reasoning extrapolation,\nand post-editing. Our findings show that for general-purpose RMs, TTS provides\nlimited and inconsistent benefits for direct translation, with performance\nquickly plateauing. However, the effectiveness of TTS is unlocked by\ndomain-specific fine-tuning, which aligns a model's reasoning process with task\nrequirements, leading to consistent improvements up to an optimal,\nself-determined reasoning depth. We also find that forcing a model to reason\nbeyond its natural stopping point consistently degrades translation quality. In\ncontrast, TTS proves highly effective in a post-editing context, reliably\nturning self-correction into a beneficial process. These results indicate that\nthe value of inference-time computation in MT lies not in enhancing single-pass\ntranslation with general models, but in targeted applications like multi-step,\nself-correction workflows and in conjunction with task-specialized models.",
            "upvotes": 0,
            "discussionId": "68f78567a7b9b96b2c13eda7",
            "ai_summary": "Test-time scaling improves translation quality in domain-specific models and post-editing but offers limited benefits for general-purpose models in direct translation.",
            "ai_keywords": [
                "Reasoning Models",
                "machine translation",
                "test-time scaling",
                "direct translation",
                "forced-reasoning extrapolation",
                "post-editing",
                "domain-specific fine-tuning",
                "reasoning depth",
                "self-correction"
            ],
            "organization": {
                "_id": "5eac39e88a595438195ab4a9",
                "name": "Helsinki-NLP",
                "fullname": "Language Technology Research Group at the University of Helsinki",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1588345309691-5dd96eb166059660ed1ee413.png"
            }
        },
        "publishedAt": "2025-10-07T17:15:18.000Z",
        "title": "Test-Time Scaling of Reasoning Models for Machine Translation",
        "summary": "Test-time scaling (TTS) has enhanced the performance of Reasoning Models\n(RMs) on various tasks such as math and coding, yet its efficacy in machine\ntranslation (MT) remains underexplored. This paper investigates whether\nincreased inference-time computation improves translation quality. We evaluate\n12 RMs across a diverse suite of MT benchmarks spanning multiple domains,\nexamining three scenarios: direct translation, forced-reasoning extrapolation,\nand post-editing. Our findings show that for general-purpose RMs, TTS provides\nlimited and inconsistent benefits for direct translation, with performance\nquickly plateauing. However, the effectiveness of TTS is unlocked by\ndomain-specific fine-tuning, which aligns a model's reasoning process with task\nrequirements, leading to consistent improvements up to an optimal,\nself-determined reasoning depth. We also find that forcing a model to reason\nbeyond its natural stopping point consistently degrades translation quality. In\ncontrast, TTS proves highly effective in a post-editing context, reliably\nturning self-correction into a beneficial process. These results indicate that\nthe value of inference-time computation in MT lies not in enhancing single-pass\ntranslation with general models, but in targeted applications like multi-step,\nself-correction workflows and in conjunction with task-specialized models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06471.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6514ba89f95f39fd02a949da",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3NqhopFY4bQIMxUKNN4es.jpeg",
            "fullname": "Zihao Li",
            "name": "Zihao-Li",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "5eac39e88a595438195ab4a9",
            "name": "Helsinki-NLP",
            "fullname": "Language Technology Research Group at the University of Helsinki",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1588345309691-5dd96eb166059660ed1ee413.png"
        },
        "isAuthorParticipating": true
    }
]