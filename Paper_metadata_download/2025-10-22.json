[
    {
        "paper": {
            "id": "2510.18866",
            "authors": [
                {
                    "_id": "68f838947669bcaeecce0c01",
                    "name": "Jizhan Fang",
                    "hidden": false
                },
                {
                    "_id": "68f838947669bcaeecce0c02",
                    "name": "Xinle Deng",
                    "hidden": false
                },
                {
                    "_id": "68f838947669bcaeecce0c03",
                    "name": "Haoming Xu",
                    "hidden": false
                },
                {
                    "_id": "68f838947669bcaeecce0c04",
                    "name": "Ziyan Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f838947669bcaeecce0c05",
                    "name": "Yuqi Tang",
                    "hidden": false
                },
                {
                    "_id": "68f838947669bcaeecce0c06",
                    "name": "Ziwen Xu",
                    "hidden": false
                },
                {
                    "_id": "68f838947669bcaeecce0c07",
                    "name": "Shumin Deng",
                    "hidden": false
                },
                {
                    "_id": "68f838947669bcaeecce0c08",
                    "user": {
                        "_id": "6122fbe636a4c36a99dbea7b",
                        "avatarUrl": "/avatars/c0cd2c1ef58e315d9adda9d26000f625.svg",
                        "isPro": false,
                        "fullname": "Yunzhi Yao",
                        "user": "cowTodd",
                        "type": "user"
                    },
                    "name": "Yunzhi Yao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:44:27.446Z",
                    "hidden": false
                },
                {
                    "_id": "68f838947669bcaeecce0c09",
                    "name": "Mengru Wang",
                    "hidden": false
                },
                {
                    "_id": "68f838947669bcaeecce0c0a",
                    "name": "Shuofei Qiao",
                    "hidden": false
                },
                {
                    "_id": "68f838947669bcaeecce0c0b",
                    "name": "Huajun Chen",
                    "hidden": false
                },
                {
                    "_id": "68f838947669bcaeecce0c0c",
                    "user": {
                        "_id": "620b3bbb0668e435407c8d0a",
                        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                        "isPro": true,
                        "fullname": "Ningyu Zhang",
                        "user": "Ningyu",
                        "type": "user"
                    },
                    "name": "Ningyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:44:29.811Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/M2oygkxDK_IQYqx0SE3VE.png"
            ],
            "publishedAt": "2025-10-21T17:58:17.000Z",
            "submittedOnDailyAt": "2025-10-22T00:37:06.095Z",
            "title": "LightMem: Lightweight and Efficient Memory-Augmented Generation",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": true,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) struggle\nto effectively leverage historical interaction information in dynamic and\ncomplex environments. Memory systems enable LLMs to move beyond stateless\ninteractions by introducing persistent information storage, retrieval, and\nutilization mechanisms. However, existing memory systems often introduce\nsubstantial time and computational overhead. To this end, we introduce a new\nmemory system called LightMem, which strikes a balance between the performance\nand efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of\nhuman memory, LightMem organizes memory into three complementary stages. First,\ncognition-inspired sensory memory rapidly filters irrelevant information\nthrough lightweight compression and groups information according to their\ntopics. Next, topic-aware short-term memory consolidates these topic-based\ngroups, organizing and summarizing content for more structured access. Finally,\nlong-term memory with sleep-time update employs an offline procedure that\ndecouples consolidation from online inference. Experiments on LongMemEval with\nGPT and Qwen backbones show that LightMem outperforms strong baselines in\naccuracy (up to 10.9% gains) while reducing token usage by up to 117x, API\ncalls by up to 159x, and runtime by over 12x. The code is available at\nhttps://github.com/zjunlp/LightMem.",
            "upvotes": 84,
            "discussionId": "68f838947669bcaeecce0c0d",
            "githubRepo": "https://github.com/zjunlp/LightMem",
            "ai_summary": "LightMem, a memory system inspired by human memory, enhances LLMs by efficiently managing historical interaction information, improving accuracy and reducing computational costs.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "memory systems",
                "LightMem",
                "Atkinson-Shiffrin model",
                "sensory memory",
                "topic-aware short-term memory",
                "long-term memory",
                "sleep-time update",
                "LongMemEval",
                "GPT",
                "Qwen",
                "token usage",
                "API calls",
                "runtime"
            ],
            "githubStars": 110,
            "organization": {
                "_id": "6345aadf5efccdc07f1365a5",
                "name": "ZhejiangUniversity",
                "fullname": "Zhejiang University"
            }
        },
        "publishedAt": "2025-10-21T13:58:17.000Z",
        "title": "LightMem: Lightweight and Efficient Memory-Augmented Generation",
        "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) struggle\nto effectively leverage historical interaction information in dynamic and\ncomplex environments. Memory systems enable LLMs to move beyond stateless\ninteractions by introducing persistent information storage, retrieval, and\nutilization mechanisms. However, existing memory systems often introduce\nsubstantial time and computational overhead. To this end, we introduce a new\nmemory system called LightMem, which strikes a balance between the performance\nand efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of\nhuman memory, LightMem organizes memory into three complementary stages. First,\ncognition-inspired sensory memory rapidly filters irrelevant information\nthrough lightweight compression and groups information according to their\ntopics. Next, topic-aware short-term memory consolidates these topic-based\ngroups, organizing and summarizing content for more structured access. Finally,\nlong-term memory with sleep-time update employs an offline procedure that\ndecouples consolidation from online inference. Experiments on LongMemEval with\nGPT and Qwen backbones show that LightMem outperforms strong baselines in\naccuracy (up to 10.9% gains) while reducing token usage by up to 117x, API\ncalls by up to 159x, and runtime by over 12x. The code is available at\nhttps://github.com/zjunlp/LightMem.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/M2oygkxDK_IQYqx0SE3VE.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18866.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 31
        },
        "organization": {
            "_id": "6345aadf5efccdc07f1365a5",
            "name": "ZhejiangUniversity",
            "fullname": "Zhejiang University"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.18135",
            "authors": [
                {
                    "_id": "68f847ed7669bcaeecce0d31",
                    "name": "Jiahan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f847ed7669bcaeecce0d32",
                    "name": "Muqing Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f847ed7669bcaeecce0d33",
                    "name": "Nanru Dai",
                    "hidden": false
                },
                {
                    "_id": "68f847ed7669bcaeecce0d34",
                    "name": "Taiming Lu",
                    "hidden": false
                },
                {
                    "_id": "68f847ed7669bcaeecce0d35",
                    "user": {
                        "_id": "61f7fa24b5e6e866f9abdaed",
                        "avatarUrl": "/avatars/8d43531365e2e78e568db9e0a421196a.svg",
                        "isPro": false,
                        "fullname": "Arda UzunoÄŸlu",
                        "user": "ardauzunoglu",
                        "type": "user"
                    },
                    "name": "Arda Uzunoglu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:43:37.393Z",
                    "hidden": false
                },
                {
                    "_id": "68f847ed7669bcaeecce0d36",
                    "user": {
                        "_id": "63fe24448b3c5087ff866b39",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fe24448b3c5087ff866b39/ObkydCg0MoFa_bNUJI805.jpeg",
                        "isPro": false,
                        "fullname": "Shunchi Zhang",
                        "user": "ShunchiZhang",
                        "type": "user"
                    },
                    "name": "Shunchi Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:43:34.948Z",
                    "hidden": false
                },
                {
                    "_id": "68f847ed7669bcaeecce0d37",
                    "name": "Yana Wei",
                    "hidden": false
                },
                {
                    "_id": "68f847ed7669bcaeecce0d38",
                    "name": "Jiahao Wang",
                    "hidden": false
                },
                {
                    "_id": "68f847ed7669bcaeecce0d39",
                    "name": "Vishal M. Patel",
                    "hidden": false
                },
                {
                    "_id": "68f847ed7669bcaeecce0d3a",
                    "name": "Paul Pu Liang",
                    "hidden": false
                },
                {
                    "_id": "68f847ed7669bcaeecce0d3b",
                    "name": "Daniel Khashabi",
                    "hidden": false
                },
                {
                    "_id": "68f847ed7669bcaeecce0d3c",
                    "name": "Cheng Peng",
                    "hidden": false
                },
                {
                    "_id": "68f847ed7669bcaeecce0d3d",
                    "name": "Rama Chellappa",
                    "hidden": false
                },
                {
                    "_id": "68f847ed7669bcaeecce0d3e",
                    "name": "Tianmin Shu",
                    "hidden": false
                },
                {
                    "_id": "68f847ed7669bcaeecce0d3f",
                    "name": "Alan Yuille",
                    "hidden": false
                },
                {
                    "_id": "68f847ed7669bcaeecce0d40",
                    "name": "Yilun Du",
                    "hidden": false
                },
                {
                    "_id": "68f847ed7669bcaeecce0d41",
                    "name": "Jieneng Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/660c9ac4b202fcf3892f62fa/mwYURbjbGWRjhpOSMNbDT.png"
            ],
            "publishedAt": "2025-10-20T22:09:15.000Z",
            "submittedOnDailyAt": "2025-10-22T01:30:11.531Z",
            "title": "World-in-World: World Models in a Closed-Loop World",
            "submittedOnDailyBy": {
                "_id": "660c9ac4b202fcf3892f62fa",
                "avatarUrl": "/avatars/7314fd5f3f642096d0e37d3194f1aa7e.svg",
                "isPro": false,
                "fullname": "Jieneng Chen",
                "user": "jienengchen",
                "type": "user"
            },
            "summary": "Generative world models (WMs) can now simulate worlds with striking visual\nrealism, which naturally raises the question of whether they can endow embodied\nagents with predictive perception for decision making. Progress on this\nquestion has been limited by fragmented evaluation: most existing benchmarks\nadopt open-loop protocols that emphasize visual quality in isolation, leaving\nthe core issue of embodied utility unresolved, i.e., do WMs actually help\nagents succeed at embodied tasks? To address this gap, we introduce\nWorld-in-World, the first open platform that benchmarks WMs in a closed-loop\nworld that mirrors real agent-environment interactions. World-in-World provides\na unified online planning strategy and a standardized action API, enabling\nheterogeneous WMs for decision making. We curate four closed-loop environments\nthat rigorously evaluate diverse WMs, prioritize task success as the primary\nmetric, and move beyond the common focus on visual quality; we also present the\nfirst data scaling law for world models in embodied settings. Our study\nuncovers three surprises: (1) visual quality alone does not guarantee task\nsuccess, controllability matters more; (2) scaling post-training with\naction-observation data is more effective than upgrading the pretrained video\ngenerators; and (3) allocating more inference-time compute allows WMs to\nsubstantially improve closed-loop performance.",
            "upvotes": 68,
            "discussionId": "68f847ed7669bcaeecce0d42",
            "githubRepo": "https://github.com/World-In-World/world-in-world",
            "ai_summary": "World-in-World evaluates generative world models in closed-loop environments, emphasizing task success over visual quality and revealing insights into controllability, data scaling, and compute allocation.",
            "ai_keywords": [
                "generative world models",
                "WM",
                "visual realism",
                "predictive perception",
                "decision making",
                "open-loop protocols",
                "closed-loop world",
                "agent-environment interactions",
                "unified online planning strategy",
                "action API",
                "task success",
                "visual quality",
                "controllability",
                "data scaling law",
                "action-observation data",
                "pretrained video generators",
                "inference-time compute"
            ],
            "githubStars": 23
        },
        "publishedAt": "2025-10-20T18:09:15.000Z",
        "title": "World-in-World: World Models in a Closed-Loop World",
        "summary": "Generative world models (WMs) can now simulate worlds with striking visual\nrealism, which naturally raises the question of whether they can endow embodied\nagents with predictive perception for decision making. Progress on this\nquestion has been limited by fragmented evaluation: most existing benchmarks\nadopt open-loop protocols that emphasize visual quality in isolation, leaving\nthe core issue of embodied utility unresolved, i.e., do WMs actually help\nagents succeed at embodied tasks? To address this gap, we introduce\nWorld-in-World, the first open platform that benchmarks WMs in a closed-loop\nworld that mirrors real agent-environment interactions. World-in-World provides\na unified online planning strategy and a standardized action API, enabling\nheterogeneous WMs for decision making. We curate four closed-loop environments\nthat rigorously evaluate diverse WMs, prioritize task success as the primary\nmetric, and move beyond the common focus on visual quality; we also present the\nfirst data scaling law for world models in embodied settings. Our study\nuncovers three surprises: (1) visual quality alone does not guarantee task\nsuccess, controllability matters more; (2) scaling post-training with\naction-observation data is more effective than upgrading the pretrained video\ngenerators; and (3) allocating more inference-time compute allows WMs to\nsubstantially improve closed-loop performance.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/660c9ac4b202fcf3892f62fa/mwYURbjbGWRjhpOSMNbDT.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18135.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "660c9ac4b202fcf3892f62fa",
            "avatarUrl": "/avatars/7314fd5f3f642096d0e37d3194f1aa7e.svg",
            "fullname": "Jieneng Chen",
            "name": "jienengchen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18121",
            "authors": [
                {
                    "_id": "68f90580b9b2e4ae04673658",
                    "name": "Yonghao Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68f90580b9b2e4ae04673659",
                    "name": "Junda Chen",
                    "hidden": false
                },
                {
                    "_id": "68f90580b9b2e4ae0467365a",
                    "name": "Bo Pang",
                    "hidden": false
                },
                {
                    "_id": "68f90580b9b2e4ae0467365b",
                    "name": "Yi Gu",
                    "hidden": false
                },
                {
                    "_id": "68f90580b9b2e4ae0467365c",
                    "name": "Yibo Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f90580b9b2e4ae0467365d",
                    "name": "Yimin Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f90580b9b2e4ae0467365e",
                    "name": "Ion Stoica",
                    "hidden": false
                },
                {
                    "_id": "68f90580b9b2e4ae0467365f",
                    "name": "Eric Xing",
                    "hidden": false
                },
                {
                    "_id": "68f90580b9b2e4ae04673660",
                    "name": "Hao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T21:40:51.000Z",
            "submittedOnDailyAt": "2025-10-22T15:04:49.121Z",
            "title": "Efficient Long-context Language Model Training by Core Attention\n  Disaggregation",
            "submittedOnDailyBy": {
                "_id": "643839d9581e6bf0fa9c835e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643839d9581e6bf0fa9c835e/JxlgR-zQhms-rfF0sDxD8.jpeg",
                "isPro": false,
                "fullname": "Junda Chen",
                "user": "GindaChen",
                "type": "user"
            },
            "summary": "We present core attention disaggregation (CAD), a technique that improves\nlong-context large language model training by decoupling the core attention\ncomputation, softmax(QK^T)V, from the rest of the model and executing it on a\nseparate pool of devices. In existing systems, core attention is colocated with\nother layers; at long context lengths, its quadratic compute growth compared to\nthe near-linear growth of other components causes load imbalance and stragglers\nacross data and pipeline parallel groups. CAD is enabled by two observations.\nFirst, core attention is stateless: it has no trainable parameters and only\nminimal transient data, so balancing reduces to scheduling compute-bound tasks.\nSecond, it is composable: modern attention kernels retain high efficiency when\nprocessing fused batches of token-level shards with arbitrary lengths. CAD\npartitions core attention into token-level tasks and dispatches them to\ndedicated attention servers, which dynamically rebatch tasks to equalize\ncompute without sacrificing kernel efficiency. We implement CAD in a system\ncalled DistCA, which uses a ping-pong execution scheme to fully overlap\ncommunication with computation and in-place execution on attention servers to\nreduce memory use. On 512 H200 GPUs and context lengths up to 512k tokens,\nDistCA improves end-to-end training throughput by up to 1.35x, eliminates data\nand pipeline parallel stragglers, and achieves near-perfect compute and memory\nbalance.",
            "upvotes": 66,
            "discussionId": "68f90581b9b2e4ae04673661",
            "ai_summary": "CAD, a technique for long-context large language model training, improves throughput and balance by decoupling and distributing core attention computations.",
            "ai_keywords": [
                "core attention disaggregation",
                "core attention",
                "softmax(QK^T)V",
                "data parallel",
                "pipeline parallel",
                "token-level shards",
                "attention servers",
                "DistCA",
                "ping-pong execution",
                "in-place execution"
            ]
        },
        "publishedAt": "2025-10-20T17:40:51.000Z",
        "title": "Efficient Long-context Language Model Training by Core Attention\n  Disaggregation",
        "summary": "We present core attention disaggregation (CAD), a technique that improves\nlong-context large language model training by decoupling the core attention\ncomputation, softmax(QK^T)V, from the rest of the model and executing it on a\nseparate pool of devices. In existing systems, core attention is colocated with\nother layers; at long context lengths, its quadratic compute growth compared to\nthe near-linear growth of other components causes load imbalance and stragglers\nacross data and pipeline parallel groups. CAD is enabled by two observations.\nFirst, core attention is stateless: it has no trainable parameters and only\nminimal transient data, so balancing reduces to scheduling compute-bound tasks.\nSecond, it is composable: modern attention kernels retain high efficiency when\nprocessing fused batches of token-level shards with arbitrary lengths. CAD\npartitions core attention into token-level tasks and dispatches them to\ndedicated attention servers, which dynamically rebatch tasks to equalize\ncompute without sacrificing kernel efficiency. We implement CAD in a system\ncalled DistCA, which uses a ping-pong execution scheme to fully overlap\ncommunication with computation and in-place execution on attention servers to\nreduce memory use. On 512 H200 GPUs and context lengths up to 512k tokens,\nDistCA improves end-to-end training throughput by up to 1.35x, eliminates data\nand pipeline parallel stragglers, and achieves near-perfect compute and memory\nbalance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18121.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643839d9581e6bf0fa9c835e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643839d9581e6bf0fa9c835e/JxlgR-zQhms-rfF0sDxD8.jpeg",
            "fullname": "Junda Chen",
            "name": "GindaChen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18701",
            "authors": [
                {
                    "_id": "68f839917669bcaeecce0c98",
                    "user": {
                        "_id": "654c6845bac6e6e49895a5b5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KXQaAxulqr8jNBSpEaYM4.png",
                        "isPro": false,
                        "fullname": "SII-Yibin Wang",
                        "user": "CodeGoat24",
                        "type": "user"
                    },
                    "name": "Yibin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:44:11.252Z",
                    "hidden": true
                },
                {
                    "_id": "68f839917669bcaeecce0c99",
                    "name": "Zhimin Li",
                    "hidden": false
                },
                {
                    "_id": "68f839917669bcaeecce0c9a",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/kajwuVzd4pDucSPlwghxo.png",
                        "isPro": true,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:44:14.239Z",
                    "hidden": false
                },
                {
                    "_id": "68f839917669bcaeecce0c9b",
                    "name": "Jiazi Bu",
                    "hidden": false
                },
                {
                    "_id": "68f839917669bcaeecce0c9c",
                    "name": "Yujie Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f839917669bcaeecce0c9d",
                    "name": "Yi Xin",
                    "hidden": false
                },
                {
                    "_id": "68f839917669bcaeecce0c9e",
                    "name": "Junjun He",
                    "hidden": false
                },
                {
                    "_id": "68f839917669bcaeecce0c9f",
                    "name": "Chunyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68f839917669bcaeecce0ca0",
                    "name": "Qinglin Lu",
                    "hidden": false
                },
                {
                    "_id": "68f839917669bcaeecce0ca1",
                    "name": "Cheng Jin",
                    "hidden": false
                },
                {
                    "_id": "68f839917669bcaeecce0ca2",
                    "name": "Jiaqi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T14:56:46.000Z",
            "submittedOnDailyAt": "2025-10-22T00:25:38.299Z",
            "title": "UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image\n  Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent progress in text-to-image (T2I) generation underscores the importance\nof reliable benchmarks in evaluating how accurately generated images reflect\nthe semantics of their textual prompt. However, (1) existing benchmarks lack\nthe diversity of prompt scenarios and multilingual support, both essential for\nreal-world applicability; (2) they offer only coarse evaluations across primary\ndimensions, covering a narrow range of sub-dimensions, and fall short in\nfine-grained sub-dimension assessment. To address these limitations, we\nintroduce UniGenBench++, a unified semantic assessment benchmark for T2I\ngeneration. Specifically, it comprises 600 prompts organized hierarchically to\nensure both coverage and efficiency: (1) spans across diverse real-world\nscenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively\nprobes T2I models' semantic consistency over 10 primary and 27 sub evaluation\ncriteria, with each prompt assessing multiple testpoints. To rigorously assess\nmodel robustness to variations in language and prompt length, we provide both\nEnglish and Chinese versions of each prompt in short and long forms. Leveraging\nthe general world knowledge and fine-grained image understanding capabilities\nof a closed-source Multi-modal Large Language Model (MLLM), i.e.,\nGemini-2.5-Pro, an effective pipeline is developed for reliable benchmark\nconstruction and streamlined model assessment. Moreover, to further facilitate\ncommunity use, we train a robust evaluation model that enables offline\nassessment of T2I model outputs. Through comprehensive benchmarking of both\nopen- and closed-sourced T2I models, we systematically reveal their strengths\nand weaknesses across various aspects.",
            "upvotes": 59,
            "discussionId": "68f839917669bcaeecce0ca3",
            "projectPage": "https://codegoat24.github.io/UniGenBench/",
            "githubRepo": "https://github.com/CodeGoat24/UniGenBench",
            "ai_summary": "UniGenBench++ is a comprehensive benchmark for text-to-image generation that evaluates semantic consistency across diverse scenarios and languages using a hierarchical prompt structure and a robust evaluation pipeline.",
            "ai_keywords": [
                "text-to-image generation",
                "semantic assessment",
                "benchmark",
                "prompt scenarios",
                "multilingual support",
                "semantic consistency",
                "evaluation criteria",
                "Multi-modal Large Language Model",
                "MLLM",
                "Gemini-2.5-Pro",
                "offline assessment"
            ],
            "githubStars": 92
        },
        "publishedAt": "2025-10-21T10:56:46.000Z",
        "title": "UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image\n  Generation",
        "summary": "Recent progress in text-to-image (T2I) generation underscores the importance\nof reliable benchmarks in evaluating how accurately generated images reflect\nthe semantics of their textual prompt. However, (1) existing benchmarks lack\nthe diversity of prompt scenarios and multilingual support, both essential for\nreal-world applicability; (2) they offer only coarse evaluations across primary\ndimensions, covering a narrow range of sub-dimensions, and fall short in\nfine-grained sub-dimension assessment. To address these limitations, we\nintroduce UniGenBench++, a unified semantic assessment benchmark for T2I\ngeneration. Specifically, it comprises 600 prompts organized hierarchically to\nensure both coverage and efficiency: (1) spans across diverse real-world\nscenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively\nprobes T2I models' semantic consistency over 10 primary and 27 sub evaluation\ncriteria, with each prompt assessing multiple testpoints. To rigorously assess\nmodel robustness to variations in language and prompt length, we provide both\nEnglish and Chinese versions of each prompt in short and long forms. Leveraging\nthe general world knowledge and fine-grained image understanding capabilities\nof a closed-source Multi-modal Large Language Model (MLLM), i.e.,\nGemini-2.5-Pro, an effective pipeline is developed for reliable benchmark\nconstruction and streamlined model assessment. Moreover, to further facilitate\ncommunity use, we train a robust evaluation model that enables offline\nassessment of T2I model outputs. Through comprehensive benchmarking of both\nopen- and closed-sourced T2I models, we systematically reveal their strengths\nand weaknesses across various aspects.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18701.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 138
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.16880",
            "authors": [
                {
                    "_id": "68f839d57669bcaeecce0cbe",
                    "user": {
                        "_id": "661b9d96c153e4a0a25adc3e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661b9d96c153e4a0a25adc3e/VRt7kCQ0KdJp-lhPLOajO.jpeg",
                        "isPro": false,
                        "fullname": "Weida Wang",
                        "user": "weidawang",
                        "type": "user"
                    },
                    "name": "Weida Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:43:56.271Z",
                    "hidden": false
                },
                {
                    "_id": "68f839d57669bcaeecce0cbf",
                    "name": "Benteng Chen",
                    "hidden": false
                },
                {
                    "_id": "68f839d57669bcaeecce0cc0",
                    "name": "Di Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f839d57669bcaeecce0cc1",
                    "name": "Wanhao Liu",
                    "hidden": false
                },
                {
                    "_id": "68f839d57669bcaeecce0cc2",
                    "user": {
                        "_id": "67348828c74a3af1aef4d3a2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67348828c74a3af1aef4d3a2/ToZ56JIM3ccAXIBNG4mx_.jpeg",
                        "isPro": false,
                        "fullname": "pushuchen",
                        "user": "komusama0930",
                        "type": "user"
                    },
                    "name": "Shuchen Pu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:43:45.089Z",
                    "hidden": false
                },
                {
                    "_id": "68f839d57669bcaeecce0cc3",
                    "name": "Ben Gao",
                    "hidden": false
                },
                {
                    "_id": "68f839d57669bcaeecce0cc4",
                    "name": "Jin Zeng",
                    "hidden": false
                },
                {
                    "_id": "68f839d57669bcaeecce0cc5",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "68f839d57669bcaeecce0cc6",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68f839d57669bcaeecce0cc7",
                    "name": "Xiaoyong Wei",
                    "hidden": false
                },
                {
                    "_id": "68f839d57669bcaeecce0cc8",
                    "name": "Tianshu Yu",
                    "hidden": false
                },
                {
                    "_id": "68f839d57669bcaeecce0cc9",
                    "name": "Tianfan Fu",
                    "hidden": false
                },
                {
                    "_id": "68f839d57669bcaeecce0cca",
                    "name": "Shuzhou Sun",
                    "hidden": false
                },
                {
                    "_id": "68f839d57669bcaeecce0ccb",
                    "name": "Jiatong Li",
                    "hidden": false
                },
                {
                    "_id": "68f839d57669bcaeecce0ccc",
                    "name": "Zifu Wang",
                    "hidden": false
                },
                {
                    "_id": "68f839d57669bcaeecce0ccd",
                    "name": "Yuqiang Li",
                    "hidden": false
                },
                {
                    "_id": "68f839d57669bcaeecce0cce",
                    "name": "Shufei Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-19T15:27:13.000Z",
            "submittedOnDailyAt": "2025-10-22T00:28:55.434Z",
            "title": "Chem-R: Learning to Reason as a Chemist",
            "submittedOnDailyBy": {
                "_id": "661b9d96c153e4a0a25adc3e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661b9d96c153e4a0a25adc3e/VRt7kCQ0KdJp-lhPLOajO.jpeg",
                "isPro": false,
                "fullname": "Weida Wang",
                "user": "weidawang",
                "type": "user"
            },
            "summary": "Although large language models (LLMs) have significant potential to advance\nchemical discovery, current LLMs lack core chemical knowledge, produce\nunreliable reasoning trajectories, and exhibit suboptimal performance across\ndiverse chemical tasks. To address these challenges, we propose Chem-R, a\ngeneralizable Chemical Reasoning model designed to emulate the deliberative\nprocesses of chemists. Chem-R is trained through a three-phase framework that\nprogressively builds advanced reasoning capabilities, including: 1) Chemical\nFoundation Training, which establishes core chemical knowledge. 2) Chemical\nReasoning Protocol Distillation, incorporating structured, expert-like\nreasoning traces to guide systematic and reliable problem solving. 3)\nMulti-task Group Relative Policy Optimization that optimizes the model for\nbalanced performance across diverse molecular- and reaction-level tasks. This\nstructured pipeline enables Chem-R to achieve state-of-the-art performance on\ncomprehensive benchmarks, surpassing leading large language models, including\nGemini-2.5-Pro and DeepSeek-R1, by up to 46% on molecular tasks and 66% on\nreaction tasks. Meanwhile, Chem-R also consistently outperforms the existing\nchemical foundation models across both molecular and reaction level tasks.\nThese results highlight Chem-R's robust generalization, interpretability, and\npotential as a foundation for next-generation AI-driven chemical discovery.",
            "upvotes": 46,
            "discussionId": "68f839d67669bcaeecce0ccf",
            "githubRepo": "https://github.com/davidweidawang/Chem-R",
            "ai_summary": "Chem-R, a three-phase trained Chemical Reasoning model, achieves superior performance on chemical tasks by integrating core knowledge, expert reasoning, and multi-task optimization.",
            "ai_keywords": [
                "Chem-R",
                "Chemical Foundation Training",
                "Chemical Reasoning Protocol Distillation",
                "Multi-task Group Relative Policy Optimization",
                "state-of-the-art performance",
                "molecular tasks",
                "reaction tasks",
                "Gemini-2.5-Pro",
                "DeepSeek-R1",
                "robust generalization",
                "interpretability"
            ],
            "githubStars": 9,
            "organization": {
                "_id": "6747ee5decec679eafb90450",
                "name": "ShanghaiAiLab",
                "fullname": "shanghai ailab "
            }
        },
        "publishedAt": "2025-10-19T11:27:13.000Z",
        "title": "Chem-R: Learning to Reason as a Chemist",
        "summary": "Although large language models (LLMs) have significant potential to advance\nchemical discovery, current LLMs lack core chemical knowledge, produce\nunreliable reasoning trajectories, and exhibit suboptimal performance across\ndiverse chemical tasks. To address these challenges, we propose Chem-R, a\ngeneralizable Chemical Reasoning model designed to emulate the deliberative\nprocesses of chemists. Chem-R is trained through a three-phase framework that\nprogressively builds advanced reasoning capabilities, including: 1) Chemical\nFoundation Training, which establishes core chemical knowledge. 2) Chemical\nReasoning Protocol Distillation, incorporating structured, expert-like\nreasoning traces to guide systematic and reliable problem solving. 3)\nMulti-task Group Relative Policy Optimization that optimizes the model for\nbalanced performance across diverse molecular- and reaction-level tasks. This\nstructured pipeline enables Chem-R to achieve state-of-the-art performance on\ncomprehensive benchmarks, surpassing leading large language models, including\nGemini-2.5-Pro and DeepSeek-R1, by up to 46% on molecular tasks and 66% on\nreaction tasks. Meanwhile, Chem-R also consistently outperforms the existing\nchemical foundation models across both molecular and reaction level tasks.\nThese results highlight Chem-R's robust generalization, interpretability, and\npotential as a foundation for next-generation AI-driven chemical discovery.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16880.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "661b9d96c153e4a0a25adc3e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661b9d96c153e4a0a25adc3e/VRt7kCQ0KdJp-lhPLOajO.jpeg",
            "fullname": "Weida Wang",
            "name": "weidawang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 0
        },
        "organization": {
            "_id": "6747ee5decec679eafb90450",
            "name": "ShanghaiAiLab",
            "fullname": "shanghai ailab "
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.18692",
            "authors": [
                {
                    "_id": "68f839b57669bcaeecce0cb3",
                    "name": "Weinan Jia",
                    "hidden": false
                },
                {
                    "_id": "68f839b57669bcaeecce0cb4",
                    "user": {
                        "_id": "64a247040b7851be3f228ee5",
                        "avatarUrl": "/avatars/9d6da9a66380e5dafd55a18d6aec89fc.svg",
                        "isPro": false,
                        "fullname": "Yuning Lu",
                        "user": "dalynlynlyn",
                        "type": "user"
                    },
                    "name": "Yuning Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:44:04.222Z",
                    "hidden": false
                },
                {
                    "_id": "68f839b57669bcaeecce0cb5",
                    "name": "Mengqi Huang",
                    "hidden": false
                },
                {
                    "_id": "68f839b57669bcaeecce0cb6",
                    "name": "Hualiang Wang",
                    "hidden": false
                },
                {
                    "_id": "68f839b57669bcaeecce0cb7",
                    "name": "Binyuan Huang",
                    "hidden": false
                },
                {
                    "_id": "68f839b57669bcaeecce0cb8",
                    "user": {
                        "_id": "6629d7c9fa14eaccf07d8633",
                        "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
                        "isPro": false,
                        "fullname": "Nan Chen",
                        "user": "CNcreator0331",
                        "type": "user"
                    },
                    "name": "Nan Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:44:00.299Z",
                    "hidden": false
                },
                {
                    "_id": "68f839b57669bcaeecce0cb9",
                    "name": "Mu Liu",
                    "hidden": false
                },
                {
                    "_id": "68f839b57669bcaeecce0cba",
                    "name": "Jidong Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f839b57669bcaeecce0cbb",
                    "name": "Zhendong Mao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T14:50:42.000Z",
            "submittedOnDailyAt": "2025-10-22T00:26:18.559Z",
            "title": "MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Long video generation with Diffusion Transformers (DiTs) is bottlenecked by\nthe quadratic scaling of full attention with sequence length. Since attention\nis highly redundant, outputs are dominated by a small subset of query-key\npairs. Existing sparse methods rely on blockwise coarse estimation, whose\naccuracy-efficiency trade-offs are constrained by block size. This paper\nintroduces Mixture-of-Groups Attention (MoGA), an efficient sparse attention\nthat uses a lightweight, learnable token router to precisely match tokens\nwithout blockwise estimation. Through semantic-aware routing, MoGA enables\neffective long-range interactions. As a kernel-free method, MoGA integrates\nseamlessly with modern attention stacks, including FlashAttention and sequence\nparallelism. Building on MoGA, we develop an efficient long video generation\nmodel that end-to-end produces minute-level, multi-shot, 480p videos at 24 fps,\nwith a context length of approximately 580k. Comprehensive experiments on\nvarious video generation tasks validate the effectiveness of our approach.",
            "upvotes": 32,
            "discussionId": "68f839b67669bcaeecce0cbc",
            "githubRepo": "https://github.com/bytedance-fanqie-ai/MoGA",
            "ai_summary": "Mixture-of-Groups Attention (MoGA) enables efficient long video generation by addressing the quadratic scaling issue of full attention in Diffusion Transformers.",
            "ai_keywords": [
                "Diffusion Transformers",
                "DiTs",
                "full attention",
                "sequence length",
                "sparse attention",
                "token router",
                "semantic-aware routing",
                "kernel-free method",
                "FlashAttention",
                "sequence parallelism",
                "long video generation",
                "minute-level",
                "multi-shot",
                "480p",
                "24 fps",
                "context length"
            ],
            "githubStars": 18,
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "publishedAt": "2025-10-21T10:50:42.000Z",
        "title": "MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation",
        "summary": "Long video generation with Diffusion Transformers (DiTs) is bottlenecked by\nthe quadratic scaling of full attention with sequence length. Since attention\nis highly redundant, outputs are dominated by a small subset of query-key\npairs. Existing sparse methods rely on blockwise coarse estimation, whose\naccuracy-efficiency trade-offs are constrained by block size. This paper\nintroduces Mixture-of-Groups Attention (MoGA), an efficient sparse attention\nthat uses a lightweight, learnable token router to precisely match tokens\nwithout blockwise estimation. Through semantic-aware routing, MoGA enables\neffective long-range interactions. As a kernel-free method, MoGA integrates\nseamlessly with modern attention stacks, including FlashAttention and sequence\nparallelism. Building on MoGA, we develop an efficient long video generation\nmodel that end-to-end produces minute-level, multi-shot, 480p videos at 24 fps,\nwith a context length of approximately 580k. Comprehensive experiments on\nvarious video generation tasks validate the effectiveness of our approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18692.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 138
        },
        "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.18876",
            "authors": [
                {
                    "_id": "68f837947669bcaeecce0be2",
                    "user": {
                        "_id": "6499809cf19fc795e7724e43",
                        "avatarUrl": "/avatars/4b3adce8c85e2f3ef05318ded6c89c3e.svg",
                        "isPro": false,
                        "fullname": "HaochenWang",
                        "user": "HaochenWang",
                        "type": "user"
                    },
                    "name": "Haochen Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:44:32.494Z",
                    "hidden": false
                },
                {
                    "_id": "68f837947669bcaeecce0be3",
                    "name": "Yuhao Wang",
                    "hidden": false
                },
                {
                    "_id": "68f837947669bcaeecce0be4",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f837947669bcaeecce0be5",
                    "name": "Yikang Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f837947669bcaeecce0be6",
                    "name": "Yanwei Li",
                    "hidden": false
                },
                {
                    "_id": "68f837947669bcaeecce0be7",
                    "name": "Jiacong Wang",
                    "hidden": false
                },
                {
                    "_id": "68f837947669bcaeecce0be8",
                    "name": "Ye Tian",
                    "hidden": false
                },
                {
                    "_id": "68f837947669bcaeecce0be9",
                    "user": {
                        "_id": "65a28e129acab19980226731",
                        "avatarUrl": "/avatars/abc3828f807efc4e03837b0eae063f98.svg",
                        "isPro": false,
                        "fullname": "Jiahao Meng",
                        "user": "marinero4972",
                        "type": "user"
                    },
                    "name": "Jiahao Meng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:44:34.889Z",
                    "hidden": false
                },
                {
                    "_id": "68f837947669bcaeecce0bea",
                    "name": "Zilong Huang",
                    "hidden": false
                },
                {
                    "_id": "68f837947669bcaeecce0beb",
                    "name": "Guangcan Mai",
                    "hidden": false
                },
                {
                    "_id": "68f837947669bcaeecce0bec",
                    "name": "Anran Wang",
                    "hidden": false
                },
                {
                    "_id": "68f837947669bcaeecce0bed",
                    "name": "Yunhai Tong",
                    "hidden": false
                },
                {
                    "_id": "68f837947669bcaeecce0bee",
                    "name": "Zhuochen Wang",
                    "hidden": false
                },
                {
                    "_id": "68f837947669bcaeecce0bef",
                    "name": "Xiangtai Li",
                    "hidden": false
                },
                {
                    "_id": "68f837947669bcaeecce0bf0",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T17:59:59.000Z",
            "submittedOnDailyAt": "2025-10-22T00:17:24.157Z",
            "title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for\n  Multimodal LLMs",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "While Multimodal Large Language Models (MLLMs) excel at holistic\nunderstanding, they struggle in capturing the dense world with complex scenes,\nrequiring fine-grained analysis of intricate details and object\ninter-relationships. Region-level MLLMs have been a promising step. However,\nprevious attempts are generally optimized to understand given regions in\nisolation, neglecting crucial global contexts. To address this, we introduce\nGrasp Any Region (GAR) for comprehen- sive region-level visual understanding.\nEmpowered by an effective RoI-aligned feature replay technique, GAR supports\n(1) precise perception by leveraging necessary global contexts, and (2)\nmodeling interactions between multiple prompts. Together, it then naturally\nachieves (3) advanced compositional reasoning to answer specific free-form\nquestions about any region, shifting the paradigm from passive description to\nactive dialogue. Moreover, we construct GAR-Bench, which not only provides a\nmore accurate evaluation of single-region comprehension, but also, more\nimportantly, measures interactions and complex reasoning across multiple\nregions. Extensive experiments have demonstrated that GAR-1B not only maintains\nthe state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5\non DLC-Bench, but also excels at modeling relationships between multiple\nprompts with advanced comprehension capabilities, even surpassing InternVL3-78B\non GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms\nin-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong\ncapabilities can be easily transferred to videos.",
            "upvotes": 29,
            "discussionId": "68f837957669bcaeecce0bf1",
            "githubRepo": "https://github.com/Haochen-Wang409/Grasp-Any-Region",
            "ai_summary": "Grasp Any Region (GAR) enhances region-level visual understanding by integrating global contexts and modeling interactions, achieving advanced reasoning and outperforming existing models in captioning and video reference tasks.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "Region-level MLLMs",
                "Grasp Any Region",
                "RoI-aligned feature replay",
                "compositional reasoning",
                "GAR-Bench",
                "captioning",
                "VideoRefer"
            ],
            "githubStars": 41,
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "publishedAt": "2025-10-21T13:59:59.000Z",
        "title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for\n  Multimodal LLMs",
        "summary": "While Multimodal Large Language Models (MLLMs) excel at holistic\nunderstanding, they struggle in capturing the dense world with complex scenes,\nrequiring fine-grained analysis of intricate details and object\ninter-relationships. Region-level MLLMs have been a promising step. However,\nprevious attempts are generally optimized to understand given regions in\nisolation, neglecting crucial global contexts. To address this, we introduce\nGrasp Any Region (GAR) for comprehen- sive region-level visual understanding.\nEmpowered by an effective RoI-aligned feature replay technique, GAR supports\n(1) precise perception by leveraging necessary global contexts, and (2)\nmodeling interactions between multiple prompts. Together, it then naturally\nachieves (3) advanced compositional reasoning to answer specific free-form\nquestions about any region, shifting the paradigm from passive description to\nactive dialogue. Moreover, we construct GAR-Bench, which not only provides a\nmore accurate evaluation of single-region comprehension, but also, more\nimportantly, measures interactions and complex reasoning across multiple\nregions. Extensive experiments have demonstrated that GAR-1B not only maintains\nthe state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5\non DLC-Bench, but also excels at modeling relationships between multiple\nprompts with advanced comprehension capabilities, even surpassing InternVL3-78B\non GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms\nin-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong\ncapabilities can be easily transferred to videos.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18876.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 138
        },
        "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18726",
            "authors": [
                {
                    "_id": "68f8390e7669bcaeecce0c7f",
                    "user": {
                        "_id": "67f9d060395fb1a0d7e4ae21",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GjpOfOuazN7IxcXBpVqRm.png",
                        "isPro": false,
                        "fullname": "Shihao Li",
                        "user": "Leexeo",
                        "type": "user"
                    },
                    "name": "Shihao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:44:17.090Z",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c80",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c81",
                    "name": "Jiangtao Wu",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c82",
                    "name": "Zhide Lei",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c83",
                    "name": "Yiwen He",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c84",
                    "name": "Runzhe Wen",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c85",
                    "name": "Chenxi Liao",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c86",
                    "name": "Chengkang Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c87",
                    "name": "An Ping",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c88",
                    "name": "Shuo Gao",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c89",
                    "name": "Suhan Wang",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c8a",
                    "name": "Zhaozhou Bian",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c8b",
                    "name": "Zijun Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c8c",
                    "name": "Jingyi Xie",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c8d",
                    "name": "Jiayi Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c8e",
                    "name": "Jing Wang",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c8f",
                    "name": "Yifan Yao",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c90",
                    "name": "Weihao Xie",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c91",
                    "name": "Yingshui Tan",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c92",
                    "name": "Yanghai Wang",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c93",
                    "name": "Qianqian Xie",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c94",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f8390e7669bcaeecce0c95",
                    "name": "Jiaheng Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T15:25:08.000Z",
            "submittedOnDailyAt": "2025-10-22T00:24:02.796Z",
            "title": "IF-VidCap: Can Video Caption Models Follow Instructions?",
            "submittedOnDailyBy": {
                "_id": "65377c30e48353201e6fdda0",
                "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
                "isPro": false,
                "fullname": "Jiaheng Liu",
                "user": "CheeryLJH",
                "type": "user"
            },
            "summary": "Although Multimodal Large Language Models (MLLMs) have demonstrated\nproficiency in video captioning, practical applications require captions that\nfollow specific user instructions rather than generating exhaustive,\nunconstrained descriptions. Current benchmarks, however, primarily assess\ndescriptive comprehensiveness while largely overlooking instruction-following\ncapabilities. To address this gap, we introduce IF-VidCap, a new benchmark for\nevaluating controllable video captioning, which contains 1,400 high-quality\nsamples. Distinct from existing video captioning or general\ninstruction-following benchmarks, IF-VidCap incorporates a systematic framework\nthat assesses captions on two dimensions: format correctness and content\ncorrectness. Our comprehensive evaluation of over 20 prominent models reveals a\nnuanced landscape: despite the continued dominance of proprietary models, the\nperformance gap is closing, with top-tier open-source solutions now achieving\nnear-parity. Furthermore, we find that models specialized for dense captioning\nunderperform general-purpose MLLMs on complex instructions, indicating that\nfuture work should simultaneously advance both descriptive richness and\ninstruction-following fidelity.",
            "upvotes": 24,
            "discussionId": "68f8390e7669bcaeecce0c96",
            "projectPage": "https://if-vidcap.github.io/",
            "githubRepo": "https://github.com/NJU-LINK/IF-VidCap",
            "ai_summary": "A new benchmark, IF-VidCap, evaluates video captioning models on instruction-following capabilities, revealing that top-tier open-source models are closing the performance gap with proprietary models.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "MLLMs",
                "video captioning",
                "instruction-following",
                "benchmark",
                "format correctness",
                "content correctness",
                "dense captioning",
                "general-purpose MLLMs"
            ],
            "githubStars": 7,
            "organization": {
                "_id": "68edc767abe005ac1b354573",
                "name": "NJU-LINK",
                "fullname": "NJU-LINK Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"
            }
        },
        "publishedAt": "2025-10-21T11:25:08.000Z",
        "title": "IF-VidCap: Can Video Caption Models Follow Instructions?",
        "summary": "Although Multimodal Large Language Models (MLLMs) have demonstrated\nproficiency in video captioning, practical applications require captions that\nfollow specific user instructions rather than generating exhaustive,\nunconstrained descriptions. Current benchmarks, however, primarily assess\ndescriptive comprehensiveness while largely overlooking instruction-following\ncapabilities. To address this gap, we introduce IF-VidCap, a new benchmark for\nevaluating controllable video captioning, which contains 1,400 high-quality\nsamples. Distinct from existing video captioning or general\ninstruction-following benchmarks, IF-VidCap incorporates a systematic framework\nthat assesses captions on two dimensions: format correctness and content\ncorrectness. Our comprehensive evaluation of over 20 prominent models reveals a\nnuanced landscape: despite the continued dominance of proprietary models, the\nperformance gap is closing, with top-tier open-source solutions now achieving\nnear-parity. Furthermore, we find that models specialized for dense captioning\nunderperform general-purpose MLLMs on complex instructions, indicating that\nfuture work should simultaneously advance both descriptive richness and\ninstruction-following fidelity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18726.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65377c30e48353201e6fdda0",
            "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
            "fullname": "Jiaheng Liu",
            "name": "CheeryLJH",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 21
        },
        "organization": {
            "_id": "68edc767abe005ac1b354573",
            "name": "NJU-LINK",
            "fullname": "NJU-LINK Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18855",
            "authors": [
                {
                    "_id": "68f8389c7669bcaeecce0c0f",
                    "name": "Ling Team",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c10",
                    "name": "Anqi Shen",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c11",
                    "name": "Baihui Li",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c12",
                    "name": "Bin Hu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c13",
                    "name": "Bin Jing",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c14",
                    "name": "Cai Chen",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c15",
                    "name": "Chao Huang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c16",
                    "name": "Chao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c17",
                    "name": "Chaokun Yang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c18",
                    "name": "Cheng Lin",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c19",
                    "name": "Chengyao Wen",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c1a",
                    "name": "Congqi Li",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c1b",
                    "name": "Deng Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c1c",
                    "name": "Dingbo Yuan",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c1d",
                    "name": "Donghai You",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c1e",
                    "name": "Fagui Mao",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c1f",
                    "name": "Fanzhuang Meng",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c20",
                    "name": "Feng Xu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c21",
                    "name": "Guojie Li",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c22",
                    "name": "Guowei Wang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c23",
                    "name": "Hao Dai",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c24",
                    "name": "Haonan Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c25",
                    "name": "Hong Liu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c26",
                    "name": "Jia Guo",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c27",
                    "name": "Jiaming Liu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c28",
                    "name": "Jian Liu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c29",
                    "name": "Jianhao Fu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c2a",
                    "name": "Jiannan Shi",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c2b",
                    "name": "Jianwen Wang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c2c",
                    "name": "Jianxin Lai",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c2d",
                    "name": "Jin Yang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c2e",
                    "name": "Jun Mei",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c2f",
                    "name": "Jun Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c30",
                    "name": "Junbo Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c31",
                    "name": "Junping Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c32",
                    "name": "Kuan Xu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c33",
                    "name": "Le Su",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c34",
                    "name": "Lei Chen",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c35",
                    "name": "Li Tang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c36",
                    "name": "Liang Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c37",
                    "name": "Liangcheng Fu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c38",
                    "name": "Lianhao Xu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c39",
                    "name": "Linfeng Shi",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c3a",
                    "name": "Lisha Liao",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c3b",
                    "name": "Longfei Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c3c",
                    "name": "Meng Li",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c3d",
                    "name": "Mingchun Chen",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c3e",
                    "name": "Qi Zuo",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c3f",
                    "name": "Qiang Cheng",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c40",
                    "name": "Qianggang Cao",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c41",
                    "name": "Qitao Shi",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c42",
                    "name": "Quanrui Guo",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c43",
                    "name": "Senlin Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c44",
                    "name": "Shaofei Wang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c45",
                    "name": "Shaomian Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c46",
                    "name": "Shuaicheng Li",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c47",
                    "name": "Shuwei Gu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c48",
                    "name": "Siba Chen",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c49",
                    "name": "Tao Wu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c4a",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c4b",
                    "name": "Tianyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c4c",
                    "name": "Tianyu Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c4d",
                    "name": "Tiwei Bie",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c4e",
                    "name": "Tongkai Yang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c4f",
                    "name": "Wang Hong",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c50",
                    "name": "Wang Ren",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c51",
                    "name": "Weihua Chen",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c52",
                    "name": "Wenbo Yu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c53",
                    "name": "Wengang Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c54",
                    "name": "Xiangchun Wang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c55",
                    "name": "Xiaodong Yan",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c56",
                    "name": "Xiaopei Wan",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c57",
                    "name": "Xin Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c58",
                    "name": "Xinyu Kong",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c59",
                    "name": "Xinyu Tang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c5a",
                    "name": "Xudong Han",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c5b",
                    "name": "Xudong Wang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c5c",
                    "name": "Xuemin Yang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c5d",
                    "name": "Xueyu Hu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c5e",
                    "name": "Yalin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c5f",
                    "name": "Yan Sun",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c60",
                    "name": "Yicheng Shan",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c61",
                    "name": "Yilong Wang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c62",
                    "name": "Yingying Xu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c63",
                    "name": "Yongkang Liu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c64",
                    "name": "Yongzhen Guo",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c65",
                    "name": "Yuanyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c66",
                    "user": {
                        "_id": "64098738342c26884c792c93",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
                        "isPro": false,
                        "fullname": "Yuchen Yan",
                        "user": "yanyc",
                        "type": "user"
                    },
                    "name": "Yuchen Yan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:44:25.067Z",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c67",
                    "name": "Yuefan Wang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c68",
                    "name": "Yuhong Guo",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c69",
                    "name": "Zehuan Li",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c6a",
                    "name": "Zhankai Xu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c6b",
                    "name": "Zhe Li",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c6c",
                    "name": "Zhenduo Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c6d",
                    "name": "Zhengke Gui",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c6e",
                    "name": "Zhenxuan Pan",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c6f",
                    "name": "Zhenyu Huang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c70",
                    "name": "Zhenzhong Lan",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c71",
                    "name": "Zhiqiang Ding",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c72",
                    "name": "Zhiqiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c73",
                    "name": "Zhixun Li",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c74",
                    "name": "Zhizhen Liu",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c75",
                    "name": "Zihao Wang",
                    "hidden": false
                },
                {
                    "_id": "68f8389c7669bcaeecce0c76",
                    "name": "Zujie Wen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T17:46:14.000Z",
            "submittedOnDailyAt": "2025-10-22T00:21:40.175Z",
            "title": "Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale\n  Thinking Model",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present Ring-1T, the first open-source, state-of-the-art thinking model\nwith a trillion-scale parameter. It features 1 trillion total parameters and\nactivates approximately 50 billion per token. Training such models at a\ntrillion-parameter scale introduces unprecedented challenges, including\ntrain-inference misalignment, inefficiencies in rollout processing, and\nbottlenecks in the RL system. To address these, we pioneer three interconnected\ninnovations: (1) IcePop stabilizes RL training via token-level discrepancy\nmasking and clipping, resolving instability from training-inference mismatches;\n(2) C3PO++ improves resource utilization for long rollouts under a token budget\nby dynamically partitioning them, thereby obtaining high time efficiency; and\n(3) ASystem, a high-performance RL framework designed to overcome the systemic\nbottlenecks that impede trillion-parameter model training. Ring-1T delivers\nbreakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on\nHMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a\nsilver medal-level result on the IMO-2025, underscoring its exceptional\nreasoning capabilities. By releasing the complete 1T parameter MoE model to the\ncommunity, we provide the research community with direct access to cutting-edge\nreasoning capabilities. This contribution marks a significant milestone in\ndemocratizing large-scale reasoning intelligence and establishes a new baseline\nfor open-source model performance.",
            "upvotes": 20,
            "discussionId": "68f8389c7669bcaeecce0c77",
            "githubRepo": "https://github.com/inclusionAI/Ring-V2",
            "ai_summary": "Ring-1T, a trillion-parameter open-source thinking model, addresses training challenges with IcePop, C3PO++, and ASystem, achieving top results across benchmarks and democratizing large-scale reasoning intelligence.",
            "ai_keywords": [
                "IcePop",
                "C3PO++",
                "ASystem",
                "MoE",
                "token-level discrepancy masking",
                "clipping",
                "resource utilization",
                "rollout processing",
                "RL system",
                "training-inference misalignment",
                "trillion-parameter model",
                "AIME-2025",
                "HMMT-2025",
                "CodeForces",
                "ARC-AGI-v1",
                "IMO-2025"
            ],
            "githubStars": 49,
            "organization": {
                "_id": "67aea5c8f086ab0f70ed97c9",
                "name": "inclusionAI",
                "fullname": "inclusionAI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
            }
        },
        "publishedAt": "2025-10-21T13:46:14.000Z",
        "title": "Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale\n  Thinking Model",
        "summary": "We present Ring-1T, the first open-source, state-of-the-art thinking model\nwith a trillion-scale parameter. It features 1 trillion total parameters and\nactivates approximately 50 billion per token. Training such models at a\ntrillion-parameter scale introduces unprecedented challenges, including\ntrain-inference misalignment, inefficiencies in rollout processing, and\nbottlenecks in the RL system. To address these, we pioneer three interconnected\ninnovations: (1) IcePop stabilizes RL training via token-level discrepancy\nmasking and clipping, resolving instability from training-inference mismatches;\n(2) C3PO++ improves resource utilization for long rollouts under a token budget\nby dynamically partitioning them, thereby obtaining high time efficiency; and\n(3) ASystem, a high-performance RL framework designed to overcome the systemic\nbottlenecks that impede trillion-parameter model training. Ring-1T delivers\nbreakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on\nHMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a\nsilver medal-level result on the IMO-2025, underscoring its exceptional\nreasoning capabilities. By releasing the complete 1T parameter MoE model to the\ncommunity, we provide the research community with direct access to cutting-edge\nreasoning capabilities. This contribution marks a significant milestone in\ndemocratizing large-scale reasoning intelligence and establishes a new baseline\nfor open-source model performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18855.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 138
        },
        "organization": {
            "_id": "67aea5c8f086ab0f70ed97c9",
            "name": "inclusionAI",
            "fullname": "inclusionAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.17699",
            "authors": [
                {
                    "_id": "68f77d6ca7b9b96b2c13ed7d",
                    "user": {
                        "_id": "64a8fc71baf671fbebb4d032",
                        "avatarUrl": "/avatars/cffd8c2154dfa57e2ce526996992ffb8.svg",
                        "isPro": false,
                        "fullname": "Aleksandr Oganov",
                        "user": "3145tttt",
                        "type": "user"
                    },
                    "name": "Aleksandr Oganov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:00:21.831Z",
                    "hidden": false
                },
                {
                    "_id": "68f77d6ca7b9b96b2c13ed7e",
                    "name": "Ilya Bykov",
                    "hidden": false
                },
                {
                    "_id": "68f77d6ca7b9b96b2c13ed7f",
                    "name": "Eva Neudachina",
                    "hidden": false
                },
                {
                    "_id": "68f77d6ca7b9b96b2c13ed80",
                    "name": "Mishan Aliev",
                    "hidden": false
                },
                {
                    "_id": "68f77d6ca7b9b96b2c13ed81",
                    "name": "Alexander Tolmachev",
                    "hidden": false
                },
                {
                    "_id": "68f77d6ca7b9b96b2c13ed82",
                    "name": "Alexander Sidorov",
                    "hidden": false
                },
                {
                    "_id": "68f77d6ca7b9b96b2c13ed83",
                    "name": "Aleksandr Zuev",
                    "hidden": false
                },
                {
                    "_id": "68f77d6ca7b9b96b2c13ed84",
                    "name": "Andrey Okhotin",
                    "hidden": false
                },
                {
                    "_id": "68f77d6ca7b9b96b2c13ed85",
                    "name": "Denis Rakitin",
                    "hidden": false
                },
                {
                    "_id": "68f77d6ca7b9b96b2c13ed86",
                    "name": "Aibek Alanov",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64a8fc71baf671fbebb4d032/02z15_R2mQTvqO0b2W3gr.png"
            ],
            "publishedAt": "2025-10-20T16:14:38.000Z",
            "submittedOnDailyAt": "2025-10-22T11:54:57.307Z",
            "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized\n  Adversarial Solver",
            "submittedOnDailyBy": {
                "_id": "64a8fc71baf671fbebb4d032",
                "avatarUrl": "/avatars/cffd8c2154dfa57e2ce526996992ffb8.svg",
                "isPro": false,
                "fullname": "Aleksandr Oganov",
                "user": "3145tttt",
                "type": "user"
            },
            "summary": "While diffusion models achieve state-of-the-art generation quality, they\nstill suffer from computationally expensive sampling. Recent works address this\nissue with gradient-based optimization methods that distill a few-step ODE\ndiffusion solver from the full sampling process, reducing the number of\nfunction evaluations from dozens to just a few. However, these approaches often\nrely on intricate training techniques and do not explicitly focus on preserving\nfine-grained details. In this paper, we introduce the Generalized Solver: a\nsimple parameterization of the ODE sampler that does not require additional\ntraining tricks and improves quality over existing approaches. We further\ncombine the original distillation loss with adversarial training, which\nmitigates artifacts and enhances detail fidelity. We call the resulting method\nthe Generalized Adversarial Solver and demonstrate its superior performance\ncompared to existing solver training methods under similar resource\nconstraints. Code is available at https://github.com/3145tttt/GAS.",
            "upvotes": 20,
            "discussionId": "68f77d6ca7b9b96b2c13ed87",
            "githubRepo": "https://github.com/3145tttt/GAS/tree/main",
            "ai_summary": "The Generalized Adversarial Solver improves diffusion model sampling efficiency and quality by combining a simple ODE solver parameterization with adversarial training.",
            "ai_keywords": [
                "diffusion models",
                "gradient-based optimization",
                "ODE diffusion solver",
                "function evaluations",
                "adversarial training",
                "Generalized Solver",
                "Generalized Adversarial Solver"
            ],
            "githubStars": 36,
            "organization": {
                "_id": "67be94e466f702bfed94ac7d",
                "name": "Bayes-Group",
                "fullname": "Bayesian Methods Research Group",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67be93b43e718f259b6abbe3/6Swfg5YVWup39avDzjk98.jpeg"
            }
        },
        "publishedAt": "2025-10-20T12:14:38.000Z",
        "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized\n  Adversarial Solver",
        "summary": "While diffusion models achieve state-of-the-art generation quality, they\nstill suffer from computationally expensive sampling. Recent works address this\nissue with gradient-based optimization methods that distill a few-step ODE\ndiffusion solver from the full sampling process, reducing the number of\nfunction evaluations from dozens to just a few. However, these approaches often\nrely on intricate training techniques and do not explicitly focus on preserving\nfine-grained details. In this paper, we introduce the Generalized Solver: a\nsimple parameterization of the ODE sampler that does not require additional\ntraining tricks and improves quality over existing approaches. We further\ncombine the original distillation loss with adversarial training, which\nmitigates artifacts and enhances detail fidelity. We call the resulting method\nthe Generalized Adversarial Solver and demonstrate its superior performance\ncompared to existing solver training methods under similar resource\nconstraints. Code is available at https://github.com/3145tttt/GAS.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64a8fc71baf671fbebb4d032/02z15_R2mQTvqO0b2W3gr.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17699.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a8fc71baf671fbebb4d032",
            "avatarUrl": "/avatars/cffd8c2154dfa57e2ce526996992ffb8.svg",
            "fullname": "Aleksandr Oganov",
            "name": "3145tttt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "67be94e466f702bfed94ac7d",
            "name": "Bayes-Group",
            "fullname": "Bayesian Methods Research Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67be93b43e718f259b6abbe3/6Swfg5YVWup39avDzjk98.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.18849",
            "authors": [
                {
                    "_id": "68f83e8d7669bcaeecce0ce8",
                    "name": "Chenghao Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f83e8d7669bcaeecce0ce9",
                    "name": "Meiling Tao",
                    "hidden": false
                },
                {
                    "_id": "68f83e8d7669bcaeecce0cea",
                    "name": "Tiannan Wang",
                    "hidden": false
                },
                {
                    "_id": "68f83e8d7669bcaeecce0ceb",
                    "name": "Dongyi Ding",
                    "hidden": false
                },
                {
                    "_id": "68f83e8d7669bcaeecce0cec",
                    "name": "Yuchen Eleanor Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f83e8d7669bcaeecce0ced",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T17:40:03.000Z",
            "submittedOnDailyAt": "2025-10-22T01:23:39.437Z",
            "title": "Towards Faithful and Controllable Personalization via Critique-Post-Edit\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "632bfaebea6e62428ab0e9c2",
                "avatarUrl": "/avatars/344aaf371bbba9aea091b12741c451e5.svg",
                "isPro": false,
                "fullname": "Tiannan Wang",
                "user": "WTNswaggy",
                "type": "user"
            },
            "summary": "Faithfully personalizing large language models (LLMs) to align with\nindividual user preferences is a critical but challenging task. While\nsupervised fine-tuning (SFT) quickly reaches a performance plateau, standard\nreinforcement learning from human feedback (RLHF) also struggles with the\nnuances of personalization. Scalar-based reward models are prone to reward\nhacking which leads to verbose and superficially personalized responses. To\naddress these limitations, we propose Critique-Post-Edit, a robust\nreinforcement learning framework that enables more faithful and controllable\npersonalization. Our framework integrates two key components: (1) a\nPersonalized Generative Reward Model (GRM) that provides multi-dimensional\nscores and textual critiques to resist reward hacking, and (2) a\nCritique-Post-Edit mechanism where the policy model revises its own outputs\nbased on these critiques for more targeted and efficient learning. Under a\nrigorous length-controlled evaluation, our method substantially outperforms\nstandard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an\naverage 11\\% win-rate improvement, and personalized Qwen2.5-14B model surpasses\nthe performance of GPT-4.1. These results demonstrate a practical path to\nfaithful, efficient, and controllable personalization.",
            "upvotes": 19,
            "discussionId": "68f83e8d7669bcaeecce0cee",
            "ai_summary": "A Critique-Post-Edit framework enhances personalization of large language models by integrating a multi-dimensional reward model and a self-revision mechanism, outperforming standard methods.",
            "ai_keywords": [
                "supervised fine-tuning",
                "reinforcement learning from human feedback",
                "reward hacking",
                "Personalized Generative Reward Model",
                "Critique-Post-Edit",
                "policy model",
                "length-controlled evaluation",
                "PPO",
                "personalization benchmarks",
                "Qwen2.5-7B",
                "Qwen2.5-14B",
                "GPT-4.1"
            ]
        },
        "publishedAt": "2025-10-21T13:40:03.000Z",
        "title": "Towards Faithful and Controllable Personalization via Critique-Post-Edit\n  Reinforcement Learning",
        "summary": "Faithfully personalizing large language models (LLMs) to align with\nindividual user preferences is a critical but challenging task. While\nsupervised fine-tuning (SFT) quickly reaches a performance plateau, standard\nreinforcement learning from human feedback (RLHF) also struggles with the\nnuances of personalization. Scalar-based reward models are prone to reward\nhacking which leads to verbose and superficially personalized responses. To\naddress these limitations, we propose Critique-Post-Edit, a robust\nreinforcement learning framework that enables more faithful and controllable\npersonalization. Our framework integrates two key components: (1) a\nPersonalized Generative Reward Model (GRM) that provides multi-dimensional\nscores and textual critiques to resist reward hacking, and (2) a\nCritique-Post-Edit mechanism where the policy model revises its own outputs\nbased on these critiques for more targeted and efficient learning. Under a\nrigorous length-controlled evaluation, our method substantially outperforms\nstandard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an\naverage 11\\% win-rate improvement, and personalized Qwen2.5-14B model surpasses\nthe performance of GPT-4.1. These results demonstrate a practical path to\nfaithful, efficient, and controllable personalization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18849.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "632bfaebea6e62428ab0e9c2",
            "avatarUrl": "/avatars/344aaf371bbba9aea091b12741c451e5.svg",
            "fullname": "Tiannan Wang",
            "name": "WTNswaggy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18019",
            "authors": [
                {
                    "_id": "68f89a6a7669bcaeecce0ef3",
                    "user": {
                        "_id": "66cfc97cbee2b9a93bcc7306",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66cfc97cbee2b9a93bcc7306/kka3lTAsjVUpHkExBgok4.jpeg",
                        "isPro": false,
                        "fullname": "Asim Mohamed",
                        "user": "asimz",
                        "type": "user"
                    },
                    "name": "Asim Mohamed",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:42:36.824Z",
                    "hidden": false
                },
                {
                    "_id": "68f89a6a7669bcaeecce0ef4",
                    "user": {
                        "_id": "63ee35c3f599efc7a010c792",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676555600618-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Martin Gubri",
                        "user": "mgubri",
                        "type": "user"
                    },
                    "name": "Martin Gubri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:42:39.466Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/s38579H1dbEL8hPEIW-vO.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/KlFdSEujZjmIz3j4_Ukc8.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/8PVTKnJ8ASPRbY6EpfmVE.png"
            ],
            "publishedAt": "2025-10-20T18:51:20.000Z",
            "submittedOnDailyAt": "2025-10-22T10:32:09.043Z",
            "title": "Is Multilingual LLM Watermarking Truly Multilingual? A Simple\n  Back-Translation Solution",
            "submittedOnDailyBy": {
                "_id": "63ee35c3f599efc7a010c792",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676555600618-noauth.jpeg",
                "isPro": false,
                "fullname": "Martin Gubri",
                "user": "mgubri",
                "type": "user"
            },
            "summary": "Multilingual watermarking aims to make large language model (LLM) outputs\ntraceable across languages, yet current methods still fall short. Despite\nclaims of cross-lingual robustness, they are evaluated only on high-resource\nlanguages. We show that existing multilingual watermarking methods are not\ntruly multilingual: they fail to remain robust under translation attacks in\nmedium- and low-resource languages. We trace this failure to semantic\nclustering, which fails when the tokenizer vocabulary contains too few\nfull-word tokens for a given language. To address this, we introduce STEAM, a\nback-translation-based detection method that restores watermark strength lost\nthrough translation. STEAM is compatible with any watermarking method, robust\nacross different tokenizers and languages, non-invasive, and easily extendable\nto new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17\nlanguages, STEAM provides a simple and robust path toward fairer watermarking\nacross diverse languages.",
            "upvotes": 15,
            "discussionId": "68f89a6b7669bcaeecce0ef5",
            "githubRepo": "https://github.com/asimzz/steam",
            "ai_summary": "STEAM, a back-translation-based detection method, enhances multilingual watermarking robustness across various languages by addressing semantic clustering failures.",
            "ai_keywords": [
                "multilingual watermarking",
                "large language model",
                "translation attacks",
                "medium-resource languages",
                "low-resource languages",
                "semantic clustering",
                "tokenizer vocabulary",
                "full-word tokens",
                "back-translation",
                "AUC",
                "TPR@1%"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "673bcad43a2fd2a3b41f64e3",
                "name": "parameterlab",
                "fullname": "Parameter Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/w3hbIZBXXMq09s0BAzwC0.png"
            }
        },
        "publishedAt": "2025-10-20T14:51:20.000Z",
        "title": "Is Multilingual LLM Watermarking Truly Multilingual? A Simple\n  Back-Translation Solution",
        "summary": "Multilingual watermarking aims to make large language model (LLM) outputs\ntraceable across languages, yet current methods still fall short. Despite\nclaims of cross-lingual robustness, they are evaluated only on high-resource\nlanguages. We show that existing multilingual watermarking methods are not\ntruly multilingual: they fail to remain robust under translation attacks in\nmedium- and low-resource languages. We trace this failure to semantic\nclustering, which fails when the tokenizer vocabulary contains too few\nfull-word tokens for a given language. To address this, we introduce STEAM, a\nback-translation-based detection method that restores watermark strength lost\nthrough translation. STEAM is compatible with any watermarking method, robust\nacross different tokenizers and languages, non-invasive, and easily extendable\nto new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17\nlanguages, STEAM provides a simple and robust path toward fairer watermarking\nacross diverse languages.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/s38579H1dbEL8hPEIW-vO.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/KlFdSEujZjmIz3j4_Ukc8.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/8PVTKnJ8ASPRbY6EpfmVE.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18019.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63ee35c3f599efc7a010c792",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676555600618-noauth.jpeg",
            "fullname": "Martin Gubri",
            "name": "mgubri",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "673bcad43a2fd2a3b41f64e3",
            "name": "parameterlab",
            "fullname": "Parameter Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/w3hbIZBXXMq09s0BAzwC0.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.17722",
            "authors": [
                {
                    "_id": "68f6f0d624c4489363111882",
                    "name": "Yaning Pan",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d624c4489363111883",
                    "name": "Zekun Wang",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d624c4489363111884",
                    "name": "Qianqian Xie",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d624c4489363111885",
                    "name": "Yongqian Wen",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d624c4489363111886",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d624c4489363111887",
                    "name": "Guohui Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d624c4489363111888",
                    "user": {
                        "_id": "68f8792d603c18f07bf357b2",
                        "avatarUrl": "/avatars/7622804ccc0c4afc90a2956661eb84af.svg",
                        "isPro": false,
                        "fullname": "Haoxuan Hu",
                        "user": "HarrisonHu",
                        "type": "user"
                    },
                    "name": "Haoxuan Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:46:03.474Z",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d624c4489363111889",
                    "name": "Zhiyu Pan",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d624c448936311188a",
                    "name": "Yibing Huang",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d624c448936311188b",
                    "name": "Zhidong Gan",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d624c448936311188c",
                    "name": "Yonghong Lin",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d624c448936311188d",
                    "name": "An Ping",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d624c448936311188e",
                    "name": "Tianhao Peng",
                    "hidden": false
                },
                {
                    "_id": "68f6f0d624c448936311188f",
                    "name": "Jiaheng Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T16:38:40.000Z",
            "submittedOnDailyAt": "2025-10-22T00:24:54.221Z",
            "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating\n  Multimodal LLMs in Multi-Turn Dialogues",
            "submittedOnDailyBy": {
                "_id": "65377c30e48353201e6fdda0",
                "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
                "isPro": false,
                "fullname": "Jiaheng Liu",
                "user": "CheeryLJH",
                "type": "user"
            },
            "summary": "The recent development of Multimodal Large Language Models (MLLMs) has\nsignificantly advanced AI's ability to understand visual modalities. However,\nexisting evaluation benchmarks remain limited to single-turn question\nanswering, overlooking the complexity of multi-turn dialogues in real-world\nscenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video\nunderstanding benchmark for evaluating MLLMs in multi-turn dialogues.\nSpecifically, our MT-Video-Bench mainly assesses six core competencies that\nfocus on perceptivity and interactivity, encompassing 987 meticulously curated\nmulti-turn dialogues from diverse domains. These capabilities are rigorously\naligned with real-world applications, such as interactive sports analysis and\nmulti-turn video-based intelligent tutoring. With MT-Video-Bench, we\nextensively evaluate various state-of-the-art open-source and closed-source\nMLLMs, revealing their significant performance discrepancies and limitations in\nhandling multi-turn video dialogues. The benchmark will be publicly available\nto foster future research.",
            "upvotes": 14,
            "discussionId": "68f6f0d624c4489363111890",
            "projectPage": "https://mt-video-bench.github.io/",
            "githubRepo": "https://github.com/NJU-LINK/MT-Video-Bench",
            "ai_summary": "MT-Video-Bench evaluates MLLMs in multi-turn video dialogues, assessing perceptivity and interactivity across diverse domains.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "MLLMs",
                "video understanding benchmark",
                "multi-turn dialogues",
                "perceptivity",
                "interactivity",
                "interactive sports analysis",
                "multi-turn video-based intelligent tutoring"
            ],
            "githubStars": 5,
            "organization": {
                "_id": "68edc767abe005ac1b354573",
                "name": "NJU-LINK",
                "fullname": "NJU-LINK Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"
            }
        },
        "publishedAt": "2025-10-20T12:38:40.000Z",
        "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating\n  Multimodal LLMs in Multi-Turn Dialogues",
        "summary": "The recent development of Multimodal Large Language Models (MLLMs) has\nsignificantly advanced AI's ability to understand visual modalities. However,\nexisting evaluation benchmarks remain limited to single-turn question\nanswering, overlooking the complexity of multi-turn dialogues in real-world\nscenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video\nunderstanding benchmark for evaluating MLLMs in multi-turn dialogues.\nSpecifically, our MT-Video-Bench mainly assesses six core competencies that\nfocus on perceptivity and interactivity, encompassing 987 meticulously curated\nmulti-turn dialogues from diverse domains. These capabilities are rigorously\naligned with real-world applications, such as interactive sports analysis and\nmulti-turn video-based intelligent tutoring. With MT-Video-Bench, we\nextensively evaluate various state-of-the-art open-source and closed-source\nMLLMs, revealing their significant performance discrepancies and limitations in\nhandling multi-turn video dialogues. The benchmark will be publicly available\nto foster future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17722.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65377c30e48353201e6fdda0",
            "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
            "fullname": "Jiaheng Liu",
            "name": "CheeryLJH",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 21
        },
        "organization": {
            "_id": "68edc767abe005ac1b354573",
            "name": "NJU-LINK",
            "fullname": "NJU-LINK Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18234",
            "authors": [
                {
                    "_id": "68f848877669bcaeecce0d44",
                    "name": "Haoran Wei",
                    "hidden": false
                },
                {
                    "_id": "68f848877669bcaeecce0d45",
                    "name": "Yaofeng Sun",
                    "hidden": false
                },
                {
                    "_id": "68f848877669bcaeecce0d46",
                    "name": "Yukun Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T02:41:44.000Z",
            "submittedOnDailyAt": "2025-10-22T11:35:18.794Z",
            "title": "DeepSeek-OCR: Contexts Optical Compression",
            "submittedOnDailyBy": {
                "_id": "651e96991b97c9f33d26bde6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg",
                "isPro": true,
                "fullname": "Elie Bakouch",
                "user": "eliebak",
                "type": "user"
            },
            "summary": "We present DeepSeek-OCR as an initial investigation into the feasibility of\ncompressing long contexts via optical 2D mapping. DeepSeek-OCR consists of two\ncomponents: DeepEncoder and DeepSeek3B-MoE-A570M as the decoder. Specifically,\nDeepEncoder serves as the core engine, designed to maintain low activations\nunder high-resolution input while achieving high compression ratios to ensure\nan optimal and manageable number of vision tokens. Experiments show that when\nthe number of text tokens is within 10 times that of vision tokens (i.e., a\ncompression ratio < 10x), the model can achieve decoding (OCR) precision of\n97%. Even at a compression ratio of 20x, the OCR accuracy still remains at\nabout 60%. This shows considerable promise for research areas such as\nhistorical long-context compression and memory forgetting mechanisms in LLMs.\nBeyond this, DeepSeek-OCR also demonstrates high practical value. On\nOmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision\ntokens, and outperforms MinerU2.0 (6000+ tokens per page on average) while\nutilizing fewer than 800 vision tokens. In production, DeepSeek-OCR can\ngenerate training data for LLMs/VLMs at a scale of 200k+ pages per day (a\nsingle A100-40G). Codes and model weights are publicly accessible at\nhttp://github.com/deepseek-ai/DeepSeek-OCR.",
            "upvotes": 13,
            "discussionId": "68f848877669bcaeecce0d47",
            "ai_summary": "DeepSeek-OCR uses optical 2D mapping to compress long contexts, achieving high OCR precision with reduced vision tokens and demonstrating practical value in document processing.",
            "ai_keywords": [
                "DeepSeek-OCR",
                "DeepEncoder",
                "DeepSeek3B-MoE-A570M",
                "optical 2D mapping",
                "vision tokens",
                "OCR precision",
                "compression ratio",
                "OmniDocBench",
                "GOT-OCR2.0",
                "MinerU2.0",
                "LLMs",
                "VLMs"
            ],
            "organization": {
                "_id": "652faff917096ceb6bf53f3f",
                "name": "deepseek-ai",
                "fullname": "DeepSeek",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"
            }
        },
        "publishedAt": "2025-10-20T22:41:44.000Z",
        "title": "DeepSeek-OCR: Contexts Optical Compression",
        "summary": "We present DeepSeek-OCR as an initial investigation into the feasibility of\ncompressing long contexts via optical 2D mapping. DeepSeek-OCR consists of two\ncomponents: DeepEncoder and DeepSeek3B-MoE-A570M as the decoder. Specifically,\nDeepEncoder serves as the core engine, designed to maintain low activations\nunder high-resolution input while achieving high compression ratios to ensure\nan optimal and manageable number of vision tokens. Experiments show that when\nthe number of text tokens is within 10 times that of vision tokens (i.e., a\ncompression ratio < 10x), the model can achieve decoding (OCR) precision of\n97%. Even at a compression ratio of 20x, the OCR accuracy still remains at\nabout 60%. This shows considerable promise for research areas such as\nhistorical long-context compression and memory forgetting mechanisms in LLMs.\nBeyond this, DeepSeek-OCR also demonstrates high practical value. On\nOmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision\ntokens, and outperforms MinerU2.0 (6000+ tokens per page on average) while\nutilizing fewer than 800 vision tokens. In production, DeepSeek-OCR can\ngenerate training data for LLMs/VLMs at a scale of 200k+ pages per day (a\nsingle A100-40G). Codes and model weights are publicly accessible at\nhttp://github.com/deepseek-ai/DeepSeek-OCR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18234.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "651e96991b97c9f33d26bde6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg",
            "fullname": "Elie Bakouch",
            "name": "eliebak",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 310
        },
        "organization": {
            "_id": "652faff917096ceb6bf53f3f",
            "name": "deepseek-ai",
            "fullname": "DeepSeek",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18775",
            "authors": [
                {
                    "_id": "68f838e87669bcaeecce0c79",
                    "name": "Teng Hu",
                    "hidden": false
                },
                {
                    "_id": "68f838e87669bcaeecce0c7a",
                    "name": "Jiangning Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f838e87669bcaeecce0c7b",
                    "name": "Zihan Su",
                    "hidden": false
                },
                {
                    "_id": "68f838e87669bcaeecce0c7c",
                    "name": "Ran Yi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T16:23:21.000Z",
            "submittedOnDailyAt": "2025-10-22T00:24:14.336Z",
            "title": "UltraGen: High-Resolution Video Generation with Hierarchical Attention",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent advances in video generation have made it possible to produce visually\ncompelling videos, with wide-ranging applications in content creation,\nentertainment, and virtual reality. However, most existing diffusion\ntransformer based video generation models are limited to low-resolution outputs\n(<=720P) due to the quadratic computational complexity of the attention\nmechanism with respect to the output width and height. This computational\nbottleneck makes native high-resolution video generation (1080P/2K/4K)\nimpractical for both training and inference. To address this challenge, we\npresent UltraGen, a novel video generation framework that enables i) efficient\nand ii) end-to-end native high-resolution video synthesis. Specifically,\nUltraGen features a hierarchical dual-branch attention architecture based on\nglobal-local attention decomposition, which decouples full attention into a\nlocal attention branch for high-fidelity regional content and a global\nattention branch for overall semantic consistency. We further propose a\nspatially compressed global modeling strategy to efficiently learn global\ndependencies, and a hierarchical cross-window local attention mechanism to\nreduce computational costs while enhancing information flow across different\nlocal windows. Extensive experiments demonstrate that UltraGen can effectively\nscale pre-trained low-resolution video models to 1080P and even 4K resolution\nfor the first time, outperforming existing state-of-the-art methods and\nsuper-resolution based two-stage pipelines in both qualitative and quantitative\nevaluations.",
            "upvotes": 11,
            "discussionId": "68f838e87669bcaeecce0c7d",
            "projectPage": "https://sjtuplayer.github.io/projects/UltraGen/",
            "ai_summary": "UltraGen, a novel video generation framework, enables efficient high-resolution video synthesis using a hierarchical dual-branch attention architecture and spatially compressed global modeling.",
            "ai_keywords": [
                "diffusion transformer",
                "video generation",
                "low-resolution",
                "high-resolution",
                "attention mechanism",
                "computational complexity",
                "global-local attention decomposition",
                "local attention",
                "global attention",
                "spatially compressed global modeling",
                "hierarchical cross-window local attention",
                "pre-trained models",
                "super-resolution",
                "two-stage pipelines"
            ]
        },
        "publishedAt": "2025-10-21T12:23:21.000Z",
        "title": "UltraGen: High-Resolution Video Generation with Hierarchical Attention",
        "summary": "Recent advances in video generation have made it possible to produce visually\ncompelling videos, with wide-ranging applications in content creation,\nentertainment, and virtual reality. However, most existing diffusion\ntransformer based video generation models are limited to low-resolution outputs\n(<=720P) due to the quadratic computational complexity of the attention\nmechanism with respect to the output width and height. This computational\nbottleneck makes native high-resolution video generation (1080P/2K/4K)\nimpractical for both training and inference. To address this challenge, we\npresent UltraGen, a novel video generation framework that enables i) efficient\nand ii) end-to-end native high-resolution video synthesis. Specifically,\nUltraGen features a hierarchical dual-branch attention architecture based on\nglobal-local attention decomposition, which decouples full attention into a\nlocal attention branch for high-fidelity regional content and a global\nattention branch for overall semantic consistency. We further propose a\nspatially compressed global modeling strategy to efficiently learn global\ndependencies, and a hierarchical cross-window local attention mechanism to\nreduce computational costs while enhancing information flow across different\nlocal windows. Extensive experiments demonstrate that UltraGen can effectively\nscale pre-trained low-resolution video models to 1080P and even 4K resolution\nfor the first time, outperforming existing state-of-the-art methods and\nsuper-resolution based two-stage pipelines in both qualitative and quantitative\nevaluations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18775.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 138
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18250",
            "authors": [
                {
                    "_id": "68f854bc7669bcaeecce0d8b",
                    "user": {
                        "_id": "64452110e1fd8d65b2790bfc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64452110e1fd8d65b2790bfc/3NIsQFV9X44X88aLXR6eE.jpeg",
                        "isPro": false,
                        "fullname": "Xiaohan Qin (SJTU) & (SII)",
                        "user": "galaxy-1",
                        "type": "user"
                    },
                    "name": "Xiaohan Qin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:43:27.238Z",
                    "hidden": false
                },
                {
                    "_id": "68f854bc7669bcaeecce0d8c",
                    "name": "Xiaoxing Wang",
                    "hidden": false
                },
                {
                    "_id": "68f854bc7669bcaeecce0d8d",
                    "name": "Ning Liao",
                    "hidden": false
                },
                {
                    "_id": "68f854bc7669bcaeecce0d8e",
                    "name": "Cancheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f854bc7669bcaeecce0d8f",
                    "name": "Xiangdong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f854bc7669bcaeecce0d90",
                    "name": "Mingquan Feng",
                    "hidden": false
                },
                {
                    "_id": "68f854bc7669bcaeecce0d91",
                    "name": "Jingzhi Wang",
                    "hidden": false
                },
                {
                    "_id": "68f854bc7669bcaeecce0d92",
                    "name": "Junchi Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T03:21:04.000Z",
            "submittedOnDailyAt": "2025-10-22T02:32:49.727Z",
            "title": "ssToken: Self-modulated and Semantic-aware Token Selection for LLM\n  Fine-tuning",
            "submittedOnDailyBy": {
                "_id": "656d8d4b1f8d9b618de91369",
                "avatarUrl": "/avatars/884dba9e56936241034b179d11a513b9.svg",
                "isPro": false,
                "fullname": "Xiangdong Zhang",
                "user": "aHapBean",
                "type": "user"
            },
            "summary": "Data quality plays a critical role in enhancing supervised fine-tuning (SFT)\nfor large language models (LLMs), and token-level data selection has emerged as\na promising direction for its fine-grained nature. Despite their strong\nempirical performance, existing token-level selection methods share two key\nlimitations: (1) requiring training or accessing an additional reference model,\nand (2) relying solely on loss information for token selection, which cannot\nwell preserve semantically important tokens that are not favored by loss-based\nmetrics. To address these challenges, we propose ssToken, a Self-modulated and\nSemantic-aware Token Selection approach. ssToken leverages readily accessible\nhistory models to compute the per-token loss difference with the current model,\nwhich serves as a self-modulated signal that enables the model to adaptively\nselect tokens along its optimization trajectory, rather than relying on excess\nloss from an offline-trained reference model as in prior works. We further\nintroduce a semantic-aware, attention-based token importance estimation metric,\northogonal to loss-based selection and providing complementary semantic\ninformation for more effective filtering. Extensive experiments across\ndifferent model families and scales demonstrate that both self-modulated\nselection and semantic-aware selection alone outperform full-data fine-tuning,\nwhile their integration--ssToken--achieves synergistic gains and further\nsurpasses prior token-level selection methods, delivering performance\nimprovements while maintaining training efficiency.",
            "upvotes": 11,
            "discussionId": "68f854bc7669bcaeecce0d93",
            "ai_summary": "ssToken, a self-modulated and semantic-aware token selection approach, enhances supervised fine-tuning of large language models by adaptively selecting tokens and providing complementary semantic information, outperforming existing methods.",
            "ai_keywords": [
                "supervised fine-tuning",
                "large language models",
                "token-level selection",
                "history models",
                "per-token loss difference",
                "self-modulated signal",
                "semantic-aware",
                "attention-based",
                "token importance estimation",
                "full-data fine-tuning",
                "synergistic gains"
            ]
        },
        "publishedAt": "2025-10-20T23:21:04.000Z",
        "title": "ssToken: Self-modulated and Semantic-aware Token Selection for LLM\n  Fine-tuning",
        "summary": "Data quality plays a critical role in enhancing supervised fine-tuning (SFT)\nfor large language models (LLMs), and token-level data selection has emerged as\na promising direction for its fine-grained nature. Despite their strong\nempirical performance, existing token-level selection methods share two key\nlimitations: (1) requiring training or accessing an additional reference model,\nand (2) relying solely on loss information for token selection, which cannot\nwell preserve semantically important tokens that are not favored by loss-based\nmetrics. To address these challenges, we propose ssToken, a Self-modulated and\nSemantic-aware Token Selection approach. ssToken leverages readily accessible\nhistory models to compute the per-token loss difference with the current model,\nwhich serves as a self-modulated signal that enables the model to adaptively\nselect tokens along its optimization trajectory, rather than relying on excess\nloss from an offline-trained reference model as in prior works. We further\nintroduce a semantic-aware, attention-based token importance estimation metric,\northogonal to loss-based selection and providing complementary semantic\ninformation for more effective filtering. Extensive experiments across\ndifferent model families and scales demonstrate that both self-modulated\nselection and semantic-aware selection alone outperform full-data fine-tuning,\nwhile their integration--ssToken--achieves synergistic gains and further\nsurpasses prior token-level selection methods, delivering performance\nimprovements while maintaining training efficiency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18250.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656d8d4b1f8d9b618de91369",
            "avatarUrl": "/avatars/884dba9e56936241034b179d11a513b9.svg",
            "fullname": "Xiangdong Zhang",
            "name": "aHapBean",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.17519",
            "authors": [
                {
                    "_id": "68f7743ea7b9b96b2c13ed5b",
                    "name": "Yongshun Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f7743ea7b9b96b2c13ed5c",
                    "name": "Zhongyi Fan",
                    "hidden": false
                },
                {
                    "_id": "68f7743ea7b9b96b2c13ed5d",
                    "name": "Yonghang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f7743ea7b9b96b2c13ed5e",
                    "name": "Zhangzikang Li",
                    "hidden": false
                },
                {
                    "_id": "68f7743ea7b9b96b2c13ed5f",
                    "name": "Weifeng Chen",
                    "hidden": false
                },
                {
                    "_id": "68f7743ea7b9b96b2c13ed60",
                    "name": "Zhongwei Feng",
                    "hidden": false
                },
                {
                    "_id": "68f7743ea7b9b96b2c13ed61",
                    "name": "Chaoyue Wang",
                    "hidden": false
                },
                {
                    "_id": "68f7743ea7b9b96b2c13ed62",
                    "name": "Peng Hou",
                    "hidden": false
                },
                {
                    "_id": "68f7743ea7b9b96b2c13ed63",
                    "name": "Anxiang Zeng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T13:20:37.000Z",
            "submittedOnDailyAt": "2025-10-22T01:01:39.210Z",
            "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation\n  Models",
            "submittedOnDailyBy": {
                "_id": "6275e270a227a8b3a707fd27",
                "avatarUrl": "/avatars/89fd0fc9ca0289745ecc28ff98c96ed2.svg",
                "isPro": false,
                "fullname": "Daniel Wang",
                "user": "Non-no",
                "type": "user"
            },
            "summary": "In recent years, large-scale generative models for visual content\n(e.g., images, videos, and 3D objects/scenes) have made remarkable\nprogress. However, training large-scale video generation models remains\nparticularly challenging and resource-intensive due to cross-modal text-video\nalignment, the long sequences involved, and the complex spatiotemporal\ndependencies. To address these challenges, we present a training framework that\noptimizes four pillars: (i) data processing, (ii) model architecture, (iii)\ntraining strategy, and (iv) infrastructure for large-scale video generation\nmodels. These optimizations delivered significant efficiency gains and\nperformance improvements across all stages of data preprocessing, video\ncompression, parameter scaling, curriculum-based pretraining, and\nalignment-focused post-training. Our resulting model, MUG-V 10B, matches recent\nstate-of-the-art video generators overall and, on e-commerce-oriented video\ngeneration tasks, surpasses leading open-source baselines in human evaluations.\nMore importantly, we open-source the complete stack, including model weights,\nMegatron-Core-based large-scale training code, and inference pipelines for\nvideo generation and enhancement. To our knowledge, this is the first public\nrelease of large-scale video generation training code that exploits\nMegatron-Core to achieve high training efficiency and near-linear multi-node\nscaling, details are available in\nhttps://github.com/Shopee-MUG/MUG-V{our webpage}.",
            "upvotes": 9,
            "discussionId": "68f7743ea7b9b96b2c13ed64",
            "projectPage": "https://github.com/Shopee-MUG/MUG-V",
            "githubRepo": "https://github.com/Shopee-MUG/MUG-V",
            "ai_summary": "A training framework for large-scale video generation models optimizes data processing, model architecture, training strategy, and infrastructure, resulting in a model that matches state-of-the-art performance and is open-sourced with Megatron-Core-based training code.",
            "ai_keywords": [
                "generative models",
                "video generation",
                "cross-modal text-video alignment",
                "spatiotemporal dependencies",
                "data processing",
                "model architecture",
                "training strategy",
                "infrastructure",
                "video compression",
                "parameter scaling",
                "curriculum-based pretraining",
                "alignment-focused post-training",
                "Megatron-Core",
                "multi-node scaling"
            ],
            "githubStars": 59,
            "organization": {
                "_id": "68f20a7975442410170e2549",
                "name": "MUG-V",
                "fullname": "shopee-llm-mug team",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68ecb48de4379b53512e4e5b/cGLOTDuzudRVS1wRzcDo7.png"
            }
        },
        "publishedAt": "2025-10-20T09:20:37.000Z",
        "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation\n  Models",
        "summary": "In recent years, large-scale generative models for visual content\n(e.g., images, videos, and 3D objects/scenes) have made remarkable\nprogress. However, training large-scale video generation models remains\nparticularly challenging and resource-intensive due to cross-modal text-video\nalignment, the long sequences involved, and the complex spatiotemporal\ndependencies. To address these challenges, we present a training framework that\noptimizes four pillars: (i) data processing, (ii) model architecture, (iii)\ntraining strategy, and (iv) infrastructure for large-scale video generation\nmodels. These optimizations delivered significant efficiency gains and\nperformance improvements across all stages of data preprocessing, video\ncompression, parameter scaling, curriculum-based pretraining, and\nalignment-focused post-training. Our resulting model, MUG-V 10B, matches recent\nstate-of-the-art video generators overall and, on e-commerce-oriented video\ngeneration tasks, surpasses leading open-source baselines in human evaluations.\nMore importantly, we open-source the complete stack, including model weights,\nMegatron-Core-based large-scale training code, and inference pipelines for\nvideo generation and enhancement. To our knowledge, this is the first public\nrelease of large-scale video generation training code that exploits\nMegatron-Core to achieve high training efficiency and near-linear multi-node\nscaling, details are available in\nhttps://github.com/Shopee-MUG/MUG-V{our webpage}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17519.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6275e270a227a8b3a707fd27",
            "avatarUrl": "/avatars/89fd0fc9ca0289745ecc28ff98c96ed2.svg",
            "fullname": "Daniel Wang",
            "name": "Non-no",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "68f20a7975442410170e2549",
            "name": "MUG-V",
            "fullname": "shopee-llm-mug team",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68ecb48de4379b53512e4e5b/cGLOTDuzudRVS1wRzcDo7.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18795",
            "authors": [
                {
                    "_id": "68f840da7669bcaeecce0cf0",
                    "name": "Xiaoxing Hu",
                    "hidden": false
                },
                {
                    "_id": "68f840da7669bcaeecce0cf1",
                    "name": "Kaicheng Yang",
                    "hidden": false
                },
                {
                    "_id": "68f840da7669bcaeecce0cf2",
                    "name": "Ziyong Feng",
                    "hidden": false
                },
                {
                    "_id": "68f840da7669bcaeecce0cf3",
                    "name": "Qi Ming",
                    "hidden": false
                },
                {
                    "_id": "68f840da7669bcaeecce0cf4",
                    "name": "Zonghao Guo",
                    "hidden": false
                },
                {
                    "_id": "68f840da7669bcaeecce0cf5",
                    "name": "Xiang An",
                    "hidden": false
                },
                {
                    "_id": "68f840da7669bcaeecce0cf6",
                    "name": "Ziyong Feng",
                    "hidden": false
                },
                {
                    "_id": "68f840da7669bcaeecce0cf7",
                    "name": "Junchi Yan",
                    "hidden": false
                },
                {
                    "_id": "68f840da7669bcaeecce0cf8",
                    "name": "Xue Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T16:48:49.000Z",
            "submittedOnDailyAt": "2025-10-22T00:58:23.012Z",
            "title": "ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder",
            "submittedOnDailyBy": {
                "_id": "63e202f352b7578dba448ab5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
                "isPro": false,
                "fullname": "Yang",
                "user": "Kaichengalex",
                "type": "user"
            },
            "summary": "The original CLIP text encoder is limited by a maximum input length of 77\ntokens, which hampers its ability to effectively process long texts and perform\nfine-grained semantic understanding. In addition, the CLIP text encoder lacks\nsupport for multilingual inputs. All these limitations significantly restrict\nits applicability across a broader range of tasks. Recent studies have\nattempted to replace the CLIP text encoder with an LLM-based embedder to\nenhance its ability in processing long texts, multilingual understanding, and\nfine-grained semantic comprehension. However, because the representation spaces\nof LLMs and the vision-language space of CLIP are pretrained independently\nwithout alignment priors, direct alignment using contrastive learning can\ndisrupt the intrinsic vision-language alignment in the CLIP image encoder,\nleading to an underutilization of the knowledge acquired during pre-training.\nTo address this challenge, we propose ProCLIP, a curriculum learning-based\nprogressive vision-language alignment framework to effectively align the CLIP\nimage encoder with an LLM-based embedder. Specifically, ProCLIP first distills\nknowledge from CLIP's text encoder into the LLM-based embedder to leverage\nCLIP's rich pretrained knowledge while establishing initial alignment between\nthe LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns\nthe CLIP image encoder with the LLM-based embedder through image-text\ncontrastive tuning, employing self-distillation regularization to avoid\noverfitting. To achieve a more effective alignment, instance semantic alignment\nloss and embedding structure alignment loss are employed during representation\ninheritance and contrastive tuning. The Code is available at\nhttps://github.com/VisionXLab/ProCLIP",
            "upvotes": 8,
            "discussionId": "68f840da7669bcaeecce0cf9",
            "githubRepo": "https://github.com/VisionXLab/ProCLIP",
            "ai_summary": "ProCLIP enhances CLIP's text processing capabilities by aligning its image encoder with an LLM-based embedder through curriculum learning and contrastive tuning, preserving CLIP's pretrained knowledge.",
            "ai_keywords": [
                "CLIP text encoder",
                "LLM-based embedder",
                "curriculum learning",
                "progressive vision-language alignment",
                "knowledge distillation",
                "image-text contrastive tuning",
                "self-distillation regularization",
                "instance semantic alignment loss",
                "embedding structure alignment loss"
            ],
            "githubStars": 11
        },
        "publishedAt": "2025-10-21T12:48:49.000Z",
        "title": "ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder",
        "summary": "The original CLIP text encoder is limited by a maximum input length of 77\ntokens, which hampers its ability to effectively process long texts and perform\nfine-grained semantic understanding. In addition, the CLIP text encoder lacks\nsupport for multilingual inputs. All these limitations significantly restrict\nits applicability across a broader range of tasks. Recent studies have\nattempted to replace the CLIP text encoder with an LLM-based embedder to\nenhance its ability in processing long texts, multilingual understanding, and\nfine-grained semantic comprehension. However, because the representation spaces\nof LLMs and the vision-language space of CLIP are pretrained independently\nwithout alignment priors, direct alignment using contrastive learning can\ndisrupt the intrinsic vision-language alignment in the CLIP image encoder,\nleading to an underutilization of the knowledge acquired during pre-training.\nTo address this challenge, we propose ProCLIP, a curriculum learning-based\nprogressive vision-language alignment framework to effectively align the CLIP\nimage encoder with an LLM-based embedder. Specifically, ProCLIP first distills\nknowledge from CLIP's text encoder into the LLM-based embedder to leverage\nCLIP's rich pretrained knowledge while establishing initial alignment between\nthe LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns\nthe CLIP image encoder with the LLM-based embedder through image-text\ncontrastive tuning, employing self-distillation regularization to avoid\noverfitting. To achieve a more effective alignment, instance semantic alignment\nloss and embedding structure alignment loss are employed during representation\ninheritance and contrastive tuning. The Code is available at\nhttps://github.com/VisionXLab/ProCLIP",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18795.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63e202f352b7578dba448ab5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
            "fullname": "Yang",
            "name": "Kaichengalex",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18632",
            "authors": [
                {
                    "_id": "68f8d62f6334fefb2d6023df",
                    "user": {
                        "_id": "6458a99c3b81018d6b93aecb",
                        "avatarUrl": "/avatars/d90f16e8d3e49e095fea4cbd899837df.svg",
                        "isPro": false,
                        "fullname": "CHEN",
                        "user": "jankin123",
                        "type": "user"
                    },
                    "name": "Zhangquan Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:40:51.777Z",
                    "hidden": false
                },
                {
                    "_id": "68f8d62f6334fefb2d6023e0",
                    "name": "Manyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f8d62f6334fefb2d6023e1",
                    "name": "Xinlei Yu",
                    "hidden": false
                },
                {
                    "_id": "68f8d62f6334fefb2d6023e2",
                    "name": "Xufang Luo",
                    "hidden": false
                },
                {
                    "_id": "68f8d62f6334fefb2d6023e3",
                    "name": "Mingze Sun",
                    "hidden": false
                },
                {
                    "_id": "68f8d62f6334fefb2d6023e4",
                    "user": {
                        "_id": "65ad57da57f263e3d030187a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dJ3DYSIlv3Pb_6IEbqwOQ.png",
                        "isPro": false,
                        "fullname": "æ½˜å­è±ª",
                        "user": "Apostle723",
                        "type": "user"
                    },
                    "name": "Zihao Pan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:40:54.539Z",
                    "hidden": false
                },
                {
                    "_id": "68f8d62f6334fefb2d6023e5",
                    "name": "Yan Feng",
                    "hidden": false
                },
                {
                    "_id": "68f8d62f6334fefb2d6023e6",
                    "name": "Peng Pei",
                    "hidden": false
                },
                {
                    "_id": "68f8d62f6334fefb2d6023e7",
                    "name": "Xunliang Cai",
                    "hidden": false
                },
                {
                    "_id": "68f8d62f6334fefb2d6023e8",
                    "name": "Ruqi Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T13:36:58.000Z",
            "submittedOnDailyAt": "2025-10-22T13:14:36.771Z",
            "title": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from\n  Limited Views",
            "submittedOnDailyBy": {
                "_id": "65ad57da57f263e3d030187a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dJ3DYSIlv3Pb_6IEbqwOQ.png",
                "isPro": false,
                "fullname": "æ½˜å­è±ª",
                "user": "Apostle723",
                "type": "user"
            },
            "summary": "Though recent advances in vision-language models (VLMs) have achieved\nremarkable progress across a wide range of multimodal tasks, understanding 3D\nspatial relationships from limited views remains a significant challenge.\nPrevious reasoning methods typically rely on pure text (e.g., topological\ncognitive maps) or on 2D visual cues. However, their limited representational\ncapacity hinders performance in specific tasks that require 3D spatial\nimagination. To address this limitation, we propose 3DThinker, a framework that\ncan effectively exploits the rich geometric information embedded within images\nwhile reasoning, like humans do. Our framework is the first to enable 3D\nmentaling during reasoning without any 3D prior input, and it does not rely on\nexplicitly labeled 3D data for training. Specifically, our training consists of\ntwo stages. First, we perform supervised training to align the 3D latent\ngenerated by VLM while reasoning with that of a 3D foundation model (e.g.,\nVGGT). Then, we optimize the entire reasoning trajectory solely based on\noutcome signals, thereby refining the underlying 3D mentaling. Extensive\nexperiments across multiple benchmarks show that 3DThinker consistently\noutperforms strong baselines and offers a new perspective toward unifying 3D\nrepresentations into multimodal reasoning. Our code will be available at\nhttps://github.com/zhangquanchen/3DThinker.",
            "upvotes": 8,
            "discussionId": "68f8d62f6334fefb2d6023e9",
            "githubRepo": "https://github.com/zhangquanchen/3DThinker",
            "ai_summary": "3DThinker is a framework that enhances multimodal reasoning by integrating 3D spatial understanding from images without requiring 3D prior input or labeled data.",
            "ai_keywords": [
                "vision-language models",
                "3D spatial relationships",
                "3D mentaling",
                "3D latent",
                "3D foundation model",
                "VGGT",
                "multimodal reasoning"
            ],
            "githubStars": 8,
            "organization": {
                "_id": "628735cbc83a2d6ab8d14a66",
                "name": "Tsinghua",
                "fullname": "Tsinghua University"
            }
        },
        "publishedAt": "2025-10-21T09:36:58.000Z",
        "title": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from\n  Limited Views",
        "summary": "Though recent advances in vision-language models (VLMs) have achieved\nremarkable progress across a wide range of multimodal tasks, understanding 3D\nspatial relationships from limited views remains a significant challenge.\nPrevious reasoning methods typically rely on pure text (e.g., topological\ncognitive maps) or on 2D visual cues. However, their limited representational\ncapacity hinders performance in specific tasks that require 3D spatial\nimagination. To address this limitation, we propose 3DThinker, a framework that\ncan effectively exploits the rich geometric information embedded within images\nwhile reasoning, like humans do. Our framework is the first to enable 3D\nmentaling during reasoning without any 3D prior input, and it does not rely on\nexplicitly labeled 3D data for training. Specifically, our training consists of\ntwo stages. First, we perform supervised training to align the 3D latent\ngenerated by VLM while reasoning with that of a 3D foundation model (e.g.,\nVGGT). Then, we optimize the entire reasoning trajectory solely based on\noutcome signals, thereby refining the underlying 3D mentaling. Extensive\nexperiments across multiple benchmarks show that 3DThinker consistently\noutperforms strong baselines and offers a new perspective toward unifying 3D\nrepresentations into multimodal reasoning. Our code will be available at\nhttps://github.com/zhangquanchen/3DThinker.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18632.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65ad57da57f263e3d030187a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dJ3DYSIlv3Pb_6IEbqwOQ.png",
            "fullname": "æ½˜å­è±ª",
            "name": "Apostle723",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "628735cbc83a2d6ab8d14a66",
            "name": "Tsinghua",
            "fullname": "Tsinghua University"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.18873",
            "authors": [
                {
                    "_id": "68f854917669bcaeecce0d81",
                    "user": {
                        "_id": "65b36a383a41095a56d0736d",
                        "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
                        "isPro": true,
                        "fullname": "ZiangZhang",
                        "user": "Viglong",
                        "type": "user"
                    },
                    "name": "Ziang Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:43:30.029Z",
                    "hidden": false
                },
                {
                    "_id": "68f854917669bcaeecce0d82",
                    "name": "Zehan Wang",
                    "hidden": false
                },
                {
                    "_id": "68f854917669bcaeecce0d83",
                    "name": "Guanghao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f854917669bcaeecce0d84",
                    "name": "Weilong Dai",
                    "hidden": false
                },
                {
                    "_id": "68f854917669bcaeecce0d85",
                    "name": "Yan Xia",
                    "hidden": false
                },
                {
                    "_id": "68f854917669bcaeecce0d86",
                    "name": "Ziang Yan",
                    "hidden": false
                },
                {
                    "_id": "68f854917669bcaeecce0d87",
                    "name": "Minjie Hong",
                    "hidden": false
                },
                {
                    "_id": "68f854917669bcaeecce0d88",
                    "name": "Zhou Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T17:59:36.000Z",
            "submittedOnDailyAt": "2025-10-22T03:15:31.897Z",
            "title": "DSI-Bench: A Benchmark for Dynamic Spatial Intelligence",
            "submittedOnDailyBy": {
                "_id": "6425761a175bd295228311a0",
                "avatarUrl": "/avatars/dcd0d267445563d0616d5a31b5d754b7.svg",
                "isPro": false,
                "fullname": "zehan wang",
                "user": "sleetwang6",
                "type": "user"
            },
            "summary": "Reasoning about dynamic spatial relationships is essential, as both observers\nand objects often move simultaneously. Although vision-language models (VLMs)\nand visual expertise models excel in 2D tasks and static scenarios, their\nability to fully understand dynamic 3D scenarios remains limited. We introduce\nDynamic Spatial Intelligence and propose DSI-Bench, a benchmark with nearly\n1,000 dynamic videos and over 1,700 manually annotated questions covering nine\ndecoupled motion patterns of observers and objects. Spatially and temporally\nsymmetric designs reduce biases and enable systematic evaluation of models'\nreasoning about self-motion and object motion. Our evaluation of 14 VLMs and\nexpert models reveals key limitations: models often conflate observer and\nobject motion, exhibit semantic biases, and fail to accurately infer relative\nrelationships in dynamic scenarios. Our DSI-Bench provides valuable findings\nand insights about the future development of general and expertise models with\ndynamic spatial intelligence.",
            "upvotes": 6,
            "discussionId": "68f854917669bcaeecce0d89",
            "projectPage": "https://dsibench.github.io/",
            "githubRepo": "https://github.com/SpatialVision/dsibench",
            "ai_summary": "DSI-Bench evaluates the dynamic spatial reasoning capabilities of vision-language and visual expertise models through a benchmark of dynamic videos and annotated questions, highlighting their limitations in understanding self-motion, object motion, and relative relationships.",
            "ai_keywords": [
                "vision-language models",
                "visual expertise models",
                "dynamic spatial intelligence",
                "DSI-Bench",
                "dynamic videos",
                "manually annotated questions",
                "motion patterns",
                "spatially symmetric designs",
                "temporally symmetric designs",
                "self-motion",
                "object motion",
                "semantic biases",
                "relative relationships"
            ],
            "githubStars": 6,
            "organization": {
                "_id": "64488b334988ee01f2a8d856",
                "name": "alibaba-inc",
                "fullname": "alibaba-inc",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
            }
        },
        "publishedAt": "2025-10-21T13:59:36.000Z",
        "title": "DSI-Bench: A Benchmark for Dynamic Spatial Intelligence",
        "summary": "Reasoning about dynamic spatial relationships is essential, as both observers\nand objects often move simultaneously. Although vision-language models (VLMs)\nand visual expertise models excel in 2D tasks and static scenarios, their\nability to fully understand dynamic 3D scenarios remains limited. We introduce\nDynamic Spatial Intelligence and propose DSI-Bench, a benchmark with nearly\n1,000 dynamic videos and over 1,700 manually annotated questions covering nine\ndecoupled motion patterns of observers and objects. Spatially and temporally\nsymmetric designs reduce biases and enable systematic evaluation of models'\nreasoning about self-motion and object motion. Our evaluation of 14 VLMs and\nexpert models reveals key limitations: models often conflate observer and\nobject motion, exhibit semantic biases, and fail to accurately infer relative\nrelationships in dynamic scenarios. Our DSI-Bench provides valuable findings\nand insights about the future development of general and expertise models with\ndynamic spatial intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18873.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6425761a175bd295228311a0",
            "avatarUrl": "/avatars/dcd0d267445563d0616d5a31b5d754b7.svg",
            "fullname": "zehan wang",
            "name": "sleetwang6",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "64488b334988ee01f2a8d856",
            "name": "alibaba-inc",
            "fullname": "alibaba-inc",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14264",
            "authors": [
                {
                    "_id": "68f318bd8589920bf4d31ac8",
                    "name": "Zheye Deng",
                    "hidden": false
                },
                {
                    "_id": "68f318bd8589920bf4d31ac9",
                    "name": "Jiashu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T03:30:22.000Z",
            "submittedOnDailyAt": "2025-10-22T05:09:32.849Z",
            "title": "AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement\n  Learning Framework for Stock Trading",
            "submittedOnDailyBy": {
                "_id": "64d63b26c13c27a7015f7943",
                "avatarUrl": "/avatars/ab4a0fd89d4cea49e0d467e4ac2d4427.svg",
                "isPro": false,
                "fullname": "CL Yu",
                "user": "clyu",
                "type": "user"
            },
            "summary": "While Large Language Model (LLM) agents show promise in automated trading,\nthey still face critical limitations. Prominent multi-agent frameworks often\nsuffer from inefficiency, produce inconsistent signals, and lack the end-to-end\noptimization required to learn a coherent strategy from market feedback. To\naddress this, we introduce AlphaQuanter, a single-agent framework that uses\nreinforcement learning (RL) to learn a dynamic policy over a transparent,\ntool-augmented decision workflow, which empowers a single agent to autonomously\norchestrate tools and proactively acquire information on demand, establishing a\ntransparent and auditable reasoning process. Extensive experiments demonstrate\nthat AlphaQuanter achieves state-of-the-art performance on key financial\nmetrics. Moreover, its interpretable reasoning reveals sophisticated\nstrategies, offering novel and valuable insights for human traders. Our code\nfor data acquisition and agent training is publicly available at:\nhttps://github.com/AlphaQuanter/AlphaQuanter",
            "upvotes": 6,
            "discussionId": "68f318bd8589920bf4d31aca",
            "projectPage": "https://alphaquanter.github.io/",
            "githubRepo": "https://github.com/AlphaQuanter/AlphaQuanter",
            "ai_summary": "AlphaQuanter, a single-agent framework using reinforcement learning, achieves top performance in automated trading by learning dynamic policies and proactively acquiring information.",
            "ai_keywords": [
                "reinforcement learning",
                "RL",
                "dynamic policy",
                "decision workflow",
                "transparent reasoning",
                "interpretable reasoning"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-10-15T23:30:22.000Z",
        "title": "AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement\n  Learning Framework for Stock Trading",
        "summary": "While Large Language Model (LLM) agents show promise in automated trading,\nthey still face critical limitations. Prominent multi-agent frameworks often\nsuffer from inefficiency, produce inconsistent signals, and lack the end-to-end\noptimization required to learn a coherent strategy from market feedback. To\naddress this, we introduce AlphaQuanter, a single-agent framework that uses\nreinforcement learning (RL) to learn a dynamic policy over a transparent,\ntool-augmented decision workflow, which empowers a single agent to autonomously\norchestrate tools and proactively acquire information on demand, establishing a\ntransparent and auditable reasoning process. Extensive experiments demonstrate\nthat AlphaQuanter achieves state-of-the-art performance on key financial\nmetrics. Moreover, its interpretable reasoning reveals sophisticated\nstrategies, offering novel and valuable insights for human traders. Our code\nfor data acquisition and agent training is publicly available at:\nhttps://github.com/AlphaQuanter/AlphaQuanter",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14264.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d63b26c13c27a7015f7943",
            "avatarUrl": "/avatars/ab4a0fd89d4cea49e0d467e4ac2d4427.svg",
            "fullname": "CL Yu",
            "name": "clyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18554",
            "authors": [
                {
                    "_id": "68f880b17669bcaeecce0e73",
                    "name": "Federico Barbero",
                    "hidden": false
                },
                {
                    "_id": "68f880b17669bcaeecce0e74",
                    "name": "Xiangming Gu",
                    "hidden": false
                },
                {
                    "_id": "68f880b17669bcaeecce0e75",
                    "name": "Christopher A. Choquette-Choo",
                    "hidden": false
                },
                {
                    "_id": "68f880b17669bcaeecce0e76",
                    "name": "Chawin Sitawarin",
                    "hidden": false
                },
                {
                    "_id": "68f880b17669bcaeecce0e77",
                    "name": "Matthew Jagielski",
                    "hidden": false
                },
                {
                    "_id": "68f880b17669bcaeecce0e78",
                    "name": "Itay Yona",
                    "hidden": false
                },
                {
                    "_id": "68f880b17669bcaeecce0e79",
                    "name": "Petar VeliÄkoviÄ‡",
                    "hidden": false
                },
                {
                    "_id": "68f880b17669bcaeecce0e7a",
                    "name": "Ilia Shumailov",
                    "hidden": false
                },
                {
                    "_id": "68f880b17669bcaeecce0e7b",
                    "name": "Jamie Hayes",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T12:06:00.000Z",
            "submittedOnDailyAt": "2025-10-22T05:30:10.271Z",
            "title": "Extracting alignment data in open models",
            "submittedOnDailyBy": {
                "_id": "6475c2794766357252e69e9f",
                "avatarUrl": "/avatars/757ed789423113369868e972f21ce559.svg",
                "isPro": false,
                "fullname": "i",
                "user": "iliashum",
                "type": "user"
            },
            "summary": "In this work, we show that it is possible to extract significant amounts of\nalignment training data from a post-trained model -- useful to steer the model\nto improve certain capabilities such as long-context reasoning, safety,\ninstruction following, and maths. While the majority of related work on\nmemorisation has focused on measuring success of training data extraction\nthrough string matching, we argue that embedding models are better suited for\nour specific goals. Distances measured through a high quality embedding model\ncan identify semantic similarities between strings that a different metric such\nas edit distance will struggle to capture. In fact, in our investigation,\napproximate string matching would have severely undercounted (by a conservative\nestimate of 10times) the amount of data that can be extracted due to trivial\nartifacts that deflate the metric. Interestingly, we find that models readily\nregurgitate training data that was used in post-training phases such as SFT or\nRL. We show that this data can be then used to train a base model, recovering a\nmeaningful amount of the original performance. We believe our work exposes a\npossibly overlooked risk towards extracting alignment data. Finally, our work\nopens up an interesting discussion on the downstream effects of distillation\npractices: since models seem to be regurgitating aspects of their training set,\ndistillation can therefore be thought of as indirectly training on the model's\noriginal dataset.",
            "upvotes": 5,
            "discussionId": "68f880b17669bcaeecce0e7c",
            "ai_summary": "Extracting alignment training data from post-trained models using embedding models reveals significant semantic similarities and potential risks in distillation practices.",
            "ai_keywords": [
                "embedding models",
                "long-context reasoning",
                "safety",
                "instruction following",
                "maths",
                "string matching",
                "edit distance",
                "semantic similarities",
                "SFT",
                "RL",
                "distillation"
            ],
            "organization": {
                "_id": "5e6aca39878b8b2bf9806447",
                "name": "google",
                "fullname": "Google",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
            }
        },
        "publishedAt": "2025-10-21T08:06:00.000Z",
        "title": "Extracting alignment data in open models",
        "summary": "In this work, we show that it is possible to extract significant amounts of\nalignment training data from a post-trained model -- useful to steer the model\nto improve certain capabilities such as long-context reasoning, safety,\ninstruction following, and maths. While the majority of related work on\nmemorisation has focused on measuring success of training data extraction\nthrough string matching, we argue that embedding models are better suited for\nour specific goals. Distances measured through a high quality embedding model\ncan identify semantic similarities between strings that a different metric such\nas edit distance will struggle to capture. In fact, in our investigation,\napproximate string matching would have severely undercounted (by a conservative\nestimate of 10times) the amount of data that can be extracted due to trivial\nartifacts that deflate the metric. Interestingly, we find that models readily\nregurgitate training data that was used in post-training phases such as SFT or\nRL. We show that this data can be then used to train a base model, recovering a\nmeaningful amount of the original performance. We believe our work exposes a\npossibly overlooked risk towards extracting alignment data. Finally, our work\nopens up an interesting discussion on the downstream effects of distillation\npractices: since models seem to be regurgitating aspects of their training set,\ndistillation can therefore be thought of as indirectly training on the model's\noriginal dataset.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18554.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6475c2794766357252e69e9f",
            "avatarUrl": "/avatars/757ed789423113369868e972f21ce559.svg",
            "fullname": "i",
            "name": "iliashum",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "5e6aca39878b8b2bf9806447",
            "name": "google",
            "fullname": "Google",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18489",
            "authors": [
                {
                    "_id": "68f86c7b7669bcaeecce0e4e",
                    "user": {
                        "_id": "669dc7a932408ac579928f1e",
                        "avatarUrl": "/avatars/4e9d248749ea554e7803ac5f693ad00f.svg",
                        "isPro": false,
                        "fullname": "Jinfeng Liu",
                        "user": "jinfengliu26",
                        "type": "user"
                    },
                    "name": "Jinfeng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:42:44.786Z",
                    "hidden": false
                },
                {
                    "_id": "68f86c7b7669bcaeecce0e4f",
                    "name": "Lingtong Kong",
                    "hidden": false
                },
                {
                    "_id": "68f86c7b7669bcaeecce0e50",
                    "name": "Mi Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f86c7b7669bcaeecce0e51",
                    "name": "Jinwen Chen",
                    "hidden": false
                },
                {
                    "_id": "68f86c7b7669bcaeecce0e52",
                    "user": {
                        "_id": "66feab48651e00e22f33222e",
                        "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
                        "isPro": false,
                        "fullname": "Dan Xu",
                        "user": "danxuhk",
                        "type": "user"
                    },
                    "name": "Dan Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:42:42.192Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T10:14:33.000Z",
            "submittedOnDailyAt": "2025-10-22T04:07:30.150Z",
            "title": "Mono4DGS-HDR: High Dynamic Range 4D Gaussian Splatting from\n  Alternating-exposure Monocular Videos",
            "submittedOnDailyBy": {
                "_id": "669dc7a932408ac579928f1e",
                "avatarUrl": "/avatars/4e9d248749ea554e7803ac5f693ad00f.svg",
                "isPro": false,
                "fullname": "Jinfeng Liu",
                "user": "jinfengliu26",
                "type": "user"
            },
            "summary": "We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D\nhigh dynamic range (HDR) scenes from unposed monocular low dynamic range (LDR)\nvideos captured with alternating exposures. To tackle such a challenging\nproblem, we present a unified framework with two-stage optimization approach\nbased on Gaussian Splatting. The first stage learns a video HDR Gaussian\nrepresentation in orthographic camera coordinate space, eliminating the need\nfor camera poses and enabling robust initial HDR video reconstruction. The\nsecond stage transforms video Gaussians into world space and jointly refines\nthe world Gaussians with camera poses. Furthermore, we propose a temporal\nluminance regularization strategy to enhance the temporal consistency of the\nHDR appearance. Since our task has not been studied before, we construct a new\nevaluation benchmark using publicly available datasets for HDR video\nreconstruction. Extensive experiments demonstrate that Mono4DGS-HDR\nsignificantly outperforms alternative solutions adapted from state-of-the-art\nmethods in both rendering quality and speed.",
            "upvotes": 4,
            "discussionId": "68f86c7b7669bcaeecce0e53",
            "projectPage": "https://liujf1226.github.io/Mono4DGS-HDR/",
            "githubRepo": "https://github.com/LiuJF1226/Mono4DGS-HDR",
            "ai_summary": "A system for reconstructing 4D HDR scenes from unposed LDR videos using Gaussian Splatting with two-stage optimization and temporal luminance regularization.",
            "ai_keywords": [
                "Gaussian Splatting",
                "orthographic camera coordinate space",
                "world space",
                "temporal luminance regularization",
                "HDR video reconstruction"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-10-21T06:14:33.000Z",
        "title": "Mono4DGS-HDR: High Dynamic Range 4D Gaussian Splatting from\n  Alternating-exposure Monocular Videos",
        "summary": "We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D\nhigh dynamic range (HDR) scenes from unposed monocular low dynamic range (LDR)\nvideos captured with alternating exposures. To tackle such a challenging\nproblem, we present a unified framework with two-stage optimization approach\nbased on Gaussian Splatting. The first stage learns a video HDR Gaussian\nrepresentation in orthographic camera coordinate space, eliminating the need\nfor camera poses and enabling robust initial HDR video reconstruction. The\nsecond stage transforms video Gaussians into world space and jointly refines\nthe world Gaussians with camera poses. Furthermore, we propose a temporal\nluminance regularization strategy to enhance the temporal consistency of the\nHDR appearance. Since our task has not been studied before, we construct a new\nevaluation benchmark using publicly available datasets for HDR video\nreconstruction. Extensive experiments demonstrate that Mono4DGS-HDR\nsignificantly outperforms alternative solutions adapted from state-of-the-art\nmethods in both rendering quality and speed.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18489.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "669dc7a932408ac579928f1e",
            "avatarUrl": "/avatars/4e9d248749ea554e7803ac5f693ad00f.svg",
            "fullname": "Jinfeng Liu",
            "name": "jinfengliu26",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.17045",
            "authors": [
                {
                    "_id": "68f840f67669bcaeecce0cfb",
                    "name": "Deepak Sridhar",
                    "hidden": false
                },
                {
                    "_id": "68f840f67669bcaeecce0cfc",
                    "name": "Kartikeya Bhardwaj",
                    "hidden": false
                },
                {
                    "_id": "68f840f67669bcaeecce0cfd",
                    "name": "Jeya Pradha Jeyaraj",
                    "hidden": false
                },
                {
                    "_id": "68f840f67669bcaeecce0cfe",
                    "name": "Nuno Vasconcelos",
                    "hidden": false
                },
                {
                    "_id": "68f840f67669bcaeecce0cff",
                    "name": "Ankita Nayak",
                    "hidden": false
                },
                {
                    "_id": "68f840f67669bcaeecce0d00",
                    "name": "Harris Teague",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-19T23:17:13.000Z",
            "submittedOnDailyAt": "2025-10-22T00:58:39.308Z",
            "title": "Video Reasoning without Training",
            "submittedOnDailyBy": {
                "_id": "656a3b869ced9d5ff5f7185d",
                "avatarUrl": "/avatars/8f61cceb74b48cc778a8b4df04c9c6cb.svg",
                "isPro": false,
                "fullname": "Deepak Sridhar",
                "user": "DeepakSridhar",
                "type": "user"
            },
            "summary": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model.",
            "upvotes": 4,
            "discussionId": "68f840f67669bcaeecce0d01",
            "ai_summary": "The paper proposes V-Reason, a method that tunes the behavior of Large Multimodal Models during inference using entropy-based optimization, improving video reasoning accuracy and efficiency without reinforcement learning or supervised fine-tuning.",
            "ai_keywords": [
                "Large Multimodal Models",
                "reinforcement learning",
                "entropy",
                "micro-explorations",
                "micro-exploitations",
                "value cache",
                "entropy-based objective",
                "video reasoning datasets"
            ],
            "organization": {
                "_id": "616851aa840fa49535b3d5b2",
                "name": "qualcomm",
                "fullname": "Qualcomm",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
            }
        },
        "publishedAt": "2025-10-19T19:17:13.000Z",
        "title": "Video Reasoning without Training",
        "summary": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17045.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656a3b869ced9d5ff5f7185d",
            "avatarUrl": "/avatars/8f61cceb74b48cc778a8b4df04c9c6cb.svg",
            "fullname": "Deepak Sridhar",
            "name": "DeepakSridhar",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.16505",
            "authors": [
                {
                    "_id": "68f72b4124c4489363111a20",
                    "name": "Lukas Selch",
                    "hidden": false
                },
                {
                    "_id": "68f72b4124c4489363111a21",
                    "name": "Yufang Hou",
                    "hidden": false
                },
                {
                    "_id": "68f72b4124c4489363111a22",
                    "name": "M. Jehanzeb Mirza",
                    "hidden": false
                },
                {
                    "_id": "68f72b4124c4489363111a23",
                    "name": "Sivan Doveh",
                    "hidden": false
                },
                {
                    "_id": "68f72b4124c4489363111a24",
                    "name": "James Glass",
                    "hidden": false
                },
                {
                    "_id": "68f72b4124c4489363111a25",
                    "name": "Rogerio Feris",
                    "hidden": false
                },
                {
                    "_id": "68f72b4124c4489363111a26",
                    "name": "Wei Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-18T13:46:26.000Z",
            "submittedOnDailyAt": "2025-10-22T05:29:45.035Z",
            "title": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal\n  Inconsistencies",
            "submittedOnDailyBy": {
                "_id": "64b99a3fd4463c3d2183739d",
                "avatarUrl": "/avatars/c65f9bc024e0e1c0073c86a6421c77c5.svg",
                "isPro": false,
                "fullname": "Wei Lin",
                "user": "wlin21at",
                "type": "user"
            },
            "summary": "Large Multimodal Models (LMMs) are increasingly applied to scientific\nresearch, yet it remains unclear whether they can reliably understand and\nreason over the multimodal complexity of papers. A central challenge lies in\ndetecting and resolving inconsistencies across text, figures, tables, and\nequations, issues that are often subtle, domain-specific, and ultimately\nundermine clarity, reproducibility, and trust. Existing benchmarks overlook\nthis issue, either isolating single modalities or relying on synthetic errors\nthat fail to capture real-world complexity. We introduce PRISMM-Bench\n(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first\nbenchmark grounded in real reviewer-flagged inconsistencies in scientific\npapers. Through a multi-stage pipeline of review mining, LLM-assisted filtering\nand human verification, we curate 262 inconsistencies from 242 papers. Based on\nthis set, we design three tasks, namely inconsistency identification, remedy\nand pair matching, which assess a model's capacity to detect, correct, and\nreason over inconsistencies across different modalities. Furthermore, to\naddress the notorious problem of choice-only shortcuts in multiple-choice\nevaluation, where models exploit answer patterns without truly understanding\nthe question, we further introduce structured JSON-based answer representations\nthat minimize linguistic biases by reducing reliance on superficial stylistic\ncues. We benchmark 21 leading LMMs, including large open-weight models\n(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5\nwith high reasoning). Results reveal strikingly low performance (26.1-54.2%),\nunderscoring the challenge of multimodal scientific reasoning and motivating\nprogress towards trustworthy scientific assistants.",
            "upvotes": 3,
            "discussionId": "68f72b4224c4489363111a27",
            "githubRepo": "https://github.com/da-luggas/prismm-bench",
            "ai_summary": "PRISMM-Bench evaluates the ability of large multimodal models to detect, correct, and reason over inconsistencies in scientific papers, revealing significant challenges in multimodal scientific reasoning.",
            "ai_keywords": [
                "Large Multimodal Models",
                "PRISMM-Bench",
                "inconsistency identification",
                "remedy",
                "pair matching",
                "review mining",
                "LLM-assisted filtering",
                "human verification",
                "structured JSON-based answer representations",
                "multimodal scientific reasoning",
                "trustworthy scientific assistants"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-10-18T09:46:26.000Z",
        "title": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal\n  Inconsistencies",
        "summary": "Large Multimodal Models (LMMs) are increasingly applied to scientific\nresearch, yet it remains unclear whether they can reliably understand and\nreason over the multimodal complexity of papers. A central challenge lies in\ndetecting and resolving inconsistencies across text, figures, tables, and\nequations, issues that are often subtle, domain-specific, and ultimately\nundermine clarity, reproducibility, and trust. Existing benchmarks overlook\nthis issue, either isolating single modalities or relying on synthetic errors\nthat fail to capture real-world complexity. We introduce PRISMM-Bench\n(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first\nbenchmark grounded in real reviewer-flagged inconsistencies in scientific\npapers. Through a multi-stage pipeline of review mining, LLM-assisted filtering\nand human verification, we curate 262 inconsistencies from 242 papers. Based on\nthis set, we design three tasks, namely inconsistency identification, remedy\nand pair matching, which assess a model's capacity to detect, correct, and\nreason over inconsistencies across different modalities. Furthermore, to\naddress the notorious problem of choice-only shortcuts in multiple-choice\nevaluation, where models exploit answer patterns without truly understanding\nthe question, we further introduce structured JSON-based answer representations\nthat minimize linguistic biases by reducing reliance on superficial stylistic\ncues. We benchmark 21 leading LMMs, including large open-weight models\n(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5\nwith high reasoning). Results reveal strikingly low performance (26.1-54.2%),\nunderscoring the challenge of multimodal scientific reasoning and motivating\nprogress towards trustworthy scientific assistants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16505.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b99a3fd4463c3d2183739d",
            "avatarUrl": "/avatars/c65f9bc024e0e1c0073c86a6421c77c5.svg",
            "fullname": "Wei Lin",
            "name": "wlin21at",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07581",
            "authors": [
                {
                    "_id": "68f47aa88589920bf4d31d7d",
                    "name": "Zhongqi Yue",
                    "hidden": false
                },
                {
                    "_id": "68f47aa88589920bf4d31d7e",
                    "name": "Weishi Wang",
                    "hidden": false
                },
                {
                    "_id": "68f47aa88589920bf4d31d7f",
                    "user": {
                        "_id": "68f7b4717714aaca65414aab",
                        "avatarUrl": "/avatars/1990d56557fbe67c3a82578079db8a62.svg",
                        "isPro": false,
                        "fullname": "yundaichuanzhan",
                        "user": "yundaichuanzhan",
                        "type": "user"
                    },
                    "name": "Yundaichuan Zhan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:46:21.495Z",
                    "hidden": false
                },
                {
                    "_id": "68f47aa88589920bf4d31d80",
                    "name": "Juncheng Li",
                    "hidden": false
                },
                {
                    "_id": "68f47aa88589920bf4d31d81",
                    "name": "Daniel Dahlmeier",
                    "hidden": false
                },
                {
                    "_id": "68f47aa88589920bf4d31d82",
                    "name": "Fredrik D. Johansson",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T21:56:58.000Z",
            "submittedOnDailyAt": "2025-10-22T14:41:44.867Z",
            "title": "Expanding the Action Space of LLMs to Reason Beyond Language",
            "submittedOnDailyBy": {
                "_id": "68f7b4717714aaca65414aab",
                "avatarUrl": "/avatars/1990d56557fbe67c3a82578079db8a62.svg",
                "isPro": false,
                "fullname": "yundaichuanzhan",
                "user": "yundaichuanzhan",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) are powerful reasoners in natural language, but\ntheir actions are typically confined to outputting vocabulary tokens. As a\nresult, interactions with external environments -- such as symbolic operators\nor simulators -- must be expressed through text in predefined formats, parsed,\nand routed to external interfaces. This overloads the model's language with\nboth reasoning and control duties, and requires a hand-crafted parser, external\nto the LLM. To address this, we decouple environment interactions from language\nby internalizing them in an Expanded Action space (ExpA), beyond the\nvocabulary. The model starts reasoning in the default language environment, but\nmay trigger routing actions and switch to an external environment at any time.\nFrom there, the model can only invoke environment-specific actions, receive\nfeedback from the environment, and potentially route back to language as a\nresult. To promote effective exploration of the expanded action space and new\nenvironments, we introduce ExpA Reinforcement Learning (EARL) with\ncounterfactual policy optimization. On tasks requiring multi-turn interactions\nand contingent planning, EARL outperforms strong baselines with\nvocabulary-constrained actions. It performs robustly across calculator-based\nmulti-task learning and, in the partially observed sorting problem, achieves\nperfect Sort-4 accuracy while self-discovering an efficient algorithm\ncompetitive with classical designs.",
            "upvotes": 3,
            "discussionId": "68f47aa88589920bf4d31d83",
            "projectPage": "https://expa-rl.github.io/",
            "githubRepo": "https://github.com/yue-zhongqi/earl",
            "ai_summary": "Expanded Action space (ExpA) with ExpA Reinforcement Learning (EARL) enhances Large Language Models (LLMs) by decoupling environment interactions from language, improving performance in multi-turn interactions and contingent planning tasks.",
            "ai_keywords": [
                "Expanded Action space",
                "ExpA",
                "ExpA Reinforcement Learning",
                "EARL",
                "counterfactual policy optimization",
                "multi-turn interactions",
                "contingent planning",
                "calculator-based multi-task learning",
                "partially observed sorting problem",
                "Sort-4 accuracy"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-10-08T17:56:58.000Z",
        "title": "Expanding the Action Space of LLMs to Reason Beyond Language",
        "summary": "Large Language Models (LLMs) are powerful reasoners in natural language, but\ntheir actions are typically confined to outputting vocabulary tokens. As a\nresult, interactions with external environments -- such as symbolic operators\nor simulators -- must be expressed through text in predefined formats, parsed,\nand routed to external interfaces. This overloads the model's language with\nboth reasoning and control duties, and requires a hand-crafted parser, external\nto the LLM. To address this, we decouple environment interactions from language\nby internalizing them in an Expanded Action space (ExpA), beyond the\nvocabulary. The model starts reasoning in the default language environment, but\nmay trigger routing actions and switch to an external environment at any time.\nFrom there, the model can only invoke environment-specific actions, receive\nfeedback from the environment, and potentially route back to language as a\nresult. To promote effective exploration of the expanded action space and new\nenvironments, we introduce ExpA Reinforcement Learning (EARL) with\ncounterfactual policy optimization. On tasks requiring multi-turn interactions\nand contingent planning, EARL outperforms strong baselines with\nvocabulary-constrained actions. It performs robustly across calculator-based\nmulti-task learning and, in the partially observed sorting problem, achieves\nperfect Sort-4 accuracy while self-discovering an efficient algorithm\ncompetitive with classical designs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07581.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68f7b4717714aaca65414aab",
            "avatarUrl": "/avatars/1990d56557fbe67c3a82578079db8a62.svg",
            "fullname": "yundaichuanzhan",
            "name": "yundaichuanzhan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.18081",
            "authors": [
                {
                    "_id": "68f8766d7669bcaeecce0e67",
                    "name": "Jiawei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f8766d7669bcaeecce0e68",
                    "name": "Andrew Estornell",
                    "hidden": false
                },
                {
                    "_id": "68f8766d7669bcaeecce0e69",
                    "name": "David D. Baek",
                    "hidden": false
                },
                {
                    "_id": "68f8766d7669bcaeecce0e6a",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "68f8766d7669bcaeecce0e6b",
                    "name": "Xiaojun Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T20:18:59.000Z",
            "submittedOnDailyAt": "2025-10-22T15:40:59.730Z",
            "title": "Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to\n  Any-Depth",
            "submittedOnDailyBy": {
                "_id": "64908d97231a197da761e512",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/yWo-f9uMxliCV-FTwAHhX.png",
                "isPro": false,
                "fullname": "Jiawei Zhang",
                "user": "javyduck",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) exhibit strong but shallow alignment: they\ndirectly refuse harmful queries when a refusal is expected at the very start of\nan assistant turn, yet this protection collapses once a harmful continuation is\nunderway (either through the adversarial attacks or via harmful\nassistant-prefill attacks). This raises a fundamental question: Can the innate\nshallow alignment in LLMs be unlocked to ensure safety at arbitrary generation\ndepths? To achieve this goal, we propose Any-Depth Alignment (ADA), an\neffective inference-time defense with negligible overhead. ADA is built based\non our observation that alignment is concentrated in the assistant header\ntokens through repeated use in shallow-refusal training, and these tokens\npossess the model's strong alignment priors. By reintroducing these tokens\nmid-stream, ADA induces the model to reassess harmfulness and recover refusals\nat any point in generation. Across diverse open-source model families (Llama,\nGemma, Mistral, Qwen, DeepSeek, and gpt-oss), ADA achieves robust safety\nperformance without requiring any changes to the base model's parameters. It\nsecures a near-100% refusal rate against challenging adversarial prefill\nattacks ranging from dozens to thousands of tokens. Furthermore, ADA reduces\nthe average success rate of prominent adversarial prompt attacks (such as GCG,\nAutoDAN, PAIR, and TAP) to below 3%. This is all accomplished while preserving\nutility on benign tasks with minimal over-refusal. ADA maintains this\nresilience even after the base model undergoes subsequent instruction tuning\n(benign or adversarial).",
            "upvotes": 2,
            "discussionId": "68f8766d7669bcaeecce0e6c",
            "ai_summary": "Any-Depth Alignment (ADA) is an inference-time defense that enhances the safety of Large Language Models (LLMs) by reintroducing alignment tokens mid-stream, ensuring robust protection against adversarial attacks without altering the model's parameters.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "shallow alignment",
                "harmful queries",
                "adversarial attacks",
                "assistant-prefill attacks",
                "Any-Depth Alignment (ADA)",
                "assistant header tokens",
                "shallow-refusal training",
                "adversarial prefill attacks",
                "adversarial prompt attacks",
                "GCG",
                "AutoDAN",
                "PAIR",
                "TAP",
                "instruction tuning"
            ],
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-10-20T16:18:59.000Z",
        "title": "Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to\n  Any-Depth",
        "summary": "Large Language Models (LLMs) exhibit strong but shallow alignment: they\ndirectly refuse harmful queries when a refusal is expected at the very start of\nan assistant turn, yet this protection collapses once a harmful continuation is\nunderway (either through the adversarial attacks or via harmful\nassistant-prefill attacks). This raises a fundamental question: Can the innate\nshallow alignment in LLMs be unlocked to ensure safety at arbitrary generation\ndepths? To achieve this goal, we propose Any-Depth Alignment (ADA), an\neffective inference-time defense with negligible overhead. ADA is built based\non our observation that alignment is concentrated in the assistant header\ntokens through repeated use in shallow-refusal training, and these tokens\npossess the model's strong alignment priors. By reintroducing these tokens\nmid-stream, ADA induces the model to reassess harmfulness and recover refusals\nat any point in generation. Across diverse open-source model families (Llama,\nGemma, Mistral, Qwen, DeepSeek, and gpt-oss), ADA achieves robust safety\nperformance without requiring any changes to the base model's parameters. It\nsecures a near-100% refusal rate against challenging adversarial prefill\nattacks ranging from dozens to thousands of tokens. Furthermore, ADA reduces\nthe average success rate of prominent adversarial prompt attacks (such as GCG,\nAutoDAN, PAIR, and TAP) to below 3%. This is all accomplished while preserving\nutility on benign tasks with minimal over-refusal. ADA maintains this\nresilience even after the base model undergoes subsequent instruction tuning\n(benign or adversarial).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18081.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64908d97231a197da761e512",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/yWo-f9uMxliCV-FTwAHhX.png",
            "fullname": "Jiawei Zhang",
            "name": "javyduck",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.15710",
            "authors": [
                {
                    "_id": "68f5cd4d8589920bf4d321cc",
                    "user": {
                        "_id": "67193c60f89011dc142caf07",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/HC3fjL9V0hhpmjc4kWnGF.png",
                        "isPro": false,
                        "fullname": "Junzhi Ning",
                        "user": "junzhin",
                        "type": "user"
                    },
                    "name": "Junzhi Ning",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:46:18.243Z",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321cd",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321ce",
                    "name": "Cheng Tang",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321cf",
                    "name": "Jiashi Lin",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321d0",
                    "name": "Chenglong Ma",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321d1",
                    "name": "Chaoyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321d2",
                    "name": "Jiyao Liu",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321d3",
                    "name": "Ying Chen",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321d4",
                    "name": "Shujian Gao",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321d5",
                    "name": "Lihao Liu",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321d6",
                    "user": {
                        "_id": "625d5b9f0bec31f086e04cd9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650285458447-noauth.jpeg",
                        "isPro": false,
                        "fullname": "YuandongPu",
                        "user": "Andrew613",
                        "type": "user"
                    },
                    "name": "Yuandong Pu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:04:45.300Z",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321d7",
                    "name": "Huihui Xu",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321d8",
                    "name": "Chenhui Gou",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321d9",
                    "name": "Ziyan Huang",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321da",
                    "name": "Yi Xin",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321db",
                    "name": "Qi Qin",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321dc",
                    "name": "Zhongying Deng",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321dd",
                    "name": "Diping Song",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321de",
                    "name": "Bin Fu",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321df",
                    "name": "Guang Yang",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321e0",
                    "name": "Yuanfeng Ji",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321e1",
                    "name": "Tianbin Li",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321e2",
                    "name": "Yanzhou Su",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321e3",
                    "name": "Jin Ye",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321e4",
                    "name": "Shixiang Tang",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321e5",
                    "name": "Ming Hu",
                    "hidden": false
                },
                {
                    "_id": "68f5cd4d8589920bf4d321e6",
                    "name": "Junjun He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T14:54:58.000Z",
            "submittedOnDailyAt": "2025-10-22T14:33:57.403Z",
            "title": "Unimedvl: Unifying Medical Multimodal Understanding And Generation\n  Through Observation-Knowledge-Analysis",
            "submittedOnDailyBy": {
                "_id": "67193c60f89011dc142caf07",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/HC3fjL9V0hhpmjc4kWnGF.png",
                "isPro": false,
                "fullname": "Junzhi Ning",
                "user": "junzhin",
                "type": "user"
            },
            "summary": "Medical diagnostic applications require models that can process multimodal\nmedical inputs (images, patient histories, lab results) and generate diverse\noutputs including both textual reports and visual content (annotations,\nsegmentation masks, and images). Despite this need, existing medical AI systems\ndisrupt this unified process: medical image understanding models interpret\nimages but cannot generate visual outputs, while medical image generation\nmodels synthesize images but cannot provide textual explanations. This leads to\ngaps in data representation, feature integration, and task-level multimodal\ncapabilities. To this end, we propose a multi-level framework that draws\ninspiration from diagnostic workflows through the\nObservation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation\nlevel, we construct UniMed-5M, a dataset comprising over 5.6M samples that\nreformat diverse unimodal data into multimodal pairs for foundational\nobservation. At the knowledge level, we propose Progressive Curriculum Learning\nthat systematically introduces medical multimodal knowledge. At the analysis\nlevel, we introduce UniMedVL, the first medical unified multimodal model for\nthe simultaneous analysis of image understanding and generation tasks within a\nsingle architecture. UniMedVL achieves superior performance on five medical\nimage understanding benchmarks, while matching specialized models in generation\nquality across eight medical imaging modalities. Crucially, our unified\narchitecture enables bidirectional knowledge sharing: generation tasks enhance\nvisual understanding features, demonstrating that integrating traditionally\nseparate capabilities within a single medical framework unlocks improvements\nacross diverse medical vision-language tasks. Code is available at\nhttps://github.com/uni-medical/UniMedVL.",
            "upvotes": 2,
            "discussionId": "68f5cd4d8589920bf4d321e7",
            "projectPage": "https://uni-medical.github.io/UniMedVL_Web/",
            "githubRepo": "https://github.com/uni-medical/UniMedVL",
            "ai_summary": "A unified multimodal medical model integrates image understanding and generation, enhancing performance across various medical vision-language tasks.",
            "ai_keywords": [
                "UniMed-5M",
                "Progressive Curriculum Learning",
                "UniMedVL",
                "medical image understanding",
                "medical image generation",
                "multimodal pairs",
                "bidirectional knowledge sharing"
            ],
            "githubStars": 13,
            "organization": {
                "_id": "672c4d30960e0409c5bf94e6",
                "name": "General-Medical-AI",
                "fullname": "General Medical AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64c0aaa4a8a2dcaa167ab5b3/tdo-7xOrIHU6DE4kCgu2N.png"
            }
        },
        "publishedAt": "2025-10-17T10:54:58.000Z",
        "title": "Unimedvl: Unifying Medical Multimodal Understanding And Generation\n  Through Observation-Knowledge-Analysis",
        "summary": "Medical diagnostic applications require models that can process multimodal\nmedical inputs (images, patient histories, lab results) and generate diverse\noutputs including both textual reports and visual content (annotations,\nsegmentation masks, and images). Despite this need, existing medical AI systems\ndisrupt this unified process: medical image understanding models interpret\nimages but cannot generate visual outputs, while medical image generation\nmodels synthesize images but cannot provide textual explanations. This leads to\ngaps in data representation, feature integration, and task-level multimodal\ncapabilities. To this end, we propose a multi-level framework that draws\ninspiration from diagnostic workflows through the\nObservation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation\nlevel, we construct UniMed-5M, a dataset comprising over 5.6M samples that\nreformat diverse unimodal data into multimodal pairs for foundational\nobservation. At the knowledge level, we propose Progressive Curriculum Learning\nthat systematically introduces medical multimodal knowledge. At the analysis\nlevel, we introduce UniMedVL, the first medical unified multimodal model for\nthe simultaneous analysis of image understanding and generation tasks within a\nsingle architecture. UniMedVL achieves superior performance on five medical\nimage understanding benchmarks, while matching specialized models in generation\nquality across eight medical imaging modalities. Crucially, our unified\narchitecture enables bidirectional knowledge sharing: generation tasks enhance\nvisual understanding features, demonstrating that integrating traditionally\nseparate capabilities within a single medical framework unlocks improvements\nacross diverse medical vision-language tasks. Code is available at\nhttps://github.com/uni-medical/UniMedVL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15710.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67193c60f89011dc142caf07",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/HC3fjL9V0hhpmjc4kWnGF.png",
            "fullname": "Junzhi Ning",
            "name": "junzhin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "672c4d30960e0409c5bf94e6",
            "name": "General-Medical-AI",
            "fullname": "General Medical AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64c0aaa4a8a2dcaa167ab5b3/tdo-7xOrIHU6DE4kCgu2N.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.15600",
            "authors": [
                {
                    "_id": "68f85e4d7669bcaeecce0da6",
                    "name": "Haoran Sun",
                    "hidden": false
                },
                {
                    "_id": "68f85e4d7669bcaeecce0da7",
                    "name": "Yankai Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f85e4d7669bcaeecce0da8",
                    "name": "Zhenyu Tang",
                    "hidden": false
                },
                {
                    "_id": "68f85e4d7669bcaeecce0da9",
                    "name": "Yaning Pan",
                    "hidden": false
                },
                {
                    "_id": "68f85e4d7669bcaeecce0daa",
                    "name": "Shuang Gu",
                    "hidden": false
                },
                {
                    "_id": "68f85e4d7669bcaeecce0dab",
                    "name": "Zekai Lin",
                    "hidden": false
                },
                {
                    "_id": "68f85e4d7669bcaeecce0dac",
                    "name": "Lilong Wang",
                    "hidden": false
                },
                {
                    "_id": "68f85e4d7669bcaeecce0dad",
                    "name": "Wenjie Lou",
                    "hidden": false
                },
                {
                    "_id": "68f85e4d7669bcaeecce0dae",
                    "name": "Lei Liu",
                    "hidden": false
                },
                {
                    "_id": "68f85e4d7669bcaeecce0daf",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "68f85e4d7669bcaeecce0db0",
                    "name": "Xiaosong Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T12:47:50.000Z",
            "submittedOnDailyAt": "2025-10-22T03:07:44.070Z",
            "title": "Unleashing Scientific Reasoning for Bio-experimental Protocol Generation\n  via Structured Component-based Reward Mechanism",
            "submittedOnDailyBy": {
                "_id": "67547707f168984215451697",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67547707f168984215451697/7Hp8Neb8_jkW4m0LXXXiM.jpeg",
                "isPro": false,
                "fullname": "manglu",
                "user": "manglu3935",
                "type": "user"
            },
            "summary": "The foundation of reproducible science lies in protocols that are precise,\nlogically ordered, and executable. The autonomous generation of these protocols\nthrough natural language queries could greatly improve the efficiency of the\nreproduction process. However, current leading large language models (LLMs)\noften generate incomplete or inconsistent protocols, limiting their utility. To\naddress this limitation, we first introduce SciRecipe, a large-scale dataset of\nover 12K structured protocols spanning 27 biological subfields and encompassing\nboth comprehension and problem-solving tasks. To further improve protocol\ngeneration, we propose the \"Sketch-and-Fill\" paradigm, which separates\nanalysis, structuring, and expression to ensure each step is explicit and\nverifiable. Complementing this, the structured component-based reward mechanism\nevaluates step granularity, action order, and semantic fidelity, aligning model\noptimization with experimental reliability. Building on these components, we\ndevelop Thoth, trained through a staged Knowledge-to-Action process that\nprogresses from knowledge acquisition to operational reasoning and ultimately\nto robust, executable protocol generation. Across multiple benchmarks, Thoth\nconsistently surpasses both proprietary and open-source LLMs, achieving\nsignificant improvements in step alignment, logical sequencing, and semantic\naccuracy. Our approach paves the way for reliable scientific assistants that\nbridge knowledge with experimental execution. All data, code, and models will\nbe released publicly.",
            "upvotes": 2,
            "discussionId": "68f85e4d7669bcaeecce0db1",
            "ai_summary": "Thoth, a large language model trained with the Sketch-and-Fill paradigm and structured component-based reward mechanism, generates more reliable and executable scientific protocols compared to existing models.",
            "ai_keywords": [
                "large language models",
                "SciRecipe",
                "Sketch-and-Fill",
                "structured component-based reward mechanism",
                "Knowledge-to-Action process",
                "step alignment",
                "logical sequencing",
                "semantic accuracy"
            ]
        },
        "publishedAt": "2025-10-17T08:47:50.000Z",
        "title": "Unleashing Scientific Reasoning for Bio-experimental Protocol Generation\n  via Structured Component-based Reward Mechanism",
        "summary": "The foundation of reproducible science lies in protocols that are precise,\nlogically ordered, and executable. The autonomous generation of these protocols\nthrough natural language queries could greatly improve the efficiency of the\nreproduction process. However, current leading large language models (LLMs)\noften generate incomplete or inconsistent protocols, limiting their utility. To\naddress this limitation, we first introduce SciRecipe, a large-scale dataset of\nover 12K structured protocols spanning 27 biological subfields and encompassing\nboth comprehension and problem-solving tasks. To further improve protocol\ngeneration, we propose the \"Sketch-and-Fill\" paradigm, which separates\nanalysis, structuring, and expression to ensure each step is explicit and\nverifiable. Complementing this, the structured component-based reward mechanism\nevaluates step granularity, action order, and semantic fidelity, aligning model\noptimization with experimental reliability. Building on these components, we\ndevelop Thoth, trained through a staged Knowledge-to-Action process that\nprogresses from knowledge acquisition to operational reasoning and ultimately\nto robust, executable protocol generation. Across multiple benchmarks, Thoth\nconsistently surpasses both proprietary and open-source LLMs, achieving\nsignificant improvements in step alignment, logical sequencing, and semantic\naccuracy. Our approach paves the way for reliable scientific assistants that\nbridge knowledge with experimental execution. All data, code, and models will\nbe released publicly.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15600.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67547707f168984215451697",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67547707f168984215451697/7Hp8Neb8_jkW4m0LXXXiM.jpeg",
            "fullname": "manglu",
            "name": "manglu3935",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14463",
            "authors": [
                {
                    "_id": "68f621a18589920bf4d322c8",
                    "user": {
                        "_id": "6676179e4b1e661916d0c654",
                        "avatarUrl": "/avatars/a074b2c7baa49de9324329c752b49dfd.svg",
                        "isPro": false,
                        "fullname": "Thomas Katraouras",
                        "user": "Tomk187",
                        "type": "user"
                    },
                    "name": "Thomas Katraouras",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-20T12:09:22.432Z",
                    "hidden": false
                },
                {
                    "_id": "68f621a18589920bf4d322c9",
                    "name": "Dimitrios Rafailidis",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T09:04:05.000Z",
            "submittedOnDailyAt": "2025-10-22T20:16:53.814Z",
            "title": "Pruning Overparameterized Multi-Task Networks for Degraded Web Image\n  Restoration",
            "submittedOnDailyBy": {
                "_id": "6676179e4b1e661916d0c654",
                "avatarUrl": "/avatars/a074b2c7baa49de9324329c752b49dfd.svg",
                "isPro": false,
                "fullname": "Thomas Katraouras",
                "user": "Tomk187",
                "type": "user"
            },
            "summary": "Image quality is a critical factor in delivering visually appealing content\non web platforms. However, images often suffer from degradation due to lossy\noperations applied by online social networks (OSNs), negatively affecting user\nexperience. Image restoration is the process of recovering a clean high-quality\nimage from a given degraded input. Recently, multi-task (all-in-one) image\nrestoration models have gained significant attention, due to their ability to\nsimultaneously handle different types of image degradations. However, these\nmodels often come with an excessively high number of trainable parameters,\nmaking them computationally inefficient. In this paper, we propose a strategy\nfor compressing multi-task image restoration models. We aim to discover highly\nsparse subnetworks within overparameterized deep models that can match or even\nsurpass the performance of their dense counterparts. The proposed model, namely\nMIR-L, utilizes an iterative pruning strategy that removes low-magnitude\nweights across multiple rounds, while resetting the remaining weights to their\noriginal initialization. This iterative process is important for the multi-task\nimage restoration model's optimization, effectively uncovering \"winning\ntickets\" that maintain or exceed state-of-the-art performance at high sparsity\nlevels. Experimental evaluation on benchmark datasets for the deraining,\ndehazing, and denoising tasks shows that MIR-L retains only 10% of the\ntrainable parameters while maintaining high image restoration performance. Our\ncode, datasets and pre-trained models are made publicly available at\nhttps://github.com/Thomkat/MIR-L.",
            "upvotes": 2,
            "discussionId": "68f621a18589920bf4d322ca",
            "githubRepo": "https://github.com/Thomkat/MIR-L",
            "ai_summary": "MIR-L, a compressed multi-task image restoration model, achieves high performance with significantly reduced parameters through iterative pruning and weight resetting.",
            "ai_keywords": [
                "multi-task image restoration",
                "overparameterized deep models",
                "sparse subnetworks",
                "iterative pruning",
                "low-magnitude weights",
                "winning tickets",
                "deraining",
                "dehazing",
                "denoising"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-10-16T05:04:05.000Z",
        "title": "Pruning Overparameterized Multi-Task Networks for Degraded Web Image\n  Restoration",
        "summary": "Image quality is a critical factor in delivering visually appealing content\non web platforms. However, images often suffer from degradation due to lossy\noperations applied by online social networks (OSNs), negatively affecting user\nexperience. Image restoration is the process of recovering a clean high-quality\nimage from a given degraded input. Recently, multi-task (all-in-one) image\nrestoration models have gained significant attention, due to their ability to\nsimultaneously handle different types of image degradations. However, these\nmodels often come with an excessively high number of trainable parameters,\nmaking them computationally inefficient. In this paper, we propose a strategy\nfor compressing multi-task image restoration models. We aim to discover highly\nsparse subnetworks within overparameterized deep models that can match or even\nsurpass the performance of their dense counterparts. The proposed model, namely\nMIR-L, utilizes an iterative pruning strategy that removes low-magnitude\nweights across multiple rounds, while resetting the remaining weights to their\noriginal initialization. This iterative process is important for the multi-task\nimage restoration model's optimization, effectively uncovering \"winning\ntickets\" that maintain or exceed state-of-the-art performance at high sparsity\nlevels. Experimental evaluation on benchmark datasets for the deraining,\ndehazing, and denoising tasks shows that MIR-L retains only 10% of the\ntrainable parameters while maintaining high image restoration performance. Our\ncode, datasets and pre-trained models are made publicly available at\nhttps://github.com/Thomkat/MIR-L.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14463.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6676179e4b1e661916d0c654",
            "avatarUrl": "/avatars/a074b2c7baa49de9324329c752b49dfd.svg",
            "fullname": "Thomas Katraouras",
            "name": "Tomk187",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.17862",
            "authors": [
                {
                    "_id": "68f94e42b9b2e4ae046736b1",
                    "name": "Yibo Peng",
                    "hidden": false
                },
                {
                    "_id": "68f94e42b9b2e4ae046736b2",
                    "name": "James Song",
                    "hidden": false
                },
                {
                    "_id": "68f94e42b9b2e4ae046736b3",
                    "name": "Lei Li",
                    "hidden": false
                },
                {
                    "_id": "68f94e42b9b2e4ae046736b4",
                    "name": "Xinyu Yang",
                    "hidden": false
                },
                {
                    "_id": "68f94e42b9b2e4ae046736b5",
                    "name": "Mihai Christodorescu",
                    "hidden": false
                },
                {
                    "_id": "68f94e42b9b2e4ae046736b6",
                    "name": "Ravi Mangal",
                    "hidden": false
                },
                {
                    "_id": "68f94e42b9b2e4ae046736b7",
                    "name": "Corina Pasareanu",
                    "hidden": false
                },
                {
                    "_id": "68f94e42b9b2e4ae046736b8",
                    "name": "Haizhong Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f94e42b9b2e4ae046736b9",
                    "name": "Beidi Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T17:16:36.000Z",
            "submittedOnDailyAt": "2025-10-22T20:07:16.889Z",
            "title": "When \"Correct\" Is Not Safe: Can We Trust Functionally Correct Patches\n  Generated by Code Agents?",
            "submittedOnDailyBy": {
                "_id": "64affbb951e993d33be0ddd1",
                "avatarUrl": "/avatars/36eb12f5c74cfe69d16a954f96819308.svg",
                "isPro": false,
                "fullname": "Haizhong",
                "user": "haizhongzheng",
                "type": "user"
            },
            "summary": "Code agents are increasingly trusted to autonomously fix bugs on platforms\nsuch as GitHub, yet their security evaluation focuses almost exclusively on\nfunctional correctness. In this paper, we reveal a novel type of threat to\nreal-world code agents: Functionally Correct yet Vulnerable (FCV) patches,\nwhich pass all test cases but contain vulnerable code. With our proposed\nFCV-Attack, which can be deliberately crafted by malicious attackers or\nimplicitly introduced by benign developers, we show that SOTA LLMs (e.g.,\nChatGPT and Claude) and agent scaffolds (e.g., SWE-agent and OpenHands) are all\nvulnerable to this FCV threat; across 12 agent-model combinations on SWE-Bench,\nthe attack only requires black-box access and a single query to the code agent\nto perform the attack. For example, for CWE-538 (information exposure\nvulnerability), the FCV-Attack attains an attack success rate of 40.7% on\nGPT-5 Mini + OpenHands. Our results reveal an important security threat\noverlooked by current evaluation paradigms and urge the development of\nsecurity-aware defenses for code agents.",
            "upvotes": 2,
            "discussionId": "68f94e42b9b2e4ae046736ba",
            "ai_summary": "FCV-Attack exploits functionally correct yet vulnerable patches in code agents, demonstrating a significant security threat to current evaluation methods.",
            "ai_keywords": [
                "Functionally Correct yet Vulnerable (FCV) patches",
                "FCV-Attack",
                "SOTA LLMs",
                "agent scaffolds",
                "SWE-Bench",
                "CWE-538",
                "information exposure vulnerability"
            ]
        },
        "publishedAt": "2025-10-15T13:16:36.000Z",
        "title": "When \"Correct\" Is Not Safe: Can We Trust Functionally Correct Patches\n  Generated by Code Agents?",
        "summary": "Code agents are increasingly trusted to autonomously fix bugs on platforms\nsuch as GitHub, yet their security evaluation focuses almost exclusively on\nfunctional correctness. In this paper, we reveal a novel type of threat to\nreal-world code agents: Functionally Correct yet Vulnerable (FCV) patches,\nwhich pass all test cases but contain vulnerable code. With our proposed\nFCV-Attack, which can be deliberately crafted by malicious attackers or\nimplicitly introduced by benign developers, we show that SOTA LLMs (e.g.,\nChatGPT and Claude) and agent scaffolds (e.g., SWE-agent and OpenHands) are all\nvulnerable to this FCV threat; across 12 agent-model combinations on SWE-Bench,\nthe attack only requires black-box access and a single query to the code agent\nto perform the attack. For example, for CWE-538 (information exposure\nvulnerability), the FCV-Attack attains an attack success rate of 40.7% on\nGPT-5 Mini + OpenHands. Our results reveal an important security threat\noverlooked by current evaluation paradigms and urge the development of\nsecurity-aware defenses for code agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17862.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64affbb951e993d33be0ddd1",
            "avatarUrl": "/avatars/36eb12f5c74cfe69d16a954f96819308.svg",
            "fullname": "Haizhong",
            "name": "haizhongzheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18087",
            "authors": [
                {
                    "_id": "68f821b07669bcaeecce0b6f",
                    "name": "Daniel Israel",
                    "hidden": false
                },
                {
                    "_id": "68f821b07669bcaeecce0b70",
                    "name": "Tian Jin",
                    "hidden": false
                },
                {
                    "_id": "68f821b07669bcaeecce0b71",
                    "name": "Ellie Cheng",
                    "hidden": false
                },
                {
                    "_id": "68f821b07669bcaeecce0b72",
                    "name": "Guy Van den Broeck",
                    "hidden": false
                },
                {
                    "_id": "68f821b07669bcaeecce0b73",
                    "name": "Aditya Grover",
                    "hidden": false
                },
                {
                    "_id": "68f821b07669bcaeecce0b74",
                    "name": "Suvinay Subramanian",
                    "hidden": false
                },
                {
                    "_id": "68f821b07669bcaeecce0b75",
                    "name": "Michael Carbin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T20:27:48.000Z",
            "submittedOnDailyAt": "2025-10-22T14:52:26.267Z",
            "title": "Planned Diffusion",
            "submittedOnDailyBy": {
                "_id": "630139f1f6bea7dd15bdaf4e",
                "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
                "isPro": false,
                "fullname": "Daniel Israel",
                "user": "danielmisrael",
                "type": "user"
            },
            "summary": "A central challenge in large language model inference is the trade-off\nbetween generation speed and output quality. Autoregressive models produce\nhigh-quality text but generate tokens sequentially. Diffusion models can\ngenerate tokens in parallel but often need many iterations to match the same\nquality. We propose planned diffusion, a hybrid method that combines the\nstrengths of both paradigms. Planned diffusion works in two stages: first, the\nmodel creates a short autoregressive plan that breaks the output into smaller,\nindependent spans. Second, the model generates these spans simultaneously using\ndiffusion. This approach expands the speed-quality Pareto frontier and provides\na practical path to faster, high-quality text generation. On AlpacaEval, a\nsuite of 805 instruction-following prompts, planned diffusion achieves\nPareto-optimal trade-off between quality and latency, achieving 1.27x to 1.81x\nspeedup over autoregressive generation with only 0.87\\% to 5.4\\% drop in win\nrate, respectively. Our sensitivity analysis shows that the planning mechanism\nof planned diffusion is minimal and reliable, and simple runtime knobs exist to\nprovide flexible control of the quality-latency trade-off.",
            "upvotes": 1,
            "discussionId": "68f821b07669bcaeecce0b76",
            "projectPage": "https://planned-diffusion.github.io/",
            "githubRepo": "https://github.com/planned-diffusion/planned-diffusion",
            "ai_summary": "Planned diffusion combines autoregressive and diffusion models to achieve faster text generation with minimal quality loss.",
            "ai_keywords": [
                "autoregressive models",
                "diffusion models",
                "planned diffusion",
                "Pareto frontier",
                "AlpacaEval",
                "win rate",
                "sensitivity analysis"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-10-20T16:27:48.000Z",
        "title": "Planned Diffusion",
        "summary": "A central challenge in large language model inference is the trade-off\nbetween generation speed and output quality. Autoregressive models produce\nhigh-quality text but generate tokens sequentially. Diffusion models can\ngenerate tokens in parallel but often need many iterations to match the same\nquality. We propose planned diffusion, a hybrid method that combines the\nstrengths of both paradigms. Planned diffusion works in two stages: first, the\nmodel creates a short autoregressive plan that breaks the output into smaller,\nindependent spans. Second, the model generates these spans simultaneously using\ndiffusion. This approach expands the speed-quality Pareto frontier and provides\na practical path to faster, high-quality text generation. On AlpacaEval, a\nsuite of 805 instruction-following prompts, planned diffusion achieves\nPareto-optimal trade-off between quality and latency, achieving 1.27x to 1.81x\nspeedup over autoregressive generation with only 0.87\\% to 5.4\\% drop in win\nrate, respectively. Our sensitivity analysis shows that the planning mechanism\nof planned diffusion is minimal and reliable, and simple runtime knobs exist to\nprovide flexible control of the quality-latency trade-off.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18087.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630139f1f6bea7dd15bdaf4e",
            "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
            "fullname": "Daniel Israel",
            "name": "danielmisrael",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.17388",
            "authors": [
                {
                    "_id": "68f6e77a24c44893631117e9",
                    "user": {
                        "_id": "68f6e73db0f82456fc365d57",
                        "avatarUrl": "/avatars/c3a21b7f28c2eca553cc5da8d0b4fd67.svg",
                        "isPro": false,
                        "fullname": "Henry Lim Yu De",
                        "user": "Henrddy211",
                        "type": "user"
                    },
                    "name": "Henry Lim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:04:16.335Z",
                    "hidden": false
                },
                {
                    "_id": "68f6e77a24c44893631117ea",
                    "name": "Kwan Hui Lim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T10:26:26.000Z",
            "submittedOnDailyAt": "2025-10-22T23:50:05.774Z",
            "title": "The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple,\n  Self-Contained Directives",
            "submittedOnDailyBy": {
                "_id": "68f6e73db0f82456fc365d57",
                "avatarUrl": "/avatars/c3a21b7f28c2eca553cc5da8d0b4fd67.svg",
                "isPro": false,
                "fullname": "Henry Lim Yu De",
                "user": "Henrddy211",
                "type": "user"
            },
            "summary": "Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot\nreasoning, yet their ability to execute simple, self-contained instructions\nremains underexplored, despite this being foundational to complex\ninstruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro\nbenchmarks, by systematically varying the format of option labels (alphabetic,\nnumeric, Roman) while keeping their meaning identical under four paradigms,\nnamely: (1) With explicit instructions, label changes cause large performance\nshifts (e.g., -30.45\\% for Roman vs. numeric), revealing instruction-format\nbias. (2) Without instructions, performance drops further (up to -10.84\\%) and\nlabel sensitivity intensifies, underscoring the role of explicit guidance. (3)\nWhen option contents are removed, models fail random-choice baselines except\nwith numeric labels, suggesting weak adherence to atomic directives. (4)\nThree-shot exemplars yield no significant gains in robustness or fidelity, and\ngeneration analyses show persistent label errors, especially for non-numeric\nformats. Across model sizes, larger LLMs achieve higher accuracy but remain\ninconsistent in instruction adherence. These results expose the insufficiencies\nof current instruction-tuning paradigms and highlight the need for evaluation\nmethods and training strategies that explicitly target atomic\ninstruction-following.",
            "upvotes": 1,
            "discussionId": "68f6e77a24c44893631117eb",
            "ai_summary": "Evaluation of instruction-tuned large language models on modified MMLU and MMLU-Pro benchmarks reveals significant instruction-format bias and highlights the need for improved atomic instruction-following.",
            "ai_keywords": [
                "instruction-tuned large language models",
                "zero-shot reasoning",
                "MMLU",
                "MMLU-Pro",
                "option labels",
                "instruction-format bias",
                "explicit instructions",
                "three-shot exemplars",
                "atomic instruction-following"
            ]
        },
        "publishedAt": "2025-10-20T06:26:26.000Z",
        "title": "The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple,\n  Self-Contained Directives",
        "summary": "Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot\nreasoning, yet their ability to execute simple, self-contained instructions\nremains underexplored, despite this being foundational to complex\ninstruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro\nbenchmarks, by systematically varying the format of option labels (alphabetic,\nnumeric, Roman) while keeping their meaning identical under four paradigms,\nnamely: (1) With explicit instructions, label changes cause large performance\nshifts (e.g., -30.45\\% for Roman vs. numeric), revealing instruction-format\nbias. (2) Without instructions, performance drops further (up to -10.84\\%) and\nlabel sensitivity intensifies, underscoring the role of explicit guidance. (3)\nWhen option contents are removed, models fail random-choice baselines except\nwith numeric labels, suggesting weak adherence to atomic directives. (4)\nThree-shot exemplars yield no significant gains in robustness or fidelity, and\ngeneration analyses show persistent label errors, especially for non-numeric\nformats. Across model sizes, larger LLMs achieve higher accuracy but remain\ninconsistent in instruction adherence. These results expose the insufficiencies\nof current instruction-tuning paradigms and highlight the need for evaluation\nmethods and training strategies that explicitly target atomic\ninstruction-following.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17388.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68f6e73db0f82456fc365d57",
            "avatarUrl": "/avatars/c3a21b7f28c2eca553cc5da8d0b4fd67.svg",
            "fullname": "Henry Lim Yu De",
            "name": "Henrddy211",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.15136",
            "authors": [
                {
                    "_id": "68f7d9ff7669bcaeecce0b3b",
                    "user": {
                        "_id": "6732b04ed7e101ba37feaf2f",
                        "avatarUrl": "/avatars/dc2f871a30e0fba47728f30aa683dd20.svg",
                        "isPro": false,
                        "fullname": "Oluwasegun Adegoke",
                        "user": "Davidavid4",
                        "type": "user"
                    },
                    "name": "Oluwasegun Adegoke",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:44:44.122Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6732b04ed7e101ba37feaf2f/6QsA3U8j_q8fh4eT-7HdJ.png"
            ],
            "publishedAt": "2025-10-16T20:53:43.000Z",
            "submittedOnDailyAt": "2025-10-22T19:07:30.235Z",
            "title": "Predicting the Unpredictable: Reproducible BiLSTM Forecasting of\n  Incident Counts in the Global Terrorism Database (GTD)",
            "submittedOnDailyBy": {
                "_id": "6732b04ed7e101ba37feaf2f",
                "avatarUrl": "/avatars/dc2f871a30e0fba47728f30aa683dd20.svg",
                "isPro": false,
                "fullname": "Oluwasegun Adegoke",
                "user": "Davidavid4",
                "type": "user"
            },
            "summary": "We study short-horizon forecasting of weekly terrorism incident counts using\nthe Global Terrorism Database (GTD, 1970--2016). We build a reproducible\npipeline with fixed time-based splits and evaluate a Bidirectional LSTM\n(BiLSTM) against strong classical anchors (seasonal-naive, linear/ARIMA) and a\ndeep LSTM-Attention baseline. On the held-out test set, the BiLSTM attains RMSE\n6.38, outperforming LSTM-Attention (9.19; +30.6\\%) and a linear lag-regression\nbaseline (+35.4\\% RMSE gain), with parallel improvements in MAE and MAPE.\nAblations varying temporal memory, training-history length, spatial grain,\nlookback size, and feature groups show that models trained on long historical\ndata generalize best; a moderate lookback (20--30 weeks) provides strong\ncontext; and bidirectional encoding is critical for capturing both build-up and\naftermath patterns within the window. Feature-group analysis indicates that\nshort-horizon structure (lagged counts and rolling statistics) contributes\nmost, with geographic and casualty features adding incremental lift. We release\ncode, configs, and compact result tables, and provide a data/ethics statement\ndocumenting GTD licensing and research-only use. Overall, the study offers a\ntransparent, baseline-beating reference for GTD incident forecasting.",
            "upvotes": 1,
            "discussionId": "68f7d9ff7669bcaeecce0b3c",
            "githubRepo": "https://github.com/Davidavid45/Deep-Learning-in-Counterterrorism",
            "ai_summary": "A Bidirectional LSTM outperforms classical and deep learning baselines in forecasting weekly terrorism incidents using the Global Terrorism Database.",
            "ai_keywords": [
                "Bidirectional LSTM",
                "BiLSTM",
                "LSTM-Attention",
                "RMSE",
                "MAE",
                "MAPE",
                "temporal memory",
                "training-history length",
                "spatial grain",
                "lookback size",
                "feature groups",
                "lagged counts",
                "rolling statistics",
                "geographic features",
                "casualty features"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-10-16T16:53:43.000Z",
        "title": "Predicting the Unpredictable: Reproducible BiLSTM Forecasting of\n  Incident Counts in the Global Terrorism Database (GTD)",
        "summary": "We study short-horizon forecasting of weekly terrorism incident counts using\nthe Global Terrorism Database (GTD, 1970--2016). We build a reproducible\npipeline with fixed time-based splits and evaluate a Bidirectional LSTM\n(BiLSTM) against strong classical anchors (seasonal-naive, linear/ARIMA) and a\ndeep LSTM-Attention baseline. On the held-out test set, the BiLSTM attains RMSE\n6.38, outperforming LSTM-Attention (9.19; +30.6\\%) and a linear lag-regression\nbaseline (+35.4\\% RMSE gain), with parallel improvements in MAE and MAPE.\nAblations varying temporal memory, training-history length, spatial grain,\nlookback size, and feature groups show that models trained on long historical\ndata generalize best; a moderate lookback (20--30 weeks) provides strong\ncontext; and bidirectional encoding is critical for capturing both build-up and\naftermath patterns within the window. Feature-group analysis indicates that\nshort-horizon structure (lagged counts and rolling statistics) contributes\nmost, with geographic and casualty features adding incremental lift. We release\ncode, configs, and compact result tables, and provide a data/ethics statement\ndocumenting GTD licensing and research-only use. Overall, the study offers a\ntransparent, baseline-beating reference for GTD incident forecasting.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6732b04ed7e101ba37feaf2f/6QsA3U8j_q8fh4eT-7HdJ.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15136.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6732b04ed7e101ba37feaf2f",
            "avatarUrl": "/avatars/dc2f871a30e0fba47728f30aa683dd20.svg",
            "fullname": "Oluwasegun Adegoke",
            "name": "Davidavid4",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.13982",
            "authors": [
                {
                    "_id": "68f9026fb9b2e4ae04673652",
                    "name": "Jinkun Chen",
                    "hidden": false
                },
                {
                    "_id": "68f9026fb9b2e4ae04673653",
                    "name": "Sher Badshah",
                    "hidden": false
                },
                {
                    "_id": "68f9026fb9b2e4ae04673654",
                    "name": "Xuemin Yu",
                    "hidden": false
                },
                {
                    "_id": "68f9026fb9b2e4ae04673655",
                    "name": "Sijia Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T18:05:06.000Z",
            "submittedOnDailyAt": "2025-10-22T15:18:52.803Z",
            "title": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires\n  Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations",
            "submittedOnDailyBy": {
                "_id": "6331dcb29e3604f3f18382f3",
                "avatarUrl": "/avatars/64d08cfce64a9efda0896f7359e30281.svg",
                "isPro": false,
                "fullname": "Jinkun Chen",
                "user": "Jinnkunn",
                "type": "user"
            },
            "summary": "What if artificial agents could not just communicate, but also evolve, adapt,\nand reshape their worlds in ways we cannot fully predict? With llm now powering\nmulti-agent systems and social simulations, we are witnessing new possibilities\nfor modeling open-ended, ever-changing environments. Yet, most current\nsimulations remain constrained within static sandboxes, characterized by\npredefined tasks, limited dynamics, and rigid evaluation criteria. These\nlimitations prevent them from capturing the complexity of real-world societies.\nIn this paper, we argue that static, task-specific benchmarks are fundamentally\ninadequate and must be rethought. We critically review emerging architectures\nthat blend llm with multi-agent dynamics, highlight key hurdles such as\nbalancing stability and diversity, evaluating unexpected behaviors, and scaling\nto greater complexity, and introduce a fresh taxonomy for this rapidly evolving\nfield. Finally, we present a research roadmap centered on open-endedness,\ncontinuous co-evolution, and the development of resilient, socially aligned AI\necosystems. We call on the community to move beyond static paradigms and help\nshape the next generation of adaptive, socially-aware multi-agent simulations.",
            "upvotes": 1,
            "discussionId": "68f9026fb9b2e4ae04673656",
            "ai_summary": "Emerging architectures combining LLMs with multi-agent dynamics offer new possibilities for modeling complex, open-ended environments, but require addressing challenges like stability, diversity, and scalability.",
            "ai_keywords": [
                "LLMs",
                "multi-agent systems",
                "social simulations",
                "open-ended environments",
                "static sandboxes",
                "predefined tasks",
                "limited dynamics",
                "rigid evaluation criteria",
                "real-world societies",
                "static benchmarks",
                "task-specific benchmarks",
                "stability",
                "diversity",
                "unexpected behaviors",
                "scaling",
                "open-endedness",
                "continuous co-evolution",
                "resilient AI ecosystems",
                "socially aligned AI"
            ]
        },
        "publishedAt": "2025-10-15T14:05:06.000Z",
        "title": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires\n  Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations",
        "summary": "What if artificial agents could not just communicate, but also evolve, adapt,\nand reshape their worlds in ways we cannot fully predict? With llm now powering\nmulti-agent systems and social simulations, we are witnessing new possibilities\nfor modeling open-ended, ever-changing environments. Yet, most current\nsimulations remain constrained within static sandboxes, characterized by\npredefined tasks, limited dynamics, and rigid evaluation criteria. These\nlimitations prevent them from capturing the complexity of real-world societies.\nIn this paper, we argue that static, task-specific benchmarks are fundamentally\ninadequate and must be rethought. We critically review emerging architectures\nthat blend llm with multi-agent dynamics, highlight key hurdles such as\nbalancing stability and diversity, evaluating unexpected behaviors, and scaling\nto greater complexity, and introduce a fresh taxonomy for this rapidly evolving\nfield. Finally, we present a research roadmap centered on open-endedness,\ncontinuous co-evolution, and the development of resilient, socially aligned AI\necosystems. We call on the community to move beyond static paradigms and help\nshape the next generation of adaptive, socially-aware multi-agent simulations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13982.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6331dcb29e3604f3f18382f3",
            "avatarUrl": "/avatars/64d08cfce64a9efda0896f7359e30281.svg",
            "fullname": "Jinkun Chen",
            "name": "Jinnkunn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.17928",
            "authors": [
                {
                    "_id": "68f84e447669bcaeecce0d5f",
                    "user": {
                        "_id": "64ec5ffec782d648d2909c68",
                        "avatarUrl": "/avatars/297f61af080206d88d68f18fd96c9aa3.svg",
                        "isPro": false,
                        "fullname": "duhe",
                        "user": "Elynden",
                        "type": "user"
                    },
                    "name": "He Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:43:32.696Z",
                    "hidden": false
                },
                {
                    "_id": "68f84e447669bcaeecce0d60",
                    "name": "Bowen Li",
                    "hidden": false
                },
                {
                    "_id": "68f84e447669bcaeecce0d61",
                    "name": "Aijun Yang",
                    "hidden": false
                },
                {
                    "_id": "68f84e447669bcaeecce0d62",
                    "name": "Siyang He",
                    "hidden": false
                },
                {
                    "_id": "68f84e447669bcaeecce0d63",
                    "name": "Qipeng Guo",
                    "hidden": false
                },
                {
                    "_id": "68f84e447669bcaeecce0d64",
                    "name": "Dacheng Tao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T11:56:35.000Z",
            "submittedOnDailyAt": "2025-10-22T04:13:19.380Z",
            "title": "EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable\n  Learning",
            "submittedOnDailyBy": {
                "_id": "64ec5ffec782d648d2909c68",
                "avatarUrl": "/avatars/297f61af080206d88d68f18fd96c9aa3.svg",
                "isPro": false,
                "fullname": "duhe",
                "user": "Elynden",
                "type": "user"
            },
            "summary": "Reliable verifiable data has become a key driver of capability gains in\nmodern language models, enabling stable reinforcement learning with verifiable\nrewards and effective distillation that transfers competence across math,\ncoding, and agentic tasks. Yet constructing generalizable synthetic verifiable\ndata remains difficult due to hallucination-prone generation, and weak or\ntrivial verification artifacts that fail to separate strong from weak\nsolutions. Existing approaches often rely on task-specific heuristics or\npost-hoc filters that do not transfer across domains and lack a principled,\nuniversal evaluator of verifiability. In this work, we introduce an\nevolutionary, task-agnostic, strategy-guided, executably-checkable data\nsynthesis framework that, from minimal seed supervision, jointly synthesizes\nproblems, diverse candidate solutions, and verification artifacts, and\niteratively discovers strategies via a consistency-based evaluator that\nenforces agreement between human-annotated and strategy-induced checks. This\npipeline upgrades filtering into principled synthesis: it reliably assembles\ncoherent, verifiable training instances and generalizes without domain-specific\nrules. Our experiments demonstrate the effectiveness of the proposed approach\nunder both RLVR and model distillation training paradigms. The results show\nthat training with our synthesized data yields significant improvements on both\nthe LiveCodeBench and AgentBench-OS tasks, highlighting the robust\ngeneralization of our framework.",
            "upvotes": 0,
            "discussionId": "68f84e447669bcaeecce0d65",
            "ai_summary": "An evolutionary framework synthesizes verifiable data for language models, improving reinforcement learning and distillation across various tasks.",
            "ai_keywords": [
                "reinforcement learning",
                "verifiable rewards",
                "distillation",
                "synthetic verifiable data",
                "hallucination-prone generation",
                "verification artifacts",
                "task-agnostic",
                "strategy-guided",
                "executably-checkable",
                "consistency-based evaluator",
                "RLVR",
                "LiveCodeBench",
                "AgentBench-OS"
            ]
        },
        "publishedAt": "2025-10-20T07:56:35.000Z",
        "title": "EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable\n  Learning",
        "summary": "Reliable verifiable data has become a key driver of capability gains in\nmodern language models, enabling stable reinforcement learning with verifiable\nrewards and effective distillation that transfers competence across math,\ncoding, and agentic tasks. Yet constructing generalizable synthetic verifiable\ndata remains difficult due to hallucination-prone generation, and weak or\ntrivial verification artifacts that fail to separate strong from weak\nsolutions. Existing approaches often rely on task-specific heuristics or\npost-hoc filters that do not transfer across domains and lack a principled,\nuniversal evaluator of verifiability. In this work, we introduce an\nevolutionary, task-agnostic, strategy-guided, executably-checkable data\nsynthesis framework that, from minimal seed supervision, jointly synthesizes\nproblems, diverse candidate solutions, and verification artifacts, and\niteratively discovers strategies via a consistency-based evaluator that\nenforces agreement between human-annotated and strategy-induced checks. This\npipeline upgrades filtering into principled synthesis: it reliably assembles\ncoherent, verifiable training instances and generalizes without domain-specific\nrules. Our experiments demonstrate the effectiveness of the proposed approach\nunder both RLVR and model distillation training paradigms. The results show\nthat training with our synthesized data yields significant improvements on both\nthe LiveCodeBench and AgentBench-OS tasks, highlighting the robust\ngeneralization of our framework.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17928.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ec5ffec782d648d2909c68",
            "avatarUrl": "/avatars/297f61af080206d88d68f18fd96c9aa3.svg",
            "fullname": "duhe",
            "name": "Elynden",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.15862",
            "authors": [
                {
                    "_id": "68f6c9a224c44893631116d4",
                    "name": "Yi Wan",
                    "hidden": false
                },
                {
                    "_id": "68f6c9a224c44893631116d5",
                    "name": "Jiuqi Wang",
                    "hidden": false
                },
                {
                    "_id": "68f6c9a224c44893631116d6",
                    "name": "Liam Li",
                    "hidden": false
                },
                {
                    "_id": "68f6c9a224c44893631116d7",
                    "name": "Jinsong Liu",
                    "hidden": false
                },
                {
                    "_id": "68f6c9a224c44893631116d8",
                    "name": "Ruihao Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f6c9a224c44893631116d9",
                    "name": "Zheqing Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T17:53:06.000Z",
            "submittedOnDailyAt": "2025-10-22T09:47:17.662Z",
            "title": "PokeeResearch: Effective Deep Research via Reinforcement Learning from\n  AI Feedback and Robust Reasoning Scaffold",
            "submittedOnDailyBy": {
                "_id": "64ea85fa2f70f2a4c75e5bff",
                "avatarUrl": "/avatars/48841e49d0e1d9a907019c31454245e6.svg",
                "isPro": false,
                "fullname": "Zheqing Zhu",
                "user": "billmatrix",
                "type": "user"
            },
            "summary": "Tool-augmented large language models (LLMs) are emerging as deep research\nagents, systems that decompose complex queries, retrieve external evidence, and\nsynthesize grounded responses. Yet current agents remain limited by shallow\nretrieval, weak alignment metrics, and brittle tool-use behavior. We introduce\nPokeeResearch-7B, a 7B-parameter deep research agent built under a unified\nreinforcement learning framework for robustness, alignment, and scalability.\nPokeeResearch-7B is trained by an annotation-free Reinforcement Learning from\nAI Feedback (RLAIF) framework to optimize policies using LLM-based reward\nsignals that capture factual accuracy, citation faithfulness, and instruction\nadherence. A chain-of-thought-driven multi-call reasoning scaffold further\nenhances robustness through self-verification and adaptive recovery from tool\nfailures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves\nstate-of-the-art performance among 7B-scale deep research agents. This\nhighlights that careful reinforcement learning and reasoning design can produce\nefficient, resilient, and research-grade AI agents. The model and inference\ncode is open-sourced under MIT license at\nhttps://github.com/Pokee-AI/PokeeResearchOSS.",
            "upvotes": 0,
            "discussionId": "68f6c9a224c44893631116da",
            "githubRepo": "https://github.com/Pokee-AI/PokeeResearchOSS",
            "ai_summary": "PokeeResearch-7B, a 7B-parameter deep research agent, achieves state-of-the-art performance using reinforcement learning and chain-of-thought reasoning to enhance robustness and alignment.",
            "ai_keywords": [
                "deep research agents",
                "reinforcement learning",
                "annotation-free Reinforcement Learning from AI Feedback (RLAIF)",
                "LLM-based reward signals",
                "chain-of-thought-driven reasoning",
                "self-verification",
                "adaptive recovery",
                "deep research benchmarks"
            ],
            "githubStars": 39,
            "organization": {
                "_id": "68e97b8f1b306d9020afe847",
                "name": "PokeeAI",
                "fullname": "Pokee AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ea85fa2f70f2a4c75e5bff/PRdfmd18tCbW-aT0NPZ4f.png"
            }
        },
        "publishedAt": "2025-10-17T13:53:06.000Z",
        "title": "PokeeResearch: Effective Deep Research via Reinforcement Learning from\n  AI Feedback and Robust Reasoning Scaffold",
        "summary": "Tool-augmented large language models (LLMs) are emerging as deep research\nagents, systems that decompose complex queries, retrieve external evidence, and\nsynthesize grounded responses. Yet current agents remain limited by shallow\nretrieval, weak alignment metrics, and brittle tool-use behavior. We introduce\nPokeeResearch-7B, a 7B-parameter deep research agent built under a unified\nreinforcement learning framework for robustness, alignment, and scalability.\nPokeeResearch-7B is trained by an annotation-free Reinforcement Learning from\nAI Feedback (RLAIF) framework to optimize policies using LLM-based reward\nsignals that capture factual accuracy, citation faithfulness, and instruction\nadherence. A chain-of-thought-driven multi-call reasoning scaffold further\nenhances robustness through self-verification and adaptive recovery from tool\nfailures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves\nstate-of-the-art performance among 7B-scale deep research agents. This\nhighlights that careful reinforcement learning and reasoning design can produce\nefficient, resilient, and research-grade AI agents. The model and inference\ncode is open-sourced under MIT license at\nhttps://github.com/Pokee-AI/PokeeResearchOSS.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15862.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ea85fa2f70f2a4c75e5bff",
            "avatarUrl": "/avatars/48841e49d0e1d9a907019c31454245e6.svg",
            "fullname": "Zheqing Zhu",
            "name": "billmatrix",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "68e97b8f1b306d9020afe847",
            "name": "PokeeAI",
            "fullname": "Pokee AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ea85fa2f70f2a4c75e5bff/PRdfmd18tCbW-aT0NPZ4f.png"
        },
        "isAuthorParticipating": false
    }
]