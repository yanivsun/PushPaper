[
    {
        "paper": {
            "id": "2510.19338",
            "authors": [
                {
                    "_id": "68f98d4fb9b2e4ae046737dc",
                    "name": "Ling Team",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737dd",
                    "name": "Bin Han",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737de",
                    "name": "Caizhi Tang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737df",
                    "name": "Chen Liang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e0",
                    "name": "Donghao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e1",
                    "name": "Fan Yuan",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e2",
                    "name": "Feng Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e3",
                    "name": "Jie Gao",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e4",
                    "user": {
                        "_id": "64993d522ca6f96c8b8988ca",
                        "avatarUrl": "/avatars/0a8087cbbbcf5772288fe60c2c47eadb.svg",
                        "isPro": false,
                        "fullname": "Jingyu, Hu",
                        "user": "hjyai94",
                        "type": "user"
                    },
                    "name": "Jingyu Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:49:07.705Z",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e5",
                    "user": {
                        "_id": "641c10cdbfe1bb9c4bce652f",
                        "avatarUrl": "/avatars/7ed954de541ac5bc80f58fb5db3866ea.svg",
                        "isPro": false,
                        "fullname": "longfei li",
                        "user": "long0x0",
                        "type": "user"
                    },
                    "name": "Longfei Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:49:12.772Z",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e6",
                    "name": "Meng Li",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e7",
                    "name": "Mingyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e8",
                    "name": "Peijie Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e9",
                    "name": "Peng Jiao",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737ea",
                    "user": {
                        "_id": "5fde26773930f07f74aaf912",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fde26773930f07f74aaf912/SD93SVyVCRcTJbogBcf9J.jpeg",
                        "isPro": false,
                        "fullname": "Qian Zhao",
                        "user": "im0qianqian",
                        "type": "user"
                    },
                    "name": "Qian Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:48:55.104Z",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737eb",
                    "name": "Qingyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737ec",
                    "name": "Wenbo Shen",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737ed",
                    "name": "Xinxing Yang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737ee",
                    "name": "Yalin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737ef",
                    "name": "Yankun Ren",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737f0",
                    "name": "Yao Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737f1",
                    "name": "Yibo Cao",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737f2",
                    "name": "Yixuan Sun",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737f3",
                    "user": {
                        "_id": "6430d8ad440a10209803d52f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6430d8ad440a10209803d52f/BDU_9ou3flPge_uwG_3b8.jpeg",
                        "isPro": false,
                        "fullname": "Yue Zhang",
                        "user": "York-Z",
                        "type": "user"
                    },
                    "name": "Yue Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:49:02.156Z",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737f4",
                    "name": "Yuchen Fang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737f5",
                    "name": "Zibin Lin",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737f6",
                    "name": "Zixuan Cheng",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737f7",
                    "name": "Jun Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T07:59:38.000Z",
            "submittedOnDailyAt": "2025-10-23T00:35:16.243Z",
            "title": "Every Attention Matters: An Efficient Hybrid Architecture for\n  Long-Context Reasoning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "In this technical report, we present the Ring-linear model series,\nspecifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0.\nRing-mini-linear-2.0 comprises 16B parameters and 957M activations, while\nRing-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both\nmodels adopt a hybrid architecture that effectively integrates linear attention\nand softmax attention, significantly reducing I/O and computational overhead in\nlong-context inference scenarios. Compared to a 32 billion parameter dense\nmodel, this series reduces inference cost to 1/10, and compared to the original\nRing series, the cost is also reduced by over 50%. Furthermore, through\nsystematic exploration of the ratio between different attention mechanisms in\nthe hybrid architecture, we have identified the currently optimal model\nstructure. Additionally, by leveraging our self-developed high-performance FP8\noperator library-linghe, overall training efficiency has been improved by 50%.\nBenefiting from the high alignment between the training and inference engine\noperators, the models can undergo long-term, stable, and highly efficient\noptimization during the reinforcement learning phase, consistently maintaining\nSOTA performance across multiple challenging complex reasoning benchmarks.",
            "upvotes": 78,
            "discussionId": "68f98d50b9b2e4ae046737f8",
            "ai_summary": "The Ring-linear model series, including Ring-mini-linear-2.0 and Ring-flash-linear-2.0, uses a hybrid architecture combining linear and softmax attention to reduce inference costs and improve training efficiency.",
            "ai_keywords": [
                "linear attention",
                "softmax attention",
                "hybrid architecture",
                "FP8 operator library",
                "reinforcement learning",
                "SOTA performance"
            ],
            "organization": {
                "_id": "67c1d682826160b28f778510",
                "name": "antgroup",
                "fullname": "Ant Group",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
            }
        },
        "publishedAt": "2025-10-22T03:59:38.000Z",
        "title": "Every Attention Matters: An Efficient Hybrid Architecture for\n  Long-Context Reasoning",
        "summary": "In this technical report, we present the Ring-linear model series,\nspecifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0.\nRing-mini-linear-2.0 comprises 16B parameters and 957M activations, while\nRing-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both\nmodels adopt a hybrid architecture that effectively integrates linear attention\nand softmax attention, significantly reducing I/O and computational overhead in\nlong-context inference scenarios. Compared to a 32 billion parameter dense\nmodel, this series reduces inference cost to 1/10, and compared to the original\nRing series, the cost is also reduced by over 50%. Furthermore, through\nsystematic exploration of the ratio between different attention mechanisms in\nthe hybrid architecture, we have identified the currently optimal model\nstructure. Additionally, by leveraging our self-developed high-performance FP8\noperator library-linghe, overall training efficiency has been improved by 50%.\nBenefiting from the high alignment between the training and inference engine\noperators, the models can undergo long-term, stable, and highly efficient\noptimization during the reinforcement learning phase, consistently maintaining\nSOTA performance across multiple challenging complex reasoning benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19338.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 141
        },
        "organization": {
            "_id": "67c1d682826160b28f778510",
            "name": "antgroup",
            "fullname": "Ant Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.18927",
            "authors": [
                {
                    "_id": "68f9a187b9b2e4ae04673870",
                    "name": "Zhiheng Xi",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673871",
                    "name": "Xin Guo",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673872",
                    "name": "Yang Nan",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673873",
                    "name": "Enyu Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673874",
                    "user": {
                        "_id": "68f9b5fa0313464f8c8132d1",
                        "avatarUrl": "/avatars/2bb01b4ef391140daa8299f6a55c19a3.svg",
                        "isPro": false,
                        "fullname": "Shen",
                        "user": "Vindicaters",
                        "type": "user"
                    },
                    "name": "Junrui Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:47:23.670Z",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673875",
                    "name": "Wenxiang Chen",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673876",
                    "name": "Jiaqi Liu",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673877",
                    "name": "Jixuan Huang",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673878",
                    "name": "Zhihao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673879",
                    "name": "Honglin Guo",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae0467387a",
                    "name": "Xun Deng",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae0467387b",
                    "name": "Zhikai Lei",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae0467387c",
                    "name": "Miao Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae0467387d",
                    "name": "Guoteng Wang",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae0467387e",
                    "name": "Shuo Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae0467387f",
                    "name": "Peng Sun",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673880",
                    "name": "Rui Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673881",
                    "name": "Hang Yan",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673882",
                    "name": "Tao Gui",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673883",
                    "name": "Qi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673884",
                    "name": "Xuanjing Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T12:55:04.000Z",
            "submittedOnDailyAt": "2025-10-23T02:07:16.010Z",
            "title": "BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via\n  Balanced Policy Optimization with Adaptive Clipping",
            "submittedOnDailyBy": {
                "_id": "653a6e5cae155b92bae77b74",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg",
                "isPro": false,
                "fullname": "Zhiheng Xi",
                "user": "WooooDyy",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) has recently become the core paradigm for\naligning and strengthening large language models (LLMs). Yet, applying RL in\noff-policy settings--where stale data from past policies are used for\ntraining--improves sample efficiency, but remains challenging: policy entropy\ndeclines sharply, optimization often becomes unstable and may even collapse.\nThrough theoretical and empirical analysis, we identify two key insights: (i)\nan imbalance in optimization, where negative-advantage samples dominate the\npolicy gradient, suppressing useful behaviors and risking gradient explosions;\nand (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping\nmechanism in PPO-like objectives systematically blocks entropy-increasing\nupdates, thereby driving the policy toward over-exploitation at the expense of\nexploration. Building on these insights, we propose BAlanced Policy\nOptimization with Adaptive Clipping (BAPO), a simple yet effective method that\ndynamically adjusts clipping bounds to adaptively re-balance positive and\nnegative contributions, preserve entropy, and stabilize RL optimization. Across\ndiverse off-policy scenarios--including sample replay and partial rollout--BAPO\nachieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025\nbenchmarks, our 7B BAPO model surpasses open-source counterparts such as\nSkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art\nresults among models of the same scale but also outperforms leading proprietary\nsystems like o3-mini and Gemini-2.5-Flash-Thinking.",
            "upvotes": 67,
            "discussionId": "68f9a187b9b2e4ae04673885",
            "projectPage": "https://github.com/WooooDyy/BAPO",
            "githubRepo": "https://github.com/WooooDyy/BAPO",
            "ai_summary": "BAlanced Policy Optimization with Adaptive Clipping (BAPO) addresses challenges in off-policy reinforcement learning by dynamically adjusting clipping bounds to improve sample efficiency, stability, and performance in large language models.",
            "ai_keywords": [
                "reinforcement learning",
                "large language models",
                "off-policy settings",
                "policy entropy",
                "policy gradient",
                "Entropy-Clip Rule",
                "PPO-like objectives",
                "gradient explosions",
                "sample replay",
                "partial rollout",
                "AIME 2024",
                "AIME 2025",
                "SkyWork-OR1-7B",
                "o3-mini",
                "Gemini-2.5-Flash-Thinking"
            ],
            "githubStars": 49
        },
        "publishedAt": "2025-10-21T08:55:04.000Z",
        "title": "BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via\n  Balanced Policy Optimization with Adaptive Clipping",
        "summary": "Reinforcement learning (RL) has recently become the core paradigm for\naligning and strengthening large language models (LLMs). Yet, applying RL in\noff-policy settings--where stale data from past policies are used for\ntraining--improves sample efficiency, but remains challenging: policy entropy\ndeclines sharply, optimization often becomes unstable and may even collapse.\nThrough theoretical and empirical analysis, we identify two key insights: (i)\nan imbalance in optimization, where negative-advantage samples dominate the\npolicy gradient, suppressing useful behaviors and risking gradient explosions;\nand (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping\nmechanism in PPO-like objectives systematically blocks entropy-increasing\nupdates, thereby driving the policy toward over-exploitation at the expense of\nexploration. Building on these insights, we propose BAlanced Policy\nOptimization with Adaptive Clipping (BAPO), a simple yet effective method that\ndynamically adjusts clipping bounds to adaptively re-balance positive and\nnegative contributions, preserve entropy, and stabilize RL optimization. Across\ndiverse off-policy scenarios--including sample replay and partial rollout--BAPO\nachieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025\nbenchmarks, our 7B BAPO model surpasses open-source counterparts such as\nSkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art\nresults among models of the same scale but also outperforms leading proprietary\nsystems like o3-mini and Gemini-2.5-Flash-Thinking.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18927.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "653a6e5cae155b92bae77b74",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg",
            "fullname": "Zhiheng Xi",
            "name": "WooooDyy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.19363",
            "authors": [
                {
                    "_id": "68f9884db9b2e4ae0467374e",
                    "user": {
                        "_id": "6495b0b844bc2e9ce6cc849b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/j6aucl_tefMHwtD-bdUAw.jpeg",
                        "isPro": false,
                        "fullname": "Siyuan Wang",
                        "user": "OldKingMeister",
                        "type": "user"
                    },
                    "name": "Siyuan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:51:12.876Z",
                    "hidden": false
                },
                {
                    "_id": "68f9884db9b2e4ae0467374f",
                    "name": "Gaokai Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f9884db9b2e4ae04673750",
                    "name": "Li Lyna Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f9884db9b2e4ae04673751",
                    "name": "Ning Shang",
                    "hidden": false
                },
                {
                    "_id": "68f9884db9b2e4ae04673752",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "68f9884db9b2e4ae04673753",
                    "name": "Dongyao Chen",
                    "hidden": false
                },
                {
                    "_id": "68f9884db9b2e4ae04673754",
                    "name": "Mao Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T08:35:28.000Z",
            "submittedOnDailyAt": "2025-10-23T00:18:13.806Z",
            "title": "LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts",
            "submittedOnDailyBy": {
                "_id": "62b0009c72043b05d29492b2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
                "isPro": false,
                "fullname": "Li Lyna Zhang",
                "user": "lynazhang",
                "type": "user"
            },
            "summary": "Reasoning over long contexts is essential for large language models. While\nreinforcement learning (RL) enhances short-context reasoning by inducing \"Aha\"\nmoments in chain-of-thought, the advanced thinking patterns required for\nlong-context reasoning remain largely unexplored, and high-difficulty RL data\nare scarce. In this paper, we introduce LoongRL, a data-driven RL method for\nadvanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis\napproach that transforms short multi-hop QA into high-difficulty long-context\ntasks by inserting UUID chains that hide the true question among large\ncollections of distracting documents. Solving these tasks requires the model to\ntrace the correct chain step-by-step, identify the true question, retrieve\nrelevant facts and reason over them to answer correctly. RL training on\nKeyChain data induces an emergent plan-retrieve-reason-recheck reasoning\npattern that generalizes far beyond training length. Models trained at 16K\neffectively solve 128K tasks without prohibitive full-length RL rollout costs.\nOn Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA\naccuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches\na score of 74.2, rivaling much larger frontier models such as o3-mini (74.5)\nand DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all\n128K needle-in-a-haystack stress tests, and preserves short-context reasoning\ncapabilities.",
            "upvotes": 43,
            "discussionId": "68f9884db9b2e4ae04673755",
            "ai_summary": "LoongRL, a data-driven reinforcement learning method, enhances long-context reasoning by transforming short multi-hop QA into high-difficulty tasks, improving accuracy and generalization in large language models.",
            "ai_keywords": [
                "reinforcement learning",
                "LoongRL",
                "KeyChain",
                "multi-hop QA",
                "long-context reasoning",
                "plan-retrieve-reason-recheck",
                "RL rollout",
                "Qwen2.5-7B",
                "Qwen2.5-14B",
                "o3-mini",
                "DeepSeek-R1",
                "needle-in-a-haystack stress tests"
            ],
            "organization": {
                "_id": "68151d0f51add3813f3f7d1b",
                "name": "MicrosoftResearch",
                "fullname": "Microsoft Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
            }
        },
        "publishedAt": "2025-10-22T04:35:28.000Z",
        "title": "LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts",
        "summary": "Reasoning over long contexts is essential for large language models. While\nreinforcement learning (RL) enhances short-context reasoning by inducing \"Aha\"\nmoments in chain-of-thought, the advanced thinking patterns required for\nlong-context reasoning remain largely unexplored, and high-difficulty RL data\nare scarce. In this paper, we introduce LoongRL, a data-driven RL method for\nadvanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis\napproach that transforms short multi-hop QA into high-difficulty long-context\ntasks by inserting UUID chains that hide the true question among large\ncollections of distracting documents. Solving these tasks requires the model to\ntrace the correct chain step-by-step, identify the true question, retrieve\nrelevant facts and reason over them to answer correctly. RL training on\nKeyChain data induces an emergent plan-retrieve-reason-recheck reasoning\npattern that generalizes far beyond training length. Models trained at 16K\neffectively solve 128K tasks without prohibitive full-length RL rollout costs.\nOn Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA\naccuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches\na score of 74.2, rivaling much larger frontier models such as o3-mini (74.5)\nand DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all\n128K needle-in-a-haystack stress tests, and preserves short-context reasoning\ncapabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19363.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62b0009c72043b05d29492b2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
            "fullname": "Li Lyna Zhang",
            "name": "lynazhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 35
        },
        "organization": {
            "_id": "68151d0f51add3813f3f7d1b",
            "name": "MicrosoftResearch",
            "fullname": "Microsoft Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.15511",
            "authors": [
                {
                    "_id": "68f5fa388589920bf4d3225c",
                    "user": {
                        "_id": "66e56ad4c86016ecd972a9c5",
                        "avatarUrl": "/avatars/53ffcbe05c238d78ea227ed4e2b24b70.svg",
                        "isPro": false,
                        "fullname": "Giorgos Nikolaou",
                        "user": "giorgosnik02",
                        "type": "user"
                    },
                    "name": "Giorgos Nikolaou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T15:01:13.658Z",
                    "hidden": false
                },
                {
                    "_id": "68f5fa388589920bf4d3225d",
                    "user": {
                        "_id": "63ab16a6d7ee953f604ecd52",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ab16a6d7ee953f604ecd52/ujylOpczHKxU6Kfr-jGVr.png",
                        "isPro": false,
                        "fullname": "Tommaso Mencattini",
                        "user": "tmencatt",
                        "type": "user"
                    },
                    "name": "Tommaso Mencattini",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T01:07:34.245Z",
                    "hidden": false
                },
                {
                    "_id": "68f5fa388589920bf4d3225e",
                    "name": "Donato Crisostomi",
                    "hidden": false
                },
                {
                    "_id": "68f5fa388589920bf4d3225f",
                    "user": {
                        "_id": "5e8ef1f14957053f606489e6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
                        "isPro": false,
                        "fullname": "Andrea Santilli",
                        "user": "teelinsan",
                        "type": "user"
                    },
                    "name": "Andrea Santilli",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T15:01:19.541Z",
                    "hidden": false
                },
                {
                    "_id": "68f5fa388589920bf4d32260",
                    "name": "Yannis Panagakis",
                    "hidden": false
                },
                {
                    "_id": "68f5fa388589920bf4d32261",
                    "name": "Emanuele Rodola'",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T10:25:30.000Z",
            "submittedOnDailyAt": "2025-10-23T10:54:24.652Z",
            "title": "Language Models are Injective and Hence Invertible",
            "submittedOnDailyBy": {
                "_id": "63ab16a6d7ee953f604ecd52",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ab16a6d7ee953f604ecd52/ujylOpczHKxU6Kfr-jGVr.png",
                "isPro": false,
                "fullname": "Tommaso Mencattini",
                "user": "tmencatt",
                "type": "user"
            },
            "summary": "Transformer components such as non-linear activations and normalization are\ninherently non-injective, suggesting that different inputs could map to the\nsame output and prevent exact recovery of the input from a model's\nrepresentations. In this paper, we challenge this view. First, we prove\nmathematically that transformer language models mapping discrete input\nsequences to their corresponding sequence of continuous representations are\ninjective and therefore lossless, a property established at initialization and\npreserved during training. Second, we confirm this result empirically through\nbillions of collision tests on six state-of-the-art language models, and\nobserve no collisions. Third, we operationalize injectivity: we introduce\nSipIt, the first algorithm that provably and efficiently reconstructs the exact\ninput text from hidden activations, establishing linear-time guarantees and\ndemonstrating exact invertibility in practice. Overall, our work establishes\ninjectivity as a fundamental and exploitable property of language models, with\ndirect implications for transparency, interpretability, and safe deployment.",
            "upvotes": 42,
            "discussionId": "68f5fa388589920bf4d32262",
            "ai_summary": "Transformer language models are proven to be injective, allowing exact input reconstruction from hidden activations, which has implications for transparency and safety.",
            "ai_keywords": [
                "transformer components",
                "non-linear activations",
                "normalization",
                "injective",
                "language models",
                "continuous representations",
                "collision tests",
                "SipIt",
                "linear-time guarantees",
                "exact invertibility"
            ],
            "organization": {
                "_id": "6070204769a66931a0273ef7",
                "name": "gladia",
                "fullname": "Gladia Research Group",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1617961184086-5e8ef1f14957053f606489e6.png"
            }
        },
        "publishedAt": "2025-10-17T06:25:30.000Z",
        "title": "Language Models are Injective and Hence Invertible",
        "summary": "Transformer components such as non-linear activations and normalization are\ninherently non-injective, suggesting that different inputs could map to the\nsame output and prevent exact recovery of the input from a model's\nrepresentations. In this paper, we challenge this view. First, we prove\nmathematically that transformer language models mapping discrete input\nsequences to their corresponding sequence of continuous representations are\ninjective and therefore lossless, a property established at initialization and\npreserved during training. Second, we confirm this result empirically through\nbillions of collision tests on six state-of-the-art language models, and\nobserve no collisions. Third, we operationalize injectivity: we introduce\nSipIt, the first algorithm that provably and efficiently reconstructs the exact\ninput text from hidden activations, establishing linear-time guarantees and\ndemonstrating exact invertibility in practice. Overall, our work establishes\ninjectivity as a fundamental and exploitable property of language models, with\ndirect implications for transparency, interpretability, and safe deployment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15511.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63ab16a6d7ee953f604ecd52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ab16a6d7ee953f604ecd52/ujylOpczHKxU6Kfr-jGVr.png",
            "fullname": "Tommaso Mencattini",
            "name": "tmencatt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "6070204769a66931a0273ef7",
            "name": "gladia",
            "fullname": "Gladia Research Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1617961184086-5e8ef1f14957053f606489e6.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.15731",
            "authors": [
                {
                    "_id": "68f89d937669bcaeecce0f2f",
                    "user": {
                        "_id": "643986457245b3b7f453aec7",
                        "avatarUrl": "/avatars/37b3fb9a67de7674c898788999a12265.svg",
                        "isPro": false,
                        "fullname": "Maximo Rulli",
                        "user": "maximorulli",
                        "type": "user"
                    },
                    "name": "Maximo Eduardo Rulli",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:42:15.171Z",
                    "hidden": false
                },
                {
                    "_id": "68f89d937669bcaeecce0f30",
                    "name": "Simone Petruzzi",
                    "hidden": false
                },
                {
                    "_id": "68f89d937669bcaeecce0f31",
                    "name": "Edoardo Michielon",
                    "hidden": false
                },
                {
                    "_id": "68f89d937669bcaeecce0f32",
                    "name": "Fabrizio Silvestri",
                    "hidden": false
                },
                {
                    "_id": "68f89d937669bcaeecce0f33",
                    "name": "Simone Scardapane",
                    "hidden": false
                },
                {
                    "_id": "68f89d937669bcaeecce0f34",
                    "name": "Alessio Devoto",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/643986457245b3b7f453aec7/KoRDpre1sVGD2TXW-IYqT.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/643986457245b3b7f453aec7/h22QYQOvEuKU3GG9mWLFP.png",
                "https://cdn-uploads.huggingface.co/production/uploads/643986457245b3b7f453aec7/OxnCchZHPfFKbTsue4RMy.png"
            ],
            "publishedAt": "2025-10-17T15:23:58.000Z",
            "submittedOnDailyAt": "2025-10-23T04:07:43.805Z",
            "title": "Attention Sinks in Diffusion Language Models",
            "submittedOnDailyBy": {
                "_id": "643986457245b3b7f453aec7",
                "avatarUrl": "/avatars/37b3fb9a67de7674c898788999a12265.svg",
                "isPro": false,
                "fullname": "Maximo Rulli",
                "user": "maximorulli",
                "type": "user"
            },
            "summary": "Masked Diffusion Language Models (DLMs) have recently emerged as a promising\nalternative to traditional Autoregressive Models (ARMs). DLMs employ\ntransformer encoders with bidirectional attention, enabling parallel token\ngeneration while maintaining competitive performance. Although their efficiency\nand effectiveness have been extensively studied, the internal mechanisms that\ngovern DLMs remain largely unexplored. In this work, we conduct an empirical\nanalysis of DLM attention patterns, focusing on the attention sinking\nphenomenon, an effect previously observed in various transformer-based\narchitectures. Our findings reveal that DLMs also exhibit attention sinks, but\nwith distinct characteristics. First, unlike in ARMs, the sink positions in\nDLMs tend to shift throughout the generation process, displaying a dynamic\nbehaviour. Second, while ARMs are highly sensitive to the removal of attention\nsinks, DLMs remain robust: masking sinks leads to only a minor degradation in\nperformance. These results provide new insights into the inner workings of\ndiffusion-based language models and highlight fundamental differences in how\nthey allocate and utilize attention compared to autoregressive models.",
            "upvotes": 37,
            "discussionId": "68f89d947669bcaeecce0f35",
            "ai_summary": "Empirical analysis of Masked Diffusion Language Models (DLMs) reveals distinct attention sinking phenomena and robustness compared to Autoregressive Models (ARMs).",
            "ai_keywords": [
                "Masked Diffusion Language Models",
                "DLMs",
                "Autoregressive Models",
                "ARMs",
                "transformer encoders",
                "bidirectional attention",
                "parallel token generation",
                "attention sinking",
                "attention patterns",
                "dynamic behaviour",
                "performance degradation"
            ]
        },
        "publishedAt": "2025-10-17T11:23:58.000Z",
        "title": "Attention Sinks in Diffusion Language Models",
        "summary": "Masked Diffusion Language Models (DLMs) have recently emerged as a promising\nalternative to traditional Autoregressive Models (ARMs). DLMs employ\ntransformer encoders with bidirectional attention, enabling parallel token\ngeneration while maintaining competitive performance. Although their efficiency\nand effectiveness have been extensively studied, the internal mechanisms that\ngovern DLMs remain largely unexplored. In this work, we conduct an empirical\nanalysis of DLM attention patterns, focusing on the attention sinking\nphenomenon, an effect previously observed in various transformer-based\narchitectures. Our findings reveal that DLMs also exhibit attention sinks, but\nwith distinct characteristics. First, unlike in ARMs, the sink positions in\nDLMs tend to shift throughout the generation process, displaying a dynamic\nbehaviour. Second, while ARMs are highly sensitive to the removal of attention\nsinks, DLMs remain robust: masking sinks leads to only a minor degradation in\nperformance. These results provide new insights into the inner workings of\ndiffusion-based language models and highlight fundamental differences in how\nthey allocate and utilize attention compared to autoregressive models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/643986457245b3b7f453aec7/KoRDpre1sVGD2TXW-IYqT.gif",
            "https://cdn-uploads.huggingface.co/production/uploads/643986457245b3b7f453aec7/h22QYQOvEuKU3GG9mWLFP.png",
            "https://cdn-uploads.huggingface.co/production/uploads/643986457245b3b7f453aec7/OxnCchZHPfFKbTsue4RMy.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15731.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643986457245b3b7f453aec7",
            "avatarUrl": "/avatars/37b3fb9a67de7674c898788999a12265.svg",
            "fullname": "Maximo Rulli",
            "name": "maximorulli",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.19430",
            "authors": [
                {
                    "_id": "68f99e80b9b2e4ae04673853",
                    "name": "GigaBrain Team",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673854",
                    "name": "Angen Ye",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673855",
                    "name": "Boyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673856",
                    "name": "Chaojun Ni",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673857",
                    "name": "Guan Huang",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673858",
                    "name": "Guosheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673859",
                    "name": "Haoyun Li",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467385a",
                    "user": {
                        "_id": "679342914fa1841697cafac9",
                        "avatarUrl": "/avatars/217e8bd4fad85f486c4af83e29e12193.svg",
                        "isPro": false,
                        "fullname": "lijie",
                        "user": "danielleetrade",
                        "type": "user"
                    },
                    "name": "Jie Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:47:33.050Z",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467385b",
                    "name": "Jiagang Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467385c",
                    "name": "Lv Feng",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467385d",
                    "name": "Peng Li",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467385e",
                    "name": "Qiuping Deng",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467385f",
                    "name": "Runqi Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673860",
                    "name": "Wenkang Qin",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673861",
                    "name": "Xinze Chen",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673862",
                    "user": {
                        "_id": "6426616ea5ec4a5cbc535634",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6426616ea5ec4a5cbc535634/hH6JsxnXeakH3mBTeNNmO.jpeg",
                        "isPro": false,
                        "fullname": "JeffWang",
                        "user": "Jeff-Wang",
                        "type": "user"
                    },
                    "name": "Xiaofeng Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:47:40.222Z",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673863",
                    "name": "Yang Wang",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673864",
                    "name": "Yifan Li",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673865",
                    "name": "Yilong Li",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673866",
                    "name": "Yiran Ding",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673867",
                    "name": "Yuan Xu",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673868",
                    "name": "Yun Ye",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673869",
                    "user": {
                        "_id": "63ecdc6b54bba6781518cf48",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ecdc6b54bba6781518cf48/d5DrpcYnxjW0Z1BrjrftB.jpeg",
                        "isPro": false,
                        "fullname": "YuKun Zhou",
                        "user": "1zeryu",
                        "type": "user"
                    },
                    "name": "Yukun Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T01:07:22.528Z",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467386a",
                    "name": "Zhehao Dong",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467386b",
                    "name": "Zhenan Wang",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467386c",
                    "name": "Zhichao Liu",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467386d",
                    "name": "Zheng Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T09:57:13.000Z",
            "submittedOnDailyAt": "2025-10-23T03:11:44.473Z",
            "title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model",
            "submittedOnDailyBy": {
                "_id": "6426616ea5ec4a5cbc535634",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6426616ea5ec4a5cbc535634/hH6JsxnXeakH3mBTeNNmO.jpeg",
                "isPro": false,
                "fullname": "JeffWang",
                "user": "Jeff-Wang",
                "type": "user"
            },
            "summary": "Training Vision-Language-Action (VLA) models for generalist robots typically\nrequires large-scale real-world robot data, which is expensive and\ntime-consuming to collect. The inefficiency of physical data collection\nseverely limits the scalability, and generalization capacity of current VLA\nsystems. To address this challenge, we introduce GigaBrain-0, a novel VLA\nfoundation model empowered by world model-generated data (e.g., video\ngeneration, real2real transfer, human transfer, view transfer, sim2real\ntransfer data). By leveraging world models to generate diverse data at scale,\nGigaBrain-0 significantly reduces reliance on real robot data while improving\ncross-task generalization. Our approach further improves policy robustness\nthrough RGBD input modeling and embodied Chain-of-Thought (CoT) supervision,\nenabling the model to reason about spatial geometry, object states, and\nlong-horizon dependencies during task execution. This leads to substantial\ngains in real-world performance on dexterous, long-horizon, and mobile\nmanipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves\nsuperior generalization across variations in appearances (e.g., textures,\ncolors), object placements, and camera viewpoints. Additionally, we present\nGigaBrain-0-Small, an optimized lightweight variant designed to run efficiently\non devices such as the NVIDIA Jetson AGX Orin.",
            "upvotes": 33,
            "discussionId": "68f99e81b9b2e4ae0467386e",
            "projectPage": "https://gigabrain0.github.io/",
            "githubRepo": "https://github.com/open-gigaai/giga-brain-0",
            "ai_summary": "GigaBrain-0, a VLA foundation model, uses world model-generated data to enhance cross-task generalization and policy robustness, improving real-world performance on complex manipulation tasks.",
            "ai_keywords": [
                "VLA models",
                "world model-generated data",
                "video generation",
                "real2real transfer",
                "human transfer",
                "view transfer",
                "sim2real transfer",
                "RGBD input modeling",
                "Chain-of-Thought supervision",
                "spatial geometry",
                "object states",
                "long-horizon dependencies",
                "dexterous manipulation",
                "mobile manipulation"
            ],
            "githubStars": 21,
            "organization": {
                "_id": "68d6587936e2de9610d9f5f0",
                "name": "open-gigaai",
                "fullname": "GigaAI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68d6394328e169473e90e4a6/zUK7FKr_8XqrN0aFUgsD-.png"
            }
        },
        "publishedAt": "2025-10-22T05:57:13.000Z",
        "title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model",
        "summary": "Training Vision-Language-Action (VLA) models for generalist robots typically\nrequires large-scale real-world robot data, which is expensive and\ntime-consuming to collect. The inefficiency of physical data collection\nseverely limits the scalability, and generalization capacity of current VLA\nsystems. To address this challenge, we introduce GigaBrain-0, a novel VLA\nfoundation model empowered by world model-generated data (e.g., video\ngeneration, real2real transfer, human transfer, view transfer, sim2real\ntransfer data). By leveraging world models to generate diverse data at scale,\nGigaBrain-0 significantly reduces reliance on real robot data while improving\ncross-task generalization. Our approach further improves policy robustness\nthrough RGBD input modeling and embodied Chain-of-Thought (CoT) supervision,\nenabling the model to reason about spatial geometry, object states, and\nlong-horizon dependencies during task execution. This leads to substantial\ngains in real-world performance on dexterous, long-horizon, and mobile\nmanipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves\nsuperior generalization across variations in appearances (e.g., textures,\ncolors), object placements, and camera viewpoints. Additionally, we present\nGigaBrain-0-Small, an optimized lightweight variant designed to run efficiently\non devices such as the NVIDIA Jetson AGX Orin.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19430.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6426616ea5ec4a5cbc535634",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6426616ea5ec4a5cbc535634/hH6JsxnXeakH3mBTeNNmO.jpeg",
            "fullname": "JeffWang",
            "name": "Jeff-Wang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "68d6587936e2de9610d9f5f0",
            "name": "open-gigaai",
            "fullname": "GigaAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68d6394328e169473e90e4a6/zUK7FKr_8XqrN0aFUgsD-.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.19307",
            "authors": [
                {
                    "_id": "68f9929eb9b2e4ae04673817",
                    "user": {
                        "_id": "657152eb12f162153b50ec9d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
                        "isPro": false,
                        "fullname": "Byung-Kwan Lee",
                        "user": "BK-Lee",
                        "type": "user"
                    },
                    "name": "Byung-Kwan Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:48:24.236Z",
                    "hidden": false
                },
                {
                    "_id": "68f9929eb9b2e4ae04673818",
                    "name": "Ryo Hachiuma",
                    "hidden": false
                },
                {
                    "_id": "68f9929eb9b2e4ae04673819",
                    "name": "Yong Man Ro",
                    "hidden": false
                },
                {
                    "_id": "68f9929eb9b2e4ae0467381a",
                    "name": "Yu-Chiang Frank Wang",
                    "hidden": false
                },
                {
                    "_id": "68f9929eb9b2e4ae0467381b",
                    "name": "Yueh-Hua Wu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/657152eb12f162153b50ec9d/ur9tbO9Ie-KPIHBbzRIQ5.mp4"
            ],
            "publishedAt": "2025-10-22T07:12:14.000Z",
            "submittedOnDailyAt": "2025-10-23T01:01:49.909Z",
            "title": "Unified Reinforcement and Imitation Learning for Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "657152eb12f162153b50ec9d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
                "isPro": false,
                "fullname": "Byung-Kwan Lee",
                "user": "BK-Lee",
                "type": "user"
            },
            "summary": "Vision-Language Models (VLMs) have achieved remarkable progress, yet their\nlarge scale often renders them impractical for resource-constrained\nenvironments. This paper introduces Unified Reinforcement and Imitation\nLearning (RIL), a novel and efficient training algorithm designed to create\npowerful, lightweight VLMs. RIL distinctively combines the strengths of\nreinforcement learning with adversarial imitation learning. This enables\nsmaller student VLMs not only to mimic the sophisticated text generation of\nlarge teacher models but also to systematically improve their generative\ncapabilities through reinforcement signals. Key to our imitation framework is\nan LLM-based discriminator that adeptly distinguishes between student and\nteacher outputs, complemented by guidance from multiple large teacher VLMs to\nensure diverse learning. This unified learning strategy, leveraging both\nreinforcement and imitation, empowers student models to achieve significant\nperformance gains, making them competitive with leading closed-source VLMs.\nExtensive experiments on diverse vision-language benchmarks demonstrate that\nRIL significantly narrows the performance gap with state-of-the-art open- and\nclosed-source VLMs and, in several instances, surpasses them.",
            "upvotes": 19,
            "discussionId": "68f9929fb9b2e4ae0467381c",
            "projectPage": "https://byungkwanlee.github.io/RIL-page/",
            "ai_summary": "A unified reinforcement and imitation learning algorithm creates efficient, lightweight vision-language models that match or exceed leading VLMs in performance.",
            "ai_keywords": [
                "Unified Reinforcement and Imitation Learning (RIL)",
                "reinforcement learning",
                "adversarial imitation learning",
                "LLM-based discriminator",
                "vision-language models (VLMs)",
                "text generation",
                "generative capabilities",
                "vision-language benchmarks"
            ],
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-10-22T03:12:14.000Z",
        "title": "Unified Reinforcement and Imitation Learning for Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) have achieved remarkable progress, yet their\nlarge scale often renders them impractical for resource-constrained\nenvironments. This paper introduces Unified Reinforcement and Imitation\nLearning (RIL), a novel and efficient training algorithm designed to create\npowerful, lightweight VLMs. RIL distinctively combines the strengths of\nreinforcement learning with adversarial imitation learning. This enables\nsmaller student VLMs not only to mimic the sophisticated text generation of\nlarge teacher models but also to systematically improve their generative\ncapabilities through reinforcement signals. Key to our imitation framework is\nan LLM-based discriminator that adeptly distinguishes between student and\nteacher outputs, complemented by guidance from multiple large teacher VLMs to\nensure diverse learning. This unified learning strategy, leveraging both\nreinforcement and imitation, empowers student models to achieve significant\nperformance gains, making them competitive with leading closed-source VLMs.\nExtensive experiments on diverse vision-language benchmarks demonstrate that\nRIL significantly narrows the performance gap with state-of-the-art open- and\nclosed-source VLMs and, in several instances, surpasses them.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/657152eb12f162153b50ec9d/ur9tbO9Ie-KPIHBbzRIQ5.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19307.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "657152eb12f162153b50ec9d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
            "fullname": "Byung-Kwan Lee",
            "name": "BK-Lee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 61
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.19488",
            "authors": [
                {
                    "_id": "68f98a91b9b2e4ae0467376d",
                    "user": {
                        "_id": "669ca7e678115e16bdfc9bfc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669ca7e678115e16bdfc9bfc/pku8NvQKqfNQACRqm1YrW.jpeg",
                        "isPro": true,
                        "fullname": "Lu Dunjie",
                        "user": "ludunjie",
                        "type": "user"
                    },
                    "name": "Dunjie Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:50:39.730Z",
                    "hidden": false
                },
                {
                    "_id": "68f98a91b9b2e4ae0467376e",
                    "name": "Yiheng Xu",
                    "hidden": false
                },
                {
                    "_id": "68f98a91b9b2e4ae0467376f",
                    "name": "Junli Wang",
                    "hidden": false
                },
                {
                    "_id": "68f98a91b9b2e4ae04673770",
                    "name": "Haoyuan Wu",
                    "hidden": false
                },
                {
                    "_id": "68f98a91b9b2e4ae04673771",
                    "name": "Xinyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68f98a91b9b2e4ae04673772",
                    "name": "Zekun Wang",
                    "hidden": false
                },
                {
                    "_id": "68f98a91b9b2e4ae04673773",
                    "name": "Junlin Yang",
                    "hidden": false
                },
                {
                    "_id": "68f98a91b9b2e4ae04673774",
                    "name": "Hongjin Su",
                    "hidden": false
                },
                {
                    "_id": "68f98a91b9b2e4ae04673775",
                    "name": "Jixuan Chen",
                    "hidden": false
                },
                {
                    "_id": "68f98a91b9b2e4ae04673776",
                    "name": "Junda Chen",
                    "hidden": false
                },
                {
                    "_id": "68f98a91b9b2e4ae04673777",
                    "name": "Yuchen Mao",
                    "hidden": false
                },
                {
                    "_id": "68f98a91b9b2e4ae04673778",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f98a91b9b2e4ae04673779",
                    "name": "Junyang Lin",
                    "hidden": false
                },
                {
                    "_id": "68f98a91b9b2e4ae0467377a",
                    "user": {
                        "_id": "61e4c4ca1ab24785ac11ba69",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e4c4ca1ab24785ac11ba69/1Q1zhhyGSJ9RJG9MzwxVv.jpeg",
                        "isPro": false,
                        "fullname": "Binyuan Hui",
                        "user": "huybery",
                        "type": "user"
                    },
                    "name": "Binyuan Hui",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:50:33.160Z",
                    "hidden": false
                },
                {
                    "_id": "68f98a91b9b2e4ae0467377b",
                    "name": "Tao Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T11:25:48.000Z",
            "submittedOnDailyAt": "2025-10-23T00:23:34.212Z",
            "title": "VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Training computer-use agents requires massive amounts of GUI interaction\ndata, but manually annotating action trajectories at scale is prohibitively\nexpensive. We present VideoAgentTrek, a scalable pipeline that automatically\nmines training data from publicly available screen-recorded videos at web\nscale, eliminating the need for manual annotation. Our approach addresses a key\nchallenge: raw videos contain implicit demonstrations but lack explicit action\nlabels. To solve this, we develop Video2Action, an inverse dynamics module\n(IDM) with two components: (1) a video grounding model that detects and\nlocalizes GUI actions with precise temporal boundaries and context, and (2) an\naction-content recognizer that extracts structured parameters like click\ncoordinates and typed text with high fidelity. Applied to 39,000 YouTube\ntutorial videos, our pipeline generates 1.52 million interaction steps\nautomatically. We leverage this data through continued pretraining followed by\nsupervised fine-tuning. On OSWorld-Verified, our approach improves task success\nrates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On\nAgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results\ndemonstrate that passive internet videos can be transformed into high-quality\nsupervision for computer-use agents, providing a scalable alternative to\nexpensive manual annotation.",
            "upvotes": 17,
            "discussionId": "68f98a91b9b2e4ae0467377c",
            "projectPage": "https://videoagenttrek.github.io/",
            "ai_summary": "VideoAgentTrek automatically extracts GUI interaction data from YouTube videos using Video2Action, an inverse dynamics module, improving task success rates and step accuracy for computer-use agents.",
            "ai_keywords": [
                "inverse dynamics module",
                "video grounding model",
                "action-content recognizer",
                "continued pretraining",
                "supervised fine-tuning",
                "OSWorld-Verified",
                "AgentNetBench"
            ]
        },
        "publishedAt": "2025-10-22T07:25:48.000Z",
        "title": "VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos",
        "summary": "Training computer-use agents requires massive amounts of GUI interaction\ndata, but manually annotating action trajectories at scale is prohibitively\nexpensive. We present VideoAgentTrek, a scalable pipeline that automatically\nmines training data from publicly available screen-recorded videos at web\nscale, eliminating the need for manual annotation. Our approach addresses a key\nchallenge: raw videos contain implicit demonstrations but lack explicit action\nlabels. To solve this, we develop Video2Action, an inverse dynamics module\n(IDM) with two components: (1) a video grounding model that detects and\nlocalizes GUI actions with precise temporal boundaries and context, and (2) an\naction-content recognizer that extracts structured parameters like click\ncoordinates and typed text with high fidelity. Applied to 39,000 YouTube\ntutorial videos, our pipeline generates 1.52 million interaction steps\nautomatically. We leverage this data through continued pretraining followed by\nsupervised fine-tuning. On OSWorld-Verified, our approach improves task success\nrates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On\nAgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results\ndemonstrate that passive internet videos can be transformed into high-quality\nsupervision for computer-use agents, providing a scalable alternative to\nexpensive manual annotation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19488.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 141
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.19808",
            "authors": [
                {
                    "_id": "68f988deb9b2e4ae0467375c",
                    "name": "Yusu Qian",
                    "hidden": false
                },
                {
                    "_id": "68f988deb9b2e4ae0467375d",
                    "name": "Eli Bocek-Rivele",
                    "hidden": false
                },
                {
                    "_id": "68f988deb9b2e4ae0467375e",
                    "name": "Liangchen Song",
                    "hidden": false
                },
                {
                    "_id": "68f988deb9b2e4ae0467375f",
                    "name": "Jialing Tong",
                    "hidden": false
                },
                {
                    "_id": "68f988deb9b2e4ae04673760",
                    "name": "Yinfei Yang",
                    "hidden": false
                },
                {
                    "_id": "68f988deb9b2e4ae04673761",
                    "name": "Jiasen Lu",
                    "hidden": false
                },
                {
                    "_id": "68f988deb9b2e4ae04673762",
                    "name": "Wenze Hu",
                    "hidden": false
                },
                {
                    "_id": "68f988deb9b2e4ae04673763",
                    "name": "Zhe Gan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T17:43:15.000Z",
            "submittedOnDailyAt": "2025-10-23T00:16:18.374Z",
            "title": "Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent advances in multimodal models have demonstrated remarkable text-guided\nimage editing capabilities, with systems like GPT-4o and Nano-Banana setting\nnew benchmarks. However, the research community's progress remains constrained\nby the absence of large-scale, high-quality, and openly accessible datasets\nbuilt from real images. We introduce Pico-Banana-400K, a comprehensive\n400K-image dataset for instruction-based image editing. Our dataset is\nconstructed by leveraging Nano-Banana to generate diverse edit pairs from real\nphotographs in the OpenImages collection. What distinguishes Pico-Banana-400K\nfrom previous synthetic datasets is our systematic approach to quality and\ndiversity. We employ a fine-grained image editing taxonomy to ensure\ncomprehensive coverage of edit types while maintaining precise content\npreservation and instruction faithfulness through MLLM-based quality scoring\nand careful curation. Beyond single turn editing, Pico-Banana-400K enables\nresearch into complex editing scenarios. The dataset includes three specialized\nsubsets: (1) a 72K-example multi-turn collection for studying sequential\nediting, reasoning, and planning across consecutive modifications; (2) a\n56K-example preference subset for alignment research and reward model training;\nand (3) paired long-short editing instructions for developing instruction\nrewriting and summarization capabilities. By providing this large-scale,\nhigh-quality, and task-rich resource, Pico-Banana-400K establishes a robust\nfoundation for training and benchmarking the next generation of text-guided\nimage editing models.",
            "upvotes": 16,
            "discussionId": "68f988deb9b2e4ae04673764",
            "githubRepo": "https://github.com/apple/pico-banana-400k",
            "ai_summary": "Pico-Banana-400K is a large-scale, high-quality dataset for instruction-based image editing, featuring diverse edit pairs, multi-turn editing, preference subsets, and long-short instruction pairs, enabling comprehensive research and benchmarking.",
            "ai_keywords": [
                "multimodal models",
                "text-guided image editing",
                "GPT-4o",
                "Nano-Banana",
                "OpenImages",
                "fine-grained image editing taxonomy",
                "MLLM-based quality scoring",
                "multi-turn editing",
                "preference subset",
                "instruction rewriting",
                "instruction summarization"
            ],
            "githubStars": 104,
            "organization": {
                "_id": "628cbd99ef14f971b69948ab",
                "name": "apple",
                "fullname": "Apple",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
            }
        },
        "publishedAt": "2025-10-22T13:43:15.000Z",
        "title": "Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing",
        "summary": "Recent advances in multimodal models have demonstrated remarkable text-guided\nimage editing capabilities, with systems like GPT-4o and Nano-Banana setting\nnew benchmarks. However, the research community's progress remains constrained\nby the absence of large-scale, high-quality, and openly accessible datasets\nbuilt from real images. We introduce Pico-Banana-400K, a comprehensive\n400K-image dataset for instruction-based image editing. Our dataset is\nconstructed by leveraging Nano-Banana to generate diverse edit pairs from real\nphotographs in the OpenImages collection. What distinguishes Pico-Banana-400K\nfrom previous synthetic datasets is our systematic approach to quality and\ndiversity. We employ a fine-grained image editing taxonomy to ensure\ncomprehensive coverage of edit types while maintaining precise content\npreservation and instruction faithfulness through MLLM-based quality scoring\nand careful curation. Beyond single turn editing, Pico-Banana-400K enables\nresearch into complex editing scenarios. The dataset includes three specialized\nsubsets: (1) a 72K-example multi-turn collection for studying sequential\nediting, reasoning, and planning across consecutive modifications; (2) a\n56K-example preference subset for alignment research and reward model training;\nand (3) paired long-short editing instructions for developing instruction\nrewriting and summarization capabilities. By providing this large-scale,\nhigh-quality, and task-rich resource, Pico-Banana-400K establishes a robust\nfoundation for training and benchmarking the next generation of text-guided\nimage editing models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19808.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 141
        },
        "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.19336",
            "authors": [
                {
                    "_id": "68f98a97b9b2e4ae0467377e",
                    "name": "Kai Shi",
                    "hidden": false
                },
                {
                    "_id": "68f98a97b9b2e4ae0467377f",
                    "name": "Jun Yang",
                    "hidden": false
                },
                {
                    "_id": "68f98a97b9b2e4ae04673780",
                    "name": "Ni Yang",
                    "hidden": false
                },
                {
                    "_id": "68f98a97b9b2e4ae04673781",
                    "name": "Binqiang Pan",
                    "hidden": false
                },
                {
                    "_id": "68f98a97b9b2e4ae04673782",
                    "name": "Qingsong Xie",
                    "hidden": false
                },
                {
                    "_id": "68f98a97b9b2e4ae04673783",
                    "name": "Chao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f98a97b9b2e4ae04673784",
                    "name": "Zhenyu Yang",
                    "hidden": false
                },
                {
                    "_id": "68f98a97b9b2e4ae04673785",
                    "name": "Tianhuang Su",
                    "hidden": false
                },
                {
                    "_id": "68f98a97b9b2e4ae04673786",
                    "name": "Haonan Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T07:57:59.000Z",
            "submittedOnDailyAt": "2025-10-23T00:27:06.757Z",
            "title": "DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile\n  Phone Agents",
            "submittedOnDailyBy": {
                "_id": "66e24afddce93c7249b418c0",
                "avatarUrl": "/avatars/14fd5ee60999a7a34a45c8291b2116c6.svg",
                "isPro": false,
                "fullname": "OPPO AI Center",
                "user": "AIGCer-OPPO",
                "type": "user"
            },
            "summary": "Mobile Phone Agents (MPAs) have emerged as a promising research direction due\nto their broad applicability across diverse scenarios. While Multimodal Large\nLanguage Models (MLLMs) serve as the foundation for MPAs, their effectiveness\nin handling multiple mobile phone tasks simultaneously remains limited.\nAlthough multitask supervised fine-tuning (SFT) is widely adopted for multitask\nlearning, existing approaches struggle to determine optimal training data\ncompositions for peak performance. To address this challenge, we propose DaMo\n(Data Mixture Optimizer) - a novel solution employing a trainable network that\npredicts optimal data mixtures by forecasting downstream task performance for\nany given dataset ratio. To support comprehensive evaluation, we introduce\nPhoneAgentBench, the first specialized benchmark to evaluate MLLMs on\nmultimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse\nreal-world industrial mobile application scenarios. Demonstrating strong\npredictive capability (R^2=0.81) in small-scale pilot experiments, DaMo\nefficiently extrapolates optimal data mixing configurations. Our results show\nDaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to\nalternative methods. Furthermore, extensive experiments across established\nbenchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench\nreveal DaMo's superior generalization, outperforming other approaches by 2.57%\nin terms of average score. When used solely for MLLM optimization on the\nBFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably,\nDaMo maintains robust scalability, preserving its effectiveness when applied to\nother model architectures. The code and dataset are available at\nhttps://github.com/OPPO-Mente-Lab/DaMo.git",
            "upvotes": 16,
            "discussionId": "68f98a97b9b2e4ae04673787",
            "ai_summary": "DaMo, a trainable network optimizing data mixtures for Multimodal Large Language Models, enhances performance across various mobile phone tasks and benchmarks.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "multitask supervised fine-tuning",
                "DaMo",
                "Data Mixture Optimizer",
                "PhoneAgentBench",
                "BFCL-v3",
                "MME-Reasoning",
                "MME-Perception",
                "OCRBench"
            ],
            "organization": {
                "_id": "67177eecd0fad5b4ccc09461",
                "name": "OPPOer",
                "fullname": "OPPO",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66e24afddce93c7249b418c0/gQ-XFJehEyAH12zhbeR8Z.png"
            }
        },
        "publishedAt": "2025-10-22T03:57:59.000Z",
        "title": "DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile\n  Phone Agents",
        "summary": "Mobile Phone Agents (MPAs) have emerged as a promising research direction due\nto their broad applicability across diverse scenarios. While Multimodal Large\nLanguage Models (MLLMs) serve as the foundation for MPAs, their effectiveness\nin handling multiple mobile phone tasks simultaneously remains limited.\nAlthough multitask supervised fine-tuning (SFT) is widely adopted for multitask\nlearning, existing approaches struggle to determine optimal training data\ncompositions for peak performance. To address this challenge, we propose DaMo\n(Data Mixture Optimizer) - a novel solution employing a trainable network that\npredicts optimal data mixtures by forecasting downstream task performance for\nany given dataset ratio. To support comprehensive evaluation, we introduce\nPhoneAgentBench, the first specialized benchmark to evaluate MLLMs on\nmultimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse\nreal-world industrial mobile application scenarios. Demonstrating strong\npredictive capability (R^2=0.81) in small-scale pilot experiments, DaMo\nefficiently extrapolates optimal data mixing configurations. Our results show\nDaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to\nalternative methods. Furthermore, extensive experiments across established\nbenchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench\nreveal DaMo's superior generalization, outperforming other approaches by 2.57%\nin terms of average score. When used solely for MLLM optimization on the\nBFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably,\nDaMo maintains robust scalability, preserving its effectiveness when applied to\nother model architectures. The code and dataset are available at\nhttps://github.com/OPPO-Mente-Lab/DaMo.git",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19336.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66e24afddce93c7249b418c0",
            "avatarUrl": "/avatars/14fd5ee60999a7a34a45c8291b2116c6.svg",
            "fullname": "OPPO AI Center",
            "name": "AIGCer-OPPO",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "67177eecd0fad5b4ccc09461",
            "name": "OPPOer",
            "fullname": "OPPO",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66e24afddce93c7249b418c0/gQ-XFJehEyAH12zhbeR8Z.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.19817",
            "authors": [
                {
                    "_id": "68f98861b9b2e4ae04673757",
                    "name": "Jake Poznanski",
                    "hidden": false
                },
                {
                    "_id": "68f98861b9b2e4ae04673758",
                    "name": "Luca Soldaini",
                    "hidden": false
                },
                {
                    "_id": "68f98861b9b2e4ae04673759",
                    "name": "Kyle Lo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T17:53:02.000Z",
            "submittedOnDailyAt": "2025-10-23T00:14:13.036Z",
            "title": "olmOCR 2: Unit Test Rewards for Document OCR",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present olmOCR 2, the latest in our family of powerful OCR systems for\nconverting digitized print documents, like PDFs, into clean, naturally ordered\nplain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision\nlanguage model (VLM) trained using reinforcement learning with verifiable\nrewards (RLVR), where our rewards are a diverse set of binary unit tests. To\nscale unit test creation, we develop a pipeline for generating synthetic\ndocuments with diverse and challenging layouts, known ground-truth HTML source\ncode, and extracted test cases. We show that RL training on these test cases\nresults in state-of-the-art performance on olmOCR-Bench, our English-language\nOCR benchmark, with the largest improvements in math formula conversion, table\nparsing, and multi-column layouts compared to previous versions. We release our\nmodel, data and code under permissive open licenses.",
            "upvotes": 8,
            "discussionId": "68f98861b9b2e4ae0467375a",
            "projectPage": "https://olmocr.allen.ai/",
            "ai_summary": "olmOCR 2, a vision language model trained with reinforcement learning and verifiable rewards, achieves state-of-the-art performance in OCR tasks, particularly in math formula conversion, table parsing, and multi-column layouts.",
            "ai_keywords": [
                "vision language model",
                "reinforcement learning",
                "verifiable rewards",
                "synthetic documents",
                "ground-truth HTML",
                "test cases",
                "olmOCR-Bench",
                "math formula conversion",
                "table parsing",
                "multi-column layouts"
            ],
            "organization": {
                "_id": "5e70f3648ce3c604d78fe132",
                "name": "allenai",
                "fullname": "Ai2",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"
            }
        },
        "publishedAt": "2025-10-22T13:53:02.000Z",
        "title": "olmOCR 2: Unit Test Rewards for Document OCR",
        "summary": "We present olmOCR 2, the latest in our family of powerful OCR systems for\nconverting digitized print documents, like PDFs, into clean, naturally ordered\nplain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision\nlanguage model (VLM) trained using reinforcement learning with verifiable\nrewards (RLVR), where our rewards are a diverse set of binary unit tests. To\nscale unit test creation, we develop a pipeline for generating synthetic\ndocuments with diverse and challenging layouts, known ground-truth HTML source\ncode, and extracted test cases. We show that RL training on these test cases\nresults in state-of-the-art performance on olmOCR-Bench, our English-language\nOCR benchmark, with the largest improvements in math formula conversion, table\nparsing, and multi-column layouts compared to previous versions. We release our\nmodel, data and code under permissive open licenses.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19817.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 141
        },
        "organization": {
            "_id": "5e70f3648ce3c604d78fe132",
            "name": "allenai",
            "fullname": "Ai2",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.19592",
            "authors": [
                {
                    "_id": "68f9a201b9b2e4ae04673887",
                    "name": "Su Ho Han",
                    "hidden": false
                },
                {
                    "_id": "68f9a201b9b2e4ae04673888",
                    "user": {
                        "_id": "6513030fb3a463e17df56edd",
                        "avatarUrl": "/avatars/867bd4316b2de758654ad3a84ea868c1.svg",
                        "isPro": false,
                        "fullname": "Hyun, Jeongseok",
                        "user": "js-hyun",
                        "type": "user"
                    },
                    "name": "Jeongseok Hyun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:47:16.248Z",
                    "hidden": false
                },
                {
                    "_id": "68f9a201b9b2e4ae04673889",
                    "name": "Pilhyeon Lee",
                    "hidden": false
                },
                {
                    "_id": "68f9a201b9b2e4ae0467388a",
                    "name": "Minho Shim",
                    "hidden": false
                },
                {
                    "_id": "68f9a201b9b2e4ae0467388b",
                    "name": "Dongyoon Wee",
                    "hidden": false
                },
                {
                    "_id": "68f9a201b9b2e4ae0467388c",
                    "name": "Seon Joo Kim",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6513030fb3a463e17df56edd/zdwwFVQBPiXu0WaYTPBLk.gif"
            ],
            "publishedAt": "2025-10-22T13:42:59.000Z",
            "submittedOnDailyAt": "2025-10-23T11:24:06.997Z",
            "title": "Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning\n  Segmentation",
            "submittedOnDailyBy": {
                "_id": "6513030fb3a463e17df56edd",
                "avatarUrl": "/avatars/867bd4316b2de758654ad3a84ea868c1.svg",
                "isPro": false,
                "fullname": "Hyun, Jeongseok",
                "user": "js-hyun",
                "type": "user"
            },
            "summary": "Multimodal large language models (MLLMs) demonstrate strong video\nunderstanding by attending to visual tokens relevant to textual queries. To\ndirectly adapt this for localization in a training-free manner, we cast video\nreasoning segmentation as a video QA task and extract attention maps via\nrollout mechanism. However, raw attention maps are noisy and poorly aligned\nwith object regions. We propose Decomposed Attention Fusion (DecAF), which\nrefines these maps through two mechanisms: (1) contrastive object-background\nfusion and (2) complementary video-frame fusion. This method suppresses\nirrelevant activations and enhances object-focused cues, enabling direct\nconversion of attention maps into coarse segmentation masks. In addition, we\nintroduce attention-guided SAM2 prompting for obtaining fine-grained masks.\nUnlike existing methods that jointly train MLLMs with SAM, our method operates\nentirely without retraining. DecAF outperforms training-free methods and\nachieves performance comparable to training-based methods on both referring and\nreasoning VOS benchmarks. The code will be available at\nhttps://github.com/HYUNJS/DecAF.",
            "upvotes": 7,
            "discussionId": "68f9a201b9b2e4ae0467388d",
            "projectPage": "https://www.jshyun.me/projects/decaf",
            "githubRepo": "https://github.com/HYUNJS/DecAF",
            "ai_summary": "Decomposed Attention Fusion (DecAF) enhances video object segmentation by refining attention maps from multimodal large language models without retraining.",
            "ai_keywords": [
                "multimodal large language models",
                "video reasoning segmentation",
                "video QA task",
                "rollout mechanism",
                "attention maps",
                "contrastive object-background fusion",
                "complementary video-frame fusion",
                "attention-guided SAM2 prompting",
                "referring VOS benchmarks",
                "reasoning VOS benchmarks"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-10-22T09:42:59.000Z",
        "title": "Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning\n  Segmentation",
        "summary": "Multimodal large language models (MLLMs) demonstrate strong video\nunderstanding by attending to visual tokens relevant to textual queries. To\ndirectly adapt this for localization in a training-free manner, we cast video\nreasoning segmentation as a video QA task and extract attention maps via\nrollout mechanism. However, raw attention maps are noisy and poorly aligned\nwith object regions. We propose Decomposed Attention Fusion (DecAF), which\nrefines these maps through two mechanisms: (1) contrastive object-background\nfusion and (2) complementary video-frame fusion. This method suppresses\nirrelevant activations and enhances object-focused cues, enabling direct\nconversion of attention maps into coarse segmentation masks. In addition, we\nintroduce attention-guided SAM2 prompting for obtaining fine-grained masks.\nUnlike existing methods that jointly train MLLMs with SAM, our method operates\nentirely without retraining. DecAF outperforms training-free methods and\nachieves performance comparable to training-based methods on both referring and\nreasoning VOS benchmarks. The code will be available at\nhttps://github.com/HYUNJS/DecAF.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6513030fb3a463e17df56edd/zdwwFVQBPiXu0WaYTPBLk.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19592.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6513030fb3a463e17df56edd",
            "avatarUrl": "/avatars/867bd4316b2de758654ad3a84ea868c1.svg",
            "fullname": "Hyun, Jeongseok",
            "name": "js-hyun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.16844",
            "authors": [
                {
                    "_id": "68f86f6b7669bcaeecce0e59",
                    "name": "Jiajie Jin",
                    "hidden": false
                },
                {
                    "_id": "68f86f6b7669bcaeecce0e5a",
                    "name": "Yuyao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f86f6b7669bcaeecce0e5b",
                    "name": "Yimeng Xu",
                    "hidden": false
                },
                {
                    "_id": "68f86f6b7669bcaeecce0e5c",
                    "name": "Hongjin Qian",
                    "hidden": false
                },
                {
                    "_id": "68f86f6b7669bcaeecce0e5d",
                    "name": "Yutao Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f86f6b7669bcaeecce0e5e",
                    "name": "Zhicheng Dou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-19T14:05:35.000Z",
            "submittedOnDailyAt": "2025-10-23T00:50:26.504Z",
            "title": "FinSight: Towards Real-World Financial Deep Research",
            "submittedOnDailyBy": {
                "_id": "6695f14df0ffd8e3a379ad61",
                "avatarUrl": "/avatars/5ebb7e55ee9c2d93850b279f440675b0.svg",
                "isPro": false,
                "fullname": "Jiajie Jin",
                "user": "jinjiajie",
                "type": "user"
            },
            "summary": "Generating professional financial reports is a labor-intensive and\nintellectually demanding process that current AI systems struggle to fully\nautomate. To address this challenge, we introduce FinSight (Financial InSight),\na novel multi agent framework for producing high-quality, multimodal financial\nreports. The foundation of FinSight is the Code Agent with Variable Memory\n(CAVM) architecture, which unifies external data, designed tools, and agents\ninto a programmable variable space, enabling flexible data collection, analysis\nand report generation through executable code. To ensure professional-grade\nvisualization, we propose an Iterative Vision-Enhanced Mechanism that\nprogressively refines raw visual outputs into polished financial charts.\nFurthermore, a two stage Writing Framework expands concise Chain-of-Analysis\nsegments into coherent, citation-aware, and multimodal reports, ensuring both\nanalytical depth and structural consistency. Experiments on various company and\nindustry-level tasks demonstrate that FinSight significantly outperforms all\nbaselines, including leading deep research systems in terms of factual\naccuracy, analytical depth, and presentation quality, demonstrating a clear\npath toward generating reports that approach human-expert quality.",
            "upvotes": 7,
            "discussionId": "68f86f6b7669bcaeecce0e5f",
            "ai_summary": "FinSight, a multi-agent framework using CAVM architecture and iterative vision-enhanced mechanism, generates high-quality, multimodal financial reports with superior accuracy and presentation quality compared to existing systems.",
            "ai_keywords": [
                "multi agent framework",
                "Code Agent with Variable Memory (CAVM)",
                "programmable variable space",
                "Iterative Vision-Enhanced Mechanism",
                "Chain-of-Analysis",
                "citation-aware",
                "multimodal reports"
            ]
        },
        "publishedAt": "2025-10-19T10:05:35.000Z",
        "title": "FinSight: Towards Real-World Financial Deep Research",
        "summary": "Generating professional financial reports is a labor-intensive and\nintellectually demanding process that current AI systems struggle to fully\nautomate. To address this challenge, we introduce FinSight (Financial InSight),\na novel multi agent framework for producing high-quality, multimodal financial\nreports. The foundation of FinSight is the Code Agent with Variable Memory\n(CAVM) architecture, which unifies external data, designed tools, and agents\ninto a programmable variable space, enabling flexible data collection, analysis\nand report generation through executable code. To ensure professional-grade\nvisualization, we propose an Iterative Vision-Enhanced Mechanism that\nprogressively refines raw visual outputs into polished financial charts.\nFurthermore, a two stage Writing Framework expands concise Chain-of-Analysis\nsegments into coherent, citation-aware, and multimodal reports, ensuring both\nanalytical depth and structural consistency. Experiments on various company and\nindustry-level tasks demonstrate that FinSight significantly outperforms all\nbaselines, including leading deep research systems in terms of factual\naccuracy, analytical depth, and presentation quality, demonstrating a clear\npath toward generating reports that approach human-expert quality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16844.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6695f14df0ffd8e3a379ad61",
            "avatarUrl": "/avatars/5ebb7e55ee9c2d93850b279f440675b0.svg",
            "fullname": "Jiajie Jin",
            "name": "jinjiajie",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.15050",
            "authors": [
                {
                    "_id": "68f8a910c7cbb0ef2aa92df0",
                    "user": {
                        "_id": "67257ee0938e718957c9c100",
                        "avatarUrl": "/avatars/db1f792ee1a5d860ea4e98c11a016a1b.svg",
                        "isPro": false,
                        "fullname": "Chao Huang",
                        "user": "ChaoHuangCS",
                        "type": "user"
                    },
                    "name": "Chao Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:42:05.689Z",
                    "hidden": false
                },
                {
                    "_id": "68f8a910c7cbb0ef2aa92df1",
                    "name": "Zeliang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f8a910c7cbb0ef2aa92df2",
                    "name": "Jiang Liu",
                    "hidden": false
                },
                {
                    "_id": "68f8a910c7cbb0ef2aa92df3",
                    "name": "Ximeng Sun",
                    "hidden": false
                },
                {
                    "_id": "68f8a910c7cbb0ef2aa92df4",
                    "name": "Jialian Wu",
                    "hidden": false
                },
                {
                    "_id": "68f8a910c7cbb0ef2aa92df5",
                    "name": "Xiaodong Yu",
                    "hidden": false
                },
                {
                    "_id": "68f8a910c7cbb0ef2aa92df6",
                    "name": "Ze Wang",
                    "hidden": false
                },
                {
                    "_id": "68f8a910c7cbb0ef2aa92df7",
                    "name": "Chenliang Xu",
                    "hidden": false
                },
                {
                    "_id": "68f8a910c7cbb0ef2aa92df8",
                    "name": "Emad Barsoum",
                    "hidden": false
                },
                {
                    "_id": "68f8a910c7cbb0ef2aa92df9",
                    "name": "Zicheng Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T18:06:46.000Z",
            "submittedOnDailyAt": "2025-10-23T07:36:30.010Z",
            "title": "Directional Reasoning Injection for Fine-Tuning MLLMs",
            "submittedOnDailyBy": {
                "_id": "67257ee0938e718957c9c100",
                "avatarUrl": "/avatars/db1f792ee1a5d860ea4e98c11a016a1b.svg",
                "isPro": false,
                "fullname": "Chao Huang",
                "user": "ChaoHuangCS",
                "type": "user"
            },
            "summary": "Multimodal large language models (MLLMs) are rapidly advancing, yet their\nreasoning ability often lags behind that of strong text-only counterparts.\nExisting methods to bridge this gap rely on supervised fine-tuning over\nlarge-scale multimodal reasoning data or reinforcement learning, both of which\nare resource-intensive. A promising alternative is model merging, which\ninterpolates parameters between reasoning-enhanced LLMs and multimodal\nvariants. However, our analysis shows that naive merging is not always a \"free\nlunch\": its effectiveness varies drastically across model families, with some\n(e.g., LLaVA, Idefics) benefiting while others (e.g., Qwen) suffer performance\ndegradation. To address this, we propose Directional Reasoning Injection for\nFine-Tuning (DRIFT) MLLMs, a lightweight method that transfers reasoning\nknowledge in the gradient space, without destabilizing multimodal alignment.\nDRIFT precomputes a reasoning prior as the parameter-space difference between\nreasoning and multimodal variants, then uses it to bias gradients during\nmultimodal fine-tuning. This approach preserves the simplicity of standard\nsupervised fine-tuning pipelines while enabling efficient reasoning transfer.\nExtensive experiments on multimodal reasoning benchmarks, including MathVista\nand MathVerse, demonstrate that DRIFT consistently improves reasoning\nperformance over naive merging and supervised fine-tuning, while matching or\nsurpassing training-heavy methods at a fraction of the cost.",
            "upvotes": 7,
            "discussionId": "68f8a910c7cbb0ef2aa92dfa",
            "githubRepo": "https://github.com/WikiChao/DRIFT",
            "ai_summary": "DRIFT, a lightweight method, enhances multimodal large language models' reasoning ability by transferring knowledge in gradient space, outperforming naive merging and supervised fine-tuning with reduced computational cost.",
            "ai_keywords": [
                "multimodal large language models",
                "reasoning ability",
                "supervised fine-tuning",
                "reinforcement learning",
                "model merging",
                "parameter-space",
                "reasoning-enhanced LLMs",
                "multimodal variants",
                "directional reasoning injection",
                "gradient space",
                "reasoning prior",
                "multimodal alignment",
                "MathVista",
                "MathVerse"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-10-16T14:06:46.000Z",
        "title": "Directional Reasoning Injection for Fine-Tuning MLLMs",
        "summary": "Multimodal large language models (MLLMs) are rapidly advancing, yet their\nreasoning ability often lags behind that of strong text-only counterparts.\nExisting methods to bridge this gap rely on supervised fine-tuning over\nlarge-scale multimodal reasoning data or reinforcement learning, both of which\nare resource-intensive. A promising alternative is model merging, which\ninterpolates parameters between reasoning-enhanced LLMs and multimodal\nvariants. However, our analysis shows that naive merging is not always a \"free\nlunch\": its effectiveness varies drastically across model families, with some\n(e.g., LLaVA, Idefics) benefiting while others (e.g., Qwen) suffer performance\ndegradation. To address this, we propose Directional Reasoning Injection for\nFine-Tuning (DRIFT) MLLMs, a lightweight method that transfers reasoning\nknowledge in the gradient space, without destabilizing multimodal alignment.\nDRIFT precomputes a reasoning prior as the parameter-space difference between\nreasoning and multimodal variants, then uses it to bias gradients during\nmultimodal fine-tuning. This approach preserves the simplicity of standard\nsupervised fine-tuning pipelines while enabling efficient reasoning transfer.\nExtensive experiments on multimodal reasoning benchmarks, including MathVista\nand MathVerse, demonstrate that DRIFT consistently improves reasoning\nperformance over naive merging and supervised fine-tuning, while matching or\nsurpassing training-heavy methods at a fraction of the cost.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15050.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67257ee0938e718957c9c100",
            "avatarUrl": "/avatars/db1f792ee1a5d860ea4e98c11a016a1b.svg",
            "fullname": "Chao Huang",
            "name": "ChaoHuangCS",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.19386",
            "authors": [
                {
                    "_id": "68f98cceb9b2e4ae046737c3",
                    "name": "Ning Li",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737c4",
                    "name": "Qiqiang Lin",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737c5",
                    "name": "Zheng Wu",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737c6",
                    "name": "Xiaoyun Mo",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737c7",
                    "name": "Weiming Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737c8",
                    "name": "Yin Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737c9",
                    "name": "Xiangmou Qu",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737ca",
                    "name": "Jiamu Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737cb",
                    "name": "Jun Wang",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737cc",
                    "name": "Congmin Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737cd",
                    "name": "Yuanyi Song",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737ce",
                    "name": "Hongjiang Chen",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737cf",
                    "name": "Heyuan Huang",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737d0",
                    "name": "Jihong Wang",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737d1",
                    "name": "Jiaxin Yin",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737d2",
                    "name": "Jingwei Yu",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737d3",
                    "name": "Junwei Liao",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737d4",
                    "name": "Qiuying Peng",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737d5",
                    "name": "Xingyu Lou",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737d6",
                    "name": "Jun Wang",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737d7",
                    "name": "Weiwen Liu",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737d8",
                    "name": "Zhuosheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f98cceb9b2e4ae046737d9",
                    "name": "Weinan Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T09:02:48.000Z",
            "submittedOnDailyAt": "2025-10-23T00:33:11.605Z",
            "title": "ColorAgent: Building A Robust, Personalized, and Interactive OS Agent",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "With the advancements in hardware, software, and large language model\ntechnologies, the interaction between humans and operating systems has evolved\nfrom the command-line interface to the rapidly emerging AI agent interactions.\nBuilding an operating system (OS) agent capable of executing user instructions\nand faithfully following user desires is becoming a reality. In this technical\nreport, we present ColorAgent, an OS agent designed to engage in long-horizon,\nrobust interactions with the environment while also enabling personalized and\nproactive user interaction. To enable long-horizon interactions with the\nenvironment, we enhance the model's capabilities through step-wise\nreinforcement learning and self-evolving training, while also developing a\ntailored multi-agent framework that ensures generality, consistency, and\nrobustness. In terms of user interaction, we explore personalized user intent\nrecognition and proactive engagement, positioning the OS agent not merely as an\nautomation tool but as a warm, collaborative partner. We evaluate ColorAgent on\nthe AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2%\nand 50.7%, respectively, establishing a new state of the art. Nonetheless, we\nnote that current benchmarks are insufficient for a comprehensive evaluation of\nOS agents and propose further exploring directions in future work, particularly\nin the areas of evaluation paradigms, agent collaboration, and security. Our\ncode is available at https://github.com/MadeAgents/mobile-use.",
            "upvotes": 6,
            "discussionId": "68f98cceb9b2e4ae046737da",
            "githubRepo": "https://github.com/MadeAgents/mobile-use",
            "ai_summary": "ColorAgent, an OS agent using step-wise reinforcement learning and a multi-agent framework, achieves high success rates in long-horizon interactions and personalized user engagement on Android benchmarks.",
            "ai_keywords": [
                "step-wise reinforcement learning",
                "self-evolving training",
                "multi-agent framework",
                "personalized user intent recognition",
                "proactive engagement",
                "AndroidWorld",
                "AndroidLab"
            ],
            "githubStars": 88
        },
        "publishedAt": "2025-10-22T05:02:48.000Z",
        "title": "ColorAgent: Building A Robust, Personalized, and Interactive OS Agent",
        "summary": "With the advancements in hardware, software, and large language model\ntechnologies, the interaction between humans and operating systems has evolved\nfrom the command-line interface to the rapidly emerging AI agent interactions.\nBuilding an operating system (OS) agent capable of executing user instructions\nand faithfully following user desires is becoming a reality. In this technical\nreport, we present ColorAgent, an OS agent designed to engage in long-horizon,\nrobust interactions with the environment while also enabling personalized and\nproactive user interaction. To enable long-horizon interactions with the\nenvironment, we enhance the model's capabilities through step-wise\nreinforcement learning and self-evolving training, while also developing a\ntailored multi-agent framework that ensures generality, consistency, and\nrobustness. In terms of user interaction, we explore personalized user intent\nrecognition and proactive engagement, positioning the OS agent not merely as an\nautomation tool but as a warm, collaborative partner. We evaluate ColorAgent on\nthe AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2%\nand 50.7%, respectively, establishing a new state of the art. Nonetheless, we\nnote that current benchmarks are insufficient for a comprehensive evaluation of\nOS agents and propose further exploring directions in future work, particularly\nin the areas of evaluation paradigms, agent collaboration, and security. Our\ncode is available at https://github.com/MadeAgents/mobile-use.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19386.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 141
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.19316",
            "authors": [
                {
                    "_id": "68f98c87b9b2e4ae04673796",
                    "user": {
                        "_id": "65745569839aa08899ea5d27",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
                        "isPro": false,
                        "fullname": "kailinjiang",
                        "user": "kailinjiang",
                        "type": "user"
                    },
                    "name": "Kailin Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:50:15.872Z",
                    "hidden": false
                },
                {
                    "_id": "68f98c87b9b2e4ae04673797",
                    "name": "Hongbo Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f98c87b9b2e4ae04673798",
                    "name": "Ning Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f98c87b9b2e4ae04673799",
                    "name": "Zhi Gao",
                    "hidden": false
                },
                {
                    "_id": "68f98c87b9b2e4ae0467379a",
                    "name": "Jinhe Bi",
                    "hidden": false
                },
                {
                    "_id": "68f98c87b9b2e4ae0467379b",
                    "name": "Yuchen Ren",
                    "hidden": false
                },
                {
                    "_id": "68f98c87b9b2e4ae0467379c",
                    "name": "Bin Li",
                    "hidden": false
                },
                {
                    "_id": "68f98c87b9b2e4ae0467379d",
                    "name": "Yuntao Du",
                    "hidden": false
                },
                {
                    "_id": "68f98c87b9b2e4ae0467379e",
                    "name": "Lei Liu",
                    "hidden": false
                },
                {
                    "_id": "68f98c87b9b2e4ae0467379f",
                    "name": "Qing Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T07:26:55.000Z",
            "submittedOnDailyAt": "2025-10-23T00:34:23.319Z",
            "title": "KORE: Enhancing Knowledge Injection for Large Multimodal Models via\n  Knowledge-Oriented Augmentations and Constraints",
            "submittedOnDailyBy": {
                "_id": "65745569839aa08899ea5d27",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
                "isPro": false,
                "fullname": "kailinjiang",
                "user": "kailinjiang",
                "type": "user"
            },
            "summary": "Large Multimodal Models encode extensive factual knowledge in their\npre-trained weights. However, its knowledge remains static and limited, unable\nto keep pace with real-world developments, which hinders continuous knowledge\nacquisition. Effective knowledge injection thus becomes critical, involving two\ngoals: knowledge adaptation (injecting new knowledge) and knowledge retention\n(preserving old knowledge). Existing methods often struggle to learn new\nknowledge and suffer from catastrophic forgetting. To address this, we propose\nKORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints\nfor injecting new knowledge into large multimodal models while preserving old\nknowledge. Unlike general text or image data augmentation, KORE automatically\nconverts individual knowledge items into structured and comprehensive knowledge\nto ensure that the model accurately learns new knowledge, enabling accurate\nadaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix\nof LMM's linear layer activations and initializes the adapter by projecting the\noriginal weights into the matrix's null space, defining a fine-tuning direction\nthat minimizes interference with previous knowledge, enabling powerful\nretention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B,\nLLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new\nknowledge injection performance and effectively mitigates catastrophic\nforgetting.",
            "upvotes": 6,
            "discussionId": "68f98c87b9b2e4ae046737a0",
            "projectPage": "https://kore-lmm.github.io/",
            "githubRepo": "https://github.com/KORE-LMM/KORE",
            "ai_summary": "KORE is a method for injecting new knowledge into large multimodal models while preserving old knowledge, using structured augmentations and covariance matrix constraints to minimize catastrophic forgetting.",
            "ai_keywords": [
                "knowledge adaptation",
                "knowledge retention",
                "catastrophic forgetting",
                "KORE",
                "KnOwledge-oRientEd augmentations",
                "covariance matrix",
                "linear layer activations",
                "adapter",
                "fine-tuning direction",
                "LMMs",
                "LLaVA-v1.5-7B",
                "LLaVA-v1.5-13B",
                "Qwen2.5-VL-7B"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-10-22T03:26:55.000Z",
        "title": "KORE: Enhancing Knowledge Injection for Large Multimodal Models via\n  Knowledge-Oriented Augmentations and Constraints",
        "summary": "Large Multimodal Models encode extensive factual knowledge in their\npre-trained weights. However, its knowledge remains static and limited, unable\nto keep pace with real-world developments, which hinders continuous knowledge\nacquisition. Effective knowledge injection thus becomes critical, involving two\ngoals: knowledge adaptation (injecting new knowledge) and knowledge retention\n(preserving old knowledge). Existing methods often struggle to learn new\nknowledge and suffer from catastrophic forgetting. To address this, we propose\nKORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints\nfor injecting new knowledge into large multimodal models while preserving old\nknowledge. Unlike general text or image data augmentation, KORE automatically\nconverts individual knowledge items into structured and comprehensive knowledge\nto ensure that the model accurately learns new knowledge, enabling accurate\nadaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix\nof LMM's linear layer activations and initializes the adapter by projecting the\noriginal weights into the matrix's null space, defining a fine-tuning direction\nthat minimizes interference with previous knowledge, enabling powerful\nretention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B,\nLLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new\nknowledge injection performance and effectively mitigates catastrophic\nforgetting.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19316.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65745569839aa08899ea5d27",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
            "fullname": "kailinjiang",
            "name": "kailinjiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.19028",
            "authors": [
                {
                    "_id": "68f9ad08b9b2e4ae046738d2",
                    "user": {
                        "_id": "6576ace7769f3ee9bd7b1b88",
                        "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
                        "isPro": false,
                        "fullname": "Eunsu Kim",
                        "user": "EunsuKim",
                        "type": "user"
                    },
                    "name": "Eunsu Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:46:34.645Z",
                    "hidden": false
                },
                {
                    "_id": "68f9ad08b9b2e4ae046738d3",
                    "name": "Junyeong Park",
                    "hidden": false
                },
                {
                    "_id": "68f9ad08b9b2e4ae046738d4",
                    "name": "Juhyun Oh",
                    "hidden": false
                },
                {
                    "_id": "68f9ad08b9b2e4ae046738d5",
                    "name": "Kiwoong Park",
                    "hidden": false
                },
                {
                    "_id": "68f9ad08b9b2e4ae046738d6",
                    "user": {
                        "_id": "654f3cca8cc59d5b490b805b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654f3cca8cc59d5b490b805b/WAZZjN4q8VGp3VS2lAWyy.png",
                        "isPro": true,
                        "fullname": "Seyoung Song",
                        "user": "seyoungsong",
                        "type": "user"
                    },
                    "name": "Seyoung Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:46:27.519Z",
                    "hidden": false
                },
                {
                    "_id": "68f9ad08b9b2e4ae046738d7",
                    "name": "A. Seza Dogruoz",
                    "hidden": false
                },
                {
                    "_id": "68f9ad08b9b2e4ae046738d8",
                    "name": "Najoung Kim",
                    "hidden": false
                },
                {
                    "_id": "68f9ad08b9b2e4ae046738d9",
                    "name": "Alice Oh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T19:12:47.000Z",
            "submittedOnDailyAt": "2025-10-23T02:52:18.993Z",
            "title": "Are they lovers or friends? Evaluating LLMs' Social Reasoning in English\n  and Korean Dialogues",
            "submittedOnDailyBy": {
                "_id": "6576ace7769f3ee9bd7b1b88",
                "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
                "isPro": false,
                "fullname": "Eunsu Kim",
                "user": "EunsuKim",
                "type": "user"
            },
            "summary": "As large language models (LLMs) are increasingly used in human-AI\ninteractions, their social reasoning capabilities in interpersonal contexts are\ncritical. We introduce SCRIPTS, a 1k-dialogue dataset in English and Korean,\nsourced from movie scripts. The task involves evaluating models' social\nreasoning capability to infer the interpersonal relationships (e.g., friends,\nsisters, lovers) between speakers in each dialogue. Each dialogue is annotated\nwith probabilistic relational labels (Highly Likely, Less Likely, Unlikely) by\nnative (or equivalent) Korean and English speakers from Korea and the U.S.\nEvaluating nine models on our task, current proprietary LLMs achieve around\n75-80% on the English dataset, whereas their performance on Korean drops to\n58-69%. More strikingly, models select Unlikely relationships in 10-25% of\ntheir responses. Furthermore, we find that thinking models and chain-of-thought\nprompting, effective for general reasoning, provide minimal benefits for social\nreasoning and occasionally amplify social biases. Our findings reveal\nsignificant limitations in current LLMs' social reasoning capabilities,\nhighlighting the need for efforts to develop socially-aware language models.",
            "upvotes": 6,
            "discussionId": "68f9ad08b9b2e4ae046738da",
            "githubRepo": "https://github.com/rladmstn1714/SCRIPTS",
            "ai_summary": "Current large language models exhibit significant limitations in social reasoning, particularly in inferring interpersonal relationships across different languages, and thinking models or chain-of-thought prompting offer minimal improvement.",
            "ai_keywords": [
                "large language models",
                "social reasoning",
                "interpersonal relationships",
                "SCRIPTS",
                "dialogue dataset",
                "probabilistic relational labels",
                "thinking models",
                "chain-of-thought prompting",
                "socially-aware language models"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-10-21T15:12:47.000Z",
        "title": "Are they lovers or friends? Evaluating LLMs' Social Reasoning in English\n  and Korean Dialogues",
        "summary": "As large language models (LLMs) are increasingly used in human-AI\ninteractions, their social reasoning capabilities in interpersonal contexts are\ncritical. We introduce SCRIPTS, a 1k-dialogue dataset in English and Korean,\nsourced from movie scripts. The task involves evaluating models' social\nreasoning capability to infer the interpersonal relationships (e.g., friends,\nsisters, lovers) between speakers in each dialogue. Each dialogue is annotated\nwith probabilistic relational labels (Highly Likely, Less Likely, Unlikely) by\nnative (or equivalent) Korean and English speakers from Korea and the U.S.\nEvaluating nine models on our task, current proprietary LLMs achieve around\n75-80% on the English dataset, whereas their performance on Korean drops to\n58-69%. More strikingly, models select Unlikely relationships in 10-25% of\ntheir responses. Furthermore, we find that thinking models and chain-of-thought\nprompting, effective for general reasoning, provide minimal benefits for social\nreasoning and occasionally amplify social biases. Our findings reveal\nsignificant limitations in current LLMs' social reasoning capabilities,\nhighlighting the need for efforts to develop socially-aware language models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19028.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6576ace7769f3ee9bd7b1b88",
            "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
            "fullname": "Eunsu Kim",
            "name": "EunsuKim",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.18313",
            "authors": [
                {
                    "_id": "68f98bbdb9b2e4ae04673789",
                    "name": "Bohan Li",
                    "hidden": false
                },
                {
                    "_id": "68f98bbdb9b2e4ae0467378a",
                    "user": {
                        "_id": "664b4ae793aed26946d7477c",
                        "avatarUrl": "/avatars/2087c10526a80f238836468b0d21f7cb.svg",
                        "isPro": false,
                        "fullname": "Zhuang Ma",
                        "user": "Genshushu",
                        "type": "user"
                    },
                    "name": "Zhuang Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:50:26.168Z",
                    "hidden": false
                },
                {
                    "_id": "68f98bbdb9b2e4ae0467378b",
                    "name": "Dalong Du",
                    "hidden": false
                },
                {
                    "_id": "68f98bbdb9b2e4ae0467378c",
                    "name": "Baorui Peng",
                    "hidden": false
                },
                {
                    "_id": "68f98bbdb9b2e4ae0467378d",
                    "name": "Zhujin Liang",
                    "hidden": false
                },
                {
                    "_id": "68f98bbdb9b2e4ae0467378e",
                    "name": "Zhenqiang Liu",
                    "hidden": false
                },
                {
                    "_id": "68f98bbdb9b2e4ae0467378f",
                    "name": "Chao Ma",
                    "hidden": false
                },
                {
                    "_id": "68f98bbdb9b2e4ae04673790",
                    "name": "Yueming Jin",
                    "hidden": false
                },
                {
                    "_id": "68f98bbdb9b2e4ae04673791",
                    "name": "Hao Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f98bbdb9b2e4ae04673792",
                    "name": "Wenjun Zeng",
                    "hidden": false
                },
                {
                    "_id": "68f98bbdb9b2e4ae04673793",
                    "name": "Xin Jin",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/CFPOwVCZTaYYRz8UV-xHt.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/Br2oqSl1Zmq0Nr6uSbIpl.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/tbqN08_U18WLXJtPLQLOE.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/YpgOAUqkyroUMo796hCWX.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/3ftuJdOh4SK2bAfooEWIw.mp4"
            ],
            "publishedAt": "2025-10-21T05:49:01.000Z",
            "submittedOnDailyAt": "2025-10-23T00:33:35.049Z",
            "title": "OmniNWM: Omniscient Driving Navigation World Models",
            "submittedOnDailyBy": {
                "_id": "66480d4cea529a27ecaaee66",
                "avatarUrl": "/avatars/936f77f0c3c304e1d170d3a5d2737485.svg",
                "isPro": false,
                "fullname": "Bohan Li",
                "user": "Arlolo0",
                "type": "user"
            },
            "summary": "Autonomous driving world models are expected to work effectively across three\ncore dimensions: state, action, and reward. Existing models, however, are\ntypically restricted to limited state modalities, short video sequences,\nimprecise action control, and a lack of reward awareness. In this paper, we\nintroduce OmniNWM, an omniscient panoramic navigation world model that\naddresses all three dimensions within a unified framework. For state, OmniNWM\njointly generates panoramic videos of RGB, semantics, metric depth, and 3D\noccupancy. A flexible forcing strategy enables high-quality long-horizon\nauto-regressive generation. For action, we introduce a normalized panoramic\nPlucker ray-map representation that encodes input trajectories into pixel-level\nsignals, enabling highly precise and generalizable control over panoramic video\ngeneration. Regarding reward, we move beyond learning reward functions with\nexternal image-based models: instead, we leverage the generated 3D occupancy to\ndirectly define rule-based dense rewards for driving compliance and safety.\nExtensive experiments demonstrate that OmniNWM achieves state-of-the-art\nperformance in video generation, control accuracy, and long-horizon stability,\nwhile providing a reliable closed-loop evaluation framework through\noccupancy-grounded rewards. Project page is available at\nhttps://github.com/Arlo0o/OmniNWM.",
            "upvotes": 6,
            "discussionId": "68f98bbdb9b2e4ae04673794",
            "projectPage": "https://arlo0o.github.io/OmniNWM/",
            "githubRepo": "https://github.com/Ma-Zhuang/OmniNWM",
            "ai_summary": "OmniNWM is a unified world model for autonomous driving that generates panoramic videos, encodes actions using Plucker ray-maps, and defines dense rewards based on 3D occupancy, achieving top performance in video generation, control, and stability.",
            "ai_keywords": [
                "panoramic navigation world model",
                "OmniNWM",
                "panoramic videos",
                "RGB",
                "semantics",
                "metric depth",
                "3D occupancy",
                "auto-regressive generation",
                "normalized panoramic Plucker ray-map",
                "rule-based dense rewards",
                "driving compliance",
                "safety",
                "occupancy-grounded rewards"
            ],
            "githubStars": 48
        },
        "publishedAt": "2025-10-21T01:49:01.000Z",
        "title": "OmniNWM: Omniscient Driving Navigation World Models",
        "summary": "Autonomous driving world models are expected to work effectively across three\ncore dimensions: state, action, and reward. Existing models, however, are\ntypically restricted to limited state modalities, short video sequences,\nimprecise action control, and a lack of reward awareness. In this paper, we\nintroduce OmniNWM, an omniscient panoramic navigation world model that\naddresses all three dimensions within a unified framework. For state, OmniNWM\njointly generates panoramic videos of RGB, semantics, metric depth, and 3D\noccupancy. A flexible forcing strategy enables high-quality long-horizon\nauto-regressive generation. For action, we introduce a normalized panoramic\nPlucker ray-map representation that encodes input trajectories into pixel-level\nsignals, enabling highly precise and generalizable control over panoramic video\ngeneration. Regarding reward, we move beyond learning reward functions with\nexternal image-based models: instead, we leverage the generated 3D occupancy to\ndirectly define rule-based dense rewards for driving compliance and safety.\nExtensive experiments demonstrate that OmniNWM achieves state-of-the-art\nperformance in video generation, control accuracy, and long-horizon stability,\nwhile providing a reliable closed-loop evaluation framework through\noccupancy-grounded rewards. Project page is available at\nhttps://github.com/Arlo0o/OmniNWM.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/CFPOwVCZTaYYRz8UV-xHt.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/Br2oqSl1Zmq0Nr6uSbIpl.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/tbqN08_U18WLXJtPLQLOE.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/YpgOAUqkyroUMo796hCWX.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/3ftuJdOh4SK2bAfooEWIw.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18313.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66480d4cea529a27ecaaee66",
            "avatarUrl": "/avatars/936f77f0c3c304e1d170d3a5d2737485.svg",
            "fullname": "Bohan Li",
            "name": "Arlolo0",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.19286",
            "authors": [
                {
                    "_id": "68f98627b9b2e4ae04673747",
                    "name": "Reza Esfandiarpoor",
                    "hidden": false
                },
                {
                    "_id": "68f98627b9b2e4ae04673748",
                    "name": "Vishwas Suryanarayanan",
                    "hidden": false
                },
                {
                    "_id": "68f98627b9b2e4ae04673749",
                    "name": "Stephen H. Bach",
                    "hidden": false
                },
                {
                    "_id": "68f98627b9b2e4ae0467374a",
                    "name": "Vishal Chowdhary",
                    "hidden": false
                },
                {
                    "_id": "68f98627b9b2e4ae0467374b",
                    "name": "Anthony Aue",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T06:42:01.000Z",
            "submittedOnDailyAt": "2025-10-23T00:05:22.073Z",
            "title": "TheMCPCompany: Creating General-purpose Agents with Task-specific Tools",
            "submittedOnDailyBy": {
                "_id": "63316bb618711776b465b788",
                "avatarUrl": "/avatars/ac53c8e81f0b09232eab0ca698e1400c.svg",
                "isPro": false,
                "fullname": "Reza Esfandiarpoor",
                "user": "rezaesfandiarpoor",
                "type": "user"
            },
            "summary": "Since the introduction of the Model Context Protocol (MCP), the number of\navailable tools for Large Language Models (LLMs) has increased significantly.\nThese task-specific tool sets offer an alternative to general-purpose tools\nsuch as web browsers, while being easier to develop and maintain than GUIs.\nHowever, current general-purpose agents predominantly rely on web browsers for\ninteracting with the environment. Here, we introduce TheMCPCompany, a benchmark\nfor evaluating tool-calling agents on tasks that involve interacting with\nvarious real-world services. We use the REST APIs of these services to create\nMCP servers, which include over 18,000 tools. We also provide manually\nannotated ground-truth tools for each task. In our experiments, we use the\nground truth tools to show the potential of tool-calling agents for both\nimproving performance and reducing costs assuming perfect tool retrieval. Next,\nwe explore agent performance using tool retrieval to study the real-world\npracticality of tool-based agents. While all models with tool retrieval perform\nsimilarly or better than browser-based agents, smaller models cannot take full\nadvantage of the available tools through retrieval. On the other hand, GPT-5's\nperformance with tool retrieval is very close to its performance with\nground-truth tools. Overall, our work shows that the most advanced reasoning\nmodels are effective at discovering tools in simpler environments, but\nseriously struggle with navigating complex enterprise environments.\nTheMCPCompany reveals that navigating tens of thousands of tools and combining\nthem in non-trivial ways to solve complex problems is still a challenging task\nfor current models and requires both better reasoning and better retrieval\nmodels.",
            "upvotes": 5,
            "discussionId": "68f98628b9b2e4ae0467374c",
            "githubRepo": "https://github.com/Reza-esfandiarpoor/the-mcp-company",
            "ai_summary": "TheMCPCompany evaluates tool-calling agents using REST APIs for interacting with real-world services, showing that advanced models perform well in simpler environments but struggle with complex enterprise environments.",
            "ai_keywords": [
                "Model Context Protocol",
                "Large Language Models",
                "tool-calling agents",
                "REST APIs",
                "ground-truth tools",
                "tool retrieval",
                "GPT-5",
                "reasoning models",
                "retrieval models"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-10-22T02:42:01.000Z",
        "title": "TheMCPCompany: Creating General-purpose Agents with Task-specific Tools",
        "summary": "Since the introduction of the Model Context Protocol (MCP), the number of\navailable tools for Large Language Models (LLMs) has increased significantly.\nThese task-specific tool sets offer an alternative to general-purpose tools\nsuch as web browsers, while being easier to develop and maintain than GUIs.\nHowever, current general-purpose agents predominantly rely on web browsers for\ninteracting with the environment. Here, we introduce TheMCPCompany, a benchmark\nfor evaluating tool-calling agents on tasks that involve interacting with\nvarious real-world services. We use the REST APIs of these services to create\nMCP servers, which include over 18,000 tools. We also provide manually\nannotated ground-truth tools for each task. In our experiments, we use the\nground truth tools to show the potential of tool-calling agents for both\nimproving performance and reducing costs assuming perfect tool retrieval. Next,\nwe explore agent performance using tool retrieval to study the real-world\npracticality of tool-based agents. While all models with tool retrieval perform\nsimilarly or better than browser-based agents, smaller models cannot take full\nadvantage of the available tools through retrieval. On the other hand, GPT-5's\nperformance with tool retrieval is very close to its performance with\nground-truth tools. Overall, our work shows that the most advanced reasoning\nmodels are effective at discovering tools in simpler environments, but\nseriously struggle with navigating complex enterprise environments.\nTheMCPCompany reveals that navigating tens of thousands of tools and combining\nthem in non-trivial ways to solve complex problems is still a challenging task\nfor current models and requires both better reasoning and better retrieval\nmodels.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19286.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63316bb618711776b465b788",
            "avatarUrl": "/avatars/ac53c8e81f0b09232eab0ca698e1400c.svg",
            "fullname": "Reza Esfandiarpoor",
            "name": "rezaesfandiarpoor",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.19127",
            "authors": [
                {
                    "_id": "68fa75c0f158a71c5a2f56d6",
                    "name": "Daniel Zhao",
                    "hidden": false
                },
                {
                    "_id": "68fa75c0f158a71c5a2f56d7",
                    "name": "Daniel Beaglehole",
                    "hidden": false
                },
                {
                    "_id": "68fa75c0f158a71c5a2f56d8",
                    "name": "Taylor Berg-Kirkpatrick",
                    "hidden": false
                },
                {
                    "_id": "68fa75c0f158a71c5a2f56d9",
                    "name": "Julian McAuley",
                    "hidden": false
                },
                {
                    "_id": "68fa75c0f158a71c5a2f56da",
                    "user": {
                        "_id": "643060c6cb3fe707b24c53a2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643060c6cb3fe707b24c53a2/MIoM9hrX0vV4XRyrm-4Kz.jpeg",
                        "isPro": false,
                        "fullname": "Zachary Novack",
                        "user": "ZacharyNovack",
                        "type": "user"
                    },
                    "name": "Zachary Novack",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T01:06:59.534Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/643060c6cb3fe707b24c53a2/j8D_GS6qAu_j1QnrlaOcK.qt",
                "https://cdn-uploads.huggingface.co/production/uploads/643060c6cb3fe707b24c53a2/n1ivIZ62DY7y4kCHquOio.qt"
            ],
            "publishedAt": "2025-10-21T23:23:14.000Z",
            "submittedOnDailyAt": "2025-10-23T17:46:56.887Z",
            "title": "Steering Autoregressive Music Generation with Recursive Feature Machines",
            "submittedOnDailyBy": {
                "_id": "643060c6cb3fe707b24c53a2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643060c6cb3fe707b24c53a2/MIoM9hrX0vV4XRyrm-4Kz.jpeg",
                "isPro": false,
                "fullname": "Zachary Novack",
                "user": "ZacharyNovack",
                "type": "user"
            },
            "summary": "Controllable music generation remains a significant challenge, with existing\nmethods often requiring model retraining or introducing audible artifacts. We\nintroduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs)\nto enable fine-grained, interpretable control over frozen, pre-trained music\nmodels by directly steering their internal activations. RFMs analyze a model's\ninternal gradients to produce interpretable \"concept directions\", or specific\naxes in the activation space that correspond to musical attributes like notes\nor chords. We first train lightweight RFM probes to discover these directions\nwithin MusicGen's hidden states; then, during inference, we inject them back\ninto the model to guide the generation process in real-time without per-step\noptimization. We present advanced mechanisms for this control, including\ndynamic, time-varying schedules and methods for the simultaneous enforcement of\nmultiple musical properties. Our method successfully navigates the trade-off\nbetween control and generation quality: we can increase the accuracy of\ngenerating a target musical note from 0.23 to 0.82, while text prompt adherence\nremains within approximately 0.02 of the unsteered baseline, demonstrating\neffective control with minimal impact on prompt fidelity. We release code to\nencourage further exploration on RFMs in the music domain.",
            "upvotes": 5,
            "discussionId": "68fa75c0f158a71c5a2f56db",
            "ai_summary": "MusicRFM uses Recursive Feature Machines to enable real-time, fine-grained control over pre-trained music models by steering their internal activations, improving musical note accuracy with minimal impact on prompt fidelity.",
            "ai_keywords": [
                "Recursive Feature Machines",
                "RFMs",
                "concept directions",
                "activation space",
                "MusicGen",
                "hidden states",
                "dynamic schedules",
                "simultaneous enforcement",
                "musical properties",
                "text prompt adherence"
            ]
        },
        "publishedAt": "2025-10-21T19:23:14.000Z",
        "title": "Steering Autoregressive Music Generation with Recursive Feature Machines",
        "summary": "Controllable music generation remains a significant challenge, with existing\nmethods often requiring model retraining or introducing audible artifacts. We\nintroduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs)\nto enable fine-grained, interpretable control over frozen, pre-trained music\nmodels by directly steering their internal activations. RFMs analyze a model's\ninternal gradients to produce interpretable \"concept directions\", or specific\naxes in the activation space that correspond to musical attributes like notes\nor chords. We first train lightweight RFM probes to discover these directions\nwithin MusicGen's hidden states; then, during inference, we inject them back\ninto the model to guide the generation process in real-time without per-step\noptimization. We present advanced mechanisms for this control, including\ndynamic, time-varying schedules and methods for the simultaneous enforcement of\nmultiple musical properties. Our method successfully navigates the trade-off\nbetween control and generation quality: we can increase the accuracy of\ngenerating a target musical note from 0.23 to 0.82, while text prompt adherence\nremains within approximately 0.02 of the unsteered baseline, demonstrating\neffective control with minimal impact on prompt fidelity. We release code to\nencourage further exploration on RFMs in the music domain.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/643060c6cb3fe707b24c53a2/j8D_GS6qAu_j1QnrlaOcK.qt",
            "https://cdn-uploads.huggingface.co/production/uploads/643060c6cb3fe707b24c53a2/n1ivIZ62DY7y4kCHquOio.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19127.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643060c6cb3fe707b24c53a2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643060c6cb3fe707b24c53a2/MIoM9hrX0vV4XRyrm-4Kz.jpeg",
            "fullname": "Zachary Novack",
            "name": "ZacharyNovack",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.18941",
            "authors": [
                {
                    "_id": "68f972efb9b2e4ae04673706",
                    "name": "Zhilin Wang",
                    "hidden": false
                },
                {
                    "_id": "68f972efb9b2e4ae04673707",
                    "name": "Jaehun Jung",
                    "hidden": false
                },
                {
                    "_id": "68f972efb9b2e4ae04673708",
                    "name": "Ximing Lu",
                    "hidden": false
                },
                {
                    "_id": "68f972efb9b2e4ae04673709",
                    "name": "Shizhe Diao",
                    "hidden": false
                },
                {
                    "_id": "68f972efb9b2e4ae0467370a",
                    "name": "Ellie Evans",
                    "hidden": false
                },
                {
                    "_id": "68f972efb9b2e4ae0467370b",
                    "name": "Jiaqi Zeng",
                    "hidden": false
                },
                {
                    "_id": "68f972efb9b2e4ae0467370c",
                    "name": "Pavlo Molchanov",
                    "hidden": false
                },
                {
                    "_id": "68f972efb9b2e4ae0467370d",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "68f972efb9b2e4ae0467370e",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "68f972efb9b2e4ae0467370f",
                    "name": "Yi Dong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T17:59:44.000Z",
            "submittedOnDailyAt": "2025-10-23T02:05:26.971Z",
            "title": "ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to\n  Answer and Judge",
            "submittedOnDailyBy": {
                "_id": "633bd54b00732349209a18fe",
                "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
                "isPro": false,
                "fullname": "Shizhe Diao",
                "user": "shizhediao",
                "type": "user"
            },
            "summary": "Evaluating progress in large language models (LLMs) is often constrained by\nthe challenge of verifying responses, limiting assessments to tasks like\nmathematics, programming, and short-form question-answering. However, many\nreal-world applications require evaluating LLMs in processing professional\ndocuments, synthesizing information, and generating comprehensive reports in\nresponse to user queries. We introduce ProfBench: a set of over 7000\nresponse-criterion pairs as evaluated by human-experts with professional\nknowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We\nbuild robust and affordable LLM-Judges to evaluate ProfBench rubrics, by\nmitigating self-enhancement bias and reducing the cost of evaluation by 2-3\norders of magnitude, to make it fair and accessible to the broader community.\nOur findings reveal that ProfBench poses significant challenges even for\nstate-of-the-art LLMs, with top-performing models like GPT-5-high achieving\nonly 65.9\\% overall performance. Furthermore, we identify notable performance\ndisparities between proprietary and open-weight models and provide insights\ninto the role that extended thinking plays in addressing complex,\nprofessional-domain tasks. Data:\nhttps://huggingface.co/datasets/nvidia/ProfBench and Code:\nhttps://github.com/NVlabs/ProfBench",
            "upvotes": 5,
            "discussionId": "68f972efb9b2e4ae04673710",
            "ai_summary": "ProfBench evaluates large language models in professional domains using human-expert criteria, revealing challenges and performance disparities between proprietary and open-weight models.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "ProfBench",
                "response-criterion pairs",
                "human-experts",
                "LLM-Judges",
                "self-enhancement bias",
                "GPT-5-high",
                "extended thinking",
                "professional-domain tasks"
            ],
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-10-21T13:59:44.000Z",
        "title": "ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to\n  Answer and Judge",
        "summary": "Evaluating progress in large language models (LLMs) is often constrained by\nthe challenge of verifying responses, limiting assessments to tasks like\nmathematics, programming, and short-form question-answering. However, many\nreal-world applications require evaluating LLMs in processing professional\ndocuments, synthesizing information, and generating comprehensive reports in\nresponse to user queries. We introduce ProfBench: a set of over 7000\nresponse-criterion pairs as evaluated by human-experts with professional\nknowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We\nbuild robust and affordable LLM-Judges to evaluate ProfBench rubrics, by\nmitigating self-enhancement bias and reducing the cost of evaluation by 2-3\norders of magnitude, to make it fair and accessible to the broader community.\nOur findings reveal that ProfBench poses significant challenges even for\nstate-of-the-art LLMs, with top-performing models like GPT-5-high achieving\nonly 65.9\\% overall performance. Furthermore, we identify notable performance\ndisparities between proprietary and open-weight models and provide insights\ninto the role that extended thinking plays in addressing complex,\nprofessional-domain tasks. Data:\nhttps://huggingface.co/datasets/nvidia/ProfBench and Code:\nhttps://github.com/NVlabs/ProfBench",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18941.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "633bd54b00732349209a18fe",
            "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
            "fullname": "Shizhe Diao",
            "name": "shizhediao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18940",
            "authors": [
                {
                    "_id": "68f998bdb9b2e4ae0467382f",
                    "user": {
                        "_id": "62b8e6adaf1addc58cfcc578",
                        "avatarUrl": "/avatars/3f2b190eea73bd2bf2b4ddec1014edf5.svg",
                        "isPro": false,
                        "fullname": "ZhiZhang",
                        "user": "ZhiZhang",
                        "type": "user"
                    },
                    "name": "Zhi Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:48:14.251Z",
                    "hidden": false
                },
                {
                    "_id": "68f998bdb9b2e4ae04673830",
                    "name": "Yixian Shen",
                    "hidden": false
                },
                {
                    "_id": "68f998bdb9b2e4ae04673831",
                    "name": "Congfeng Cao",
                    "hidden": false
                },
                {
                    "_id": "68f998bdb9b2e4ae04673832",
                    "name": "Ekaterina Shutova",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62b8e6adaf1addc58cfcc578/yBex6P35yrg7PiO8WMIsQ.png"
            ],
            "publishedAt": "2025-10-21T17:59:24.000Z",
            "submittedOnDailyAt": "2025-10-23T01:42:36.167Z",
            "title": "NeuroAda: Activating Each Neuron's Potential for Parameter-Efficient\n  Fine-Tuning",
            "submittedOnDailyBy": {
                "_id": "62b8e6adaf1addc58cfcc578",
                "avatarUrl": "/avatars/3f2b190eea73bd2bf2b4ddec1014edf5.svg",
                "isPro": false,
                "fullname": "ZhiZhang",
                "user": "ZhiZhang",
                "type": "user"
            },
            "summary": "Existing parameter-efficient fine-tuning (PEFT) methods primarily fall into\ntwo categories: addition-based and selective in-situ adaptation. The former,\nsuch as LoRA, introduce additional modules to adapt the model to downstream\ntasks, offering strong memory efficiency. However, their representational\ncapacity is often limited, making them less suitable for fine-grained\nadaptation. In contrast, the latter directly fine-tunes a carefully chosen\nsubset of the original model parameters, allowing for more precise and\neffective adaptation, but at the cost of significantly increased memory\nconsumption. To reconcile this trade-off, we propose NeuroAda, a novel PEFT\nmethod that enables fine-grained model finetuning while maintaining high memory\nefficiency. Our approach first identifies important parameters (i.e.,\nconnections within the network) as in selective adaptation, and then introduces\nbypass connections for these selected parameters. During finetuning, only the\nbypass connections are updated, leaving the original model parameters frozen.\nEmpirical results on 23+ tasks spanning both natural language generation and\nunderstanding demonstrate that NeuroAda achieves state-of-the-art performance\nwith as little as leq 0.02% trainable parameters, while reducing\nCUDA memory usage by up to 60%. We release our code here:\nhttps://github.com/FightingFighting/NeuroAda.git.",
            "upvotes": 5,
            "discussionId": "68f998bdb9b2e4ae04673833",
            "githubRepo": "https://github.com/\nFightingFighting/NeuroAda.git",
            "ai_summary": "NeuroAda is a parameter-efficient fine-tuning method that combines selective adaptation with bypass connections to achieve high performance with minimal trainable parameters and reduced memory usage.",
            "ai_keywords": [
                "parameter-efficient fine-tuning",
                "PEFT",
                "LoRA",
                "addition-based",
                "selective in-situ adaptation",
                "NeuroAda",
                "important parameters",
                "bypass connections",
                "natural language generation",
                "natural language understanding"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "6274e45cbe455dadd1063972",
                "name": "uva",
                "fullname": "University of Amsterdam",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1651827662266-6273a78c3d70b36612a8bd9e.png"
            }
        },
        "publishedAt": "2025-10-21T13:59:24.000Z",
        "title": "NeuroAda: Activating Each Neuron's Potential for Parameter-Efficient\n  Fine-Tuning",
        "summary": "Existing parameter-efficient fine-tuning (PEFT) methods primarily fall into\ntwo categories: addition-based and selective in-situ adaptation. The former,\nsuch as LoRA, introduce additional modules to adapt the model to downstream\ntasks, offering strong memory efficiency. However, their representational\ncapacity is often limited, making them less suitable for fine-grained\nadaptation. In contrast, the latter directly fine-tunes a carefully chosen\nsubset of the original model parameters, allowing for more precise and\neffective adaptation, but at the cost of significantly increased memory\nconsumption. To reconcile this trade-off, we propose NeuroAda, a novel PEFT\nmethod that enables fine-grained model finetuning while maintaining high memory\nefficiency. Our approach first identifies important parameters (i.e.,\nconnections within the network) as in selective adaptation, and then introduces\nbypass connections for these selected parameters. During finetuning, only the\nbypass connections are updated, leaving the original model parameters frozen.\nEmpirical results on 23+ tasks spanning both natural language generation and\nunderstanding demonstrate that NeuroAda achieves state-of-the-art performance\nwith as little as leq 0.02% trainable parameters, while reducing\nCUDA memory usage by up to 60%. We release our code here:\nhttps://github.com/FightingFighting/NeuroAda.git.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62b8e6adaf1addc58cfcc578/yBex6P35yrg7PiO8WMIsQ.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18940.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62b8e6adaf1addc58cfcc578",
            "avatarUrl": "/avatars/3f2b190eea73bd2bf2b4ddec1014edf5.svg",
            "fullname": "ZhiZhang",
            "name": "ZhiZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "6274e45cbe455dadd1063972",
            "name": "uva",
            "fullname": "University of Amsterdam",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1651827662266-6273a78c3d70b36612a8bd9e.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.17932",
            "authors": [
                {
                    "_id": "68f867b07669bcaeecce0e2d",
                    "name": "Jiahao Tang",
                    "hidden": false
                },
                {
                    "_id": "68f867b07669bcaeecce0e2e",
                    "name": "Henry Hengyuan Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f867b07669bcaeecce0e2f",
                    "name": "Lijian Wu",
                    "hidden": false
                },
                {
                    "_id": "68f867b07669bcaeecce0e30",
                    "name": "Yifei Tao",
                    "hidden": false
                },
                {
                    "_id": "68f867b07669bcaeecce0e31",
                    "name": "Dongxing Mao",
                    "hidden": false
                },
                {
                    "_id": "68f867b07669bcaeecce0e32",
                    "name": "Yang Wan",
                    "hidden": false
                },
                {
                    "_id": "68f867b07669bcaeecce0e33",
                    "name": "Jingru Tan",
                    "hidden": false
                },
                {
                    "_id": "68f867b07669bcaeecce0e34",
                    "name": "Min Zeng",
                    "hidden": false
                },
                {
                    "_id": "68f867b07669bcaeecce0e35",
                    "name": "Min Li",
                    "hidden": false
                },
                {
                    "_id": "68f867b07669bcaeecce0e36",
                    "name": "Alex Jinpeng Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T15:11:56.000Z",
            "submittedOnDailyAt": "2025-10-23T04:00:16.699Z",
            "title": "From Charts to Code: A Hierarchical Benchmark for Multimodal Models",
            "submittedOnDailyBy": {
                "_id": "647d7eb9770c299e56f5b39b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647d7eb9770c299e56f5b39b/CC5JJgkyLkXOxw-BeT4G5.jpeg",
                "isPro": false,
                "fullname": "Henry Hengyuan Zhao",
                "user": "hhenryz",
                "type": "user"
            },
            "summary": "We introduce Chart2Code, a new benchmark for evaluating the chart\nunderstanding and code generation capabilities of large multimodal models\n(LMMs). Chart2Code is explicitly designed from a user-driven perspective,\ncapturing diverse real-world scenarios and progressively increasing task\ndifficulty. It consists of three levels: Level 1 (Chart Reproduction)\nreproduces charts from a reference figure and user query; Level 2 (Chart\nEditing) involves complex modifications such as changing chart types or adding\nelements; and Level 3 (Long-Table to Chart Generation) requires models to\ntransform long, information-dense tables into faithful charts following user\ninstructions. To our knowledge, this is the first hierarchical benchmark that\nreflects practical chart2code usage while systematically scaling task\ncomplexity. In total, Chart2Code contains 2,023 tasks across 22 chart types,\npaired with multi-level evaluation metrics that assess both code correctness\nand the visual fidelity of rendered charts. We benchmark 25 state-of-the-art\n(SoTA) LMMs, including both proprietary and the latest open-source models such\nas GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental\nresults demonstrate that even the SoTA model GPT-5 averages only 0.57 on\ncode-based evaluation and 0.22 on chart-quality assessment across the editing\ntasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark\nwill drive advances in multimodal reasoning and foster the development of more\nrobust and general-purpose LMMs. Our code and data are available on Chart2Code.",
            "upvotes": 5,
            "discussionId": "68f867b17669bcaeecce0e37",
            "projectPage": "https://csu-jpg.github.io/Chart2Code.github.io/",
            "githubRepo": "https://github.com/CSU-JPG/Chart2Code",
            "ai_summary": "Chart2Code is a hierarchical benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models, featuring three levels of increasing complexity and diverse real-world scenarios.",
            "ai_keywords": [
                "multimodal models",
                "LMMs",
                "Chart2Code",
                "Chart Reproduction",
                "Chart Editing",
                "Long-Table to Chart Generation",
                "hierarchical benchmark",
                "code correctness",
                "visual fidelity",
                "GPT-5",
                "Qwen2.5-VL",
                "InternVL3/3.5",
                "MiMo-VL",
                "Seed-1.6-VL"
            ],
            "githubStars": 8
        },
        "publishedAt": "2025-10-20T11:11:56.000Z",
        "title": "From Charts to Code: A Hierarchical Benchmark for Multimodal Models",
        "summary": "We introduce Chart2Code, a new benchmark for evaluating the chart\nunderstanding and code generation capabilities of large multimodal models\n(LMMs). Chart2Code is explicitly designed from a user-driven perspective,\ncapturing diverse real-world scenarios and progressively increasing task\ndifficulty. It consists of three levels: Level 1 (Chart Reproduction)\nreproduces charts from a reference figure and user query; Level 2 (Chart\nEditing) involves complex modifications such as changing chart types or adding\nelements; and Level 3 (Long-Table to Chart Generation) requires models to\ntransform long, information-dense tables into faithful charts following user\ninstructions. To our knowledge, this is the first hierarchical benchmark that\nreflects practical chart2code usage while systematically scaling task\ncomplexity. In total, Chart2Code contains 2,023 tasks across 22 chart types,\npaired with multi-level evaluation metrics that assess both code correctness\nand the visual fidelity of rendered charts. We benchmark 25 state-of-the-art\n(SoTA) LMMs, including both proprietary and the latest open-source models such\nas GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental\nresults demonstrate that even the SoTA model GPT-5 averages only 0.57 on\ncode-based evaluation and 0.22 on chart-quality assessment across the editing\ntasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark\nwill drive advances in multimodal reasoning and foster the development of more\nrobust and general-purpose LMMs. Our code and data are available on Chart2Code.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17932.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647d7eb9770c299e56f5b39b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647d7eb9770c299e56f5b39b/CC5JJgkyLkXOxw-BeT4G5.jpeg",
            "fullname": "Henry Hengyuan Zhao",
            "name": "hhenryz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.19457",
            "authors": [
                {
                    "_id": "68f98c94b9b2e4ae046737a2",
                    "user": {
                        "_id": "65745569839aa08899ea5d27",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
                        "isPro": false,
                        "fullname": "kailinjiang",
                        "user": "kailinjiang",
                        "type": "user"
                    },
                    "name": "Kailin Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:50:08.809Z",
                    "hidden": false
                },
                {
                    "_id": "68f98c94b9b2e4ae046737a3",
                    "name": "Ning Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f98c94b9b2e4ae046737a4",
                    "name": "Yuchen Ren",
                    "hidden": false
                },
                {
                    "_id": "68f98c94b9b2e4ae046737a5",
                    "name": "Yuchen Li",
                    "hidden": false
                },
                {
                    "_id": "68f98c94b9b2e4ae046737a6",
                    "name": "Yifan Gao",
                    "hidden": false
                },
                {
                    "_id": "68f98c94b9b2e4ae046737a7",
                    "name": "Jinhe Bi",
                    "hidden": false
                },
                {
                    "_id": "68f98c94b9b2e4ae046737a8",
                    "name": "Yunpu Ma",
                    "hidden": false
                },
                {
                    "_id": "68f98c94b9b2e4ae046737a9",
                    "name": "Qingqing Liu",
                    "hidden": false
                },
                {
                    "_id": "68f98c94b9b2e4ae046737aa",
                    "name": "Xianhao Wang",
                    "hidden": false
                },
                {
                    "_id": "68f98c94b9b2e4ae046737ab",
                    "name": "Yifan Jia",
                    "hidden": false
                },
                {
                    "_id": "68f98c94b9b2e4ae046737ac",
                    "name": "Hongbo Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f98c94b9b2e4ae046737ad",
                    "name": "Yaocong Hu",
                    "hidden": false
                },
                {
                    "_id": "68f98c94b9b2e4ae046737ae",
                    "name": "Bin Li",
                    "hidden": false
                },
                {
                    "_id": "68f98c94b9b2e4ae046737af",
                    "name": "Lei Liu",
                    "hidden": false
                },
                {
                    "_id": "68f98c94b9b2e4ae046737b0",
                    "name": "Yuntao Du",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T10:41:57.000Z",
            "submittedOnDailyAt": "2025-10-23T00:36:11.231Z",
            "title": "MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for\n  Large Multimodal Models",
            "submittedOnDailyBy": {
                "_id": "65745569839aa08899ea5d27",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
                "isPro": false,
                "fullname": "kailinjiang",
                "user": "kailinjiang",
                "type": "user"
            },
            "summary": "Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal\npre-training, yet their static representations struggle to maintain an accurate\nunderstanding of time-sensitive factual knowledge. Existing benchmarks remain\nconstrained by static designs, inadequately evaluating LMMs' ability to\nunderstand time-sensitive knowledge. To address this gap, we propose MINED, a\ncomprehensive benchmark that evaluates temporal awareness along 6 key\ndimensions and 11 challenging tasks: cognition, awareness, trustworthiness,\nunderstanding, reasoning, and robustness. MINED is constructed from Wikipedia\nby two professional annotators, containing 2,104 time-sensitive knowledge\nsamples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED\nshows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07,\nwhile most open-source LMMs still lack time understanding ability. Meanwhile,\nLMMs perform best on organization knowledge, whereas their performance is\nweakest on sport. To address these challenges, we investigate the feasibility\nof updating time-sensitive knowledge in LMMs through knowledge editing methods\nand observe that LMMs can effectively update knowledge via knowledge editing\nmethods in single editing scenarios.",
            "upvotes": 4,
            "discussionId": "68f98c94b9b2e4ae046737b1",
            "projectPage": "https://mined-lmm.github.io/"
        },
        "publishedAt": "2025-10-22T06:41:57.000Z",
        "title": "MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for\n  Large Multimodal Models",
        "summary": "Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal\npre-training, yet their static representations struggle to maintain an accurate\nunderstanding of time-sensitive factual knowledge. Existing benchmarks remain\nconstrained by static designs, inadequately evaluating LMMs' ability to\nunderstand time-sensitive knowledge. To address this gap, we propose MINED, a\ncomprehensive benchmark that evaluates temporal awareness along 6 key\ndimensions and 11 challenging tasks: cognition, awareness, trustworthiness,\nunderstanding, reasoning, and robustness. MINED is constructed from Wikipedia\nby two professional annotators, containing 2,104 time-sensitive knowledge\nsamples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED\nshows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07,\nwhile most open-source LMMs still lack time understanding ability. Meanwhile,\nLMMs perform best on organization knowledge, whereas their performance is\nweakest on sport. To address these challenges, we investigate the feasibility\nof updating time-sensitive knowledge in LMMs through knowledge editing methods\nand observe that LMMs can effectively update knowledge via knowledge editing\nmethods in single editing scenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19457.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65745569839aa08899ea5d27",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
            "fullname": "kailinjiang",
            "name": "kailinjiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.18917",
            "authors": [
                {
                    "_id": "68f980bcb9b2e4ae04673722",
                    "user": {
                        "_id": "67d43aeb6ffb8add49ea6712",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d43aeb6ffb8add49ea6712/0h4bV3Ptgh_zTRUHVqwe2.jpeg",
                        "isPro": false,
                        "fullname": "Mandip Goswami",
                        "user": "mandipgoswami",
                        "type": "user"
                    },
                    "name": "Mandip Goswami",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:56:42.707Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T06:53:14.000Z",
            "submittedOnDailyAt": "2025-10-23T01:32:06.704Z",
            "title": "RIR-Mega: a large-scale simulated room impulse response dataset for\n  machine learning and room acoustics modeling",
            "submittedOnDailyBy": {
                "_id": "67d43aeb6ffb8add49ea6712",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d43aeb6ffb8add49ea6712/0h4bV3Ptgh_zTRUHVqwe2.jpeg",
                "isPro": false,
                "fullname": "Mandip Goswami",
                "user": "mandipgoswami",
                "type": "user"
            },
            "summary": "Room impulse responses are a core resource for dereverberation, robust speech\nrecognition, source localization, and room acoustics estimation. We present\nRIR-Mega, a large collection of simulated RIRs described by a compact, machine\nfriendly metadata schema and distributed with simple tools for validation and\nreuse. The dataset ships with a Hugging Face Datasets loader, scripts for\nmetadata checks and checksums, and a reference regression baseline that\npredicts RT60 like targets from waveforms. On a train and validation split of\n36,000 and 4,000 examples, a small Random Forest on lightweight time and\nspectral features reaches a mean absolute error near 0.013 s and a root mean\nsquare error near 0.022 s. We host a subset with 1,000 linear array RIRs and\n3,000 circular array RIRs on Hugging Face for streaming and quick tests, and\npreserve the complete 50,000 RIR archive on Zenodo. The dataset and code are\npublic to support reproducible studies.",
            "upvotes": 4,
            "discussionId": "68f980bcb9b2e4ae04673723",
            "projectPage": "https://doi.org/10.5281/zenodo.17387402",
            "githubRepo": "https://github.com/mandip42/rirmega",
            "ai_summary": "RIR-Mega is a large dataset of simulated room impulse responses with tools for validation and a baseline model for predicting RT60 from waveforms.",
            "ai_keywords": [
                "Hugging Face Datasets",
                "Random Forest",
                "time features",
                "spectral features",
                "RT60",
                "room impulse responses"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-10-21T02:53:14.000Z",
        "title": "RIR-Mega: a large-scale simulated room impulse response dataset for\n  machine learning and room acoustics modeling",
        "summary": "Room impulse responses are a core resource for dereverberation, robust speech\nrecognition, source localization, and room acoustics estimation. We present\nRIR-Mega, a large collection of simulated RIRs described by a compact, machine\nfriendly metadata schema and distributed with simple tools for validation and\nreuse. The dataset ships with a Hugging Face Datasets loader, scripts for\nmetadata checks and checksums, and a reference regression baseline that\npredicts RT60 like targets from waveforms. On a train and validation split of\n36,000 and 4,000 examples, a small Random Forest on lightweight time and\nspectral features reaches a mean absolute error near 0.013 s and a root mean\nsquare error near 0.022 s. We host a subset with 1,000 linear array RIRs and\n3,000 circular array RIRs on Hugging Face for streaming and quick tests, and\npreserve the complete 50,000 RIR archive on Zenodo. The dataset and code are\npublic to support reproducible studies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18917.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67d43aeb6ffb8add49ea6712",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d43aeb6ffb8add49ea6712/0h4bV3Ptgh_zTRUHVqwe2.jpeg",
            "fullname": "Mandip Goswami",
            "name": "mandipgoswami",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.18428",
            "authors": [
                {
                    "_id": "68f998c9b9b2e4ae04673835",
                    "name": "Minwei Kong",
                    "hidden": false
                },
                {
                    "_id": "68f998c9b9b2e4ae04673836",
                    "name": "Ao Qu",
                    "hidden": false
                },
                {
                    "_id": "68f998c9b9b2e4ae04673837",
                    "name": "Xiaotong Guo",
                    "hidden": false
                },
                {
                    "_id": "68f998c9b9b2e4ae04673838",
                    "name": "Wenbin Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68f998c9b9b2e4ae04673839",
                    "name": "Chonghe Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f998c9b9b2e4ae0467383a",
                    "name": "Han Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f998c9b9b2e4ae0467383b",
                    "name": "Yining Ma",
                    "hidden": false
                },
                {
                    "_id": "68f998c9b9b2e4ae0467383c",
                    "name": "Dingyi Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68f998c9b9b2e4ae0467383d",
                    "name": "Yuhan Tang",
                    "hidden": false
                },
                {
                    "_id": "68f998c9b9b2e4ae0467383e",
                    "name": "Junyi Li",
                    "hidden": false
                },
                {
                    "_id": "68f998c9b9b2e4ae0467383f",
                    "name": "Hai Wang",
                    "hidden": false
                },
                {
                    "_id": "68f998c9b9b2e4ae04673840",
                    "name": "Cathy Wu",
                    "hidden": false
                },
                {
                    "_id": "68f998c9b9b2e4ae04673841",
                    "name": "Jinhua Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T09:03:26.000Z",
            "submittedOnDailyAt": "2025-10-23T01:25:54.240Z",
            "title": "AlphaOPT: Formulating Optimization Programs with Self-Improving LLM\n  Experience Library",
            "submittedOnDailyBy": {
                "_id": "6556223bb3bb6c3a41809f7e",
                "avatarUrl": "/avatars/ea7640aa9ae4fe35a2e245b3b6f8c0ff.svg",
                "isPro": true,
                "fullname": "Ao Qu",
                "user": "quao627",
                "type": "user"
            },
            "summary": "Optimization modeling enables critical decisions across industries but\nremains difficult to automate: informal language must be mapped to precise\nmathematical formulations and executable solver code. Prior LLM approaches\neither rely on brittle prompting or costly retraining with limited\ngeneralization. We present AlphaOPT, a self-improving experience library that\nenables an LLM to learn from limited demonstrations (even answers alone,\nwithout gold-standard programs) and solver feedback - without annotated\nreasoning traces or parameter updates. AlphaOPT operates in a continual\ntwo-phase cycle: (i) a Library Learning phase that reflects on failed attempts,\nextracting solver-verified, structured insights as {taxonomy, condition,\nexplanation, example}; and (ii) a Library Evolution phase that diagnoses\nretrieval misalignments and refines the applicability conditions of stored\ninsights, improving transfer across tasks. This design (1) learns efficiently\nfrom limited demonstrations without curated rationales, (2) expands continually\nwithout costly retraining by updating the library rather than model weights,\nand (3) makes knowledge explicit and interpretable for human inspection and\nintervention. Experiments show that AlphaOPT steadily improves with more data\n(65% to 72% from 100 to 300 training items) and surpasses the strongest\nbaseline by 7.7% on the out-of-distribution OptiBench dataset when trained only\non answers. Code and data are available at:\nhttps://github.com/Minw913/AlphaOPT.",
            "upvotes": 3,
            "discussionId": "68f998cab9b2e4ae04673842",
            "ai_summary": "AlphaOPT is a self-improving library that enables an LLM to learn from limited demonstrations and solver feedback, improving optimization modeling across industries without costly retraining.",
            "ai_keywords": [
                "LLM",
                "AlphaOPT",
                "Library Learning",
                "Library Evolution",
                "solver-verified",
                "structured insights",
                "taxonomy",
                "condition",
                "explanation",
                "example",
                "retrieval misalignments",
                "applicability conditions",
                "transfer across tasks",
                "OptiBench dataset"
            ]
        },
        "publishedAt": "2025-10-21T05:03:26.000Z",
        "title": "AlphaOPT: Formulating Optimization Programs with Self-Improving LLM\n  Experience Library",
        "summary": "Optimization modeling enables critical decisions across industries but\nremains difficult to automate: informal language must be mapped to precise\nmathematical formulations and executable solver code. Prior LLM approaches\neither rely on brittle prompting or costly retraining with limited\ngeneralization. We present AlphaOPT, a self-improving experience library that\nenables an LLM to learn from limited demonstrations (even answers alone,\nwithout gold-standard programs) and solver feedback - without annotated\nreasoning traces or parameter updates. AlphaOPT operates in a continual\ntwo-phase cycle: (i) a Library Learning phase that reflects on failed attempts,\nextracting solver-verified, structured insights as {taxonomy, condition,\nexplanation, example}; and (ii) a Library Evolution phase that diagnoses\nretrieval misalignments and refines the applicability conditions of stored\ninsights, improving transfer across tasks. This design (1) learns efficiently\nfrom limited demonstrations without curated rationales, (2) expands continually\nwithout costly retraining by updating the library rather than model weights,\nand (3) makes knowledge explicit and interpretable for human inspection and\nintervention. Experiments show that AlphaOPT steadily improves with more data\n(65% to 72% from 100 to 300 training items) and surpasses the strongest\nbaseline by 7.7% on the out-of-distribution OptiBench dataset when trained only\non answers. Code and data are available at:\nhttps://github.com/Minw913/AlphaOPT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18428.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6556223bb3bb6c3a41809f7e",
            "avatarUrl": "/avatars/ea7640aa9ae4fe35a2e245b3b6f8c0ff.svg",
            "fullname": "Ao Qu",
            "name": "quao627",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18909",
            "authors": [
                {
                    "_id": "68f9b1efb9b2e4ae0467390d",
                    "name": "Hongyi He",
                    "hidden": false
                },
                {
                    "_id": "68f9b1efb9b2e4ae0467390e",
                    "name": "Xiao Liu",
                    "hidden": false
                },
                {
                    "_id": "68f9b1efb9b2e4ae0467390f",
                    "name": "Zhenghao Lin",
                    "hidden": false
                },
                {
                    "_id": "68f9b1efb9b2e4ae04673910",
                    "name": "Mingni Tang",
                    "hidden": false
                },
                {
                    "_id": "68f9b1efb9b2e4ae04673911",
                    "name": "Yi Cheng",
                    "hidden": false
                },
                {
                    "_id": "68f9b1efb9b2e4ae04673912",
                    "name": "Jintao Wang",
                    "hidden": false
                },
                {
                    "_id": "68f9b1efb9b2e4ae04673913",
                    "name": "Wenjie Li",
                    "hidden": false
                },
                {
                    "_id": "68f9b1efb9b2e4ae04673914",
                    "name": "Peng Cheng",
                    "hidden": false
                },
                {
                    "_id": "68f9b1efb9b2e4ae04673915",
                    "name": "Yeyun Gong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T03:37:31.000Z",
            "submittedOnDailyAt": "2025-10-23T03:11:38.991Z",
            "title": "Learning from the Best, Differently: A Diversity-Driven Rethinking on\n  Data Selection",
            "submittedOnDailyBy": {
                "_id": "63fb6e281b4b1bd4e7ffc5be",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fb6e281b4b1bd4e7ffc5be/aiRu_bulgnxvEMrjipGoQ.jpeg",
                "isPro": false,
                "fullname": "Xiao Liu",
                "user": "lx865712528",
                "type": "user"
            },
            "summary": "High-quality pre-training data is crutial for large language models, where\nquality captures factual reliability and semantic value, and diversity ensures\nbroad coverage and distributional heterogeneity. Existing approaches typically\nrely on single or multiple-dimensional score-based selection. However, directly\nselecting top-scored data often degrades performance, and sampling from a\nbroader range is required to recover results. The above non-monotonicity\nbetween dataset scores and downstream benchmark results reveals a fundamental\nbias: score-based methods collapse correlated dimensions, causing top-scored\ndata to appear high-quality while systematically overlooking diversity. We\nargue that ensuring diversity requires decomposing correlated metrics into\northogonal feature dimensions, from which the top-scored data can be directly\nselected. Therefore, we proposed the Orthogonal Diversity-Aware Selection\n(ODiS) algorithm, which preserves both quality and diversity during data\nselection. First, ODiS evaluates data from multiple dimensions, covering\nlanguage quality, knowledge quality, and comprehension difficulty. The\nmulti-dimensional scores are then decorrelated via Principal Component Analysis\n(PCA), yielding orthogonal evaluation dimensions. For each dimension, a\nRoberta-based scorer is trained to regress the data onto PCA-projected scores,\nenabling scalable inference on large corpora. Finally, ODiS constructs the\ntraining dataset by selecting top-scored data within each orthogonal dimension,\nthereby ensuring both quality and diversity. Empirical results show that\nODiS-selected data exhibit less than 2\\% inter-dimension overlap, confirming\northogonality between dimensions. More importantly, models trained with\nODiS-selected data significantly outperform other baselines on downstream\nbenchmarks, highlighting the necessity of orthogonal, diversity-aware data\nselection for LLMs.",
            "upvotes": 3,
            "discussionId": "68f9b1efb9b2e4ae04673916",
            "ai_summary": "The Orthogonal Diversity-Aware Selection (ODiS) algorithm enhances large language model performance by ensuring both quality and diversity in training data through orthogonal decomposition of evaluation dimensions.",
            "ai_keywords": [
                "Orthogonal Diversity-Aware Selection",
                "ODiS",
                "Principal Component Analysis",
                "PCA",
                "Roberta-based scorer",
                "language quality",
                "knowledge quality",
                "comprehension difficulty",
                "large language models",
                "LLMs"
            ]
        },
        "publishedAt": "2025-10-20T23:37:31.000Z",
        "title": "Learning from the Best, Differently: A Diversity-Driven Rethinking on\n  Data Selection",
        "summary": "High-quality pre-training data is crutial for large language models, where\nquality captures factual reliability and semantic value, and diversity ensures\nbroad coverage and distributional heterogeneity. Existing approaches typically\nrely on single or multiple-dimensional score-based selection. However, directly\nselecting top-scored data often degrades performance, and sampling from a\nbroader range is required to recover results. The above non-monotonicity\nbetween dataset scores and downstream benchmark results reveals a fundamental\nbias: score-based methods collapse correlated dimensions, causing top-scored\ndata to appear high-quality while systematically overlooking diversity. We\nargue that ensuring diversity requires decomposing correlated metrics into\northogonal feature dimensions, from which the top-scored data can be directly\nselected. Therefore, we proposed the Orthogonal Diversity-Aware Selection\n(ODiS) algorithm, which preserves both quality and diversity during data\nselection. First, ODiS evaluates data from multiple dimensions, covering\nlanguage quality, knowledge quality, and comprehension difficulty. The\nmulti-dimensional scores are then decorrelated via Principal Component Analysis\n(PCA), yielding orthogonal evaluation dimensions. For each dimension, a\nRoberta-based scorer is trained to regress the data onto PCA-projected scores,\nenabling scalable inference on large corpora. Finally, ODiS constructs the\ntraining dataset by selecting top-scored data within each orthogonal dimension,\nthereby ensuring both quality and diversity. Empirical results show that\nODiS-selected data exhibit less than 2\\% inter-dimension overlap, confirming\northogonality between dimensions. More importantly, models trained with\nODiS-selected data significantly outperform other baselines on downstream\nbenchmarks, highlighting the necessity of orthogonal, diversity-aware data\nselection for LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18909.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63fb6e281b4b1bd4e7ffc5be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fb6e281b4b1bd4e7ffc5be/aiRu_bulgnxvEMrjipGoQ.jpeg",
            "fullname": "Xiao Liu",
            "name": "lx865712528",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18091",
            "authors": [
                {
                    "_id": "68f9f0aeb9b2e4ae04673b2d",
                    "name": "Rohan Choudhury",
                    "hidden": false
                },
                {
                    "_id": "68f9f0aeb9b2e4ae04673b2e",
                    "name": "JungEun Kim",
                    "hidden": false
                },
                {
                    "_id": "68f9f0aeb9b2e4ae04673b2f",
                    "name": "Jinhyung Park",
                    "hidden": false
                },
                {
                    "_id": "68f9f0aeb9b2e4ae04673b30",
                    "name": "Eunho Yang",
                    "hidden": false
                },
                {
                    "_id": "68f9f0aeb9b2e4ae04673b31",
                    "name": "Lszl A. Jeni",
                    "hidden": false
                },
                {
                    "_id": "68f9f0aeb9b2e4ae04673b32",
                    "name": "Kris M. Kitani",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T20:37:11.000Z",
            "submittedOnDailyAt": "2025-10-23T19:17:18.998Z",
            "title": "Accelerating Vision Transformers with Adaptive Patch Sizes",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": true,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "Vision Transformers (ViTs) partition input images into uniformly sized\npatches regardless of their content, resulting in long input sequence lengths\nfor high-resolution images. We present Adaptive Patch Transformers (APT), which\naddresses this by using multiple different patch sizes within the same image.\nAPT reduces the total number of input tokens by allocating larger patch sizes\nin more homogeneous areas and smaller patches in more complex ones. APT\nachieves a drastic speedup in ViT inference and training, increasing throughput\nby 40% on ViT-L and 50% on ViT-H while maintaining downstream performance, and\ncan be applied to a previously fine-tuned ViT, converging in as little as 1\nepoch. It also significantly reduces training and inference time without loss\nof performance in high-resolution dense visual tasks, achieving up to 30\\%\nfaster training and inference in visual QA, object detection, and semantic\nsegmentation.",
            "upvotes": 3,
            "discussionId": "68f9f0aeb9b2e4ae04673b33",
            "githubRepo": "https://github.com/rccchoudhury/apt",
            "ai_summary": "Adaptive Patch Transformers (APT) improve Vision Transformer (ViT) efficiency by using variable patch sizes, enhancing speed without compromising performance.",
            "ai_keywords": [
                "Vision Transformers",
                "Adaptive Patch Transformers",
                "patch sizes",
                "input tokens",
                "throughput",
                "inference",
                "training",
                "visual QA",
                "object detection",
                "semantic segmentation"
            ],
            "githubStars": 12,
            "organization": {
                "_id": "6362c6c180c1a705a6ed0d57",
                "name": "CMU-SCS",
                "fullname": "Carnegie Mellon University School of Computer Science"
            }
        },
        "publishedAt": "2025-10-20T16:37:11.000Z",
        "title": "Accelerating Vision Transformers with Adaptive Patch Sizes",
        "summary": "Vision Transformers (ViTs) partition input images into uniformly sized\npatches regardless of their content, resulting in long input sequence lengths\nfor high-resolution images. We present Adaptive Patch Transformers (APT), which\naddresses this by using multiple different patch sizes within the same image.\nAPT reduces the total number of input tokens by allocating larger patch sizes\nin more homogeneous areas and smaller patches in more complex ones. APT\nachieves a drastic speedup in ViT inference and training, increasing throughput\nby 40% on ViT-L and 50% on ViT-H while maintaining downstream performance, and\ncan be applied to a previously fine-tuned ViT, converging in as little as 1\nepoch. It also significantly reduces training and inference time without loss\nof performance in high-resolution dense visual tasks, achieving up to 30\\%\nfaster training and inference in visual QA, object detection, and semantic\nsegmentation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18091.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1002
        },
        "organization": {
            "_id": "6362c6c180c1a705a6ed0d57",
            "name": "CMU-SCS",
            "fullname": "Carnegie Mellon University School of Computer Science"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.20168",
            "authors": [
                {
                    "_id": "68fad42ef158a71c5a2f57d0",
                    "name": "Tian Lan",
                    "hidden": false
                },
                {
                    "_id": "68fad42ef158a71c5a2f57d1",
                    "name": "Bin Zhu",
                    "hidden": false
                },
                {
                    "_id": "68fad42ef158a71c5a2f57d2",
                    "name": "Qianghuai Jia",
                    "hidden": false
                },
                {
                    "_id": "68fad42ef158a71c5a2f57d3",
                    "name": "Junyang Ren",
                    "hidden": false
                },
                {
                    "_id": "68fad42ef158a71c5a2f57d4",
                    "name": "Haijun Li",
                    "hidden": false
                },
                {
                    "_id": "68fad42ef158a71c5a2f57d5",
                    "name": "Longyue Wang",
                    "hidden": false
                },
                {
                    "_id": "68fad42ef158a71c5a2f57d6",
                    "name": "Zhao Xu",
                    "hidden": false
                },
                {
                    "_id": "68fad42ef158a71c5a2f57d7",
                    "name": "Weihua Luo",
                    "hidden": false
                },
                {
                    "_id": "68fad42ef158a71c5a2f57d8",
                    "name": "Kaifu Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T03:28:45.000Z",
            "submittedOnDailyAt": "2025-10-23T23:52:46.668Z",
            "title": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information\n  Seeking",
            "submittedOnDailyBy": {
                "_id": "627ca439a09edfe62239c671",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1652335660508-noauth.jpeg",
                "isPro": false,
                "fullname": "Tian Lan",
                "user": "GMFTBY",
                "type": "user"
            },
            "summary": "Current search agents fundamentally lack the ability to simultaneously\nperform deep reasoning over multi-hop retrieval and\nwide-scale information collection-a critical deficiency for real-world\napplications like comprehensive market analysis and business development. To\nbridge this gap, we introduce DeepWideSearch, the first benchmark explicitly\ndesigned to evaluate agents to integrate depth and width in information\nseeking. In DeepWideSearch, agents must process a large volume of data, each\nrequiring deep reasoning over multi-hop retrieval paths. Specifically, we\npropose two methods to converse established datasets, resulting in a curated\ncollection of 220 questions spanning 15 diverse domains. Extensive experiments\ndemonstrate that even state-of-the-art agents achieve only 2.39% average\nsuccess rate on DeepWideSearch, highlighting the substantial challenge of\nintegrating depth and width search in information-seeking tasks. Furthermore,\nour error analysis reveals four failure modes: lack of reflection, overreliance\non internal knowledge, insufficient retrieval, and context overflow-exposing\nkey limitations in current agent architectures. We publicly release\nDeepWideSearch to catalyze future research on more capable and robust\ninformation-seeking agents.",
            "upvotes": 2,
            "discussionId": "68fad42ff158a71c5a2f57d9",
            "ai_summary": "DeepWideSearch is a benchmark that evaluates agents' ability to integrate deep reasoning and wide-scale information collection, revealing significant challenges and limitations in current agent architectures.",
            "ai_keywords": [
                "deep reasoning",
                "multi-hop retrieval",
                "wide-scale information collection",
                "DeepWideSearch",
                "benchmark",
                "information-seeking tasks",
                "reflection",
                "internal knowledge",
                "retrieval",
                "context overflow"
            ],
            "organization": {
                "_id": "6662a91edd706a226d18cc5a",
                "name": "AIDC-AI",
                "fullname": "AIDC-AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/666a9d46a638e57bb7907929/CRc-9MCuH2q9hjTScyTPE.png"
            }
        },
        "publishedAt": "2025-10-22T23:28:45.000Z",
        "title": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information\n  Seeking",
        "summary": "Current search agents fundamentally lack the ability to simultaneously\nperform deep reasoning over multi-hop retrieval and\nwide-scale information collection-a critical deficiency for real-world\napplications like comprehensive market analysis and business development. To\nbridge this gap, we introduce DeepWideSearch, the first benchmark explicitly\ndesigned to evaluate agents to integrate depth and width in information\nseeking. In DeepWideSearch, agents must process a large volume of data, each\nrequiring deep reasoning over multi-hop retrieval paths. Specifically, we\npropose two methods to converse established datasets, resulting in a curated\ncollection of 220 questions spanning 15 diverse domains. Extensive experiments\ndemonstrate that even state-of-the-art agents achieve only 2.39% average\nsuccess rate on DeepWideSearch, highlighting the substantial challenge of\nintegrating depth and width search in information-seeking tasks. Furthermore,\nour error analysis reveals four failure modes: lack of reflection, overreliance\non internal knowledge, insufficient retrieval, and context overflow-exposing\nkey limitations in current agent architectures. We publicly release\nDeepWideSearch to catalyze future research on more capable and robust\ninformation-seeking agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20168.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "627ca439a09edfe62239c671",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1652335660508-noauth.jpeg",
            "fullname": "Tian Lan",
            "name": "GMFTBY",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "organization": {
            "_id": "6662a91edd706a226d18cc5a",
            "name": "AIDC-AI",
            "fullname": "AIDC-AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/666a9d46a638e57bb7907929/CRc-9MCuH2q9hjTScyTPE.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.19753",
            "authors": [
                {
                    "_id": "68f9b3f2b9b2e4ae04673918",
                    "name": "Qilin Ye",
                    "hidden": false
                },
                {
                    "_id": "68f9b3f2b9b2e4ae04673919",
                    "user": {
                        "_id": "63c8454e46421a2efe82709d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png",
                        "isPro": true,
                        "fullname": "Deqing Fu",
                        "user": "deqing",
                        "type": "user"
                    },
                    "name": "Deqing Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:46:13.568Z",
                    "hidden": false
                },
                {
                    "_id": "68f9b3f2b9b2e4ae0467391a",
                    "name": "Robin Jia",
                    "hidden": false
                },
                {
                    "_id": "68f9b3f2b9b2e4ae0467391b",
                    "name": "Vatsal Sharan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T16:43:32.000Z",
            "submittedOnDailyAt": "2025-10-23T03:21:04.789Z",
            "title": "When Do Transformers Learn Heuristics for Graph Connectivity?",
            "submittedOnDailyBy": {
                "_id": "63c8454e46421a2efe82709d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png",
                "isPro": true,
                "fullname": "Deqing Fu",
                "user": "deqing",
                "type": "user"
            },
            "summary": "Transformers often fail to learn generalizable algorithms, instead relying on\nbrittle heuristics. Using graph connectivity as a testbed, we explain this\nphenomenon both theoretically and empirically. We consider a simplified\nTransformer architecture, the disentangled Transformer, and prove that an\nL-layer model has capacity to solve for graphs with diameters up to exactly\n3^L, implementing an algorithm equivalent to computing powers of the\nadjacency matrix. We analyze the training-dynamics, and show that the learned\nstrategy hinges on whether most training instances are within this model\ncapacity. Within-capacity graphs (diameter leq 3^L) drive the learning of a\ncorrect algorithmic solution while beyond-capacity graphs drive the learning of\na simple heuristic based on node degrees. Finally, we empirically demonstrate\nthat restricting training data within a model's capacity leads to both standard\nand disentangled transformers learning the exact algorithm rather than the\ndegree-based heuristic.",
            "upvotes": 2,
            "discussionId": "68f9b3f3b9b2e4ae0467391c",
            "ai_summary": "Transformers struggle with generalizable algorithms, preferring heuristics; a disentangled Transformer can learn graph algorithms within its capacity but resorts to heuristics otherwise.",
            "ai_keywords": [
                "Transformers",
                "disentangled Transformer",
                "graph connectivity",
                "adjacency matrix",
                "training-dynamics",
                "model capacity",
                "graph diameter",
                "algorithmic solution",
                "degree-based heuristic"
            ]
        },
        "publishedAt": "2025-10-22T12:43:32.000Z",
        "title": "When Do Transformers Learn Heuristics for Graph Connectivity?",
        "summary": "Transformers often fail to learn generalizable algorithms, instead relying on\nbrittle heuristics. Using graph connectivity as a testbed, we explain this\nphenomenon both theoretically and empirically. We consider a simplified\nTransformer architecture, the disentangled Transformer, and prove that an\nL-layer model has capacity to solve for graphs with diameters up to exactly\n3^L, implementing an algorithm equivalent to computing powers of the\nadjacency matrix. We analyze the training-dynamics, and show that the learned\nstrategy hinges on whether most training instances are within this model\ncapacity. Within-capacity graphs (diameter leq 3^L) drive the learning of a\ncorrect algorithmic solution while beyond-capacity graphs drive the learning of\na simple heuristic based on node degrees. Finally, we empirically demonstrate\nthat restricting training data within a model's capacity leads to both standard\nand disentangled transformers learning the exact algorithm rather than the\ndegree-based heuristic.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19753.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63c8454e46421a2efe82709d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png",
            "fullname": "Deqing Fu",
            "name": "deqing",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.19631",
            "authors": [
                {
                    "_id": "68fad2aef158a71c5a2f57c5",
                    "name": "Yiqian Yang",
                    "hidden": false
                },
                {
                    "_id": "68fad2aef158a71c5a2f57c6",
                    "name": "Tian Lan",
                    "hidden": false
                },
                {
                    "_id": "68fad2aef158a71c5a2f57c7",
                    "name": "Qianghuai Jia",
                    "hidden": false
                },
                {
                    "_id": "68fad2aef158a71c5a2f57c8",
                    "name": "Li Zhu",
                    "hidden": false
                },
                {
                    "_id": "68fad2aef158a71c5a2f57c9",
                    "name": "Hui Jiang",
                    "hidden": false
                },
                {
                    "_id": "68fad2aef158a71c5a2f57ca",
                    "name": "Hang Zhu",
                    "hidden": false
                },
                {
                    "_id": "68fad2aef158a71c5a2f57cb",
                    "name": "Longyue Wang",
                    "hidden": false
                },
                {
                    "_id": "68fad2aef158a71c5a2f57cc",
                    "name": "Weihua Luo",
                    "hidden": false
                },
                {
                    "_id": "68fad2aef158a71c5a2f57cd",
                    "name": "Kaifu Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T14:28:33.000Z",
            "submittedOnDailyAt": "2025-10-23T23:47:55.008Z",
            "title": "HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search\n  Agents in Hierarchical Rule Application",
            "submittedOnDailyBy": {
                "_id": "627ca439a09edfe62239c671",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1652335660508-noauth.jpeg",
                "isPro": false,
                "fullname": "Tian Lan",
                "user": "GMFTBY",
                "type": "user"
            },
            "summary": "Effective deep search agents must not only access open-domain and\ndomain-specific knowledge but also apply complex rules-such as legal clauses,\nmedical manuals and tariff rules. These rules often feature vague boundaries\nand implicit logic relationships, making precise application challenging for\nagents. However, this critical capability is largely overlooked by current\nagent benchmarks.\n  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level\ne-commerce benchmark designed to evaluate deep search agents in hierarchical\nrule application. In this task, the deep reasoning process of agents is guided\nby these rules to predict 10-digit Harmonized System Code (HSCode) of products\nwith noisy but realistic descriptions. These codes, established by the World\nCustoms Organization, are vital for global supply chain efficiency. Built from\nreal-world data collected from large-scale e-commerce platforms, our proposed\nHSCodeComp comprises 632 product entries spanning diverse product categories,\nwith these HSCodes annotated by several human experts.\n  Extensive experimental results on several state-of-the-art LLMs, open-source,\nand closed-source agents reveal a huge performance gap: best agent achieves\nonly 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides,\ndetailed analysis demonstrates the challenges of hierarchical rule application,\nand test-time scaling fails to improve performance further.",
            "upvotes": 2,
            "discussionId": "68fad2aef158a71c5a2f57ce",
            "githubRepo": "https://github.com/AIDC-AI/Marco-Search-Agent",
            "ai_summary": "HSCodeComp evaluates deep search agents' hierarchical rule application in predicting product HS Codes, revealing significant performance gaps compared to human experts.",
            "ai_keywords": [
                "deep search agents",
                "hierarchical rule application",
                "Harmonized System Code (HSCode)",
                "global supply chain efficiency",
                "LLMs",
                "open-source agents",
                "closed-source agents",
                "test-time scaling"
            ],
            "organization": {
                "_id": "6662a91edd706a226d18cc5a",
                "name": "AIDC-AI",
                "fullname": "AIDC-AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/666a9d46a638e57bb7907929/CRc-9MCuH2q9hjTScyTPE.png"
            }
        },
        "publishedAt": "2025-10-22T10:28:33.000Z",
        "title": "HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search\n  Agents in Hierarchical Rule Application",
        "summary": "Effective deep search agents must not only access open-domain and\ndomain-specific knowledge but also apply complex rules-such as legal clauses,\nmedical manuals and tariff rules. These rules often feature vague boundaries\nand implicit logic relationships, making precise application challenging for\nagents. However, this critical capability is largely overlooked by current\nagent benchmarks.\n  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level\ne-commerce benchmark designed to evaluate deep search agents in hierarchical\nrule application. In this task, the deep reasoning process of agents is guided\nby these rules to predict 10-digit Harmonized System Code (HSCode) of products\nwith noisy but realistic descriptions. These codes, established by the World\nCustoms Organization, are vital for global supply chain efficiency. Built from\nreal-world data collected from large-scale e-commerce platforms, our proposed\nHSCodeComp comprises 632 product entries spanning diverse product categories,\nwith these HSCodes annotated by several human experts.\n  Extensive experimental results on several state-of-the-art LLMs, open-source,\nand closed-source agents reveal a huge performance gap: best agent achieves\nonly 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides,\ndetailed analysis demonstrates the challenges of hierarchical rule application,\nand test-time scaling fails to improve performance further.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19631.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "627ca439a09edfe62239c671",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1652335660508-noauth.jpeg",
            "fullname": "Tian Lan",
            "name": "GMFTBY",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "organization": {
            "_id": "6662a91edd706a226d18cc5a",
            "name": "AIDC-AI",
            "fullname": "AIDC-AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/666a9d46a638e57bb7907929/CRc-9MCuH2q9hjTScyTPE.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18840",
            "authors": [
                {
                    "_id": "68f9f336b9b2e4ae04673b51",
                    "user": {
                        "_id": "66ebce4f322dcfe976c30460",
                        "avatarUrl": "/avatars/aafb9e5e639861aa711679d76211a44e.svg",
                        "isPro": false,
                        "fullname": "Ling Xing",
                        "user": "ling441",
                        "type": "user"
                    },
                    "name": "Ling Xing",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:44:39.643Z",
                    "hidden": false
                },
                {
                    "_id": "68f9f336b9b2e4ae04673b52",
                    "name": "Alex Jinpeng Wang",
                    "hidden": false
                },
                {
                    "_id": "68f9f336b9b2e4ae04673b53",
                    "name": "Rui Yan",
                    "hidden": false
                },
                {
                    "_id": "68f9f336b9b2e4ae04673b54",
                    "name": "Hongyu Qu",
                    "hidden": false
                },
                {
                    "_id": "68f9f336b9b2e4ae04673b55",
                    "name": "Zechao Li",
                    "hidden": false
                },
                {
                    "_id": "68f9f336b9b2e4ae04673b56",
                    "name": "Jinhui Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T17:34:48.000Z",
            "submittedOnDailyAt": "2025-10-23T07:53:05.198Z",
            "title": "See the Text: From Tokenization to Visual Reading",
            "submittedOnDailyBy": {
                "_id": "66ebce4f322dcfe976c30460",
                "avatarUrl": "/avatars/aafb9e5e639861aa711679d76211a44e.svg",
                "isPro": false,
                "fullname": "Ling Xing",
                "user": "ling441",
                "type": "user"
            },
            "summary": "People see text. Humans read by recognizing words as visual objects,\nincluding their shapes, layouts, and patterns, before connecting them to\nmeaning, which enables us to handle typos, distorted fonts, and various scripts\neffectively. Modern large language models (LLMs), however, rely on subword\ntokenization, fragmenting text into pieces from a fixed vocabulary. While\neffective for high-resource languages, this approach over-segments low-resource\nlanguages, yielding long, linguistically meaningless sequences and inflating\ncomputation. In this work, we challenge this entrenched paradigm and move\ntoward a vision-centric alternative. Our method, SeeTok, renders text as images\n(visual-text) and leverages pretrained multimodal LLMs to interpret them,\nreusing strong OCR and text-vision alignment abilities learned from large-scale\nmultimodal training. Across three different language tasks, SeeTok matches or\nsurpasses subword tokenizers while requiring 4.43 times fewer tokens and\nreducing FLOPs by 70.5%, with additional gains in cross-lingual generalization,\nrobustness to typographic noise, and linguistic hierarchy. SeeTok signals a\nshift from symbolic tokenization to human-like visual reading, and takes a step\ntoward more natural and cognitively inspired language models.",
            "upvotes": 2,
            "discussionId": "68f9f337b9b2e4ae04673b57",
            "ai_summary": "SeeTok, a vision-centric method, renders text as images and uses pretrained multimodal LLMs to interpret them, offering efficiency and robustness improvements over traditional subword tokenization.",
            "ai_keywords": [
                "subword tokenization",
                "SeeTok",
                "visual-text",
                "pretrained multimodal LLMs",
                "OCR",
                "text-vision alignment",
                "cross-lingual generalization",
                "typographic noise",
                "linguistic hierarchy"
            ]
        },
        "publishedAt": "2025-10-21T13:34:48.000Z",
        "title": "See the Text: From Tokenization to Visual Reading",
        "summary": "People see text. Humans read by recognizing words as visual objects,\nincluding their shapes, layouts, and patterns, before connecting them to\nmeaning, which enables us to handle typos, distorted fonts, and various scripts\neffectively. Modern large language models (LLMs), however, rely on subword\ntokenization, fragmenting text into pieces from a fixed vocabulary. While\neffective for high-resource languages, this approach over-segments low-resource\nlanguages, yielding long, linguistically meaningless sequences and inflating\ncomputation. In this work, we challenge this entrenched paradigm and move\ntoward a vision-centric alternative. Our method, SeeTok, renders text as images\n(visual-text) and leverages pretrained multimodal LLMs to interpret them,\nreusing strong OCR and text-vision alignment abilities learned from large-scale\nmultimodal training. Across three different language tasks, SeeTok matches or\nsurpasses subword tokenizers while requiring 4.43 times fewer tokens and\nreducing FLOPs by 70.5%, with additional gains in cross-lingual generalization,\nrobustness to typographic noise, and linguistic hierarchy. SeeTok signals a\nshift from symbolic tokenization to human-like visual reading, and takes a step\ntoward more natural and cognitively inspired language models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18840.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66ebce4f322dcfe976c30460",
            "avatarUrl": "/avatars/aafb9e5e639861aa711679d76211a44e.svg",
            "fullname": "Ling Xing",
            "name": "ling441",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.18279",
            "authors": [
                {
                    "_id": "68f999f8b9b2e4ae04673844",
                    "user": {
                        "_id": "68c855762302522c5693dd92",
                        "avatarUrl": "/avatars/c03c8eba0a21566996502a200f83285b.svg",
                        "isPro": false,
                        "fullname": "Yanhong Li",
                        "user": "yanhong-l",
                        "type": "user"
                    },
                    "name": "Yanhong Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:47:58.299Z",
                    "hidden": false
                },
                {
                    "_id": "68f999f8b9b2e4ae04673845",
                    "name": "Zixuan Lan",
                    "hidden": false
                },
                {
                    "_id": "68f999f8b9b2e4ae04673846",
                    "name": "Jiawei Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T04:07:20.000Z",
            "submittedOnDailyAt": "2025-10-23T20:38:07.751Z",
            "title": "Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text\n  Inputs in Multimodal LLMs",
            "submittedOnDailyBy": {
                "_id": "68c855762302522c5693dd92",
                "avatarUrl": "/avatars/c03c8eba0a21566996502a200f83285b.svg",
                "isPro": false,
                "fullname": "Yanhong Li",
                "user": "yanhong-l",
                "type": "user"
            },
            "summary": "Large language models (LLMs) and their multimodal variants can now process\nvisual inputs, including images of text. This raises an intriguing question:\ncan we compress textual inputs by feeding them as images to reduce token usage\nwhile preserving performance? In this paper, we show that visual text\nrepresentations are a practical and surprisingly effective form of input\ncompression for decoder LLMs. We exploit the idea of rendering long text inputs\nas a single image and provide it directly to the model. This leads to\ndramatically reduced number of decoder tokens required, offering a new form of\ninput compression. Through experiments on two distinct benchmarks RULER\n(long-context retrieval) and CNN/DailyMail (document summarization) we\ndemonstrate that this text-as-image method yields substantial token savings\n(often nearly half) without degrading task performance.",
            "upvotes": 2,
            "discussionId": "68f999f9b9b2e4ae04673847",
            "githubRepo": "https://github.com/yanhong-lbh/text_or_pixels",
            "ai_summary": "Rendering text as images reduces token usage for decoder LLMs without compromising performance on tasks like long-context retrieval and document summarization.",
            "ai_keywords": [
                "large language models",
                "multimodal variants",
                "visual inputs",
                "text-as-image",
                "input compression",
                "decoder tokens",
                "RULER",
                "CNN/DailyMail",
                "token savings"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-10-21T00:07:20.000Z",
        "title": "Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text\n  Inputs in Multimodal LLMs",
        "summary": "Large language models (LLMs) and their multimodal variants can now process\nvisual inputs, including images of text. This raises an intriguing question:\ncan we compress textual inputs by feeding them as images to reduce token usage\nwhile preserving performance? In this paper, we show that visual text\nrepresentations are a practical and surprisingly effective form of input\ncompression for decoder LLMs. We exploit the idea of rendering long text inputs\nas a single image and provide it directly to the model. This leads to\ndramatically reduced number of decoder tokens required, offering a new form of\ninput compression. Through experiments on two distinct benchmarks RULER\n(long-context retrieval) and CNN/DailyMail (document summarization) we\ndemonstrate that this text-as-image method yields substantial token savings\n(often nearly half) without degrading task performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18279.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68c855762302522c5693dd92",
            "avatarUrl": "/avatars/c03c8eba0a21566996502a200f83285b.svg",
            "fullname": "Yanhong Li",
            "name": "yanhong-l",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 0
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.16435",
            "authors": [
                {
                    "_id": "68fa51e9f158a71c5a2f5691",
                    "user": {
                        "_id": "62057eac12ccba127e39ab41",
                        "avatarUrl": "/avatars/0a23de3c4c5fcc3bc6452cedf7597f1b.svg",
                        "isPro": false,
                        "fullname": "Lennart",
                        "user": "lwachowiak",
                        "type": "user"
                    },
                    "name": "Lennart Wachowiak",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T01:06:40.257Z",
                    "hidden": false
                },
                {
                    "_id": "68fa51e9f158a71c5a2f5692",
                    "name": "Andrew Coles",
                    "hidden": false
                },
                {
                    "_id": "68fa51e9f158a71c5a2f5693",
                    "name": "Gerard Canal",
                    "hidden": false
                },
                {
                    "_id": "68fa51e9f158a71c5a2f5694",
                    "name": "Oya Celiktutan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-18T10:16:45.000Z",
            "submittedOnDailyAt": "2025-10-23T15:19:21.467Z",
            "title": "What Questions Should Robots Be Able to Answer? A Dataset of User\n  Questions for Explainable Robotics",
            "submittedOnDailyBy": {
                "_id": "62057eac12ccba127e39ab41",
                "avatarUrl": "/avatars/0a23de3c4c5fcc3bc6452cedf7597f1b.svg",
                "isPro": false,
                "fullname": "Lennart",
                "user": "lwachowiak",
                "type": "user"
            },
            "summary": "With the growing use of large language models and conversational interfaces\nin human-robot interaction, robots' ability to answer user questions is more\nimportant than ever. We therefore introduce a dataset of 1,893 user questions\nfor household robots, collected from 100 participants and organized into 12\ncategories and 70 subcategories. Most work in explainable robotics focuses on\nwhy-questions. In contrast, our dataset provides a wide variety of questions,\nfrom questions about simple execution details to questions about how the robot\nwould act in hypothetical scenarios -- thus giving roboticists valuable\ninsights into what questions their robot needs to be able to answer. To collect\nthe dataset, we created 15 video stimuli and 7 text stimuli, depicting robots\nperforming varied household tasks. We then asked participants on Prolific what\nquestions they would want to ask the robot in each portrayed situation. In the\nfinal dataset, the most frequent categories are questions about task execution\ndetails (22.5%), the robot's capabilities (12.7%), and performance assessments\n(11.3%). Although questions about how robots would handle potentially difficult\nscenarios and ensure correct behavior are less frequent, users rank them as the\nmost important for robots to be able to answer. Moreover, we find that users\nwho identify as novices in robotics ask different questions than more\nexperienced users. Novices are more likely to inquire about simple facts, such\nas what the robot did or the current state of the environment. As robots enter\nenvironments shared with humans and language becomes central to giving\ninstructions and interaction, this dataset provides a valuable foundation for\n(i) identifying the information robots need to log and expose to conversational\ninterfaces, (ii) benchmarking question-answering modules, and (iii) designing\nexplanation strategies that align with user expectations.",
            "upvotes": 2,
            "discussionId": "68fa51e9f158a71c5a2f5695",
            "ai_summary": "A dataset of user questions for household robots is introduced, providing insights into the types of questions users ask and the information robots need to answer, supporting the development of conversational interfaces and explanation strategies.",
            "ai_keywords": [
                ""
            ]
        },
        "publishedAt": "2025-10-18T06:16:45.000Z",
        "title": "What Questions Should Robots Be Able to Answer? A Dataset of User\n  Questions for Explainable Robotics",
        "summary": "With the growing use of large language models and conversational interfaces\nin human-robot interaction, robots' ability to answer user questions is more\nimportant than ever. We therefore introduce a dataset of 1,893 user questions\nfor household robots, collected from 100 participants and organized into 12\ncategories and 70 subcategories. Most work in explainable robotics focuses on\nwhy-questions. In contrast, our dataset provides a wide variety of questions,\nfrom questions about simple execution details to questions about how the robot\nwould act in hypothetical scenarios -- thus giving roboticists valuable\ninsights into what questions their robot needs to be able to answer. To collect\nthe dataset, we created 15 video stimuli and 7 text stimuli, depicting robots\nperforming varied household tasks. We then asked participants on Prolific what\nquestions they would want to ask the robot in each portrayed situation. In the\nfinal dataset, the most frequent categories are questions about task execution\ndetails (22.5%), the robot's capabilities (12.7%), and performance assessments\n(11.3%). Although questions about how robots would handle potentially difficult\nscenarios and ensure correct behavior are less frequent, users rank them as the\nmost important for robots to be able to answer. Moreover, we find that users\nwho identify as novices in robotics ask different questions than more\nexperienced users. Novices are more likely to inquire about simple facts, such\nas what the robot did or the current state of the environment. As robots enter\nenvironments shared with humans and language becomes central to giving\ninstructions and interaction, this dataset provides a valuable foundation for\n(i) identifying the information robots need to log and expose to conversational\ninterfaces, (ii) benchmarking question-answering modules, and (iii) designing\nexplanation strategies that align with user expectations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16435.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62057eac12ccba127e39ab41",
            "avatarUrl": "/avatars/0a23de3c4c5fcc3bc6452cedf7597f1b.svg",
            "fullname": "Lennart",
            "name": "lwachowiak",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.15015",
            "authors": [
                {
                    "_id": "68fa18a3f158a71c5a2f55cf",
                    "name": "Mor Ventura",
                    "hidden": false
                },
                {
                    "_id": "68fa18a3f158a71c5a2f55d0",
                    "name": "Michael Toker",
                    "hidden": false
                },
                {
                    "_id": "68fa18a3f158a71c5a2f55d1",
                    "name": "Or Patashnik",
                    "hidden": false
                },
                {
                    "_id": "68fa18a3f158a71c5a2f55d2",
                    "name": "Yonatan Belinkov",
                    "hidden": false
                },
                {
                    "_id": "68fa18a3f158a71c5a2f55d3",
                    "name": "Roi Reichart",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:39:21.000Z",
            "submittedOnDailyAt": "2025-10-23T10:30:50.983Z",
            "title": "DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage\n  Mitigation in Text-to-Image Models",
            "submittedOnDailyBy": {
                "_id": "623f4bd2e801e8c1e59d948e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648315730065-623f4bd2e801e8c1e59d948e.jpeg",
                "isPro": false,
                "fullname": "Mor Ventura",
                "user": "MorVentura",
                "type": "user"
            },
            "summary": "Text-to-Image (T2I) models have advanced rapidly, yet they remain vulnerable\nto semantic leakage, the unintended transfer of semantically related features\nbetween distinct entities. Existing mitigation strategies are often\noptimization-based or dependent on external inputs. We introduce DeLeaker, a\nlightweight, optimization-free inference-time approach that mitigates leakage\nby directly intervening on the model's attention maps. Throughout the diffusion\nprocess, DeLeaker dynamically reweights attention maps to suppress excessive\ncross-entity interactions while strengthening the identity of each entity. To\nsupport systematic evaluation, we introduce SLIM (Semantic Leakage in IMages),\nthe first dataset dedicated to semantic leakage, comprising 1,130\nhuman-verified samples spanning diverse scenarios, together with a novel\nautomatic evaluation framework. Experiments demonstrate that DeLeaker\nconsistently outperforms all baselines, even when they are provided with\nexternal information, achieving effective leakage mitigation without\ncompromising fidelity or quality. These results underscore the value of\nattention control and pave the way for more semantically precise T2I models.",
            "upvotes": 2,
            "discussionId": "68fa18a4f158a71c5a2f55d4",
            "ai_summary": "DeLeaker mitigates semantic leakage in text-to-image models by dynamically reweighting attention maps during the diffusion process, outperforming existing methods without compromising quality.",
            "ai_keywords": [
                "text-to-image models",
                "semantic leakage",
                "attention maps",
                "diffusion process",
                "SLIM",
                "automatic evaluation framework"
            ]
        },
        "publishedAt": "2025-10-16T13:39:21.000Z",
        "title": "DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage\n  Mitigation in Text-to-Image Models",
        "summary": "Text-to-Image (T2I) models have advanced rapidly, yet they remain vulnerable\nto semantic leakage, the unintended transfer of semantically related features\nbetween distinct entities. Existing mitigation strategies are often\noptimization-based or dependent on external inputs. We introduce DeLeaker, a\nlightweight, optimization-free inference-time approach that mitigates leakage\nby directly intervening on the model's attention maps. Throughout the diffusion\nprocess, DeLeaker dynamically reweights attention maps to suppress excessive\ncross-entity interactions while strengthening the identity of each entity. To\nsupport systematic evaluation, we introduce SLIM (Semantic Leakage in IMages),\nthe first dataset dedicated to semantic leakage, comprising 1,130\nhuman-verified samples spanning diverse scenarios, together with a novel\nautomatic evaluation framework. Experiments demonstrate that DeLeaker\nconsistently outperforms all baselines, even when they are provided with\nexternal information, achieving effective leakage mitigation without\ncompromising fidelity or quality. These results underscore the value of\nattention control and pave the way for more semantically precise T2I models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15015.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "623f4bd2e801e8c1e59d948e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648315730065-623f4bd2e801e8c1e59d948e.jpeg",
            "fullname": "Mor Ventura",
            "name": "MorVentura",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.19492",
            "authors": [
                {
                    "_id": "68f9ab5bb9b2e4ae046738bb",
                    "name": "Ryuto Koike",
                    "hidden": false
                },
                {
                    "_id": "68f9ab5bb9b2e4ae046738bc",
                    "user": {
                        "_id": "6509e1e01b3694179dee256e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sUbok2yloLElaebkps3lT.png",
                        "isPro": false,
                        "fullname": "Liam Dugan",
                        "user": "liamdugan",
                        "type": "user"
                    },
                    "name": "Liam Dugan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:47:10.561Z",
                    "hidden": false
                },
                {
                    "_id": "68f9ab5bb9b2e4ae046738bd",
                    "name": "Masahiro Kaneko",
                    "hidden": false
                },
                {
                    "_id": "68f9ab5bb9b2e4ae046738be",
                    "name": "Chris Callison-Burch",
                    "hidden": false
                },
                {
                    "_id": "68f9ab5bb9b2e4ae046738bf",
                    "name": "Naoaki Okazaki",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T11:39:01.000Z",
            "submittedOnDailyAt": "2025-10-23T02:45:24.799Z",
            "title": "Machine Text Detectors are Membership Inference Attacks",
            "submittedOnDailyBy": {
                "_id": "6509e1e01b3694179dee256e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sUbok2yloLElaebkps3lT.png",
                "isPro": false,
                "fullname": "Liam Dugan",
                "user": "liamdugan",
                "type": "user"
            },
            "summary": "Although membership inference attacks (MIAs) and machine-generated text\ndetection target different goals, identifying training samples and synthetic\ntexts, their methods often exploit similar signals based on a language model's\nprobability distribution. Despite this shared methodological foundation, the\ntwo tasks have been independently studied, which may lead to conclusions that\noverlook stronger methods and valuable insights developed in the other task. In\nthis work, we theoretically and empirically investigate the transferability,\ni.e., how well a method originally developed for one task performs on the\nother, between MIAs and machine text detection. For our theoretical\ncontribution, we prove that the metric that achieves the asymptotically highest\nperformance on both tasks is the same. We unify a large proportion of the\nexisting literature in the context of this optimal metric and hypothesize that\nthe accuracy with which a given method approximates this metric is directly\ncorrelated with its transferability. Our large-scale empirical experiments,\nincluding 7 state-of-the-art MIA methods and 5 state-of-the-art machine text\ndetectors across 13 domains and 10 generators, demonstrate very strong rank\ncorrelation (rho > 0.6) in cross-task performance. We notably find that\nBinoculars, originally designed for machine text detection, achieves\nstate-of-the-art performance on MIA benchmarks as well, demonstrating the\npractical impact of the transferability. Our findings highlight the need for\ngreater cross-task awareness and collaboration between the two research\ncommunities. To facilitate cross-task developments and fair evaluations, we\nintroduce MINT, a unified evaluation suite for MIAs and machine-generated text\ndetection, with implementation of 15 recent methods from both tasks.",
            "upvotes": 1,
            "discussionId": "68f9ab5bb9b2e4ae046738c0",
            "githubRepo": "https://github.com/ryuryukke/mint",
            "ai_summary": "Theoretical and empirical investigation shows strong transferability between membership inference attacks and machine-generated text detection, highlighting the need for cross-task collaboration and introducing MINT for unified evaluation.",
            "ai_keywords": [
                "membership inference attacks",
                "machine-generated text detection",
                "language model",
                "probability distribution",
                "transferability",
                "Binoculars",
                "MINT"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-10-22T07:39:01.000Z",
        "title": "Machine Text Detectors are Membership Inference Attacks",
        "summary": "Although membership inference attacks (MIAs) and machine-generated text\ndetection target different goals, identifying training samples and synthetic\ntexts, their methods often exploit similar signals based on a language model's\nprobability distribution. Despite this shared methodological foundation, the\ntwo tasks have been independently studied, which may lead to conclusions that\noverlook stronger methods and valuable insights developed in the other task. In\nthis work, we theoretically and empirically investigate the transferability,\ni.e., how well a method originally developed for one task performs on the\nother, between MIAs and machine text detection. For our theoretical\ncontribution, we prove that the metric that achieves the asymptotically highest\nperformance on both tasks is the same. We unify a large proportion of the\nexisting literature in the context of this optimal metric and hypothesize that\nthe accuracy with which a given method approximates this metric is directly\ncorrelated with its transferability. Our large-scale empirical experiments,\nincluding 7 state-of-the-art MIA methods and 5 state-of-the-art machine text\ndetectors across 13 domains and 10 generators, demonstrate very strong rank\ncorrelation (rho > 0.6) in cross-task performance. We notably find that\nBinoculars, originally designed for machine text detection, achieves\nstate-of-the-art performance on MIA benchmarks as well, demonstrating the\npractical impact of the transferability. Our findings highlight the need for\ngreater cross-task awareness and collaboration between the two research\ncommunities. To facilitate cross-task developments and fair evaluations, we\nintroduce MINT, a unified evaluation suite for MIAs and machine-generated text\ndetection, with implementation of 15 recent methods from both tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19492.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6509e1e01b3694179dee256e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sUbok2yloLElaebkps3lT.png",
            "fullname": "Liam Dugan",
            "name": "liamdugan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.18034",
            "authors": [
                {
                    "_id": "68fa831af158a71c5a2f570a",
                    "name": "Roberto Brusnicki",
                    "hidden": false
                },
                {
                    "_id": "68fa831af158a71c5a2f570b",
                    "name": "David Pop",
                    "hidden": false
                },
                {
                    "_id": "68fa831af158a71c5a2f570c",
                    "user": {
                        "_id": "672288dd7055eec76d377268",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/672288dd7055eec76d377268/bBQ51VwKZJIkHkNffuEuJ.jpeg",
                        "isPro": true,
                        "fullname": "Yuan Gao",
                        "user": "Yuan-avs",
                        "type": "user"
                    },
                    "name": "Yuan Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T01:06:21.063Z",
                    "hidden": false
                },
                {
                    "_id": "68fa831af158a71c5a2f570d",
                    "name": "Mattia Piccinini",
                    "hidden": false
                },
                {
                    "_id": "68fa831af158a71c5a2f570e",
                    "name": "Johannes Betz",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T19:14:29.000Z",
            "submittedOnDailyAt": "2025-10-23T18:04:54.346Z",
            "title": "SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection",
            "submittedOnDailyBy": {
                "_id": "672288dd7055eec76d377268",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/672288dd7055eec76d377268/bBQ51VwKZJIkHkNffuEuJ.jpeg",
                "isPro": true,
                "fullname": "Yuan Gao",
                "user": "Yuan-avs",
                "type": "user"
            },
            "summary": "Autonomous driving systems remain critically vulnerable to the long-tail of\nrare, out-of-distribution scenarios with semantic anomalies. While Vision\nLanguage Models (VLMs) offer promising reasoning capabilities, naive prompting\napproaches yield unreliable performance and depend on expensive proprietary\nmodels, limiting practical deployment. We introduce SAVANT (Semantic Analysis\nwith Vision-Augmented Anomaly deTection), a structured reasoning framework that\nachieves high accuracy and recall in detecting anomalous driving scenarios from\ninput images through layered scene analysis and a two-phase pipeline:\nstructured scene description extraction followed by multi-modal evaluation. Our\napproach transforms VLM reasoning from ad-hoc prompting to systematic analysis\nacross four semantic layers: Street, Infrastructure, Movable Objects, and\nEnvironment. SAVANT achieves 89.6% recall and 88.0% accuracy on real-world\ndriving scenarios, significantly outperforming unstructured baselines. More\nimportantly, we demonstrate that our structured framework enables a fine-tuned\n7B parameter open-source model (Qwen2.5VL) to achieve 90.8% recall and 93.8%\naccuracy - surpassing all models evaluated while enabling local deployment at\nnear-zero cost. By automatically labeling over 9,640 real-world images with\nhigh accuracy, SAVANT addresses the critical data scarcity problem in anomaly\ndetection and provides a practical path toward reliable, accessible semantic\nmonitoring for autonomous systems.",
            "upvotes": 0,
            "discussionId": "68fa831af158a71c5a2f570f",
            "ai_summary": "SAVANT, a structured reasoning framework using VLMs, achieves high accuracy and recall in detecting anomalous driving scenarios through layered scene analysis and multi-modal evaluation.",
            "ai_keywords": [
                "Vision Language Models",
                "structured reasoning framework",
                "layered scene analysis",
                "multi-modal evaluation",
                "semantic layers",
                "Street",
                "Infrastructure",
                "Movable Objects",
                "Environment",
                "anomaly detection",
                "data scarcity",
                "semantic monitoring"
            ]
        },
        "publishedAt": "2025-10-20T15:14:29.000Z",
        "title": "SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection",
        "summary": "Autonomous driving systems remain critically vulnerable to the long-tail of\nrare, out-of-distribution scenarios with semantic anomalies. While Vision\nLanguage Models (VLMs) offer promising reasoning capabilities, naive prompting\napproaches yield unreliable performance and depend on expensive proprietary\nmodels, limiting practical deployment. We introduce SAVANT (Semantic Analysis\nwith Vision-Augmented Anomaly deTection), a structured reasoning framework that\nachieves high accuracy and recall in detecting anomalous driving scenarios from\ninput images through layered scene analysis and a two-phase pipeline:\nstructured scene description extraction followed by multi-modal evaluation. Our\napproach transforms VLM reasoning from ad-hoc prompting to systematic analysis\nacross four semantic layers: Street, Infrastructure, Movable Objects, and\nEnvironment. SAVANT achieves 89.6% recall and 88.0% accuracy on real-world\ndriving scenarios, significantly outperforming unstructured baselines. More\nimportantly, we demonstrate that our structured framework enables a fine-tuned\n7B parameter open-source model (Qwen2.5VL) to achieve 90.8% recall and 93.8%\naccuracy - surpassing all models evaluated while enabling local deployment at\nnear-zero cost. By automatically labeling over 9,640 real-world images with\nhigh accuracy, SAVANT addresses the critical data scarcity problem in anomaly\ndetection and provides a practical path toward reliable, accessible semantic\nmonitoring for autonomous systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18034.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "672288dd7055eec76d377268",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/672288dd7055eec76d377268/bBQ51VwKZJIkHkNffuEuJ.jpeg",
            "fullname": "Yuan Gao",
            "name": "Yuan-avs",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]