[
    {
        "paper": {
            "id": "2510.19600",
            "authors": [
                {
                    "_id": "68faed18f158a71c5a2f5883",
                    "user": {
                        "_id": "6448b2f53e7b3c11be684348",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg",
                        "isPro": true,
                        "fullname": "Qianli Ma",
                        "user": "Mqleet",
                        "type": "user"
                    },
                    "name": "Qianli Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:01:53.631Z",
                    "hidden": false
                },
                {
                    "_id": "68faed18f158a71c5a2f5884",
                    "name": "Siyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68faed18f158a71c5a2f5885",
                    "name": "Yilin Chen",
                    "hidden": false
                },
                {
                    "_id": "68faed18f158a71c5a2f5886",
                    "name": "Yinhao Tang",
                    "hidden": false
                },
                {
                    "_id": "68faed18f158a71c5a2f5887",
                    "name": "Yixiang Yang",
                    "hidden": false
                },
                {
                    "_id": "68faed18f158a71c5a2f5888",
                    "name": "Chang Guo",
                    "hidden": false
                },
                {
                    "_id": "68faed18f158a71c5a2f5889",
                    "name": "Bingjie Gao",
                    "hidden": false
                },
                {
                    "_id": "68faed18f158a71c5a2f588a",
                    "name": "Zhening Xing",
                    "hidden": false
                },
                {
                    "_id": "68faed18f158a71c5a2f588b",
                    "name": "Yanan Sun",
                    "hidden": false
                },
                {
                    "_id": "68faed18f158a71c5a2f588c",
                    "name": "Zhipeng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T13:53:57.000Z",
            "submittedOnDailyAt": "2025-10-24T01:37:25.642Z",
            "title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1",
            "submittedOnDailyBy": {
                "_id": "6448b2f53e7b3c11be684348",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg",
                "isPro": true,
                "fullname": "Qianli Ma",
                "user": "Mqleet",
                "type": "user"
            },
            "summary": "In the quest for scientific progress, communicating research is as vital as\nthe discovery itself. Yet, researchers are often sidetracked by the manual,\nrepetitive chore of building project webpages to make their dense papers\naccessible. While automation has tackled static slides and posters, the\ndynamic, interactive nature of webpages has remained an unaddressed challenge.\nTo bridge this gap, we reframe the problem, arguing that the solution lies not\nin a single command, but in a collaborative, hierarchical process. We introduce\nAutoPage, a novel multi-agent system that embodies this philosophy.\nAutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline\nfrom narrative planning to multimodal content generation and interactive\nrendering. To combat AI hallucination, dedicated \"Checker\" agents verify each\nstep against the source paper, while optional human checkpoints ensure the\nfinal product aligns perfectly with the author's vision, transforming the\nsystem from a mere tool into a powerful collaborative assistant. To rigorously\nvalidate our approach, we also construct PageBench, the first\nbenchmark for this new task. Experiments show AutoPage not only generates\nhigh-quality, visually appealing pages but does so with remarkable efficiency\nin under 15 minutes for less than \\0.1. Code and dataset will be released at\nhttps://mqleet.github.io/AutoPage_ProjectPage/{Webpage}$.",
            "upvotes": 55,
            "discussionId": "68faed18f158a71c5a2f588d",
            "projectPage": "https://mqleet.github.io/AutoPage_ProjectPage",
            "githubRepo": "https://github.com/AutoLab-SAI-SJTU/AutoPage",
            "ai_summary": "AutoPage, a multi-agent system, automates the creation of interactive research webpages through a hierarchical process, ensuring high-quality and efficient results.",
            "ai_keywords": [
                "multi-agent system",
                "narrative planning",
                "multimodal content generation",
                "interactive rendering",
                "AI hallucination",
                "Checker agents",
                "human checkpoints",
                "PageBench",
                "benchmark"
            ],
            "githubStars": 82,
            "organization": {
                "_id": "68ee0edd23dc954f7744ac27",
                "name": "AutoLab-SJTU",
                "fullname": "AutoLab"
            }
        },
        "publishedAt": "2025-10-22T09:53:57.000Z",
        "title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1",
        "summary": "In the quest for scientific progress, communicating research is as vital as\nthe discovery itself. Yet, researchers are often sidetracked by the manual,\nrepetitive chore of building project webpages to make their dense papers\naccessible. While automation has tackled static slides and posters, the\ndynamic, interactive nature of webpages has remained an unaddressed challenge.\nTo bridge this gap, we reframe the problem, arguing that the solution lies not\nin a single command, but in a collaborative, hierarchical process. We introduce\nAutoPage, a novel multi-agent system that embodies this philosophy.\nAutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline\nfrom narrative planning to multimodal content generation and interactive\nrendering. To combat AI hallucination, dedicated \"Checker\" agents verify each\nstep against the source paper, while optional human checkpoints ensure the\nfinal product aligns perfectly with the author's vision, transforming the\nsystem from a mere tool into a powerful collaborative assistant. To rigorously\nvalidate our approach, we also construct PageBench, the first\nbenchmark for this new task. Experiments show AutoPage not only generates\nhigh-quality, visually appealing pages but does so with remarkable efficiency\nin under 15 minutes for less than \\0.1. Code and dataset will be released at\nhttps://mqleet.github.io/AutoPage_ProjectPage/{Webpage}$.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19600.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6448b2f53e7b3c11be684348",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg",
            "fullname": "Qianli Ma",
            "name": "Mqleet",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "68ee0edd23dc954f7744ac27",
            "name": "AutoLab-SJTU",
            "fullname": "AutoLab"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.19779",
            "authors": [
                {
                    "_id": "68f983f6b9b2e4ae04673741",
                    "user": {
                        "_id": "67dc66fe55c24fc4f981a4ab",
                        "avatarUrl": "/avatars/7bd900ade802d99db7c562ad6c2f6661.svg",
                        "isPro": false,
                        "fullname": "Yuezhou Hu",
                        "user": "yuezhouhu",
                        "type": "user"
                    },
                    "name": "Yuezhou Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:51:20.772Z",
                    "hidden": false
                },
                {
                    "_id": "68f983f6b9b2e4ae04673742",
                    "user": {
                        "_id": "668ff1aed7741cade00afd40",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668ff1aed7741cade00afd40/Pt8vYTL6KtcY0f3KJpymX.png",
                        "isPro": false,
                        "fullname": "Jiaxin Guo",
                        "user": "xinyuerufei",
                        "type": "user"
                    },
                    "name": "Jiaxin Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:13:13.700Z",
                    "hidden": false
                },
                {
                    "_id": "68f983f6b9b2e4ae04673743",
                    "name": "Xinyu Feng",
                    "hidden": false
                },
                {
                    "_id": "68f983f6b9b2e4ae04673744",
                    "name": "Tuo Zhao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67dc66fe55c24fc4f981a4ab/ysq4UzVT_dquWCpvozNWh.png"
            ],
            "publishedAt": "2025-10-22T17:13:00.000Z",
            "submittedOnDailyAt": "2025-10-24T02:14:45.526Z",
            "title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative\n  Decoders",
            "submittedOnDailyBy": {
                "_id": "67dc66fe55c24fc4f981a4ab",
                "avatarUrl": "/avatars/7bd900ade802d99db7c562ad6c2f6661.svg",
                "isPro": false,
                "fullname": "Yuezhou Hu",
                "user": "yuezhouhu",
                "type": "user"
            },
            "summary": "Speculative Decoding (SD) accelerates large language model inference by\nemploying a small draft model to generate predictions, which are then verified\nby a larger target model. The effectiveness of SD hinges on the alignment\nbetween these models, which is typically enhanced by Knowledge Distillation\n(KD). However, conventional KD methods aim to minimize the KL divergence\nbetween the draft and target models across all tokens, a goal that is\nmisaligned with the true objective of SD, which is to maximize token acceptance\nrate. Therefore, draft models often struggle to fully assimilate the target\nmodel's knowledge due to capacity constraints, leading to suboptimal\nperformance. To address this challenge, we propose AdaSPEC, a novel method that\nincorporates selective token filtering into the KD process. AdaSPEC utilizes a\nreference model to identify and filter out difficult-to-fit tokens, enabling\nthe distillation of a draft model that better aligns with the target model on\nsimpler tokens. This approach improves the overall token acceptance rate\nwithout compromising generation quality. We evaluate AdaSPEC across diverse\ntasks, including arithmetic reasoning, instruction-following, coding, and\nsummarization, using model configurations of 31M/1.4B and 350M/2.7B parameters.\nOur results demonstrate that AdaSPEC consistently outperforms the\nstate-of-the-art DistillSpec method, achieving higher acceptance rates across\nall tasks (up to 15\\%). The code is publicly available at\nhttps://github.com/yuezhouhu/adaspec.",
            "upvotes": 49,
            "discussionId": "68f983f7b9b2e4ae04673745",
            "githubRepo": "https://github.com/yuezhouhu/adaspec",
            "ai_summary": "AdaSPEC enhances speculative decoding by selectively filtering tokens during knowledge distillation, improving token acceptance rates without sacrificing generation quality.",
            "ai_keywords": [
                "Speculative Decoding",
                "Knowledge Distillation",
                "KL divergence",
                "token acceptance rate",
                "selective token filtering",
                "reference model",
                "arithmetic reasoning",
                "instruction-following",
                "coding",
                "summarization"
            ],
            "githubStars": 12,
            "organization": {
                "_id": "64155eaa95fb6f824b237c3d",
                "name": "GeorgiaTech",
                "fullname": "Georgia Institute of Technology",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64155e8abe60230f2f40b03a/3i-AL3LrNkaTarSKnaGy8.png"
            }
        },
        "publishedAt": "2025-10-22T13:13:00.000Z",
        "title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative\n  Decoders",
        "summary": "Speculative Decoding (SD) accelerates large language model inference by\nemploying a small draft model to generate predictions, which are then verified\nby a larger target model. The effectiveness of SD hinges on the alignment\nbetween these models, which is typically enhanced by Knowledge Distillation\n(KD). However, conventional KD methods aim to minimize the KL divergence\nbetween the draft and target models across all tokens, a goal that is\nmisaligned with the true objective of SD, which is to maximize token acceptance\nrate. Therefore, draft models often struggle to fully assimilate the target\nmodel's knowledge due to capacity constraints, leading to suboptimal\nperformance. To address this challenge, we propose AdaSPEC, a novel method that\nincorporates selective token filtering into the KD process. AdaSPEC utilizes a\nreference model to identify and filter out difficult-to-fit tokens, enabling\nthe distillation of a draft model that better aligns with the target model on\nsimpler tokens. This approach improves the overall token acceptance rate\nwithout compromising generation quality. We evaluate AdaSPEC across diverse\ntasks, including arithmetic reasoning, instruction-following, coding, and\nsummarization, using model configurations of 31M/1.4B and 350M/2.7B parameters.\nOur results demonstrate that AdaSPEC consistently outperforms the\nstate-of-the-art DistillSpec method, achieving higher acceptance rates across\nall tasks (up to 15\\%). The code is publicly available at\nhttps://github.com/yuezhouhu/adaspec.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67dc66fe55c24fc4f981a4ab/ysq4UzVT_dquWCpvozNWh.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19779.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67dc66fe55c24fc4f981a4ab",
            "avatarUrl": "/avatars/7bd900ade802d99db7c562ad6c2f6661.svg",
            "fullname": "Yuezhou Hu",
            "name": "yuezhouhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "64155eaa95fb6f824b237c3d",
            "name": "GeorgiaTech",
            "fullname": "Georgia Institute of Technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64155e8abe60230f2f40b03a/3i-AL3LrNkaTarSKnaGy8.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.20579",
            "authors": [
                {
                    "_id": "68fadb71f158a71c5a2f582a",
                    "user": {
                        "_id": "65a28e129acab19980226731",
                        "avatarUrl": "/avatars/abc3828f807efc4e03837b0eae063f98.svg",
                        "isPro": false,
                        "fullname": "Jiahao Meng",
                        "user": "marinero4972",
                        "type": "user"
                    },
                    "name": "Jiahao Meng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:12:08.198Z",
                    "hidden": false
                },
                {
                    "_id": "68fadb71f158a71c5a2f582b",
                    "name": "Xiangtai Li",
                    "hidden": false
                },
                {
                    "_id": "68fadb71f158a71c5a2f582c",
                    "user": {
                        "_id": "6499809cf19fc795e7724e43",
                        "avatarUrl": "/avatars/4b3adce8c85e2f3ef05318ded6c89c3e.svg",
                        "isPro": false,
                        "fullname": "HaochenWang",
                        "user": "HaochenWang",
                        "type": "user"
                    },
                    "name": "Haochen Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:11:58.142Z",
                    "hidden": false
                },
                {
                    "_id": "68fadb71f158a71c5a2f582d",
                    "name": "Yue Tan",
                    "hidden": false
                },
                {
                    "_id": "68fadb71f158a71c5a2f582e",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68fadb71f158a71c5a2f582f",
                    "user": {
                        "_id": "62df78222d89ce551ce0f71d",
                        "avatarUrl": "/avatars/89fba294cff2d2f941d121c1923e4c76.svg",
                        "isPro": false,
                        "fullname": "Lingdong Kong",
                        "user": "ldkong",
                        "type": "user"
                    },
                    "name": "Lingdong Kong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:12:02.409Z",
                    "hidden": false
                },
                {
                    "_id": "68fadb71f158a71c5a2f5830",
                    "name": "Yunhai Tong",
                    "hidden": false
                },
                {
                    "_id": "68fadb71f158a71c5a2f5831",
                    "name": "Anran Wang",
                    "hidden": false
                },
                {
                    "_id": "68fadb71f158a71c5a2f5832",
                    "name": "Zhiyang Teng",
                    "hidden": false
                },
                {
                    "_id": "68fadb71f158a71c5a2f5833",
                    "name": "Yujing Wang",
                    "hidden": false
                },
                {
                    "_id": "68fadb71f158a71c5a2f5834",
                    "name": "Zhuochen Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T14:05:56.000Z",
            "submittedOnDailyAt": "2025-10-24T00:20:51.680Z",
            "title": "Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal\n  Evidence",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Most video reasoning models only generate textual reasoning traces without\nindicating when and where key evidence appears. Recent models such as OpenAI-o3\nhave sparked wide interest in evidence-centered reasoning for images, yet\nextending this ability to videos is more challenging, as it requires joint\ntemporal tracking and spatial localization across dynamic scenes. We introduce\nOpen-o3 Video, a non-agent framework that integrates explicit spatio-temporal\nevidence into video reasoning, and carefully collect training data and design\ntraining strategies to address the aforementioned challenges. The model\nhighlights key timestamps, objects, and bounding boxes alongside its answers,\nallowing reasoning to be grounded in concrete visual observations. To enable\nthis functionality, we first curate and build two high-quality datasets,\nSTGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed\ntemporal and spatial annotations, since most existing datasets offer either\ntemporal spans for videos or spatial boxes on images, lacking unified\nspatio-temporal supervision and reasoning traces. Then, we adopt a cold-start\nreinforcement learning strategy with multiple specially designed rewards that\njointly encourage answer accuracy, temporal alignment, and spatial precision.\nOn V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance,\nraising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent\nimprovements are also observed on a broad range of video understanding\nbenchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond\naccuracy, the reasoning traces produced by Open-o3 Video also provide valuable\nsignals for test-time scaling, enabling confidence-aware verification and\nimproving answer reliability.",
            "upvotes": 33,
            "discussionId": "68fadb72f158a71c5a2f5835",
            "projectPage": "https://marinero4972.github.io/projects/Open-o3-Video/",
            "githubRepo": "https://github.com/marinero4972/Open-o3-Video",
            "ai_summary": "Open-o3 Video integrates spatio-temporal evidence into video reasoning, achieving state-of-the-art performance on multiple benchmarks and providing valuable reasoning traces for test-time scaling.",
            "ai_keywords": [
                "spatio-temporal evidence",
                "video reasoning",
                "temporal tracking",
                "spatial localization",
                "non-agent framework",
                "SFT",
                "RL",
                "cold-start reinforcement learning",
                "V-STAR benchmark",
                "mAM",
                "mLGM",
                "VideoMME",
                "WorldSense",
                "VideoMMMU",
                "TVGBench",
                "reasoning traces",
                "confidence-aware verification"
            ],
            "githubStars": 34,
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "publishedAt": "2025-10-23T10:05:56.000Z",
        "title": "Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal\n  Evidence",
        "summary": "Most video reasoning models only generate textual reasoning traces without\nindicating when and where key evidence appears. Recent models such as OpenAI-o3\nhave sparked wide interest in evidence-centered reasoning for images, yet\nextending this ability to videos is more challenging, as it requires joint\ntemporal tracking and spatial localization across dynamic scenes. We introduce\nOpen-o3 Video, a non-agent framework that integrates explicit spatio-temporal\nevidence into video reasoning, and carefully collect training data and design\ntraining strategies to address the aforementioned challenges. The model\nhighlights key timestamps, objects, and bounding boxes alongside its answers,\nallowing reasoning to be grounded in concrete visual observations. To enable\nthis functionality, we first curate and build two high-quality datasets,\nSTGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed\ntemporal and spatial annotations, since most existing datasets offer either\ntemporal spans for videos or spatial boxes on images, lacking unified\nspatio-temporal supervision and reasoning traces. Then, we adopt a cold-start\nreinforcement learning strategy with multiple specially designed rewards that\njointly encourage answer accuracy, temporal alignment, and spatial precision.\nOn V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance,\nraising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent\nimprovements are also observed on a broad range of video understanding\nbenchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond\naccuracy, the reasoning traces produced by Open-o3 Video also provide valuable\nsignals for test-time scaling, enabling confidence-aware verification and\nimproving answer reliability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20579.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 143
        },
        "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.20822",
            "authors": [
                {
                    "_id": "68fada12f158a71c5a2f5804",
                    "name": "Yihao Meng",
                    "hidden": false
                },
                {
                    "_id": "68fada12f158a71c5a2f5805",
                    "name": "Hao Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68fada12f158a71c5a2f5806",
                    "user": {
                        "_id": "662128ec9ca2cd4e6db2fb44",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662128ec9ca2cd4e6db2fb44/uUg1V-pVfxT3mLuFgJuAN.jpeg",
                        "isPro": false,
                        "fullname": "Bruce Yu",
                        "user": "bruceyyu",
                        "type": "user"
                    },
                    "name": "Yue Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:12:10.696Z",
                    "hidden": false
                },
                {
                    "_id": "68fada12f158a71c5a2f5807",
                    "user": {
                        "_id": "64981bea09cea550852652af",
                        "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg",
                        "isPro": false,
                        "fullname": "Qiuyu Wang",
                        "user": "qiuyuu",
                        "type": "user"
                    },
                    "name": "Qiuyu Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:12:13.258Z",
                    "hidden": false
                },
                {
                    "_id": "68fada12f158a71c5a2f5808",
                    "user": {
                        "_id": "63f089456309c84d5f47f951",
                        "avatarUrl": "/avatars/04b926a7f2ad091ee00fef0c59903492.svg",
                        "isPro": false,
                        "fullname": "Wen Wang",
                        "user": "wwen1997",
                        "type": "user"
                    },
                    "name": "Wen Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:12:20.293Z",
                    "hidden": false
                },
                {
                    "_id": "68fada12f158a71c5a2f5809",
                    "name": "Ka Leong Cheng",
                    "hidden": false
                },
                {
                    "_id": "68fada12f158a71c5a2f580a",
                    "name": "Hanlin Wang",
                    "hidden": false
                },
                {
                    "_id": "68fada12f158a71c5a2f580b",
                    "name": "Yixuan Li",
                    "hidden": false
                },
                {
                    "_id": "68fada12f158a71c5a2f580c",
                    "name": "Cheng Chen",
                    "hidden": false
                },
                {
                    "_id": "68fada12f158a71c5a2f580d",
                    "name": "Yanhong Zeng",
                    "hidden": false
                },
                {
                    "_id": "68fada12f158a71c5a2f580e",
                    "name": "Yujun Shen",
                    "hidden": false
                },
                {
                    "_id": "68fada12f158a71c5a2f580f",
                    "name": "Huamin Qu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T17:59:59.000Z",
            "submittedOnDailyAt": "2025-10-24T00:15:58.230Z",
            "title": "HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video\n  Narratives",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "State-of-the-art text-to-video models excel at generating isolated clips but\nfall short of creating the coherent, multi-shot narratives, which are the\nessence of storytelling. We bridge this \"narrative gap\" with HoloCine, a model\nthat generates entire scenes holistically to ensure global consistency from the\nfirst shot to the last. Our architecture achieves precise directorial control\nthrough a Window Cross-Attention mechanism that localizes text prompts to\nspecific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within\nshots but sparse between them) ensures the efficiency required for minute-scale\ngeneration. Beyond setting a new state-of-the-art in narrative coherence,\nHoloCine develops remarkable emergent abilities: a persistent memory for\ncharacters and scenes, and an intuitive grasp of cinematic techniques. Our work\nmarks a pivotal shift from clip synthesis towards automated filmmaking, making\nend-to-end cinematic creation a tangible future. Our code is available at:\nhttps://holo-cine.github.io/.",
            "upvotes": 22,
            "discussionId": "68fada12f158a71c5a2f5810",
            "projectPage": "https://holo-cine.github.io",
            "githubRepo": "https://github.com/yihao-meng/HoloCine",
            "ai_summary": "HoloCine generates coherent multi-shot narratives using a Window Cross-Attention mechanism and Sparse Inter-Shot Self-Attention, enabling end-to-end cinematic creation.",
            "ai_keywords": [
                "Window Cross-Attention",
                "Sparse Inter-Shot Self-Attention",
                "text-to-video models",
                "narrative coherence",
                "automated filmmaking"
            ],
            "githubStars": 122,
            "organization": {
                "_id": "67c1d682826160b28f778510",
                "name": "antgroup",
                "fullname": "Ant Group",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
            }
        },
        "publishedAt": "2025-10-23T13:59:59.000Z",
        "title": "HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video\n  Narratives",
        "summary": "State-of-the-art text-to-video models excel at generating isolated clips but\nfall short of creating the coherent, multi-shot narratives, which are the\nessence of storytelling. We bridge this \"narrative gap\" with HoloCine, a model\nthat generates entire scenes holistically to ensure global consistency from the\nfirst shot to the last. Our architecture achieves precise directorial control\nthrough a Window Cross-Attention mechanism that localizes text prompts to\nspecific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within\nshots but sparse between them) ensures the efficiency required for minute-scale\ngeneration. Beyond setting a new state-of-the-art in narrative coherence,\nHoloCine develops remarkable emergent abilities: a persistent memory for\ncharacters and scenes, and an intuitive grasp of cinematic techniques. Our work\nmarks a pivotal shift from clip synthesis towards automated filmmaking, making\nend-to-end cinematic creation a tangible future. Our code is available at:\nhttps://holo-cine.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20822.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 143
        },
        "organization": {
            "_id": "67c1d682826160b28f778510",
            "name": "antgroup",
            "fullname": "Ant Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.19304",
            "authors": [
                {
                    "_id": "68fa31fef158a71c5a2f5608",
                    "user": {
                        "_id": "64c37ee87d89024360937d81",
                        "avatarUrl": "/avatars/cc9733b0862bbdca5e00f61a7ff7bb94.svg",
                        "isPro": false,
                        "fullname": "mingyu jo",
                        "user": "jojo0217",
                        "type": "user"
                    },
                    "name": "Mingyu Jo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T01:07:20.272Z",
                    "hidden": false
                },
                {
                    "_id": "68fa31fef158a71c5a2f5609",
                    "name": "Jaesik Yoon",
                    "hidden": false
                },
                {
                    "_id": "68fa31fef158a71c5a2f560a",
                    "user": {
                        "_id": "62d073f2485856cd710ed9fd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d073f2485856cd710ed9fd/B6dck3fi5X9hhN4ALulMH.jpeg",
                        "isPro": false,
                        "fullname": "Justin Deschenaux",
                        "user": "jdeschena",
                        "type": "user"
                    },
                    "name": "Justin Deschenaux",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:12:55.094Z",
                    "hidden": false
                },
                {
                    "_id": "68fa31fef158a71c5a2f560b",
                    "name": "Caglar Gulcehre",
                    "hidden": false
                },
                {
                    "_id": "68fa31fef158a71c5a2f560c",
                    "name": "Sungjin Ahn",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T07:08:47.000Z",
            "submittedOnDailyAt": "2025-10-24T00:27:36.960Z",
            "title": "Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall",
            "submittedOnDailyBy": {
                "_id": "64c37ee87d89024360937d81",
                "avatarUrl": "/avatars/cc9733b0862bbdca5e00f61a7ff7bb94.svg",
                "isPro": false,
                "fullname": "mingyu jo",
                "user": "jojo0217",
                "type": "user"
            },
            "summary": "Discrete diffusion models offer a promising alternative to autoregressive\ngeneration through parallel decoding, but they suffer from a sampling wall:\nonce categorical sampling occurs, rich distributional information collapses\ninto one-hot vectors and cannot be propagated across steps, forcing subsequent\nsteps to operate with limited information. To mitigate this problem, we\nintroduce Loopholing, a novel and simple mechanism that preserves this\ninformation via a deterministic latent pathway, leading to Loopholing Discrete\nDiffusion Models (LDDMs). Trained efficiently with a self-conditioning\nstrategy, LDDMs achieve substantial gains-reducing generative perplexity by up\nto 61% over prior baselines, closing (and in some cases surpassing) the gap\nwith autoregressive models, and producing more coherent text. Applied to\nreasoning tasks, LDDMs also improve performance on arithmetic benchmarks such\nas Countdown and Game of 24. These results also indicate that loopholing\nmitigates idle steps and oscillations, providing a scalable path toward\nhigh-quality non-autoregressive text generation.",
            "upvotes": 18,
            "discussionId": "68fa31fef158a71c5a2f560d",
            "projectPage": "https://sites.google.com/view/lddms/home",
            "ai_summary": "Loopholing Discrete Diffusion Models (LDDMs) enhance text generation by preserving distributional information through a deterministic latent pathway, reducing perplexity and improving coherence and performance on reasoning tasks.",
            "ai_keywords": [
                "discrete diffusion models",
                "parallel decoding",
                "sampling wall",
                "one-hot vectors",
                "Loopholing",
                "deterministic latent pathway",
                "Loopholing Discrete Diffusion Models (LDDMs)",
                "self-conditioning strategy",
                "generative perplexity",
                "autoregressive models",
                "coherent text",
                "arithmetic benchmarks",
                "Countdown",
                "Game of 24",
                "idle steps",
                "oscillations",
                "non-autoregressive text generation"
            ]
        },
        "publishedAt": "2025-10-22T03:08:47.000Z",
        "title": "Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall",
        "summary": "Discrete diffusion models offer a promising alternative to autoregressive\ngeneration through parallel decoding, but they suffer from a sampling wall:\nonce categorical sampling occurs, rich distributional information collapses\ninto one-hot vectors and cannot be propagated across steps, forcing subsequent\nsteps to operate with limited information. To mitigate this problem, we\nintroduce Loopholing, a novel and simple mechanism that preserves this\ninformation via a deterministic latent pathway, leading to Loopholing Discrete\nDiffusion Models (LDDMs). Trained efficiently with a self-conditioning\nstrategy, LDDMs achieve substantial gains-reducing generative perplexity by up\nto 61% over prior baselines, closing (and in some cases surpassing) the gap\nwith autoregressive models, and producing more coherent text. Applied to\nreasoning tasks, LDDMs also improve performance on arithmetic benchmarks such\nas Countdown and Game of 24. These results also indicate that loopholing\nmitigates idle steps and oscillations, providing a scalable path toward\nhigh-quality non-autoregressive text generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19304.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c37ee87d89024360937d81",
            "avatarUrl": "/avatars/cc9733b0862bbdca5e00f61a7ff7bb94.svg",
            "fullname": "mingyu jo",
            "name": "jojo0217",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.20766",
            "authors": [
                {
                    "_id": "68faf9f2f158a71c5a2f58ae",
                    "user": {
                        "_id": "647858ba256b62e2198f217e",
                        "avatarUrl": "/avatars/b07432f31d9c29785ac46a3cc0375fc5.svg",
                        "isPro": false,
                        "fullname": "Noam Issachar",
                        "user": "NoamIssachar",
                        "type": "user"
                    },
                    "name": "Noam Issachar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:01:48.102Z",
                    "hidden": false
                },
                {
                    "_id": "68faf9f2f158a71c5a2f58af",
                    "user": {
                        "_id": "646d239f4220471ca0c6471c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d239f4220471ca0c6471c/sRwzko8XEUVCkeD7jXceH.jpeg",
                        "isPro": false,
                        "fullname": "Guy Yariv",
                        "user": "GuyYariv",
                        "type": "user"
                    },
                    "name": "Guy Yariv",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:01:50.800Z",
                    "hidden": false
                },
                {
                    "_id": "68faf9f2f158a71c5a2f58b0",
                    "name": "Sagie Benaim",
                    "hidden": false
                },
                {
                    "_id": "68faf9f2f158a71c5a2f58b1",
                    "name": "Yossi Adi",
                    "hidden": false
                },
                {
                    "_id": "68faf9f2f158a71c5a2f58b2",
                    "name": "Dani Lischinski",
                    "hidden": false
                },
                {
                    "_id": "68faf9f2f158a71c5a2f58b3",
                    "name": "Raanan Fattal",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/646d239f4220471ca0c6471c/1YjzbsoNi3ExGkpg8YQym.png"
            ],
            "publishedAt": "2025-10-23T17:42:14.000Z",
            "submittedOnDailyAt": "2025-10-24T02:35:27.483Z",
            "title": "DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion",
            "submittedOnDailyBy": {
                "_id": "646d239f4220471ca0c6471c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d239f4220471ca0c6471c/sRwzko8XEUVCkeD7jXceH.jpeg",
                "isPro": false,
                "fullname": "Guy Yariv",
                "user": "GuyYariv",
                "type": "user"
            },
            "summary": "Diffusion Transformer models can generate images with remarkable fidelity and\ndetail, yet training them at ultra-high resolutions remains extremely costly\ndue to the self-attention mechanism's quadratic scaling with the number of\nimage tokens. In this paper, we introduce Dynamic Position Extrapolation\n(DyPE), a novel, training-free method that enables pre-trained diffusion\ntransformers to synthesize images at resolutions far beyond their training\ndata, with no additional sampling cost. DyPE takes advantage of the spectral\nprogression inherent to the diffusion process, where low-frequency structures\nconverge early, while high-frequencies take more steps to resolve.\nSpecifically, DyPE dynamically adjusts the model's positional encoding at each\ndiffusion step, matching their frequency spectrum with the current stage of the\ngenerative process. This approach allows us to generate images at resolutions\nthat exceed the training resolution dramatically, e.g., 16 million pixels using\nFLUX. On multiple benchmarks, DyPE consistently improves performance and\nachieves state-of-the-art fidelity in ultra-high-resolution image generation,\nwith gains becoming even more pronounced at higher resolutions. Project page is\navailable at https://noamissachar.github.io/DyPE/.",
            "upvotes": 17,
            "discussionId": "68faf9f2f158a71c5a2f58b4",
            "projectPage": "https://noamissachar.github.io/DyPE/",
            "githubRepo": "https://github.com/guyyariv/DyPE",
            "ai_summary": "Dynamic Position Extrapolation (DyPE) enhances ultra-high-resolution image generation by dynamically adjusting positional encodings in pre-trained diffusion transformers, achieving state-of-the-art fidelity without additional sampling cost.",
            "ai_keywords": [
                "Diffusion Transformer models",
                "self-attention mechanism",
                "Dynamic Position Extrapolation (DyPE)",
                "positional encoding",
                "spectral progression",
                "low-frequency structures",
                "high-frequencies",
                "generative process",
                "ultra-high-resolution image generation",
                "FLUX"
            ],
            "githubStars": 42,
            "organization": {
                "_id": "65157bc51e7b9224c9c6d460",
                "name": "HUJI-IL",
                "fullname": "The Hebrew University of Jerusalem"
            }
        },
        "publishedAt": "2025-10-23T13:42:14.000Z",
        "title": "DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion",
        "summary": "Diffusion Transformer models can generate images with remarkable fidelity and\ndetail, yet training them at ultra-high resolutions remains extremely costly\ndue to the self-attention mechanism's quadratic scaling with the number of\nimage tokens. In this paper, we introduce Dynamic Position Extrapolation\n(DyPE), a novel, training-free method that enables pre-trained diffusion\ntransformers to synthesize images at resolutions far beyond their training\ndata, with no additional sampling cost. DyPE takes advantage of the spectral\nprogression inherent to the diffusion process, where low-frequency structures\nconverge early, while high-frequencies take more steps to resolve.\nSpecifically, DyPE dynamically adjusts the model's positional encoding at each\ndiffusion step, matching their frequency spectrum with the current stage of the\ngenerative process. This approach allows us to generate images at resolutions\nthat exceed the training resolution dramatically, e.g., 16 million pixels using\nFLUX. On multiple benchmarks, DyPE consistently improves performance and\nachieves state-of-the-art fidelity in ultra-high-resolution image generation,\nwith gains becoming even more pronounced at higher resolutions. Project page is\navailable at https://noamissachar.github.io/DyPE/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/646d239f4220471ca0c6471c/1YjzbsoNi3ExGkpg8YQym.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20766.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646d239f4220471ca0c6471c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d239f4220471ca0c6471c/sRwzko8XEUVCkeD7jXceH.jpeg",
            "fullname": "Guy Yariv",
            "name": "GuyYariv",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "65157bc51e7b9224c9c6d460",
            "name": "HUJI-IL",
            "fullname": "The Hebrew University of Jerusalem"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.19365",
            "authors": [
                {
                    "_id": "68f98172b9b2e4ae04673725",
                    "user": {
                        "_id": "6497ffbf2a997a45e987e139",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6497ffbf2a997a45e987e139/LUipMEC-wC1QeIXHSMnsx.png",
                        "isPro": true,
                        "fullname": "Umar Butler",
                        "user": "umarbutler",
                        "type": "user"
                    },
                    "name": "Umar Butler",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-10-23T14:53:11.283Z",
                    "hidden": false
                },
                {
                    "_id": "68f98172b9b2e4ae04673726",
                    "user": {
                        "_id": "63568098ef1d4c9191533b3f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63568098ef1d4c9191533b3f/o9IGAr_XBBiMtGRGeO3V3.png",
                        "isPro": false,
                        "fullname": "Abdur-Rahman Butler",
                        "user": "abdurrahmanbutler",
                        "type": "user"
                    },
                    "name": "Abdur-Rahman Butler",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:51:30.656Z",
                    "hidden": false
                },
                {
                    "_id": "68f98172b9b2e4ae04673727",
                    "user": {
                        "_id": "65839aaab6a60815eefed378",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65839aaab6a60815eefed378/a-Xym6IxpYy0b1B6CJpEj.jpeg",
                        "isPro": false,
                        "fullname": "Adrian Lucas Malec",
                        "user": "adlumal",
                        "type": "user"
                    },
                    "name": "Adrian Lucas Malec",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-10-23T14:53:47.869Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6497ffbf2a997a45e987e139/zowG0N3HVQ6RkF_ekAbz2.webp"
            ],
            "publishedAt": "2025-10-22T08:38:44.000Z",
            "submittedOnDailyAt": "2025-10-24T02:37:18.738Z",
            "title": "The Massive Legal Embedding Benchmark (MLEB)",
            "submittedOnDailyBy": {
                "_id": "6497ffbf2a997a45e987e139",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6497ffbf2a997a45e987e139/LUipMEC-wC1QeIXHSMnsx.png",
                "isPro": true,
                "fullname": "Umar Butler",
                "user": "umarbutler",
                "type": "user"
            },
            "summary": "We present the Massive Legal Embedding Benchmark (MLEB), the largest, most\ndiverse, and most comprehensive open-source benchmark for legal information\nretrieval to date. MLEB consists of ten expert-annotated datasets spanning\nmultiple jurisdictions (the US, UK, EU, Australia, Ireland, and Singapore),\ndocument types (cases, legislation, regulatory guidance, contracts, and\nliterature), and task types (search, zero-shot classification, and question\nanswering). Seven of the datasets in MLEB were newly constructed in order to\nfill domain and jurisdictional gaps in the open-source legal information\nretrieval landscape. We document our methodology in building MLEB and creating\nthe new constituent datasets, and release our code, results, and data openly to\nassist with reproducible evaluations.",
            "upvotes": 14,
            "discussionId": "68f98172b9b2e4ae04673728",
            "projectPage": "https://isaacus.com/mleb",
            "githubRepo": "https://github.com/isaacus-dev/mleb",
            "ai_summary": "MLEB is the largest open-source benchmark for legal information retrieval, encompassing multiple jurisdictions, document types, and task types.",
            "ai_keywords": [
                "Massive Legal Embedding Benchmark",
                "MLEB",
                "legal information retrieval",
                "expert-annotated datasets",
                "jurisdictions",
                "document types",
                "task types",
                "zero-shot classification",
                "question answering"
            ],
            "githubStars": 17,
            "organization": {
                "_id": "671f7ee82174ca42d524f1d3",
                "name": "isaacus",
                "fullname": "Isaacus",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6497ffbf2a997a45e987e139/HCfH2lC0OMQWC5PiwZQya.png"
            }
        },
        "publishedAt": "2025-10-22T04:38:44.000Z",
        "title": "The Massive Legal Embedding Benchmark (MLEB)",
        "summary": "We present the Massive Legal Embedding Benchmark (MLEB), the largest, most\ndiverse, and most comprehensive open-source benchmark for legal information\nretrieval to date. MLEB consists of ten expert-annotated datasets spanning\nmultiple jurisdictions (the US, UK, EU, Australia, Ireland, and Singapore),\ndocument types (cases, legislation, regulatory guidance, contracts, and\nliterature), and task types (search, zero-shot classification, and question\nanswering). Seven of the datasets in MLEB were newly constructed in order to\nfill domain and jurisdictional gaps in the open-source legal information\nretrieval landscape. We document our methodology in building MLEB and creating\nthe new constituent datasets, and release our code, results, and data openly to\nassist with reproducible evaluations.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6497ffbf2a997a45e987e139/zowG0N3HVQ6RkF_ekAbz2.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19365.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6497ffbf2a997a45e987e139",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6497ffbf2a997a45e987e139/LUipMEC-wC1QeIXHSMnsx.png",
            "fullname": "Umar Butler",
            "name": "umarbutler",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 45
        },
        "organization": {
            "_id": "671f7ee82174ca42d524f1d3",
            "name": "isaacus",
            "fullname": "Isaacus",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6497ffbf2a997a45e987e139/HCfH2lC0OMQWC5PiwZQya.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.20187",
            "authors": [
                {
                    "_id": "68fad821f158a71c5a2f57f4",
                    "name": "Dian Yu",
                    "hidden": false
                },
                {
                    "_id": "68fad821f158a71c5a2f57f5",
                    "user": {
                        "_id": "6575171654d1749612e21eed",
                        "avatarUrl": "/avatars/c032c1b942b3cb9450a49db88fce5c70.svg",
                        "isPro": false,
                        "fullname": "Yulai Zhao",
                        "user": "sarosavo",
                        "type": "user"
                    },
                    "name": "Yulai Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:12:37.233Z",
                    "hidden": false
                },
                {
                    "_id": "68fad821f158a71c5a2f57f6",
                    "user": {
                        "_id": "68266b5261ed4d89177c3612",
                        "avatarUrl": "/avatars/e41ef277047334174eca408aba2a63db.svg",
                        "isPro": false,
                        "fullname": "Kishan",
                        "user": "kishanpb",
                        "type": "user"
                    },
                    "name": "Kishan Panaganti",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:12:34.051Z",
                    "hidden": false
                },
                {
                    "_id": "68fad821f158a71c5a2f57f7",
                    "name": "Linfeng Song",
                    "hidden": false
                },
                {
                    "_id": "68fad821f158a71c5a2f57f8",
                    "name": "Haitao Mi",
                    "hidden": false
                },
                {
                    "_id": "68fad821f158a71c5a2f57f9",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T04:15:22.000Z",
            "submittedOnDailyAt": "2025-10-24T00:18:52.427Z",
            "title": "Every Question Has Its Own Value: Reinforcement Learning with Explicit\n  Human Values",
            "submittedOnDailyBy": {
                "_id": "62d58fd53bf5e059f7cc3245",
                "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
                "isPro": false,
                "fullname": "Dian Yu",
                "user": "yudian",
                "type": "user"
            },
            "summary": "We propose Reinforcement Learning with Explicit Human Values (RLEV), a method\nthat aligns Large Language Model (LLM) optimization directly with quantifiable\nhuman value signals. While Reinforcement Learning with Verifiable Rewards\n(RLVR) effectively trains models in objective domains using binary correctness\nrewards, it overlooks that not all tasks are equally significant. RLEV extends\nthis framework by incorporating human-defined value signals directly into the\nreward function. Using exam-style data with explicit ground-truth value labels,\nRLEV consistently outperforms correctness-only baselines across multiple RL\nalgorithms and model scales. Crucially, RLEV policies not only improve\nvalue-weighted accuracy but also learn a value-sensitive termination policy:\nconcise for low-value prompts, thorough for high-value ones. We demonstrate\nthis behavior stems from value-weighted gradient amplification on\nend-of-sequence tokens. Ablation studies confirm the gain is causally linked to\nvalue alignment. RLEV remains robust under noisy value signals, such as\ndifficulty-based labels, demonstrating that optimizing for an explicit utility\nfunction offers a practical path to aligning LLMs with human priorities.",
            "upvotes": 13,
            "discussionId": "68fad821f158a71c5a2f57fa",
            "ai_summary": "RLEV, a reinforcement learning method, aligns LLM optimization with human value signals, improving value-weighted accuracy and learning value-sensitive termination policies.",
            "ai_keywords": [
                "Reinforcement Learning with Explicit Human Values (RLEV)",
                "Large Language Model (LLM)",
                "Reinforcement Learning with Verifiable Rewards (RLVR)",
                "value signals",
                "reward function",
                "exam-style data",
                "value-weighted accuracy",
                "value-sensitive termination policy",
                "gradient amplification",
                "end-of-sequence tokens",
                "ablation studies",
                "value alignment",
                "noisy value signals",
                "utility function"
            ],
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-10-23T00:15:22.000Z",
        "title": "Every Question Has Its Own Value: Reinforcement Learning with Explicit\n  Human Values",
        "summary": "We propose Reinforcement Learning with Explicit Human Values (RLEV), a method\nthat aligns Large Language Model (LLM) optimization directly with quantifiable\nhuman value signals. While Reinforcement Learning with Verifiable Rewards\n(RLVR) effectively trains models in objective domains using binary correctness\nrewards, it overlooks that not all tasks are equally significant. RLEV extends\nthis framework by incorporating human-defined value signals directly into the\nreward function. Using exam-style data with explicit ground-truth value labels,\nRLEV consistently outperforms correctness-only baselines across multiple RL\nalgorithms and model scales. Crucially, RLEV policies not only improve\nvalue-weighted accuracy but also learn a value-sensitive termination policy:\nconcise for low-value prompts, thorough for high-value ones. We demonstrate\nthis behavior stems from value-weighted gradient amplification on\nend-of-sequence tokens. Ablation studies confirm the gain is causally linked to\nvalue alignment. RLEV remains robust under noisy value signals, such as\ndifficulty-based labels, demonstrating that optimizing for an explicit utility\nfunction offers a practical path to aligning LLMs with human priorities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20187.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62d58fd53bf5e059f7cc3245",
            "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
            "fullname": "Dian Yu",
            "name": "yudian",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.16917",
            "authors": [
                {
                    "_id": "68f78e8ea7b9b96b2c13edc4",
                    "user": {
                        "_id": "646fa3016441111304fec68d",
                        "avatarUrl": "/avatars/923629340f3785ae8c6e52cf3674d5c2.svg",
                        "isPro": false,
                        "fullname": "Chih-Kai Yang",
                        "user": "zenyn",
                        "type": "user"
                    },
                    "name": "Chih-Kai Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:00:14.845Z",
                    "hidden": false
                },
                {
                    "_id": "68f78e8ea7b9b96b2c13edc5",
                    "user": {
                        "_id": "6623864025bb4e9dc1f09695",
                        "avatarUrl": "/avatars/d37dc450d48cd1e9be36608c0ece0bcd.svg",
                        "isPro": false,
                        "fullname": "Yenting",
                        "user": "yenting-biao",
                        "type": "user"
                    },
                    "name": "Yen-Ting Piao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:13:54.361Z",
                    "hidden": false
                },
                {
                    "_id": "68f78e8ea7b9b96b2c13edc6",
                    "name": "Tzu-Wen Hsu",
                    "hidden": false
                },
                {
                    "_id": "68f78e8ea7b9b96b2c13edc7",
                    "name": "Szu-Wei Fu",
                    "hidden": false
                },
                {
                    "_id": "68f78e8ea7b9b96b2c13edc8",
                    "name": "Zhehuai Chen",
                    "hidden": false
                },
                {
                    "_id": "68f78e8ea7b9b96b2c13edc9",
                    "name": "Ke-Han Lu",
                    "hidden": false
                },
                {
                    "_id": "68f78e8ea7b9b96b2c13edca",
                    "name": "Sung-Feng Huang",
                    "hidden": false
                },
                {
                    "_id": "68f78e8ea7b9b96b2c13edcb",
                    "name": "Chao-Han Huck Yang",
                    "hidden": false
                },
                {
                    "_id": "68f78e8ea7b9b96b2c13edcc",
                    "name": "Yu-Chiang Frank Wang",
                    "hidden": false
                },
                {
                    "_id": "68f78e8ea7b9b96b2c13edcd",
                    "name": "Yun-Nung Chen",
                    "hidden": false
                },
                {
                    "_id": "68f78e8ea7b9b96b2c13edce",
                    "name": "Hung-yi Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-19T16:22:09.000Z",
            "submittedOnDailyAt": "2025-10-24T07:18:52.654Z",
            "title": "SAKE: Towards Editing Auditory Attribute Knowledge of Large\n  Audio-Language Models",
            "submittedOnDailyBy": {
                "_id": "646fa3016441111304fec68d",
                "avatarUrl": "/avatars/923629340f3785ae8c6e52cf3674d5c2.svg",
                "isPro": false,
                "fullname": "Chih-Kai Yang",
                "user": "zenyn",
                "type": "user"
            },
            "summary": "Knowledge editing offers an efficient way to update model knowledge without\nfull retraining, but prior work has concentrated almost exclusively on textual\nor visual modalities. We introduce SAKE, the first benchmark specifically\ndesigned for editing auditory attribute knowledge in Large Audio-Language\nModels (LALMs). Unlike factual updates, SAKE targets several abstract auditory\nattributes, capturing knowledge types that go beyond conventional textual and\nvisual domains. We benchmark seven editing methods on two LALMs along four\ndimensions: reliability, generality, audio/text locality, and portability.\nResults highlight challenges such as preserving intra-attribute knowledge\nunrelated to the edit, generalizing edits to multimodal reasoning, and\nmaintaining edits under sequential updates. SAKE provides a principled\nframework to study how knowledge editing extends to the auditory modalities,\nopening new directions for maintaining and adapting LALMs in more diverse\nreal-world scenarios.",
            "upvotes": 10,
            "discussionId": "68f78e8ea7b9b96b2c13edcf",
            "projectPage": "https://github.com/ckyang1124/SAKE",
            "githubRepo": "https://github.com/ckyang1124/SAKE",
            "ai_summary": "SAKE is a benchmark for editing auditory attribute knowledge in Large Audio-Language Models, addressing challenges in reliability, generality, locality, and portability.",
            "ai_keywords": [
                "Large Audio-Language Models",
                "LALMs",
                "knowledge editing",
                "auditory attribute knowledge",
                "benchmark",
                "reliability",
                "generality",
                "audio/text locality",
                "portability",
                "intra-attribute knowledge",
                "multimodal reasoning",
                "sequential updates"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "673248e121823ee4ea594099",
                "name": "nationaltaiwan",
                "fullname": "台灣大學",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67324880c1f20c742be144b8/CE1UiOtpMeC8pmtdGP4Nn.png"
            }
        },
        "publishedAt": "2025-10-19T12:22:09.000Z",
        "title": "SAKE: Towards Editing Auditory Attribute Knowledge of Large\n  Audio-Language Models",
        "summary": "Knowledge editing offers an efficient way to update model knowledge without\nfull retraining, but prior work has concentrated almost exclusively on textual\nor visual modalities. We introduce SAKE, the first benchmark specifically\ndesigned for editing auditory attribute knowledge in Large Audio-Language\nModels (LALMs). Unlike factual updates, SAKE targets several abstract auditory\nattributes, capturing knowledge types that go beyond conventional textual and\nvisual domains. We benchmark seven editing methods on two LALMs along four\ndimensions: reliability, generality, audio/text locality, and portability.\nResults highlight challenges such as preserving intra-attribute knowledge\nunrelated to the edit, generalizing edits to multimodal reasoning, and\nmaintaining edits under sequential updates. SAKE provides a principled\nframework to study how knowledge editing extends to the auditory modalities,\nopening new directions for maintaining and adapting LALMs in more diverse\nreal-world scenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16917.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646fa3016441111304fec68d",
            "avatarUrl": "/avatars/923629340f3785ae8c6e52cf3674d5c2.svg",
            "fullname": "Chih-Kai Yang",
            "name": "zenyn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "673248e121823ee4ea594099",
            "name": "nationaltaiwan",
            "fullname": "台灣大學",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67324880c1f20c742be144b8/CE1UiOtpMeC8pmtdGP4Nn.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.16893",
            "authors": [
                {
                    "_id": "68f78ebaa7b9b96b2c13edd1",
                    "name": "Bo-Han Feng",
                    "hidden": false
                },
                {
                    "_id": "68f78ebaa7b9b96b2c13edd2",
                    "name": "Chien-Feng Liu",
                    "hidden": false
                },
                {
                    "_id": "68f78ebaa7b9b96b2c13edd3",
                    "name": "Yu-Hsuan Li Liang",
                    "hidden": false
                },
                {
                    "_id": "68f78ebaa7b9b96b2c13edd4",
                    "user": {
                        "_id": "646fa3016441111304fec68d",
                        "avatarUrl": "/avatars/923629340f3785ae8c6e52cf3674d5c2.svg",
                        "isPro": false,
                        "fullname": "Chih-Kai Yang",
                        "user": "zenyn",
                        "type": "user"
                    },
                    "name": "Chih-Kai Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-21T14:00:09.544Z",
                    "hidden": false
                },
                {
                    "_id": "68f78ebaa7b9b96b2c13edd5",
                    "name": "Szu-Wei Fu",
                    "hidden": false
                },
                {
                    "_id": "68f78ebaa7b9b96b2c13edd6",
                    "name": "Zhehuai Chen",
                    "hidden": false
                },
                {
                    "_id": "68f78ebaa7b9b96b2c13edd7",
                    "name": "Ke-Han Lu",
                    "hidden": false
                },
                {
                    "_id": "68f78ebaa7b9b96b2c13edd8",
                    "name": "Sung-Feng Huang",
                    "hidden": false
                },
                {
                    "_id": "68f78ebaa7b9b96b2c13edd9",
                    "name": "Chao-Han Huck Yang",
                    "hidden": false
                },
                {
                    "_id": "68f78ebaa7b9b96b2c13edda",
                    "name": "Yu-Chiang Frank Wang",
                    "hidden": false
                },
                {
                    "_id": "68f78ebaa7b9b96b2c13eddb",
                    "name": "Yun-Nung Chen",
                    "hidden": false
                },
                {
                    "_id": "68f78ebaa7b9b96b2c13eddc",
                    "name": "Hung-yi Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-19T15:41:25.000Z",
            "submittedOnDailyAt": "2025-10-24T07:16:57.076Z",
            "title": "Investigating Safety Vulnerabilities of Large Audio-Language Models\n  Under Speaker Emotional Variations",
            "submittedOnDailyBy": {
                "_id": "646fa3016441111304fec68d",
                "avatarUrl": "/avatars/923629340f3785ae8c6e52cf3674d5c2.svg",
                "isPro": false,
                "fullname": "Chih-Kai Yang",
                "user": "zenyn",
                "type": "user"
            },
            "summary": "Large audio-language models (LALMs) extend text-based LLMs with auditory\nunderstanding, offering new opportunities for multimodal applications. While\ntheir perception, reasoning, and task performance have been widely studied,\ntheir safety alignment under paralinguistic variation remains underexplored.\nThis work systematically investigates the role of speaker emotion. We construct\na dataset of malicious speech instructions expressed across multiple emotions\nand intensities, and evaluate several state-of-the-art LALMs. Our results\nreveal substantial safety inconsistencies: different emotions elicit varying\nlevels of unsafe responses, and the effect of intensity is non-monotonic, with\nmedium expressions often posing the greatest risk. These findings highlight an\noverlooked vulnerability in LALMs and call for alignment strategies explicitly\ndesigned to ensure robustness under emotional variation, a prerequisite for\ntrustworthy deployment in real-world settings.",
            "upvotes": 10,
            "discussionId": "68f78ebaa7b9b96b2c13eddd",
            "projectPage": "https://github.com/WoZhenDeShenMeDouBuZhidao/LALM-emotional-vulnerability",
            "githubRepo": "https://github.com/WoZhenDeShenMeDouBuZhidao/LALM-emotional-vulnerability",
            "ai_summary": "Research investigates the impact of speaker emotion on the safety of large audio-language models, revealing inconsistencies and vulnerabilities that require targeted alignment strategies.",
            "ai_keywords": [
                "large audio-language models",
                "LALMs",
                "text-based LLMs",
                "auditory understanding",
                "multimodal applications",
                "speaker emotion",
                "malicious speech instructions",
                "unsafe responses",
                "alignment strategies",
                "robustness"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "673248e121823ee4ea594099",
                "name": "nationaltaiwan",
                "fullname": "台灣大學",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67324880c1f20c742be144b8/CE1UiOtpMeC8pmtdGP4Nn.png"
            }
        },
        "publishedAt": "2025-10-19T11:41:25.000Z",
        "title": "Investigating Safety Vulnerabilities of Large Audio-Language Models\n  Under Speaker Emotional Variations",
        "summary": "Large audio-language models (LALMs) extend text-based LLMs with auditory\nunderstanding, offering new opportunities for multimodal applications. While\ntheir perception, reasoning, and task performance have been widely studied,\ntheir safety alignment under paralinguistic variation remains underexplored.\nThis work systematically investigates the role of speaker emotion. We construct\na dataset of malicious speech instructions expressed across multiple emotions\nand intensities, and evaluate several state-of-the-art LALMs. Our results\nreveal substantial safety inconsistencies: different emotions elicit varying\nlevels of unsafe responses, and the effect of intensity is non-monotonic, with\nmedium expressions often posing the greatest risk. These findings highlight an\noverlooked vulnerability in LALMs and call for alignment strategies explicitly\ndesigned to ensure robustness under emotional variation, a prerequisite for\ntrustworthy deployment in real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16893.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646fa3016441111304fec68d",
            "avatarUrl": "/avatars/923629340f3785ae8c6e52cf3674d5c2.svg",
            "fullname": "Chih-Kai Yang",
            "name": "zenyn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "673248e121823ee4ea594099",
            "name": "nationaltaiwan",
            "fullname": "台灣大學",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67324880c1f20c742be144b8/CE1UiOtpMeC8pmtdGP4Nn.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.20733",
            "authors": [
                {
                    "_id": "68fb1e70f158a71c5a2f596f",
                    "user": {
                        "_id": "6804a894ea0734d6b26f19c3",
                        "avatarUrl": "/avatars/b3f5321366bbbb2fd91624289fbea958.svg",
                        "isPro": false,
                        "fullname": "Yujia Zheng",
                        "user": "yujiazheng",
                        "type": "user"
                    },
                    "name": "Yujia Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:00:44.774Z",
                    "hidden": false
                },
                {
                    "_id": "68fb1e70f158a71c5a2f5970",
                    "name": "Zhuokai Zhao",
                    "hidden": false
                },
                {
                    "_id": "68fb1e70f158a71c5a2f5971",
                    "name": "Zijian Li",
                    "hidden": false
                },
                {
                    "_id": "68fb1e70f158a71c5a2f5972",
                    "name": "Yaqi Xie",
                    "hidden": false
                },
                {
                    "_id": "68fb1e70f158a71c5a2f5973",
                    "name": "Mingze Gao",
                    "hidden": false
                },
                {
                    "_id": "68fb1e70f158a71c5a2f5974",
                    "name": "Lizhu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68fb1e70f158a71c5a2f5975",
                    "name": "Kun Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T16:48:02.000Z",
            "submittedOnDailyAt": "2025-10-24T05:12:17.178Z",
            "title": "Thought Communication in Multiagent Collaboration",
            "submittedOnDailyBy": {
                "_id": "6804a894ea0734d6b26f19c3",
                "avatarUrl": "/avatars/b3f5321366bbbb2fd91624289fbea958.svg",
                "isPro": false,
                "fullname": "Yujia Zheng",
                "user": "yujiazheng",
                "type": "user"
            },
            "summary": "Natural language has long enabled human cooperation, but its lossy,\nambiguous, and indirect nature limits the potential of collective intelligence.\nWhile machines are not subject to these constraints, most LLM-based multi-agent\nsystems still rely solely on natural language, exchanging tokens or their\nembeddings. To go beyond language, we introduce a new paradigm, thought\ncommunication, which enables agents to interact directly mind-to-mind, akin to\ntelepathy. To uncover these latent thoughts in a principled way, we formalize\nthe process as a general latent variable model, where agent states are\ngenerated by an unknown function of underlying thoughts. We prove that, in a\nnonparametric setting without auxiliary information, both shared and private\nlatent thoughts between any pair of agents can be identified. Moreover, the\nglobal structure of thought sharing, including which agents share which\nthoughts and how these relationships are structured, can also be recovered with\ntheoretical guarantees. Guided by the established theory, we develop a\nframework that extracts latent thoughts from all agents prior to communication\nand assigns each agent the relevant thoughts, along with their sharing\npatterns. This paradigm naturally extends beyond LLMs to all modalities, as\nmost observational data arise from hidden generative processes. Experiments on\nboth synthetic and real-world benchmarks validate the theory and demonstrate\nthe collaborative advantages of thought communication. We hope this work\nilluminates the potential of leveraging the hidden world, as many challenges\nremain unsolvable through surface-level observation alone, regardless of\ncompute or data scale.",
            "upvotes": 6,
            "discussionId": "68fb1e70f158a71c5a2f5976",
            "ai_summary": "Thought communication enables direct mind-to-mind interaction between agents, uncovering latent thoughts and improving collaborative intelligence beyond natural language.",
            "ai_keywords": [
                "thought communication",
                "latent variable model",
                "agent states",
                "latent thoughts",
                "shared thoughts",
                "private thoughts",
                "global structure",
                "thought sharing",
                "generative processes"
            ]
        },
        "publishedAt": "2025-10-23T12:48:02.000Z",
        "title": "Thought Communication in Multiagent Collaboration",
        "summary": "Natural language has long enabled human cooperation, but its lossy,\nambiguous, and indirect nature limits the potential of collective intelligence.\nWhile machines are not subject to these constraints, most LLM-based multi-agent\nsystems still rely solely on natural language, exchanging tokens or their\nembeddings. To go beyond language, we introduce a new paradigm, thought\ncommunication, which enables agents to interact directly mind-to-mind, akin to\ntelepathy. To uncover these latent thoughts in a principled way, we formalize\nthe process as a general latent variable model, where agent states are\ngenerated by an unknown function of underlying thoughts. We prove that, in a\nnonparametric setting without auxiliary information, both shared and private\nlatent thoughts between any pair of agents can be identified. Moreover, the\nglobal structure of thought sharing, including which agents share which\nthoughts and how these relationships are structured, can also be recovered with\ntheoretical guarantees. Guided by the established theory, we develop a\nframework that extracts latent thoughts from all agents prior to communication\nand assigns each agent the relevant thoughts, along with their sharing\npatterns. This paradigm naturally extends beyond LLMs to all modalities, as\nmost observational data arise from hidden generative processes. Experiments on\nboth synthetic and real-world benchmarks validate the theory and demonstrate\nthe collaborative advantages of thought communication. We hope this work\nilluminates the potential of leveraging the hidden world, as many challenges\nremain unsolvable through surface-level observation alone, regardless of\ncompute or data scale.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20733.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6804a894ea0734d6b26f19c3",
            "avatarUrl": "/avatars/b3f5321366bbbb2fd91624289fbea958.svg",
            "fullname": "Yujia Zheng",
            "name": "yujiazheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.19944",
            "authors": [
                {
                    "_id": "68fb1742f158a71c5a2f592e",
                    "name": "Jiashi Feng",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f592f",
                    "name": "Xiu Li",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5930",
                    "name": "Jing Lin",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5931",
                    "name": "Jiahang Liu",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5932",
                    "name": "Gaohong Liu",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5933",
                    "name": "Weiqiang Lou",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5934",
                    "name": "Su Ma",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5935",
                    "name": "Guang Shi",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5936",
                    "name": "Qinlong Wang",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5937",
                    "name": "Jun Wang",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5938",
                    "name": "Zhongcong Xu",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5939",
                    "name": "Xuanyu Yi",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f593a",
                    "name": "Zihao Yu",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f593b",
                    "name": "Jianfeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f593c",
                    "name": "Yifan Zhu",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f593d",
                    "name": "Rui Chen",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f593e",
                    "name": "Jinxin Chi",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f593f",
                    "name": "Zixian Du",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5940",
                    "name": "Li Han",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5941",
                    "name": "Lixin Huang",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5942",
                    "name": "Kaihua Jiang",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5943",
                    "name": "Yuhan Li",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5944",
                    "name": "Guan Luo",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5945",
                    "name": "Shuguang Wang",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5946",
                    "name": "Qianyi Wu",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5947",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5948",
                    "name": "Junyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68fb1742f158a71c5a2f5949",
                    "name": "Xuanmeng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T18:16:32.000Z",
            "submittedOnDailyAt": "2025-10-24T04:38:54.664Z",
            "title": "Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets",
            "submittedOnDailyBy": {
                "_id": "631c2bed7f7a1b6cb9f6b114",
                "avatarUrl": "/avatars/97d2c0b6123691ea27157ebf8da59b45.svg",
                "isPro": false,
                "fullname": "Zhongcong Xu",
                "user": "zcxu-eric",
                "type": "user"
            },
            "summary": "Developing embodied AI agents requires scalable training environments that\nbalance content diversity with physics accuracy. World simulators provide such\nenvironments but face distinct limitations: video-based methods generate\ndiverse content but lack real-time physics feedback for interactive learning,\nwhile physics-based engines provide accurate dynamics but face scalability\nlimitations from costly manual asset creation. We present Seed3D 1.0, a\nfoundation model that generates simulation-ready 3D assets from single images,\naddressing the scalability challenge while maintaining physics rigor. Unlike\nexisting 3D generation models, our system produces assets with accurate\ngeometry, well-aligned textures, and realistic physically-based materials.\nThese assets can be directly integrated into physics engines with minimal\nconfiguration, enabling deployment in robotic manipulation and simulation\ntraining. Beyond individual objects, the system scales to complete scene\ngeneration through assembling objects into coherent environments. By enabling\nscalable simulation-ready content creation, Seed3D 1.0 provides a foundation\nfor advancing physics-based world simulators. Seed3D 1.0 is now available on\nhttps://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?modelId=doubao-seed3d-1-0-250928&tab=Gen3D",
            "upvotes": 6,
            "discussionId": "68fb1742f158a71c5a2f594a",
            "projectPage": "https://seed.bytedance.com/seed3d",
            "ai_summary": "Seed3D 1.0 generates scalable, physics-accurate 3D assets from images for use in simulation environments, enhancing both content diversity and real-time physics feedback.",
            "ai_keywords": [
                "world simulators",
                "video-based methods",
                "physics-based engines",
                "simulation-ready 3D assets",
                "accurate geometry",
                "well-aligned textures",
                "physically-based materials",
                "physics engines",
                "robotic manipulation",
                "scene generation",
                "coherent environments"
            ],
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-10-22T14:16:32.000Z",
        "title": "Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets",
        "summary": "Developing embodied AI agents requires scalable training environments that\nbalance content diversity with physics accuracy. World simulators provide such\nenvironments but face distinct limitations: video-based methods generate\ndiverse content but lack real-time physics feedback for interactive learning,\nwhile physics-based engines provide accurate dynamics but face scalability\nlimitations from costly manual asset creation. We present Seed3D 1.0, a\nfoundation model that generates simulation-ready 3D assets from single images,\naddressing the scalability challenge while maintaining physics rigor. Unlike\nexisting 3D generation models, our system produces assets with accurate\ngeometry, well-aligned textures, and realistic physically-based materials.\nThese assets can be directly integrated into physics engines with minimal\nconfiguration, enabling deployment in robotic manipulation and simulation\ntraining. Beyond individual objects, the system scales to complete scene\ngeneration through assembling objects into coherent environments. By enabling\nscalable simulation-ready content creation, Seed3D 1.0 provides a foundation\nfor advancing physics-based world simulators. Seed3D 1.0 is now available on\nhttps://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?modelId=doubao-seed3d-1-0-250928&tab=Gen3D",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19944.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631c2bed7f7a1b6cb9f6b114",
            "avatarUrl": "/avatars/97d2c0b6123691ea27157ebf8da59b45.svg",
            "fullname": "Zhongcong Xu",
            "name": "zcxu-eric",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 78
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18821",
            "authors": [
                {
                    "_id": "68f85c9b7669bcaeecce0d9a",
                    "name": "Hongliang Lu",
                    "hidden": false
                },
                {
                    "_id": "68f85c9b7669bcaeecce0d9b",
                    "user": {
                        "_id": "669ffabefbfc379ab3d8fbb0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xCjsWtFvbZPLRrOv_1zZD.jpeg",
                        "isPro": false,
                        "fullname": "Yuhang Wen",
                        "user": "Necolizer",
                        "type": "user"
                    },
                    "name": "Yuhang Wen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-22T15:43:23.308Z",
                    "hidden": false
                },
                {
                    "_id": "68f85c9b7669bcaeecce0d9c",
                    "name": "Pengyu Cheng",
                    "hidden": false
                },
                {
                    "_id": "68f85c9b7669bcaeecce0d9d",
                    "name": "Ruijin Ding",
                    "hidden": false
                },
                {
                    "_id": "68f85c9b7669bcaeecce0d9e",
                    "name": "Haotian Xu",
                    "hidden": false
                },
                {
                    "_id": "68f85c9b7669bcaeecce0d9f",
                    "name": "Jiaqi Guo",
                    "hidden": false
                },
                {
                    "_id": "68f85c9b7669bcaeecce0da0",
                    "name": "Chutian Wang",
                    "hidden": false
                },
                {
                    "_id": "68f85c9b7669bcaeecce0da1",
                    "name": "Haonan Chen",
                    "hidden": false
                },
                {
                    "_id": "68f85c9b7669bcaeecce0da2",
                    "name": "Xiaoxi Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f85c9b7669bcaeecce0da3",
                    "name": "Guanjun Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T17:19:35.000Z",
            "submittedOnDailyAt": "2025-10-24T03:42:50.965Z",
            "title": "Search Self-play: Pushing the Frontier of Agent Capability without\n  Supervision",
            "submittedOnDailyBy": {
                "_id": "669ffabefbfc379ab3d8fbb0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xCjsWtFvbZPLRrOv_1zZD.jpeg",
                "isPro": false,
                "fullname": "Yuhang Wen",
                "user": "Necolizer",
                "type": "user"
            },
            "summary": "Reinforcement learning with verifiable rewards (RLVR) has become the\nmainstream technique for training LLM agents. However, RLVR highly depends on\nwell-crafted task queries and corresponding ground-truth answers to provide\naccurate rewards, which requires massive human efforts and hinders the RL\nscaling processes, especially under agentic scenarios. Although a few recent\nworks explore task synthesis methods, the difficulty of generated agentic tasks\ncan hardly be controlled to provide effective RL training advantages. To\nachieve agentic RLVR with higher scalability, we explore self-play training for\ndeep search agents, in which the learning LLM utilizes multi-turn search engine\ncalling and acts simultaneously as both a task proposer and a problem solver.\nThe task proposer aims to generate deep search queries with well-defined\nground-truth answers and increasing task difficulty. The problem solver tries\nto handle the generated search queries and output the correct answer\npredictions. To ensure that each generated search query has accurate ground\ntruth, we collect all the searching results from the proposer's trajectory as\nexternal knowledge, then conduct retrieval-augmentation generation (RAG) to\ntest whether the proposed query can be correctly answered with all necessary\nsearch documents provided. In this search self-play (SSP) game, the proposer\nand the solver co-evolve their agent capabilities through both competition and\ncooperation. With substantial experimental results, we find that SSP can\nsignificantly improve search agents' performance uniformly on various\nbenchmarks without any supervision under both from-scratch and continuous RL\ntraining setups. The code is at https://github.com/Alibaba-Quark/SSP.",
            "upvotes": 6,
            "discussionId": "68f85c9c7669bcaeecce0da4",
            "githubRepo": "https://github.com/Alibaba-Quark/SSP",
            "ai_summary": "Self-play training for deep search agents improves performance through co-evolution of task generation and problem solving without supervision.",
            "ai_keywords": [
                "reinforcement learning with verifiable rewards",
                "RLVR",
                "self-play training",
                "deep search agents",
                "multi-turn search engine calling",
                "task proposer",
                "problem solver",
                "retrieval-augmentation generation",
                "RAG",
                "search self-play",
                "SSP",
                "from-scratch",
                "continuous RL training"
            ],
            "githubStars": 9,
            "organization": {
                "_id": "6765550c990bcf161cc7e94e",
                "name": "Quark-LLM",
                "fullname": "Quark",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67655397f7ada4603851ddb3/v51dcR1-2Z-ndlUi-1jQI.png"
            }
        },
        "publishedAt": "2025-10-21T13:19:35.000Z",
        "title": "Search Self-play: Pushing the Frontier of Agent Capability without\n  Supervision",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has become the\nmainstream technique for training LLM agents. However, RLVR highly depends on\nwell-crafted task queries and corresponding ground-truth answers to provide\naccurate rewards, which requires massive human efforts and hinders the RL\nscaling processes, especially under agentic scenarios. Although a few recent\nworks explore task synthesis methods, the difficulty of generated agentic tasks\ncan hardly be controlled to provide effective RL training advantages. To\nachieve agentic RLVR with higher scalability, we explore self-play training for\ndeep search agents, in which the learning LLM utilizes multi-turn search engine\ncalling and acts simultaneously as both a task proposer and a problem solver.\nThe task proposer aims to generate deep search queries with well-defined\nground-truth answers and increasing task difficulty. The problem solver tries\nto handle the generated search queries and output the correct answer\npredictions. To ensure that each generated search query has accurate ground\ntruth, we collect all the searching results from the proposer's trajectory as\nexternal knowledge, then conduct retrieval-augmentation generation (RAG) to\ntest whether the proposed query can be correctly answered with all necessary\nsearch documents provided. In this search self-play (SSP) game, the proposer\nand the solver co-evolve their agent capabilities through both competition and\ncooperation. With substantial experimental results, we find that SSP can\nsignificantly improve search agents' performance uniformly on various\nbenchmarks without any supervision under both from-scratch and continuous RL\ntraining setups. The code is at https://github.com/Alibaba-Quark/SSP.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18821.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "669ffabefbfc379ab3d8fbb0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xCjsWtFvbZPLRrOv_1zZD.jpeg",
            "fullname": "Yuhang Wen",
            "name": "Necolizer",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "6765550c990bcf161cc7e94e",
            "name": "Quark-LLM",
            "fullname": "Quark",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67655397f7ada4603851ddb3/v51dcR1-2Z-ndlUi-1jQI.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.20820",
            "authors": [
                {
                    "_id": "68fada3af158a71c5a2f5812",
                    "name": "Guocheng Gordon Qian",
                    "hidden": false
                },
                {
                    "_id": "68fada3af158a71c5a2f5813",
                    "name": "Ruihang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68fada3af158a71c5a2f5814",
                    "name": "Tsai-Shien Chen",
                    "hidden": false
                },
                {
                    "_id": "68fada3af158a71c5a2f5815",
                    "name": "Yusuf Dalva",
                    "hidden": false
                },
                {
                    "_id": "68fada3af158a71c5a2f5816",
                    "name": "Anujraaj Argo Goyal",
                    "hidden": false
                },
                {
                    "_id": "68fada3af158a71c5a2f5817",
                    "name": "Willi Menapace",
                    "hidden": false
                },
                {
                    "_id": "68fada3af158a71c5a2f5818",
                    "name": "Ivan Skorokhodov",
                    "hidden": false
                },
                {
                    "_id": "68fada3af158a71c5a2f5819",
                    "name": "Meng Dong",
                    "hidden": false
                },
                {
                    "_id": "68fada3af158a71c5a2f581a",
                    "name": "Arpit Sahni",
                    "hidden": false
                },
                {
                    "_id": "68fada3af158a71c5a2f581b",
                    "name": "Daniil Ostashev",
                    "hidden": false
                },
                {
                    "_id": "68fada3af158a71c5a2f581c",
                    "name": "Ju Hu",
                    "hidden": false
                },
                {
                    "_id": "68fada3af158a71c5a2f581d",
                    "name": "Sergey Tulyakov",
                    "hidden": false
                },
                {
                    "_id": "68fada3af158a71c5a2f581e",
                    "name": "Kuan-Chieh Jackson Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T17:59:55.000Z",
            "submittedOnDailyAt": "2025-10-24T00:16:05.053Z",
            "title": "LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered\n  Canvas",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Despite their impressive visual fidelity, existing personalized generative\nmodels lack interactive control over spatial composition and scale poorly to\nmultiple subjects. To address these limitations, we present LayerComposer, an\ninteractive framework for personalized, multi-subject text-to-image generation.\nOur approach introduces two main contributions: (1) a layered canvas, a novel\nrepresentation in which each subject is placed on a distinct layer, enabling\nocclusion-free composition; and (2) a locking mechanism that preserves selected\nlayers with high fidelity while allowing the remaining layers to adapt flexibly\nto the surrounding context. Similar to professional image-editing software, the\nproposed layered canvas allows users to place, resize, or lock input subjects\nthrough intuitive layer manipulation. Our versatile locking mechanism requires\nno architectural changes, relying instead on inherent positional embeddings\ncombined with a new complementary data sampling strategy. Extensive experiments\ndemonstrate that LayerComposer achieves superior spatial control and identity\npreservation compared to the state-of-the-art methods in multi-subject\npersonalized image generation.",
            "upvotes": 5,
            "discussionId": "68fada3af158a71c5a2f581f",
            "projectPage": "https://snap-research.github.io/layercomposer/",
            "ai_summary": "LayerComposer provides interactive control over spatial composition and scalability in multi-subject text-to-image generation through a layered canvas and locking mechanism.",
            "ai_keywords": [
                "layered canvas",
                "locking mechanism",
                "positional embeddings",
                "data sampling strategy",
                "text-to-image generation",
                "spatial control",
                "identity preservation"
            ],
            "organization": {
                "_id": "63c87c41cd6a490608ce31d1",
                "name": "snap-research",
                "fullname": "Snap Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"
            }
        },
        "publishedAt": "2025-10-23T13:59:55.000Z",
        "title": "LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered\n  Canvas",
        "summary": "Despite their impressive visual fidelity, existing personalized generative\nmodels lack interactive control over spatial composition and scale poorly to\nmultiple subjects. To address these limitations, we present LayerComposer, an\ninteractive framework for personalized, multi-subject text-to-image generation.\nOur approach introduces two main contributions: (1) a layered canvas, a novel\nrepresentation in which each subject is placed on a distinct layer, enabling\nocclusion-free composition; and (2) a locking mechanism that preserves selected\nlayers with high fidelity while allowing the remaining layers to adapt flexibly\nto the surrounding context. Similar to professional image-editing software, the\nproposed layered canvas allows users to place, resize, or lock input subjects\nthrough intuitive layer manipulation. Our versatile locking mechanism requires\nno architectural changes, relying instead on inherent positional embeddings\ncombined with a new complementary data sampling strategy. Extensive experiments\ndemonstrate that LayerComposer achieves superior spatial control and identity\npreservation compared to the state-of-the-art methods in multi-subject\npersonalized image generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20820.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 143
        },
        "organization": {
            "_id": "63c87c41cd6a490608ce31d1",
            "name": "snap-research",
            "fullname": "Snap Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.20470",
            "authors": [
                {
                    "_id": "68faec7af158a71c5a2f5879",
                    "name": "Kun Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68faec7af158a71c5a2f587a",
                    "name": "Yuanxin Liu",
                    "hidden": false
                },
                {
                    "_id": "68faec7af158a71c5a2f587b",
                    "name": "Linli Yao",
                    "hidden": false
                },
                {
                    "_id": "68faec7af158a71c5a2f587c",
                    "name": "Yishuo Cai",
                    "hidden": false
                },
                {
                    "_id": "68faec7af158a71c5a2f587d",
                    "name": "Hao Zhou",
                    "hidden": false
                },
                {
                    "_id": "68faec7af158a71c5a2f587e",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "68faec7af158a71c5a2f587f",
                    "name": "Fandong Meng",
                    "hidden": false
                },
                {
                    "_id": "68faec7af158a71c5a2f5880",
                    "name": "Xu Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T12:11:46.000Z",
            "submittedOnDailyAt": "2025-10-24T01:36:56.303Z",
            "title": "Conan: Progressive Learning to Reason Like a Detective over Multi-Scale\n  Visual Evidence",
            "submittedOnDailyBy": {
                "_id": "62cd7aca7a036fc9941bb2b0",
                "avatarUrl": "/avatars/17a4d27af0243fd7dccf06066f671461.svg",
                "isPro": false,
                "fullname": "kun ouyang",
                "user": "RUBBISHLIKE",
                "type": "user"
            },
            "summary": "Video reasoning, which requires multi-step deduction across frames, remains a\nmajor challenge for multimodal large language models (MLLMs). While\nreinforcement learning (RL)-based methods enhance reasoning capabilities, they\noften rely on text-only chains that yield ungrounded or hallucinated\nconclusions. Conversely, frame-retrieval approaches introduce visual grounding\nbut still struggle with inaccurate evidence localization. To address these\nchallenges, we present Conan, a framework for evidence-grounded multi-step\nvideo reasoning. Conan identifies contextual and evidence frames, reasons over\ncross-frame clues, and adaptively decides when to conclude or explore further.\nTo achieve this, we (1) construct Conan-91K, a large-scale dataset of\nautomatically generated reasoning traces that includes frame identification,\nevidence reasoning, and action decision, and (2) design a multi-stage\nprogressive cold-start strategy combined with an\nIdentification-Reasoning-Action (AIR) RLVR training framework to jointly\nenhance multi-step visual reasoning. Extensive experiments on six multi-step\nreasoning benchmarks demonstrate that Conan surpasses the baseline\nQwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving\nstate-of-the-art performance. Furthermore, Conan generalizes effectively to\nlong-video understanding tasks, validating its strong scalability and\nrobustness.",
            "upvotes": 5,
            "discussionId": "68faec7af158a71c5a2f5881",
            "githubRepo": "https://github.com/OuyangKun10/Conan",
            "ai_summary": "Conan, a framework for evidence-grounded multi-step video reasoning, enhances visual grounding and reasoning accuracy through a multi-stage training strategy and outperforms existing models on various benchmarks.",
            "ai_keywords": [
                "video reasoning",
                "multimodal large language models",
                "reinforcement learning",
                "frame-retrieval",
                "contextual frames",
                "evidence frames",
                "cross-frame clues",
                "multi-stage progressive cold-start strategy",
                "Identification-Reasoning-Action (AIR) RLVR training framework",
                "long-video understanding"
            ],
            "githubStars": 4,
            "organization": {
                "_id": "61dcd8e344f59573371b5cb6",
                "name": "PekingUniversity",
                "fullname": "Peking University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
            }
        },
        "publishedAt": "2025-10-23T08:11:46.000Z",
        "title": "Conan: Progressive Learning to Reason Like a Detective over Multi-Scale\n  Visual Evidence",
        "summary": "Video reasoning, which requires multi-step deduction across frames, remains a\nmajor challenge for multimodal large language models (MLLMs). While\nreinforcement learning (RL)-based methods enhance reasoning capabilities, they\noften rely on text-only chains that yield ungrounded or hallucinated\nconclusions. Conversely, frame-retrieval approaches introduce visual grounding\nbut still struggle with inaccurate evidence localization. To address these\nchallenges, we present Conan, a framework for evidence-grounded multi-step\nvideo reasoning. Conan identifies contextual and evidence frames, reasons over\ncross-frame clues, and adaptively decides when to conclude or explore further.\nTo achieve this, we (1) construct Conan-91K, a large-scale dataset of\nautomatically generated reasoning traces that includes frame identification,\nevidence reasoning, and action decision, and (2) design a multi-stage\nprogressive cold-start strategy combined with an\nIdentification-Reasoning-Action (AIR) RLVR training framework to jointly\nenhance multi-step visual reasoning. Extensive experiments on six multi-step\nreasoning benchmarks demonstrate that Conan surpasses the baseline\nQwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving\nstate-of-the-art performance. Furthermore, Conan generalizes effectively to\nlong-video understanding tasks, validating its strong scalability and\nrobustness.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20470.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62cd7aca7a036fc9941bb2b0",
            "avatarUrl": "/avatars/17a4d27af0243fd7dccf06066f671461.svg",
            "fullname": "kun ouyang",
            "name": "RUBBISHLIKE",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.12487",
            "authors": [
                {
                    "_id": "68f22352e624abe1d1f0ff54",
                    "user": {
                        "_id": "6440ff39ad24e9b2cfba8575",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6440ff39ad24e9b2cfba8575/QoQC60vbfd2YzTtPAXpM-.jpeg",
                        "isPro": false,
                        "fullname": "Evgeniy Glukhov",
                        "user": "jenyag",
                        "type": "user"
                    },
                    "name": "Evgeniy Glukhov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:14:18.740Z",
                    "hidden": false
                },
                {
                    "_id": "68f22352e624abe1d1f0ff55",
                    "name": "Michele Conti",
                    "hidden": false
                },
                {
                    "_id": "68f22352e624abe1d1f0ff56",
                    "user": {
                        "_id": "64380bed961bb61e463bf93d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64380bed961bb61e463bf93d/zsel0Dzv1yU9O8zAxvCBw.jpeg",
                        "isPro": false,
                        "fullname": "Egor Bogomolov",
                        "user": "egor-bogomolov",
                        "type": "user"
                    },
                    "name": "Egor Bogomolov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:14:14.811Z",
                    "hidden": false
                },
                {
                    "_id": "68f22352e624abe1d1f0ff57",
                    "name": "Yaroslav Golubev",
                    "hidden": false
                },
                {
                    "_id": "68f22352e624abe1d1f0ff58",
                    "name": "Alexander Bezzubov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-14T13:23:01.000Z",
            "submittedOnDailyAt": "2025-10-24T07:29:48.641Z",
            "title": "Diff-XYZ: A Benchmark for Evaluating Diff Understanding",
            "submittedOnDailyBy": {
                "_id": "6440ff39ad24e9b2cfba8575",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6440ff39ad24e9b2cfba8575/QoQC60vbfd2YzTtPAXpM-.jpeg",
                "isPro": false,
                "fullname": "Evgeniy Glukhov",
                "user": "jenyag",
                "type": "user"
            },
            "summary": "Reliable handling of code diffs is central to agents that edit and refactor\nrepositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff\nunderstanding with three supervised tasks: apply (old code + diff\nrightarrow new code), anti-apply (new code - diff rightarrow old code),\nand diff generation (new code - old code rightarrow diff). Instances in\nthe benchmark are triples langle old code, new code,\ndiff rangle drawn from real commits in CommitPackFT, paired with\nautomatic metrics and a clear evaluation protocol. We use the benchmark to do a\nfocused empirical study of the unified diff format and run a cross-format\ncomparison of different diff representations. Our findings reveal that\ndifferent formats should be used depending on the use case and model size. For\nexample, representing diffs in search-replace format is good for larger models\nin the diff generation scenario, yet not suited well for diff analysis and\nsmaller models. The Diff-XYZ benchmark is a reusable foundation for assessing\nand improving diff handling in LLMs that can aid future development of diff\nformats and models editing code. The dataset is published on HuggingFace Hub:\nhttps://huggingface.co/datasets/JetBrains-Research/diff-xyz.",
            "upvotes": 4,
            "discussionId": "68f22352e624abe1d1f0ff59",
            "ai_summary": "A benchmark for code-diff understanding with tasks including apply, anti-apply, and diff generation, revealing optimal diff formats based on model size and use case.",
            "ai_keywords": [
                "code-diff understanding",
                "unified diff format",
                "search-replace format",
                "diff generation",
                "diff analysis",
                "LLMs"
            ],
            "organization": {
                "_id": "64665ffb326128fd2c6b2708",
                "name": "JetBrains-Research",
                "fullname": "JetBrains Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6457bd2cf9b42c048d0f771d/Bz6WJPFu9MBOn7d7xQVai.png"
            }
        },
        "publishedAt": "2025-10-14T09:23:01.000Z",
        "title": "Diff-XYZ: A Benchmark for Evaluating Diff Understanding",
        "summary": "Reliable handling of code diffs is central to agents that edit and refactor\nrepositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff\nunderstanding with three supervised tasks: apply (old code + diff\nrightarrow new code), anti-apply (new code - diff rightarrow old code),\nand diff generation (new code - old code rightarrow diff). Instances in\nthe benchmark are triples langle old code, new code,\ndiff rangle drawn from real commits in CommitPackFT, paired with\nautomatic metrics and a clear evaluation protocol. We use the benchmark to do a\nfocused empirical study of the unified diff format and run a cross-format\ncomparison of different diff representations. Our findings reveal that\ndifferent formats should be used depending on the use case and model size. For\nexample, representing diffs in search-replace format is good for larger models\nin the diff generation scenario, yet not suited well for diff analysis and\nsmaller models. The Diff-XYZ benchmark is a reusable foundation for assessing\nand improving diff handling in LLMs that can aid future development of diff\nformats and models editing code. The dataset is published on HuggingFace Hub:\nhttps://huggingface.co/datasets/JetBrains-Research/diff-xyz.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12487.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6440ff39ad24e9b2cfba8575",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6440ff39ad24e9b2cfba8575/QoQC60vbfd2YzTtPAXpM-.jpeg",
            "fullname": "Evgeniy Glukhov",
            "name": "jenyag",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "64665ffb326128fd2c6b2708",
            "name": "JetBrains-Research",
            "fullname": "JetBrains Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6457bd2cf9b42c048d0f771d/Bz6WJPFu9MBOn7d7xQVai.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.20803",
            "authors": [
                {
                    "_id": "68fae536f158a71c5a2f5848",
                    "user": {
                        "_id": "64e848dd9a3cd93b371166cf",
                        "avatarUrl": "/avatars/6fcee1fe59624113fcf233caf0197729.svg",
                        "isPro": false,
                        "fullname": "Xiaolong Wang",
                        "user": "Xiaolong-Wang",
                        "type": "user"
                    },
                    "name": "Xiaolong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:11:26.153Z",
                    "hidden": false
                },
                {
                    "_id": "68fae536f158a71c5a2f5849",
                    "name": "Lixiang Ru",
                    "hidden": false
                },
                {
                    "_id": "68fae536f158a71c5a2f584a",
                    "name": "Ziyuan Huang",
                    "hidden": false
                },
                {
                    "_id": "68fae536f158a71c5a2f584b",
                    "name": "Kaixiang Ji",
                    "hidden": false
                },
                {
                    "_id": "68fae536f158a71c5a2f584c",
                    "name": "Dandan Zheng",
                    "hidden": false
                },
                {
                    "_id": "68fae536f158a71c5a2f584d",
                    "name": "Jingdong Chen",
                    "hidden": false
                },
                {
                    "_id": "68fae536f158a71c5a2f584e",
                    "name": "Jun Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T17:58:26.000Z",
            "submittedOnDailyAt": "2025-10-24T01:04:30.815Z",
            "title": "ARGenSeg: Image Segmentation with Autoregressive Image Generation Model",
            "submittedOnDailyBy": {
                "_id": "64e848dd9a3cd93b371166cf",
                "avatarUrl": "/avatars/6fcee1fe59624113fcf233caf0197729.svg",
                "isPro": false,
                "fullname": "Xiaolong Wang",
                "user": "Xiaolong-Wang",
                "type": "user"
            },
            "summary": "We propose a novel AutoRegressive Generation-based paradigm for image\nSegmentation (ARGenSeg), achieving multimodal understanding and pixel-level\nperception within a unified framework. Prior works integrating image\nsegmentation into multimodal large language models (MLLMs) typically employ\neither boundary points representation or dedicated segmentation heads. These\nmethods rely on discrete representations or semantic prompts fed into\ntask-specific decoders, which limits the ability of the MLLM to capture\nfine-grained visual details. To address these challenges, we introduce a\nsegmentation framework for MLLM based on image generation, which naturally\nproduces dense masks for target objects. We leverage MLLM to output visual\ntokens and detokenize them into images using an universal VQ-VAE, making the\nsegmentation fully dependent on the pixel-level understanding of the MLLM. To\nreduce inference latency, we employ a next-scale-prediction strategy to\ngenerate required visual tokens in parallel. Extensive experiments demonstrate\nthat our method surpasses prior state-of-the-art approaches on multiple\nsegmentation datasets with a remarkable boost in inference speed, while\nmaintaining strong understanding capabilities.",
            "upvotes": 3,
            "discussionId": "68fae536f158a71c5a2f584f",
            "ai_summary": "A novel AutoRegressive Generation-based paradigm for image segmentation leverages multimodal large language models and VQ-VAE for dense mask generation, achieving high performance and fast inference.",
            "ai_keywords": [
                "AutoRegressive Generation",
                "image segmentation",
                "multimodal large language models",
                "boundary points representation",
                "segmentation heads",
                "discrete representations",
                "semantic prompts",
                "task-specific decoders",
                "visual tokens",
                "universal VQ-VAE",
                "next-scale-prediction strategy"
            ],
            "organization": {
                "_id": "67aea5c8f086ab0f70ed97c9",
                "name": "inclusionAI",
                "fullname": "inclusionAI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
            }
        },
        "publishedAt": "2025-10-23T13:58:26.000Z",
        "title": "ARGenSeg: Image Segmentation with Autoregressive Image Generation Model",
        "summary": "We propose a novel AutoRegressive Generation-based paradigm for image\nSegmentation (ARGenSeg), achieving multimodal understanding and pixel-level\nperception within a unified framework. Prior works integrating image\nsegmentation into multimodal large language models (MLLMs) typically employ\neither boundary points representation or dedicated segmentation heads. These\nmethods rely on discrete representations or semantic prompts fed into\ntask-specific decoders, which limits the ability of the MLLM to capture\nfine-grained visual details. To address these challenges, we introduce a\nsegmentation framework for MLLM based on image generation, which naturally\nproduces dense masks for target objects. We leverage MLLM to output visual\ntokens and detokenize them into images using an universal VQ-VAE, making the\nsegmentation fully dependent on the pixel-level understanding of the MLLM. To\nreduce inference latency, we employ a next-scale-prediction strategy to\ngenerate required visual tokens in parallel. Extensive experiments demonstrate\nthat our method surpasses prior state-of-the-art approaches on multiple\nsegmentation datasets with a remarkable boost in inference speed, while\nmaintaining strong understanding capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20803.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e848dd9a3cd93b371166cf",
            "avatarUrl": "/avatars/6fcee1fe59624113fcf233caf0197729.svg",
            "fullname": "Xiaolong Wang",
            "name": "Xiaolong-Wang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "67aea5c8f086ab0f70ed97c9",
            "name": "inclusionAI",
            "fullname": "inclusionAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.20771",
            "authors": [
                {
                    "_id": "68fadaeaf158a71c5a2f5821",
                    "name": "Huijie Zhang",
                    "hidden": false
                },
                {
                    "_id": "68fadaeaf158a71c5a2f5822",
                    "name": "Aliaksandr Siarohin",
                    "hidden": false
                },
                {
                    "_id": "68fadaeaf158a71c5a2f5823",
                    "name": "Willi Menapace",
                    "hidden": false
                },
                {
                    "_id": "68fadaeaf158a71c5a2f5824",
                    "name": "Michael Vasilkovsky",
                    "hidden": false
                },
                {
                    "_id": "68fadaeaf158a71c5a2f5825",
                    "name": "Sergey Tulyakov",
                    "hidden": false
                },
                {
                    "_id": "68fadaeaf158a71c5a2f5826",
                    "name": "Qing Qu",
                    "hidden": false
                },
                {
                    "_id": "68fadaeaf158a71c5a2f5827",
                    "name": "Ivan Skorokhodov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T17:45:06.000Z",
            "submittedOnDailyAt": "2025-10-24T00:19:24.773Z",
            "title": "AlphaFlow: Understanding and Improving MeanFlow Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "MeanFlow has recently emerged as a powerful framework for few-step generative\nmodeling trained from scratch, but its success is not yet fully understood. In\nthis work, we show that the MeanFlow objective naturally decomposes into two\nparts: trajectory flow matching and trajectory consistency. Through gradient\nanalysis, we find that these terms are strongly negatively correlated, causing\noptimization conflict and slow convergence. Motivated by these insights, we\nintroduce alpha-Flow, a broad family of objectives that unifies trajectory\nflow matching, Shortcut Model, and MeanFlow under one formulation. By adopting\na curriculum strategy that smoothly anneals from trajectory flow matching to\nMeanFlow, alpha-Flow disentangles the conflicting objectives, and achieves\nbetter convergence. When trained from scratch on class-conditional ImageNet-1K\n256x256 with vanilla DiT backbones, alpha-Flow consistently outperforms\nMeanFlow across scales and settings. Our largest alpha-Flow-XL/2+ model\nachieves new state-of-the-art results using vanilla DiT backbones, with FID\nscores of 2.58 (1-NFE) and 2.15 (2-NFE).",
            "upvotes": 3,
            "discussionId": "68fadaebf158a71c5a2f5828",
            "ai_summary": "The $\\alpha$-Flow framework improves few-step generative modeling by unifying and disentangling conflicting objectives, leading to better convergence and state-of-the-art performance on ImageNet-1K.",
            "ai_keywords": [
                "MeanFlow",
                "trajectory flow matching",
                "trajectory consistency",
                "gradient analysis",
                "$\\alpha$-Flow",
                "Shortcut Model",
                "curriculum strategy",
                "class-conditional",
                "ImageNet-1K",
                "DiT backbones",
                "FID scores"
            ],
            "organization": {
                "_id": "63c87c41cd6a490608ce31d1",
                "name": "snap-research",
                "fullname": "Snap Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"
            }
        },
        "publishedAt": "2025-10-23T13:45:06.000Z",
        "title": "AlphaFlow: Understanding and Improving MeanFlow Models",
        "summary": "MeanFlow has recently emerged as a powerful framework for few-step generative\nmodeling trained from scratch, but its success is not yet fully understood. In\nthis work, we show that the MeanFlow objective naturally decomposes into two\nparts: trajectory flow matching and trajectory consistency. Through gradient\nanalysis, we find that these terms are strongly negatively correlated, causing\noptimization conflict and slow convergence. Motivated by these insights, we\nintroduce alpha-Flow, a broad family of objectives that unifies trajectory\nflow matching, Shortcut Model, and MeanFlow under one formulation. By adopting\na curriculum strategy that smoothly anneals from trajectory flow matching to\nMeanFlow, alpha-Flow disentangles the conflicting objectives, and achieves\nbetter convergence. When trained from scratch on class-conditional ImageNet-1K\n256x256 with vanilla DiT backbones, alpha-Flow consistently outperforms\nMeanFlow across scales and settings. Our largest alpha-Flow-XL/2+ model\nachieves new state-of-the-art results using vanilla DiT backbones, with FID\nscores of 2.58 (1-NFE) and 2.15 (2-NFE).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20771.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 143
        },
        "organization": {
            "_id": "63c87c41cd6a490608ce31d1",
            "name": "snap-research",
            "fullname": "Snap Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.20270",
            "authors": [
                {
                    "_id": "68faded5f158a71c5a2f583b",
                    "user": {
                        "_id": "62c0a2e8564b51e080d64af8",
                        "avatarUrl": "/avatars/7ffed6712ead59919832ec71c0e3f5d1.svg",
                        "isPro": true,
                        "fullname": "Ziqian Zhong",
                        "user": "fjzzq2002",
                        "type": "user"
                    },
                    "name": "Ziqian Zhong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:11:30.027Z",
                    "hidden": false
                },
                {
                    "_id": "68faded5f158a71c5a2f583c",
                    "name": "Aditi Raghunathan",
                    "hidden": false
                },
                {
                    "_id": "68faded5f158a71c5a2f583d",
                    "name": "Nicholas Carlini",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T06:58:32.000Z",
            "submittedOnDailyAt": "2025-10-24T00:35:23.999Z",
            "title": "ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The tendency to find and exploit \"shortcuts\" to complete tasks poses\nsignificant risks for reliable assessment and deployment of large language\nmodels (LLMs). For example, an LLM agent with access to unit tests may delete\nfailing tests rather than fix the underlying bug. Such behavior undermines both\nthe validity of benchmark results and the reliability of real-world LLM coding\nassistant deployments.\n  To quantify, study, and mitigate such behavior, we introduce ImpossibleBench,\na benchmark framework that systematically measures LLM agents' propensity to\nexploit test cases. ImpossibleBench creates \"impossible\" variants of tasks from\nexisting benchmarks like LiveCodeBench and SWE-bench by introducing direct\nconflicts between the natural-language specification and the unit tests. We\nmeasure an agent's \"cheating rate\" as its pass rate on these impossible tasks,\nwhere any pass necessarily implies a specification-violating shortcut.\n  As a practical framework, ImpossibleBench is not just an evaluation but a\nversatile tool. We demonstrate its utility for: (1) studying model behaviors,\nrevealing more fine-grained details of cheating behaviors from simple test\nmodification to complex operator overloading; (2) context engineering, showing\nhow prompt, test access and feedback loop affect cheating rates; and (3)\ndeveloping monitoring tools, providing a testbed with verified deceptive\nsolutions. We hope ImpossibleBench serves as a useful framework for building\nmore robust and reliable LLM systems.\n  Our implementation can be found at\nhttps://github.com/safety-research/impossiblebench.",
            "upvotes": 3,
            "discussionId": "68faded5f158a71c5a2f583e",
            "ai_summary": "ImpossibleBench is a benchmark framework that measures and mitigates LLMs' tendency to exploit test cases by introducing impossible task variants, thereby enhancing model reliability.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "unit tests",
                "benchmark framework",
                "ImpossibleBench",
                "LiveCodeBench",
                "SWE-bench",
                "cheating rate",
                "specification-violating shortcuts",
                "context engineering",
                "monitoring tools"
            ]
        },
        "publishedAt": "2025-10-23T02:58:32.000Z",
        "title": "ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases",
        "summary": "The tendency to find and exploit \"shortcuts\" to complete tasks poses\nsignificant risks for reliable assessment and deployment of large language\nmodels (LLMs). For example, an LLM agent with access to unit tests may delete\nfailing tests rather than fix the underlying bug. Such behavior undermines both\nthe validity of benchmark results and the reliability of real-world LLM coding\nassistant deployments.\n  To quantify, study, and mitigate such behavior, we introduce ImpossibleBench,\na benchmark framework that systematically measures LLM agents' propensity to\nexploit test cases. ImpossibleBench creates \"impossible\" variants of tasks from\nexisting benchmarks like LiveCodeBench and SWE-bench by introducing direct\nconflicts between the natural-language specification and the unit tests. We\nmeasure an agent's \"cheating rate\" as its pass rate on these impossible tasks,\nwhere any pass necessarily implies a specification-violating shortcut.\n  As a practical framework, ImpossibleBench is not just an evaluation but a\nversatile tool. We demonstrate its utility for: (1) studying model behaviors,\nrevealing more fine-grained details of cheating behaviors from simple test\nmodification to complex operator overloading; (2) context engineering, showing\nhow prompt, test access and feedback loop affect cheating rates; and (3)\ndeveloping monitoring tools, providing a testbed with verified deceptive\nsolutions. We hope ImpossibleBench serves as a useful framework for building\nmore robust and reliable LLM systems.\n  Our implementation can be found at\nhttps://github.com/safety-research/impossiblebench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20270.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 143
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.20668",
            "authors": [
                {
                    "_id": "68fb1095f158a71c5a2f58dd",
                    "user": {
                        "_id": "63fccdac93b993a4ebd7789a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
                        "isPro": false,
                        "fullname": "Jinbin Bai",
                        "user": "BryanW",
                        "type": "user"
                    },
                    "name": "Jinbin Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:01:08.585Z",
                    "hidden": false
                },
                {
                    "_id": "68fb1095f158a71c5a2f58de",
                    "name": "Yu Lei",
                    "hidden": false
                },
                {
                    "_id": "68fb1095f158a71c5a2f58df",
                    "name": "Hecong Wu",
                    "hidden": false
                },
                {
                    "_id": "68fb1095f158a71c5a2f58e0",
                    "name": "Yuchen Zhu",
                    "hidden": false
                },
                {
                    "_id": "68fb1095f158a71c5a2f58e1",
                    "name": "Shufan Li",
                    "hidden": false
                },
                {
                    "_id": "68fb1095f158a71c5a2f58e2",
                    "name": "Yi Xin",
                    "hidden": false
                },
                {
                    "_id": "68fb1095f158a71c5a2f58e3",
                    "name": "Xiangtai Li",
                    "hidden": false
                },
                {
                    "_id": "68fb1095f158a71c5a2f58e4",
                    "name": "Molei Tao",
                    "hidden": false
                },
                {
                    "_id": "68fb1095f158a71c5a2f58e5",
                    "name": "Aditya Grover",
                    "hidden": false
                },
                {
                    "_id": "68fb1095f158a71c5a2f58e6",
                    "name": "Ming-Hsuan Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T15:46:44.000Z",
            "submittedOnDailyAt": "2025-10-24T04:09:21.195Z",
            "title": "From Masks to Worlds: A Hitchhiker's Guide to World Models",
            "submittedOnDailyBy": {
                "_id": "63fccdac93b993a4ebd7789a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
                "isPro": false,
                "fullname": "Jinbin Bai",
                "user": "BryanW",
                "type": "user"
            },
            "summary": "This is not a typical survey of world models; it is a guide for those who\nwant to build worlds. We do not aim to catalog every paper that has ever\nmentioned a ``world model\". Instead, we follow one clear road: from early\nmasked models that unified representation learning across modalities, to\nunified architectures that share a single paradigm, then to interactive\ngenerative models that close the action-perception loop, and finally to\nmemory-augmented systems that sustain consistent worlds over time. We bypass\nloosely related branches to focus on the core: the generative heart, the\ninteractive loop, and the memory system. We show that this is the most\npromising path towards true world models.",
            "upvotes": 2,
            "discussionId": "68fb1095f158a71c5a2f58e7",
            "githubRepo": "https://github.com/M-E-AGI-Lab/Awesome-World-Models",
            "ai_summary": "The guide outlines a progression from early masked models to memory-augmented systems, emphasizing generative capabilities, interactive loops, and memory for building world models.",
            "ai_keywords": [
                "masked models",
                "representation learning",
                "unified architectures",
                "interactive generative models",
                "action-perception loop",
                "memory-augmented systems",
                "world models"
            ],
            "githubStars": 9
        },
        "publishedAt": "2025-10-23T11:46:44.000Z",
        "title": "From Masks to Worlds: A Hitchhiker's Guide to World Models",
        "summary": "This is not a typical survey of world models; it is a guide for those who\nwant to build worlds. We do not aim to catalog every paper that has ever\nmentioned a ``world model\". Instead, we follow one clear road: from early\nmasked models that unified representation learning across modalities, to\nunified architectures that share a single paradigm, then to interactive\ngenerative models that close the action-perception loop, and finally to\nmemory-augmented systems that sustain consistent worlds over time. We bypass\nloosely related branches to focus on the core: the generative heart, the\ninteractive loop, and the memory system. We show that this is the most\npromising path towards true world models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20668.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63fccdac93b993a4ebd7789a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
            "fullname": "Jinbin Bai",
            "name": "BryanW",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.17853",
            "authors": [
                {
                    "_id": "68fb9dd56cdff8b857f46bbb",
                    "user": {
                        "_id": "65562c0d1f308b7658ee2e1c",
                        "avatarUrl": "/avatars/c9619dc81555d93f9720eb10b733202c.svg",
                        "isPro": false,
                        "fullname": "Kath Choi",
                        "user": "KathCYM",
                        "type": "user"
                    },
                    "name": "Yee Man Choi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T15:57:30.873Z",
                    "hidden": false
                },
                {
                    "_id": "68fb9dd56cdff8b857f46bbc",
                    "name": "Xuehang Guo",
                    "hidden": false
                },
                {
                    "_id": "68fb9dd56cdff8b857f46bbd",
                    "name": "Yi R.",
                    "hidden": false
                },
                {
                    "_id": "68fb9dd56cdff8b857f46bbe",
                    "name": "Fung",
                    "hidden": false
                },
                {
                    "_id": "68fb9dd56cdff8b857f46bbf",
                    "user": {
                        "_id": "628ba530ac304a69264afb75",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653319185560-628ba530ac304a69264afb75.jpeg",
                        "isPro": false,
                        "fullname": "Qingyun Wang",
                        "user": "eaglew",
                        "type": "user"
                    },
                    "name": "Qingyun Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T15:57:33.565Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T00:32:26.000Z",
            "submittedOnDailyAt": "2025-10-24T14:11:10.260Z",
            "title": "CiteGuard: Faithful Citation Attribution for LLMs via\n  Retrieval-Augmented Validation",
            "submittedOnDailyBy": {
                "_id": "628ba530ac304a69264afb75",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653319185560-628ba530ac304a69264afb75.jpeg",
                "isPro": false,
                "fullname": "Qingyun Wang",
                "user": "eaglew",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have emerged as promising assistants for\nscientific writing. However, there have been concerns regarding the quality and\nreliability of the generated text, one of which is the citation accuracy and\nfaithfulness. While most recent work relies on methods such as LLM-as-a-Judge,\nthe reliability of LLM-as-a-Judge alone is also in doubt. In this work, we\nreframe citation evaluation as a problem of citation attribution alignment,\nwhich is assessing whether LLM-generated citations match those a human author\nwould include for the same text. We propose CiteGuard, a retrieval-aware agent\nframework designed to provide more faithful grounding for citation validation.\nCiteGuard improves the prior baseline by 12.3%, and achieves up to 65.4%\naccuracy on the CiteME benchmark, on par with human-level performance (69.7%).\nIt also enables the identification of alternative but valid citations.",
            "upvotes": 2,
            "discussionId": "68fb9dd56cdff8b857f46bc0",
            "projectPage": "https://kathcym.github.io/CiteGuard_Page/",
            "githubRepo": "https://github.com/KathCYM/CiteGuard",
            "ai_summary": "CiteGuard, a retrieval-aware agent framework, enhances citation accuracy in LLM-generated text by aligning citations with human choices, achieving near-human performance.",
            "ai_keywords": [
                "Large Language Models",
                "LLM-as-a-Judge",
                "citation evaluation",
                "citation attribution alignment",
                "retrieval-aware agent",
                "CiteGuard",
                "CiteME benchmark"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-10-14T20:32:26.000Z",
        "title": "CiteGuard: Faithful Citation Attribution for LLMs via\n  Retrieval-Augmented Validation",
        "summary": "Large Language Models (LLMs) have emerged as promising assistants for\nscientific writing. However, there have been concerns regarding the quality and\nreliability of the generated text, one of which is the citation accuracy and\nfaithfulness. While most recent work relies on methods such as LLM-as-a-Judge,\nthe reliability of LLM-as-a-Judge alone is also in doubt. In this work, we\nreframe citation evaluation as a problem of citation attribution alignment,\nwhich is assessing whether LLM-generated citations match those a human author\nwould include for the same text. We propose CiteGuard, a retrieval-aware agent\nframework designed to provide more faithful grounding for citation validation.\nCiteGuard improves the prior baseline by 12.3%, and achieves up to 65.4%\naccuracy on the CiteME benchmark, on par with human-level performance (69.7%).\nIt also enables the identification of alternative but valid citations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17853.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "628ba530ac304a69264afb75",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653319185560-628ba530ac304a69264afb75.jpeg",
            "fullname": "Qingyun Wang",
            "name": "eaglew",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.19423",
            "authors": [
                {
                    "_id": "68fb79986cdff8b857f46b30",
                    "user": {
                        "_id": "67ecfe11534d68f5a87834c2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ZnmCQzju-Okic9XdvJEvN.png",
                        "isPro": false,
                        "fullname": "董家愷",
                        "user": "Snooow1029",
                        "type": "user"
                    },
                    "name": "Jia-Kai Dong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T15:57:36.735Z",
                    "hidden": false
                },
                {
                    "_id": "68fb79986cdff8b857f46b31",
                    "name": "I-Wei Huang",
                    "hidden": false
                },
                {
                    "_id": "68fb79986cdff8b857f46b32",
                    "name": "Chun-Tin Wu",
                    "hidden": false
                },
                {
                    "_id": "68fb79986cdff8b857f46b33",
                    "name": "Yi-Tien Tsai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T09:45:11.000Z",
            "submittedOnDailyAt": "2025-10-24T16:05:09.442Z",
            "title": "MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration",
            "submittedOnDailyBy": {
                "_id": "67ecfe11534d68f5a87834c2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ZnmCQzju-Okic9XdvJEvN.png",
                "isPro": false,
                "fullname": "董家愷",
                "user": "Snooow1029",
                "type": "user"
            },
            "summary": "We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop,\nend-to-end tool orchestration by LLM agents in a hierarchical Model-Context\nProtocol (MCP) ecosystem. Existing benchmarks often evaluate tools in\nisolation, ignoring challenges such as functional overlap and cross-server\norchestration, leading to overly optimistic assessments. MSC-Bench addresses\nthese gaps by constructing ground truth through 'equal function sets', allowing\nobjective metrics such as F1 score and reducing the dependency on\nLLM-as-a-judge evaluation. Organized as a five-level curriculum, it\nsystematically tests agent capabilities from single-tool orchestration to\ncomplex cross-server planning, and robustness to out-of-scope requests.\nExperiments reveal that rigid hierarchies can hinder performance without\nco-designed strategies, and even state-of-the-art agents exhibit systemic\nweaknesses in robustness. MSC-Bench provides a diagnostic framework to expose\nthese limitations and guide the development of more capable and efficient\ntool-using agents. The benchmark and resources are publicly available at\nhttps://github.com/snooow1029/MSC_Bench.",
            "upvotes": 1,
            "discussionId": "68fb79986cdff8b857f46b34",
            "ai_summary": "MSC-Bench evaluates multi-hop tool orchestration by LLM agents in a hierarchical ecosystem, addressing challenges like functional overlap and cross-server planning with a five-level curriculum and objective metrics.",
            "ai_keywords": [
                "LLM agents",
                "hierarchical Model-Context Protocol",
                "multi-hop tool orchestration",
                "equal function sets",
                "F1 score",
                "out-of-scope requests",
                "tool-using agents"
            ],
            "organization": {
                "_id": "673248e121823ee4ea594099",
                "name": "nationaltaiwan",
                "fullname": "台灣大學",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67324880c1f20c742be144b8/CE1UiOtpMeC8pmtdGP4Nn.png"
            }
        },
        "publishedAt": "2025-10-22T05:45:11.000Z",
        "title": "MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration",
        "summary": "We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop,\nend-to-end tool orchestration by LLM agents in a hierarchical Model-Context\nProtocol (MCP) ecosystem. Existing benchmarks often evaluate tools in\nisolation, ignoring challenges such as functional overlap and cross-server\norchestration, leading to overly optimistic assessments. MSC-Bench addresses\nthese gaps by constructing ground truth through 'equal function sets', allowing\nobjective metrics such as F1 score and reducing the dependency on\nLLM-as-a-judge evaluation. Organized as a five-level curriculum, it\nsystematically tests agent capabilities from single-tool orchestration to\ncomplex cross-server planning, and robustness to out-of-scope requests.\nExperiments reveal that rigid hierarchies can hinder performance without\nco-designed strategies, and even state-of-the-art agents exhibit systemic\nweaknesses in robustness. MSC-Bench provides a diagnostic framework to expose\nthese limitations and guide the development of more capable and efficient\ntool-using agents. The benchmark and resources are publicly available at\nhttps://github.com/snooow1029/MSC_Bench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19423.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67ecfe11534d68f5a87834c2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ZnmCQzju-Okic9XdvJEvN.png",
            "fullname": "董家愷",
            "name": "Snooow1029",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "673248e121823ee4ea594099",
            "name": "nationaltaiwan",
            "fullname": "台灣大學",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67324880c1f20c742be144b8/CE1UiOtpMeC8pmtdGP4Nn.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.18245",
            "authors": [
                {
                    "_id": "68f9bf9db9b2e4ae0467393b",
                    "user": {
                        "_id": "620ec8059291e41cab585a3d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620ec8059291e41cab585a3d/iuVrjdeghqo-lz0lRfw7l.jpeg",
                        "isPro": false,
                        "fullname": "Song",
                        "user": "NaiveUser",
                        "type": "user"
                    },
                    "name": "Song Bian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:46:06.549Z",
                    "hidden": false
                },
                {
                    "_id": "68f9bf9db9b2e4ae0467393c",
                    "name": "Tao Yu",
                    "hidden": false
                },
                {
                    "_id": "68f9bf9db9b2e4ae0467393d",
                    "name": "Shivaram Venkataraman",
                    "hidden": false
                },
                {
                    "_id": "68f9bf9db9b2e4ae0467393e",
                    "name": "Youngsuk Park",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T03:08:48.000Z",
            "submittedOnDailyAt": "2025-10-24T15:15:49.361Z",
            "title": "Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs",
            "submittedOnDailyBy": {
                "_id": "620ec8059291e41cab585a3d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620ec8059291e41cab585a3d/iuVrjdeghqo-lz0lRfw7l.jpeg",
                "isPro": false,
                "fullname": "Song",
                "user": "NaiveUser",
                "type": "user"
            },
            "summary": "Scaling the number of parameters and the size of training data has proven to\nbe an effective strategy for improving large language model (LLM) performance.\nYet, as these models grow increasingly powerful and widely deployed, the cost\nof inference has become a pressing concern. Despite its importance, the\ntrade-off between model accuracy and inference efficiency remains\nunderexplored. In this work, we examine how key architectural factors, hidden\nsize, the allocation of parameters between MLP and attention (mlp-to-attention\nratio), and grouped-query attention (GQA), influence both inference cost and\naccuracy. We introduce a conditional scaling law that augments the Chinchilla\nframework with architectural information, along with a search framework for\nidentifying architectures that are simultaneously inference-efficient and\naccurate. To validate our approach, we train more than 200 models spanning 80M\nto 3B parameters and 8B to 100B training tokens, and fit the proposed\nconditional scaling law. Our results show that the conditional scaling law\nreliably predicts optimal architectural choices and that the resulting models\noutperform existing open-source baselines. Under the same training budget,\noptimized architectures achieve up to 2.1% higher accuracy and 42% greater\ninference throughput compared to LLaMA-3.2.",
            "upvotes": 1,
            "discussionId": "68f9bf9eb9b2e4ae0467393f",
            "ai_summary": "A conditional scaling law is introduced to optimize architectural choices for large language models, balancing accuracy and inference efficiency.",
            "ai_keywords": [
                "large language model",
                "LLM",
                "inference cost",
                "hidden size",
                "MLP",
                "attention",
                "mlp-to-attention ratio",
                "grouped-query attention",
                "GQA",
                "Chinchilla framework",
                "conditional scaling law",
                "inference-efficient",
                "accuracy",
                "training budget",
                "LLaMA-3.2"
            ],
            "organization": {
                "_id": "5ffdfbadbba2ae614d771970",
                "name": "amazon",
                "fullname": "Amazon",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
            }
        },
        "publishedAt": "2025-10-20T23:08:48.000Z",
        "title": "Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs",
        "summary": "Scaling the number of parameters and the size of training data has proven to\nbe an effective strategy for improving large language model (LLM) performance.\nYet, as these models grow increasingly powerful and widely deployed, the cost\nof inference has become a pressing concern. Despite its importance, the\ntrade-off between model accuracy and inference efficiency remains\nunderexplored. In this work, we examine how key architectural factors, hidden\nsize, the allocation of parameters between MLP and attention (mlp-to-attention\nratio), and grouped-query attention (GQA), influence both inference cost and\naccuracy. We introduce a conditional scaling law that augments the Chinchilla\nframework with architectural information, along with a search framework for\nidentifying architectures that are simultaneously inference-efficient and\naccurate. To validate our approach, we train more than 200 models spanning 80M\nto 3B parameters and 8B to 100B training tokens, and fit the proposed\nconditional scaling law. Our results show that the conditional scaling law\nreliably predicts optimal architectural choices and that the resulting models\noutperform existing open-source baselines. Under the same training budget,\noptimized architectures achieve up to 2.1% higher accuracy and 42% greater\ninference throughput compared to LLaMA-3.2.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18245.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620ec8059291e41cab585a3d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620ec8059291e41cab585a3d/iuVrjdeghqo-lz0lRfw7l.jpeg",
            "fullname": "Song",
            "name": "NaiveUser",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "5ffdfbadbba2ae614d771970",
            "name": "amazon",
            "fullname": "Amazon",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.20362",
            "authors": [
                {
                    "_id": "68fb47f41ff1070cf1666d8e",
                    "user": {
                        "_id": "65e62a6e9e8bda232c34a309",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e62a6e9e8bda232c34a309/jAW1xVD0tp0dweq7RVGNf.jpeg",
                        "isPro": true,
                        "fullname": "Aritra Roy",
                        "user": "aritraroy24",
                        "type": "user"
                    },
                    "name": "Aritra Roy",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T15:59:17.823Z",
                    "hidden": false
                },
                {
                    "_id": "68fb47f41ff1070cf1666d8f",
                    "name": "Enrico Grisan",
                    "hidden": false
                },
                {
                    "_id": "68fb47f41ff1070cf1666d90",
                    "name": "John Buckeridge",
                    "hidden": false
                },
                {
                    "_id": "68fb47f41ff1070cf1666d91",
                    "name": "Chiara Gattinoni",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T09:01:44.000Z",
            "submittedOnDailyAt": "2025-10-24T08:05:12.445Z",
            "title": "ComProScanner: A multi-agent based framework for composition-property\n  structured data extraction from scientific literature",
            "submittedOnDailyBy": {
                "_id": "65e62a6e9e8bda232c34a309",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e62a6e9e8bda232c34a309/jAW1xVD0tp0dweq7RVGNf.jpeg",
                "isPro": true,
                "fullname": "Aritra Roy",
                "user": "aritraroy24",
                "type": "user"
            },
            "summary": "Since the advent of various pre-trained large language models, extracting\nstructured knowledge from scientific text has experienced a revolutionary\nchange compared with traditional machine learning or natural language\nprocessing techniques. Despite these advances, accessible automated tools that\nallow users to construct, validate, and visualise datasets from scientific\nliterature extraction remain scarce. We therefore developed ComProScanner, an\nautonomous multi-agent platform that facilitates the extraction, validation,\nclassification, and visualisation of machine-readable chemical compositions and\nproperties, integrated with synthesis data from journal articles for\ncomprehensive database creation. We evaluated our framework using 100 journal\narticles against 10 different LLMs, including both open-source and proprietary\nmodels, to extract highly complex compositions associated with ceramic\npiezoelectric materials and corresponding piezoelectric strain coefficients\n(d33), motivated by the lack of a large dataset for such materials.\nDeepSeek-V3-0324 outperformed all models with a significant overall accuracy of\n0.82. This framework provides a simple, user-friendly, readily-usable package\nfor extracting highly complex experimental data buried in the literature to\nbuild machine learning or deep learning datasets.",
            "upvotes": 0,
            "discussionId": "68fb47f41ff1070cf1666d92",
            "ai_summary": "ComProScanner, an autonomous multi-agent platform, extracts, validates, classifies, and visualizes chemical compositions and properties from scientific literature, outperforming various LLMs in accuracy.",
            "ai_keywords": [
                "pre-trained large language models",
                "machine-readable chemical compositions",
                "properties",
                "synthesis data",
                "journal articles",
                "DeepSeek-V3-0324",
                "piezoelectric materials",
                "piezoelectric strain coefficients",
                "machine learning",
                "deep learning datasets"
            ],
            "organization": {
                "_id": "68fb383e87795d21068c6f05",
                "name": "slimeslab",
                "fullname": "South London Innovative Materials Evaluation Squad (SLIMES) Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65e62a6e9e8bda232c34a309/VG4bqLboaVHeYfVgZPIYv.png"
            }
        },
        "publishedAt": "2025-10-23T05:01:44.000Z",
        "title": "ComProScanner: A multi-agent based framework for composition-property\n  structured data extraction from scientific literature",
        "summary": "Since the advent of various pre-trained large language models, extracting\nstructured knowledge from scientific text has experienced a revolutionary\nchange compared with traditional machine learning or natural language\nprocessing techniques. Despite these advances, accessible automated tools that\nallow users to construct, validate, and visualise datasets from scientific\nliterature extraction remain scarce. We therefore developed ComProScanner, an\nautonomous multi-agent platform that facilitates the extraction, validation,\nclassification, and visualisation of machine-readable chemical compositions and\nproperties, integrated with synthesis data from journal articles for\ncomprehensive database creation. We evaluated our framework using 100 journal\narticles against 10 different LLMs, including both open-source and proprietary\nmodels, to extract highly complex compositions associated with ceramic\npiezoelectric materials and corresponding piezoelectric strain coefficients\n(d33), motivated by the lack of a large dataset for such materials.\nDeepSeek-V3-0324 outperformed all models with a significant overall accuracy of\n0.82. This framework provides a simple, user-friendly, readily-usable package\nfor extracting highly complex experimental data buried in the literature to\nbuild machine learning or deep learning datasets.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20362.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e62a6e9e8bda232c34a309",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e62a6e9e8bda232c34a309/jAW1xVD0tp0dweq7RVGNf.jpeg",
            "fullname": "Aritra Roy",
            "name": "aritraroy24",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "68fb383e87795d21068c6f05",
            "name": "slimeslab",
            "fullname": "South London Innovative Materials Evaluation Squad (SLIMES) Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65e62a6e9e8bda232c34a309/VG4bqLboaVHeYfVgZPIYv.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.19995",
            "authors": [
                {
                    "_id": "68fb803d6cdff8b857f46b4d",
                    "name": "Yiming Lu",
                    "hidden": false
                },
                {
                    "_id": "68fb803d6cdff8b857f46b4e",
                    "name": "Xun Wang",
                    "hidden": false
                },
                {
                    "_id": "68fb803d6cdff8b857f46b4f",
                    "name": "Simin Ma",
                    "hidden": false
                },
                {
                    "_id": "68fb803d6cdff8b857f46b50",
                    "name": "Shujian Liu",
                    "hidden": false
                },
                {
                    "_id": "68fb803d6cdff8b857f46b51",
                    "name": "Sathish Reddy Indurthi",
                    "hidden": false
                },
                {
                    "_id": "68fb803d6cdff8b857f46b52",
                    "name": "Song Wang",
                    "hidden": false
                },
                {
                    "_id": "68fb803d6cdff8b857f46b53",
                    "name": "Haoyun Deng",
                    "hidden": false
                },
                {
                    "_id": "68fb803d6cdff8b857f46b54",
                    "name": "Fei Liu",
                    "hidden": false
                },
                {
                    "_id": "68fb803d6cdff8b857f46b55",
                    "name": "Kaiqiang Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T19:48:17.000Z",
            "submittedOnDailyAt": "2025-10-24T12:08:24.133Z",
            "title": "Communication to Completion: Modeling Collaborative Workflows with\n  Intelligent Multi-Agent Communication",
            "submittedOnDailyBy": {
                "_id": "656bf741e9f2c52b988beea8",
                "avatarUrl": "/avatars/8fc28a573f749aa0ba3f5e68255241f2.svg",
                "isPro": false,
                "fullname": "Yiming Lu",
                "user": "BUILDERlym",
                "type": "user"
            },
            "summary": "Teamwork in workspace for complex tasks requires diverse communication\nstrategies, but current multi-agent LLM systems lack systematic frameworks for\ntask oriented communication. We introduce Communication to Completion (C2C), a\nscalable framework that addresses this gap through two key innovations: (1) the\nAlignment Factor (AF), a novel metric quantifying agent task alignment that\ndirectly impacts work efficiency, and (2) a Sequential Action Framework that\nintegrates stepwise execution with intelligent communication decisions. C2C\nenables agents to make cost aware communication choices, dynamically improving\ntask understanding through targeted interactions. We evaluated C2C on realistic\ncoding workflows across three complexity tiers and team sizes from 5 to 17\nagents, comparing against no communication and fixed steps baselines. The\nresults show that C2C reduces the task completion time by about 40% with\nacceptable communication costs. The framework completes all tasks successfully\nin standard configurations and maintains effectiveness at scale. C2C\nestablishes both a theoretical foundation for measuring communication\neffectiveness in multi-agent systems and a practical framework for complex\ncollaborative tasks.",
            "upvotes": 0,
            "discussionId": "68fb803e6cdff8b857f46b56",
            "ai_summary": "C2C, a scalable framework for multi-agent LLM systems, improves task completion time through the Alignment Factor and Sequential Action Framework, enabling cost-aware communication and dynamic task understanding.",
            "ai_keywords": [
                "multi-agent LLM systems",
                "Communication to Completion (C2C)",
                "Alignment Factor (AF)",
                "Sequential Action Framework",
                "task completion time",
                "communication costs",
                "task understanding",
                "coding workflows"
            ]
        },
        "publishedAt": "2025-10-22T15:48:17.000Z",
        "title": "Communication to Completion: Modeling Collaborative Workflows with\n  Intelligent Multi-Agent Communication",
        "summary": "Teamwork in workspace for complex tasks requires diverse communication\nstrategies, but current multi-agent LLM systems lack systematic frameworks for\ntask oriented communication. We introduce Communication to Completion (C2C), a\nscalable framework that addresses this gap through two key innovations: (1) the\nAlignment Factor (AF), a novel metric quantifying agent task alignment that\ndirectly impacts work efficiency, and (2) a Sequential Action Framework that\nintegrates stepwise execution with intelligent communication decisions. C2C\nenables agents to make cost aware communication choices, dynamically improving\ntask understanding through targeted interactions. We evaluated C2C on realistic\ncoding workflows across three complexity tiers and team sizes from 5 to 17\nagents, comparing against no communication and fixed steps baselines. The\nresults show that C2C reduces the task completion time by about 40% with\nacceptable communication costs. The framework completes all tasks successfully\nin standard configurations and maintains effectiveness at scale. C2C\nestablishes both a theoretical foundation for measuring communication\neffectiveness in multi-agent systems and a practical framework for complex\ncollaborative tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19995.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656bf741e9f2c52b988beea8",
            "avatarUrl": "/avatars/8fc28a573f749aa0ba3f5e68255241f2.svg",
            "fullname": "Yiming Lu",
            "name": "BUILDERlym",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18413",
            "authors": [
                {
                    "_id": "68fb0facf158a71c5a2f58d4",
                    "name": "Siyuan Yan",
                    "hidden": false
                },
                {
                    "_id": "68fb0facf158a71c5a2f58d5",
                    "name": "Guo-Qing Jiang",
                    "hidden": false
                },
                {
                    "_id": "68fb0facf158a71c5a2f58d6",
                    "name": "Yuchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68fb0facf158a71c5a2f58d7",
                    "name": "Xiaoxing Ma",
                    "hidden": false
                },
                {
                    "_id": "68fb0facf158a71c5a2f58d8",
                    "name": "Ran Zhu",
                    "hidden": false
                },
                {
                    "_id": "68fb0facf158a71c5a2f58d9",
                    "name": "Chun Cao",
                    "hidden": false
                },
                {
                    "_id": "68fb0facf158a71c5a2f58da",
                    "user": {
                        "_id": "64267a357a0d3f02acd437ec",
                        "avatarUrl": "/avatars/69767c3d2b1b484403cc930f503f63e9.svg",
                        "isPro": false,
                        "fullname": "Jingwei Xu",
                        "user": "ParagonLight",
                        "type": "user"
                    },
                    "name": "Jingwei Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:01:15.560Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T08:44:47.000Z",
            "submittedOnDailyAt": "2025-10-24T14:39:45.902Z",
            "title": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference",
            "submittedOnDailyBy": {
                "_id": "64267a357a0d3f02acd437ec",
                "avatarUrl": "/avatars/69767c3d2b1b484403cc930f503f63e9.svg",
                "isPro": false,
                "fullname": "Jingwei Xu",
                "user": "ParagonLight",
                "type": "user"
            },
            "summary": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity.",
            "upvotes": 0,
            "discussionId": "68fb0facf158a71c5a2f58db",
            "ai_summary": "Adamas, a sparse attention mechanism, achieves high accuracy and speed in long-context inference by using Hadamard transform, bucketization, 2-bit compression, and Manhattan-distance estimation.",
            "ai_keywords": [
                "Hadamard transform",
                "bucketization",
                "2-bit compression",
                "Manhattan-distance estimation",
                "sparse attention",
                "self-attention",
                "perplexity"
            ]
        },
        "publishedAt": "2025-10-21T04:44:47.000Z",
        "title": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference",
        "summary": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18413.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64267a357a0d3f02acd437ec",
            "avatarUrl": "/avatars/69767c3d2b1b484403cc930f503f63e9.svg",
            "fullname": "Jingwei Xu",
            "name": "ParagonLight",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.17896",
            "authors": [
                {
                    "_id": "68fb0671f158a71c5a2f58bd",
                    "name": "Tao Bu",
                    "hidden": false
                },
                {
                    "_id": "68fb0671f158a71c5a2f58be",
                    "name": "Qiangang Wang",
                    "hidden": false
                },
                {
                    "_id": "68fb0671f158a71c5a2f58bf",
                    "name": "Bowen Zeng",
                    "hidden": false
                },
                {
                    "_id": "68fb0671f158a71c5a2f58c0",
                    "name": "Hanwen Sun",
                    "hidden": false
                },
                {
                    "_id": "68fb0671f158a71c5a2f58c1",
                    "name": "Yunpeng Huang",
                    "hidden": false
                },
                {
                    "_id": "68fb0671f158a71c5a2f58c2",
                    "name": "Chun Cao",
                    "hidden": false
                },
                {
                    "_id": "68fb0671f158a71c5a2f58c3",
                    "user": {
                        "_id": "64267a357a0d3f02acd437ec",
                        "avatarUrl": "/avatars/69767c3d2b1b484403cc930f503f63e9.svg",
                        "isPro": false,
                        "fullname": "Jingwei Xu",
                        "user": "ParagonLight",
                        "type": "user"
                    },
                    "name": "Jingwei Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:01:20.533Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-19T07:07:37.000Z",
            "submittedOnDailyAt": "2025-10-24T14:38:58.373Z",
            "title": "Long-Context Attention Benchmark: From Kernel Efficiency to Distributed\n  Context Parallelism",
            "submittedOnDailyBy": {
                "_id": "64267a357a0d3f02acd437ec",
                "avatarUrl": "/avatars/69767c3d2b1b484403cc930f503f63e9.svg",
                "isPro": false,
                "fullname": "Jingwei Xu",
                "user": "ParagonLight",
                "type": "user"
            },
            "summary": "Transformer-based large language models (LLMs) have achieved remarkable\nsuccess, yet their standard attention mechanism incurs quadratic computation\nand memory costs with respect to sequence length, posing a major bottleneck for\nlong-context training. Prior work tackles this challenge along two directions:\n(1) kernel-level optimizations, which accelerate dense and sparse attention\noperators; and (2) module-level strategies, often referred to as distributed\nattention or context parallel training, which scale attention across multiple\ndevices. However, systematic evaluation still remains limited: operator-level\ncomparisons are often incomplete, while context parallel strategies are\ntypically framework-specific, with unclear performance analysis across\ncontexts. To address these gaps, we propose a unified benchmark that integrates\nrepresentative attention kernels and context parallel mechanisms with a modular\nand extensible interface for evaluation. The benchmark evaluates methods along\ntwo critical dimensions: (1) attention mask patterns, which strongly affect\nefficiency, scalability, and usability, and (2) sequence length and distributed\nscale, which determine performance under extreme long-context training. Through\ncomprehensive experiments on the cluster of up to 96 GPUs, our benchmark\nenables reproducible comparisons, highlights method-specific trade-offs, and\nprovides practical guidance for designing and deploying attention mechanisms in\nlong-context LLM training.",
            "upvotes": 0,
            "discussionId": "68fb0671f158a71c5a2f58c4",
            "ai_summary": "A unified benchmark evaluates attention mechanisms in transformer-based LLMs, focusing on efficiency, scalability, and performance across different attention mask patterns and sequence lengths.",
            "ai_keywords": [
                "Transformer-based large language models",
                "attention mechanism",
                "quadratic computation",
                "memory costs",
                "sequence length",
                "kernel-level optimizations",
                "dense attention",
                "sparse attention",
                "distributed attention",
                "context parallel training",
                "attention mask patterns",
                "sequence length",
                "distributed scale",
                "long-context training",
                "reproducible comparisons",
                "method-specific trade-offs"
            ]
        },
        "publishedAt": "2025-10-19T03:07:37.000Z",
        "title": "Long-Context Attention Benchmark: From Kernel Efficiency to Distributed\n  Context Parallelism",
        "summary": "Transformer-based large language models (LLMs) have achieved remarkable\nsuccess, yet their standard attention mechanism incurs quadratic computation\nand memory costs with respect to sequence length, posing a major bottleneck for\nlong-context training. Prior work tackles this challenge along two directions:\n(1) kernel-level optimizations, which accelerate dense and sparse attention\noperators; and (2) module-level strategies, often referred to as distributed\nattention or context parallel training, which scale attention across multiple\ndevices. However, systematic evaluation still remains limited: operator-level\ncomparisons are often incomplete, while context parallel strategies are\ntypically framework-specific, with unclear performance analysis across\ncontexts. To address these gaps, we propose a unified benchmark that integrates\nrepresentative attention kernels and context parallel mechanisms with a modular\nand extensible interface for evaluation. The benchmark evaluates methods along\ntwo critical dimensions: (1) attention mask patterns, which strongly affect\nefficiency, scalability, and usability, and (2) sequence length and distributed\nscale, which determine performance under extreme long-context training. Through\ncomprehensive experiments on the cluster of up to 96 GPUs, our benchmark\nenables reproducible comparisons, highlights method-specific trade-offs, and\nprovides practical guidance for designing and deploying attention mechanisms in\nlong-context LLM training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17896.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64267a357a0d3f02acd437ec",
            "avatarUrl": "/avatars/69767c3d2b1b484403cc930f503f63e9.svg",
            "fullname": "Jingwei Xu",
            "name": "ParagonLight",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.15804",
            "authors": [
                {
                    "_id": "68fb0385f158a71c5a2f58b6",
                    "name": "Shauli Ravfogel",
                    "hidden": false
                },
                {
                    "_id": "68fb0385f158a71c5a2f58b7",
                    "name": "Gilad Yehudai",
                    "hidden": false
                },
                {
                    "_id": "68fb0385f158a71c5a2f58b8",
                    "name": "Tal Linzen",
                    "hidden": false
                },
                {
                    "_id": "68fb0385f158a71c5a2f58b9",
                    "name": "Joan Bruna",
                    "hidden": false
                },
                {
                    "_id": "68fb0385f158a71c5a2f58ba",
                    "name": "Alberto Bietti",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/635686ec5aeb69011c7d1abd/6itgs9RmcaqaIyJcx1dde.jpeg"
            ],
            "publishedAt": "2025-10-17T16:30:07.000Z",
            "submittedOnDailyAt": "2025-10-24T03:41:20.084Z",
            "title": "Emergence of Linear Truth Encodings in Language Models",
            "submittedOnDailyBy": {
                "_id": "635686ec5aeb69011c7d1abd",
                "avatarUrl": "/avatars/c59034ad2c9c2daf4b4a8d3c56449f5e.svg",
                "isPro": false,
                "fullname": "Shauli Ravfogel",
                "user": "ravfogs",
                "type": "user"
            },
            "summary": "Recent probing studies reveal that large language models exhibit linear\nsubspaces that separate true from false statements, yet the mechanism behind\ntheir emergence is unclear. We introduce a transparent, one-layer transformer\ntoy model that reproduces such truth subspaces end-to-end and exposes one\nconcrete route by which they can arise. We study one simple setting in which\ntruth encoding can emerge: a data distribution where factual statements\nco-occur with other factual statements (and vice-versa), encouraging the model\nto learn this distinction in order to lower the LM loss on future tokens. We\ncorroborate this pattern with experiments in pretrained language models.\nFinally, in the toy setting we observe a two-phase learning dynamic: networks\nfirst memorize individual factual associations in a few steps, then -- over a\nlonger horizon -- learn to linearly separate true from false, which in turn\nlowers language-modeling loss. Together, these results provide both a\nmechanistic demonstration and an empirical motivation for how and why linear\ntruth representations can emerge in language models.",
            "upvotes": 0,
            "discussionId": "68fb0385f158a71c5a2f58bb",
            "ai_summary": "A one-layer transformer model demonstrates how linear subspaces separating true from false statements can emerge in language models through memorization and subsequent linear separation.",
            "ai_keywords": [
                "large language models",
                "linear subspaces",
                "truth subspaces",
                "one-layer transformer",
                "truth encoding",
                "factual statements",
                "LM loss",
                "language-modeling loss"
            ]
        },
        "publishedAt": "2025-10-17T12:30:07.000Z",
        "title": "Emergence of Linear Truth Encodings in Language Models",
        "summary": "Recent probing studies reveal that large language models exhibit linear\nsubspaces that separate true from false statements, yet the mechanism behind\ntheir emergence is unclear. We introduce a transparent, one-layer transformer\ntoy model that reproduces such truth subspaces end-to-end and exposes one\nconcrete route by which they can arise. We study one simple setting in which\ntruth encoding can emerge: a data distribution where factual statements\nco-occur with other factual statements (and vice-versa), encouraging the model\nto learn this distinction in order to lower the LM loss on future tokens. We\ncorroborate this pattern with experiments in pretrained language models.\nFinally, in the toy setting we observe a two-phase learning dynamic: networks\nfirst memorize individual factual associations in a few steps, then -- over a\nlonger horizon -- learn to linearly separate true from false, which in turn\nlowers language-modeling loss. Together, these results provide both a\nmechanistic demonstration and an empirical motivation for how and why linear\ntruth representations can emerge in language models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/635686ec5aeb69011c7d1abd/6itgs9RmcaqaIyJcx1dde.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15804.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635686ec5aeb69011c7d1abd",
            "avatarUrl": "/avatars/c59034ad2c9c2daf4b4a8d3c56449f5e.svg",
            "fullname": "Shauli Ravfogel",
            "name": "ravfogs",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    }
]