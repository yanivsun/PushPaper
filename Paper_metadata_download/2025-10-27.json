[
    {
        "paper": {
            "id": "2510.21618",
            "authors": [
                {
                    "_id": "68fed3876cdff8b857f47116",
                    "name": "Xiaoxi Li",
                    "hidden": false
                },
                {
                    "_id": "68fed3876cdff8b857f47117",
                    "user": {
                        "_id": "63db16330cc3bc12bc0b6f8f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63db16330cc3bc12bc0b6f8f/ld0JQIfX1SBlDVDOmw9VT.jpeg",
                        "isPro": false,
                        "fullname": "Wenxiang Jiao",
                        "user": "wxjiao",
                        "type": "user"
                    },
                    "name": "Wenxiang Jiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:27:31.186Z",
                    "hidden": false
                },
                {
                    "_id": "68fed3876cdff8b857f47118",
                    "name": "Jiarui Jin",
                    "hidden": false
                },
                {
                    "_id": "68fed3876cdff8b857f47119",
                    "user": {
                        "_id": "61cd4b833dd34ba1985e0753",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
                        "isPro": false,
                        "fullname": "KABI",
                        "user": "dongguanting",
                        "type": "user"
                    },
                    "name": "Guanting Dong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:27:28.550Z",
                    "hidden": false
                },
                {
                    "_id": "68fed3876cdff8b857f4711a",
                    "name": "Jiajie Jin",
                    "hidden": false
                },
                {
                    "_id": "68fed3876cdff8b857f4711b",
                    "name": "Yinuo Wang",
                    "hidden": false
                },
                {
                    "_id": "68fed3876cdff8b857f4711c",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "68fed3876cdff8b857f4711d",
                    "name": "Yutao Zhu",
                    "hidden": false
                },
                {
                    "_id": "68fed3876cdff8b857f4711e",
                    "name": "Ji-Rong Wen",
                    "hidden": false
                },
                {
                    "_id": "68fed3876cdff8b857f4711f",
                    "name": "Yuan Lu",
                    "hidden": false
                },
                {
                    "_id": "68fed3876cdff8b857f47120",
                    "name": "Zhicheng Dou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-24T16:24:01.000Z",
            "submittedOnDailyAt": "2025-10-27T00:37:53.632Z",
            "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets",
            "submittedOnDailyBy": {
                "_id": "66e03eace17fb5ff054b7686",
                "avatarUrl": "/avatars/2b739ff11e43dd9e701c647a92617f20.svg",
                "isPro": false,
                "fullname": "Xiaoxi Li",
                "user": "lixiaoxi45",
                "type": "user"
            },
            "summary": "Large reasoning models have demonstrated strong problem-solving abilities,\nyet real-world tasks often require external tools and long-horizon\ninteractions. Existing agent frameworks typically follow predefined workflows,\nwhich limit autonomous and global task completion. In this paper, we introduce\nDeepAgent, an end-to-end deep reasoning agent that performs autonomous\nthinking, tool discovery, and action execution within a single, coherent\nreasoning process. To address the challenges of long-horizon interactions,\nparticularly the context length explosion from multiple tool calls and the\naccumulation of interaction history, we introduce an autonomous memory folding\nmechanism that compresses past interactions into structured episodic, working,\nand tool memories, reducing error accumulation while preserving critical\ninformation. To teach general-purpose tool use efficiently and stably, we\ndevelop an end-to-end reinforcement learning strategy, namely ToolPO, that\nleverages LLM-simulated APIs and applies tool-call advantage attribution to\nassign fine-grained credit to the tool invocation tokens. Extensive experiments\non eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,\nTMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,\nHLE), demonstrate that DeepAgent consistently outperforms baselines across both\nlabeled-tool and open-set tool retrieval scenarios. This work takes a step\ntoward more general and capable agents for real-world applications. The code\nand demo are available at https://github.com/RUC-NLPIR/DeepAgent.",
            "upvotes": 63,
            "discussionId": "68fed3876cdff8b857f47121",
            "githubRepo": "https://github.com/RUC-NLPIR/DeepAgent",
            "ai_summary": "DeepAgent, an end-to-end deep reasoning agent, autonomously performs thinking, tool discovery, and action execution using memory folding and reinforcement learning, outperforming baselines in various tool-use and application tasks.",
            "ai_keywords": [
                "DeepAgent",
                "autonomous thinking",
                "tool discovery",
                "action execution",
                "autonomous memory folding",
                "episodic memory",
                "working memory",
                "tool memory",
                "reinforcement learning",
                "ToolPO",
                "LLM-simulated APIs",
                "tool-call advantage attribution",
                "ToolBench",
                "API-Bank",
                "TMDB",
                "Spotify",
                "ToolHop",
                "ALFWorld",
                "WebShop",
                "GAIA",
                "HLE"
            ],
            "githubStars": 117
        },
        "publishedAt": "2025-10-24T12:24:01.000Z",
        "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets",
        "summary": "Large reasoning models have demonstrated strong problem-solving abilities,\nyet real-world tasks often require external tools and long-horizon\ninteractions. Existing agent frameworks typically follow predefined workflows,\nwhich limit autonomous and global task completion. In this paper, we introduce\nDeepAgent, an end-to-end deep reasoning agent that performs autonomous\nthinking, tool discovery, and action execution within a single, coherent\nreasoning process. To address the challenges of long-horizon interactions,\nparticularly the context length explosion from multiple tool calls and the\naccumulation of interaction history, we introduce an autonomous memory folding\nmechanism that compresses past interactions into structured episodic, working,\nand tool memories, reducing error accumulation while preserving critical\ninformation. To teach general-purpose tool use efficiently and stably, we\ndevelop an end-to-end reinforcement learning strategy, namely ToolPO, that\nleverages LLM-simulated APIs and applies tool-call advantage attribution to\nassign fine-grained credit to the tool invocation tokens. Extensive experiments\non eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,\nTMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,\nHLE), demonstrate that DeepAgent consistently outperforms baselines across both\nlabeled-tool and open-set tool retrieval scenarios. This work takes a step\ntoward more general and capable agents for real-world applications. The code\nand demo are available at https://github.com/RUC-NLPIR/DeepAgent.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21618.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "66e03eace17fb5ff054b7686",
            "avatarUrl": "/avatars/2b739ff11e43dd9e701c647a92617f20.svg",
            "fullname": "Xiaoxi Li",
            "name": "lixiaoxi45",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.20888",
            "authors": [
                {
                    "_id": "68feb9916cdff8b857f470b6",
                    "user": {
                        "_id": "650447dd52ca06fef957f05d",
                        "avatarUrl": "/avatars/511c11ac9b3cc7a162bda5e07f6ee0a3.svg",
                        "isPro": true,
                        "fullname": "Yuxuan BIAN",
                        "user": "BianYx",
                        "type": "user"
                    },
                    "name": "Yuxuan Bian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:27:54.652Z",
                    "hidden": false
                },
                {
                    "_id": "68feb9916cdff8b857f470b7",
                    "name": "Xin Chen",
                    "hidden": false
                },
                {
                    "_id": "68feb9916cdff8b857f470b8",
                    "name": "Zenan Li",
                    "hidden": false
                },
                {
                    "_id": "68feb9916cdff8b857f470b9",
                    "name": "Tiancheng Zhi",
                    "hidden": false
                },
                {
                    "_id": "68feb9916cdff8b857f470ba",
                    "name": "Shen Sang",
                    "hidden": false
                },
                {
                    "_id": "68feb9916cdff8b857f470bb",
                    "name": "Linjie Luo",
                    "hidden": false
                },
                {
                    "_id": "68feb9916cdff8b857f470bc",
                    "name": "Qiang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T17:59:52.000Z",
            "submittedOnDailyAt": "2025-10-27T00:31:33.559Z",
            "title": "Video-As-Prompt: Unified Semantic Control for Video Generation",
            "submittedOnDailyBy": {
                "_id": "650447dd52ca06fef957f05d",
                "avatarUrl": "/avatars/511c11ac9b3cc7a162bda5e07f6ee0a3.svg",
                "isPro": true,
                "fullname": "Yuxuan BIAN",
                "user": "BianYx",
                "type": "user"
            },
            "summary": "Unified, generalizable semantic control in video generation remains a\ncritical open challenge. Existing methods either introduce artifacts by\nenforcing inappropriate pixel-wise priors from structure-based controls, or\nrely on non-generalizable, condition-specific finetuning or task-specific\narchitectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes\nthis problem as in-context generation. VAP leverages a reference video as a\ndirect semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via\na plug-and-play Mixture-of-Transformers (MoT) expert. This architecture\nprevents catastrophic forgetting and is guided by a temporally biased position\nembedding that eliminates spurious mapping priors for robust context retrieval.\nTo power this approach and catalyze future research, we built VAP-Data, the\nlargest dataset for semantic-controlled video generation with over 100K paired\nvideos across 100 semantic conditions. As a single unified model, VAP sets a\nnew state-of-the-art for open-source methods, achieving a 38.7% user preference\nrate that rivals leading condition-specific commercial models. VAP's strong\nzero-shot generalization and support for various downstream applications mark a\nsignificant advance toward general-purpose, controllable video generation.",
            "upvotes": 35,
            "discussionId": "68feb9916cdff8b857f470bd",
            "projectPage": "https://bytedance.github.io/Video-As-Prompt/",
            "githubRepo": "https://github.com/bytedance/Video-As-Prompt",
            "ai_summary": "Video-As-Prompt (VAP) uses a reference video to guide a frozen Video Diffusion Transformer via a Mixture-of-Transformers expert, achieving state-of-the-art results in semantic-controlled video generation with strong zero-shot generalization.",
            "ai_keywords": [
                "Video-As-Prompt",
                "Video Diffusion Transformer",
                "DiT",
                "Mixture-of-Transformers",
                "MoT",
                "catastrophic forgetting",
                "temporally biased position embedding",
                "VAP-Data",
                "zero-shot generalization"
            ],
            "githubStars": 112,
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "publishedAt": "2025-10-23T13:59:52.000Z",
        "title": "Video-As-Prompt: Unified Semantic Control for Video Generation",
        "summary": "Unified, generalizable semantic control in video generation remains a\ncritical open challenge. Existing methods either introduce artifacts by\nenforcing inappropriate pixel-wise priors from structure-based controls, or\nrely on non-generalizable, condition-specific finetuning or task-specific\narchitectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes\nthis problem as in-context generation. VAP leverages a reference video as a\ndirect semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via\na plug-and-play Mixture-of-Transformers (MoT) expert. This architecture\nprevents catastrophic forgetting and is guided by a temporally biased position\nembedding that eliminates spurious mapping priors for robust context retrieval.\nTo power this approach and catalyze future research, we built VAP-Data, the\nlargest dataset for semantic-controlled video generation with over 100K paired\nvideos across 100 semantic conditions. As a single unified model, VAP sets a\nnew state-of-the-art for open-source methods, achieving a 38.7% user preference\nrate that rivals leading condition-specific commercial models. VAP's strong\nzero-shot generalization and support for various downstream applications mark a\nsignificant advance toward general-purpose, controllable video generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20888.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "650447dd52ca06fef957f05d",
            "avatarUrl": "/avatars/511c11ac9b3cc7a162bda5e07f6ee0a3.svg",
            "fullname": "Yuxuan BIAN",
            "name": "BianYx",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.21682",
            "authors": [
                {
                    "_id": "68fecebf6cdff8b857f470e5",
                    "name": "Sikuang Li",
                    "hidden": false
                },
                {
                    "_id": "68fecebf6cdff8b857f470e6",
                    "name": "Chen Yang",
                    "hidden": false
                },
                {
                    "_id": "68fecebf6cdff8b857f470e7",
                    "user": {
                        "_id": "652cf49cc6857682d3168f5a",
                        "avatarUrl": "/avatars/9ccffa04832429aad37320a301ea8e36.svg",
                        "isPro": false,
                        "fullname": "Jiemin Fang",
                        "user": "JieminFang",
                        "type": "user"
                    },
                    "name": "Jiemin Fang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:27:40.128Z",
                    "hidden": false
                },
                {
                    "_id": "68fecebf6cdff8b857f470e8",
                    "name": "Taoran Yi",
                    "hidden": false
                },
                {
                    "_id": "68fecebf6cdff8b857f470e9",
                    "name": "Jia Lu",
                    "hidden": false
                },
                {
                    "_id": "68fecebf6cdff8b857f470ea",
                    "name": "Jiazhong Cen",
                    "hidden": false
                },
                {
                    "_id": "68fecebf6cdff8b857f470eb",
                    "name": "Lingxi Xie",
                    "hidden": false
                },
                {
                    "_id": "68fecebf6cdff8b857f470ec",
                    "name": "Wei Shen",
                    "hidden": false
                },
                {
                    "_id": "68fecebf6cdff8b857f470ed",
                    "name": "Qi Tian",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/D2ll1V8_PMy2ibjSCQHuu.mp4"
            ],
            "publishedAt": "2025-10-24T17:39:52.000Z",
            "submittedOnDailyAt": "2025-10-27T00:18:48.656Z",
            "title": "WorldGrow: Generating Infinite 3D World",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We tackle the challenge of generating the infinitely extendable 3D world --\nlarge, continuous environments with coherent geometry and realistic appearance.\nExisting methods face key challenges: 2D-lifting approaches suffer from\ngeometric and appearance inconsistencies across views, 3D implicit\nrepresentations are hard to scale up, and current 3D foundation models are\nmostly object-centric, limiting their applicability to scene-level generation.\nOur key insight is leveraging strong generation priors from pre-trained 3D\nmodels for structured scene block generation. To this end, we propose\nWorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our\nmethod features three core components: (1) a data curation pipeline that\nextracts high-quality scene blocks for training, making the 3D structured\nlatent representations suitable for scene generation; (2) a 3D block inpainting\nmechanism that enables context-aware scene extension; and (3) a coarse-to-fine\ngeneration strategy that ensures both global layout plausibility and local\ngeometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset,\nWorldGrow achieves SOTA performance in geometry reconstruction, while uniquely\nsupporting infinite scene generation with photorealistic and structurally\nconsistent outputs. These results highlight its capability for constructing\nlarge-scale virtual environments and potential for building future world\nmodels.",
            "upvotes": 28,
            "discussionId": "68fecebf6cdff8b857f470ee",
            "ai_summary": "WorldGrow, a hierarchical framework, generates large, continuous 3D environments with coherent geometry and realistic appearance using pre-trained 3D models and a coarse-to-fine generation strategy.",
            "ai_keywords": [
                "3D implicit representations",
                "3D foundation models",
                "scene block generation",
                "data curation pipeline",
                "3D block inpainting",
                "coarse-to-fine generation",
                "geometry reconstruction",
                "3D-FRONT dataset",
                "world models"
            ]
        },
        "publishedAt": "2025-10-24T13:39:52.000Z",
        "title": "WorldGrow: Generating Infinite 3D World",
        "summary": "We tackle the challenge of generating the infinitely extendable 3D world --\nlarge, continuous environments with coherent geometry and realistic appearance.\nExisting methods face key challenges: 2D-lifting approaches suffer from\ngeometric and appearance inconsistencies across views, 3D implicit\nrepresentations are hard to scale up, and current 3D foundation models are\nmostly object-centric, limiting their applicability to scene-level generation.\nOur key insight is leveraging strong generation priors from pre-trained 3D\nmodels for structured scene block generation. To this end, we propose\nWorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our\nmethod features three core components: (1) a data curation pipeline that\nextracts high-quality scene blocks for training, making the 3D structured\nlatent representations suitable for scene generation; (2) a 3D block inpainting\nmechanism that enables context-aware scene extension; and (3) a coarse-to-fine\ngeneration strategy that ensures both global layout plausibility and local\ngeometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset,\nWorldGrow achieves SOTA performance in geometry reconstruction, while uniquely\nsupporting infinite scene generation with photorealistic and structurally\nconsistent outputs. These results highlight its capability for constructing\nlarge-scale virtual environments and potential for building future world\nmodels.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/D2ll1V8_PMy2ibjSCQHuu.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21682.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 147
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.21583",
            "authors": [
                {
                    "_id": "68feef306cdff8b857f471c6",
                    "name": "Yifu Luo",
                    "hidden": false
                },
                {
                    "_id": "68feef306cdff8b857f471c7",
                    "user": {
                        "_id": "647076467fd7ecdbd0ea03b1",
                        "avatarUrl": "/avatars/6e090ea5f88977c6f70544175094c2a6.svg",
                        "isPro": false,
                        "fullname": "Penghui Du",
                        "user": "eternaldolphin",
                        "type": "user"
                    },
                    "name": "Penghui Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:25:20.740Z",
                    "hidden": false
                },
                {
                    "_id": "68feef306cdff8b857f471c8",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "68feef306cdff8b857f471c9",
                    "name": "Sinan Du",
                    "hidden": false
                },
                {
                    "_id": "68feef306cdff8b857f471ca",
                    "name": "Tiantian Zhang",
                    "hidden": false
                },
                {
                    "_id": "68feef306cdff8b857f471cb",
                    "name": "Yongzhe Chang",
                    "hidden": false
                },
                {
                    "_id": "68feef306cdff8b857f471cc",
                    "user": {
                        "_id": "68e741ea3edb0ff47e20084e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68e741ea3edb0ff47e20084e/OyBgFqcU4QWyPF_K58Gt5.jpeg",
                        "isPro": false,
                        "fullname": "Wu Kai",
                        "user": "KaiiWuu1993",
                        "type": "user"
                    },
                    "name": "Kai Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:25:12.953Z",
                    "hidden": false
                },
                {
                    "_id": "68feef306cdff8b857f471cd",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "68feef306cdff8b857f471ce",
                    "name": "Xueqian Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-24T15:50:36.000Z",
            "submittedOnDailyAt": "2025-10-27T04:06:40.964Z",
            "title": "Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image\n  Generation",
            "submittedOnDailyBy": {
                "_id": "6419e0b3ed725fef6444f53a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6419e0b3ed725fef6444f53a/3ZzIafBJa50-gAu1PASuG.png",
                "isPro": false,
                "fullname": "Yu Changqian",
                "user": "Changqian",
                "type": "user"
            },
            "summary": "Group Relative Policy Optimization (GRPO) has shown strong potential for\nflow-matching-based text-to-image (T2I) generation, but it faces two key\nlimitations: inaccurate advantage attribution, and the neglect of temporal\ndynamics of generation. In this work, we argue that shifting the optimization\nparadigm from the step level to the chunk level can effectively alleviate these\nissues. Building on this idea, we propose Chunk-GRPO, the first chunk-level\nGRPO-based approach for T2I generation. The insight is to group consecutive\nsteps into coherent 'chunk's that capture the intrinsic temporal dynamics of\nflow matching, and to optimize policies at the chunk level. In addition, we\nintroduce an optional weighted sampling strategy to further enhance\nperformance. Extensive experiments show that ChunkGRPO achieves superior\nresults in both preference alignment and image quality, highlighting the\npromise of chunk-level optimization for GRPO-based methods.",
            "upvotes": 28,
            "discussionId": "68feef306cdff8b857f471cf",
            "ai_summary": "Chunk-GRPO, a chunk-level optimization approach for text-to-image generation, improves preference alignment and image quality by addressing inaccurate advantage attribution and neglecting temporal dynamics.",
            "ai_keywords": [
                "flow-matching-based",
                "text-to-image",
                "T2I",
                "Group Relative Policy Optimization",
                "GRPO",
                "chunk-level",
                "temporal dynamics",
                "policy optimization",
                "weighted sampling strategy"
            ],
            "organization": {
                "_id": "665f02ce9f9e5b38d0a256a8",
                "name": "Kwai-Kolors",
                "fullname": "Kolors Team, Kuaishou Technology",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f0babaef9cc6810cec02ff/sVnELkcfVo5kxg5308rkr.png"
            }
        },
        "publishedAt": "2025-10-24T11:50:36.000Z",
        "title": "Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image\n  Generation",
        "summary": "Group Relative Policy Optimization (GRPO) has shown strong potential for\nflow-matching-based text-to-image (T2I) generation, but it faces two key\nlimitations: inaccurate advantage attribution, and the neglect of temporal\ndynamics of generation. In this work, we argue that shifting the optimization\nparadigm from the step level to the chunk level can effectively alleviate these\nissues. Building on this idea, we propose Chunk-GRPO, the first chunk-level\nGRPO-based approach for T2I generation. The insight is to group consecutive\nsteps into coherent 'chunk's that capture the intrinsic temporal dynamics of\nflow matching, and to optimize policies at the chunk level. In addition, we\nintroduce an optional weighted sampling strategy to further enhance\nperformance. Extensive experiments show that ChunkGRPO achieves superior\nresults in both preference alignment and image quality, highlighting the\npromise of chunk-level optimization for GRPO-based methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21583.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6419e0b3ed725fef6444f53a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6419e0b3ed725fef6444f53a/3ZzIafBJa50-gAu1PASuG.png",
            "fullname": "Yu Changqian",
            "name": "Changqian",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "665f02ce9f9e5b38d0a256a8",
            "name": "Kwai-Kolors",
            "fullname": "Kolors Team, Kuaishou Technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f0babaef9cc6810cec02ff/sVnELkcfVo5kxg5308rkr.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.19871",
            "authors": [
                {
                    "_id": "68fedbcc6cdff8b857f4716e",
                    "name": "Yatai Ji",
                    "hidden": false
                },
                {
                    "_id": "68fedbcc6cdff8b857f4716f",
                    "user": {
                        "_id": "63a9414e32ed73936ec0a0c8",
                        "avatarUrl": "/avatars/f181e1bb480502c2680be5296a036bdd.svg",
                        "isPro": true,
                        "fullname": "wybertwang",
                        "user": "wybertwang",
                        "type": "user"
                    },
                    "name": "Teng Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:27:18.548Z",
                    "hidden": false
                },
                {
                    "_id": "68fedbcc6cdff8b857f47170",
                    "name": "Yuying Ge",
                    "hidden": false
                },
                {
                    "_id": "68fedbcc6cdff8b857f47171",
                    "name": "Zhiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68fedbcc6cdff8b857f47172",
                    "name": "Sidi Yang",
                    "hidden": false
                },
                {
                    "_id": "68fedbcc6cdff8b857f47173",
                    "name": "Ying Shan",
                    "hidden": false
                },
                {
                    "_id": "68fedbcc6cdff8b857f47174",
                    "name": "Ping Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T06:58:55.000Z",
            "submittedOnDailyAt": "2025-10-27T01:17:48.832Z",
            "title": "From Denoising to Refining: A Corrective Framework for Vision-Language\n  Diffusion Model",
            "submittedOnDailyBy": {
                "_id": "64c8b0a2f3d2a59a431dbb8e",
                "avatarUrl": "/avatars/bf130fdca7a2a0fe394558e2bf22c920.svg",
                "isPro": false,
                "fullname": "yatai ji",
                "user": "jiyatai",
                "type": "user"
            },
            "summary": "Discrete diffusion models have emerged as a promising direction for\nvision-language tasks, offering bidirectional context modeling and theoretical\nparallelization. However, their practical application is severely hindered by a\ntrain-inference discrepancy, which leads to catastrophic error cascades:\ninitial token errors during parallel decoding pollute the generation context,\ntriggering a chain reaction of compounding errors and leading to syntactic\nerrors and semantic hallucinations. To address this fundamental challenge, we\nreframe the generation process from passive denoising to active refining. We\nintroduce ReDiff, a refining-enhanced diffusion framework that teaches the\nmodel to identify and correct its own errors. Our approach features a two-stage\ntraining process: first, we instill a foundational revision capability by\ntraining the model to revise synthetic errors; second, we implement a novel\nonline self-correction loop where the model is explicitly trained to revise its\nown flawed drafts by learning from an expert's corrections. This mistake-driven\nlearning endows the model with the crucial ability to revisit and refine its\nalready generated output, effectively breaking the error cascade. Extensive\nexperiments demonstrate that ReDiff significantly improves the coherence and\nfactual accuracy of generated content, enabling stable and efficient parallel\ngeneration far superior to traditional denoising methods. Our codes and models\nare available at https://rediff-hku.github.io/.",
            "upvotes": 26,
            "discussionId": "68fedbcc6cdff8b857f47175",
            "projectPage": "https://rediff-hku.github.io/",
            "githubRepo": "https://github.com/jiyt17/ReDiff",
            "ai_summary": "ReDiff, a refining-enhanced diffusion framework, addresses train-inference discrepancies in discrete diffusion models by enabling the model to identify and correct its own errors, improving coherence and factual accuracy in generated content.",
            "ai_keywords": [
                "discrete diffusion models",
                "bidirectional context modeling",
                "theoretical parallelization",
                "train-inference discrepancy",
                "catastrophic error cascades",
                "parallel decoding",
                "syntactic errors",
                "semantic hallucinations",
                "ReDiff",
                "refining-enhanced diffusion framework",
                "two-stage training process",
                "synthetic errors",
                "online self-correction loop",
                "mistake-driven learning",
                "stable and efficient parallel generation"
            ],
            "githubStars": 27,
            "organization": {
                "_id": "66deb312fd7d68a29348aa8d",
                "name": "TheHKU",
                "fullname": "Hong Kong University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66dc525add44163a31059cf6/kyqlTADY27mPRTqznqQFL.png"
            }
        },
        "publishedAt": "2025-10-22T02:58:55.000Z",
        "title": "From Denoising to Refining: A Corrective Framework for Vision-Language\n  Diffusion Model",
        "summary": "Discrete diffusion models have emerged as a promising direction for\nvision-language tasks, offering bidirectional context modeling and theoretical\nparallelization. However, their practical application is severely hindered by a\ntrain-inference discrepancy, which leads to catastrophic error cascades:\ninitial token errors during parallel decoding pollute the generation context,\ntriggering a chain reaction of compounding errors and leading to syntactic\nerrors and semantic hallucinations. To address this fundamental challenge, we\nreframe the generation process from passive denoising to active refining. We\nintroduce ReDiff, a refining-enhanced diffusion framework that teaches the\nmodel to identify and correct its own errors. Our approach features a two-stage\ntraining process: first, we instill a foundational revision capability by\ntraining the model to revise synthetic errors; second, we implement a novel\nonline self-correction loop where the model is explicitly trained to revise its\nown flawed drafts by learning from an expert's corrections. This mistake-driven\nlearning endows the model with the crucial ability to revisit and refine its\nalready generated output, effectively breaking the error cascade. Extensive\nexperiments demonstrate that ReDiff significantly improves the coherence and\nfactual accuracy of generated content, enabling stable and efficient parallel\ngeneration far superior to traditional denoising methods. Our codes and models\nare available at https://rediff-hku.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19871.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c8b0a2f3d2a59a431dbb8e",
            "avatarUrl": "/avatars/bf130fdca7a2a0fe394558e2bf22c920.svg",
            "fullname": "yatai ji",
            "name": "jiyatai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "66deb312fd7d68a29348aa8d",
            "name": "TheHKU",
            "fullname": "Hong Kong University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66dc525add44163a31059cf6/kyqlTADY27mPRTqznqQFL.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.14901",
            "authors": [
                {
                    "_id": "68fa7a16f158a71c5a2f56f9",
                    "name": "Aayush Karan",
                    "hidden": false
                },
                {
                    "_id": "68fa7a16f158a71c5a2f56fa",
                    "name": "Yilun Du",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-16T17:18:11.000Z",
            "submittedOnDailyAt": "2025-10-27T03:10:01.698Z",
            "title": "Reasoning with Sampling: Your Base Model is Smarter Than You Think",
            "submittedOnDailyBy": {
                "_id": "64675fd0b990713c50317559",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64675fd0b990713c50317559/qGUiaMyAd4mUjXJe0EXVU.png",
                "isPro": false,
                "fullname": "Zhi Zhou",
                "user": "WNJXYK",
                "type": "user"
            },
            "summary": "Frontier reasoning models have exhibited incredible capabilities across a\nwide array of disciplines, driven by posttraining large language models (LLMs)\nwith reinforcement learning (RL). However, despite the widespread success of\nthis paradigm, much of the literature has been devoted to disentangling truly\nnovel behaviors that emerge during RL but are not present in the base models.\nIn our work, we approach this question from a different angle, instead asking\nwhether comparable reasoning capabilites can be elicited from base models at\ninference time by pure sampling, without any additional training. Inspired by\nMarkov chain Monte Carlo (MCMC) techniques for sampling from sharpened\ndistributions, we propose a simple iterative sampling algorithm leveraging the\nbase models' own likelihoods. Over different base models, we show that our\nalgorithm offers substantial boosts in reasoning that nearly match and even\noutperform those from RL on a wide variety of single-shot tasks, including\nMATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in\ndiversity over multiple samples that is characteristic of RL-posttraining.\nCrucially, our method does not require training, curated datasets, or a\nverifier, suggesting broad applicability beyond easily verifiable domains.",
            "upvotes": 25,
            "discussionId": "68fa7a16f158a71c5a2f56fb",
            "projectPage": "https://aakaran.github.io/reasoning_with_sampling/",
            "githubRepo": "https://github.com/aakaran/reasoning-with-sampling",
            "ai_summary": "An iterative sampling algorithm enhances reasoning capabilities in base models without additional training, matching or outperforming reinforcement learning on single-shot tasks.",
            "ai_keywords": [
                "posttraining large language models",
                "reinforcement learning",
                "Markov chain Monte Carlo",
                "iterative sampling algorithm",
                "likelihoods",
                "MATH500",
                "HumanEval",
                "GPQA",
                "diversity collapse"
            ],
            "githubStars": 223,
            "organization": {
                "_id": "63663038361a96184dbad334",
                "name": "Harvard",
                "fullname": "Harvard University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1667641389305-6366110f575c93cedafde54e.jpeg"
            }
        },
        "publishedAt": "2025-10-16T13:18:11.000Z",
        "title": "Reasoning with Sampling: Your Base Model is Smarter Than You Think",
        "summary": "Frontier reasoning models have exhibited incredible capabilities across a\nwide array of disciplines, driven by posttraining large language models (LLMs)\nwith reinforcement learning (RL). However, despite the widespread success of\nthis paradigm, much of the literature has been devoted to disentangling truly\nnovel behaviors that emerge during RL but are not present in the base models.\nIn our work, we approach this question from a different angle, instead asking\nwhether comparable reasoning capabilites can be elicited from base models at\ninference time by pure sampling, without any additional training. Inspired by\nMarkov chain Monte Carlo (MCMC) techniques for sampling from sharpened\ndistributions, we propose a simple iterative sampling algorithm leveraging the\nbase models' own likelihoods. Over different base models, we show that our\nalgorithm offers substantial boosts in reasoning that nearly match and even\noutperform those from RL on a wide variety of single-shot tasks, including\nMATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in\ndiversity over multiple samples that is characteristic of RL-posttraining.\nCrucially, our method does not require training, curated datasets, or a\nverifier, suggesting broad applicability beyond easily verifiable domains.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14901.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "64675fd0b990713c50317559",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64675fd0b990713c50317559/qGUiaMyAd4mUjXJe0EXVU.png",
            "fullname": "Zhi Zhou",
            "name": "WNJXYK",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "63663038361a96184dbad334",
            "name": "Harvard",
            "fullname": "Harvard University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1667641389305-6366110f575c93cedafde54e.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.18212",
            "authors": [
                {
                    "_id": "68f91c1db9b2e4ae0467366b",
                    "name": "Dan Hendrycks",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae0467366c",
                    "name": "Dawn Song",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae0467366d",
                    "name": "Christian Szegedy",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae0467366e",
                    "name": "Honglak Lee",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae0467366f",
                    "name": "Yarin Gal",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673670",
                    "name": "Erik Brynjolfsson",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673671",
                    "name": "Sharon Li",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673672",
                    "name": "Andy Zou",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673673",
                    "name": "Lionel Levine",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673674",
                    "name": "Bo Han",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673675",
                    "name": "Jie Fu",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673676",
                    "name": "Ziwei Liu",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673677",
                    "name": "Jinwoo Shin",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673678",
                    "name": "Kimin Lee",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673679",
                    "name": "Mantas Mazeika",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae0467367a",
                    "name": "Long Phan",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae0467367b",
                    "name": "George Ingebretsen",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae0467367c",
                    "name": "Adam Khoja",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae0467367d",
                    "name": "Cihang Xie",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae0467367e",
                    "name": "Olawale Salaudeen",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae0467367f",
                    "name": "Matthias Hein",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673680",
                    "name": "Kevin Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673681",
                    "name": "Alexander Pan",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673682",
                    "name": "David Duvenaud",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673683",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673684",
                    "name": "Steve Omohundro",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673685",
                    "name": "Gabriel Alfour",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673686",
                    "name": "Max Tegmark",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673687",
                    "name": "Kevin McGrew",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673688",
                    "name": "Gary Marcus",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae04673689",
                    "name": "Jaan Tallinn",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae0467368a",
                    "name": "Eric Schmidt",
                    "hidden": false
                },
                {
                    "_id": "68f91c1db9b2e4ae0467368b",
                    "name": "Yoshua Bengio",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T01:28:35.000Z",
            "submittedOnDailyAt": "2025-10-27T00:52:54.087Z",
            "title": "A Definition of AGI",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The lack of a concrete definition for Artificial General Intelligence (AGI)\nobscures the gap between today's specialized AI and human-level cognition. This\npaper introduces a quantifiable framework to address this, defining AGI as\nmatching the cognitive versatility and proficiency of a well-educated adult. To\noperationalize this, we ground our methodology in Cattell-Horn-Carroll theory,\nthe most empirically validated model of human cognition. The framework dissects\ngeneral intelligence into ten core cognitive domains-including reasoning,\nmemory, and perception-and adapts established human psychometric batteries to\nevaluate AI systems. Application of this framework reveals a highly \"jagged\"\ncognitive profile in contemporary models. While proficient in\nknowledge-intensive domains, current AI systems have critical deficits in\nfoundational cognitive machinery, particularly long-term memory storage. The\nresulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify\nboth rapid progress and the substantial gap remaining before AGI.",
            "upvotes": 24,
            "discussionId": "68f91c1eb9b2e4ae0467368c",
            "ai_summary": "A quantifiable framework based on Cattell-Horn-Carroll theory evaluates AI systems across ten cognitive domains, revealing significant gaps in foundational cognitive abilities like long-term memory.",
            "ai_keywords": [
                "Cattell-Horn-Carroll theory",
                "cognitive domains",
                "reasoning",
                "memory",
                "perception",
                "psychometric batteries",
                "AGI scores"
            ]
        },
        "publishedAt": "2025-10-20T21:28:35.000Z",
        "title": "A Definition of AGI",
        "summary": "The lack of a concrete definition for Artificial General Intelligence (AGI)\nobscures the gap between today's specialized AI and human-level cognition. This\npaper introduces a quantifiable framework to address this, defining AGI as\nmatching the cognitive versatility and proficiency of a well-educated adult. To\noperationalize this, we ground our methodology in Cattell-Horn-Carroll theory,\nthe most empirically validated model of human cognition. The framework dissects\ngeneral intelligence into ten core cognitive domains-including reasoning,\nmemory, and perception-and adapts established human psychometric batteries to\nevaluate AI systems. Application of this framework reveals a highly \"jagged\"\ncognitive profile in contemporary models. While proficient in\nknowledge-intensive domains, current AI systems have critical deficits in\nfoundational cognitive machinery, particularly long-term memory storage. The\nresulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify\nboth rapid progress and the substantial gap remaining before AGI.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18212.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 147
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.21270",
            "authors": [
                {
                    "_id": "68fee8b16cdff8b857f471b2",
                    "name": "Xinghao Wang",
                    "hidden": false
                },
                {
                    "_id": "68fee8b16cdff8b857f471b3",
                    "name": "Pengyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68fee8b16cdff8b857f471b4",
                    "name": "Dong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68fee8b16cdff8b857f471b5",
                    "name": "Chenkun Tan",
                    "hidden": false
                },
                {
                    "_id": "68fee8b16cdff8b857f471b6",
                    "name": "Shaojun Zhou",
                    "hidden": false
                },
                {
                    "_id": "68fee8b16cdff8b857f471b7",
                    "name": "Zhaoxiang Liu",
                    "hidden": false
                },
                {
                    "_id": "68fee8b16cdff8b857f471b8",
                    "name": "Shiguo Lian",
                    "hidden": false
                },
                {
                    "_id": "68fee8b16cdff8b857f471b9",
                    "name": "Fangxu Liu",
                    "hidden": false
                },
                {
                    "_id": "68fee8b16cdff8b857f471ba",
                    "name": "Kai Song",
                    "hidden": false
                },
                {
                    "_id": "68fee8b16cdff8b857f471bb",
                    "name": "Xipeng Qiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-24T09:11:50.000Z",
            "submittedOnDailyAt": "2025-10-27T02:11:36.414Z",
            "title": "Sparser Block-Sparse Attention via Token Permutation",
            "submittedOnDailyBy": {
                "_id": "6191f22d08a57f265f7f5266",
                "avatarUrl": "/avatars/215164e7b6d025a3c32555ff541cdd62.svg",
                "isPro": false,
                "fullname": "XinghaoWang",
                "user": "Singhoo",
                "type": "user"
            },
            "summary": "Scaling the context length of large language models (LLMs) offers significant\nbenefits but is computationally expensive. This expense stems primarily from\nthe self-attention mechanism, whose O(N^2) complexity with respect to\nsequence length presents a major bottleneck for both memory and latency.\nFortunately, the attention matrix is often sparse, particularly for long\nsequences, suggesting an opportunity for optimization. Block-sparse attention\nhas emerged as a promising solution that partitions sequences into blocks and\nskips computation for a subset of these blocks. However, the effectiveness of\nthis method is highly dependent on the underlying attention patterns, which can\nlead to sub-optimal block-level sparsity. For instance, important key tokens\nfor queries within a single block may be scattered across numerous other\nblocks, leading to computational redundancy. In this work, we propose Permuted\nBlock-Sparse Attention (PBS-Attn), a plug-and-play method that\nleverages the permutation properties of attention to increase block-level\nsparsity and enhance the computational efficiency of LLM prefilling. We conduct\ncomprehensive experiments on challenging real-world long-context datasets,\ndemonstrating that PBS-Attn consistently outperforms existing block-sparse\nattention methods in model accuracy and closely matches the full attention\nbaseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn\nachieves an end-to-end speedup of up to 2.75times in long-context\nprefilling, confirming its practical viability. Code available at\nhttps://github.com/xinghaow99/pbs-attn",
            "upvotes": 21,
            "discussionId": "68fee8b16cdff8b857f471bc",
            "githubRepo": "https://github.com/xinghaow99/pbs-attn",
            "ai_summary": "Permuted Block-Sparse Attention improves computational efficiency in large language models by enhancing block-level sparsity in the self-attention mechanism, achieving significant speedups without compromising accuracy.",
            "ai_keywords": [
                "self-attention mechanism",
                "block-sparse attention",
                "Permuted Block-Sparse Attention",
                "PBS-Attn",
                "permuted-FlashAttention kernels"
            ],
            "githubStars": 17,
            "organization": {
                "_id": "643cb0625fcffe09fb6ca688",
                "name": "Fudan-University",
                "fullname": "Fudan University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"
            }
        },
        "publishedAt": "2025-10-24T05:11:50.000Z",
        "title": "Sparser Block-Sparse Attention via Token Permutation",
        "summary": "Scaling the context length of large language models (LLMs) offers significant\nbenefits but is computationally expensive. This expense stems primarily from\nthe self-attention mechanism, whose O(N^2) complexity with respect to\nsequence length presents a major bottleneck for both memory and latency.\nFortunately, the attention matrix is often sparse, particularly for long\nsequences, suggesting an opportunity for optimization. Block-sparse attention\nhas emerged as a promising solution that partitions sequences into blocks and\nskips computation for a subset of these blocks. However, the effectiveness of\nthis method is highly dependent on the underlying attention patterns, which can\nlead to sub-optimal block-level sparsity. For instance, important key tokens\nfor queries within a single block may be scattered across numerous other\nblocks, leading to computational redundancy. In this work, we propose Permuted\nBlock-Sparse Attention (PBS-Attn), a plug-and-play method that\nleverages the permutation properties of attention to increase block-level\nsparsity and enhance the computational efficiency of LLM prefilling. We conduct\ncomprehensive experiments on challenging real-world long-context datasets,\ndemonstrating that PBS-Attn consistently outperforms existing block-sparse\nattention methods in model accuracy and closely matches the full attention\nbaseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn\nachieves an end-to-end speedup of up to 2.75times in long-context\nprefilling, confirming its practical viability. Code available at\nhttps://github.com/xinghaow99/pbs-attn",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21270.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6191f22d08a57f265f7f5266",
            "avatarUrl": "/avatars/215164e7b6d025a3c32555ff541cdd62.svg",
            "fullname": "XinghaoWang",
            "name": "Singhoo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "643cb0625fcffe09fb6ca688",
            "name": "Fudan-University",
            "fullname": "Fudan University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.20286",
            "authors": [
                {
                    "_id": "68fb1853f158a71c5a2f594c",
                    "user": {
                        "_id": "68fb2e8097199a90be333fe6",
                        "avatarUrl": "/avatars/b798b45e53d23bc6d291b6030653c8b6.svg",
                        "isPro": false,
                        "fullname": "Liangyu Chen",
                        "user": "CleyChen",
                        "type": "user"
                    },
                    "name": "Liangyu Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:29:20.509Z",
                    "hidden": false
                },
                {
                    "_id": "68fb1853f158a71c5a2f594d",
                    "name": "Hanzhang Zhou",
                    "hidden": false
                },
                {
                    "_id": "68fb1853f158a71c5a2f594e",
                    "name": "Chenglin Cai",
                    "hidden": false
                },
                {
                    "_id": "68fb1853f158a71c5a2f594f",
                    "user": {
                        "_id": "6825ddbfcbbc52f7ad9aa4d1",
                        "avatarUrl": "/avatars/53def7ee23686a75a976a16d7533856f.svg",
                        "isPro": false,
                        "fullname": "zhang",
                        "user": "Jiananhello",
                        "type": "user"
                    },
                    "name": "Jianan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:00:57.725Z",
                    "hidden": false
                },
                {
                    "_id": "68fb1853f158a71c5a2f5950",
                    "name": "Panrong Tong",
                    "hidden": false
                },
                {
                    "_id": "68fb1853f158a71c5a2f5951",
                    "user": {
                        "_id": "638c05b7d274cbbad284ced0",
                        "avatarUrl": "/avatars/e26f659316b4975562e006c081a795ba.svg",
                        "isPro": false,
                        "fullname": "Quyu Kong",
                        "user": "kongquyu",
                        "type": "user"
                    },
                    "name": "Quyu Kong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T16:00:53.633Z",
                    "hidden": false
                },
                {
                    "_id": "68fb1853f158a71c5a2f5952",
                    "name": "Xu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68fb1853f158a71c5a2f5953",
                    "name": "Chen Liu",
                    "hidden": false
                },
                {
                    "_id": "68fb1853f158a71c5a2f5954",
                    "user": {
                        "_id": "669cefd6119595d21b55a995",
                        "avatarUrl": "/avatars/bafc2387ee70b263bf45c42159381da8.svg",
                        "isPro": false,
                        "fullname": "Yuqi Liu",
                        "user": "Ricky06662",
                        "type": "user"
                    },
                    "name": "Yuqi Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:29:22.553Z",
                    "hidden": false
                },
                {
                    "_id": "68fb1853f158a71c5a2f5955",
                    "name": "Wenxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68fb1853f158a71c5a2f5956",
                    "name": "Yue Wang",
                    "hidden": false
                },
                {
                    "_id": "68fb1853f158a71c5a2f5957",
                    "name": "Qin Jin",
                    "hidden": false
                },
                {
                    "_id": "68fb1853f158a71c5a2f5958",
                    "name": "Steven Hoi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T07:18:32.000Z",
            "submittedOnDailyAt": "2025-10-27T00:35:43.317Z",
            "title": "UI-Ins: Enhancing GUI Grounding with Multi-Perspective\n  Instruction-as-Reasoning",
            "submittedOnDailyBy": {
                "_id": "669cefd6119595d21b55a995",
                "avatarUrl": "/avatars/bafc2387ee70b263bf45c42159381da8.svg",
                "isPro": false,
                "fullname": "Yuqi Liu",
                "user": "Ricky06662",
                "type": "user"
            },
            "summary": "GUI grounding, which maps natural-language instructions to actionable UI\nelements, is a core capability of GUI agents. Prior works largely treats\ninstructions as a static proxy for user intent, overlooking the impact of\ninstruction diversity and quality on grounding performance. Through a careful\ninvestigation of existing grounding datasets, we find a 23.3% flaw rate in\ntheir instructions and show that inference-time exploitation of instruction\ndiversity yields up to a substantial 76% relative performance improvement. In\nthis paper, we introduce the Instruction-as-Reasoning paradigm, treating\ninstructions as dynamic analytical pathways that offer distinct perspectives\nand enabling the model to select the most effective pathway during reasoning.\nTo achieve this, we propose a two-stage training framework: supervised\nfine-tuning (SFT) on synthesized, diverse instructions to instill\nmulti-perspective reasoning, followed by reinforcement learning (RL) to\noptimize pathway selection and composition. Our resulting models, UI-Ins-7B and\nUI-Ins-32B, achieve state-of-the-art results on five challenging grounding\nbenchmarks and exhibit emergent reasoning, selectively composing and\nsynthesizing novel instruction pathways at inference. In particular, UI-Ins-32B\nattains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on\nScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model\ndemonstrates strong agentic potential, achieving a 74.1% success rate on\nAndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals\nadditional insights such as how reasoning can be formulated to enhance rather\nthan hinder grounding performance, and how our method mitigates policy collapse\nin the SFT+RL framework. All code and model checkpoints will be publicly\nreleased in https://github.com/alibaba/UI-Ins.",
            "upvotes": 17,
            "discussionId": "68fb1853f158a71c5a2f5959",
            "ai_summary": "The Instruction-as-Reasoning paradigm enhances GUI grounding by treating instructions as dynamic pathways, improving performance through multi-perspective reasoning and reinforcement learning.",
            "ai_keywords": [
                "GUI grounding",
                "Instruction-as-Reasoning",
                "supervised fine-tuning",
                "reinforcement learning",
                "multi-perspective reasoning",
                "pathway selection",
                "policy collapse"
            ],
            "organization": {
                "_id": "67d15cca6e2cf0e062dbfb54",
                "name": "AlibabaTongyiLab",
                "fullname": "TongyiLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
            }
        },
        "publishedAt": "2025-10-23T03:18:32.000Z",
        "title": "UI-Ins: Enhancing GUI Grounding with Multi-Perspective\n  Instruction-as-Reasoning",
        "summary": "GUI grounding, which maps natural-language instructions to actionable UI\nelements, is a core capability of GUI agents. Prior works largely treats\ninstructions as a static proxy for user intent, overlooking the impact of\ninstruction diversity and quality on grounding performance. Through a careful\ninvestigation of existing grounding datasets, we find a 23.3% flaw rate in\ntheir instructions and show that inference-time exploitation of instruction\ndiversity yields up to a substantial 76% relative performance improvement. In\nthis paper, we introduce the Instruction-as-Reasoning paradigm, treating\ninstructions as dynamic analytical pathways that offer distinct perspectives\nand enabling the model to select the most effective pathway during reasoning.\nTo achieve this, we propose a two-stage training framework: supervised\nfine-tuning (SFT) on synthesized, diverse instructions to instill\nmulti-perspective reasoning, followed by reinforcement learning (RL) to\noptimize pathway selection and composition. Our resulting models, UI-Ins-7B and\nUI-Ins-32B, achieve state-of-the-art results on five challenging grounding\nbenchmarks and exhibit emergent reasoning, selectively composing and\nsynthesizing novel instruction pathways at inference. In particular, UI-Ins-32B\nattains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on\nScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model\ndemonstrates strong agentic potential, achieving a 74.1% success rate on\nAndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals\nadditional insights such as how reasoning can be formulated to enhance rather\nthan hinder grounding performance, and how our method mitigates policy collapse\nin the SFT+RL framework. All code and model checkpoints will be publicly\nreleased in https://github.com/alibaba/UI-Ins.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20286.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "669cefd6119595d21b55a995",
            "avatarUrl": "/avatars/bafc2387ee70b263bf45c42159381da8.svg",
            "fullname": "Yuqi Liu",
            "name": "Ricky06662",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "67d15cca6e2cf0e062dbfb54",
            "name": "AlibabaTongyiLab",
            "fullname": "TongyiLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.21697",
            "authors": [
                {
                    "_id": "68fed3166cdff8b857f4710e",
                    "name": "Nir Goren",
                    "hidden": false
                },
                {
                    "_id": "68fed3166cdff8b857f4710f",
                    "user": {
                        "_id": "6619099997f7e11bd9c7f484",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4KmSN9a_R4sOdFfsElb_0.png",
                        "isPro": false,
                        "fullname": "Shai Yehezkel",
                        "user": "Kariander1",
                        "type": "user"
                    },
                    "name": "Shai Yehezkel",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T14:16:42.304Z",
                    "hidden": false
                },
                {
                    "_id": "68fed3166cdff8b857f47110",
                    "name": "Omer Dahary",
                    "hidden": false
                },
                {
                    "_id": "68fed3166cdff8b857f47111",
                    "name": "Andrey Voynov",
                    "hidden": false
                },
                {
                    "_id": "68fed3166cdff8b857f47112",
                    "name": "Or Patashnik",
                    "hidden": false
                },
                {
                    "_id": "68fed3166cdff8b857f47113",
                    "name": "Daniel Cohen-Or",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-24T17:57:31.000Z",
            "submittedOnDailyAt": "2025-10-27T00:34:20.112Z",
            "title": "Visual Diffusion Models are Geometric Solvers",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "In this paper we show that visual diffusion models can serve as effective\ngeometric solvers: they can directly reason about geometric problems by working\nin pixel space. We first demonstrate this on the Inscribed Square Problem, a\nlong-standing problem in geometry that asks whether every Jordan curve contains\nfour points forming a square. We then extend the approach to two other\nwell-known hard geometric problems: the Steiner Tree Problem and the Simple\nPolygon Problem.\n  Our method treats each problem instance as an image and trains a standard\nvisual diffusion model that transforms Gaussian noise into an image\nrepresenting a valid approximate solution that closely matches the exact one.\nThe model learns to transform noisy geometric structures into correct\nconfigurations, effectively recasting geometric reasoning as image generation.\n  Unlike prior work that necessitates specialized architectures and\ndomain-specific adaptations when applying diffusion to parametric geometric\nrepresentations, we employ a standard visual diffusion model that operates on\nthe visual representation of the problem. This simplicity highlights a\nsurprising bridge between generative modeling and geometric problem solving.\nBeyond the specific problems studied here, our results point toward a broader\nparadigm: operating in image space provides a general and practical framework\nfor approximating notoriously hard problems, and opens the door to tackling a\nfar wider class of challenging geometric tasks.",
            "upvotes": 15,
            "discussionId": "68fed3166cdff8b857f47114",
            "projectPage": "https://kariander1.github.io/visual-geo-solver/",
            "githubRepo": "https://github.com/kariander1/visual-geo-solver",
            "ai_summary": "Visual diffusion models can solve geometric problems by transforming noisy images into valid solutions, demonstrating a novel approach to geometric reasoning through image generation.",
            "ai_keywords": [
                "visual diffusion models",
                "Inscribed Square Problem",
                "Steiner Tree Problem",
                "Simple Polygon Problem",
                "Gaussian noise",
                "image representation",
                "geometric reasoning",
                "image generation",
                "parametric geometric representations"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-10-24T13:57:31.000Z",
        "title": "Visual Diffusion Models are Geometric Solvers",
        "summary": "In this paper we show that visual diffusion models can serve as effective\ngeometric solvers: they can directly reason about geometric problems by working\nin pixel space. We first demonstrate this on the Inscribed Square Problem, a\nlong-standing problem in geometry that asks whether every Jordan curve contains\nfour points forming a square. We then extend the approach to two other\nwell-known hard geometric problems: the Steiner Tree Problem and the Simple\nPolygon Problem.\n  Our method treats each problem instance as an image and trains a standard\nvisual diffusion model that transforms Gaussian noise into an image\nrepresenting a valid approximate solution that closely matches the exact one.\nThe model learns to transform noisy geometric structures into correct\nconfigurations, effectively recasting geometric reasoning as image generation.\n  Unlike prior work that necessitates specialized architectures and\ndomain-specific adaptations when applying diffusion to parametric geometric\nrepresentations, we employ a standard visual diffusion model that operates on\nthe visual representation of the problem. This simplicity highlights a\nsurprising bridge between generative modeling and geometric problem solving.\nBeyond the specific problems studied here, our results point toward a broader\nparadigm: operating in image space provides a general and practical framework\nfor approximating notoriously hard problems, and opens the door to tackling a\nfar wider class of challenging geometric tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21697.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 147
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.20479",
            "authors": [
                {
                    "_id": "68fb97d46cdff8b857f46ba2",
                    "user": {
                        "_id": "66e6e89b3fbe08f0a3a07066",
                        "avatarUrl": "/avatars/c76cffd761422d3cf999d94b48423f9d.svg",
                        "isPro": false,
                        "fullname": "Makise Kurisu",
                        "user": "Makise-Kurisu",
                        "type": "user"
                    },
                    "name": "Bowen Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:28:59.550Z",
                    "hidden": false
                },
                {
                    "_id": "68fb97d46cdff8b857f46ba3",
                    "name": "Haiyuan Wan",
                    "hidden": false
                },
                {
                    "_id": "68fb97d46cdff8b857f46ba4",
                    "name": "Liwen Shi",
                    "hidden": false
                },
                {
                    "_id": "68fb97d46cdff8b857f46ba5",
                    "name": "Chen Yang",
                    "hidden": false
                },
                {
                    "_id": "68fb97d46cdff8b857f46ba6",
                    "name": "Peng He",
                    "hidden": false
                },
                {
                    "_id": "68fb97d46cdff8b857f46ba7",
                    "name": "Yue Ma",
                    "hidden": false
                },
                {
                    "_id": "68fb97d46cdff8b857f46ba8",
                    "name": "Haochen Han",
                    "hidden": false
                },
                {
                    "_id": "68fb97d46cdff8b857f46ba9",
                    "name": "Wenhao Li",
                    "hidden": false
                },
                {
                    "_id": "68fb97d46cdff8b857f46baa",
                    "name": "Tiao Tan",
                    "hidden": false
                },
                {
                    "_id": "68fb97d46cdff8b857f46bab",
                    "name": "Yongjian Li",
                    "hidden": false
                },
                {
                    "_id": "68fb97d46cdff8b857f46bac",
                    "name": "Fangming Liu",
                    "hidden": false
                },
                {
                    "_id": "68fb97d46cdff8b857f46bad",
                    "name": "Yifan Gong",
                    "hidden": false
                },
                {
                    "_id": "68fb97d46cdff8b857f46bae",
                    "name": "Sheng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T12:17:37.000Z",
            "submittedOnDailyAt": "2025-10-27T01:33:51.463Z",
            "title": "RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via\n  Hierarchical Model Merging",
            "submittedOnDailyBy": {
                "_id": "65a8bcb717d869bb7487c2a1",
                "avatarUrl": "/avatars/261c28f7e616a8482970f50c1f8919fd.svg",
                "isPro": false,
                "fullname": "Alex Haiyuan Wan",
                "user": "haiyuanwan",
                "type": "user"
            },
            "summary": "We unveil that internal representations in large language models (LLMs) serve\nas reliable proxies of learned knowledge, and propose RECALL, a novel\nrepresentation-aware model merging framework for continual learning without\naccess to historical data. RECALL computes inter-model similarity from\nlayer-wise hidden representations over clustered typical samples, and performs\nadaptive, hierarchical parameter fusion to align knowledge across models. This\ndesign enables the preservation of domain-general features in shallow layers\nwhile allowing task-specific adaptation in deeper layers. Unlike prior methods\nthat require task labels or incur performance trade-offs, RECALL achieves\nseamless multi-domain integration and strong resistance to catastrophic\nforgetting. Extensive experiments across five NLP tasks and multiple continual\nlearning scenarios show that RECALL outperforms baselines in both knowledge\nretention and generalization, providing a scalable and data-free solution for\nevolving LLMs.",
            "upvotes": 11,
            "discussionId": "68fb97d46cdff8b857f46baf",
            "ai_summary": "RECALL is a representation-aware framework for continual learning in large language models that merges models without historical data, preserving domain-general features and adapting to task-specific knowledge.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "RECALL",
                "representation-aware",
                "continual learning",
                "inter-model similarity",
                "layer-wise hidden representations",
                "adaptive",
                "hierarchical parameter fusion",
                "domain-general features",
                "task-specific adaptation",
                "catastrophic forgetting",
                "NLP tasks"
            ],
            "organization": {
                "_id": "628735cbc83a2d6ab8d14a66",
                "name": "Tsinghua",
                "fullname": "Tsinghua University"
            }
        },
        "publishedAt": "2025-10-23T08:17:37.000Z",
        "title": "RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via\n  Hierarchical Model Merging",
        "summary": "We unveil that internal representations in large language models (LLMs) serve\nas reliable proxies of learned knowledge, and propose RECALL, a novel\nrepresentation-aware model merging framework for continual learning without\naccess to historical data. RECALL computes inter-model similarity from\nlayer-wise hidden representations over clustered typical samples, and performs\nadaptive, hierarchical parameter fusion to align knowledge across models. This\ndesign enables the preservation of domain-general features in shallow layers\nwhile allowing task-specific adaptation in deeper layers. Unlike prior methods\nthat require task labels or incur performance trade-offs, RECALL achieves\nseamless multi-domain integration and strong resistance to catastrophic\nforgetting. Extensive experiments across five NLP tasks and multiple continual\nlearning scenarios show that RECALL outperforms baselines in both knowledge\nretention and generalization, providing a scalable and data-free solution for\nevolving LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20479.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65a8bcb717d869bb7487c2a1",
            "avatarUrl": "/avatars/261c28f7e616a8482970f50c1f8919fd.svg",
            "fullname": "Alex Haiyuan Wan",
            "name": "haiyuanwan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "628735cbc83a2d6ab8d14a66",
            "name": "Tsinghua",
            "fullname": "Tsinghua University"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.20206",
            "authors": [
                {
                    "_id": "68fee03d6cdff8b857f47181",
                    "user": {
                        "_id": "645e0282412d9e9cd044c764",
                        "avatarUrl": "/avatars/1eebe4e89e151611ea96c38483549bd0.svg",
                        "isPro": false,
                        "fullname": "gao",
                        "user": "bingjie",
                        "type": "user"
                    },
                    "name": "Bingjie Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:27:06.672Z",
                    "hidden": false
                },
                {
                    "_id": "68fee03d6cdff8b857f47182",
                    "user": {
                        "_id": "6448b2f53e7b3c11be684348",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg",
                        "isPro": true,
                        "fullname": "Qianli Ma",
                        "user": "Mqleet",
                        "type": "user"
                    },
                    "name": "Qianli Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:27:15.362Z",
                    "hidden": false
                },
                {
                    "_id": "68fee03d6cdff8b857f47183",
                    "name": "Xiaoxue Wu",
                    "hidden": false
                },
                {
                    "_id": "68fee03d6cdff8b857f47184",
                    "name": "Shuai Yang",
                    "hidden": false
                },
                {
                    "_id": "68fee03d6cdff8b857f47185",
                    "user": {
                        "_id": "66e29caa3be36ad40059c6b8",
                        "avatarUrl": "/avatars/aefb15339c82b45d4565abf4129939e7.svg",
                        "isPro": false,
                        "fullname": "Guanzhou Lan",
                        "user": "Guanzhou111",
                        "type": "user"
                    },
                    "name": "Guanzhou Lan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:27:03.687Z",
                    "hidden": false
                },
                {
                    "_id": "68fee03d6cdff8b857f47186",
                    "name": "Haonan Zhao",
                    "hidden": false
                },
                {
                    "_id": "68fee03d6cdff8b857f47187",
                    "name": "Jiaxuan Chen",
                    "hidden": false
                },
                {
                    "_id": "68fee03d6cdff8b857f47188",
                    "name": "Qingyang Liu",
                    "hidden": false
                },
                {
                    "_id": "68fee03d6cdff8b857f47189",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68fee03d6cdff8b857f4718a",
                    "name": "Xinyuan Chen",
                    "hidden": false
                },
                {
                    "_id": "68fee03d6cdff8b857f4718b",
                    "name": "Yaohui Wang",
                    "hidden": false
                },
                {
                    "_id": "68fee03d6cdff8b857f4718c",
                    "name": "Li Niu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T04:45:09.000Z",
            "submittedOnDailyAt": "2025-10-27T01:46:25.602Z",
            "title": "RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via\n  Data Alignment and Test-Time Scaling",
            "submittedOnDailyBy": {
                "_id": "6448b2f53e7b3c11be684348",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg",
                "isPro": true,
                "fullname": "Qianli Ma",
                "user": "Mqleet",
                "type": "user"
            },
            "summary": "Prompt design plays a crucial role in text-to-video (T2V) generation, yet\nuser-provided prompts are often short, unstructured, and misaligned with\ntraining data, limiting the generative potential of diffusion-based T2V models.\nWe present RAPO++, a cross-stage prompt optimization framework that\nunifies training-data--aligned refinement, test-time iterative scaling, and\nlarge language model (LLM) fine-tuning to substantially improve T2V generation\nwithout modifying the underlying generative backbone. In Stage 1,\nRetrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with\nsemantically relevant modifiers retrieved from a relation graph and refactors\nthem to match training distributions, enhancing compositionality and\nmulti-object fidelity. Stage 2 introduces Sample-Specific Prompt\nOptimization (SSPO), a closed-loop mechanism that iteratively refines prompts\nusing multi-source feedback -- including semantic alignment, spatial fidelity,\ntemporal coherence, and task-specific signals such as optical flow -- yielding\nprogressively improved video generation quality. Stage 3 leverages\noptimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing\ntask-specific optimization patterns and enabling efficient, high-quality prompt\ngeneration even before inference. Extensive experiments across five\nstate-of-the-art T2V models and five benchmarks demonstrate that RAPO++\nachieves significant gains in semantic alignment, compositional reasoning,\ntemporal stability, and physical plausibility, outperforming existing methods\nby large margins. Our results highlight RAPO++ as a model-agnostic,\ncost-efficient, and scalable solution that sets a new standard for prompt\noptimization in T2V generation. The code is available at\nhttps://github.com/Vchitect/RAPO.",
            "upvotes": 11,
            "discussionId": "68fee03d6cdff8b857f4718d",
            "projectPage": "https://whynothaha.github.io/RAPO_plus_github/",
            "githubRepo": "https://github.com/Vchitect/RAPO",
            "ai_summary": "RAPO++ enhances text-to-video generation by optimizing user prompts through retrieval, iterative refinement, and LLM fine-tuning, improving semantic alignment, compositionality, and temporal coherence.",
            "ai_keywords": [
                "diffusion-based T2V models",
                "cross-stage prompt optimization",
                "Retrieval-Augmented Prompt Optimization (RAPO)",
                "Sample-Specific Prompt Optimization (SSPO)",
                "large language model (LLM) fine-tuning",
                "relation graph",
                "semantic alignment",
                "spatial fidelity",
                "temporal coherence",
                "optical flow",
                "compositional reasoning",
                "temporal stability",
                "physical plausibility"
            ],
            "githubStars": 104,
            "organization": {
                "_id": "63e5ef7bf2e9a8f22c515654",
                "name": "SJTU",
                "fullname": "Shanghai Jiao Tong University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
            }
        },
        "publishedAt": "2025-10-23T00:45:09.000Z",
        "title": "RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via\n  Data Alignment and Test-Time Scaling",
        "summary": "Prompt design plays a crucial role in text-to-video (T2V) generation, yet\nuser-provided prompts are often short, unstructured, and misaligned with\ntraining data, limiting the generative potential of diffusion-based T2V models.\nWe present RAPO++, a cross-stage prompt optimization framework that\nunifies training-data--aligned refinement, test-time iterative scaling, and\nlarge language model (LLM) fine-tuning to substantially improve T2V generation\nwithout modifying the underlying generative backbone. In Stage 1,\nRetrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with\nsemantically relevant modifiers retrieved from a relation graph and refactors\nthem to match training distributions, enhancing compositionality and\nmulti-object fidelity. Stage 2 introduces Sample-Specific Prompt\nOptimization (SSPO), a closed-loop mechanism that iteratively refines prompts\nusing multi-source feedback -- including semantic alignment, spatial fidelity,\ntemporal coherence, and task-specific signals such as optical flow -- yielding\nprogressively improved video generation quality. Stage 3 leverages\noptimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing\ntask-specific optimization patterns and enabling efficient, high-quality prompt\ngeneration even before inference. Extensive experiments across five\nstate-of-the-art T2V models and five benchmarks demonstrate that RAPO++\nachieves significant gains in semantic alignment, compositional reasoning,\ntemporal stability, and physical plausibility, outperforming existing methods\nby large margins. Our results highlight RAPO++ as a model-agnostic,\ncost-efficient, and scalable solution that sets a new standard for prompt\noptimization in T2V generation. The code is available at\nhttps://github.com/Vchitect/RAPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20206.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6448b2f53e7b3c11be684348",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg",
            "fullname": "Qianli Ma",
            "name": "Mqleet",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "63e5ef7bf2e9a8f22c515654",
            "name": "SJTU",
            "fullname": "Shanghai Jiao Tong University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.21223",
            "authors": [
                {
                    "_id": "68fed3e16cdff8b857f47123",
                    "user": {
                        "_id": "67cff3de91473f9c5ccf0fa8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67cff3de91473f9c5ccf0fa8/lyHDTF4foLuOHBPmKc2NH.jpeg",
                        "isPro": false,
                        "fullname": "Kexuan Shi",
                        "user": "KexuanShi",
                        "type": "user"
                    },
                    "name": "Kexuan Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:27:25.298Z",
                    "hidden": false
                },
                {
                    "_id": "68fed3e16cdff8b857f47124",
                    "name": "Yandong Wen",
                    "hidden": false
                },
                {
                    "_id": "68fed3e16cdff8b857f47125",
                    "name": "Weiyang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-24T07:54:06.000Z",
            "submittedOnDailyAt": "2025-10-27T00:54:49.563Z",
            "title": "Model Merging with Functional Dual Anchors",
            "submittedOnDailyBy": {
                "_id": "67cff3de91473f9c5ccf0fa8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67cff3de91473f9c5ccf0fa8/lyHDTF4foLuOHBPmKc2NH.jpeg",
                "isPro": false,
                "fullname": "Kexuan Shi",
                "user": "KexuanShi",
                "type": "user"
            },
            "summary": "Model merging is an efficient post-training strategy for integrating\nknowledge from multiple finetuned checkpoints of a shared foundation model.\nExisting methods operate in the parameter space, combining task vectors to\nmitigate conflicts, but remain constrained by parameter inconsistencies. We\npropose Functional Dual Anchors (FDAs), a framework that instead models the\ninput-representation space. FDAs are synthetic inputs whose induced gradients\nalign with task vectors, capturing task-specific functional shifts relative to\nthe pretrained model. This perspective bridges joint multi-task training and\npost-hoc merging, offering both robustness and flexibility. We further\nintroduce a principled initialization scheme and show that FDAs are\ncomplementary to parameter-space model merging. Comprehensive experiments\ndemonstrate the effectiveness of FDAs in model merging.",
            "upvotes": 10,
            "discussionId": "68fed3e16cdff8b857f47126",
            "projectPage": "https://spherelab.ai/fda/",
            "githubRepo": "https://github.com/Sphere-AI-Lab/fda/tree/main",
            "ai_summary": "Functional Dual Anchors (FDAs) enhance model merging by aligning gradients with task vectors in the input-representation space, offering robustness and flexibility compared to parameter-space methods.",
            "ai_keywords": [
                "Functional Dual Anchors",
                "FDAs",
                "input-representation space",
                "task vectors",
                "model merging",
                "parameter-space",
                "joint multi-task training",
                "post-hoc merging"
            ],
            "githubStars": 16,
            "organization": {
                "_id": "6814c6bec8d4f013b0851a7f",
                "name": "SphereLab",
                "fullname": "CUHK SphereLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/648905d1a15c43c791d4381f/3vQFn8UJLYbGZ04CADQsJ.png"
            }
        },
        "publishedAt": "2025-10-24T03:54:06.000Z",
        "title": "Model Merging with Functional Dual Anchors",
        "summary": "Model merging is an efficient post-training strategy for integrating\nknowledge from multiple finetuned checkpoints of a shared foundation model.\nExisting methods operate in the parameter space, combining task vectors to\nmitigate conflicts, but remain constrained by parameter inconsistencies. We\npropose Functional Dual Anchors (FDAs), a framework that instead models the\ninput-representation space. FDAs are synthetic inputs whose induced gradients\nalign with task vectors, capturing task-specific functional shifts relative to\nthe pretrained model. This perspective bridges joint multi-task training and\npost-hoc merging, offering both robustness and flexibility. We further\nintroduce a principled initialization scheme and show that FDAs are\ncomplementary to parameter-space model merging. Comprehensive experiments\ndemonstrate the effectiveness of FDAs in model merging.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21223.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67cff3de91473f9c5ccf0fa8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67cff3de91473f9c5ccf0fa8/lyHDTF4foLuOHBPmKc2NH.jpeg",
            "fullname": "Kexuan Shi",
            "name": "KexuanShi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "6814c6bec8d4f013b0851a7f",
            "name": "SphereLab",
            "fullname": "CUHK SphereLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/648905d1a15c43c791d4381f/3vQFn8UJLYbGZ04CADQsJ.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.13251",
            "authors": [
                {
                    "_id": "68f1a9556e0bef323a68fd7e",
                    "user": {
                        "_id": "66e345c9596fcff3e4b22e5a",
                        "avatarUrl": "/avatars/9137b89d03df15601f2288286b71a56e.svg",
                        "isPro": false,
                        "fullname": "Minji Kim",
                        "user": "byminji",
                        "type": "user"
                    },
                    "name": "Minji Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T15:02:00.655Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a9556e0bef323a68fd7f",
                    "user": {
                        "_id": "67c6a1e75e2443d7d5f85cb3",
                        "avatarUrl": "/avatars/0569b368520411ab828d46725bc3896a.svg",
                        "isPro": false,
                        "fullname": "Taekyung Kim",
                        "user": "taekyung-k",
                        "type": "user"
                    },
                    "name": "Taekyung Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-17T04:10:22.216Z",
                    "hidden": false
                },
                {
                    "_id": "68f1a9556e0bef323a68fd80",
                    "name": "Bohyung Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-15T07:59:06.000Z",
            "submittedOnDailyAt": "2025-10-27T02:28:01.834Z",
            "title": "Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs",
            "submittedOnDailyBy": {
                "_id": "67c6a1e75e2443d7d5f85cb3",
                "avatarUrl": "/avatars/0569b368520411ab828d46725bc3896a.svg",
                "isPro": false,
                "fullname": "Taekyung Kim",
                "user": "taekyung-k",
                "type": "user"
            },
            "summary": "Video Large Language Models (VideoLLMs) extend the capabilities of\nvision-language models to spatiotemporal inputs, enabling tasks such as video\nquestion answering (VideoQA). Despite recent advances in VideoLLMs, their\ninternal mechanisms on where and how they extract and propagate video and\ntextual information remain less explored. In this study, we investigate the\ninternal information flow of VideoLLMs using mechanistic interpretability\ntechniques. Our analysis reveals consistent patterns across diverse VideoQA\ntasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frame\ninteractions in early-to-middle layers, (2) followed by progressive\nvideo-language integration in middle layers. This is facilitated by alignment\nbetween video representations and linguistic embeddings containing temporal\nconcepts. (3) Upon completion of this integration, the model is ready to\ngenerate correct answers in middle-to-late layers. (4) Based on our analysis,\nwe show that VideoLLMs can retain their VideoQA performance by selecting these\neffective information pathways while suppressing a substantial amount of\nattention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a\nblueprint on how VideoLLMs perform temporal reasoning and offer practical\ninsights for improving model interpretability and downstream generalization.\nOur project page with the source code is available at\nhttps://map-the-flow.github.io",
            "upvotes": 10,
            "discussionId": "68f1a9566e0bef323a68fd81",
            "projectPage": "https://map-the-flow.github.io",
            "githubRepo": "https://github.com/byminji/map-the-flow",
            "ai_summary": "Video Large Language Models (VideoLLMs) perform video question answering by initiating temporal reasoning through cross-frame interactions, followed by video-language integration, and generate answers using effective information pathways while suppressing unnecessary attention edges.",
            "ai_keywords": [
                "Video Large Language Models",
                "VideoLLMs",
                "video question answering",
                "VideoQA",
                "mechanistic interpretability",
                "temporal reasoning",
                "cross-frame interactions",
                "video-language integration",
                "linguistic embeddings",
                "attention edges"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-10-15T03:59:06.000Z",
        "title": "Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs",
        "summary": "Video Large Language Models (VideoLLMs) extend the capabilities of\nvision-language models to spatiotemporal inputs, enabling tasks such as video\nquestion answering (VideoQA). Despite recent advances in VideoLLMs, their\ninternal mechanisms on where and how they extract and propagate video and\ntextual information remain less explored. In this study, we investigate the\ninternal information flow of VideoLLMs using mechanistic interpretability\ntechniques. Our analysis reveals consistent patterns across diverse VideoQA\ntasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frame\ninteractions in early-to-middle layers, (2) followed by progressive\nvideo-language integration in middle layers. This is facilitated by alignment\nbetween video representations and linguistic embeddings containing temporal\nconcepts. (3) Upon completion of this integration, the model is ready to\ngenerate correct answers in middle-to-late layers. (4) Based on our analysis,\nwe show that VideoLLMs can retain their VideoQA performance by selecting these\neffective information pathways while suppressing a substantial amount of\nattention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a\nblueprint on how VideoLLMs perform temporal reasoning and offer practical\ninsights for improving model interpretability and downstream generalization.\nOur project page with the source code is available at\nhttps://map-the-flow.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13251.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67c6a1e75e2443d7d5f85cb3",
            "avatarUrl": "/avatars/0569b368520411ab828d46725bc3896a.svg",
            "fullname": "Taekyung Kim",
            "name": "taekyung-k",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.21553",
            "authors": [
                {
                    "_id": "68fed89d6cdff8b857f4715f",
                    "name": "Jared Claypoole",
                    "hidden": false
                },
                {
                    "_id": "68fed89d6cdff8b857f47160",
                    "name": "Yunye Gong",
                    "hidden": false
                },
                {
                    "_id": "68fed89d6cdff8b857f47161",
                    "name": "Noson S. Yanofsky",
                    "hidden": false
                },
                {
                    "_id": "68fed89d6cdff8b857f47162",
                    "name": "Ajay Divakaran",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-24T15:12:08.000Z",
            "submittedOnDailyAt": "2025-10-27T01:00:12.146Z",
            "title": "Document Understanding, Measurement, and Manipulation Using Category\n  Theory",
            "submittedOnDailyBy": {
                "_id": "68fed6db05d20c85c3de791f",
                "avatarUrl": "/avatars/ee28ae5ed20e41cb7143507fa62a14a7.svg",
                "isPro": false,
                "fullname": "Ajay Divakaran",
                "user": "AjayDivakaran",
                "type": "user"
            },
            "summary": "We apply category theory to extract multimodal document structure which leads\nus to develop information theoretic measures, content summarization and\nextension, and self-supervised improvement of large pretrained models. We first\ndevelop a mathematical representation of a document as a category of\nquestion-answer pairs. Second, we develop an orthogonalization procedure to\ndivide the information contained in one or more documents into non-overlapping\npieces. The structures extracted in the first and second steps lead us to\ndevelop methods to measure and enumerate the information contained in a\ndocument. We also build on those steps to develop new summarization techniques,\nas well as to develop a solution to a new problem viz. exegesis resulting in an\nextension of the original document. Our question-answer pair methodology\nenables a novel rate distortion analysis of summarization techniques. We\nimplement our techniques using large pretrained models, and we propose a\nmultimodal extension of our overall mathematical framework. Finally, we develop\na novel self-supervised method using RLVR to improve large pretrained models\nusing consistency constraints such as composability and closure under certain\noperations that stem naturally from our category theoretic framework.",
            "upvotes": 4,
            "discussionId": "68fed89d6cdff8b857f47163",
            "ai_summary": "Category theory is used to develop information-theoretic measures, summarization, and self-supervised improvement of large pretrained models through a mathematical framework of question-answer pairs and orthogonalization.",
            "ai_keywords": [
                "category theory",
                "multimodal document structure",
                "information theoretic measures",
                "content summarization",
                "self-supervised improvement",
                "large pretrained models",
                "question-answer pairs",
                "orthogonalization",
                "rate distortion analysis",
                "RLVR",
                "composability",
                "closure"
            ],
            "organization": {
                "_id": "649c744a281fd55e9b7f3343",
                "name": "SRIintl",
                "fullname": "SRI International",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/gRtiyIG2JA4-MlqBz8MI2.png"
            }
        },
        "publishedAt": "2025-10-24T11:12:08.000Z",
        "title": "Document Understanding, Measurement, and Manipulation Using Category\n  Theory",
        "summary": "We apply category theory to extract multimodal document structure which leads\nus to develop information theoretic measures, content summarization and\nextension, and self-supervised improvement of large pretrained models. We first\ndevelop a mathematical representation of a document as a category of\nquestion-answer pairs. Second, we develop an orthogonalization procedure to\ndivide the information contained in one or more documents into non-overlapping\npieces. The structures extracted in the first and second steps lead us to\ndevelop methods to measure and enumerate the information contained in a\ndocument. We also build on those steps to develop new summarization techniques,\nas well as to develop a solution to a new problem viz. exegesis resulting in an\nextension of the original document. Our question-answer pair methodology\nenables a novel rate distortion analysis of summarization techniques. We\nimplement our techniques using large pretrained models, and we propose a\nmultimodal extension of our overall mathematical framework. Finally, we develop\na novel self-supervised method using RLVR to improve large pretrained models\nusing consistency constraints such as composability and closure under certain\noperations that stem naturally from our category theoretic framework.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21553.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68fed6db05d20c85c3de791f",
            "avatarUrl": "/avatars/ee28ae5ed20e41cb7143507fa62a14a7.svg",
            "fullname": "Ajay Divakaran",
            "name": "AjayDivakaran",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "649c744a281fd55e9b7f3343",
            "name": "SRIintl",
            "fullname": "SRI International",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/gRtiyIG2JA4-MlqBz8MI2.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.20535",
            "authors": [
                {
                    "_id": "68fb287f1ff1070cf1666d1f",
                    "user": {
                        "_id": "66ea9f89a43597a36208be6c",
                        "avatarUrl": "/avatars/7024721892d8171923a8d4dced143d09.svg",
                        "isPro": false,
                        "fullname": "Hippolyte Pilchen",
                        "user": "HippolyteP",
                        "type": "user"
                    },
                    "name": "Hippolyte Pilchen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T15:59:40.720Z",
                    "hidden": false
                },
                {
                    "_id": "68fb287f1ff1070cf1666d20",
                    "name": "Edouard Grave",
                    "hidden": false
                },
                {
                    "_id": "68fb287f1ff1070cf1666d21",
                    "name": "Patrick Prez",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T13:20:57.000Z",
            "submittedOnDailyAt": "2025-10-27T00:25:32.894Z",
            "title": "ARC-Encoder: learning compressed text representations for large language\n  models",
            "submittedOnDailyBy": {
                "_id": "66ea9f89a43597a36208be6c",
                "avatarUrl": "/avatars/7024721892d8171923a8d4dced143d09.svg",
                "isPro": false,
                "fullname": "Hippolyte Pilchen",
                "user": "HippolyteP",
                "type": "user"
            },
            "summary": "Recent techniques such as retrieval-augmented generation or chain-of-thought\nreasoning have led to longer contexts and increased inference costs. Context\ncompression techniques can reduce these costs, but the most effective\napproaches require fine-tuning the target model or even modifying its\narchitecture. This can degrade its general abilities when not used for this\nspecific purpose. Here we explore an alternative approach: an encoder that\ncompresses the context into continuous representations which replace token\nembeddings in decoder LLMs. First, we perform a systematic study of training\nstrategies and architecture choices for the encoder. Our findings led to the\ndesign of an Adaptable text Representations Compressor, named ARC-Encoder,\nwhich outputs x-times fewer continuous representations (typically\nx!in!{4,8}) than text tokens. We evaluate ARC-Encoder across a variety\nof LLM usage scenarios, ranging from in-context learning to context window\nextension, on both instruct and base decoders. Results show that ARC-Encoder\nachieves state-of-the-art performance on several benchmarks while improving\ncomputational efficiency at inference. Finally, we demonstrate that our models\ncan be adapted to multiple decoders simultaneously, allowing a single encoder\nto generalize across different decoder LLMs. This makes ARC-Encoder a flexible\nand efficient solution for portable encoders that work seamlessly with multiple\nLLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder\n, fine-tuning dataset and pretrained models are available at\nhttps://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .",
            "upvotes": 4,
            "discussionId": "68fb287f1ff1070cf1666d22",
            "githubRepo": "https://github.com/kyutai-labs/ARC-Encoder",
            "ai_summary": "An ARC-Encoder compresses context into continuous representations for LLMs, improving inference efficiency and performance across various scenarios.",
            "ai_keywords": [
                "retrieval-augmented generation",
                "chain-of-thought reasoning",
                "context compression",
                "token embeddings",
                "decoder LLMs",
                "Adaptable text Representations Compressor",
                "ARC-Encoder",
                "in-context learning",
                "context window extension",
                "instruct decoders",
                "base decoders",
                "pretrained models"
            ],
            "githubStars": 14,
            "organization": {
                "_id": "6683d6350b54a28aff6645fe",
                "name": "kyutai",
                "fullname": "Kyutai",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6355a3c1805be5a8f30fea49/8xGdIOlfkopZfhbMitw_k.jpeg"
            }
        },
        "publishedAt": "2025-10-23T09:20:57.000Z",
        "title": "ARC-Encoder: learning compressed text representations for large language\n  models",
        "summary": "Recent techniques such as retrieval-augmented generation or chain-of-thought\nreasoning have led to longer contexts and increased inference costs. Context\ncompression techniques can reduce these costs, but the most effective\napproaches require fine-tuning the target model or even modifying its\narchitecture. This can degrade its general abilities when not used for this\nspecific purpose. Here we explore an alternative approach: an encoder that\ncompresses the context into continuous representations which replace token\nembeddings in decoder LLMs. First, we perform a systematic study of training\nstrategies and architecture choices for the encoder. Our findings led to the\ndesign of an Adaptable text Representations Compressor, named ARC-Encoder,\nwhich outputs x-times fewer continuous representations (typically\nx!in!{4,8}) than text tokens. We evaluate ARC-Encoder across a variety\nof LLM usage scenarios, ranging from in-context learning to context window\nextension, on both instruct and base decoders. Results show that ARC-Encoder\nachieves state-of-the-art performance on several benchmarks while improving\ncomputational efficiency at inference. Finally, we demonstrate that our models\ncan be adapted to multiple decoders simultaneously, allowing a single encoder\nto generalize across different decoder LLMs. This makes ARC-Encoder a flexible\nand efficient solution for portable encoders that work seamlessly with multiple\nLLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder\n, fine-tuning dataset and pretrained models are available at\nhttps://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20535.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66ea9f89a43597a36208be6c",
            "avatarUrl": "/avatars/7024721892d8171923a8d4dced143d09.svg",
            "fullname": "Hippolyte Pilchen",
            "name": "HippolyteP",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "6683d6350b54a28aff6645fe",
            "name": "kyutai",
            "fullname": "Kyutai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6355a3c1805be5a8f30fea49/8xGdIOlfkopZfhbMitw_k.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.21652",
            "authors": [
                {
                    "_id": "68fed3ee6cdff8b857f47128",
                    "name": "Jonathan Bragg",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47129",
                    "name": "Mike D'Arcy",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f4712a",
                    "name": "Nishant Balepur",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f4712b",
                    "name": "Dan Bareket",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f4712c",
                    "name": "Bhavana Dalvi",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f4712d",
                    "name": "Sergey Feldman",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f4712e",
                    "name": "Dany Haddad",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f4712f",
                    "name": "Jena D. Hwang",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47130",
                    "name": "Peter Jansen",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47131",
                    "name": "Varsha Kishore",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47132",
                    "name": "Bodhisattwa Prasad Majumder",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47133",
                    "name": "Aakanksha Naik",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47134",
                    "name": "Sigal Rahamimov",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47135",
                    "name": "Kyle Richardson",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47136",
                    "name": "Amanpreet Singh",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47137",
                    "name": "Harshit Surana",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47138",
                    "name": "Aryeh Tiktinsky",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47139",
                    "name": "Rosni Vasu",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f4713a",
                    "name": "Guy Wiener",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f4713b",
                    "name": "Chloe Anastasiades",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f4713c",
                    "name": "Stefan Candra",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f4713d",
                    "name": "Jason Dunkelberger",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f4713e",
                    "name": "Dan Emery",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f4713f",
                    "name": "Rob Evans",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47140",
                    "name": "Malachi Hamada",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47141",
                    "name": "Regan Huff",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47142",
                    "name": "Rodney Kinney",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47143",
                    "name": "Matt Latzke",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47144",
                    "name": "Jaron Lochner",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47145",
                    "name": "Ruben Lozano-Aguilera",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47146",
                    "name": "Cecile Nguyen",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47147",
                    "name": "Smita Rao",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47148",
                    "name": "Amber Tanaka",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f47149",
                    "name": "Brooke Vlahos",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f4714a",
                    "name": "Peter Clark",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f4714b",
                    "name": "Doug Downey",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f4714c",
                    "name": "Yoav Goldberg",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f4714d",
                    "name": "Ashish Sabharwal",
                    "hidden": false
                },
                {
                    "_id": "68fed3ee6cdff8b857f4714e",
                    "name": "Daniel S. Weld",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-24T17:10:26.000Z",
            "submittedOnDailyAt": "2025-10-27T00:37:44.377Z",
            "title": "AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research\n  Suite",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "AI agents hold the potential to revolutionize scientific productivity by\nautomating literature reviews, replicating experiments, analyzing data, and\neven proposing new directions of inquiry; indeed, there are now many such\nagents, ranging from general-purpose \"deep research\" systems to specialized\nscience-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of\nthese agents is critical for progress. Yet existing benchmarks fall short on\nseveral fronts: they (1) fail to provide holistic, product-informed measures of\nreal-world use cases such as science research; (2) lack reproducible agent\ntools necessary for a controlled comparison of core agentic capabilities; (3)\ndo not account for confounding variables such as model cost and tool access;\n(4) do not provide standardized interfaces for quick agent prototyping and\nevaluation; and (5) lack comprehensive baseline agents necessary to identify\ntrue advances. In response, we define principles and tooling for more\nrigorously benchmarking agents. Using these, we present AstaBench, a suite that\nprovides the first holistic measure of agentic ability to perform scientific\nresearch, comprising 2400+ problems spanning the entire scientific discovery\nprocess and multiple scientific domains, and including many problems inspired\nby actual user requests to deployed Asta agents. Our suite comes with the first\nscientific research environment with production-grade search tools that enable\ncontrolled, reproducible evaluation, better accounting for confounders.\nAlongside, we provide a comprehensive suite of nine science-optimized classes\nof Asta agents and numerous baselines. Our extensive evaluation of 57 agents\nacross 22 agent classes reveals several interesting findings, most importantly\nthat despite meaningful progress on certain individual aspects, AI remains far\nfrom solving the challenge of science research assistance.",
            "upvotes": 3,
            "discussionId": "68fed3ee6cdff8b857f4714f",
            "ai_summary": "AstaBench provides a comprehensive benchmark suite for evaluating AI agents in scientific research, revealing that while progress has been made, AI still falls short in fully assisting scientific research.",
            "ai_keywords": [
                "AI agents",
                "deep research systems",
                "AI Scientist",
                "AIGS",
                "AstaBench",
                "scientific discovery process",
                "production-grade search tools",
                "science-optimized agents",
                "baseline agents"
            ]
        },
        "publishedAt": "2025-10-24T13:10:26.000Z",
        "title": "AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research\n  Suite",
        "summary": "AI agents hold the potential to revolutionize scientific productivity by\nautomating literature reviews, replicating experiments, analyzing data, and\neven proposing new directions of inquiry; indeed, there are now many such\nagents, ranging from general-purpose \"deep research\" systems to specialized\nscience-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of\nthese agents is critical for progress. Yet existing benchmarks fall short on\nseveral fronts: they (1) fail to provide holistic, product-informed measures of\nreal-world use cases such as science research; (2) lack reproducible agent\ntools necessary for a controlled comparison of core agentic capabilities; (3)\ndo not account for confounding variables such as model cost and tool access;\n(4) do not provide standardized interfaces for quick agent prototyping and\nevaluation; and (5) lack comprehensive baseline agents necessary to identify\ntrue advances. In response, we define principles and tooling for more\nrigorously benchmarking agents. Using these, we present AstaBench, a suite that\nprovides the first holistic measure of agentic ability to perform scientific\nresearch, comprising 2400+ problems spanning the entire scientific discovery\nprocess and multiple scientific domains, and including many problems inspired\nby actual user requests to deployed Asta agents. Our suite comes with the first\nscientific research environment with production-grade search tools that enable\ncontrolled, reproducible evaluation, better accounting for confounders.\nAlongside, we provide a comprehensive suite of nine science-optimized classes\nof Asta agents and numerous baselines. Our extensive evaluation of 57 agents\nacross 22 agent classes reveals several interesting findings, most importantly\nthat despite meaningful progress on certain individual aspects, AI remains far\nfrom solving the challenge of science research assistance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21652.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 147
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.21447",
            "authors": [
                {
                    "_id": "68fed0996cdff8b857f47101",
                    "name": "Yu Yang",
                    "hidden": false
                },
                {
                    "_id": "68fed0996cdff8b857f47102",
                    "user": {
                        "_id": "643eac99510c894d9110a9ed",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643eac99510c894d9110a9ed/VYPrfvMoe-gOoQhLU_v84.jpeg",
                        "isPro": false,
                        "fullname": "Zhilu Zhang",
                        "user": "cszhilu1998",
                        "type": "user"
                    },
                    "name": "Zhilu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:27:36.894Z",
                    "hidden": false
                },
                {
                    "_id": "68fed0996cdff8b857f47103",
                    "name": "Xiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68fed0996cdff8b857f47104",
                    "name": "Yihan Zeng",
                    "hidden": false
                },
                {
                    "_id": "68fed0996cdff8b857f47105",
                    "name": "Hui Li",
                    "hidden": false
                },
                {
                    "_id": "68fed0996cdff8b857f47106",
                    "name": "Wangmeng Zuo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/viGvTpS6zP3f5OTYnZT6q.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/rTAT8-YnnJbMDl-XyBJbz.mp4"
            ],
            "publishedAt": "2025-10-24T13:25:39.000Z",
            "submittedOnDailyAt": "2025-10-27T00:24:31.888Z",
            "title": "PhysWorld: From Real Videos to World Models of Deformable Objects via\n  Physics-Aware Demonstration Synthesis",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Interactive world models that simulate object dynamics are crucial for\nrobotics, VR, and AR. However, it remains a significant challenge to learn\nphysics-consistent dynamics models from limited real-world video data,\nespecially for deformable objects with spatially-varying physical properties.\nTo overcome the challenge of data scarcity, we propose PhysWorld, a novel\nframework that utilizes a simulator to synthesize physically plausible and\ndiverse demonstrations to learn efficient world models. Specifically, we first\nconstruct a physics-consistent digital twin within MPM simulator via\nconstitutive model selection and global-to-local optimization of physical\nproperties. Subsequently, we apply part-aware perturbations to the physical\nproperties and generate various motion patterns for the digital twin,\nsynthesizing extensive and diverse demonstrations. Finally, using these\ndemonstrations, we train a lightweight GNN-based world model that is embedded\nwith physical properties. The real video can be used to further refine the\nphysical properties. PhysWorld achieves accurate and fast future predictions\nfor various deformable objects, and also generalizes well to novel\ninteractions. Experiments show that PhysWorld has competitive performance while\nenabling inference speeds 47 times faster than the recent state-of-the-art\nmethod, i.e., PhysTwin.",
            "upvotes": 3,
            "discussionId": "68fed09a6cdff8b857f47107",
            "projectPage": "https://physworld.github.io/",
            "githubRepo": "https://github.com/AlanYoung123/PhysWorld",
            "ai_summary": "PhysWorld uses a simulator to generate diverse demonstrations for training a GNN-based world model, enabling accurate and fast predictions for deformable objects with competitive performance and faster inference speeds.",
            "ai_keywords": [
                "physics-consistent dynamics models",
                "deformable objects",
                "spatially-varying physical properties",
                "MPM simulator",
                "constitutive model selection",
                "global-to-local optimization",
                "part-aware perturbations",
                "GNN-based world model",
                "future predictions",
                "novel interactions",
                "PhysTwin"
            ],
            "githubStars": 9
        },
        "publishedAt": "2025-10-24T09:25:39.000Z",
        "title": "PhysWorld: From Real Videos to World Models of Deformable Objects via\n  Physics-Aware Demonstration Synthesis",
        "summary": "Interactive world models that simulate object dynamics are crucial for\nrobotics, VR, and AR. However, it remains a significant challenge to learn\nphysics-consistent dynamics models from limited real-world video data,\nespecially for deformable objects with spatially-varying physical properties.\nTo overcome the challenge of data scarcity, we propose PhysWorld, a novel\nframework that utilizes a simulator to synthesize physically plausible and\ndiverse demonstrations to learn efficient world models. Specifically, we first\nconstruct a physics-consistent digital twin within MPM simulator via\nconstitutive model selection and global-to-local optimization of physical\nproperties. Subsequently, we apply part-aware perturbations to the physical\nproperties and generate various motion patterns for the digital twin,\nsynthesizing extensive and diverse demonstrations. Finally, using these\ndemonstrations, we train a lightweight GNN-based world model that is embedded\nwith physical properties. The real video can be used to further refine the\nphysical properties. PhysWorld achieves accurate and fast future predictions\nfor various deformable objects, and also generalizes well to novel\ninteractions. Experiments show that PhysWorld has competitive performance while\nenabling inference speeds 47 times faster than the recent state-of-the-art\nmethod, i.e., PhysTwin.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/viGvTpS6zP3f5OTYnZT6q.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/rTAT8-YnnJbMDl-XyBJbz.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21447.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 147
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.20780",
            "authors": [
                {
                    "_id": "68fb8ba56cdff8b857f46b90",
                    "user": {
                        "_id": "62495cb96ee7ee6b646db130",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg",
                        "isPro": false,
                        "fullname": "Runzhe Zhan",
                        "user": "rzzhan",
                        "type": "user"
                    },
                    "name": "Runzhe Zhan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:29:02.663Z",
                    "hidden": false
                },
                {
                    "_id": "68fb8ba56cdff8b857f46b91",
                    "name": "Zhihong Huang",
                    "hidden": false
                },
                {
                    "_id": "68fb8ba56cdff8b857f46b92",
                    "name": "Xinyi Yang",
                    "hidden": false
                },
                {
                    "_id": "68fb8ba56cdff8b857f46b93",
                    "name": "Lidia S. Chao",
                    "hidden": false
                },
                {
                    "_id": "68fb8ba56cdff8b857f46b94",
                    "name": "Min Yang",
                    "hidden": false
                },
                {
                    "_id": "68fb8ba56cdff8b857f46b95",
                    "name": "Derek F. Wong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T17:48:36.000Z",
            "submittedOnDailyAt": "2025-10-27T02:33:08.616Z",
            "title": "Are Large Reasoning Models Good Translation Evaluators? Analysis and\n  Performance Boost",
            "submittedOnDailyBy": {
                "_id": "62495cb96ee7ee6b646db130",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg",
                "isPro": false,
                "fullname": "Runzhe Zhan",
                "user": "rzzhan",
                "type": "user"
            },
            "summary": "Recent advancements in large reasoning models (LRMs) have introduced an\nintermediate \"thinking\" process prior to generating final answers, improving\ntheir reasoning capabilities on complex downstream tasks. However, the\npotential of LRMs as evaluators for machine translation (MT) quality remains\nunderexplored. We provides the first systematic analysis of LRM-as-a-judge in\nMT evaluation. We identify key challenges, revealing LRMs require tailored\nevaluation materials, tend to \"overthink\" simpler instances and have issues\nwith scoring mechanisms leading to overestimation. To address these, we propose\nto calibrate LRM thinking by training them on synthetic, human-like thinking\ntrajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this\napproach largely reduces thinking budgets by ~35x while concurrently improving\nevaluation performance across different LRM scales from 7B to 32B (e.g.,\nR1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These\nfindings highlight the potential of efficiently calibrated LRMs to advance\nfine-grained automatic MT evaluation.",
            "upvotes": 3,
            "discussionId": "68fb8ba56cdff8b857f46b96",
            "githubRepo": "https://github.com/NLP2CT/ThinMQM",
            "ai_summary": "Calibrating large reasoning models with synthetic human-like thinking trajectories improves their efficiency and performance in machine translation evaluation.",
            "ai_keywords": [
                "large reasoning models",
                "thinking process",
                "machine translation",
                "evaluation",
                "synthetic thinking trajectories",
                "WMT24 Metrics",
                "correlation point improvement"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-10-23T13:48:36.000Z",
        "title": "Are Large Reasoning Models Good Translation Evaluators? Analysis and\n  Performance Boost",
        "summary": "Recent advancements in large reasoning models (LRMs) have introduced an\nintermediate \"thinking\" process prior to generating final answers, improving\ntheir reasoning capabilities on complex downstream tasks. However, the\npotential of LRMs as evaluators for machine translation (MT) quality remains\nunderexplored. We provides the first systematic analysis of LRM-as-a-judge in\nMT evaluation. We identify key challenges, revealing LRMs require tailored\nevaluation materials, tend to \"overthink\" simpler instances and have issues\nwith scoring mechanisms leading to overestimation. To address these, we propose\nto calibrate LRM thinking by training them on synthetic, human-like thinking\ntrajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this\napproach largely reduces thinking budgets by ~35x while concurrently improving\nevaluation performance across different LRM scales from 7B to 32B (e.g.,\nR1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These\nfindings highlight the potential of efficiently calibrated LRMs to advance\nfine-grained automatic MT evaluation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20780.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62495cb96ee7ee6b646db130",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg",
            "fullname": "Runzhe Zhan",
            "name": "rzzhan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.17234",
            "authors": [
                {
                    "_id": "68f9ab58b9b2e4ae046738b1",
                    "user": {
                        "_id": "66e03d7123e5f162e7e5c5ac",
                        "avatarUrl": "/avatars/28089b6ffeacafff2c798b7579689c1e.svg",
                        "isPro": false,
                        "fullname": "hongyuyang",
                        "user": "hongyuyang23casia",
                        "type": "user"
                    },
                    "name": "Yuyang Hong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-25T14:34:02.018Z",
                    "hidden": false
                },
                {
                    "_id": "68f9ab58b9b2e4ae046738b2",
                    "name": "Qi Yang",
                    "hidden": false
                },
                {
                    "_id": "68f9ab58b9b2e4ae046738b3",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f9ab58b9b2e4ae046738b4",
                    "name": "Zili Wang",
                    "hidden": false
                },
                {
                    "_id": "68f9ab58b9b2e4ae046738b5",
                    "name": "Zhaojin Fu",
                    "hidden": false
                },
                {
                    "_id": "68f9ab58b9b2e4ae046738b6",
                    "name": "Kun Ding",
                    "hidden": false
                },
                {
                    "_id": "68f9ab58b9b2e4ae046738b7",
                    "name": "Bin Fan",
                    "hidden": false
                },
                {
                    "_id": "68f9ab58b9b2e4ae046738b8",
                    "name": "Shiming Xiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T07:23:36.000Z",
            "submittedOnDailyAt": "2025-10-27T01:37:02.299Z",
            "title": "Taming Modality Entanglement in Continual Audio-Visual Segmentation",
            "submittedOnDailyBy": {
                "_id": "66e03d7123e5f162e7e5c5ac",
                "avatarUrl": "/avatars/28089b6ffeacafff2c798b7579689c1e.svg",
                "isPro": false,
                "fullname": "hongyuyang",
                "user": "hongyuyang23casia",
                "type": "user"
            },
            "summary": "Recently, significant progress has been made in multi-modal continual\nlearning, aiming to learn new tasks sequentially in multi-modal settings while\npreserving performance on previously learned ones. However, existing methods\nmainly focus on coarse-grained tasks, with limitations in addressing modality\nentanglement in fine-grained continual learning settings. To bridge this gap,\nwe introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to\ncontinuously segment new classes guided by audio. Through comprehensive\nanalysis, two critical challenges are identified: 1) multi-modal semantic\ndrift, where a sounding objects is labeled as background in sequential tasks;\n2) co-occurrence confusion, where frequent co-occurring classes tend to be\nconfused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework\nis designed to address these challenges. Specifically, for multi-modal semantic\ndrift, a Multi-modal Sample Selection (MSS) strategy is proposed to select\nsamples with high modal consistency for rehearsal. Meanwhile, for co-occurence\nconfusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed,\nallowing for the increase of rehearsal sample frequency of those confusable\nclasses during training process. Moreover, we construct three audio-visual\nincremental scenarios to verify effectiveness of our method. Comprehensive\nexperiments demonstrate that our method significantly outperforms single-modal\ncontinual learning methods.",
            "upvotes": 3,
            "discussionId": "68f9ab58b9b2e4ae046738b9",
            "ai_summary": "A novel Continual Audio-Visual Segmentation (CAVS) task addresses challenges in multi-modal continual learning through a Collision-based Multi-modal Rehearsal (CMR) framework, improving performance over single-modal methods.",
            "ai_keywords": [
                "multi-modal continual learning",
                "Continual Audio-Visual Segmentation",
                "CAVS",
                "multi-modal semantic drift",
                "co-occurrence confusion",
                "Collision-based Multi-modal Rehearsal",
                "CMR",
                "Multi-modal Sample Selection",
                "MSS",
                "Collision-based Sample Rehearsal",
                "CSR"
            ]
        },
        "publishedAt": "2025-10-20T03:23:36.000Z",
        "title": "Taming Modality Entanglement in Continual Audio-Visual Segmentation",
        "summary": "Recently, significant progress has been made in multi-modal continual\nlearning, aiming to learn new tasks sequentially in multi-modal settings while\npreserving performance on previously learned ones. However, existing methods\nmainly focus on coarse-grained tasks, with limitations in addressing modality\nentanglement in fine-grained continual learning settings. To bridge this gap,\nwe introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to\ncontinuously segment new classes guided by audio. Through comprehensive\nanalysis, two critical challenges are identified: 1) multi-modal semantic\ndrift, where a sounding objects is labeled as background in sequential tasks;\n2) co-occurrence confusion, where frequent co-occurring classes tend to be\nconfused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework\nis designed to address these challenges. Specifically, for multi-modal semantic\ndrift, a Multi-modal Sample Selection (MSS) strategy is proposed to select\nsamples with high modal consistency for rehearsal. Meanwhile, for co-occurence\nconfusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed,\nallowing for the increase of rehearsal sample frequency of those confusable\nclasses during training process. Moreover, we construct three audio-visual\nincremental scenarios to verify effectiveness of our method. Comprehensive\nexperiments demonstrate that our method significantly outperforms single-modal\ncontinual learning methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17234.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66e03d7123e5f162e7e5c5ac",
            "avatarUrl": "/avatars/28089b6ffeacafff2c798b7579689c1e.svg",
            "fullname": "hongyuyang",
            "name": "hongyuyang23casia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.21581",
            "authors": [
                {
                    "_id": "68ff50f8f3205b9e39c5f3fb",
                    "user": {
                        "_id": "63357214eb6132ca653020e7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63357214eb6132ca653020e7/A_imoyFDx30wgi0guG_s8.png",
                        "isPro": true,
                        "fullname": "Ciara",
                        "user": "CiaraRowles",
                        "type": "user"
                    },
                    "name": "Ciara Rowles",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T14:16:23.529Z",
                    "hidden": false
                },
                {
                    "_id": "68ff50f8f3205b9e39c5f3fc",
                    "name": "Varun Jampani",
                    "hidden": false
                },
                {
                    "_id": "68ff50f8f3205b9e39c5f3fd",
                    "name": "Simon Donn",
                    "hidden": false
                },
                {
                    "_id": "68ff50f8f3205b9e39c5f3fe",
                    "name": "Shimon Vainer",
                    "hidden": false
                },
                {
                    "_id": "68ff50f8f3205b9e39c5f3ff",
                    "name": "Julian Parker",
                    "hidden": false
                },
                {
                    "_id": "68ff50f8f3205b9e39c5f400",
                    "name": "Zach Evans",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-24T15:49:54.000Z",
            "submittedOnDailyAt": "2025-10-27T09:32:42.885Z",
            "title": "Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video",
            "submittedOnDailyBy": {
                "_id": "63357214eb6132ca653020e7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63357214eb6132ca653020e7/A_imoyFDx30wgi0guG_s8.png",
                "isPro": true,
                "fullname": "Ciara",
                "user": "CiaraRowles",
                "type": "user"
            },
            "summary": "Foley Control is a lightweight approach to video-guided Foley that keeps\npretrained single-modality models frozen and learns only a small\ncross-attention bridge between them. We connect V-JEPA2 video embeddings to a\nfrozen Stable Audio Open DiT text-to-audio (T2A) model by inserting compact\nvideo cross-attention after the model's existing text cross-attention, so\nprompts set global semantics while video refines timing and local dynamics. The\nfrozen backbones retain strong marginals (video; audio given text) and the\nbridge learns the audio-video dependency needed for synchronization -- without\nretraining the audio prior. To cut memory and stabilize training, we pool video\ntokens before conditioning. On curated video-audio benchmarks, Foley Control\ndelivers competitive temporal and semantic alignment with far fewer trainable\nparameters than recent multi-modal systems, while preserving prompt-driven\ncontrollability and production-friendly modularity (swap/upgrade encoders or\nthe T2A backbone without end-to-end retraining). Although we focus on\nVideo-to-Foley, the same bridge design can potentially extend to other audio\nmodalities (e.g., speech).",
            "upvotes": 2,
            "discussionId": "68ff50f9f3205b9e39c5f401",
            "ai_summary": "Foley Control uses a small cross-attention bridge to synchronize video and audio without retraining, achieving competitive alignment with fewer parameters and maintaining modularity.",
            "ai_keywords": [
                "cross-attention",
                "V-JEPA2",
                "Stable Audio Open DiT",
                "text-to-audio",
                "video embeddings",
                "audio-video dependency",
                "video tokens",
                "temporal alignment",
                "semantic alignment",
                "prompt-driven controllability",
                "production-friendly modularity"
            ],
            "organization": {
                "_id": "62e1573a6fb6e362b4a90690",
                "name": "stabilityai",
                "fullname": "Stability AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png"
            }
        },
        "publishedAt": "2025-10-24T11:49:54.000Z",
        "title": "Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video",
        "summary": "Foley Control is a lightweight approach to video-guided Foley that keeps\npretrained single-modality models frozen and learns only a small\ncross-attention bridge between them. We connect V-JEPA2 video embeddings to a\nfrozen Stable Audio Open DiT text-to-audio (T2A) model by inserting compact\nvideo cross-attention after the model's existing text cross-attention, so\nprompts set global semantics while video refines timing and local dynamics. The\nfrozen backbones retain strong marginals (video; audio given text) and the\nbridge learns the audio-video dependency needed for synchronization -- without\nretraining the audio prior. To cut memory and stabilize training, we pool video\ntokens before conditioning. On curated video-audio benchmarks, Foley Control\ndelivers competitive temporal and semantic alignment with far fewer trainable\nparameters than recent multi-modal systems, while preserving prompt-driven\ncontrollability and production-friendly modularity (swap/upgrade encoders or\nthe T2A backbone without end-to-end retraining). Although we focus on\nVideo-to-Foley, the same bridge design can potentially extend to other audio\nmodalities (e.g., speech).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21581.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63357214eb6132ca653020e7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63357214eb6132ca653020e7/A_imoyFDx30wgi0guG_s8.png",
            "fullname": "Ciara",
            "name": "CiaraRowles",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 119
        },
        "organization": {
            "_id": "62e1573a6fb6e362b4a90690",
            "name": "stabilityai",
            "fullname": "Stability AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.21057",
            "authors": [
                {
                    "_id": "68ff4d0ef3205b9e39c5f3e0",
                    "name": "Nils Philipp Walter",
                    "hidden": false
                },
                {
                    "_id": "68ff4d0ef3205b9e39c5f3e1",
                    "name": "Chawin Sitawarin",
                    "hidden": false
                },
                {
                    "_id": "68ff4d0ef3205b9e39c5f3e2",
                    "name": "Jamie Hayes",
                    "hidden": false
                },
                {
                    "_id": "68ff4d0ef3205b9e39c5f3e3",
                    "name": "David Stutz",
                    "hidden": false
                },
                {
                    "_id": "68ff4d0ef3205b9e39c5f3e4",
                    "user": {
                        "_id": "6475c2794766357252e69e9f",
                        "avatarUrl": "/avatars/757ed789423113369868e972f21ce559.svg",
                        "isPro": false,
                        "fullname": "i",
                        "user": "iliashum",
                        "type": "user"
                    },
                    "name": "Ilia Shumailov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T14:16:35.925Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-24T00:04:07.000Z",
            "submittedOnDailyAt": "2025-10-27T09:15:59.674Z",
            "title": "Soft Instruction De-escalation Defense",
            "submittedOnDailyBy": {
                "_id": "6475c2794766357252e69e9f",
                "avatarUrl": "/avatars/757ed789423113369868e972f21ce559.svg",
                "isPro": false,
                "fullname": "i",
                "user": "iliashum",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment; this makes them susceptible to\nprompt injections when dealing with untrusted data. To overcome this\nlimitation, we propose SIC (Soft Instruction Control)-a simple yet effective\niterative prompt sanitization loop designed for tool-augmented LLM agents. Our\nmethod repeatedly inspects incoming data for instructions that could compromise\nagent behavior. If such content is found, the malicious content is rewritten,\nmasked, or removed, and the result is re-evaluated. The process continues until\nthe input is clean or a maximum iteration limit is reached; if imperative\ninstruction-like content remains, the agent halts to ensure security. By\nallowing multiple passes, our approach acknowledges that individual rewrites\nmay fail but enables the system to catch and correct missed injections in later\nsteps. Although immediately useful, worst-case analysis shows that SIC is not\ninfallible; strong adversary can still get a 15% ASR by embedding\nnon-imperative workflows. This nonetheless raises the bar.",
            "upvotes": 2,
            "discussionId": "68ff4d0ef3205b9e39c5f3e5",
            "ai_summary": "SIC, an iterative prompt sanitization method, enhances the security of tool-augmented LLM agents by repeatedly inspecting and cleaning incoming data to prevent prompt injection attacks.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "prompt injections",
                "iterative prompt sanitization",
                "tool-augmented LLM agents",
                "imperative instruction-like content",
                "worst-case analysis",
                "ASR"
            ],
            "organization": {
                "_id": "5e6aca39878b8b2bf9806447",
                "name": "google",
                "fullname": "Google",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
            }
        },
        "publishedAt": "2025-10-23T20:04:07.000Z",
        "title": "Soft Instruction De-escalation Defense",
        "summary": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment; this makes them susceptible to\nprompt injections when dealing with untrusted data. To overcome this\nlimitation, we propose SIC (Soft Instruction Control)-a simple yet effective\niterative prompt sanitization loop designed for tool-augmented LLM agents. Our\nmethod repeatedly inspects incoming data for instructions that could compromise\nagent behavior. If such content is found, the malicious content is rewritten,\nmasked, or removed, and the result is re-evaluated. The process continues until\nthe input is clean or a maximum iteration limit is reached; if imperative\ninstruction-like content remains, the agent halts to ensure security. By\nallowing multiple passes, our approach acknowledges that individual rewrites\nmay fail but enables the system to catch and correct missed injections in later\nsteps. Although immediately useful, worst-case analysis shows that SIC is not\ninfallible; strong adversary can still get a 15% ASR by embedding\nnon-imperative workflows. This nonetheless raises the bar.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21057.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6475c2794766357252e69e9f",
            "avatarUrl": "/avatars/757ed789423113369868e972f21ce559.svg",
            "fullname": "i",
            "name": "iliashum",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "5e6aca39878b8b2bf9806447",
            "name": "google",
            "fullname": "Google",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.11370",
            "authors": [
                {
                    "_id": "68ff46536cdff8b857f47319",
                    "user": {
                        "_id": "6686f695840ee769597de318",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6686f695840ee769597de318/50XZQ80Y2PElI35O0qVFc.jpeg",
                        "isPro": false,
                        "fullname": "Wenhan Ma",
                        "user": "CuteNPC",
                        "type": "user"
                    },
                    "name": "Wenhan Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-10-27T14:46:19.865Z",
                    "hidden": false
                },
                {
                    "_id": "68ff46536cdff8b857f4731a",
                    "name": "Hailin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ff46536cdff8b857f4731b",
                    "name": "Liang Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ff46536cdff8b857f4731c",
                    "name": "Yifan Song",
                    "hidden": false
                },
                {
                    "_id": "68ff46536cdff8b857f4731d",
                    "name": "Yudong Wang",
                    "hidden": false
                },
                {
                    "_id": "68ff46536cdff8b857f4731e",
                    "name": "Zhifang Sui",
                    "hidden": false
                },
                {
                    "_id": "68ff46536cdff8b857f4731f",
                    "user": {
                        "_id": "6538815d1bdb3c40db94fbfa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6538815d1bdb3c40db94fbfa/id7aSY8JUgKK2agKWLERt.jpeg",
                        "isPro": false,
                        "fullname": "Fuli Luo",
                        "user": "luofuli",
                        "type": "user"
                    },
                    "name": "Fuli Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-10-27T14:47:12.114Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-13T13:11:27.000Z",
            "submittedOnDailyAt": "2025-10-27T08:47:39.587Z",
            "title": "Stabilizing MoE Reinforcement Learning by Aligning Training and\n  Inference Routers",
            "submittedOnDailyBy": {
                "_id": "63a369d98c0c89dcae3b8329",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
                "isPro": true,
                "fullname": "Adina Yakefu",
                "user": "AdinaY",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing\nthe capabilities of large language models. However, in Mixture-of-Experts (MoE)\nmodels, the routing mechanism often introduces instability, even leading to\ncatastrophic RL training collapse. We analyze the training-inference\nconsistency of MoE models and identify a notable discrepancy in routing\nbehaviors between the two phases. Moreover, even under identical conditions,\nthe routing framework can yield divergent expert selections across repeated\nforward passes. To address this foundational inconsistency, we propose Rollout\nRouting Replay (R3), a method that records routing distributions from the\ninference engine and replays them during training. R3 significantly reduces\ntraining-inference policy KL divergence and mitigates extreme discrepancies\nwithout compromising training speed. Extensive experiments on various settings\nconfirm that R3 succeeds in stabilizing RL training, preventing collapse and\noutperforming methods such as GSPO and TIS. We believe this work can offer a\nnew solution for stabilizing RL in MoE models.",
            "upvotes": 2,
            "discussionId": "68ff46536cdff8b857f47320",
            "ai_summary": "Rollout Routing Replay (R3) stabilizes reinforcement learning training in Mixture-of-Experts models by reducing discrepancies between training and inference routing behaviors.",
            "ai_keywords": [
                "Mixture-of-Experts",
                "routing mechanism",
                "reinforcement learning",
                "training-inference consistency",
                "Rollout Routing Replay",
                "KL divergence",
                "GSPO",
                "TIS"
            ],
            "organization": {
                "_id": "680cb4c37f289defb2210940",
                "name": "XiaomiMiMo",
                "fullname": "Xiaomi MiMo",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/680cb7d1233834890a64acee/6w2qX9u28SPLL11os2Y9C.png"
            }
        },
        "publishedAt": "2025-10-13T09:11:27.000Z",
        "title": "Stabilizing MoE Reinforcement Learning by Aligning Training and\n  Inference Routers",
        "summary": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing\nthe capabilities of large language models. However, in Mixture-of-Experts (MoE)\nmodels, the routing mechanism often introduces instability, even leading to\ncatastrophic RL training collapse. We analyze the training-inference\nconsistency of MoE models and identify a notable discrepancy in routing\nbehaviors between the two phases. Moreover, even under identical conditions,\nthe routing framework can yield divergent expert selections across repeated\nforward passes. To address this foundational inconsistency, we propose Rollout\nRouting Replay (R3), a method that records routing distributions from the\ninference engine and replays them during training. R3 significantly reduces\ntraining-inference policy KL divergence and mitigates extreme discrepancies\nwithout compromising training speed. Extensive experiments on various settings\nconfirm that R3 succeeds in stabilizing RL training, preventing collapse and\noutperforming methods such as GSPO and TIS. We believe this work can offer a\nnew solution for stabilizing RL in MoE models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11370.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63a369d98c0c89dcae3b8329",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
            "fullname": "Adina Yakefu",
            "name": "AdinaY",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1149
        },
        "organization": {
            "_id": "680cb4c37f289defb2210940",
            "name": "XiaomiMiMo",
            "fullname": "Xiaomi MiMo",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/680cb7d1233834890a64acee/6w2qX9u28SPLL11os2Y9C.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.21440",
            "authors": [
                {
                    "_id": "68ff96cf22d452aac6dd41ee",
                    "name": "Giovanni Trappolini",
                    "hidden": false
                },
                {
                    "_id": "68ff96cf22d452aac6dd41ef",
                    "name": "Florin Cuconasu",
                    "hidden": false
                },
                {
                    "_id": "68ff96cf22d452aac6dd41f0",
                    "name": "Simone Filice",
                    "hidden": false
                },
                {
                    "_id": "68ff96cf22d452aac6dd41f1",
                    "name": "Yoelle Maarek",
                    "hidden": false
                },
                {
                    "_id": "68ff96cf22d452aac6dd41f2",
                    "name": "Fabrizio Silvestri",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-24T13:17:00.000Z",
            "submittedOnDailyAt": "2025-10-27T14:42:21.024Z",
            "title": "Redefining Retrieval Evaluation in the Era of LLMs",
            "submittedOnDailyBy": {
                "_id": "631e51736d6a5870f3d7d0fe",
                "avatarUrl": "/avatars/9eb93a278d01c3abd6dc28c708f0d107.svg",
                "isPro": false,
                "fullname": "Giovanni Trappolini",
                "user": "rarinpear",
                "type": "user"
            },
            "summary": "Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR,\nassume that human users sequentially examine documents with diminishing\nattention to lower ranks. This assumption breaks down in Retrieval Augmented\nGeneration (RAG) systems, where search results are consumed by Large Language\nModels (LLMs), which, unlike humans, process all retrieved documents as a whole\nrather than sequentially. Additionally, traditional IR metrics do not account\nfor related but irrelevant documents that actively degrade generation quality,\nrather than merely being ignored. Due to these two major misalignments, namely\nhuman vs. machine position discount and human relevance vs. machine utility,\nclassical IR metrics do not accurately predict RAG performance. We introduce a\nutility-based annotation schema that quantifies both the positive contribution\nof relevant passages and the negative impact of distracting ones. Building on\nthis foundation, we propose UDCG (Utility and Distraction-aware Cumulative\nGain), a metric using an LLM-oriented positional discount to directly optimize\nthe correlation with the end-to-end answer accuracy. Experiments on five\ndatasets and six LLMs demonstrate that UDCG improves correlation by up to 36%\ncompared to traditional metrics. Our work provides a critical step toward\naligning IR evaluation with LLM consumers and enables more reliable assessment\nof RAG components",
            "upvotes": 1,
            "discussionId": "68ff96d022d452aac6dd41f3",
            "githubRepo": "https://github.com/GiovanniTRA/UDCG",
            "ai_summary": "A new metric, UDCG, is introduced to better evaluate Retrieval Augmented Generation systems by accounting for both the utility of relevant documents and the distraction of irrelevant ones, improving correlation with end-to-end answer accuracy.",
            "ai_keywords": [
                "Retrieval Augmented Generation",
                "Large Language Models",
                "nDCG",
                "MAP",
                "MRR",
                "utility-based annotation schema",
                "UDCG",
                "positional discount",
                "end-to-end answer accuracy"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "6448cad23adf50d86406b0a3",
                "name": "tiiuae",
                "fullname": "Technology Innovation Institute",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
            }
        },
        "publishedAt": "2025-10-24T09:17:00.000Z",
        "title": "Redefining Retrieval Evaluation in the Era of LLMs",
        "summary": "Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR,\nassume that human users sequentially examine documents with diminishing\nattention to lower ranks. This assumption breaks down in Retrieval Augmented\nGeneration (RAG) systems, where search results are consumed by Large Language\nModels (LLMs), which, unlike humans, process all retrieved documents as a whole\nrather than sequentially. Additionally, traditional IR metrics do not account\nfor related but irrelevant documents that actively degrade generation quality,\nrather than merely being ignored. Due to these two major misalignments, namely\nhuman vs. machine position discount and human relevance vs. machine utility,\nclassical IR metrics do not accurately predict RAG performance. We introduce a\nutility-based annotation schema that quantifies both the positive contribution\nof relevant passages and the negative impact of distracting ones. Building on\nthis foundation, we propose UDCG (Utility and Distraction-aware Cumulative\nGain), a metric using an LLM-oriented positional discount to directly optimize\nthe correlation with the end-to-end answer accuracy. Experiments on five\ndatasets and six LLMs demonstrate that UDCG improves correlation by up to 36%\ncompared to traditional metrics. Our work provides a critical step toward\naligning IR evaluation with LLM consumers and enables more reliable assessment\nof RAG components",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21440.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "631e51736d6a5870f3d7d0fe",
            "avatarUrl": "/avatars/9eb93a278d01c3abd6dc28c708f0d107.svg",
            "fullname": "Giovanni Trappolini",
            "name": "rarinpear",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6448cad23adf50d86406b0a3",
            "name": "tiiuae",
            "fullname": "Technology Innovation Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.21111",
            "authors": [
                {
                    "_id": "68ff1bad6cdff8b857f4726a",
                    "user": {
                        "_id": "64c8fa690d3d1b209d2821a1",
                        "avatarUrl": "/avatars/1d8e8755384e17898bfda71137bcc91d.svg",
                        "isPro": false,
                        "fullname": "Jett",
                        "user": "JettZhou",
                        "type": "user"
                    },
                    "name": "Weijie Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:24:53.173Z",
                    "hidden": false
                },
                {
                    "_id": "68ff1bad6cdff8b857f4726b",
                    "name": "Xuantang Xiong",
                    "hidden": false
                },
                {
                    "_id": "68ff1bad6cdff8b857f4726c",
                    "name": "Yi Peng",
                    "hidden": false
                },
                {
                    "_id": "68ff1bad6cdff8b857f4726d",
                    "name": "Manli Tao",
                    "hidden": false
                },
                {
                    "_id": "68ff1bad6cdff8b857f4726e",
                    "name": "Chaoyang Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ff1bad6cdff8b857f4726f",
                    "name": "Honghui Dong",
                    "hidden": false
                },
                {
                    "_id": "68ff1bad6cdff8b857f47270",
                    "name": "Ming Tang",
                    "hidden": false
                },
                {
                    "_id": "68ff1bad6cdff8b857f47271",
                    "name": "Jinqiao Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-24T02:59:00.000Z",
            "submittedOnDailyAt": "2025-10-27T05:45:45.308Z",
            "title": "PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language\n  Models in Physical Environments",
            "submittedOnDailyBy": {
                "_id": "64c8fa690d3d1b209d2821a1",
                "avatarUrl": "/avatars/1d8e8755384e17898bfda71137bcc91d.svg",
                "isPro": false,
                "fullname": "Jett",
                "user": "JettZhou",
                "type": "user"
            },
            "summary": "Visual reasoning in multimodal large language models (MLLMs) has primarily\nbeen studied in static, fully observable settings, limiting their effectiveness\nin real-world environments where information is often incomplete due to\nocclusion or limited field of view. Humans, in contrast, actively explore and\ninteract with their environment-moving, examining, and manipulating objects-to\ngather information through a closed-loop process integrating perception,\nreasoning, and action. Inspired by this human capability, we introduce the\nActive Visual Reasoning (AVR) task, extending visual reasoning to partially\nobservable, interactive environments. AVR necessitates agents to: (1) actively\nacquire information via sequential physical actions, (2) integrate observations\nacross multiple steps for coherent reasoning, and (3) dynamically adjust\ndecisions based on evolving visual feedback. To rigorously evaluate AVR, we\nintroduce CLEVR-AVR, a simulation benchmark featuring multi-round interactive\nenvironments designed to assess both reasoning correctness and\ninformation-gathering efficiency. We present AVR-152k, a large-scale dataset\nthat offers rich Chain-of-Thought (CoT) annotations detailing iterative\nreasoning for uncertainty identification, action-conditioned information gain\nprediction, and information-maximizing action selection, crucial for training\nagents in a higher-order Markov Decision Process. Building on this, we develop\nPhysVLM-AVR, an MLLM achieving state-of-the-art performance on CLEVR-AVR,\nembodied reasoning (OpenEQA, RoboVQA), and passive visual reasoning (GeoMath,\nGeometry30K). Our analysis also reveals that current embodied MLLMs, despite\ndetecting information incompleteness, struggle to actively acquire and\nintegrate new information through interaction, highlighting a fundamental gap\nin active reasoning capabilities.",
            "upvotes": 1,
            "discussionId": "68ff1bae6cdff8b857f47272",
            "ai_summary": "Active Visual Reasoning (AVR) extends visual reasoning to interactive, partially observable environments, requiring agents to sequentially gather and integrate information for coherent decision-making, as evaluated by the CLEVR-AVR benchmark.",
            "ai_keywords": [
                "Active Visual Reasoning",
                "AVR",
                "CLEVR-AVR",
                "Chain-of-Thought",
                "CoT",
                "embodied reasoning",
                "OpenEQA",
                "RoboVQA",
                "passive visual reasoning",
                "GeoMath",
                "Geometry30K",
                "Markov Decision Process"
            ]
        },
        "publishedAt": "2025-10-23T22:59:00.000Z",
        "title": "PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language\n  Models in Physical Environments",
        "summary": "Visual reasoning in multimodal large language models (MLLMs) has primarily\nbeen studied in static, fully observable settings, limiting their effectiveness\nin real-world environments where information is often incomplete due to\nocclusion or limited field of view. Humans, in contrast, actively explore and\ninteract with their environment-moving, examining, and manipulating objects-to\ngather information through a closed-loop process integrating perception,\nreasoning, and action. Inspired by this human capability, we introduce the\nActive Visual Reasoning (AVR) task, extending visual reasoning to partially\nobservable, interactive environments. AVR necessitates agents to: (1) actively\nacquire information via sequential physical actions, (2) integrate observations\nacross multiple steps for coherent reasoning, and (3) dynamically adjust\ndecisions based on evolving visual feedback. To rigorously evaluate AVR, we\nintroduce CLEVR-AVR, a simulation benchmark featuring multi-round interactive\nenvironments designed to assess both reasoning correctness and\ninformation-gathering efficiency. We present AVR-152k, a large-scale dataset\nthat offers rich Chain-of-Thought (CoT) annotations detailing iterative\nreasoning for uncertainty identification, action-conditioned information gain\nprediction, and information-maximizing action selection, crucial for training\nagents in a higher-order Markov Decision Process. Building on this, we develop\nPhysVLM-AVR, an MLLM achieving state-of-the-art performance on CLEVR-AVR,\nembodied reasoning (OpenEQA, RoboVQA), and passive visual reasoning (GeoMath,\nGeometry30K). Our analysis also reveals that current embodied MLLMs, despite\ndetecting information incompleteness, struggle to actively acquire and\nintegrate new information through interaction, highlighting a fundamental gap\nin active reasoning capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21111.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64c8fa690d3d1b209d2821a1",
            "avatarUrl": "/avatars/1d8e8755384e17898bfda71137bcc91d.svg",
            "fullname": "Jett",
            "name": "JettZhou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.20708",
            "authors": [
                {
                    "_id": "68fb75884799a63578375e0b",
                    "user": {
                        "_id": "68fb75566585cd537eb0981c",
                        "avatarUrl": "/avatars/6960280563c3b9d4e77aa82f25cfd1fa.svg",
                        "isPro": false,
                        "fullname": "Samuel Soutullo",
                        "user": "ssoutullo",
                        "type": "user"
                    },
                    "name": "Samuel Soutullo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-24T15:57:40.348Z",
                    "hidden": false
                },
                {
                    "_id": "68fb75884799a63578375e0c",
                    "name": "Miguel Yermo",
                    "hidden": false
                },
                {
                    "_id": "68fb75884799a63578375e0d",
                    "name": "David L. Vilario",
                    "hidden": false
                },
                {
                    "_id": "68fb75884799a63578375e0e",
                    "name": "scar G. Lorenzo",
                    "hidden": false
                },
                {
                    "_id": "68fb75884799a63578375e0f",
                    "name": "Jos C. Cabaleiro",
                    "hidden": false
                },
                {
                    "_id": "68fb75884799a63578375e10",
                    "name": "Francisco F. Rivera",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T16:22:58.000Z",
            "submittedOnDailyAt": "2025-10-27T11:15:43.778Z",
            "title": "ALICE-LRI: A General Method for Lossless Range Image Generation for\n  Spinning LiDAR Sensors without Calibration Metadata",
            "submittedOnDailyBy": {
                "_id": "68fb75566585cd537eb0981c",
                "avatarUrl": "/avatars/6960280563c3b9d4e77aa82f25cfd1fa.svg",
                "isPro": false,
                "fullname": "Samuel Soutullo",
                "user": "ssoutullo",
                "type": "user"
            },
            "summary": "3D LiDAR sensors are essential for autonomous navigation, environmental\nmonitoring, and precision mapping in remote sensing applications. To\nefficiently process the massive point clouds generated by these sensors, LiDAR\ndata is often projected into 2D range images that organize points by their\nangular positions and distances. While these range image representations enable\nefficient processing, conventional projection methods suffer from fundamental\ngeometric inconsistencies that cause irreversible information loss,\ncompromising high-fidelity applications. We present ALICE-LRI (Automatic LiDAR\nIntrinsic Calibration Estimation for Lossless Range Images), the first general,\nsensor-agnostic method that achieves lossless range image generation from\nspinning LiDAR point clouds without requiring manufacturer metadata or\ncalibration files. Our algorithm automatically reverse-engineers the intrinsic\ngeometry of any spinning LiDAR sensor by inferring critical parameters\nincluding laser beam configuration, angular distributions, and per-beam\ncalibration corrections, enabling lossless projection and complete point cloud\nreconstruction with zero point loss. Comprehensive evaluation across the\ncomplete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfect\npoint preservation, with zero points lost across all point clouds. Geometric\naccuracy is maintained well within sensor precision limits, establishing\ngeometric losslessness with real-time performance. We also present a\ncompression case study that validates substantial downstream benefits,\ndemonstrating significant quality improvements in practical applications. This\nparadigm shift from approximate to lossless LiDAR projections opens new\npossibilities for high-precision remote sensing applications requiring complete\ngeometric preservation.",
            "upvotes": 1,
            "discussionId": "68fb75884799a63578375e11",
            "projectPage": "https://alice-lri.github.io/alice-lri/",
            "githubRepo": "https://github.com/alice-lri/alice-lri",
            "ai_summary": "ALICE-LRI is a sensor-agnostic method that achieves lossless range image generation from spinning LiDAR point clouds, preserving all points and maintaining geometric accuracy in real-time.",
            "ai_keywords": [
                "LiDAR",
                "point clouds",
                "range images",
                "intrinsic calibration",
                "laser beam configuration",
                "angular distributions",
                "per-beam calibration corrections",
                "KITTI",
                "DurLAR",
                "geometric accuracy",
                "real-time performance"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-10-23T12:22:58.000Z",
        "title": "ALICE-LRI: A General Method for Lossless Range Image Generation for\n  Spinning LiDAR Sensors without Calibration Metadata",
        "summary": "3D LiDAR sensors are essential for autonomous navigation, environmental\nmonitoring, and precision mapping in remote sensing applications. To\nefficiently process the massive point clouds generated by these sensors, LiDAR\ndata is often projected into 2D range images that organize points by their\nangular positions and distances. While these range image representations enable\nefficient processing, conventional projection methods suffer from fundamental\ngeometric inconsistencies that cause irreversible information loss,\ncompromising high-fidelity applications. We present ALICE-LRI (Automatic LiDAR\nIntrinsic Calibration Estimation for Lossless Range Images), the first general,\nsensor-agnostic method that achieves lossless range image generation from\nspinning LiDAR point clouds without requiring manufacturer metadata or\ncalibration files. Our algorithm automatically reverse-engineers the intrinsic\ngeometry of any spinning LiDAR sensor by inferring critical parameters\nincluding laser beam configuration, angular distributions, and per-beam\ncalibration corrections, enabling lossless projection and complete point cloud\nreconstruction with zero point loss. Comprehensive evaluation across the\ncomplete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfect\npoint preservation, with zero points lost across all point clouds. Geometric\naccuracy is maintained well within sensor precision limits, establishing\ngeometric losslessness with real-time performance. We also present a\ncompression case study that validates substantial downstream benefits,\ndemonstrating significant quality improvements in practical applications. This\nparadigm shift from approximate to lossless LiDAR projections opens new\npossibilities for high-precision remote sensing applications requiring complete\ngeometric preservation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20708.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "68fb75566585cd537eb0981c",
            "avatarUrl": "/avatars/6960280563c3b9d4e77aa82f25cfd1fa.svg",
            "fullname": "Samuel Soutullo",
            "name": "ssoutullo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]