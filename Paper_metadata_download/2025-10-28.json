[
    {
        "paper": {
            "id": "2510.23607",
            "authors": [
                {
                    "_id": "690034a222d452aac6dd4346",
                    "name": "Yujia Zhang",
                    "hidden": false
                },
                {
                    "_id": "690034a222d452aac6dd4347",
                    "user": {
                        "_id": "643e5d6a1d0e956d94bb3608",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643e5d6a1d0e956d94bb3608/mTnu7Dts2RmDklHlp9Gqu.jpeg",
                        "isPro": false,
                        "fullname": "Xiaoyang Wu",
                        "user": "Gofinge",
                        "type": "user"
                    },
                    "name": "Xiaoyang Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:29.001Z",
                    "hidden": false
                },
                {
                    "_id": "690034a222d452aac6dd4348",
                    "name": "Yixing Lao",
                    "hidden": false
                },
                {
                    "_id": "690034a222d452aac6dd4349",
                    "user": {
                        "_id": "6423e35b30b0e4ab36dd1b16",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6423e35b30b0e4ab36dd1b16/pea6LVDS9PQAxvQt9GUiZ.jpeg",
                        "isPro": false,
                        "fullname": "Wang Chengyao",
                        "user": "wcy1122",
                        "type": "user"
                    },
                    "name": "Chengyao Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:26.782Z",
                    "hidden": false
                },
                {
                    "_id": "690034a222d452aac6dd434a",
                    "name": "Zhuotao Tian",
                    "hidden": false
                },
                {
                    "_id": "690034a222d452aac6dd434b",
                    "name": "Naiyan Wang",
                    "hidden": false
                },
                {
                    "_id": "690034a222d452aac6dd434c",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T17:59:59.000Z",
            "submittedOnDailyAt": "2025-10-28T01:46:45.129Z",
            "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial\n  Representations",
            "submittedOnDailyBy": {
                "_id": "643e5d6a1d0e956d94bb3608",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643e5d6a1d0e956d94bb3608/mTnu7Dts2RmDklHlp9Gqu.jpeg",
                "isPro": false,
                "fullname": "Xiaoyang Wu",
                "user": "Gofinge",
                "type": "user"
            },
            "summary": "Humans learn abstract concepts through multisensory synergy, and once formed,\nsuch representations can often be recalled from a single modality. Inspired by\nthis principle, we introduce Concerto, a minimalist simulation of human concept\nlearning for spatial cognition, combining 3D intra-modal self-distillation with\n2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more\ncoherent and informative spatial features, as demonstrated by zero-shot\nvisualizations. It outperforms both standalone SOTA 2D and 3D self-supervised\nmodels by 14.2% and 4.8%, respectively, as well as their feature concatenation,\nin linear probing for 3D scene perception. With full fine-tuning, Concerto sets\nnew SOTA results across multiple scene understanding benchmarks (e.g., 80.7%\nmIoU on ScanNet). We further present a variant of Concerto tailored for\nvideo-lifted point cloud spatial understanding, and a translator that linearly\nprojects Concerto representations into CLIP's language space, enabling\nopen-world perception. These results highlight that Concerto emerges spatial\nrepresentations with superior fine-grained geometric and semantic consistency.",
            "upvotes": 147,
            "discussionId": "690034a222d452aac6dd434d",
            "projectPage": "https://pointcept.github.io/Concerto/",
            "githubRepo": "https://github.com/Pointcept/Pointcept",
            "ai_summary": "Concerto, a minimalist model combining 3D self-distillation and 2D-3D joint embedding, achieves superior spatial feature learning and outperforms existing models in scene understanding and open-world perception.",
            "ai_keywords": [
                "3D intra-modal self-distillation",
                "2D-3D cross-modal joint embedding",
                "zero-shot visualizations",
                "linear probing",
                "3D scene perception",
                "ScanNet",
                "mIoU",
                "video-lifted point cloud",
                "CLIP's language space",
                "open-world perception",
                "fine-grained geometric and semantic consistency"
            ],
            "githubStars": 2566,
            "organization": {
                "_id": "643e5de3c0ed86c416e8eb25",
                "name": "Pointcept",
                "fullname": "Pointcept",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643e5d6a1d0e956d94bb3608/dtbQ44h-xIjwQlHT9Nbbv.png"
            }
        },
        "publishedAt": "2025-10-27T13:59:59.000Z",
        "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial\n  Representations",
        "summary": "Humans learn abstract concepts through multisensory synergy, and once formed,\nsuch representations can often be recalled from a single modality. Inspired by\nthis principle, we introduce Concerto, a minimalist simulation of human concept\nlearning for spatial cognition, combining 3D intra-modal self-distillation with\n2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more\ncoherent and informative spatial features, as demonstrated by zero-shot\nvisualizations. It outperforms both standalone SOTA 2D and 3D self-supervised\nmodels by 14.2% and 4.8%, respectively, as well as their feature concatenation,\nin linear probing for 3D scene perception. With full fine-tuning, Concerto sets\nnew SOTA results across multiple scene understanding benchmarks (e.g., 80.7%\nmIoU on ScanNet). We further present a variant of Concerto tailored for\nvideo-lifted point cloud spatial understanding, and a translator that linearly\nprojects Concerto representations into CLIP's language space, enabling\nopen-world perception. These results highlight that Concerto emerges spatial\nrepresentations with superior fine-grained geometric and semantic consistency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23607.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643e5d6a1d0e956d94bb3608",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643e5d6a1d0e956d94bb3608/mTnu7Dts2RmDklHlp9Gqu.jpeg",
            "fullname": "Xiaoyang Wu",
            "name": "Gofinge",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "643e5de3c0ed86c416e8eb25",
            "name": "Pointcept",
            "fullname": "Pointcept",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643e5d6a1d0e956d94bb3608/dtbQ44h-xIjwQlHT9Nbbv.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.23564",
            "authors": [
                {
                    "_id": "69003a0022d452aac6dd438f",
                    "name": "Zhaoyang Yu",
                    "hidden": false
                },
                {
                    "_id": "69003a0022d452aac6dd4390",
                    "user": {
                        "_id": "65f40e83653c231cbaf7defe",
                        "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg",
                        "isPro": false,
                        "fullname": "Jiayi Zhang",
                        "user": "didiforhugface",
                        "type": "user"
                    },
                    "name": "Jiayi Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:18.381Z",
                    "hidden": false
                },
                {
                    "_id": "69003a0022d452aac6dd4391",
                    "name": "Huixue Su",
                    "hidden": false
                },
                {
                    "_id": "69003a0022d452aac6dd4392",
                    "name": "Yufan Zhao",
                    "hidden": false
                },
                {
                    "_id": "69003a0022d452aac6dd4393",
                    "name": "Yifan Wu",
                    "hidden": false
                },
                {
                    "_id": "69003a0022d452aac6dd4394",
                    "name": "Mingyi Deng",
                    "hidden": false
                },
                {
                    "_id": "69003a0022d452aac6dd4395",
                    "name": "Jinyu Xiang",
                    "hidden": false
                },
                {
                    "_id": "69003a0022d452aac6dd4396",
                    "name": "Yizhang Lin",
                    "hidden": false
                },
                {
                    "_id": "69003a0022d452aac6dd4397",
                    "name": "Lingxiao Tang",
                    "hidden": false
                },
                {
                    "_id": "69003a0022d452aac6dd4398",
                    "user": {
                        "_id": "6742e356f8883755e01c6053",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6742e356f8883755e01c6053/11gforECOJfeY5_O9ERuO.png",
                        "isPro": false,
                        "fullname": "Yingchao YL Li",
                        "user": "yclee0716",
                        "type": "user"
                    },
                    "name": "Yingchao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:16.025Z",
                    "hidden": false
                },
                {
                    "_id": "69003a0022d452aac6dd4399",
                    "name": "Yuyu Luo",
                    "hidden": false
                },
                {
                    "_id": "69003a0022d452aac6dd439a",
                    "name": "Bang Liu",
                    "hidden": false
                },
                {
                    "_id": "69003a0022d452aac6dd439b",
                    "name": "Chenglin Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T17:35:15.000Z",
            "submittedOnDailyAt": "2025-10-28T02:26:28.802Z",
            "title": "ReCode: Unify Plan and Action for Universal Granularity Control",
            "submittedOnDailyBy": {
                "_id": "640dc84b474aa6f89554d518",
                "avatarUrl": "/avatars/9fcee1023ed5c6cddb3c342e19f18295.svg",
                "isPro": false,
                "fullname": "Zhaoyang Yu",
                "user": "MoshiQAQ",
                "type": "user"
            },
            "summary": "Real-world tasks require decisions at varying granularities, and humans excel\nat this by leveraging a unified cognitive representation where planning is\nfundamentally understood as a high-level form of action. However, current Large\nLanguage Model (LLM)-based agents lack this crucial capability to operate\nfluidly across decision granularities. This limitation stems from existing\nparadigms that enforce a rigid separation between high-level planning and\nlow-level action, which impairs dynamic adaptability and limits generalization.\nWe propose ReCode (Recursive Code Generation), a novel paradigm that addresses\nthis limitation by unifying planning and action within a single code\nrepresentation. In this representation, ReCode treats high-level plans as\nabstract placeholder functions, which the agent then recursively decomposes\ninto finer-grained sub-functions until reaching primitive actions. This\nrecursive approach dissolves the rigid boundary between plan and action,\nenabling the agent to dynamically control its decision granularity.\nFurthermore, the recursive structure inherently generates rich,\nmulti-granularity training data, enabling models to learn hierarchical\ndecision-making processes. Extensive experiments show ReCode significantly\nsurpasses advanced baselines in inference performance and demonstrates\nexceptional data efficiency in training, validating our core insight that\nunifying planning and action through recursive code generation is a powerful\nand effective approach to achieving universal granularity control. The code is\navailable at https://github.com/FoundationAgents/ReCode.",
            "upvotes": 100,
            "discussionId": "69003a0122d452aac6dd439c",
            "githubRepo": "https://github.com/FoundationAgents/ReCode",
            "ai_summary": "ReCode, a recursive code generation paradigm, unifies high-level planning and low-level action in a single representation, enhancing decision granularity and data efficiency in LLM-based agents.",
            "ai_keywords": [
                "Large Language Model",
                "ReCode",
                "Recursive Code Generation",
                "high-level planning",
                "low-level action",
                "decision granularity",
                "abstract placeholder functions",
                "primitive actions",
                "hierarchical decision-making processes",
                "inference performance",
                "data efficiency"
            ],
            "githubStars": 19
        },
        "publishedAt": "2025-10-27T13:35:15.000Z",
        "title": "ReCode: Unify Plan and Action for Universal Granularity Control",
        "summary": "Real-world tasks require decisions at varying granularities, and humans excel\nat this by leveraging a unified cognitive representation where planning is\nfundamentally understood as a high-level form of action. However, current Large\nLanguage Model (LLM)-based agents lack this crucial capability to operate\nfluidly across decision granularities. This limitation stems from existing\nparadigms that enforce a rigid separation between high-level planning and\nlow-level action, which impairs dynamic adaptability and limits generalization.\nWe propose ReCode (Recursive Code Generation), a novel paradigm that addresses\nthis limitation by unifying planning and action within a single code\nrepresentation. In this representation, ReCode treats high-level plans as\nabstract placeholder functions, which the agent then recursively decomposes\ninto finer-grained sub-functions until reaching primitive actions. This\nrecursive approach dissolves the rigid boundary between plan and action,\nenabling the agent to dynamically control its decision granularity.\nFurthermore, the recursive structure inherently generates rich,\nmulti-granularity training data, enabling models to learn hierarchical\ndecision-making processes. Extensive experiments show ReCode significantly\nsurpasses advanced baselines in inference performance and demonstrates\nexceptional data efficiency in training, validating our core insight that\nunifying planning and action through recursive code generation is a powerful\nand effective approach to achieving universal granularity control. The code is\navailable at https://github.com/FoundationAgents/ReCode.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23564.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "640dc84b474aa6f89554d518",
            "avatarUrl": "/avatars/9fcee1023ed5c6cddb3c342e19f18295.svg",
            "fullname": "Zhaoyang Yu",
            "name": "MoshiQAQ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.23587",
            "authors": [
                {
                    "_id": "690065ad22d452aac6dd4454",
                    "name": "Yizhang Zhu",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd4455",
                    "name": "Liangwei Wang",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd4456",
                    "name": "Chenyu Yang",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd4457",
                    "name": "Xiaotian Lin",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd4458",
                    "name": "Boyan Li",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd4459",
                    "name": "Wei Zhou",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd445a",
                    "name": "Xinyu Liu",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd445b",
                    "name": "Zhangyang Peng",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd445c",
                    "name": "Tianqi Luo",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd445d",
                    "name": "Yu Li",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd445e",
                    "name": "Chengliang Chai",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd445f",
                    "name": "Chong Chen",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd4460",
                    "name": "Shimin Di",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd4461",
                    "name": "Ju Fan",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd4462",
                    "name": "Ji Sun",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd4463",
                    "name": "Nan Tang",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd4464",
                    "name": "Fugee Tsung",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd4465",
                    "name": "Jiannan Wang",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd4466",
                    "name": "Chenglin Wu",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd4467",
                    "name": "Yanwei Xu",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd4468",
                    "name": "Shaolei Zhang",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd4469",
                    "name": "Yong Zhang",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd446a",
                    "name": "Xuanhe Zhou",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd446b",
                    "name": "Guoliang Li",
                    "hidden": false
                },
                {
                    "_id": "690065ad22d452aac6dd446c",
                    "name": "Yuyu Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T17:54:07.000Z",
            "submittedOnDailyAt": "2025-10-28T05:18:46.268Z",
            "title": "A Survey of Data Agents: Emerging Paradigm or Overstated Hype?",
            "submittedOnDailyBy": {
                "_id": "65dd77bfcb021a4a9ebdc62f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65dd77bfcb021a4a9ebdc62f/o58j3T670xByIjJhnNLj9.png",
                "isPro": false,
                "fullname": "Derrick ZHU",
                "user": "derrickzhu",
                "type": "user"
            },
            "summary": "The rapid advancement of large language models (LLMs) has spurred the\nemergence of data agents--autonomous systems designed to orchestrate Data + AI\necosystems for tackling complex data-related tasks. However, the term \"data\nagent\" currently suffers from terminological ambiguity and inconsistent\nadoption, conflating simple query responders with sophisticated autonomous\narchitectures. This terminological ambiguity fosters mismatched user\nexpectations, accountability challenges, and barriers to industry growth.\nInspired by the SAE J3016 standard for driving automation, this survey\nintroduces the first systematic hierarchical taxonomy for data agents,\ncomprising six levels that delineate and trace progressive shifts in autonomy,\nfrom manual operations (L0) to a vision of generative, fully autonomous data\nagents (L5), thereby clarifying capability boundaries and responsibility\nallocation. Through this lens, we offer a structured review of existing\nresearch arranged by increasing autonomy, encompassing specialized data agents\nfor data management, preparation, and analysis, alongside emerging efforts\ntoward versatile, comprehensive systems with enhanced autonomy. We further\nanalyze critical evolutionary leaps and technical gaps for advancing data\nagents, especially the ongoing L2-to-L3 transition, where data agents evolve\nfrom procedural execution to autonomous orchestration. Finally, we conclude\nwith a forward-looking roadmap, envisioning the advent of proactive, generative\ndata agents.",
            "upvotes": 54,
            "discussionId": "690065ae22d452aac6dd446d",
            "githubRepo": "https://github.com/HKUSTDial/awesome-data-agents",
            "ai_summary": "A systematic taxonomy for data agents is introduced to clarify their autonomy levels and capabilities, addressing terminological ambiguity and guiding future research and development.",
            "ai_keywords": [
                "large language models",
                "data agents",
                "SAE J3016 standard",
                "driving automation",
                "hierarchical taxonomy",
                "autonomy levels",
                "data management",
                "data preparation",
                "data analysis",
                "generative data agents"
            ],
            "githubStars": 80
        },
        "publishedAt": "2025-10-27T13:54:07.000Z",
        "title": "A Survey of Data Agents: Emerging Paradigm or Overstated Hype?",
        "summary": "The rapid advancement of large language models (LLMs) has spurred the\nemergence of data agents--autonomous systems designed to orchestrate Data + AI\necosystems for tackling complex data-related tasks. However, the term \"data\nagent\" currently suffers from terminological ambiguity and inconsistent\nadoption, conflating simple query responders with sophisticated autonomous\narchitectures. This terminological ambiguity fosters mismatched user\nexpectations, accountability challenges, and barriers to industry growth.\nInspired by the SAE J3016 standard for driving automation, this survey\nintroduces the first systematic hierarchical taxonomy for data agents,\ncomprising six levels that delineate and trace progressive shifts in autonomy,\nfrom manual operations (L0) to a vision of generative, fully autonomous data\nagents (L5), thereby clarifying capability boundaries and responsibility\nallocation. Through this lens, we offer a structured review of existing\nresearch arranged by increasing autonomy, encompassing specialized data agents\nfor data management, preparation, and analysis, alongside emerging efforts\ntoward versatile, comprehensive systems with enhanced autonomy. We further\nanalyze critical evolutionary leaps and technical gaps for advancing data\nagents, especially the ongoing L2-to-L3 transition, where data agents evolve\nfrom procedural execution to autonomous orchestration. Finally, we conclude\nwith a forward-looking roadmap, envisioning the advent of proactive, generative\ndata agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23587.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65dd77bfcb021a4a9ebdc62f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65dd77bfcb021a4a9ebdc62f/o58j3T670xByIjJhnNLj9.png",
            "fullname": "Derrick ZHU",
            "name": "derrickzhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.23588",
            "authors": [
                {
                    "_id": "6900348b22d452aac6dd433b",
                    "name": "Guangting Zheng",
                    "hidden": false
                },
                {
                    "_id": "6900348b22d452aac6dd433c",
                    "name": "Qinyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "6900348b22d452aac6dd433d",
                    "name": "Tao Yang",
                    "hidden": false
                },
                {
                    "_id": "6900348b22d452aac6dd433e",
                    "name": "Fei Xiao",
                    "hidden": false
                },
                {
                    "_id": "6900348b22d452aac6dd433f",
                    "name": "Zhijie Lin",
                    "hidden": false
                },
                {
                    "_id": "6900348b22d452aac6dd4340",
                    "user": {
                        "_id": "6381c5d63680a7cf34e08ca9",
                        "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
                        "isPro": false,
                        "fullname": "wujie10558@gmail.com",
                        "user": "wujie10",
                        "type": "user"
                    },
                    "name": "Jie Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:31.445Z",
                    "hidden": false
                },
                {
                    "_id": "6900348b22d452aac6dd4341",
                    "name": "Jiajun Deng",
                    "hidden": false
                },
                {
                    "_id": "6900348b22d452aac6dd4342",
                    "name": "Yanyong Zhang",
                    "hidden": false
                },
                {
                    "_id": "6900348b22d452aac6dd4343",
                    "name": "Rui Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T17:54:08.000Z",
            "submittedOnDailyAt": "2025-10-28T02:09:47.863Z",
            "title": "FARMER: Flow AutoRegressive Transformer over Pixels",
            "submittedOnDailyBy": {
                "_id": "6381c5d63680a7cf34e08ca9",
                "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
                "isPro": false,
                "fullname": "wujie10558@gmail.com",
                "user": "wujie10",
                "type": "user"
            },
            "summary": "Directly modeling the explicit likelihood of the raw data distribution is key\ntopic in the machine learning area, which achieves the scaling successes in\nLarge Language Models by autoregressive modeling. However, continuous AR\nmodeling over visual pixel data suffer from extremely long sequences and\nhigh-dimensional spaces. In this paper, we present FARMER, a novel end-to-end\ngenerative framework that unifies Normalizing Flows (NF) and Autoregressive\n(AR) models for tractable likelihood estimation and high-quality image\nsynthesis directly from raw pixels. FARMER employs an invertible autoregressive\nflow to transform images into latent sequences, whose distribution is modeled\nimplicitly by an autoregressive model. To address the redundancy and complexity\nin pixel-level modeling, we propose a self-supervised dimension reduction\nscheme that partitions NF latent channels into informative and redundant\ngroups, enabling more effective and efficient AR modeling. Furthermore, we\ndesign a one-step distillation scheme to significantly accelerate inference\nspeed and introduce a resampling-based classifier-free guidance algorithm to\nboost image generation quality. Extensive experiments demonstrate that FARMER\nachieves competitive performance compared to existing pixel-based generative\nmodels while providing exact likelihoods and scalable training.",
            "upvotes": 45,
            "discussionId": "6900348b22d452aac6dd4344",
            "ai_summary": "FARMER, a generative framework combining Normalizing Flows and Autoregressive models, achieves competitive image synthesis from raw pixels with exact likelihoods and scalable training.",
            "ai_keywords": [
                "Normalizing Flows",
                "Autoregressive models",
                "invertible autoregressive flow",
                "self-supervised dimension reduction",
                "one-step distillation",
                "resampling-based classifier-free guidance"
            ]
        },
        "publishedAt": "2025-10-27T13:54:08.000Z",
        "title": "FARMER: Flow AutoRegressive Transformer over Pixels",
        "summary": "Directly modeling the explicit likelihood of the raw data distribution is key\ntopic in the machine learning area, which achieves the scaling successes in\nLarge Language Models by autoregressive modeling. However, continuous AR\nmodeling over visual pixel data suffer from extremely long sequences and\nhigh-dimensional spaces. In this paper, we present FARMER, a novel end-to-end\ngenerative framework that unifies Normalizing Flows (NF) and Autoregressive\n(AR) models for tractable likelihood estimation and high-quality image\nsynthesis directly from raw pixels. FARMER employs an invertible autoregressive\nflow to transform images into latent sequences, whose distribution is modeled\nimplicitly by an autoregressive model. To address the redundancy and complexity\nin pixel-level modeling, we propose a self-supervised dimension reduction\nscheme that partitions NF latent channels into informative and redundant\ngroups, enabling more effective and efficient AR modeling. Furthermore, we\ndesign a one-step distillation scheme to significantly accelerate inference\nspeed and introduce a resampling-based classifier-free guidance algorithm to\nboost image generation quality. Extensive experiments demonstrate that FARMER\nachieves competitive performance compared to existing pixel-based generative\nmodels while providing exact likelihoods and scalable training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23588.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6381c5d63680a7cf34e08ca9",
            "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
            "fullname": "wujie10558@gmail.com",
            "name": "wujie10",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.23581",
            "authors": [
                {
                    "_id": "690043b122d452aac6dd43db",
                    "name": "Junyoung Seo",
                    "hidden": false
                },
                {
                    "_id": "690043b122d452aac6dd43dc",
                    "name": "Rodrigo Mira",
                    "hidden": false
                },
                {
                    "_id": "690043b122d452aac6dd43dd",
                    "name": "Alexandros Haliassos",
                    "hidden": false
                },
                {
                    "_id": "690043b122d452aac6dd43de",
                    "name": "Stella Bounareli",
                    "hidden": false
                },
                {
                    "_id": "690043b122d452aac6dd43df",
                    "name": "Honglie Chen",
                    "hidden": false
                },
                {
                    "_id": "690043b122d452aac6dd43e0",
                    "name": "Linh Tran",
                    "hidden": false
                },
                {
                    "_id": "690043b122d452aac6dd43e1",
                    "name": "Seungryong Kim",
                    "hidden": false
                },
                {
                    "_id": "690043b122d452aac6dd43e2",
                    "name": "Zoe Landgraf",
                    "hidden": false
                },
                {
                    "_id": "690043b122d452aac6dd43e3",
                    "name": "Jie Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T17:50:19.000Z",
            "submittedOnDailyAt": "2025-10-28T02:48:23.987Z",
            "title": "Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human\n  Animation",
            "submittedOnDailyBy": {
                "_id": "635bcc3086f52b24c1e6422d",
                "avatarUrl": "/avatars/078710e49e849e2d890b4496bdc72d0d.svg",
                "isPro": true,
                "fullname": "Junyoung Seo",
                "user": "jyseo",
                "type": "user"
            },
            "summary": "Audio-driven human animation models often suffer from identity drift during\ntemporal autoregressive generation, where characters gradually lose their\nidentity over time. One solution is to generate keyframes as intermediate\ntemporal anchors that prevent degradation, but this requires an additional\nkeyframe generation stage and can restrict natural motion dynamics. To address\nthis, we propose Lookahead Anchoring, which leverages keyframes from future\ntimesteps ahead of the current generation window, rather than within it. This\ntransforms keyframes from fixed boundaries into directional beacons: the model\ncontinuously pursues these future anchors while responding to immediate audio\ncues, maintaining consistent identity through persistent guidance. This also\nenables self-keyframing, where the reference image serves as the lookahead\ntarget, eliminating the need for keyframe generation entirely. We find that the\ntemporal lookahead distance naturally controls the balance between expressivity\nand consistency: larger distances allow for greater motion freedom, while\nsmaller ones strengthen identity adherence. When applied to three recent human\nanimation models, Lookahead Anchoring achieves superior lip synchronization,\nidentity preservation, and visual quality, demonstrating improved temporal\nconditioning across several different architectures. Video results are\navailable at the following link: https://lookahead-anchoring.github.io.",
            "upvotes": 40,
            "discussionId": "690043b222d452aac6dd43e4",
            "projectPage": "https://lookahead-anchoring.github.io/",
            "ai_summary": "Lookahead Anchoring improves audio-driven human animation by using future keyframes as dynamic guides, enhancing lip synchronization, identity preservation, and visual quality.",
            "ai_keywords": [
                "temporal autoregressive generation",
                "identity drift",
                "keyframes",
                "lookahead anchoring",
                "directional beacons",
                "self-keyframing",
                "temporal lookahead distance",
                "temporal conditioning"
            ]
        },
        "publishedAt": "2025-10-27T13:50:19.000Z",
        "title": "Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human\n  Animation",
        "summary": "Audio-driven human animation models often suffer from identity drift during\ntemporal autoregressive generation, where characters gradually lose their\nidentity over time. One solution is to generate keyframes as intermediate\ntemporal anchors that prevent degradation, but this requires an additional\nkeyframe generation stage and can restrict natural motion dynamics. To address\nthis, we propose Lookahead Anchoring, which leverages keyframes from future\ntimesteps ahead of the current generation window, rather than within it. This\ntransforms keyframes from fixed boundaries into directional beacons: the model\ncontinuously pursues these future anchors while responding to immediate audio\ncues, maintaining consistent identity through persistent guidance. This also\nenables self-keyframing, where the reference image serves as the lookahead\ntarget, eliminating the need for keyframe generation entirely. We find that the\ntemporal lookahead distance naturally controls the balance between expressivity\nand consistency: larger distances allow for greater motion freedom, while\nsmaller ones strengthen identity adherence. When applied to three recent human\nanimation models, Lookahead Anchoring achieves superior lip synchronization,\nidentity preservation, and visual quality, demonstrating improved temporal\nconditioning across several different architectures. Video results are\navailable at the following link: https://lookahead-anchoring.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23581.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "635bcc3086f52b24c1e6422d",
            "avatarUrl": "/avatars/078710e49e849e2d890b4496bdc72d0d.svg",
            "fullname": "Junyoung Seo",
            "name": "jyseo",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.21817",
            "authors": [
                {
                    "_id": "69002d2622d452aac6dd42d4",
                    "name": "Xiaoyu Liu",
                    "hidden": false
                },
                {
                    "_id": "69002d2622d452aac6dd42d5",
                    "name": "Chaoyou Fu",
                    "hidden": false
                },
                {
                    "_id": "69002d2622d452aac6dd42d6",
                    "name": "Chi Yan",
                    "hidden": false
                },
                {
                    "_id": "69002d2622d452aac6dd42d7",
                    "name": "Chu Wu",
                    "hidden": false
                },
                {
                    "_id": "69002d2622d452aac6dd42d8",
                    "name": "Haihan Gao",
                    "hidden": false
                },
                {
                    "_id": "69002d2622d452aac6dd42d9",
                    "name": "Yi-Fan Zhang",
                    "hidden": false
                },
                {
                    "_id": "69002d2622d452aac6dd42da",
                    "name": "Shaoqi Dong",
                    "hidden": false
                },
                {
                    "_id": "69002d2622d452aac6dd42db",
                    "name": "Cheng Qian",
                    "hidden": false
                },
                {
                    "_id": "69002d2622d452aac6dd42dc",
                    "name": "Bin Luo",
                    "hidden": false
                },
                {
                    "_id": "69002d2622d452aac6dd42dd",
                    "name": "Xiuyong Yang",
                    "hidden": false
                },
                {
                    "_id": "69002d2622d452aac6dd42de",
                    "name": "Guanwu Li",
                    "hidden": false
                },
                {
                    "_id": "69002d2622d452aac6dd42df",
                    "name": "Yusheng Cai",
                    "hidden": false
                },
                {
                    "_id": "69002d2622d452aac6dd42e0",
                    "name": "Yunhang Shen",
                    "hidden": false
                },
                {
                    "_id": "69002d2622d452aac6dd42e1",
                    "name": "Deqiang Jiang",
                    "hidden": false
                },
                {
                    "_id": "69002d2622d452aac6dd42e2",
                    "name": "Haoyu Cao",
                    "hidden": false
                },
                {
                    "_id": "69002d2622d452aac6dd42e3",
                    "user": {
                        "_id": "647401e50da364bd0d002f2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png",
                        "isPro": false,
                        "fullname": "XING SUN",
                        "user": "tedsun",
                        "type": "user"
                    },
                    "name": "Xing Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:42.216Z",
                    "hidden": false
                },
                {
                    "_id": "69002d2622d452aac6dd42e4",
                    "name": "Caifeng Shan",
                    "hidden": false
                },
                {
                    "_id": "69002d2622d452aac6dd42e5",
                    "name": "Ran He",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/623d8ca4c29adf5ef6175615/YqWLnrC3Cf4cOxO20icQF.png"
            ],
            "publishedAt": "2025-10-21T17:59:56.000Z",
            "submittedOnDailyAt": "2025-10-28T01:13:48.742Z",
            "title": "VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing,\n  Speaking, and Acting",
            "submittedOnDailyBy": {
                "_id": "623d8ca4c29adf5ef6175615",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
                "isPro": false,
                "fullname": "Yi-Fan Zhang",
                "user": "yifanzhang114",
                "type": "user"
            },
            "summary": "Current Vision-Language-Action (VLA) models are often constrained by a rigid,\nstatic interaction paradigm, which lacks the ability to see, hear, speak, and\nact concurrently as well as handle real-time user interruptions dynamically.\nThis hinders seamless embodied collaboration, resulting in an inflexible and\nunresponsive user experience. To address these limitations, we introduce\nVITA-E, a novel embodied interaction framework designed for both behavioral\nconcurrency and nearly real-time interruption. The core of our approach is a\ndual-model architecture where two parallel VLA instances operate as an ``Active\nModel'' and a ``Standby Model'', allowing the embodied agent to observe its\nenvironment, listen to user speech, provide verbal responses, and execute\nactions, all concurrently and interruptibly, mimicking human-like multitasking\ncapabilities. We further propose a ``model-as-controller'' paradigm, where we\nfine-tune the VLM to generate special tokens that serve as direct system-level\ncommands, coupling the model's reasoning with the system's behavior.\nExperiments conducted on a physical humanoid platform demonstrate that VITA-E\ncan reliably handle complex interactive scenarios. Our framework is compatible\nwith various dual-system VLA models, achieving an extremely high success rate\non emergency stops and speech interruptions while also successfully performing\nconcurrent speech and action. This represents a significant step towards more\nnatural and capable embodied assistants.",
            "upvotes": 40,
            "discussionId": "69002d2722d452aac6dd42e6",
            "projectPage": "https://lxysl.github.io/VITA-E/",
            "githubRepo": "https://github.com/Tencent/VITA/tree/VITA-E",
            "ai_summary": "VITA-E, a dual-model embodied interaction framework, enables concurrent and interruptible vision-language-action capabilities, enhancing real-time user interaction and multitasking.",
            "ai_keywords": [
                "embodied interaction framework",
                "dual-model architecture",
                "Active Model",
                "Standby Model",
                "model-as-controller",
                "VLM",
                "special tokens",
                "system-level commands",
                "emergency stops",
                "speech interruptions",
                "concurrent speech and action"
            ],
            "githubStars": 113
        },
        "publishedAt": "2025-10-21T13:59:56.000Z",
        "title": "VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing,\n  Speaking, and Acting",
        "summary": "Current Vision-Language-Action (VLA) models are often constrained by a rigid,\nstatic interaction paradigm, which lacks the ability to see, hear, speak, and\nact concurrently as well as handle real-time user interruptions dynamically.\nThis hinders seamless embodied collaboration, resulting in an inflexible and\nunresponsive user experience. To address these limitations, we introduce\nVITA-E, a novel embodied interaction framework designed for both behavioral\nconcurrency and nearly real-time interruption. The core of our approach is a\ndual-model architecture where two parallel VLA instances operate as an ``Active\nModel'' and a ``Standby Model'', allowing the embodied agent to observe its\nenvironment, listen to user speech, provide verbal responses, and execute\nactions, all concurrently and interruptibly, mimicking human-like multitasking\ncapabilities. We further propose a ``model-as-controller'' paradigm, where we\nfine-tune the VLM to generate special tokens that serve as direct system-level\ncommands, coupling the model's reasoning with the system's behavior.\nExperiments conducted on a physical humanoid platform demonstrate that VITA-E\ncan reliably handle complex interactive scenarios. Our framework is compatible\nwith various dual-system VLA models, achieving an extremely high success rate\non emergency stops and speech interruptions while also successfully performing\nconcurrent speech and action. This represents a significant step towards more\nnatural and capable embodied assistants.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/623d8ca4c29adf5ef6175615/YqWLnrC3Cf4cOxO20icQF.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21817.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "fullname": "Yi-Fan Zhang",
            "name": "yifanzhang114",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 20
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.22201",
            "authors": [
                {
                    "_id": "69002ae322d452aac6dd42aa",
                    "user": {
                        "_id": "630461624ec2dfa82a5ad7e7",
                        "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
                        "isPro": false,
                        "fullname": "Minho Park",
                        "user": "mpark",
                        "type": "user"
                    },
                    "name": "Minho Park",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:49.518Z",
                    "hidden": false
                },
                {
                    "_id": "69002ae322d452aac6dd42ab",
                    "user": {
                        "_id": "64797735a68454566356b708",
                        "avatarUrl": "/avatars/3424d022dd8ad29b56eb41814c5c3dee.svg",
                        "isPro": false,
                        "fullname": "Kinam Kim",
                        "user": "kinam0252",
                        "type": "user"
                    },
                    "name": "Kinam Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:46.749Z",
                    "hidden": false
                },
                {
                    "_id": "69002ae322d452aac6dd42ac",
                    "name": "Junha Hyung",
                    "hidden": false
                },
                {
                    "_id": "69002ae322d452aac6dd42ad",
                    "name": "Hyojin Jang",
                    "hidden": false
                },
                {
                    "_id": "69002ae322d452aac6dd42ae",
                    "name": "Hoiyeong Jin",
                    "hidden": false
                },
                {
                    "_id": "69002ae322d452aac6dd42af",
                    "name": "Jooyeol Yun",
                    "hidden": false
                },
                {
                    "_id": "69002ae322d452aac6dd42b0",
                    "name": "Hojoon Lee",
                    "hidden": false
                },
                {
                    "_id": "69002ae322d452aac6dd42b1",
                    "name": "Jaegul Choo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/630461624ec2dfa82a5ad7e7/MDwVqWBQkIBhBlLgwciwW.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/630461624ec2dfa82a5ad7e7/terac32nyczWudLi39iRt.mp4"
            ],
            "publishedAt": "2025-10-25T07:44:33.000Z",
            "submittedOnDailyAt": "2025-10-28T01:06:06.608Z",
            "title": "ACG: Action Coherence Guidance for Flow-based VLA models",
            "submittedOnDailyBy": {
                "_id": "630461624ec2dfa82a5ad7e7",
                "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
                "isPro": false,
                "fullname": "Minho Park",
                "user": "mpark",
                "type": "user"
            },
            "summary": "Diffusion and flow matching models have emerged as powerful robot policies,\nenabling Vision-Language-Action (VLA) models to generalize across diverse\nscenes and instructions. Yet, when trained via imitation learning, their high\ngenerative capacity makes them sensitive to noise in human demonstrations:\njerks, pauses, and jitter which reduce action coherence. Reduced action\ncoherence causes instability and trajectory drift during deployment, failures\nthat are catastrophic in fine-grained manipulation where precision is crucial.\nIn this paper, we present Action Coherence Guidance (ACG) for VLA models, a\ntraining-free test-time guidance algorithm that improves action coherence and\nthereby yields performance gains. Evaluated on RoboCasa, DexMimicGen, and\nreal-world SO-101 tasks, ACG consistently improves action coherence and boosts\nsuccess rates across diverse manipulation tasks. Code and project page are\navailable at https://github.com/DAVIAN-Robotics/ACG and\nhttps://DAVIAN-Robotics.github.io/ACG , respectively.",
            "upvotes": 33,
            "discussionId": "69002ae322d452aac6dd42b2",
            "projectPage": "https://davian-robotics.github.io/ACG",
            "githubRepo": "https://github.com/DAVIAN-Robotics/ACG",
            "ai_summary": "Action Coherence Guidance (ACG) improves action coherence in Vision-Language-Action (VLA) models during test time, enhancing performance in diverse manipulation tasks.",
            "ai_keywords": [
                "diffusion models",
                "flow matching models",
                "Vision-Language-Action (VLA) models",
                "imitation learning",
                "action coherence",
                "trajectory drift",
                "RoboCasa",
                "DexMimicGen",
                "SO-101 tasks",
                "Action Coherence Guidance (ACG)"
            ],
            "githubStars": 13,
            "organization": {
                "_id": "68f61ab10a6265597402e1b1",
                "name": "DAVIAN-Robotics",
                "fullname": "DAVIAN Robotics",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630461624ec2dfa82a5ad7e7/0NA6xGIRlNalvgIxvZIXD.png"
            }
        },
        "publishedAt": "2025-10-25T03:44:33.000Z",
        "title": "ACG: Action Coherence Guidance for Flow-based VLA models",
        "summary": "Diffusion and flow matching models have emerged as powerful robot policies,\nenabling Vision-Language-Action (VLA) models to generalize across diverse\nscenes and instructions. Yet, when trained via imitation learning, their high\ngenerative capacity makes them sensitive to noise in human demonstrations:\njerks, pauses, and jitter which reduce action coherence. Reduced action\ncoherence causes instability and trajectory drift during deployment, failures\nthat are catastrophic in fine-grained manipulation where precision is crucial.\nIn this paper, we present Action Coherence Guidance (ACG) for VLA models, a\ntraining-free test-time guidance algorithm that improves action coherence and\nthereby yields performance gains. Evaluated on RoboCasa, DexMimicGen, and\nreal-world SO-101 tasks, ACG consistently improves action coherence and boosts\nsuccess rates across diverse manipulation tasks. Code and project page are\navailable at https://github.com/DAVIAN-Robotics/ACG and\nhttps://DAVIAN-Robotics.github.io/ACG , respectively.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/630461624ec2dfa82a5ad7e7/MDwVqWBQkIBhBlLgwciwW.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/630461624ec2dfa82a5ad7e7/terac32nyczWudLi39iRt.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22201.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "630461624ec2dfa82a5ad7e7",
            "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
            "fullname": "Minho Park",
            "name": "mpark",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "68f61ab10a6265597402e1b1",
            "name": "DAVIAN-Robotics",
            "fullname": "DAVIAN Robotics",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630461624ec2dfa82a5ad7e7/0NA6xGIRlNalvgIxvZIXD.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.22521",
            "authors": [
                {
                    "_id": "6900782d646208eac0d1ef17",
                    "user": {
                        "_id": "6565d47e75968d50cc2735ac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6565d47e75968d50cc2735ac/eX-h3GWaudhJii1IS7v8K.jpeg",
                        "isPro": false,
                        "fullname": "Yang Tian",
                        "user": "TyangJN",
                        "type": "user"
                    },
                    "name": "Yang Tian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:26:45.865Z",
                    "hidden": false
                },
                {
                    "_id": "6900782d646208eac0d1ef18",
                    "name": "Fan Liu",
                    "hidden": false
                },
                {
                    "_id": "6900782d646208eac0d1ef19",
                    "name": "Jingyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6900782d646208eac0d1ef1a",
                    "name": "Wei Bi",
                    "hidden": false
                },
                {
                    "_id": "6900782d646208eac0d1ef1b",
                    "name": "Yupeng Hu",
                    "hidden": false
                },
                {
                    "_id": "6900782d646208eac0d1ef1c",
                    "name": "Liqiang Nie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-26T04:13:31.000Z",
            "submittedOnDailyAt": "2025-10-28T14:22:48.399Z",
            "title": "Open Multimodal Retrieval-Augmented Factual Image Generation",
            "submittedOnDailyBy": {
                "_id": "6565d47e75968d50cc2735ac",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6565d47e75968d50cc2735ac/eX-h3GWaudhJii1IS7v8K.jpeg",
                "isPro": false,
                "fullname": "Yang Tian",
                "user": "TyangJN",
                "type": "user"
            },
            "summary": "Large Multimodal Models (LMMs) have achieved remarkable progress in\ngenerating photorealistic and prompt-aligned images, but they often produce\noutputs that contradict verifiable knowledge, especially when prompts involve\nfine-grained attributes or time-sensitive events. Conventional\nretrieval-augmented approaches attempt to address this issue by introducing\nexternal information, yet they are fundamentally incapable of grounding\ngeneration in accurate and evolving knowledge due to their reliance on static\nsources and shallow evidence integration. To bridge this gap, we introduce\nORIG, an agentic open multimodal retrieval-augmented framework for Factual\nImage Generation (FIG), a new task that requires both visual realism and\nfactual grounding. ORIG iteratively retrieves and filters multimodal evidence\nfrom the web and incrementally integrates the refined knowledge into enriched\nprompts to guide generation. To support systematic evaluation, we build\nFIG-Eval, a benchmark spanning ten categories across perceptual, compositional,\nand temporal dimensions. Experiments demonstrate that ORIG substantially\nimproves factual consistency and overall image quality over strong baselines,\nhighlighting the potential of open multimodal retrieval for factual image\ngeneration.",
            "upvotes": 30,
            "discussionId": "6900782d646208eac0d1ef1d",
            "projectPage": "https://tyangjn.github.io/orig.github.io/",
            "githubRepo": "https://github.com/TyangJN/ORIG",
            "ai_summary": "ORIG, an agentic open multimodal retrieval-augmented framework, enhances factual consistency and image quality in factual image generation by iteratively integrating refined web-based evidence into prompts.",
            "ai_keywords": [
                "Large Multimodal Models",
                "factual image generation",
                "retrieval-augmented",
                "multimodal evidence",
                "perceptual",
                "compositional",
                "temporal dimensions",
                "factual consistency"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-10-26T00:13:31.000Z",
        "title": "Open Multimodal Retrieval-Augmented Factual Image Generation",
        "summary": "Large Multimodal Models (LMMs) have achieved remarkable progress in\ngenerating photorealistic and prompt-aligned images, but they often produce\noutputs that contradict verifiable knowledge, especially when prompts involve\nfine-grained attributes or time-sensitive events. Conventional\nretrieval-augmented approaches attempt to address this issue by introducing\nexternal information, yet they are fundamentally incapable of grounding\ngeneration in accurate and evolving knowledge due to their reliance on static\nsources and shallow evidence integration. To bridge this gap, we introduce\nORIG, an agentic open multimodal retrieval-augmented framework for Factual\nImage Generation (FIG), a new task that requires both visual realism and\nfactual grounding. ORIG iteratively retrieves and filters multimodal evidence\nfrom the web and incrementally integrates the refined knowledge into enriched\nprompts to guide generation. To support systematic evaluation, we build\nFIG-Eval, a benchmark spanning ten categories across perceptual, compositional,\nand temporal dimensions. Experiments demonstrate that ORIG substantially\nimproves factual consistency and overall image quality over strong baselines,\nhighlighting the potential of open multimodal retrieval for factual image\ngeneration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22521.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6565d47e75968d50cc2735ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6565d47e75968d50cc2735ac/eX-h3GWaudhJii1IS7v8K.jpeg",
            "fullname": "Yang Tian",
            "name": "TyangJN",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.22733",
            "authors": [
                {
                    "_id": "6900329b22d452aac6dd431c",
                    "user": {
                        "_id": "630b167317bbe470568db9b5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b167317bbe470568db9b5/pEc1mwlfX1uq28jzbqCaP.png",
                        "isPro": false,
                        "fullname": "Qi Liu",
                        "user": "liuqi6777",
                        "type": "user"
                    },
                    "name": "Qi Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:34.058Z",
                    "hidden": false
                },
                {
                    "_id": "6900329b22d452aac6dd431d",
                    "name": "Yanzhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6900329b22d452aac6dd431e",
                    "name": "Mingxin Li",
                    "hidden": false
                },
                {
                    "_id": "6900329b22d452aac6dd431f",
                    "name": "Dingkun Long",
                    "hidden": false
                },
                {
                    "_id": "6900329b22d452aac6dd4320",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "6900329b22d452aac6dd4321",
                    "name": "Jiaxin Mao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-26T16:04:48.000Z",
            "submittedOnDailyAt": "2025-10-28T01:38:21.829Z",
            "title": "E^2Rank: Your Text Embedding can Also be an Effective\n  and Efficient Listwise Reranker",
            "submittedOnDailyBy": {
                "_id": "630b167317bbe470568db9b5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b167317bbe470568db9b5/pEc1mwlfX1uq28jzbqCaP.png",
                "isPro": false,
                "fullname": "Qi Liu",
                "user": "liuqi6777",
                "type": "user"
            },
            "summary": "Text embedding models serve as a fundamental component in real-world search\napplications. By mapping queries and documents into a shared embedding space,\nthey deliver competitive retrieval performance with high efficiency. However,\ntheir ranking fidelity remains limited compared to dedicated rerankers,\nespecially recent LLM-based listwise rerankers, which capture fine-grained\nquery-document and document-document interactions. In this paper, we propose a\nsimple yet effective unified framework E^2Rank, means Efficient\nEmbedding-based Ranking (also means Embedding-to-Rank), which extends a single\ntext embedding model to perform both high-quality retrieval and listwise\nreranking through continued training under a listwise ranking objective,\nthereby achieving strong effectiveness with remarkable efficiency. By applying\ncosine similarity between the query and document embeddings as a unified\nranking function, the listwise ranking prompt, which is constructed from the\noriginal query and its candidate documents, serves as an enhanced query\nenriched with signals from the top-K documents, akin to pseudo-relevance\nfeedback (PRF) in traditional retrieval models. This design preserves the\nefficiency and representational quality of the base embedding model while\nsignificantly improving its reranking performance. Empirically,\nE^2Rank achieves state-of-the-art results on the BEIR\nreranking benchmark and demonstrates competitive performance on the\nreasoning-intensive BRIGHT benchmark, with very low reranking latency. We also\nshow that the ranking training process improves embedding performance on the\nMTEB benchmark. Our findings indicate that a single embedding model can\neffectively unify retrieval and reranking, offering both computational\nefficiency and competitive ranking accuracy.",
            "upvotes": 28,
            "discussionId": "6900329c22d452aac6dd4322",
            "projectPage": "https://alibaba-nlp.github.io/E2Rank/",
            "githubRepo": "https://github.com/Alibaba-NLP/E2Rank",
            "ai_summary": "A unified framework extends a single text embedding model to perform both retrieval and listwise reranking, achieving state-of-the-art results with low latency.",
            "ai_keywords": [
                "text embedding models",
                "shared embedding space",
                "ranking fidelity",
                "LLM-based listwise rerankers",
                "listwise ranking objective",
                "cosine similarity",
                "unified ranking function",
                "listwise ranking prompt",
                "pseudo-relevance feedback",
                "BEIR reranking benchmark",
                "BRIGHT benchmark",
                "MTEB benchmark",
                "embedding performance"
            ],
            "githubStars": 22,
            "organization": {
                "_id": "661f98de142a51d630dbbcc4",
                "name": "Alibaba-NLP",
                "fullname": "Alibaba-NLP",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/dfd_EcIfylvu3sdc2WMqX.png"
            }
        },
        "publishedAt": "2025-10-26T12:04:48.000Z",
        "title": "E^2Rank: Your Text Embedding can Also be an Effective\n  and Efficient Listwise Reranker",
        "summary": "Text embedding models serve as a fundamental component in real-world search\napplications. By mapping queries and documents into a shared embedding space,\nthey deliver competitive retrieval performance with high efficiency. However,\ntheir ranking fidelity remains limited compared to dedicated rerankers,\nespecially recent LLM-based listwise rerankers, which capture fine-grained\nquery-document and document-document interactions. In this paper, we propose a\nsimple yet effective unified framework E^2Rank, means Efficient\nEmbedding-based Ranking (also means Embedding-to-Rank), which extends a single\ntext embedding model to perform both high-quality retrieval and listwise\nreranking through continued training under a listwise ranking objective,\nthereby achieving strong effectiveness with remarkable efficiency. By applying\ncosine similarity between the query and document embeddings as a unified\nranking function, the listwise ranking prompt, which is constructed from the\noriginal query and its candidate documents, serves as an enhanced query\nenriched with signals from the top-K documents, akin to pseudo-relevance\nfeedback (PRF) in traditional retrieval models. This design preserves the\nefficiency and representational quality of the base embedding model while\nsignificantly improving its reranking performance. Empirically,\nE^2Rank achieves state-of-the-art results on the BEIR\nreranking benchmark and demonstrates competitive performance on the\nreasoning-intensive BRIGHT benchmark, with very low reranking latency. We also\nshow that the ranking training process improves embedding performance on the\nMTEB benchmark. Our findings indicate that a single embedding model can\neffectively unify retrieval and reranking, offering both computational\nefficiency and competitive ranking accuracy.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22733.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "630b167317bbe470568db9b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b167317bbe470568db9b5/pEc1mwlfX1uq28jzbqCaP.png",
            "fullname": "Qi Liu",
            "name": "liuqi6777",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "organization": {
            "_id": "661f98de142a51d630dbbcc4",
            "name": "Alibaba-NLP",
            "fullname": "Alibaba-NLP",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/dfd_EcIfylvu3sdc2WMqX.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.22706",
            "authors": [
                {
                    "_id": "69003b1c22d452aac6dd43a3",
                    "user": {
                        "_id": "667b8de7a68bf81afe668afe",
                        "avatarUrl": "/avatars/aeff10805ff858332e6f6a58735dbbd9.svg",
                        "isPro": false,
                        "fullname": "leoli",
                        "user": "lifuguan",
                        "type": "user"
                    },
                    "name": "Hao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:13.580Z",
                    "hidden": false
                },
                {
                    "_id": "69003b1c22d452aac6dd43a4",
                    "name": "Zhengyu Zou",
                    "hidden": false
                },
                {
                    "_id": "69003b1c22d452aac6dd43a5",
                    "name": "Fangfu Liu",
                    "hidden": false
                },
                {
                    "_id": "69003b1c22d452aac6dd43a6",
                    "name": "Xuanyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "69003b1c22d452aac6dd43a7",
                    "name": "Fangzhou Hong",
                    "hidden": false
                },
                {
                    "_id": "69003b1c22d452aac6dd43a8",
                    "name": "Yukang Cao",
                    "hidden": false
                },
                {
                    "_id": "69003b1c22d452aac6dd43a9",
                    "name": "Yushi Lan",
                    "hidden": false
                },
                {
                    "_id": "69003b1c22d452aac6dd43aa",
                    "name": "Manyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "69003b1c22d452aac6dd43ab",
                    "name": "Gang Yu",
                    "hidden": false
                },
                {
                    "_id": "69003b1c22d452aac6dd43ac",
                    "name": "Dingwen Zhang",
                    "hidden": false
                },
                {
                    "_id": "69003b1c22d452aac6dd43ad",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-26T14:57:44.000Z",
            "submittedOnDailyAt": "2025-10-28T02:20:52.943Z",
            "title": "IGGT: Instance-Grounded Geometry Transformer for Semantic 3D\n  Reconstruction",
            "submittedOnDailyBy": {
                "_id": "6505a02f9310ce8c400edc63",
                "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
                "isPro": false,
                "fullname": "Fangfu Liu",
                "user": "Liuff23",
                "type": "user"
            },
            "summary": "Humans naturally perceive the geometric structure and semantic content of a\n3D world as intertwined dimensions, enabling coherent and accurate\nunderstanding of complex scenes. However, most prior approaches prioritize\ntraining large geometry models for low-level 3D reconstruction and treat\nhigh-level spatial understanding in isolation, overlooking the crucial\ninterplay between these two fundamental aspects of 3D-scene analysis, thereby\nlimiting generalization and leading to poor performance in downstream 3D\nunderstanding tasks. Recent attempts have mitigated this issue by simply\naligning 3D models with specific language models, thus restricting perception\nto the aligned model's capacity and limiting adaptability to downstream tasks.\nIn this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an\nend-to-end large unified transformer to unify the knowledge for both spatial\nreconstruction and instance-level contextual understanding. Specifically, we\ndesign a 3D-Consistent Contrastive Learning strategy that guides IGGT to encode\na unified representation with geometric structures and instance-grounded\nclustering through only 2D visual inputs. This representation supports\nconsistent lifting of 2D visual inputs into a coherent 3D scene with explicitly\ndistinct object instances. To facilitate this task, we further construct\nInsScene-15K, a large-scale dataset with high-quality RGB images, poses, depth\nmaps, and 3D-consistent instance-level mask annotations with a novel data\ncuration pipeline.",
            "upvotes": 28,
            "discussionId": "69003b1d22d452aac6dd43ae",
            "githubRepo": "https://github.com/lifuguan/IGGT_official",
            "ai_summary": "InstanceGrounded Geometry Transformer (IGGT) unifies 3D reconstruction and instance-level understanding using a unified transformer and 3D-Consistent Contrastive Learning, supported by a new dataset InsScene-15K.",
            "ai_keywords": [
                "InstanceGrounded Geometry Transformer",
                "IGGT",
                "3D-Consistent Contrastive Learning",
                "3D reconstruction",
                "instance-level contextual understanding",
                "3D scene",
                "object instances",
                "InsScene-15K"
            ],
            "githubStars": 43
        },
        "publishedAt": "2025-10-26T10:57:44.000Z",
        "title": "IGGT: Instance-Grounded Geometry Transformer for Semantic 3D\n  Reconstruction",
        "summary": "Humans naturally perceive the geometric structure and semantic content of a\n3D world as intertwined dimensions, enabling coherent and accurate\nunderstanding of complex scenes. However, most prior approaches prioritize\ntraining large geometry models for low-level 3D reconstruction and treat\nhigh-level spatial understanding in isolation, overlooking the crucial\ninterplay between these two fundamental aspects of 3D-scene analysis, thereby\nlimiting generalization and leading to poor performance in downstream 3D\nunderstanding tasks. Recent attempts have mitigated this issue by simply\naligning 3D models with specific language models, thus restricting perception\nto the aligned model's capacity and limiting adaptability to downstream tasks.\nIn this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an\nend-to-end large unified transformer to unify the knowledge for both spatial\nreconstruction and instance-level contextual understanding. Specifically, we\ndesign a 3D-Consistent Contrastive Learning strategy that guides IGGT to encode\na unified representation with geometric structures and instance-grounded\nclustering through only 2D visual inputs. This representation supports\nconsistent lifting of 2D visual inputs into a coherent 3D scene with explicitly\ndistinct object instances. To facilitate this task, we further construct\nInsScene-15K, a large-scale dataset with high-quality RGB images, poses, depth\nmaps, and 3D-consistent instance-level mask annotations with a novel data\ncuration pipeline.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22706.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6505a02f9310ce8c400edc63",
            "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
            "fullname": "Fangfu Liu",
            "name": "Liuff23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.23451",
            "authors": [
                {
                    "_id": "69002f8622d452aac6dd42f3",
                    "name": "Zhuoran Jin",
                    "hidden": false
                },
                {
                    "_id": "69002f8622d452aac6dd42f4",
                    "name": "Hongbang Yuan",
                    "hidden": false
                },
                {
                    "_id": "69002f8622d452aac6dd42f5",
                    "name": "Kejian Zhu",
                    "hidden": false
                },
                {
                    "_id": "69002f8622d452aac6dd42f6",
                    "name": "Jiachun Li",
                    "hidden": false
                },
                {
                    "_id": "69002f8622d452aac6dd42f7",
                    "name": "Pengfei Cao",
                    "hidden": false
                },
                {
                    "_id": "69002f8622d452aac6dd42f8",
                    "name": "Yubo Chen",
                    "hidden": false
                },
                {
                    "_id": "69002f8622d452aac6dd42f9",
                    "name": "Kang Liu",
                    "hidden": false
                },
                {
                    "_id": "69002f8622d452aac6dd42fa",
                    "name": "Jun Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T15:53:20.000Z",
            "submittedOnDailyAt": "2025-10-28T01:30:06.348Z",
            "title": "Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with\n  Free-Form Preferences",
            "submittedOnDailyBy": {
                "_id": "643379416c6ecd58798421b3",
                "avatarUrl": "/avatars/831db7eab2663abc33b176cf386b02f2.svg",
                "isPro": false,
                "fullname": "Zhuoran Jin",
                "user": "jinzhuoran",
                "type": "user"
            },
            "summary": "Reward models (RMs) play a critical role in aligning AI behaviors with human\npreferences, yet they face two fundamental challenges: (1) Modality Imbalance,\nwhere most RMs are mainly focused on text and image modalities, offering\nlimited support for video, audio, and other modalities; and (2) Preference\nRigidity, where training on fixed binary preference pairs fails to capture the\ncomplexity and diversity of personalized preferences. To address the above\nchallenges, we propose Omni-Reward, a step toward generalist omni-modal reward\nmodeling with support for free-form preferences, consisting of: (1) Evaluation:\nWe introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form\npreferences, covering nine tasks across five modalities including text, image,\nvideo, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal\npreference dataset comprising 248K general preference pairs and 69K\ninstruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We\npropose Omni-RewardModel, which includes both discriminative and generative\nRMs, and achieves strong performance on Omni-RewardBench as well as other\nwidely used reward modeling benchmarks.",
            "upvotes": 25,
            "discussionId": "69002f8622d452aac6dd42fb",
            "githubRepo": "https://github.com/HongbangYuan/OmniReward",
            "ai_summary": "Omni-Reward addresses modality imbalance and preference rigidity in reward models by introducing a benchmark, dataset, and model that support multiple modalities and free-form preferences.",
            "ai_keywords": [
                "reward models",
                "modality imbalance",
                "preference rigidity",
                "Omni-Reward",
                "Omni-RewardBench",
                "Omni-RewardData",
                "Omni-RewardModel",
                "discriminative RMs",
                "generative RMs"
            ],
            "githubStars": 21,
            "organization": {
                "_id": "640a887796aae649741a586f",
                "name": "CASIA",
                "fullname": "Chinese Academic of Science Institute of Automation",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"
            }
        },
        "publishedAt": "2025-10-27T11:53:20.000Z",
        "title": "Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with\n  Free-Form Preferences",
        "summary": "Reward models (RMs) play a critical role in aligning AI behaviors with human\npreferences, yet they face two fundamental challenges: (1) Modality Imbalance,\nwhere most RMs are mainly focused on text and image modalities, offering\nlimited support for video, audio, and other modalities; and (2) Preference\nRigidity, where training on fixed binary preference pairs fails to capture the\ncomplexity and diversity of personalized preferences. To address the above\nchallenges, we propose Omni-Reward, a step toward generalist omni-modal reward\nmodeling with support for free-form preferences, consisting of: (1) Evaluation:\nWe introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form\npreferences, covering nine tasks across five modalities including text, image,\nvideo, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal\npreference dataset comprising 248K general preference pairs and 69K\ninstruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We\npropose Omni-RewardModel, which includes both discriminative and generative\nRMs, and achieves strong performance on Omni-RewardBench as well as other\nwidely used reward modeling benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23451.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "643379416c6ecd58798421b3",
            "avatarUrl": "/avatars/831db7eab2663abc33b176cf386b02f2.svg",
            "fullname": "Zhuoran Jin",
            "name": "jinzhuoran",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "640a887796aae649741a586f",
            "name": "CASIA",
            "fullname": "Chinese Academic of Science Institute of Automation",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.23052",
            "authors": [
                {
                    "_id": "69003b6822d452aac6dd43b8",
                    "user": {
                        "_id": "671b3d82101c0d9f78749256",
                        "avatarUrl": "/avatars/902c1006280c6552885b0b946cd2bb16.svg",
                        "isPro": false,
                        "fullname": "Zhanchao Zhou",
                        "user": "Zcchill",
                        "type": "user"
                    },
                    "name": "Zhanchao Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:08.904Z",
                    "hidden": false
                },
                {
                    "_id": "69003b6822d452aac6dd43b9",
                    "name": "Xiaodong Chen",
                    "hidden": false
                },
                {
                    "_id": "69003b6822d452aac6dd43ba",
                    "user": {
                        "_id": "63898c562a897944ea5f07a2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63898c562a897944ea5f07a2/oG1ia44ICpAk1tReIaX0G.jpeg",
                        "isPro": false,
                        "fullname": "Haoxing chen",
                        "user": "HaoxingChen",
                        "type": "user"
                    },
                    "name": "Haoxing Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:11.263Z",
                    "hidden": false
                },
                {
                    "_id": "69003b6822d452aac6dd43bb",
                    "name": "Zhenzhong Lan",
                    "hidden": false
                },
                {
                    "_id": "69003b6822d452aac6dd43bc",
                    "name": "Jianguo Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T06:28:58.000Z",
            "submittedOnDailyAt": "2025-10-28T02:18:36.150Z",
            "title": "Knocking-Heads Attention",
            "submittedOnDailyBy": {
                "_id": "671b3d82101c0d9f78749256",
                "avatarUrl": "/avatars/902c1006280c6552885b0b946cd2bb16.svg",
                "isPro": false,
                "fullname": "Zhanchao Zhou",
                "user": "Zcchill",
                "type": "user"
            },
            "summary": "Multi-head attention (MHA) has become the cornerstone of modern large\nlanguage models, enhancing representational capacity through parallel attention\nheads. However, increasing the number of heads inherently weakens individual\nhead capacity, and existing attention mechanisms - whether standard MHA or its\nvariants like grouped-query attention (GQA) and grouped-tied attention (GTA) -\nsimply concatenate outputs from isolated heads without strong interaction. To\naddress this limitation, we propose knocking-heads attention (KHA), which\nenables attention heads to \"knock\" on each other - facilitating cross-head\nfeature-level interactions before the scaled dot-product attention. This is\nachieved by applying a shared, diagonally-initialized projection matrix across\nall heads. The diagonal initialization preserves head-specific specialization\nat the start of training while allowing the model to progressively learn\nintegrated cross-head representations. KHA adds only minimal parameters and\nFLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention\nvariants. We validate KHA by training a 6.1B parameter MoE model (1.01B\nactivated) on 1T high-quality tokens. Compared to baseline attention\nmechanisms, KHA brings superior and more stable training dynamics, achieving\nbetter performance across downstream tasks.",
            "upvotes": 22,
            "discussionId": "69003b6922d452aac6dd43bd",
            "ai_summary": "Knocking-heads attention (KHA) enhances multi-head attention by enabling cross-head interactions, improving training dynamics and performance in large language models.",
            "ai_keywords": [
                "multi-head attention",
                "MHA",
                "grouped-query attention",
                "GQA",
                "grouped-tied attention",
                "GTA",
                "knocking-heads attention",
                "KHA",
                "scaled dot-product attention",
                "parameter-efficient",
                "MoE",
                "high-quality tokens",
                "training dynamics",
                "downstream tasks"
            ]
        },
        "publishedAt": "2025-10-27T02:28:58.000Z",
        "title": "Knocking-Heads Attention",
        "summary": "Multi-head attention (MHA) has become the cornerstone of modern large\nlanguage models, enhancing representational capacity through parallel attention\nheads. However, increasing the number of heads inherently weakens individual\nhead capacity, and existing attention mechanisms - whether standard MHA or its\nvariants like grouped-query attention (GQA) and grouped-tied attention (GTA) -\nsimply concatenate outputs from isolated heads without strong interaction. To\naddress this limitation, we propose knocking-heads attention (KHA), which\nenables attention heads to \"knock\" on each other - facilitating cross-head\nfeature-level interactions before the scaled dot-product attention. This is\nachieved by applying a shared, diagonally-initialized projection matrix across\nall heads. The diagonal initialization preserves head-specific specialization\nat the start of training while allowing the model to progressively learn\nintegrated cross-head representations. KHA adds only minimal parameters and\nFLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention\nvariants. We validate KHA by training a 6.1B parameter MoE model (1.01B\nactivated) on 1T high-quality tokens. Compared to baseline attention\nmechanisms, KHA brings superior and more stable training dynamics, achieving\nbetter performance across downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23052.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "671b3d82101c0d9f78749256",
            "avatarUrl": "/avatars/902c1006280c6552885b0b946cd2bb16.svg",
            "fullname": "Zhanchao Zhou",
            "name": "Zcchill",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.23603",
            "authors": [
                {
                    "_id": "6900320522d452aac6dd4309",
                    "user": {
                        "_id": "64a3fe3dde901eb01df12398",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a3fe3dde901eb01df12398/Js2bEx4rxKuEKVt5z9I2D.jpeg",
                        "isPro": false,
                        "fullname": "YuqianYuan",
                        "user": "CircleRadon",
                        "type": "user"
                    },
                    "name": "Yuqian Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:36.571Z",
                    "hidden": false
                },
                {
                    "_id": "6900320522d452aac6dd430a",
                    "name": "Wenqiao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6900320522d452aac6dd430b",
                    "name": "Xin Li",
                    "hidden": false
                },
                {
                    "_id": "6900320522d452aac6dd430c",
                    "name": "Shihao Wang",
                    "hidden": false
                },
                {
                    "_id": "6900320522d452aac6dd430d",
                    "name": "Kehan Li",
                    "hidden": false
                },
                {
                    "_id": "6900320522d452aac6dd430e",
                    "name": "Wentong Li",
                    "hidden": false
                },
                {
                    "_id": "6900320522d452aac6dd430f",
                    "name": "Jun Xiao",
                    "hidden": false
                },
                {
                    "_id": "6900320522d452aac6dd4310",
                    "name": "Lei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6900320522d452aac6dd4311",
                    "name": "Beng Chin Ooi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T17:59:32.000Z",
            "submittedOnDailyAt": "2025-10-28T01:31:39.211Z",
            "title": "PixelRefer: A Unified Framework for Spatio-Temporal Object Referring\n  with Arbitrary Granularity",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Multimodal large language models (MLLMs) have demonstrated strong\ngeneral-purpose capabilities in open-world visual comprehension. However, most\nexisting MLLMs primarily focus on holistic, scene-level understanding, often\noverlooking the need for fine-grained, object-centric reasoning. In this paper,\nwe present PixelRefer, a unified region-level MLLM framework that enables\nadvanced fine-grained understanding over user-specified regions across both\nimages and videos. Motivated by the observation that LLM attention\npredominantly focuses on object-level tokens, we propose a Scale-Adaptive\nObject Tokenizer (SAOT) to generate compact and semantically rich object\nrepresentations from free-form regions. Our analysis reveals that global visual\ntokens contribute mainly in early LLM layers, inspiring the design of\nPixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion\nmodule to pre-fuse global context into object tokens. This yields a lightweight\nObject-Only Framework that substantially reduces computational cost while\nmaintaining high semantic fidelity. To facilitate fine-grained instruction\ntuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction\ndataset. Extensive experiments across a range of benchmarks validate that\nPixelRefer achieves leading performance with fewer training samples, while\nPixelRefer-Lite offers competitive accuracy with notable gains in efficiency.",
            "upvotes": 21,
            "discussionId": "6900320622d452aac6dd4312",
            "projectPage": "https://circleradon.github.io/PixelRefer/",
            "ai_summary": "PixelRefer is a unified region-level MLLM framework that enhances fine-grained object-centric understanding using a Scale-Adaptive Object Tokenizer and Object-Centric Infusion module, achieving high performance and efficiency.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "open-world visual comprehension",
                "fine-grained reasoning",
                "PixelRefer",
                "region-level",
                "Scale-Adaptive Object Tokenizer",
                "SAOT",
                "object-level tokens",
                "global visual tokens",
                "PixelRefer-Lite",
                "Object-Centric Infusion module",
                "object-centric instruction dataset",
                "PixelRefer-2.2M"
            ]
        },
        "publishedAt": "2025-10-27T13:59:32.000Z",
        "title": "PixelRefer: A Unified Framework for Spatio-Temporal Object Referring\n  with Arbitrary Granularity",
        "summary": "Multimodal large language models (MLLMs) have demonstrated strong\ngeneral-purpose capabilities in open-world visual comprehension. However, most\nexisting MLLMs primarily focus on holistic, scene-level understanding, often\noverlooking the need for fine-grained, object-centric reasoning. In this paper,\nwe present PixelRefer, a unified region-level MLLM framework that enables\nadvanced fine-grained understanding over user-specified regions across both\nimages and videos. Motivated by the observation that LLM attention\npredominantly focuses on object-level tokens, we propose a Scale-Adaptive\nObject Tokenizer (SAOT) to generate compact and semantically rich object\nrepresentations from free-form regions. Our analysis reveals that global visual\ntokens contribute mainly in early LLM layers, inspiring the design of\nPixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion\nmodule to pre-fuse global context into object tokens. This yields a lightweight\nObject-Only Framework that substantially reduces computational cost while\nmaintaining high semantic fidelity. To facilitate fine-grained instruction\ntuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction\ndataset. Extensive experiments across a range of benchmarks validate that\nPixelRefer achieves leading performance with fewer training samples, while\nPixelRefer-Lite offers competitive accuracy with notable gains in efficiency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23603.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 148
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.23393",
            "authors": [
                {
                    "_id": "69008b6a646208eac0d1ef43",
                    "user": {
                        "_id": "62ecd8a0b72a69615d6bbd76",
                        "avatarUrl": "/avatars/73c47466cbd3ae217b23e5e76bd0f740.svg",
                        "isPro": false,
                        "fullname": "Farid Bagirov",
                        "user": "kraalfar",
                        "type": "user"
                    },
                    "name": "Farid Bagirov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:19:43.421Z",
                    "hidden": false
                },
                {
                    "_id": "69008b6a646208eac0d1ef44",
                    "name": "Mikhail Arkhipov",
                    "hidden": false
                },
                {
                    "_id": "69008b6a646208eac0d1ef45",
                    "name": "Ksenia Sycheva",
                    "hidden": false
                },
                {
                    "_id": "69008b6a646208eac0d1ef46",
                    "user": {
                        "_id": "6440ff39ad24e9b2cfba8575",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6440ff39ad24e9b2cfba8575/QoQC60vbfd2YzTtPAXpM-.jpeg",
                        "isPro": false,
                        "fullname": "Evgeniy Glukhov",
                        "user": "jenyag",
                        "type": "user"
                    },
                    "name": "Evgeniy Glukhov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T14:32:12.590Z",
                    "hidden": false
                },
                {
                    "_id": "69008b6a646208eac0d1ef47",
                    "user": {
                        "_id": "64380bed961bb61e463bf93d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64380bed961bb61e463bf93d/zsel0Dzv1yU9O8zAxvCBw.jpeg",
                        "isPro": false,
                        "fullname": "Egor Bogomolov",
                        "user": "egor-bogomolov",
                        "type": "user"
                    },
                    "name": "Egor Bogomolov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:19:40.431Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T14:47:30.000Z",
            "submittedOnDailyAt": "2025-10-28T08:20:28.265Z",
            "title": "The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N\n  Sampling via max@k Optimisation",
            "submittedOnDailyBy": {
                "_id": "6440ff39ad24e9b2cfba8575",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6440ff39ad24e9b2cfba8575/QoQC60vbfd2YzTtPAXpM-.jpeg",
                "isPro": false,
                "fullname": "Evgeniy Glukhov",
                "user": "jenyag",
                "type": "user"
            },
            "summary": "The application of Reinforcement Learning with Verifiable Rewards (RLVR) to\nmathematical and coding domains has demonstrated significant improvements in\nthe reasoning and problem-solving abilities of Large Language Models. Despite\nits success in single generation problem solving, the reinforcement learning\nfine-tuning process may harm the model's exploration ability, as reflected in\ndecreased diversity of generations and a resulting degradation of performance\nduring Best-of-N sampling for large N values. In this work, we focus on\noptimizing the max@k metric, a continuous generalization of pass@k. We derive\nan unbiased on-policy gradient estimate for direct optimization of this metric.\nFurthermore, we extend our derivations to the off-policy updates, a common\nelement in modern RLVR algorithms, that allows better sample efficiency.\nEmpirically, we show that our objective effectively optimizes max@k metric in\noff-policy scenarios, aligning the model with the Best-of-N inference strategy.",
            "upvotes": 14,
            "discussionId": "69008b6b646208eac0d1ef48",
            "ai_summary": "Optimizing the max@k metric through unbiased gradient estimates in RLVR improves the diversity and performance of Large Language Models in Best-of-N sampling scenarios.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards (RLVR)",
                "Large Language Models",
                "max@k metric",
                "pass@k",
                "on-policy gradient estimate",
                "off-policy updates",
                "Best-of-N sampling"
            ],
            "organization": {
                "_id": "6460c8ca1db65f878513d6ec",
                "name": "JetBrains",
                "fullname": "JetBrains",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6645d54b780d46f274dd4145/8RHS3MzGGBFWtH1-bynJ0.png"
            }
        },
        "publishedAt": "2025-10-27T10:47:30.000Z",
        "title": "The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N\n  Sampling via max@k Optimisation",
        "summary": "The application of Reinforcement Learning with Verifiable Rewards (RLVR) to\nmathematical and coding domains has demonstrated significant improvements in\nthe reasoning and problem-solving abilities of Large Language Models. Despite\nits success in single generation problem solving, the reinforcement learning\nfine-tuning process may harm the model's exploration ability, as reflected in\ndecreased diversity of generations and a resulting degradation of performance\nduring Best-of-N sampling for large N values. In this work, we focus on\noptimizing the max@k metric, a continuous generalization of pass@k. We derive\nan unbiased on-policy gradient estimate for direct optimization of this metric.\nFurthermore, we extend our derivations to the off-policy updates, a common\nelement in modern RLVR algorithms, that allows better sample efficiency.\nEmpirically, we show that our objective effectively optimizes max@k metric in\noff-policy scenarios, aligning the model with the Best-of-N inference strategy.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23393.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6440ff39ad24e9b2cfba8575",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6440ff39ad24e9b2cfba8575/QoQC60vbfd2YzTtPAXpM-.jpeg",
            "fullname": "Evgeniy Glukhov",
            "name": "jenyag",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "6460c8ca1db65f878513d6ec",
            "name": "JetBrains",
            "fullname": "JetBrains",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6645d54b780d46f274dd4145/8RHS3MzGGBFWtH1-bynJ0.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.22946",
            "authors": [
                {
                    "_id": "6900334b22d452aac6dd4324",
                    "name": "Zeyu Wang",
                    "hidden": false
                },
                {
                    "_id": "6900334b22d452aac6dd4325",
                    "name": "Zilong Chen",
                    "hidden": false
                },
                {
                    "_id": "6900334b22d452aac6dd4326",
                    "name": "Chenhui Gou",
                    "hidden": false
                },
                {
                    "_id": "6900334b22d452aac6dd4327",
                    "name": "Feng Li",
                    "hidden": false
                },
                {
                    "_id": "6900334b22d452aac6dd4328",
                    "name": "Chaorui Deng",
                    "hidden": false
                },
                {
                    "_id": "6900334b22d452aac6dd4329",
                    "name": "Deyao Zhu",
                    "hidden": false
                },
                {
                    "_id": "6900334b22d452aac6dd432a",
                    "name": "Kunchang Li",
                    "hidden": false
                },
                {
                    "_id": "6900334b22d452aac6dd432b",
                    "name": "Weihao Yu",
                    "hidden": false
                },
                {
                    "_id": "6900334b22d452aac6dd432c",
                    "name": "Haoqin Tu",
                    "hidden": false
                },
                {
                    "_id": "6900334b22d452aac6dd432d",
                    "name": "Haoqi Fan",
                    "hidden": false
                },
                {
                    "_id": "6900334b22d452aac6dd432e",
                    "name": "Cihang Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T02:59:57.000Z",
            "submittedOnDailyAt": "2025-10-28T01:37:01.506Z",
            "title": "LightBagel: A Light-weighted, Double Fusion Framework for Unified\n  Multimodal Understanding and Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Unified multimodal models have recently shown remarkable gains in both\ncapability and versatility, yet most leading systems are still trained from\nscratch and require substantial computational resources. In this paper, we show\nthat competitive performance can be obtained far more efficiently by\nstrategically fusing publicly available models specialized for either\ngeneration or understanding. Our key design is to retain the original blocks\nwhile additionally interleaving multimodal self-attention blocks throughout the\nnetworks. This double fusion mechanism (1) effectively enables rich multi-modal\nfusion while largely preserving the original strengths of the base models, and\n(2) catalyzes synergistic fusion of high-level semantic representations from\nthe understanding encoder with low-level spatial signals from the generation\nencoder. By training with only ~ 35B tokens, this approach achieves strong\nresults across multiple benchmarks: 0.91 on GenEval for compositional\ntext-to-image generation, 82.16 on DPG-Bench for complex text-to-image\ngeneration, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By\nfully releasing the entire suite of code, model weights, and datasets, we hope\nto support future research on unified multimodal modeling.",
            "upvotes": 13,
            "discussionId": "6900334b22d452aac6dd432f",
            "projectPage": "https://ucsc-vlaa.github.io/LightBagel/",
            "ai_summary": "A unified multimodal model achieves competitive performance with efficient fusion of generation and understanding models, using interleaved multimodal self-attention blocks and minimal training resources.",
            "ai_keywords": [
                "multimodal models",
                "self-attention blocks",
                "compositional text-to-image generation",
                "complex text-to-image generation",
                "image editing"
            ]
        },
        "publishedAt": "2025-10-26T22:59:57.000Z",
        "title": "LightBagel: A Light-weighted, Double Fusion Framework for Unified\n  Multimodal Understanding and Generation",
        "summary": "Unified multimodal models have recently shown remarkable gains in both\ncapability and versatility, yet most leading systems are still trained from\nscratch and require substantial computational resources. In this paper, we show\nthat competitive performance can be obtained far more efficiently by\nstrategically fusing publicly available models specialized for either\ngeneration or understanding. Our key design is to retain the original blocks\nwhile additionally interleaving multimodal self-attention blocks throughout the\nnetworks. This double fusion mechanism (1) effectively enables rich multi-modal\nfusion while largely preserving the original strengths of the base models, and\n(2) catalyzes synergistic fusion of high-level semantic representations from\nthe understanding encoder with low-level spatial signals from the generation\nencoder. By training with only ~ 35B tokens, this approach achieves strong\nresults across multiple benchmarks: 0.91 on GenEval for compositional\ntext-to-image generation, 82.16 on DPG-Bench for complex text-to-image\ngeneration, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By\nfully releasing the entire suite of code, model weights, and datasets, we hope\nto support future research on unified multimodal modeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22946.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 148
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.22200",
            "authors": [
                {
                    "_id": "690036a622d452aac6dd4356",
                    "name": "Meituan LongCat Team",
                    "hidden": false
                },
                {
                    "_id": "690036a622d452aac6dd4357",
                    "name": "Xunliang Cai",
                    "hidden": false
                },
                {
                    "_id": "690036a622d452aac6dd4358",
                    "name": "Qilong Huang",
                    "hidden": false
                },
                {
                    "_id": "690036a622d452aac6dd4359",
                    "name": "Zhuoliang Kang",
                    "hidden": false
                },
                {
                    "_id": "690036a622d452aac6dd435a",
                    "name": "Hongyu Li",
                    "hidden": false
                },
                {
                    "_id": "690036a622d452aac6dd435b",
                    "name": "Shijun Liang",
                    "hidden": false
                },
                {
                    "_id": "690036a622d452aac6dd435c",
                    "name": "Liya Ma",
                    "hidden": false
                },
                {
                    "_id": "690036a622d452aac6dd435d",
                    "name": "Siyu Ren",
                    "hidden": false
                },
                {
                    "_id": "690036a622d452aac6dd435e",
                    "name": "Xiaoming Wei",
                    "hidden": false
                },
                {
                    "_id": "690036a622d452aac6dd435f",
                    "name": "Rixu Xie",
                    "hidden": false
                },
                {
                    "_id": "690036a622d452aac6dd4360",
                    "name": "Tong Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-25T07:41:02.000Z",
            "submittedOnDailyAt": "2025-10-28T01:51:30.094Z",
            "title": "LongCat-Video Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Video generation is a critical pathway toward world models, with efficient\nlong video inference as a key capability. Toward this end, we introduce\nLongCat-Video, a foundational video generation model with 13.6B parameters,\ndelivering strong performance across multiple video generation tasks. It\nparticularly excels in efficient and high-quality long video generation,\nrepresenting our first step toward world models. Key features include: Unified\narchitecture for multiple tasks: Built on the Diffusion Transformer (DiT)\nframework, LongCat-Video supports Text-to-Video, Image-to-Video, and\nVideo-Continuation tasks with a single model; Long video generation:\nPretraining on Video-Continuation tasks enables LongCat-Video to maintain high\nquality and temporal coherence in the generation of minutes-long videos;\nEfficient inference: LongCat-Video generates 720p, 30fps videos within minutes\nby employing a coarse-to-fine generation strategy along both the temporal and\nspatial axes. Block Sparse Attention further enhances efficiency, particularly\nat high resolutions; Strong performance with multi-reward RLHF: Multi-reward\nRLHF training enables LongCat-Video to achieve performance on par with the\nlatest closed-source and leading open-source models. Code and model weights are\npublicly available to accelerate progress in the field.",
            "upvotes": 10,
            "discussionId": "690036a722d452aac6dd4361",
            "githubRepo": "https://github.com/meituan-longcat/LongCat-Video",
            "ai_summary": "LongCat-Video, a 13.6B parameter video generation model based on the Diffusion Transformer framework, excels in efficient and high-quality long video generation across multiple tasks using unified architecture, coarse-to-fine generation, and block sparse attention.",
            "ai_keywords": [
                "Diffusion Transformer",
                "Text-to-Video",
                "Image-to-Video",
                "Video-Continuation",
                "temporal coherence",
                "coarse-to-fine generation",
                "block sparse attention",
                "multi-reward RLHF"
            ],
            "githubStars": 680
        },
        "publishedAt": "2025-10-25T03:41:02.000Z",
        "title": "LongCat-Video Technical Report",
        "summary": "Video generation is a critical pathway toward world models, with efficient\nlong video inference as a key capability. Toward this end, we introduce\nLongCat-Video, a foundational video generation model with 13.6B parameters,\ndelivering strong performance across multiple video generation tasks. It\nparticularly excels in efficient and high-quality long video generation,\nrepresenting our first step toward world models. Key features include: Unified\narchitecture for multiple tasks: Built on the Diffusion Transformer (DiT)\nframework, LongCat-Video supports Text-to-Video, Image-to-Video, and\nVideo-Continuation tasks with a single model; Long video generation:\nPretraining on Video-Continuation tasks enables LongCat-Video to maintain high\nquality and temporal coherence in the generation of minutes-long videos;\nEfficient inference: LongCat-Video generates 720p, 30fps videos within minutes\nby employing a coarse-to-fine generation strategy along both the temporal and\nspatial axes. Block Sparse Attention further enhances efficiency, particularly\nat high resolutions; Strong performance with multi-reward RLHF: Multi-reward\nRLHF training enables LongCat-Video to achieve performance on par with the\nlatest closed-source and leading open-source models. Code and model weights are\npublicly available to accelerate progress in the field.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22200.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 148
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.23544",
            "authors": [
                {
                    "_id": "6900350c22d452aac6dd434f",
                    "user": {
                        "_id": "64dc29d9b5d625e0e9a6ecb9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
                        "isPro": false,
                        "fullname": "Tingyu Song",
                        "user": "songtingyu",
                        "type": "user"
                    },
                    "name": "Tingyu Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:24.305Z",
                    "hidden": false
                },
                {
                    "_id": "6900350c22d452aac6dd4350",
                    "name": "Yilun Zhao",
                    "hidden": false
                },
                {
                    "_id": "6900350c22d452aac6dd4351",
                    "name": "Siyue Zhang",
                    "hidden": false
                },
                {
                    "_id": "6900350c22d452aac6dd4352",
                    "name": "Chen Zhao",
                    "hidden": false
                },
                {
                    "_id": "6900350c22d452aac6dd4353",
                    "name": "Arman Cohan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T17:19:37.000Z",
            "submittedOnDailyAt": "2025-10-28T01:44:45.429Z",
            "title": "LimRank: Less is More for Reasoning-Intensive Information Reranking",
            "submittedOnDailyBy": {
                "_id": "64dc29d9b5d625e0e9a6ecb9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
                "isPro": false,
                "fullname": "Tingyu Song",
                "user": "songtingyu",
                "type": "user"
            },
            "summary": "Existing approaches typically rely on large-scale fine-tuning to adapt LLMs\nfor information reranking tasks, which is computationally expensive. In this\nwork, we demonstrate that modern LLMs can be effectively adapted using only\nminimal, high-quality supervision. To enable this, we design\nLIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating\ndiverse, challenging, and realistic reranking examples. Using this synthetic\ndata, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two\nchallenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and\nFollowIR for instruction-following retrieval. Our experiments demonstrate that\nLIMRANK achieves competitive performance, while being trained on less than 5%\nof the data typically used in prior work. Further ablation studies demonstrate\nthe effectiveness of LIMRANK-SYNTHESIZER and the strong generalization\ncapabilities of LIMRANK across downstream tasks, including scientific\nliterature search and retrieval-augmented generation for knowledge-intensive\nproblem solving.",
            "upvotes": 8,
            "discussionId": "6900350c22d452aac6dd4354",
            "ai_summary": "LIMRANK-SYNTHESIZER generates synthetic data to fine-tune LIMRANK, achieving competitive performance with minimal supervision on information reranking tasks.",
            "ai_keywords": [
                "LIMRANK-SYNTHESIZER",
                "reranker model",
                "LIMRANK",
                "BRIGHT",
                "FollowIR",
                "scientific literature search",
                "retrieval-augmented generation"
            ]
        },
        "publishedAt": "2025-10-27T13:19:37.000Z",
        "title": "LimRank: Less is More for Reasoning-Intensive Information Reranking",
        "summary": "Existing approaches typically rely on large-scale fine-tuning to adapt LLMs\nfor information reranking tasks, which is computationally expensive. In this\nwork, we demonstrate that modern LLMs can be effectively adapted using only\nminimal, high-quality supervision. To enable this, we design\nLIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating\ndiverse, challenging, and realistic reranking examples. Using this synthetic\ndata, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two\nchallenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and\nFollowIR for instruction-following retrieval. Our experiments demonstrate that\nLIMRANK achieves competitive performance, while being trained on less than 5%\nof the data typically used in prior work. Further ablation studies demonstrate\nthe effectiveness of LIMRANK-SYNTHESIZER and the strong generalization\ncapabilities of LIMRANK across downstream tasks, including scientific\nliterature search and retrieval-augmented generation for knowledge-intensive\nproblem solving.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23544.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64dc29d9b5d625e0e9a6ecb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
            "fullname": "Tingyu Song",
            "name": "songtingyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.23272",
            "authors": [
                {
                    "_id": "690061a922d452aac6dd444a",
                    "name": "Bang Xiao",
                    "hidden": false
                },
                {
                    "_id": "690061a922d452aac6dd444b",
                    "name": "Lingjie Jiang",
                    "hidden": false
                },
                {
                    "_id": "690061a922d452aac6dd444c",
                    "name": "Shaohan Huang",
                    "hidden": false
                },
                {
                    "_id": "690061a922d452aac6dd444d",
                    "name": "Tengchao Lv",
                    "hidden": false
                },
                {
                    "_id": "690061a922d452aac6dd444e",
                    "name": "Yupan Huang",
                    "hidden": false
                },
                {
                    "_id": "690061a922d452aac6dd444f",
                    "name": "Xun Wu",
                    "hidden": false
                },
                {
                    "_id": "690061a922d452aac6dd4450",
                    "name": "Lei Cui",
                    "hidden": false
                },
                {
                    "_id": "690061a922d452aac6dd4451",
                    "name": "Furu Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T12:32:33.000Z",
            "submittedOnDailyAt": "2025-10-28T04:55:20.255Z",
            "title": "Code Aesthetics with Agentic Reward Feedback",
            "submittedOnDailyBy": {
                "_id": "66ab80e9bfb7d73a56bc293c",
                "avatarUrl": "/avatars/9644266304c832c74ef572b5eb2d9468.svg",
                "isPro": false,
                "fullname": "Jack",
                "user": "lingjie23",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have become valuable assistants for developers\nin code-related tasks. While LLMs excel at traditional programming tasks such\nas code generation and bug fixing, they struggle with visually-oriented coding\ntasks, often producing suboptimal aesthetics. In this paper, we introduce a new\npipeline to enhance the aesthetic quality of LLM-generated code. We first\nconstruct AesCode-358K, a large-scale instruction-tuning dataset focused on\ncode aesthetics. Next, we propose agentic reward feedback, a multi-agent system\nthat evaluates executability, static aesthetics, and interactive aesthetics.\nBuilding on this, we develop GRPO-AR, which integrates these signals into the\nGRPO algorithm for joint optimization of functionality and code aesthetics.\nFinally, we develop OpenDesign, a benchmark for assessing code aesthetics.\nExperimental results show that combining supervised fine-tuning on AesCode-358K\nwith reinforcement learning using agentic reward feedback significantly\nimproves performance on OpenDesign and also enhances results on existing\nbenchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o\nand GPT-4.1, and achieves performance comparable to large open-source models\nwith 480B-685B parameters, underscoring the effectiveness of our approach.",
            "upvotes": 7,
            "discussionId": "690061aa22d452aac6dd4452",
            "projectPage": "https://bangx7.github.io/code-aesthetics/",
            "ai_summary": "A new pipeline enhances the aesthetic quality of LLM-generated code through instruction-tuning, agentic reward feedback, and joint optimization, outperforming existing models.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "code aesthetics",
                "AesCode-358K",
                "agentic reward feedback",
                "multi-agent system",
                "GRPO-AR",
                "GRPO algorithm",
                "OpenDesign",
                "supervised fine-tuning",
                "reinforcement learning",
                "PandasPlotBench",
                "AesCoder-4B",
                "GPT-4o",
                "GPT-4.1"
            ]
        },
        "publishedAt": "2025-10-27T08:32:33.000Z",
        "title": "Code Aesthetics with Agentic Reward Feedback",
        "summary": "Large Language Models (LLMs) have become valuable assistants for developers\nin code-related tasks. While LLMs excel at traditional programming tasks such\nas code generation and bug fixing, they struggle with visually-oriented coding\ntasks, often producing suboptimal aesthetics. In this paper, we introduce a new\npipeline to enhance the aesthetic quality of LLM-generated code. We first\nconstruct AesCode-358K, a large-scale instruction-tuning dataset focused on\ncode aesthetics. Next, we propose agentic reward feedback, a multi-agent system\nthat evaluates executability, static aesthetics, and interactive aesthetics.\nBuilding on this, we develop GRPO-AR, which integrates these signals into the\nGRPO algorithm for joint optimization of functionality and code aesthetics.\nFinally, we develop OpenDesign, a benchmark for assessing code aesthetics.\nExperimental results show that combining supervised fine-tuning on AesCode-358K\nwith reinforcement learning using agentic reward feedback significantly\nimproves performance on OpenDesign and also enhances results on existing\nbenchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o\nand GPT-4.1, and achieves performance comparable to large open-source models\nwith 480B-685B parameters, underscoring the effectiveness of our approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23272.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66ab80e9bfb7d73a56bc293c",
            "avatarUrl": "/avatars/9644266304c832c74ef572b5eb2d9468.svg",
            "fullname": "Jack",
            "name": "lingjie23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.21003",
            "authors": [
                {
                    "_id": "68fff75322d452aac6dd4260",
                    "user": {
                        "_id": "63c7ecbcaa2a7669c45ad49e",
                        "avatarUrl": "/avatars/eba0250d328b0d850d3a6d3057bea583.svg",
                        "isPro": false,
                        "fullname": "Enshu Liu",
                        "user": "jsttlgdkycy",
                        "type": "user"
                    },
                    "name": "Enshu Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:51.770Z",
                    "hidden": false
                },
                {
                    "_id": "68fff75322d452aac6dd4261",
                    "name": "Qian Chen",
                    "hidden": false
                },
                {
                    "_id": "68fff75322d452aac6dd4262",
                    "name": "Xuefei Ning",
                    "hidden": false
                },
                {
                    "_id": "68fff75322d452aac6dd4263",
                    "name": "Shengen Yan",
                    "hidden": false
                },
                {
                    "_id": "68fff75322d452aac6dd4264",
                    "name": "Guohao Dai",
                    "hidden": false
                },
                {
                    "_id": "68fff75322d452aac6dd4265",
                    "user": {
                        "_id": "64c832a8c547ed5243d29630",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c832a8c547ed5243d29630/a46V0xOVRVknM9mUjKkTf.jpeg",
                        "isPro": false,
                        "fullname": "Zinan Lin",
                        "user": "fjxmlzn",
                        "type": "user"
                    },
                    "name": "Zinan Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:53.977Z",
                    "hidden": false
                },
                {
                    "_id": "68fff75322d452aac6dd4266",
                    "name": "Yu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T21:21:38.000Z",
            "submittedOnDailyAt": "2025-10-28T02:02:01.208Z",
            "title": "Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models\n  with Conditional Score Distillation",
            "submittedOnDailyBy": {
                "_id": "64c832a8c547ed5243d29630",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c832a8c547ed5243d29630/a46V0xOVRVknM9mUjKkTf.jpeg",
                "isPro": false,
                "fullname": "Zinan Lin",
                "user": "fjxmlzn",
                "type": "user"
            },
            "summary": "Image Auto-regressive (AR) models have emerged as a powerful paradigm of\nvisual generative models. Despite their promising performance, they suffer from\nslow generation speed due to the large number of sampling steps required.\nAlthough Distilled Decoding 1 (DD1) was recently proposed to enable few-step\nsampling for image AR models, it still incurs significant performance\ndegradation in the one-step setting, and relies on a pre-defined mapping that\nlimits its flexibility. In this work, we propose a new method, Distilled\nDecoding 2 (DD2), to further advances the feasibility of one-step sampling for\nimage AR models. Unlike DD1, DD2 does not without rely on a pre-defined\nmapping. We view the original AR model as a teacher model which provides the\nground truth conditional score in the latent embedding space at each token\nposition. Based on this, we propose a novel conditional score\ndistillation loss to train a one-step generator. Specifically, we train a\nseparate network to predict the conditional score of the generated distribution\nand apply score distillation at every token position conditioned on previous\ntokens. Experimental results show that DD2 enables one-step sampling for image\nAR models with an minimal FID increase from 3.40 to 5.43 on ImageNet-256.\nCompared to the strongest baseline DD1, DD2 reduces the gap between the\none-step sampling and original AR model by 67%, with up to 12.3times\ntraining speed-up simultaneously. DD2 takes a significant step toward the goal\nof one-step AR generation, opening up new possibilities for fast and\nhigh-quality AR modeling. Code is available at\nhttps://github.com/imagination-research/Distilled-Decoding-2.",
            "upvotes": 7,
            "discussionId": "68fff75422d452aac6dd4267",
            "ai_summary": "A new method, Distilled Decoding 2 (DD2), enables one-step sampling for image auto-regressive models with minimal performance degradation and significant speed-up compared to previous methods.",
            "ai_keywords": [
                "image auto-regressive models",
                "Distilled Decoding 1 (DD1)",
                "Distilled Decoding 2 (DD2)",
                "conditional score distillation loss",
                "latent embedding space",
                "token position",
                "one-step sampling",
                "FID",
                "ImageNet-256"
            ]
        },
        "publishedAt": "2025-10-23T17:21:38.000Z",
        "title": "Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models\n  with Conditional Score Distillation",
        "summary": "Image Auto-regressive (AR) models have emerged as a powerful paradigm of\nvisual generative models. Despite their promising performance, they suffer from\nslow generation speed due to the large number of sampling steps required.\nAlthough Distilled Decoding 1 (DD1) was recently proposed to enable few-step\nsampling for image AR models, it still incurs significant performance\ndegradation in the one-step setting, and relies on a pre-defined mapping that\nlimits its flexibility. In this work, we propose a new method, Distilled\nDecoding 2 (DD2), to further advances the feasibility of one-step sampling for\nimage AR models. Unlike DD1, DD2 does not without rely on a pre-defined\nmapping. We view the original AR model as a teacher model which provides the\nground truth conditional score in the latent embedding space at each token\nposition. Based on this, we propose a novel conditional score\ndistillation loss to train a one-step generator. Specifically, we train a\nseparate network to predict the conditional score of the generated distribution\nand apply score distillation at every token position conditioned on previous\ntokens. Experimental results show that DD2 enables one-step sampling for image\nAR models with an minimal FID increase from 3.40 to 5.43 on ImageNet-256.\nCompared to the strongest baseline DD1, DD2 reduces the gap between the\none-step sampling and original AR model by 67%, with up to 12.3times\ntraining speed-up simultaneously. DD2 takes a significant step toward the goal\nof one-step AR generation, opening up new possibilities for fast and\nhigh-quality AR modeling. Code is available at\nhttps://github.com/imagination-research/Distilled-Decoding-2.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21003.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c832a8c547ed5243d29630",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c832a8c547ed5243d29630/a46V0xOVRVknM9mUjKkTf.jpeg",
            "fullname": "Zinan Lin",
            "name": "fjxmlzn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.23571",
            "authors": [
                {
                    "_id": "6900399b22d452aac6dd4384",
                    "name": "Yash Jangir",
                    "hidden": false
                },
                {
                    "_id": "6900399b22d452aac6dd4385",
                    "name": "Yidi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6900399b22d452aac6dd4386",
                    "name": "Kashu Yamazaki",
                    "hidden": false
                },
                {
                    "_id": "6900399b22d452aac6dd4387",
                    "name": "Chenyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6900399b22d452aac6dd4388",
                    "name": "Kuan-Hsun Tu",
                    "hidden": false
                },
                {
                    "_id": "6900399b22d452aac6dd4389",
                    "name": "Tsung-Wei Ke",
                    "hidden": false
                },
                {
                    "_id": "6900399b22d452aac6dd438a",
                    "name": "Lei Ke",
                    "hidden": false
                },
                {
                    "_id": "6900399b22d452aac6dd438b",
                    "name": "Yonatan Bisk",
                    "hidden": false
                },
                {
                    "_id": "6900399b22d452aac6dd438c",
                    "name": "Katerina Fragkiadaki",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/zpl9k6gQ2jfwjzzubtCbd.mp4"
            ],
            "publishedAt": "2025-10-27T17:41:38.000Z",
            "submittedOnDailyAt": "2025-10-28T02:03:56.227Z",
            "title": "RobotArena infty: Scalable Robot Benchmarking via Real-to-Sim\n  Translation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The pursuit of robot generalists - instructable agents capable of performing\ndiverse tasks across diverse environments - demands rigorous and scalable\nevaluation. Yet real-world testing of robot policies remains fundamentally\nconstrained: it is labor-intensive, slow, unsafe at scale, and difficult to\nreproduce. Existing simulation benchmarks are similarly limited, as they train\nand test policies within the same synthetic domains and cannot assess models\ntrained from real-world demonstrations or alternative simulation environments.\nAs policies expand in scope and complexity, these barriers only intensify,\nsince defining \"success\" in robotics often hinges on nuanced human judgments of\nexecution quality. In this paper, we introduce a new benchmarking framework\nthat overcomes these challenges by shifting VLA evaluation into large-scale\nsimulated environments augmented with online human feedback. Leveraging\nadvances in vision-language models, 2D-to-3D generative modeling, and\ndifferentiable rendering, our approach automatically converts video\ndemonstrations from widely used robot datasets into simulated counterparts.\nWithin these digital twins, we assess VLA policies using both automated\nVLM-guided scoring and scalable human preference judgments collected from\ncrowdworkers, transforming human involvement from tedious scene setup,\nresetting, and safety supervision into lightweight preference comparisons. To\nmeasure robustness, we systematically perturb simulated environments along\nmultiple axes, such as textures and object placements, stress-testing policy\ngeneralization under controlled variation. The result is a continuously\nevolving, reproducible, and scalable benchmark for real-world trained robot\nmanipulation policies, addressing a critical missing capability in today's\nrobotics landscape.",
            "upvotes": 6,
            "discussionId": "6900399c22d452aac6dd438d",
            "ai_summary": "A new benchmarking framework uses large-scale simulated environments with human feedback to evaluate robot policies, addressing limitations in real-world testing and existing simulation benchmarks.",
            "ai_keywords": [
                "robot generalists",
                "instructable agents",
                "rigorous evaluation",
                "real-world testing",
                "simulation benchmarks",
                "VLA evaluation",
                "vision-language models",
                "2D-to-3D generative modeling",
                "differentiable rendering",
                "video demonstrations",
                "digital twins",
                "automated VLM-guided scoring",
                "human preference judgments",
                "crowdworkers",
                "policy generalization",
                "robustness",
                "simulated environments"
            ]
        },
        "publishedAt": "2025-10-27T13:41:38.000Z",
        "title": "RobotArena infty: Scalable Robot Benchmarking via Real-to-Sim\n  Translation",
        "summary": "The pursuit of robot generalists - instructable agents capable of performing\ndiverse tasks across diverse environments - demands rigorous and scalable\nevaluation. Yet real-world testing of robot policies remains fundamentally\nconstrained: it is labor-intensive, slow, unsafe at scale, and difficult to\nreproduce. Existing simulation benchmarks are similarly limited, as they train\nand test policies within the same synthetic domains and cannot assess models\ntrained from real-world demonstrations or alternative simulation environments.\nAs policies expand in scope and complexity, these barriers only intensify,\nsince defining \"success\" in robotics often hinges on nuanced human judgments of\nexecution quality. In this paper, we introduce a new benchmarking framework\nthat overcomes these challenges by shifting VLA evaluation into large-scale\nsimulated environments augmented with online human feedback. Leveraging\nadvances in vision-language models, 2D-to-3D generative modeling, and\ndifferentiable rendering, our approach automatically converts video\ndemonstrations from widely used robot datasets into simulated counterparts.\nWithin these digital twins, we assess VLA policies using both automated\nVLM-guided scoring and scalable human preference judgments collected from\ncrowdworkers, transforming human involvement from tedious scene setup,\nresetting, and safety supervision into lightweight preference comparisons. To\nmeasure robustness, we systematically perturb simulated environments along\nmultiple axes, such as textures and object placements, stress-testing policy\ngeneralization under controlled variation. The result is a continuously\nevolving, reproducible, and scalable benchmark for real-world trained robot\nmanipulation policies, addressing a critical missing capability in today's\nrobotics landscape.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/zpl9k6gQ2jfwjzzubtCbd.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23571.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 148
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.23479",
            "authors": [
                {
                    "_id": "6900f489646208eac0d1f0a2",
                    "name": "Xin Jin",
                    "hidden": false
                },
                {
                    "_id": "6900f489646208eac0d1f0a3",
                    "name": "Siyuan Li",
                    "hidden": false
                },
                {
                    "_id": "6900f489646208eac0d1f0a4",
                    "name": "Siyong Jian",
                    "hidden": false
                },
                {
                    "_id": "6900f489646208eac0d1f0a5",
                    "name": "Kai Yu",
                    "hidden": false
                },
                {
                    "_id": "6900f489646208eac0d1f0a6",
                    "name": "Huan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T16:12:40.000Z",
            "submittedOnDailyAt": "2025-10-28T15:42:06.985Z",
            "title": "MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal\n  Understanding",
            "submittedOnDailyBy": {
                "_id": "66aa39349238d9c3a1c7f9dc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aa39349238d9c3a1c7f9dc/mj6r7uxEYXM502x296UMf.jpeg",
                "isPro": false,
                "fullname": "Xin Jin",
                "user": "Xin1118",
                "type": "user"
            },
            "summary": "Vision-language alignment in multi-modal large language models (MLLMs)\ntypically relies on supervised fine-tuning (SFT) or reinforcement learning\n(RL). SFT is stable and efficient but requires large-scale human annotations\nand cannot capture subtle preferences, while RL brings in a reward signal for\ntraining, but suffers from overhead and instability. These limitations\nhighlight a trade-off between scalability, robustness, and alignment quality.\nTo address this, we propose MergeMix, a training-time augmentation paradigm\nthat bridges SFT and RL. It first applies an attention-aware image mixing via\ntoken merge with more cluster representation and spatial context, and then\npresents a preference-driven training paradigm for MLLMs by building preference\npairs with mixed images and raw images, and optimizing via SimPO loss. As a\nmixup augmentation, MergeMix enhances attention consistency and efficiency,\nsurpassing other heuristic-based methods in classification. Extensive\nexperiments demonstrate that MergeMix achieves competitive accuracy with\nimproved efficiency, providing a scalable approach to preference alignment in\nclassification and MLLMs.",
            "upvotes": 6,
            "discussionId": "6900f489646208eac0d1f0a7",
            "ai_summary": "MergeMix, a training-time augmentation method, combines attention-aware image mixing and preference-driven training to improve vision-language alignment in multi-modal large language models with enhanced efficiency and accuracy.",
            "ai_keywords": [
                "supervised fine-tuning",
                "reinforcement learning",
                "attention-aware image mixing",
                "token merge",
                "cluster representation",
                "spatial context",
                "preference-driven training",
                "preference pairs",
                "SimPO loss",
                "mixup augmentation",
                "attention consistency"
            ],
            "organization": {
                "_id": "67d81e5e25ad2831362ec592",
                "name": "WestlakeUniversity",
                "fullname": "Westlake University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62b624f3b52bef716e248fd7/80xTZ-_peWYICJr1JDkOw.png"
            }
        },
        "publishedAt": "2025-10-27T12:12:40.000Z",
        "title": "MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal\n  Understanding",
        "summary": "Vision-language alignment in multi-modal large language models (MLLMs)\ntypically relies on supervised fine-tuning (SFT) or reinforcement learning\n(RL). SFT is stable and efficient but requires large-scale human annotations\nand cannot capture subtle preferences, while RL brings in a reward signal for\ntraining, but suffers from overhead and instability. These limitations\nhighlight a trade-off between scalability, robustness, and alignment quality.\nTo address this, we propose MergeMix, a training-time augmentation paradigm\nthat bridges SFT and RL. It first applies an attention-aware image mixing via\ntoken merge with more cluster representation and spatial context, and then\npresents a preference-driven training paradigm for MLLMs by building preference\npairs with mixed images and raw images, and optimizing via SimPO loss. As a\nmixup augmentation, MergeMix enhances attention consistency and efficiency,\nsurpassing other heuristic-based methods in classification. Extensive\nexperiments demonstrate that MergeMix achieves competitive accuracy with\nimproved efficiency, providing a scalable approach to preference alignment in\nclassification and MLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23479.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66aa39349238d9c3a1c7f9dc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aa39349238d9c3a1c7f9dc/mj6r7uxEYXM502x296UMf.jpeg",
            "fullname": "Xin Jin",
            "name": "Xin1118",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "67d81e5e25ad2831362ec592",
            "name": "WestlakeUniversity",
            "fullname": "Westlake University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62b624f3b52bef716e248fd7/80xTZ-_peWYICJr1JDkOw.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.23595",
            "authors": [
                {
                    "_id": "69012426646208eac0d1f15a",
                    "name": "Yixing Chen",
                    "hidden": false
                },
                {
                    "_id": "69012426646208eac0d1f15b",
                    "name": "Yiding Wang",
                    "hidden": false
                },
                {
                    "_id": "69012426646208eac0d1f15c",
                    "name": "Siqi Zhu",
                    "hidden": false
                },
                {
                    "_id": "69012426646208eac0d1f15d",
                    "name": "Haofei Yu",
                    "hidden": false
                },
                {
                    "_id": "69012426646208eac0d1f15e",
                    "name": "Tao Feng",
                    "hidden": false
                },
                {
                    "_id": "69012426646208eac0d1f15f",
                    "name": "Muhan Zhan",
                    "hidden": false
                },
                {
                    "_id": "69012426646208eac0d1f160",
                    "name": "Mostofa Patwary",
                    "hidden": false
                },
                {
                    "_id": "69012426646208eac0d1f161",
                    "name": "Jiaxuan You",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T17:58:02.000Z",
            "submittedOnDailyAt": "2025-10-28T18:45:24.506Z",
            "title": "Multi-Agent Evolve: LLM Self-Improve through Co-evolution",
            "submittedOnDailyBy": {
                "_id": "65621fd68631d43d2baf33b2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1JemNCnPkS1mE3SNygsE2.png",
                "isPro": false,
                "fullname": "siqi zhu",
                "user": "zsqzz",
                "type": "user"
            },
            "summary": "Reinforcement Learning (RL) has demonstrated significant potential in\nenhancing the reasoning capabilities of large language models (LLMs). However,\nthe success of RL for LLMs heavily relies on human-curated datasets and\nverifiable rewards, which limit their scalability and generality. Recent\nSelf-Play RL methods, inspired by the success of the paradigm in games and Go,\naim to enhance LLM reasoning capabilities without human-annotated data.\nHowever, their methods primarily depend on a grounded environment for feedback\n(e.g., a Python interpreter or a game engine); extending them to general\ndomains remains challenging. To address these challenges, we propose\nMulti-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in\nsolving diverse tasks, including mathematics, reasoning, and general knowledge\nQ&A. The core design of MAE is based on a triplet of interacting agents\n(Proposer, Solver, Judge) that are instantiated from a single LLM, and applies\nreinforcement learning to optimize their behaviors. The Proposer generates\nquestions, the Solver attempts solutions, and the Judge evaluates both while\nco-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves\nan average improvement of 4.54% on multiple benchmarks. These results highlight\nMAE as a scalable, data-efficient method for enhancing the general reasoning\nabilities of LLMs with minimal reliance on human-curated supervision.",
            "upvotes": 5,
            "discussionId": "69012427646208eac0d1f162",
            "ai_summary": "Multi-Agent Evolve (MAE) framework uses reinforcement learning to enhance LLM reasoning across diverse tasks with minimal human supervision.",
            "ai_keywords": [
                "Reinforcement Learning (RL)",
                "large language models (LLMs)",
                "Self-Play RL",
                "grounded environment",
                "Multi-Agent Evolve (MAE)",
                "Proposer",
                "Solver",
                "Judge",
                "co-evolving",
                "Qwen2.5-3B-Instruct",
                "general reasoning abilities"
            ],
            "organization": {
                "_id": "65448bef5b5d9185ba3202b9",
                "name": "UIUC-CS",
                "fullname": "University of Illinois at Urbana-Champaign",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
            }
        },
        "publishedAt": "2025-10-27T13:58:02.000Z",
        "title": "Multi-Agent Evolve: LLM Self-Improve through Co-evolution",
        "summary": "Reinforcement Learning (RL) has demonstrated significant potential in\nenhancing the reasoning capabilities of large language models (LLMs). However,\nthe success of RL for LLMs heavily relies on human-curated datasets and\nverifiable rewards, which limit their scalability and generality. Recent\nSelf-Play RL methods, inspired by the success of the paradigm in games and Go,\naim to enhance LLM reasoning capabilities without human-annotated data.\nHowever, their methods primarily depend on a grounded environment for feedback\n(e.g., a Python interpreter or a game engine); extending them to general\ndomains remains challenging. To address these challenges, we propose\nMulti-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in\nsolving diverse tasks, including mathematics, reasoning, and general knowledge\nQ&A. The core design of MAE is based on a triplet of interacting agents\n(Proposer, Solver, Judge) that are instantiated from a single LLM, and applies\nreinforcement learning to optimize their behaviors. The Proposer generates\nquestions, the Solver attempts solutions, and the Judge evaluates both while\nco-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves\nan average improvement of 4.54% on multiple benchmarks. These results highlight\nMAE as a scalable, data-efficient method for enhancing the general reasoning\nabilities of LLMs with minimal reliance on human-curated supervision.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23595.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65621fd68631d43d2baf33b2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1JemNCnPkS1mE3SNygsE2.png",
            "fullname": "siqi zhu",
            "name": "zsqzz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "65448bef5b5d9185ba3202b9",
            "name": "UIUC-CS",
            "fullname": "University of Illinois at Urbana-Champaign",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.23594",
            "authors": [
                {
                    "_id": "6900326e22d452aac6dd4314",
                    "name": "Yusu Qian",
                    "hidden": false
                },
                {
                    "_id": "6900326e22d452aac6dd4315",
                    "name": "Cheng Wan",
                    "hidden": false
                },
                {
                    "_id": "6900326e22d452aac6dd4316",
                    "name": "Chao Jia",
                    "hidden": false
                },
                {
                    "_id": "6900326e22d452aac6dd4317",
                    "name": "Yinfei Yang",
                    "hidden": false
                },
                {
                    "_id": "6900326e22d452aac6dd4318",
                    "name": "Qingyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "6900326e22d452aac6dd4319",
                    "name": "Zhe Gan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T17:57:52.000Z",
            "submittedOnDailyAt": "2025-10-28T01:33:31.648Z",
            "title": "PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error\n  Detection",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce PRISM-Bench, a benchmark of puzzle-based visual\nchallenges designed to evaluate not only whether models can solve problems, but\nhow their reasoning unfolds. Unlike prior evaluations that measure only\nfinal-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual\npuzzle and a step-by-step chain-of-thought (CoT) containing exactly one error,\nmodels must identify the first incorrect step. This setting enables\nfine-grained assessment of logical consistency, error detection, and visual\nreasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric,\nand analogical reasoning, resisting shortcuts based on superficial pattern\nmatching. Evaluations across state-of-the-art MLLMs reveal a persistent gap\nbetween fluent generation and faithful reasoning: models that produce plausible\nCoTs often fail to locate simple logical faults. By disentangling answer\ngeneration from reasoning verification, PRISM-Bench offers a sharper lens on\nmultimodal reasoning competence and underscores the need for diagnostic\nevaluation protocols in the development of trustworthy MLLMs.",
            "upvotes": 4,
            "discussionId": "6900326e22d452aac6dd431a",
            "githubRepo": "https://github.com/JornyWan/PRISM-Bench",
            "ai_summary": "PRISM-Bench evaluates models' reasoning processes by identifying errors in step-by-step solutions to visual puzzles, highlighting gaps between fluent generation and logical consistency.",
            "ai_keywords": [
                "PRISM-Bench",
                "chain-of-thought",
                "logical consistency",
                "error detection",
                "visual reasoning",
                "multi-step reasoning",
                "symbolic reasoning",
                "geometric reasoning",
                "analogical reasoning",
                "MLLMs",
                "multimodal reasoning"
            ],
            "githubStars": 7,
            "organization": {
                "_id": "628cbd99ef14f971b69948ab",
                "name": "apple",
                "fullname": "Apple",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
            }
        },
        "publishedAt": "2025-10-27T13:57:52.000Z",
        "title": "PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error\n  Detection",
        "summary": "We introduce PRISM-Bench, a benchmark of puzzle-based visual\nchallenges designed to evaluate not only whether models can solve problems, but\nhow their reasoning unfolds. Unlike prior evaluations that measure only\nfinal-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual\npuzzle and a step-by-step chain-of-thought (CoT) containing exactly one error,\nmodels must identify the first incorrect step. This setting enables\nfine-grained assessment of logical consistency, error detection, and visual\nreasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric,\nand analogical reasoning, resisting shortcuts based on superficial pattern\nmatching. Evaluations across state-of-the-art MLLMs reveal a persistent gap\nbetween fluent generation and faithful reasoning: models that produce plausible\nCoTs often fail to locate simple logical faults. By disentangling answer\ngeneration from reasoning verification, PRISM-Bench offers a sharper lens on\nmultimodal reasoning competence and underscores the need for diagnostic\nevaluation protocols in the development of trustworthy MLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23594.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 148
        },
        "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.22975",
            "authors": [
                {
                    "_id": "690051e222d452aac6dd4400",
                    "name": "Rishit Dagli",
                    "hidden": false
                },
                {
                    "_id": "690051e222d452aac6dd4401",
                    "name": "Donglai Xiang",
                    "hidden": false
                },
                {
                    "_id": "690051e222d452aac6dd4402",
                    "name": "Vismay Modi",
                    "hidden": false
                },
                {
                    "_id": "690051e222d452aac6dd4403",
                    "name": "Charles Loop",
                    "hidden": false
                },
                {
                    "_id": "690051e222d452aac6dd4404",
                    "name": "Clement Fuji Tsang",
                    "hidden": false
                },
                {
                    "_id": "690051e222d452aac6dd4405",
                    "name": "Anka He Chen",
                    "hidden": false
                },
                {
                    "_id": "690051e222d452aac6dd4406",
                    "name": "Anita Hu",
                    "hidden": false
                },
                {
                    "_id": "690051e222d452aac6dd4407",
                    "name": "Gavriel State",
                    "hidden": false
                },
                {
                    "_id": "690051e222d452aac6dd4408",
                    "name": "David I. W. Levin",
                    "hidden": false
                },
                {
                    "_id": "690051e222d452aac6dd4409",
                    "name": "Maria Shugrina",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T03:56:25.000Z",
            "submittedOnDailyAt": "2025-10-28T04:02:14.537Z",
            "title": "VoMP: Predicting Volumetric Mechanical Property Fields",
            "submittedOnDailyBy": {
                "_id": "60796959c59d9e1697fa2324",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
                "isPro": false,
                "fullname": "Rishit Dagli",
                "user": "rishitdagli",
                "type": "user"
            },
            "summary": "Physical simulation relies on spatially-varying mechanical properties, often\nlaboriously hand-crafted. VoMP is a feed-forward method trained to predict\nYoung's modulus (E), Poisson's ratio (nu), and density (rho) throughout\nthe volume of 3D objects, in any representation that can be rendered and\nvoxelized. VoMP aggregates per-voxel multi-view features and passes them to our\ntrained Geometry Transformer to predict per-voxel material latent codes. These\nlatents reside on a manifold of physically plausible materials, which we learn\nfrom a real-world dataset, guaranteeing the validity of decoded per-voxel\nmaterials. To obtain object-level training data, we propose an annotation\npipeline combining knowledge from segmented 3D datasets, material databases,\nand a vision-language model, along with a new benchmark. Experiments show that\nVoMP estimates accurate volumetric properties, far outperforming prior art in\naccuracy and speed.",
            "upvotes": 4,
            "discussionId": "690051e222d452aac6dd440a",
            "projectPage": "https://research.nvidia.com/labs/sil/projects/vomp",
            "ai_summary": "VoMP uses a feed-forward method with a Geometry Transformer to predict accurate volumetric material properties from 3D objects, outperforming existing methods in both accuracy and speed.",
            "ai_keywords": [
                "feed-forward method",
                "Young's modulus",
                "Poisson's ratio",
                "density",
                "3D objects",
                "voxelization",
                "per-voxel multi-view features",
                "Geometry Transformer",
                "material latent codes",
                "manifold of physically plausible materials",
                "annotation pipeline",
                "segmented 3D datasets",
                "material databases",
                "vision-language model",
                "benchmark"
            ],
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-10-26T23:56:25.000Z",
        "title": "VoMP: Predicting Volumetric Mechanical Property Fields",
        "summary": "Physical simulation relies on spatially-varying mechanical properties, often\nlaboriously hand-crafted. VoMP is a feed-forward method trained to predict\nYoung's modulus (E), Poisson's ratio (nu), and density (rho) throughout\nthe volume of 3D objects, in any representation that can be rendered and\nvoxelized. VoMP aggregates per-voxel multi-view features and passes them to our\ntrained Geometry Transformer to predict per-voxel material latent codes. These\nlatents reside on a manifold of physically plausible materials, which we learn\nfrom a real-world dataset, guaranteeing the validity of decoded per-voxel\nmaterials. To obtain object-level training data, we propose an annotation\npipeline combining knowledge from segmented 3D datasets, material databases,\nand a vision-language model, along with a new benchmark. Experiments show that\nVoMP estimates accurate volumetric properties, far outperforming prior art in\naccuracy and speed.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22975.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60796959c59d9e1697fa2324",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
            "fullname": "Rishit Dagli",
            "name": "rishitdagli",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.22907",
            "authors": [
                {
                    "_id": "6900310d22d452aac6dd4305",
                    "user": {
                        "_id": "647bf082aba7062fe5c51ca9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
                        "isPro": false,
                        "fullname": "Yifan Zhang",
                        "user": "yifAI",
                        "type": "user"
                    },
                    "name": "Yifan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:38.913Z",
                    "hidden": false
                },
                {
                    "_id": "6900310d22d452aac6dd4306",
                    "name": "Lanser Contributors",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T01:25:20.000Z",
            "submittedOnDailyAt": "2025-10-28T01:28:47.140Z",
            "title": "Language Server CLI Empowers Language Agents with Process Rewards",
            "submittedOnDailyBy": {
                "_id": "647bf082aba7062fe5c51ca9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
                "isPro": false,
                "fullname": "Yifan Zhang",
                "user": "yifAI",
                "type": "user"
            },
            "summary": "Large language models routinely hallucinate APIs and mislocalize edits, while\nlanguage servers compute verified, IDE-grade facts about real code. We present\nLanser-CLI, a CLI-first orchestration layer that pins and mediates a Language\nServer Protocol (LSP) server for coding agents and CI, exposing deterministic,\nreplayable workflows. Our position is that language servers provide not only\nstructural information (definitions, references, types, diagnostics) but also\nan actionable process reward: machine-checked, step-wise signals that align an\nagent's planning loop with program reality. In this work, Lanser-CLI\ncontributes: (i) a robust addressing scheme beyond brittle \"file:line:col\" via\na Selector DSL (symbolic, AST-path, and content-anchored selectors) with a\nprincipled relocation algorithm; (ii) deterministic Analysis Bundles that\nnormalize Language Server responses and capture environment/capability metadata\nwith stable content hashes; (iii) a safety envelope for mutating operations\n(rename, code actions) with preview, workspace jails, and Git-aware,\ntransactional apply; and (iv) a process-reward functional derived from Language\nServer facts (diagnostic deltas, disambiguation confidence, and safe-apply\nchecks) that is computable online and replayable offline. We formalize\ndeterminism under frozen snapshots and establish a monotonicity property for\nthe process reward, making it suitable for process supervision and\ncounterfactual analysis. Project Page:\nhttps://github.com/yifanzhang-pro/lanser-cli",
            "upvotes": 4,
            "discussionId": "6900310d22d452aac6dd4307",
            "ai_summary": "Lanser-CLI orchestrates Language Server Protocol servers for coding agents and CI, providing deterministic workflows and actionable process rewards based on verified code facts.",
            "ai_keywords": [
                "Language Server Protocol",
                "LSP",
                "coding agents",
                "CI",
                "Selector DSL",
                "AST-path",
                "content-anchored selectors",
                "Analysis Bundles",
                "safety envelope",
                "preview",
                "workspace jails",
                "Git-aware",
                "transactional apply",
                "diagnostic deltas",
                "disambiguation confidence",
                "safe-apply checks",
                "determinism",
                "frozen snapshots",
                "monotonicity property",
                "process supervision",
                "counterfactual analysis"
            ]
        },
        "publishedAt": "2025-10-26T21:25:20.000Z",
        "title": "Language Server CLI Empowers Language Agents with Process Rewards",
        "summary": "Large language models routinely hallucinate APIs and mislocalize edits, while\nlanguage servers compute verified, IDE-grade facts about real code. We present\nLanser-CLI, a CLI-first orchestration layer that pins and mediates a Language\nServer Protocol (LSP) server for coding agents and CI, exposing deterministic,\nreplayable workflows. Our position is that language servers provide not only\nstructural information (definitions, references, types, diagnostics) but also\nan actionable process reward: machine-checked, step-wise signals that align an\nagent's planning loop with program reality. In this work, Lanser-CLI\ncontributes: (i) a robust addressing scheme beyond brittle \"file:line:col\" via\na Selector DSL (symbolic, AST-path, and content-anchored selectors) with a\nprincipled relocation algorithm; (ii) deterministic Analysis Bundles that\nnormalize Language Server responses and capture environment/capability metadata\nwith stable content hashes; (iii) a safety envelope for mutating operations\n(rename, code actions) with preview, workspace jails, and Git-aware,\ntransactional apply; and (iv) a process-reward functional derived from Language\nServer facts (diagnostic deltas, disambiguation confidence, and safe-apply\nchecks) that is computable online and replayable offline. We formalize\ndeterminism under frozen snapshots and establish a monotonicity property for\nthe process reward, making it suitable for process supervision and\ncounterfactual analysis. Project Page:\nhttps://github.com/yifanzhang-pro/lanser-cli",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22907.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "647bf082aba7062fe5c51ca9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
            "fullname": "Yifan Zhang",
            "name": "yifAI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 16
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.22849",
            "authors": [
                {
                    "_id": "69004a3922d452aac6dd43ec",
                    "name": "Adam Stein",
                    "hidden": false
                },
                {
                    "_id": "69004a3922d452aac6dd43ed",
                    "name": "Neelay Velingker",
                    "hidden": false
                },
                {
                    "_id": "69004a3922d452aac6dd43ee",
                    "name": "Mayur Naik",
                    "hidden": false
                },
                {
                    "_id": "69004a3922d452aac6dd43ef",
                    "name": "Eric Wong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-26T21:58:33.000Z",
            "submittedOnDailyAt": "2025-10-28T13:34:20.856Z",
            "title": "Once Upon an Input: Reasoning via Per-Instance Program Synthesis",
            "submittedOnDailyBy": {
                "_id": "64b6d0052662b49f164de3b3",
                "avatarUrl": "/avatars/12e70c77c3bb0698cb64a878ff72de21.svg",
                "isPro": false,
                "fullname": "Adam Stein",
                "user": "steinad",
                "type": "user"
            },
            "summary": "Large language models (LLMs) excel at zero-shot inference but continue to\nstruggle with complex, multi-step reasoning. Recent methods that augment LLMs\nwith intermediate reasoning steps such as Chain of Thought (CoT) and Program of\nThought (PoT) improve performance but often produce undesirable solutions,\nespecially in algorithmic domains. We introduce Per-Instance Program Synthesis\n(PIPS), a method that generates and refines programs at the instance-level\nusing structural feedback without relying on task-specific guidance or explicit\ntest cases. To further improve performance, PIPS incorporates a confidence\nmetric that dynamically chooses between direct inference and program synthesis\non a per-instance basis. Experiments across three frontier LLMs and 30\nbenchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question\nanswering tasks, relational reasoning tasks, and mathematical reasoning tasks\nshow that PIPS improves the absolute harmonic mean accuracy by up to 8.6% and\n9.4% compared to PoT and CoT respectively, and reduces undesirable program\ngenerations by 65.1% on the algorithmic tasks compared to PoT with\nGemini-2.0-Flash.",
            "upvotes": 3,
            "discussionId": "69004a3922d452aac6dd43f0",
            "githubRepo": "https://github.com/adaminsky/pips",
            "ai_summary": "Per-Instance Program Synthesis (PIPS) enhances LLM performance by generating and refining instance-level programs with structural feedback, improving accuracy and reducing undesirable solutions.",
            "ai_keywords": [
                "Large language models (LLMs)",
                "zero-shot inference",
                "complex",
                "multi-step reasoning",
                "Chain of Thought (CoT)",
                "Program of Thought (PoT)",
                "Per-Instance Program Synthesis (PIPS)",
                "structural feedback",
                "confidence metric",
                "Big Bench Extra Hard (BBEH)",
                "visual question answering",
                "relational reasoning",
                "mathematical reasoning",
                "harmonic mean accuracy"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-10-26T17:58:33.000Z",
        "title": "Once Upon an Input: Reasoning via Per-Instance Program Synthesis",
        "summary": "Large language models (LLMs) excel at zero-shot inference but continue to\nstruggle with complex, multi-step reasoning. Recent methods that augment LLMs\nwith intermediate reasoning steps such as Chain of Thought (CoT) and Program of\nThought (PoT) improve performance but often produce undesirable solutions,\nespecially in algorithmic domains. We introduce Per-Instance Program Synthesis\n(PIPS), a method that generates and refines programs at the instance-level\nusing structural feedback without relying on task-specific guidance or explicit\ntest cases. To further improve performance, PIPS incorporates a confidence\nmetric that dynamically chooses between direct inference and program synthesis\non a per-instance basis. Experiments across three frontier LLMs and 30\nbenchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question\nanswering tasks, relational reasoning tasks, and mathematical reasoning tasks\nshow that PIPS improves the absolute harmonic mean accuracy by up to 8.6% and\n9.4% compared to PoT and CoT respectively, and reduces undesirable program\ngenerations by 65.1% on the algorithmic tasks compared to PoT with\nGemini-2.0-Flash.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22849.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64b6d0052662b49f164de3b3",
            "avatarUrl": "/avatars/12e70c77c3bb0698cb64a878ff72de21.svg",
            "fullname": "Adam Stein",
            "name": "steinad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.22236",
            "authors": [
                {
                    "_id": "6900a6a5646208eac0d1efb1",
                    "name": "Kunyang Zhou",
                    "hidden": false
                },
                {
                    "_id": "6900a6a5646208eac0d1efb2",
                    "name": "Yeqin Shao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-25T09:42:49.000Z",
            "submittedOnDailyAt": "2025-10-28T09:50:52.783Z",
            "title": "DiffusionLane: Diffusion Model for Lane Detection",
            "submittedOnDailyBy": {
                "_id": "6538e13b8838e131acd4323e",
                "avatarUrl": "/avatars/052aa5d27542c617ec8d58b457b689a1.svg",
                "isPro": false,
                "fullname": "Zhou Kunyang",
                "user": "zkyseus",
                "type": "user"
            },
            "summary": "In this paper, we present a novel diffusion-based model for lane detection,\ncalled DiffusionLane, which treats the lane detection task as a denoising\ndiffusion process in the parameter space of the lane. Firstly, we add the\nGaussian noise to the parameters (the starting point and the angle) of ground\ntruth lanes to obtain noisy lane anchors, and the model learns to refine the\nnoisy lane anchors in a progressive way to obtain the target lanes. Secondly,\nwe propose a hybrid decoding strategy to address the poor feature\nrepresentation of the encoder, resulting from the noisy lane anchors.\nSpecifically, we design a hybrid diffusion decoder to combine global-level and\nlocal-level decoders for high-quality lane anchors. Then, to improve the\nfeature representation of the encoder, we employ an auxiliary head in the\ntraining stage to adopt the learnable lane anchors for enriching the\nsupervision on the encoder. Experimental results on four benchmarks, Carlane,\nTusimple, CULane, and LLAMAS, show that DiffusionLane possesses a strong\ngeneralization ability and promising detection performance compared to the\nprevious state-of-the-art methods. For example, DiffusionLane with ResNet18\nsurpasses the existing methods by at least 1\\% accuracy on the domain\nadaptation dataset Carlane. Besides, DiffusionLane with MobileNetV4 gets\n81.32\\% F1 score on CULane, 96.89\\% accuracy on Tusimple with ResNet34, and\n97.59\\% F1 score on LLAMAS with ResNet101. Code will be available at\nhttps://github.com/zkyntu/UnLanedet.",
            "upvotes": 3,
            "discussionId": "6900a6a6646208eac0d1efb3",
            "ai_summary": "DiffusionLane, a diffusion-based model, enhances lane detection by refining noisy lane anchors through a hybrid diffusion decoder and auxiliary head, achieving superior performance across multiple benchmarks.",
            "ai_keywords": [
                "diffusion-based model",
                "denoising diffusion process",
                "parameter space",
                "Gaussian noise",
                "noisy lane anchors",
                "hybrid decoding strategy",
                "global-level decoders",
                "local-level decoders",
                "high-quality lane anchors",
                "feature representation",
                "encoder",
                "auxiliary head",
                "domain adaptation",
                "Carlane",
                "Tusimple",
                "CULane",
                "LLAMAS",
                "ResNet18",
                "MobileNetV4",
                "ResNet34",
                "ResNet101"
            ],
            "organization": {
                "_id": "66b82187088299999a551ab8",
                "name": "NanTongUniversity",
                "fullname": "NanTong University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b820d76f861799b88c7874/PRfA6kuNj-aDW6nUp81wn.png"
            }
        },
        "publishedAt": "2025-10-25T05:42:49.000Z",
        "title": "DiffusionLane: Diffusion Model for Lane Detection",
        "summary": "In this paper, we present a novel diffusion-based model for lane detection,\ncalled DiffusionLane, which treats the lane detection task as a denoising\ndiffusion process in the parameter space of the lane. Firstly, we add the\nGaussian noise to the parameters (the starting point and the angle) of ground\ntruth lanes to obtain noisy lane anchors, and the model learns to refine the\nnoisy lane anchors in a progressive way to obtain the target lanes. Secondly,\nwe propose a hybrid decoding strategy to address the poor feature\nrepresentation of the encoder, resulting from the noisy lane anchors.\nSpecifically, we design a hybrid diffusion decoder to combine global-level and\nlocal-level decoders for high-quality lane anchors. Then, to improve the\nfeature representation of the encoder, we employ an auxiliary head in the\ntraining stage to adopt the learnable lane anchors for enriching the\nsupervision on the encoder. Experimental results on four benchmarks, Carlane,\nTusimple, CULane, and LLAMAS, show that DiffusionLane possesses a strong\ngeneralization ability and promising detection performance compared to the\nprevious state-of-the-art methods. For example, DiffusionLane with ResNet18\nsurpasses the existing methods by at least 1\\% accuracy on the domain\nadaptation dataset Carlane. Besides, DiffusionLane with MobileNetV4 gets\n81.32\\% F1 score on CULane, 96.89\\% accuracy on Tusimple with ResNet34, and\n97.59\\% F1 score on LLAMAS with ResNet101. Code will be available at\nhttps://github.com/zkyntu/UnLanedet.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22236.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6538e13b8838e131acd4323e",
            "avatarUrl": "/avatars/052aa5d27542c617ec8d58b457b689a1.svg",
            "fullname": "Zhou Kunyang",
            "name": "zkyseus",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "66b82187088299999a551ab8",
            "name": "NanTongUniversity",
            "fullname": "NanTong University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b820d76f861799b88c7874/PRfA6kuNj-aDW6nUp81wn.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.20512",
            "authors": [
                {
                    "_id": "68faef44f158a71c5a2f5894",
                    "user": {
                        "_id": "64b67f8a7bdf72299e4ce1e6",
                        "avatarUrl": "/avatars/73fd7ba62a329002a3c544169c4d3b7b.svg",
                        "isPro": false,
                        "fullname": "YYX",
                        "user": "liulisixin",
                        "type": "user"
                    },
                    "name": "Yixiong Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:38:18.911Z",
                    "hidden": false
                },
                {
                    "_id": "68faef44f158a71c5a2f5895",
                    "name": "Tao Wu",
                    "hidden": false
                },
                {
                    "_id": "68faef44f158a71c5a2f5896",
                    "name": "Senmao Li",
                    "hidden": false
                },
                {
                    "_id": "68faef44f158a71c5a2f5897",
                    "name": "Shiqi Yang",
                    "hidden": false
                },
                {
                    "_id": "68faef44f158a71c5a2f5898",
                    "name": "Yaxing Wang",
                    "hidden": false
                },
                {
                    "_id": "68faef44f158a71c5a2f5899",
                    "name": "Joost van de Weijer",
                    "hidden": false
                },
                {
                    "_id": "68faef44f158a71c5a2f589a",
                    "name": "Kai Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T12:56:33.000Z",
            "submittedOnDailyAt": "2025-10-28T07:59:51.293Z",
            "title": "EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion\n  Personalization",
            "submittedOnDailyBy": {
                "_id": "637e1cf4f09bf2498c543a73",
                "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
                "isPro": false,
                "fullname": "Senmao Li",
                "user": "senmaonk",
                "type": "user"
            },
            "summary": "Recent advances in accelerating text-to-image (T2I) diffusion models have\nenabled the synthesis of high-fidelity images even in a single step. However,\npersonalizing these models to incorporate novel concepts remains a challenge\ndue to the limited capacity of one-step models to capture new concept\ndistributions effectively. We propose a bidirectional concept distillation\nframework, EchoDistill, to enable one-step diffusion personalization (1-SDP).\nOur approach involves an end-to-end training process where a multi-step\ndiffusion model (teacher) and a one-step diffusion model (student) are trained\nsimultaneously. The concept is first distilled from the teacher model to the\nstudent, and then echoed back from the student to the teacher. During the\nEchoDistill, we share the text encoder between the two models to ensure\nconsistent semantic understanding. Following this, the student model is\noptimized with adversarial losses to align with the real image distribution and\nwith alignment losses to maintain consistency with the teacher's output.\nFurthermore, we introduce the bidirectional echoing refinement strategy,\nwherein the student model leverages its faster generation capability to\nfeedback to the teacher model. This bidirectional concept distillation\nmechanism not only enhances the student ability to personalize novel concepts\nbut also improves the generative quality of the teacher model. Our experiments\ndemonstrate that this collaborative framework significantly outperforms\nexisting personalization methods over the 1-SDP setup, establishing a novel\nparadigm for rapid and effective personalization in T2I diffusion models.",
            "upvotes": 3,
            "discussionId": "68faef45f158a71c5a2f589b",
            "ai_summary": "A bidirectional concept distillation framework enhances one-step text-to-image diffusion models by leveraging a multi-step model, improving personalization and generative quality.",
            "ai_keywords": [
                "text-to-image (T2I) diffusion models",
                "one-step diffusion personalization (1-SDP)",
                "bidirectional concept distillation",
                "multi-step diffusion model",
                "text encoder",
                "adversarial losses",
                "alignment losses",
                "bidirectional echoing refinement strategy"
            ]
        },
        "publishedAt": "2025-10-23T08:56:33.000Z",
        "title": "EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion\n  Personalization",
        "summary": "Recent advances in accelerating text-to-image (T2I) diffusion models have\nenabled the synthesis of high-fidelity images even in a single step. However,\npersonalizing these models to incorporate novel concepts remains a challenge\ndue to the limited capacity of one-step models to capture new concept\ndistributions effectively. We propose a bidirectional concept distillation\nframework, EchoDistill, to enable one-step diffusion personalization (1-SDP).\nOur approach involves an end-to-end training process where a multi-step\ndiffusion model (teacher) and a one-step diffusion model (student) are trained\nsimultaneously. The concept is first distilled from the teacher model to the\nstudent, and then echoed back from the student to the teacher. During the\nEchoDistill, we share the text encoder between the two models to ensure\nconsistent semantic understanding. Following this, the student model is\noptimized with adversarial losses to align with the real image distribution and\nwith alignment losses to maintain consistency with the teacher's output.\nFurthermore, we introduce the bidirectional echoing refinement strategy,\nwherein the student model leverages its faster generation capability to\nfeedback to the teacher model. This bidirectional concept distillation\nmechanism not only enhances the student ability to personalize novel concepts\nbut also improves the generative quality of the teacher model. Our experiments\ndemonstrate that this collaborative framework significantly outperforms\nexisting personalization methods over the 1-SDP setup, establishing a novel\nparadigm for rapid and effective personalization in T2I diffusion models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20512.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "637e1cf4f09bf2498c543a73",
            "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
            "fullname": "Senmao Li",
            "name": "senmaonk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.16320",
            "authors": [
                {
                    "_id": "6900c98e646208eac0d1f035",
                    "name": "Wenhao Wang",
                    "hidden": false
                },
                {
                    "_id": "6900c98e646208eac0d1f036",
                    "name": "Longqi Cai",
                    "hidden": false
                },
                {
                    "_id": "6900c98e646208eac0d1f037",
                    "name": "Taihong Xiao",
                    "hidden": false
                },
                {
                    "_id": "6900c98e646208eac0d1f038",
                    "name": "Yuxiao Wang",
                    "hidden": false
                },
                {
                    "_id": "6900c98e646208eac0d1f039",
                    "name": "Ming-Hsuan Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-18T03:08:10.000Z",
            "submittedOnDailyAt": "2025-10-28T12:18:53.749Z",
            "title": "Scaling Laws for Deepfake Detection",
            "submittedOnDailyBy": {
                "_id": "62b32a4429a410b7f6b06710",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b32a4429a410b7f6b06710/VzgvmnlYZWuifZTkIkCxy.jpeg",
                "isPro": false,
                "fullname": "Wenhao Wang",
                "user": "WenhaoWang",
                "type": "user"
            },
            "summary": "This paper presents a systematic study of scaling laws for the deepfake\ndetection task. Specifically, we analyze the model performance against the\nnumber of real image domains, deepfake generation methods, and training images.\nSince no existing dataset meets the scale requirements for this research, we\nconstruct ScaleDF, the largest dataset to date in this field, which contains\nover 5.8 million real images from 51 different datasets (domains) and more than\n8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we\nobserve power-law scaling similar to that shown in large language models\n(LLMs). Specifically, the average detection error follows a predictable\npower-law decay as either the number of real domains or the number of deepfake\nmethods increases. This key observation not only allows us to forecast the\nnumber of additional real domains or deepfake methods required to reach a\ntarget performance, but also inspires us to counter the evolving deepfake\ntechnology in a data-centric manner. Beyond this, we examine the role of\npre-training and data augmentations in deepfake detection under scaling, as\nwell as the limitations of scaling itself.",
            "upvotes": 3,
            "discussionId": "6900c98f646208eac0d1f03a",
            "ai_summary": "Research on scaling laws in deepfake detection using the largest dataset to date reveals power-law scaling similar to large language models, enabling performance forecasting and data-centric countermeasures against evolving deepfake technology.",
            "ai_keywords": [
                "deepfake detection",
                "scaling laws",
                "power-law scaling",
                "large language models",
                "pre-training",
                "data augmentations"
            ]
        },
        "publishedAt": "2025-10-17T23:08:10.000Z",
        "title": "Scaling Laws for Deepfake Detection",
        "summary": "This paper presents a systematic study of scaling laws for the deepfake\ndetection task. Specifically, we analyze the model performance against the\nnumber of real image domains, deepfake generation methods, and training images.\nSince no existing dataset meets the scale requirements for this research, we\nconstruct ScaleDF, the largest dataset to date in this field, which contains\nover 5.8 million real images from 51 different datasets (domains) and more than\n8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we\nobserve power-law scaling similar to that shown in large language models\n(LLMs). Specifically, the average detection error follows a predictable\npower-law decay as either the number of real domains or the number of deepfake\nmethods increases. This key observation not only allows us to forecast the\nnumber of additional real domains or deepfake methods required to reach a\ntarget performance, but also inspires us to counter the evolving deepfake\ntechnology in a data-centric manner. Beyond this, we examine the role of\npre-training and data augmentations in deepfake detection under scaling, as\nwell as the limitations of scaling itself.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16320.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62b32a4429a410b7f6b06710",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b32a4429a410b7f6b06710/VzgvmnlYZWuifZTkIkCxy.jpeg",
            "fullname": "Wenhao Wang",
            "name": "WenhaoWang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.07723",
            "authors": [
                {
                    "_id": "6900cd53646208eac0d1f043",
                    "user": {
                        "_id": "66c4773aea476bea05fd4d95",
                        "avatarUrl": "/avatars/59547221df76fbabba385ac28f06ff86.svg",
                        "isPro": false,
                        "fullname": "wenyue Chen",
                        "user": "xishushu",
                        "type": "user"
                    },
                    "name": "Wenyue Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T14:19:50.569Z",
                    "hidden": false
                },
                {
                    "_id": "6900cd53646208eac0d1f044",
                    "name": "Peng Li",
                    "hidden": false
                },
                {
                    "_id": "6900cd53646208eac0d1f045",
                    "name": "Wangguandong Zheng",
                    "hidden": false
                },
                {
                    "_id": "6900cd53646208eac0d1f046",
                    "name": "Chengfeng Zhao",
                    "hidden": false
                },
                {
                    "_id": "6900cd53646208eac0d1f047",
                    "name": "Mengfei Li",
                    "hidden": false
                },
                {
                    "_id": "6900cd53646208eac0d1f048",
                    "name": "Yaolong Zhu",
                    "hidden": false
                },
                {
                    "_id": "6900cd53646208eac0d1f049",
                    "name": "Zhiyang Dou",
                    "hidden": false
                },
                {
                    "_id": "6900cd53646208eac0d1f04a",
                    "name": "Ronggang Wang",
                    "hidden": false
                },
                {
                    "_id": "6900cd53646208eac0d1f04b",
                    "name": "Yuan Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T03:01:10.000Z",
            "submittedOnDailyAt": "2025-10-28T12:36:43.126Z",
            "title": "SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view\n  Human Reconstruction",
            "submittedOnDailyBy": {
                "_id": "63a369d98c0c89dcae3b8329",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
                "isPro": true,
                "fullname": "Adina Yakefu",
                "user": "AdinaY",
                "type": "user"
            },
            "summary": "Photorealistic 3D full-body human reconstruction from a single image is a\ncritical yet challenging task for applications in films and video games due to\ninherent ambiguities and severe self-occlusions. While recent approaches\nleverage SMPL estimation and SMPL-conditioned image generative models to\nhallucinate novel views, they suffer from inaccurate 3D priors estimated from\nSMPL meshes and have difficulty in handling difficult human poses and\nreconstructing fine details. In this paper, we propose SyncHuman, a novel\nframework that combines 2D multiview generative model and 3D native generative\nmodel for the first time, enabling high-quality clothed human mesh\nreconstruction from single-view images even under challenging human poses.\nMultiview generative model excels at capturing fine 2D details but struggles\nwith structural consistency, whereas 3D native generative model generates\ncoarse yet structurally consistent 3D shapes. By integrating the complementary\nstrengths of these two approaches, we develop a more effective generation\nframework. Specifically, we first jointly fine-tune the multiview generative\nmodel and the 3D native generative model with proposed pixel-aligned 2D-3D\nsynchronization attention to produce geometrically aligned 3D shapes and 2D\nmultiview images. To further improve details, we introduce a feature injection\nmechanism that lifts fine details from 2D multiview images onto the aligned 3D\nshapes, enabling accurate and high-fidelity reconstruction. Extensive\nexperiments demonstrate that SyncHuman achieves robust and photo-realistic 3D\nhuman reconstruction, even for images with challenging poses. Our method\noutperforms baseline methods in geometric accuracy and visual fidelity,\ndemonstrating a promising direction for future 3D generation models.",
            "upvotes": 3,
            "discussionId": "6900cd53646208eac0d1f04c",
            "ai_summary": "SyncHuman combines 2D multiview and 3D native generative models with pixel-aligned synchronization and feature injection to achieve high-quality, photorealistic 3D human reconstruction from single images.",
            "ai_keywords": [
                "SMPL estimation",
                "SMPL-conditioned image generative models",
                "2D multiview generative model",
                "3D native generative model",
                "pixel-aligned 2D-3D synchronization attention",
                "feature injection mechanism"
            ]
        },
        "publishedAt": "2025-10-08T23:01:10.000Z",
        "title": "SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view\n  Human Reconstruction",
        "summary": "Photorealistic 3D full-body human reconstruction from a single image is a\ncritical yet challenging task for applications in films and video games due to\ninherent ambiguities and severe self-occlusions. While recent approaches\nleverage SMPL estimation and SMPL-conditioned image generative models to\nhallucinate novel views, they suffer from inaccurate 3D priors estimated from\nSMPL meshes and have difficulty in handling difficult human poses and\nreconstructing fine details. In this paper, we propose SyncHuman, a novel\nframework that combines 2D multiview generative model and 3D native generative\nmodel for the first time, enabling high-quality clothed human mesh\nreconstruction from single-view images even under challenging human poses.\nMultiview generative model excels at capturing fine 2D details but struggles\nwith structural consistency, whereas 3D native generative model generates\ncoarse yet structurally consistent 3D shapes. By integrating the complementary\nstrengths of these two approaches, we develop a more effective generation\nframework. Specifically, we first jointly fine-tune the multiview generative\nmodel and the 3D native generative model with proposed pixel-aligned 2D-3D\nsynchronization attention to produce geometrically aligned 3D shapes and 2D\nmultiview images. To further improve details, we introduce a feature injection\nmechanism that lifts fine details from 2D multiview images onto the aligned 3D\nshapes, enabling accurate and high-fidelity reconstruction. Extensive\nexperiments demonstrate that SyncHuman achieves robust and photo-realistic 3D\nhuman reconstruction, even for images with challenging poses. Our method\noutperforms baseline methods in geometric accuracy and visual fidelity,\ndemonstrating a promising direction for future 3D generation models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07723.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63a369d98c0c89dcae3b8329",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
            "fullname": "Adina Yakefu",
            "name": "AdinaY",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1155
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.23605",
            "authors": [
                {
                    "_id": "69003a8622d452aac6dd439e",
                    "name": "Shuhong Zheng",
                    "hidden": false
                },
                {
                    "_id": "69003a8622d452aac6dd439f",
                    "name": "Ashkan Mirzaei",
                    "hidden": false
                },
                {
                    "_id": "69003a8622d452aac6dd43a0",
                    "name": "Igor Gilitschenski",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T17:59:51.000Z",
            "submittedOnDailyAt": "2025-10-28T03:51:15.069Z",
            "title": "Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with\n  Progressive Texture Infilling",
            "submittedOnDailyBy": {
                "_id": "64361417a4bd75c62cc1e534",
                "avatarUrl": "/avatars/edf6ac3fb9bd63ceb78f3b95254e6b19.svg",
                "isPro": true,
                "fullname": "Shuhong Zheng",
                "user": "ShuhongZheng",
                "type": "user"
            },
            "summary": "Current 3D/4D generation methods are usually optimized for photorealism,\nefficiency, and aesthetics. However, they often fail to preserve the semantic\nidentity of the subject across different viewpoints. Adapting generation\nmethods with one or few images of a specific subject (also known as\nPersonalization or Subject-driven generation) allows generating visual content\nthat align with the identity of the subject. However, personalized 3D/4D\ngeneration is still largely underexplored. In this work, we introduce TIRE\n(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.\nIt takes an initial 3D asset produced by an existing 3D generative model as\ninput and uses video tracking to identify the regions that need to be modified.\nThen, we adopt a subject-driven 2D inpainting model for progressively infilling\nthe identified regions. Finally, we resplat the modified 2D multi-view\nobservations back to 3D while still maintaining consistency. Extensive\nexperiments demonstrate that our approach significantly improves identity\npreservation in 3D/4D generation compared to state-of-the-art methods. Our\nproject website is available at\nhttps://zsh2000.github.io/track-inpaint-resplat.github.io/.",
            "upvotes": 2,
            "discussionId": "69003a8622d452aac6dd43a1",
            "ai_summary": "TIRE method enhances identity preservation in 3D/4D generation by tracking, inpainting, and resplatting regions of a 3D asset using video tracking and a subject-driven 2D inpainting model.",
            "ai_keywords": [
                "3D generative model",
                "video tracking",
                "subject-driven 2D inpainting model",
                "identity preservation",
                "3D/4D generation",
                "TIRE"
            ]
        },
        "publishedAt": "2025-10-27T13:59:51.000Z",
        "title": "Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with\n  Progressive Texture Infilling",
        "summary": "Current 3D/4D generation methods are usually optimized for photorealism,\nefficiency, and aesthetics. However, they often fail to preserve the semantic\nidentity of the subject across different viewpoints. Adapting generation\nmethods with one or few images of a specific subject (also known as\nPersonalization or Subject-driven generation) allows generating visual content\nthat align with the identity of the subject. However, personalized 3D/4D\ngeneration is still largely underexplored. In this work, we introduce TIRE\n(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.\nIt takes an initial 3D asset produced by an existing 3D generative model as\ninput and uses video tracking to identify the regions that need to be modified.\nThen, we adopt a subject-driven 2D inpainting model for progressively infilling\nthe identified regions. Finally, we resplat the modified 2D multi-view\nobservations back to 3D while still maintaining consistency. Extensive\nexperiments demonstrate that our approach significantly improves identity\npreservation in 3D/4D generation compared to state-of-the-art methods. Our\nproject website is available at\nhttps://zsh2000.github.io/track-inpaint-resplat.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23605.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64361417a4bd75c62cc1e534",
            "avatarUrl": "/avatars/edf6ac3fb9bd63ceb78f3b95254e6b19.svg",
            "fullname": "Shuhong Zheng",
            "name": "ShuhongZheng",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.22603",
            "authors": [
                {
                    "_id": "69007e71646208eac0d1ef2a",
                    "name": "Anand",
                    "hidden": false
                },
                {
                    "_id": "69007e71646208eac0d1ef2b",
                    "user": {
                        "_id": "64903f017b630c141867877f",
                        "avatarUrl": "/avatars/3c7b248baed446ecc2b6adc2c444320d.svg",
                        "isPro": false,
                        "fullname": "Umberto Cappellazzo",
                        "user": "hisoka94",
                        "type": "user"
                    },
                    "name": "Umberto Cappellazzo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:26:41.351Z",
                    "hidden": false
                },
                {
                    "_id": "69007e71646208eac0d1ef2c",
                    "name": "Stavros Petridis",
                    "hidden": false
                },
                {
                    "_id": "69007e71646208eac0d1ef2d",
                    "name": "Maja Pantic",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-26T09:44:20.000Z",
            "submittedOnDailyAt": "2025-10-28T07:00:30.082Z",
            "title": "Mitigating Attention Sinks and Massive Activations in Audio-Visual\n  Speech Recognition with LLMS",
            "submittedOnDailyBy": {
                "_id": "64903f017b630c141867877f",
                "avatarUrl": "/avatars/3c7b248baed446ecc2b6adc2c444320d.svg",
                "isPro": false,
                "fullname": "Umberto Cappellazzo",
                "user": "hisoka94",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have recently advanced auditory speech\nrecognition (ASR), visual speech recognition (VSR), and audio-visual speech\nrecognition (AVSR). However, understanding of their internal dynamics under\nfine-tuning remains limited. In natural language processing, recent work has\nrevealed attention sinks, tokens that attract disproportionately high\nattention, and associated massive activations in which some features of sink\ntokens exhibit huge activation in LLMs. In this work, we are the first to study\nthese phenomena in multimodal speech recognition. Through a detailed analysis\nof audio-visual LLMs, we identify attention sinks and massive activations not\nonly at the BOS token but also at intermediate low-semantic tokens across ASR,\nVSR, and AVSR. We show that massive activations originate in the MLP layers and\ncorrespond to fixed feature indices across all sink tokens. We further show\nthat intermediate sink tokens exhibit high cosine similarity to the BOS token,\nthereby amplifying attention and activation. Building on these insights, we\nintroduce a simple decorrelation loss that reduces cosine similarity between\nBOS and other tokens, effectively mitigating intermediate sinks and massive\nactivations. Furthermore, our method improves word error rate (WER) under high\naudio-visual feature downsampling while remaining stable at lower downsampling\nrates.",
            "upvotes": 2,
            "discussionId": "69007e72646208eac0d1ef2e",
            "githubRepo": "https://github.com/umbertocappellazzo/Llama-AVSR",
            "ai_summary": "Attention sinks and massive activations in multimodal speech recognition are identified and mitigated using a decorrelation loss, improving word error rate under high feature downsampling.",
            "ai_keywords": [
                "attention sinks",
                "massive activations",
                "BOS token",
                "MLP layers",
                "cosine similarity",
                "decorrelation loss",
                "word error rate",
                "audio-visual feature downsampling"
            ],
            "githubStars": 40,
            "organization": {
                "_id": "650987fc2feb9570c5137ac2",
                "name": "ImperialCollegeLondon",
                "fullname": "Imperial College London",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/u6ceSXXV6ldtt0qZOOMJw.jpeg"
            }
        },
        "publishedAt": "2025-10-26T05:44:20.000Z",
        "title": "Mitigating Attention Sinks and Massive Activations in Audio-Visual\n  Speech Recognition with LLMS",
        "summary": "Large language models (LLMs) have recently advanced auditory speech\nrecognition (ASR), visual speech recognition (VSR), and audio-visual speech\nrecognition (AVSR). However, understanding of their internal dynamics under\nfine-tuning remains limited. In natural language processing, recent work has\nrevealed attention sinks, tokens that attract disproportionately high\nattention, and associated massive activations in which some features of sink\ntokens exhibit huge activation in LLMs. In this work, we are the first to study\nthese phenomena in multimodal speech recognition. Through a detailed analysis\nof audio-visual LLMs, we identify attention sinks and massive activations not\nonly at the BOS token but also at intermediate low-semantic tokens across ASR,\nVSR, and AVSR. We show that massive activations originate in the MLP layers and\ncorrespond to fixed feature indices across all sink tokens. We further show\nthat intermediate sink tokens exhibit high cosine similarity to the BOS token,\nthereby amplifying attention and activation. Building on these insights, we\nintroduce a simple decorrelation loss that reduces cosine similarity between\nBOS and other tokens, effectively mitigating intermediate sinks and massive\nactivations. Furthermore, our method improves word error rate (WER) under high\naudio-visual feature downsampling while remaining stable at lower downsampling\nrates.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22603.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64903f017b630c141867877f",
            "avatarUrl": "/avatars/3c7b248baed446ecc2b6adc2c444320d.svg",
            "fullname": "Umberto Cappellazzo",
            "name": "hisoka94",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "650987fc2feb9570c5137ac2",
            "name": "ImperialCollegeLondon",
            "fullname": "Imperial College London",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/u6ceSXXV6ldtt0qZOOMJw.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.22317",
            "authors": [
                {
                    "_id": "69006c0222d452aac6dd4475",
                    "name": "Antal van den Bosch",
                    "hidden": false
                },
                {
                    "_id": "69006c0222d452aac6dd4476",
                    "name": "Ainhoa Risco Patón",
                    "hidden": false
                },
                {
                    "_id": "69006c0222d452aac6dd4477",
                    "name": "Teun Buijse",
                    "hidden": false
                },
                {
                    "_id": "69006c0222d452aac6dd4478",
                    "name": "Peter Berck",
                    "hidden": false
                },
                {
                    "_id": "69006c0222d452aac6dd4479",
                    "name": "Maarten van Gompel",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/634026af07e80abe233ad75f/vdNOdLgTvR7wWIr0Tyc3r.png"
            ],
            "publishedAt": "2025-10-25T14:34:18.000Z",
            "submittedOnDailyAt": "2025-10-28T06:22:09.654Z",
            "title": "Memory-based Language Models: An Efficient, Explainable, and\n  Eco-friendly Approach to Large Language Modeling",
            "submittedOnDailyBy": {
                "_id": "634026af07e80abe233ad75f",
                "avatarUrl": "/avatars/fa083c405d388c9b94adf3f6a6566c17.svg",
                "isPro": false,
                "fullname": "Antal",
                "user": "antalvdb",
                "type": "user"
            },
            "summary": "We present memory-based language modeling as an efficient, eco-friendly\nalternative to deep neural network-based language modeling. It offers\nlog-linearly scalable next-token prediction performance and strong memorization\ncapabilities. Implementing fast approximations of k-nearest neighbor\nclassification, memory-based language modeling leaves a relatively small\necological footprint both in training and in inference mode, as it relies fully\non CPUs and attains low token latencies. Its internal workings are simple and\nfully transparent. We compare our implementation of memory-based language\nmodeling, OLIFANT, with GPT-2 and GPT-Neo on next-token prediction accuracy,\nestimated emissions and speeds, and offer some deeper analyses of the model.",
            "upvotes": 2,
            "discussionId": "69006c0322d452aac6dd447a",
            "githubRepo": "https://github.com/antalvdb/olifant",
            "ai_summary": "Memory-based language modeling provides an efficient, eco-friendly alternative to deep neural networks, offering scalable performance and strong memorization with low ecological impact.",
            "ai_keywords": [
                "k-nearest neighbor classification",
                "memory-based language modeling",
                "next-token prediction",
                "ecological footprint",
                "CPUs",
                "token latencies",
                "OLIFANT",
                "GPT-2",
                "GPT-Neo"
            ],
            "githubStars": 4,
            "organization": {
                "_id": "61927d22a9638054a9818f0c",
                "name": "utrechtuniversity",
                "fullname": "Utrecht University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/FT6jcFsBsRzLhWi2-Lk0R.jpeg"
            }
        },
        "publishedAt": "2025-10-25T10:34:18.000Z",
        "title": "Memory-based Language Models: An Efficient, Explainable, and\n  Eco-friendly Approach to Large Language Modeling",
        "summary": "We present memory-based language modeling as an efficient, eco-friendly\nalternative to deep neural network-based language modeling. It offers\nlog-linearly scalable next-token prediction performance and strong memorization\ncapabilities. Implementing fast approximations of k-nearest neighbor\nclassification, memory-based language modeling leaves a relatively small\necological footprint both in training and in inference mode, as it relies fully\non CPUs and attains low token latencies. Its internal workings are simple and\nfully transparent. We compare our implementation of memory-based language\nmodeling, OLIFANT, with GPT-2 and GPT-Neo on next-token prediction accuracy,\nestimated emissions and speeds, and offer some deeper analyses of the model.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/634026af07e80abe233ad75f/vdNOdLgTvR7wWIr0Tyc3r.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22317.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "634026af07e80abe233ad75f",
            "avatarUrl": "/avatars/fa083c405d388c9b94adf3f6a6566c17.svg",
            "fullname": "Antal",
            "name": "antalvdb",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "61927d22a9638054a9818f0c",
            "name": "utrechtuniversity",
            "fullname": "Utrecht University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/FT6jcFsBsRzLhWi2-Lk0R.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.22010",
            "authors": [
                {
                    "_id": "69005f5922d452aac6dd4445",
                    "user": {
                        "_id": "670d13236f34613201303b84",
                        "avatarUrl": "/avatars/c00cc17c2e6a4a8b3d466a73c62129f4.svg",
                        "isPro": false,
                        "fullname": "Or Ronai",
                        "user": "orronai",
                        "type": "user"
                    },
                    "name": "Or Ronai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:26:57.774Z",
                    "hidden": false
                },
                {
                    "_id": "69005f5922d452aac6dd4446",
                    "name": "Vladimir Kulikov",
                    "hidden": false
                },
                {
                    "_id": "69005f5922d452aac6dd4447",
                    "name": "Tomer Michaeli",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/670d13236f34613201303b84/n_NzQ73s1ESXRZS1Srp6a.jpeg"
            ],
            "publishedAt": "2025-10-24T20:24:26.000Z",
            "submittedOnDailyAt": "2025-10-28T14:15:48.393Z",
            "title": "FlowOpt: Fast Optimization Through Whole Flow Processes for\n  Training-Free Editing",
            "submittedOnDailyBy": {
                "_id": "670d13236f34613201303b84",
                "avatarUrl": "/avatars/c00cc17c2e6a4a8b3d466a73c62129f4.svg",
                "isPro": false,
                "fullname": "Or Ronai",
                "user": "orronai",
                "type": "user"
            },
            "summary": "The remarkable success of diffusion and flow-matching models has ignited a\nsurge of works on adapting them at test time for controlled generation tasks.\nExamples range from image editing to restoration, compression and\npersonalization. However, due to the iterative nature of the sampling process\nin those models, it is computationally impractical to use gradient-based\noptimization to directly control the image generated at the end of the process.\nAs a result, existing methods typically resort to manipulating each timestep\nseparately. Here we introduce FlowOpt - a zero-order (gradient-free)\noptimization framework that treats the entire flow process as a black box,\nenabling optimization through the whole sampling path without backpropagation\nthrough the model. Our method is both highly efficient and allows users to\nmonitor the intermediate optimization results and perform early stopping if\ndesired. We prove a sufficient condition on FlowOpt's step-size, under which\nconvergence to the global optimum is guaranteed. We further show how to\nempirically estimate this upper bound so as to choose an appropriate step-size.\nWe demonstrate how FlowOpt can be used for image editing, showcasing two\noptions: (i) inversion (determining the initial noise that generates a given\nimage), and (ii) directly steering the edited image to be similar to the source\nimage while conforming to a target text prompt. In both cases, FlowOpt achieves\nstate-of-the-art results while using roughly the same number of neural function\nevaluations (NFEs) as existing methods. Code and examples are available on the\nproject's webpage.",
            "upvotes": 2,
            "discussionId": "69005f5a22d452aac6dd4448",
            "projectPage": "https://orronai.github.io/FlowOpt/",
            "githubRepo": "https://github.com/orronai/FlowOpt",
            "ai_summary": "FlowOpt, a zero-order optimization framework, enables efficient control of diffusion and flow-matching models for image editing tasks without backpropagation.",
            "ai_keywords": [
                "diffusion models",
                "flow-matching models",
                "gradient-free optimization",
                "zero-order optimization",
                "flow process",
                "black box optimization",
                "image editing",
                "inversion",
                "neural function evaluations"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-10-24T16:24:26.000Z",
        "title": "FlowOpt: Fast Optimization Through Whole Flow Processes for\n  Training-Free Editing",
        "summary": "The remarkable success of diffusion and flow-matching models has ignited a\nsurge of works on adapting them at test time for controlled generation tasks.\nExamples range from image editing to restoration, compression and\npersonalization. However, due to the iterative nature of the sampling process\nin those models, it is computationally impractical to use gradient-based\noptimization to directly control the image generated at the end of the process.\nAs a result, existing methods typically resort to manipulating each timestep\nseparately. Here we introduce FlowOpt - a zero-order (gradient-free)\noptimization framework that treats the entire flow process as a black box,\nenabling optimization through the whole sampling path without backpropagation\nthrough the model. Our method is both highly efficient and allows users to\nmonitor the intermediate optimization results and perform early stopping if\ndesired. We prove a sufficient condition on FlowOpt's step-size, under which\nconvergence to the global optimum is guaranteed. We further show how to\nempirically estimate this upper bound so as to choose an appropriate step-size.\nWe demonstrate how FlowOpt can be used for image editing, showcasing two\noptions: (i) inversion (determining the initial noise that generates a given\nimage), and (ii) directly steering the edited image to be similar to the source\nimage while conforming to a target text prompt. In both cases, FlowOpt achieves\nstate-of-the-art results while using roughly the same number of neural function\nevaluations (NFEs) as existing methods. Code and examples are available on the\nproject's webpage.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/670d13236f34613201303b84/n_NzQ73s1ESXRZS1Srp6a.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22010.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "670d13236f34613201303b84",
            "avatarUrl": "/avatars/c00cc17c2e6a4a8b3d466a73c62129f4.svg",
            "fullname": "Or Ronai",
            "name": "orronai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.21986",
            "authors": [
                {
                    "_id": "69013d33646208eac0d1f16b",
                    "name": "Dogyun Park",
                    "hidden": false
                },
                {
                    "_id": "69013d33646208eac0d1f16c",
                    "name": "Moayed Haji-Ali",
                    "hidden": false
                },
                {
                    "_id": "69013d33646208eac0d1f16d",
                    "name": "Yanyu Li",
                    "hidden": false
                },
                {
                    "_id": "69013d33646208eac0d1f16e",
                    "name": "Willi Menapace",
                    "hidden": false
                },
                {
                    "_id": "69013d33646208eac0d1f16f",
                    "name": "Sergey Tulyakov",
                    "hidden": false
                },
                {
                    "_id": "69013d33646208eac0d1f170",
                    "name": "Hyunwoo J. Kim",
                    "hidden": false
                },
                {
                    "_id": "69013d33646208eac0d1f171",
                    "name": "Aliaksandr Siarohin",
                    "hidden": false
                },
                {
                    "_id": "69013d33646208eac0d1f172",
                    "name": "Anil Kag",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-24T19:29:55.000Z",
            "submittedOnDailyAt": "2025-10-28T20:34:47.502Z",
            "title": "Sprint: Sparse-Dense Residual Fusion for Efficient Diffusion\n  Transformers",
            "submittedOnDailyBy": {
                "_id": "64e6d90c4a408888f9dcc95e",
                "avatarUrl": "/avatars/829394448bbe53bf133d6315e51a90b7.svg",
                "isPro": false,
                "fullname": "Dogyun Park",
                "user": "DogyunPark",
                "type": "user"
            },
            "summary": "Diffusion Transformers (DiTs) deliver state-of-the-art generative performance\nbut their quadratic training cost with sequence length makes large-scale\npretraining prohibitively expensive. Token dropping can reduce training cost,\nyet na\\\"ive strategies degrade representations, and existing methods are either\nparameter-heavy or fail at high drop ratios. We present SPRINT, Sparse--Dense\nResidual Fusion for Efficient Diffusion Transformers, a simple method that\nenables aggressive token dropping (up to 75%) while preserving quality. SPRINT\nleverages the complementary roles of shallow and deep layers: early layers\nprocess all tokens to capture local detail, deeper layers operate on a sparse\nsubset to cut computation, and their outputs are fused through residual\nconnections. Training follows a two-stage schedule: long masked pre-training\nfor efficiency followed by short full-token fine-tuning to close the\ntrain--inference gap. On ImageNet-1K 256x256, SPRINT achieves 9.8x training\nsavings with comparable FID/FDD, and at inference, its Path-Drop Guidance (PDG)\nnearly halves FLOPs while improving quality. These results establish SPRINT as\na simple, effective, and general solution for efficient DiT training.",
            "upvotes": 2,
            "discussionId": "69013d33646208eac0d1f173",
            "ai_summary": "SPRINT, a method for efficient training of Diffusion Transformers, achieves significant training cost reduction and maintains quality through aggressive token dropping and residual fusion.",
            "ai_keywords": [
                "Diffusion Transformers",
                "DiTs",
                "token dropping",
                "sparse-dense residual fusion",
                "local detail",
                "residual connections",
                "two-stage schedule",
                "masked pre-training",
                "full-token fine-tuning",
                "ImageNet-1K",
                "FID/FDD",
                "Path-Drop Guidance",
                "FLOPs"
            ],
            "organization": {
                "_id": "63c87c41cd6a490608ce31d1",
                "name": "snap-research",
                "fullname": "Snap Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"
            }
        },
        "publishedAt": "2025-10-24T15:29:55.000Z",
        "title": "Sprint: Sparse-Dense Residual Fusion for Efficient Diffusion\n  Transformers",
        "summary": "Diffusion Transformers (DiTs) deliver state-of-the-art generative performance\nbut their quadratic training cost with sequence length makes large-scale\npretraining prohibitively expensive. Token dropping can reduce training cost,\nyet na\\\"ive strategies degrade representations, and existing methods are either\nparameter-heavy or fail at high drop ratios. We present SPRINT, Sparse--Dense\nResidual Fusion for Efficient Diffusion Transformers, a simple method that\nenables aggressive token dropping (up to 75%) while preserving quality. SPRINT\nleverages the complementary roles of shallow and deep layers: early layers\nprocess all tokens to capture local detail, deeper layers operate on a sparse\nsubset to cut computation, and their outputs are fused through residual\nconnections. Training follows a two-stage schedule: long masked pre-training\nfor efficiency followed by short full-token fine-tuning to close the\ntrain--inference gap. On ImageNet-1K 256x256, SPRINT achieves 9.8x training\nsavings with comparable FID/FDD, and at inference, its Path-Drop Guidance (PDG)\nnearly halves FLOPs while improving quality. These results establish SPRINT as\na simple, effective, and general solution for efficient DiT training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21986.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64e6d90c4a408888f9dcc95e",
            "avatarUrl": "/avatars/829394448bbe53bf133d6315e51a90b7.svg",
            "fullname": "Dogyun Park",
            "name": "DogyunPark",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "63c87c41cd6a490608ce31d1",
            "name": "snap-research",
            "fullname": "Snap Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.21800",
            "authors": [
                {
                    "_id": "69002c5e22d452aac6dd42bb",
                    "user": {
                        "_id": "653d276681f52ceb4d12bd85",
                        "avatarUrl": "/avatars/56601a25e5f883a8f6dc15f6fd9dcc57.svg",
                        "isPro": false,
                        "fullname": "Yifeng Liu",
                        "user": "Lewis-Lau",
                        "type": "user"
                    },
                    "name": "Yifeng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:36:44.528Z",
                    "hidden": false
                },
                {
                    "_id": "69002c5e22d452aac6dd42bc",
                    "name": "Angela Yuan",
                    "hidden": false
                },
                {
                    "_id": "69002c5e22d452aac6dd42bd",
                    "name": "Quanquan Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T16:49:22.000Z",
            "submittedOnDailyAt": "2025-10-28T01:09:31.082Z",
            "title": "MARS-M: When Variance Reduction Meets Matrices",
            "submittedOnDailyBy": {
                "_id": "653d276681f52ceb4d12bd85",
                "avatarUrl": "/avatars/56601a25e5f883a8f6dc15f6fd9dcc57.svg",
                "isPro": false,
                "fullname": "Yifeng Liu",
                "user": "Lewis-Lau",
                "type": "user"
            },
            "summary": "Matrix-based preconditioned optimizers, such as Muon, have recently been\nshown to be more efficient than scalar-based optimizers for training\nlarge-scale neural networks, including large language models (LLMs). On the\nother hand, recent benchmarks on optimizers for LLM pre-training have\ndemonstrated that variance-reduction techniques such as MARS can achieve\nsubstantial speedups over standard optimizers that do not employ variance\nreduction. In this paper, to achieve the best of both worlds, we introduce\nMARS-M, a new optimizer that integrates the variance reduction technique in\nMARS with Muon. Under standard regularity conditions, we prove that Muon-M\nconverges to a first-order stationary point at a rate of\nmathcal{O}(T^{-1/3}), which improves upon\nmathcal{O}(T^{-1/4}) rate attained by Muon. Our empirical results on\nlanguage modeling and computer vision tasks demonstrate that MARS-M\nconsistently yields lower losses and improved performance across various\ndownstream benchmarks. The implementation of MARS-M is available at\nhttps://github.com/AGI-Arena/MARS/MARS_M.",
            "upvotes": 2,
            "discussionId": "69002c5e22d452aac6dd42be",
            "projectPage": "https://github.com/AGI-Arena/MARS/tree/main/MARS_M",
            "githubRepo": "https://github.com/AGI-Arena/MARS/tree/main/MARS_M",
            "ai_summary": "MARS-M, a new optimizer combining Muon and MARS techniques, achieves faster convergence and better performance in large-scale neural network training.",
            "ai_keywords": [
                "Muon",
                "MARS",
                "variance reduction",
                "first-order stationary point",
                "language modeling",
                "computer vision"
            ],
            "githubStars": 711,
            "organization": {
                "_id": "67784c39dac147922d8d09f0",
                "name": "UCLA",
                "fullname": "University of California, Los Angeles",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"
            }
        },
        "publishedAt": "2025-10-20T12:49:22.000Z",
        "title": "MARS-M: When Variance Reduction Meets Matrices",
        "summary": "Matrix-based preconditioned optimizers, such as Muon, have recently been\nshown to be more efficient than scalar-based optimizers for training\nlarge-scale neural networks, including large language models (LLMs). On the\nother hand, recent benchmarks on optimizers for LLM pre-training have\ndemonstrated that variance-reduction techniques such as MARS can achieve\nsubstantial speedups over standard optimizers that do not employ variance\nreduction. In this paper, to achieve the best of both worlds, we introduce\nMARS-M, a new optimizer that integrates the variance reduction technique in\nMARS with Muon. Under standard regularity conditions, we prove that Muon-M\nconverges to a first-order stationary point at a rate of\nmathcal{O}(T^{-1/3}), which improves upon\nmathcal{O}(T^{-1/4}) rate attained by Muon. Our empirical results on\nlanguage modeling and computer vision tasks demonstrate that MARS-M\nconsistently yields lower losses and improved performance across various\ndownstream benchmarks. The implementation of MARS-M is available at\nhttps://github.com/AGI-Arena/MARS/MARS_M.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21800.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "653d276681f52ceb4d12bd85",
            "avatarUrl": "/avatars/56601a25e5f883a8f6dc15f6fd9dcc57.svg",
            "fullname": "Yifeng Liu",
            "name": "Lewis-Lau",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "67784c39dac147922d8d09f0",
            "name": "UCLA",
            "fullname": "University of California, Los Angeles",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"
        },
        "isAuthorParticipating": true
    }
]