[
    {
        "paper": {
            "id": "2510.24668",
            "authors": [
                {
                    "_id": "690188f7646208eac0d1f482",
                    "name": "Mingyi Deng",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f483",
                    "name": "Lijun Huang",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f484",
                    "name": "Yani Fan",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f485",
                    "user": {
                        "_id": "65f40e83653c231cbaf7defe",
                        "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg",
                        "isPro": false,
                        "fullname": "Jiayi Zhang",
                        "user": "didiforhugface",
                        "type": "user"
                    },
                    "name": "Jiayi Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:43:11.330Z",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f486",
                    "name": "Fashen Ren",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f487",
                    "name": "Jinyi Bai",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f488",
                    "name": "Fuzhen Yang",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f489",
                    "name": "Dayi Miao",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f48a",
                    "name": "Zhaoyang Yu",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f48b",
                    "name": "Yifan Wu",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f48c",
                    "user": {
                        "_id": "65ddeb1e344080c5bbc547e8",
                        "avatarUrl": "/avatars/8423d009aa8f3737b2309c64d1d71b65.svg",
                        "isPro": false,
                        "fullname": "Yanfei Zhang",
                        "user": "LovinYou",
                        "type": "user"
                    },
                    "name": "Yanfei Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:42:46.305Z",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f48d",
                    "name": "Fengwei Teng",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f48e",
                    "name": "Yingjia Wan",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f48f",
                    "name": "Song Hu",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f490",
                    "name": "Yude Li",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f491",
                    "name": "Xin Jin",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f492",
                    "name": "Conghao Hu",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f493",
                    "name": "Haoyu Li",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f494",
                    "name": "Qirui Fu",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f495",
                    "name": "Tai Zhong",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f496",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f497",
                    "name": "Xiangru Tang",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f498",
                    "name": "Nan Tang",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f499",
                    "name": "Chenglin Wu",
                    "hidden": false
                },
                {
                    "_id": "690188f7646208eac0d1f49a",
                    "name": "Yuyu Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T17:35:54.000Z",
            "submittedOnDailyAt": "2025-10-29T01:58:28.190Z",
            "title": "InteractComp: Evaluating Search Agents With Ambiguous Queries",
            "submittedOnDailyBy": {
                "_id": "65f40e83653c231cbaf7defe",
                "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg",
                "isPro": false,
                "fullname": "Jiayi Zhang",
                "user": "didiforhugface",
                "type": "user"
            },
            "summary": "Language agents have demonstrated remarkable potential in web search and\ninformation retrieval. However, these search agents assume user queries are\ncomplete and unambiguous, an assumption that diverges from reality where users\nbegin with incomplete queries requiring clarification through interaction. Yet\nmost agents lack interactive mechanisms during the search process, and existing\nbenchmarks cannot assess this capability. To address this gap, we introduce\nInteractComp, a benchmark designed to evaluate whether search agents can\nrecognize query ambiguity and actively interact to resolve it during search.\nFollowing the principle of easy to verify, interact to disambiguate, we\nconstruct 210 expert-curated questions across 9 domains through a\ntarget-distractor methodology that creates genuine ambiguity resolvable only\nthrough interaction. Evaluation of 17 models reveals striking failure: the best\nmodel achieves only 13.73% accuracy despite 71.50% with complete context,\nexposing systematic overconfidence rather than reasoning deficits. Forced\ninteraction produces dramatic gains, demonstrating latent capability current\nstrategies fail to engage. Longitudinal analysis shows interaction capabilities\nstagnated over 15 months while search performance improved seven-fold,\nrevealing a critical blind spot. This stagnation, coupled with the immediate\nfeedback inherent to search tasks, makes InteractComp a valuable resource for\nboth evaluating and training interaction capabilities in search agents. The\ncode is available at https://github.com/FoundationAgents/InteractComp.",
            "upvotes": 86,
            "discussionId": "690188f7646208eac0d1f49b",
            "githubRepo": "https://github.com/FoundationAgents/InteractComp",
            "ai_summary": "InteractComp evaluates search agents' ability to recognize and resolve query ambiguity through interaction, revealing significant gaps in current models' capabilities.",
            "ai_keywords": [
                "search agents",
                "query ambiguity",
                "interaction mechanisms",
                "benchmark",
                "expert-curated questions",
                "target-distractor methodology",
                "accuracy",
                "overconfidence",
                "reasoning deficits",
                "longitudinal analysis"
            ],
            "githubStars": 10
        },
        "publishedAt": "2025-10-28T13:35:54.000Z",
        "title": "InteractComp: Evaluating Search Agents With Ambiguous Queries",
        "summary": "Language agents have demonstrated remarkable potential in web search and\ninformation retrieval. However, these search agents assume user queries are\ncomplete and unambiguous, an assumption that diverges from reality where users\nbegin with incomplete queries requiring clarification through interaction. Yet\nmost agents lack interactive mechanisms during the search process, and existing\nbenchmarks cannot assess this capability. To address this gap, we introduce\nInteractComp, a benchmark designed to evaluate whether search agents can\nrecognize query ambiguity and actively interact to resolve it during search.\nFollowing the principle of easy to verify, interact to disambiguate, we\nconstruct 210 expert-curated questions across 9 domains through a\ntarget-distractor methodology that creates genuine ambiguity resolvable only\nthrough interaction. Evaluation of 17 models reveals striking failure: the best\nmodel achieves only 13.73% accuracy despite 71.50% with complete context,\nexposing systematic overconfidence rather than reasoning deficits. Forced\ninteraction produces dramatic gains, demonstrating latent capability current\nstrategies fail to engage. Longitudinal analysis shows interaction capabilities\nstagnated over 15 months while search performance improved seven-fold,\nrevealing a critical blind spot. This stagnation, coupled with the immediate\nfeedback inherent to search tasks, makes InteractComp a valuable resource for\nboth evaluating and training interaction capabilities in search agents. The\ncode is available at https://github.com/FoundationAgents/InteractComp.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24668.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f40e83653c231cbaf7defe",
            "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg",
            "fullname": "Jiayi Zhang",
            "name": "didiforhugface",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.24701",
            "authors": [
                {
                    "_id": "69017244646208eac0d1f30e",
                    "name": "Tongyi DeepResearch Team",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f30f",
                    "name": "Baixuan Li",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f310",
                    "name": "Bo Zhang",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f311",
                    "name": "Dingchu Zhang",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f312",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f313",
                    "name": "Guangyu Li",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f314",
                    "user": {
                        "_id": "63f06116f1a47aaea5bd497b",
                        "avatarUrl": "/avatars/7d99ffa59c4579599e852a0ffb261268.svg",
                        "isPro": false,
                        "fullname": "Guoxin Chen",
                        "user": "GuoxinChen",
                        "type": "user"
                    },
                    "name": "Guoxin Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:46:37.103Z",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f315",
                    "name": "Huifeng Yin",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f316",
                    "user": {
                        "_id": "644a4fbc2166258fccc664bc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                        "isPro": false,
                        "fullname": "Jialong Wu",
                        "user": "callanwu",
                        "type": "user"
                    },
                    "name": "Jialong Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:46:31.748Z",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f317",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f318",
                    "name": "Kuan Li",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f319",
                    "user": {
                        "_id": "677f945ea82c316db164a180",
                        "avatarUrl": "/avatars/50ec99d971564944de3b1d9c17d50cfd.svg",
                        "isPro": false,
                        "fullname": "Liangcai Su",
                        "user": "HKU-Liangcai",
                        "type": "user"
                    },
                    "name": "Liangcai Su",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:45:58.344Z",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f31a",
                    "name": "Litu Ou",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f31b",
                    "name": "Liwen Zhang",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f31c",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f31d",
                    "name": "Rui Ye",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f31e",
                    "name": "Wenbiao Yin",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f31f",
                    "name": "Xinmiao Yu",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f320",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f321",
                    "user": {
                        "_id": "6622132f63598534f96ca29d",
                        "avatarUrl": "/avatars/34e61fc3101f8ebce1ef7041f761e108.svg",
                        "isPro": false,
                        "fullname": "Xixi Wu",
                        "user": "xxwu",
                        "type": "user"
                    },
                    "name": "Xixi Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:46:13.404Z",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f322",
                    "user": {
                        "_id": "65e6970d135c27ea806526fe",
                        "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg",
                        "isPro": false,
                        "fullname": "Xuanzhong Chen",
                        "user": "chenxz",
                        "type": "user"
                    },
                    "name": "Xuanzhong Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:46:23.542Z",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f323",
                    "user": {
                        "_id": "66e4019518a1920fb7ca19d7",
                        "avatarUrl": "/avatars/4f8be2f3244239510235c2ec6e8d38a6.svg",
                        "isPro": false,
                        "fullname": "Yida Zhao",
                        "user": "zhaoyd",
                        "type": "user"
                    },
                    "name": "Yida Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:46:15.620Z",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f324",
                    "name": "Zhen Zhang",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f325",
                    "name": "Zhengwei Tao",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f326",
                    "name": "Zhongwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f327",
                    "name": "Zile Qiao",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f328",
                    "name": "Chenxi Wang",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f329",
                    "name": "Donglei Yu",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f32a",
                    "name": "Gang Fu",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f32b",
                    "name": "Haiyang Shen",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f32c",
                    "name": "Jiayin Yang",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f32d",
                    "name": "Jun Lin",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f32e",
                    "name": "Junkai Zhang",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f32f",
                    "name": "Kui Zeng",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f330",
                    "name": "Li Yang",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f331",
                    "name": "Hailong Yin",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f332",
                    "name": "Maojia Song",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f333",
                    "name": "Ming Yan",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f334",
                    "name": "Peng Xia",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f335",
                    "name": "Qian Xiao",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f336",
                    "name": "Rui Min",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f337",
                    "name": "Ruixue Ding",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f338",
                    "name": "Runnan Fang",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f339",
                    "name": "Shaowei Chen",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f33a",
                    "name": "Shen Huang",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f33b",
                    "name": "Shihang Wang",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f33c",
                    "name": "Shihao Cai",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f33d",
                    "name": "Weizhou Shen",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f33e",
                    "name": "Xiaobin Wang",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f33f",
                    "name": "Xin Guan",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f340",
                    "name": "Xinyu Geng",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f341",
                    "name": "Yingcheng Shi",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f342",
                    "name": "Yuning Wu",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f343",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f344",
                    "name": "Zijian Li",
                    "hidden": false
                },
                {
                    "_id": "69017244646208eac0d1f345",
                    "name": "Yong Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T17:53:02.000Z",
            "submittedOnDailyAt": "2025-10-29T00:18:14.015Z",
            "title": "Tongyi DeepResearch Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.",
            "upvotes": 72,
            "discussionId": "69017244646208eac0d1f346",
            "projectPage": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/",
            "ai_summary": "Tongyi DeepResearch, a large language model with agentic capabilities, achieves top performance in various deep research tasks through an end-to-end training framework and automated data synthesis.",
            "ai_keywords": [
                "agentic large language model",
                "end-to-end training framework",
                "agentic mid-training",
                "agentic post-training",
                "scalable reasoning",
                "information seeking",
                "data synthesis pipeline",
                "customized environments",
                "Humanity's Last Exam",
                "BrowseComp",
                "BrowseComp-ZH",
                "WebWalkerQA",
                "xbench-DeepSearch",
                "FRAMES",
                "xbench-DeepSearch-2510"
            ],
            "organization": {
                "_id": "67d15cca6e2cf0e062dbfb54",
                "name": "AlibabaTongyiLab",
                "fullname": "TongyiLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
            }
        },
        "publishedAt": "2025-10-28T13:53:02.000Z",
        "title": "Tongyi DeepResearch Technical Report",
        "summary": "We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24701.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 149
        },
        "organization": {
            "_id": "67d15cca6e2cf0e062dbfb54",
            "name": "AlibabaTongyiLab",
            "fullname": "TongyiLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.24699",
            "authors": [
                {
                    "_id": "69017ce1646208eac0d1f3d6",
                    "name": "Rui Ye",
                    "hidden": false
                },
                {
                    "_id": "69017ce1646208eac0d1f3d7",
                    "name": "Zhongwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "69017ce1646208eac0d1f3d8",
                    "name": "Kuan Li",
                    "hidden": false
                },
                {
                    "_id": "69017ce1646208eac0d1f3d9",
                    "name": "Huifeng Yin",
                    "hidden": false
                },
                {
                    "_id": "69017ce1646208eac0d1f3da",
                    "name": "Zhengwei Tao",
                    "hidden": false
                },
                {
                    "_id": "69017ce1646208eac0d1f3db",
                    "user": {
                        "_id": "66e4019518a1920fb7ca19d7",
                        "avatarUrl": "/avatars/4f8be2f3244239510235c2ec6e8d38a6.svg",
                        "isPro": false,
                        "fullname": "Yida Zhao",
                        "user": "zhaoyd",
                        "type": "user"
                    },
                    "name": "Yida Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:45:01.706Z",
                    "hidden": false
                },
                {
                    "_id": "69017ce1646208eac0d1f3dc",
                    "user": {
                        "_id": "677f945ea82c316db164a180",
                        "avatarUrl": "/avatars/50ec99d971564944de3b1d9c17d50cfd.svg",
                        "isPro": false,
                        "fullname": "Liangcai Su",
                        "user": "HKU-Liangcai",
                        "type": "user"
                    },
                    "name": "Liangcai Su",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:44:38.182Z",
                    "hidden": false
                },
                {
                    "_id": "69017ce1646208eac0d1f3dd",
                    "name": "Liwen Zhang",
                    "hidden": false
                },
                {
                    "_id": "69017ce1646208eac0d1f3de",
                    "name": "Zile Qiao",
                    "hidden": false
                },
                {
                    "_id": "69017ce1646208eac0d1f3df",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "69017ce1646208eac0d1f3e0",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "69017ce1646208eac0d1f3e1",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "69017ce1646208eac0d1f3e2",
                    "name": "Siheng Chen",
                    "hidden": false
                },
                {
                    "_id": "69017ce1646208eac0d1f3e3",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "69017ce1646208eac0d1f3e4",
                    "name": "Yong Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T17:51:50.000Z",
            "submittedOnDailyAt": "2025-10-29T01:03:50.549Z",
            "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management",
            "submittedOnDailyBy": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
            },
            "summary": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.",
            "upvotes": 54,
            "discussionId": "69017ce2646208eac0d1f3e5",
            "ai_summary": "AgentFold, a novel proactive context management paradigm for LLM-based web agents, achieves superior performance on long-horizon tasks through dynamic context folding, surpassing larger models and proprietary agents.",
            "ai_keywords": [
                "LLM-based web agents",
                "ReAct-based agents",
                "context saturation",
                "context management",
                "cognitive workspace",
                "folding operation",
                "granular condensations",
                "deep consolidations",
                "BrowseComp",
                "BrowseComp-ZH",
                "DeepSeek-V3.1-671B-A37B",
                "OpenAI's o4-mini"
            ],
            "organization": {
                "_id": "67d15cca6e2cf0e062dbfb54",
                "name": "AlibabaTongyiLab",
                "fullname": "TongyiLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
            }
        },
        "publishedAt": "2025-10-28T13:51:50.000Z",
        "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management",
        "summary": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24699.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "fullname": "Jialong Wu",
            "name": "callanwu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 27
        },
        "organization": {
            "_id": "67d15cca6e2cf0e062dbfb54",
            "name": "AlibabaTongyiLab",
            "fullname": "TongyiLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.23763",
            "authors": [
                {
                    "_id": "690168ee646208eac0d1f2fb",
                    "user": {
                        "_id": "64c3c631e77ea9f28111172a",
                        "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
                        "isPro": false,
                        "fullname": "Siyin Wang (SII)",
                        "user": "sinwang",
                        "type": "user"
                    },
                    "name": "Siyin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:46:55.872Z",
                    "hidden": false
                },
                {
                    "_id": "690168ee646208eac0d1f2fc",
                    "name": "Jinlan Fu",
                    "hidden": false
                },
                {
                    "_id": "690168ee646208eac0d1f2fd",
                    "name": "Feihong Liu",
                    "hidden": false
                },
                {
                    "_id": "690168ee646208eac0d1f2fe",
                    "name": "Xinzhe He",
                    "hidden": false
                },
                {
                    "_id": "690168ee646208eac0d1f2ff",
                    "name": "Huangxuan Wu",
                    "hidden": false
                },
                {
                    "_id": "690168ee646208eac0d1f300",
                    "name": "Junhao Shi",
                    "hidden": false
                },
                {
                    "_id": "690168ee646208eac0d1f301",
                    "name": "Kexin Huang",
                    "hidden": false
                },
                {
                    "_id": "690168ee646208eac0d1f302",
                    "name": "Zhaoye Fei",
                    "hidden": false
                },
                {
                    "_id": "690168ee646208eac0d1f303",
                    "name": "Jingjing Gong",
                    "hidden": false
                },
                {
                    "_id": "690168ee646208eac0d1f304",
                    "name": "Zuxuan Wu",
                    "hidden": false
                },
                {
                    "_id": "690168ee646208eac0d1f305",
                    "name": "Yugang Jiang",
                    "hidden": false
                },
                {
                    "_id": "690168ee646208eac0d1f306",
                    "name": "See-Kiong Ng",
                    "hidden": false
                },
                {
                    "_id": "690168ee646208eac0d1f307",
                    "name": "Tat-Seng Chua",
                    "hidden": false
                },
                {
                    "_id": "690168ee646208eac0d1f308",
                    "name": "Xipeng Qiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T18:49:03.000Z",
            "submittedOnDailyAt": "2025-10-29T01:05:28.984Z",
            "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context",
            "submittedOnDailyBy": {
                "_id": "64c3c631e77ea9f28111172a",
                "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
                "isPro": false,
                "fullname": "Siyin Wang (SII)",
                "user": "sinwang",
                "type": "user"
            },
            "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.",
            "upvotes": 50,
            "discussionId": "690168ee646208eac0d1f309",
            "projectPage": "https://OpenMOSS.github.io/RoboOmni",
            "githubRepo": "https://github.com/OpenMOSS/RoboOmni",
            "ai_summary": "RoboOmni, a Perceiver-Thinker-Talker-Executor framework using end-to-end omni-modal LLMs, improves robotic manipulation by inferring user intentions from spoken dialogue, environmental sounds, and visual cues.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "Vision-Language-Action models",
                "cross-modal contextual instructions",
                "Perceiver-Thinker-Talker-Executor",
                "end-to-end omni-modal LLMs",
                "intention recognition",
                "interaction confirmation",
                "action execution",
                "spatiotemporal fusion",
                "OmniAction",
                "proactive intention recognition",
                "text-based baselines",
                "ASR-based baselines"
            ],
            "githubStars": 24,
            "organization": {
                "_id": "613b0dee83ec35d460684607",
                "name": "fnlp",
                "fullname": "OpenMOSS (SII, Fudan NLP)",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/xM_PjniEZ9fmDKtJN7PAG.png"
            }
        },
        "publishedAt": "2025-10-27T14:49:03.000Z",
        "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context",
        "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23763.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c3c631e77ea9f28111172a",
            "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
            "fullname": "Siyin Wang (SII)",
            "name": "sinwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "613b0dee83ec35d460684607",
            "name": "fnlp",
            "fullname": "OpenMOSS (SII, Fudan NLP)",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/xM_PjniEZ9fmDKtJN7PAG.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.23691",
            "authors": [
                {
                    "_id": "69018071646208eac0d1f445",
                    "name": "Zihao Wang",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f446",
                    "name": "Xujing Li",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f447",
                    "name": "Yining Ye",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f448",
                    "name": "Junjie Fang",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f449",
                    "user": {
                        "_id": "678a19ba39c63f336d24cc27",
                        "avatarUrl": "/avatars/5bec449236ac7d4a0936ef0dd4046761.svg",
                        "isPro": false,
                        "fullname": "Haoming Wang",
                        "user": "MingComplex",
                        "type": "user"
                    },
                    "name": "Haoming Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:43:14.628Z",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f44a",
                    "name": "Longxiang Liu",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f44b",
                    "name": "Shihao Liang",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f44c",
                    "name": "Junting Lu",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f44d",
                    "name": "Zhiyong Wu",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f44e",
                    "name": "Jiazhan Feng",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f44f",
                    "name": "Wanjun Zhong",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f450",
                    "name": "Zili Li",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f451",
                    "name": "Yu Wang",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f452",
                    "name": "Yu Miao",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f453",
                    "name": "Bo Zhou",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f454",
                    "name": "Yuanfan Li",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f455",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f456",
                    "user": {
                        "_id": "62dd6651169bd1d2ef2f3795",
                        "avatarUrl": "/avatars/bb6d1e4f31452c85036b5ef9a65321e3.svg",
                        "isPro": false,
                        "fullname": "Zhongkai Zhao",
                        "user": "KaneZZ",
                        "type": "user"
                    },
                    "name": "Zhongkai Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:43:25.797Z",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f457",
                    "name": "Faming Wu",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f458",
                    "name": "Zhengxuan Jiang",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f459",
                    "name": "Weihao Tan",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f45a",
                    "name": "Heyuan Yao",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f45b",
                    "name": "Shi Yan",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f45c",
                    "name": "Xiangyang Li",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f45d",
                    "name": "Yitao Liang",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f45e",
                    "name": "Yujia Qin",
                    "hidden": false
                },
                {
                    "_id": "69018071646208eac0d1f45f",
                    "name": "Guang Shi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T17:43:51.000Z",
            "submittedOnDailyAt": "2025-10-29T01:18:34.575Z",
            "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist\n  Multimodal Game Agents",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored to human-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories and multimodal data. Key techniques include a decaying continual\nloss to reduce causal confusion and an efficient Sparse-Thinking strategy that\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model on\nopen-world Minecraft tasks, is close to the generality of fresh humans in\nunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\nin FPS benchmarks. Scaling results on training-time and test-time confirm that\nthe unified action space sustains improvements when scaled to cross-game and\nmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntoward generalist agents with broad computer-use abilities.",
            "upvotes": 46,
            "discussionId": "69018071646208eac0d1f460",
            "projectPage": "https://seed-tars.com/game-tars",
            "ai_summary": "Game-TARS, a generalist game agent trained with a unified action space, achieves superior performance across various domains and benchmarks through large-scale pre-training and efficient reasoning strategies.",
            "ai_keywords": [
                "unified action space",
                "human-aligned native keyboard-mouse inputs",
                "decaying continual loss",
                "Sparse-Thinking strategy",
                "causal confusion",
                "open-world Minecraft tasks",
                "unseen web 3d games",
                "FPS benchmarks",
                "cross-game",
                "multimodal data",
                "generalist agents",
                "computer-use abilities"
            ],
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "publishedAt": "2025-10-27T13:43:51.000Z",
        "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist\n  Multimodal Game Agents",
        "summary": "We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored to human-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories and multimodal data. Key techniques include a decaying continual\nloss to reduce causal confusion and an efficient Sparse-Thinking strategy that\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model on\nopen-world Minecraft tasks, is close to the generality of fresh humans in\nunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\nin FPS benchmarks. Scaling results on training-time and test-time confirm that\nthe unified action space sustains improvements when scaled to cross-game and\nmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntoward generalist agents with broad computer-use abilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23691.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 149
        },
        "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24717",
            "authors": [
                {
                    "_id": "69019a46646208eac0d1f4fa",
                    "name": "Haoge Deng",
                    "hidden": false
                },
                {
                    "_id": "69019a46646208eac0d1f4fb",
                    "user": {
                        "_id": "6565bc5ee5aac326bfc98e39",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vIfHy9Y1yAK6A96UCHNBH.jpeg",
                        "isPro": false,
                        "fullname": "Ting Pan",
                        "user": "PhyscalX",
                        "type": "user"
                    },
                    "name": "Ting Pan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:41:15.558Z",
                    "hidden": false
                },
                {
                    "_id": "69019a46646208eac0d1f4fc",
                    "name": "Fan Zhang",
                    "hidden": false
                },
                {
                    "_id": "69019a46646208eac0d1f4fd",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "69019a46646208eac0d1f4fe",
                    "name": "Zhuoyan Luo",
                    "hidden": false
                },
                {
                    "_id": "69019a46646208eac0d1f4ff",
                    "name": "Yufeng Cui",
                    "hidden": false
                },
                {
                    "_id": "69019a46646208eac0d1f500",
                    "user": {
                        "_id": "656428b5462e5ebcbf537d4e",
                        "avatarUrl": "/avatars/cbedecc9c6f2afee2ca6b72efb593561.svg",
                        "isPro": false,
                        "fullname": "Wenxuan Wang",
                        "user": "Rookielion",
                        "type": "user"
                    },
                    "name": "Wenxuan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:41:13.079Z",
                    "hidden": false
                },
                {
                    "_id": "69019a46646208eac0d1f501",
                    "name": "Chunhua Shen",
                    "hidden": false
                },
                {
                    "_id": "69019a46646208eac0d1f502",
                    "name": "Shiguang Shan",
                    "hidden": false
                },
                {
                    "_id": "69019a46646208eac0d1f503",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "69019a46646208eac0d1f504",
                    "name": "Xinlong Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T17:59:57.000Z",
            "submittedOnDailyAt": "2025-10-29T03:10:28.004Z",
            "title": "Uniform Discrete Diffusion with Metric Path for Video Generation",
            "submittedOnDailyBy": {
                "_id": "64dd9404f2c8f66d52604fb3",
                "avatarUrl": "/avatars/1d7fec043e0848a145e4dcaf8973a575.svg",
                "isPro": false,
                "fullname": "Haoge Deng",
                "user": "Bitterdhg",
                "type": "user"
            },
            "summary": "Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA",
            "upvotes": 34,
            "discussionId": "69019a46646208eac0d1f505",
            "projectPage": "https://bitterdhg.github.io/URSA_page",
            "githubRepo": "https://github.com/baaivision/URSA",
            "ai_summary": "URSA, a discrete generative model, bridges the gap with continuous approaches in video generation by using a Linearized Metric Path and Resolution-dependent Timestep Shifting, achieving high-resolution and long-duration synthesis with fewer inference steps.",
            "ai_keywords": [
                "discrete generative modeling",
                "Uniform discRete diffuSion with metric pAth",
                "iterative global refinement",
                "discrete spatiotemporal tokens",
                "Linearized Metric Path",
                "Resolution-dependent Timestep Shifting",
                "asynchronous temporal fine-tuning",
                "interpolation",
                "image-to-video generation"
            ],
            "githubStars": 40,
            "organization": {
                "_id": "61be9739d2f9358e24ca0a4f",
                "name": "BAAI",
                "fullname": "Beijing Academy of Artificial Intelligence",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
            }
        },
        "publishedAt": "2025-10-28T13:59:57.000Z",
        "title": "Uniform Discrete Diffusion with Metric Path for Video Generation",
        "summary": "Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24717.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64dd9404f2c8f66d52604fb3",
            "avatarUrl": "/avatars/1d7fec043e0848a145e4dcaf8973a575.svg",
            "fullname": "Haoge Deng",
            "name": "Bitterdhg",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "61be9739d2f9358e24ca0a4f",
            "name": "BAAI",
            "fullname": "Beijing Academy of Artificial Intelligence",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24694",
            "authors": [
                {
                    "_id": "69017e04646208eac0d1f42e",
                    "user": {
                        "_id": "66e4019518a1920fb7ca19d7",
                        "avatarUrl": "/avatars/4f8be2f3244239510235c2ec6e8d38a6.svg",
                        "isPro": false,
                        "fullname": "Yida Zhao",
                        "user": "zhaoyd",
                        "type": "user"
                    },
                    "name": "Yida Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:43:33.267Z",
                    "hidden": false
                },
                {
                    "_id": "69017e04646208eac0d1f42f",
                    "name": "Kuan Li",
                    "hidden": false
                },
                {
                    "_id": "69017e04646208eac0d1f430",
                    "user": {
                        "_id": "6622132f63598534f96ca29d",
                        "avatarUrl": "/avatars/34e61fc3101f8ebce1ef7041f761e108.svg",
                        "isPro": false,
                        "fullname": "Xixi Wu",
                        "user": "xxwu",
                        "type": "user"
                    },
                    "name": "Xixi Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:43:30.221Z",
                    "hidden": false
                },
                {
                    "_id": "69017e04646208eac0d1f431",
                    "name": "Liwen Zhang",
                    "hidden": false
                },
                {
                    "_id": "69017e04646208eac0d1f432",
                    "name": "Dingchu Zhang",
                    "hidden": false
                },
                {
                    "_id": "69017e04646208eac0d1f433",
                    "name": "Baixuan Li",
                    "hidden": false
                },
                {
                    "_id": "69017e04646208eac0d1f434",
                    "name": "Maojia Song",
                    "hidden": false
                },
                {
                    "_id": "69017e04646208eac0d1f435",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "69017e04646208eac0d1f436",
                    "name": "Chenxi Wang",
                    "hidden": false
                },
                {
                    "_id": "69017e04646208eac0d1f437",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "69017e04646208eac0d1f438",
                    "name": "Kewei Tu",
                    "hidden": false
                },
                {
                    "_id": "69017e04646208eac0d1f439",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "69017e04646208eac0d1f43a",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "69017e04646208eac0d1f43b",
                    "name": "Yong Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T17:50:40.000Z",
            "submittedOnDailyAt": "2025-10-29T01:08:19.441Z",
            "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision",
            "submittedOnDailyBy": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
            },
            "summary": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents.",
            "upvotes": 21,
            "discussionId": "69017e04646208eac0d1f43c",
            "ai_summary": "Entity-aware Group Relative Policy Optimization (E-GRPO) enhances search agents by incorporating entity information into the reward function, improving accuracy and efficiency in knowledge-intensive tasks.",
            "ai_keywords": [
                "Group Relative Policy Optimization (GRPO)",
                "Entity-aware Group Relative Policy Optimization (E-GRPO)",
                "entity-centric synthetic data",
                "entity match rate",
                "near-misses",
                "question-answering (QA)",
                "deep research benchmarks",
                "tool calls"
            ],
            "organization": {
                "_id": "67d15cca6e2cf0e062dbfb54",
                "name": "AlibabaTongyiLab",
                "fullname": "TongyiLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
            }
        },
        "publishedAt": "2025-10-28T13:50:40.000Z",
        "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision",
        "summary": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24694.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "fullname": "Jialong Wu",
            "name": "callanwu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 27
        },
        "organization": {
            "_id": "67d15cca6e2cf0e062dbfb54",
            "name": "AlibabaTongyiLab",
            "fullname": "TongyiLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24563",
            "authors": [
                {
                    "_id": "690172eb646208eac0d1f357",
                    "name": "Hongrui Jia",
                    "hidden": false
                },
                {
                    "_id": "690172eb646208eac0d1f358",
                    "name": "Jitong Liao",
                    "hidden": false
                },
                {
                    "_id": "690172eb646208eac0d1f359",
                    "name": "Xi Zhang",
                    "hidden": false
                },
                {
                    "_id": "690172eb646208eac0d1f35a",
                    "user": {
                        "_id": "645b10e80c73ea27d13f7aca",
                        "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
                        "isPro": false,
                        "fullname": "xuhaiyang",
                        "user": "xhyandwyy",
                        "type": "user"
                    },
                    "name": "Haiyang Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:45:53.210Z",
                    "hidden": false
                },
                {
                    "_id": "690172eb646208eac0d1f35b",
                    "name": "Tianbao Xie",
                    "hidden": false
                },
                {
                    "_id": "690172eb646208eac0d1f35c",
                    "name": "Chaoya Jiang",
                    "hidden": false
                },
                {
                    "_id": "690172eb646208eac0d1f35d",
                    "name": "Ming Yan",
                    "hidden": false
                },
                {
                    "_id": "690172eb646208eac0d1f35e",
                    "name": "Si Liu",
                    "hidden": false
                },
                {
                    "_id": "690172eb646208eac0d1f35f",
                    "name": "Wei Ye",
                    "hidden": false
                },
                {
                    "_id": "690172eb646208eac0d1f360",
                    "name": "Fei Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T15:56:36.000Z",
            "submittedOnDailyAt": "2025-10-29T00:20:59.791Z",
            "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by the Model Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation, GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novel automated code-generation pipeline to create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-art\nmultimodal agents on OSWorld-MCP show that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively low tool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents\nand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io.",
            "upvotes": 21,
            "discussionId": "690172eb646208eac0d1f361",
            "projectPage": "https://osworld-mcp.github.io/",
            "ai_summary": "OSWorld-MCP is a benchmark that evaluates multimodal agents' tool invocation, GUI operation, and decision-making abilities, highlighting the importance of assessing tool usage in real-world scenarios.",
            "ai_keywords": [
                "multimodal agents",
                "Model Context Protocol (MCP)",
                "OSWorld-MCP",
                "automated code-generation pipeline",
                "GUI operation",
                "task success rates",
                "tool invocation rates"
            ],
            "organization": {
                "_id": "67d15cca6e2cf0e062dbfb54",
                "name": "AlibabaTongyiLab",
                "fullname": "TongyiLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
            }
        },
        "publishedAt": "2025-10-28T11:56:36.000Z",
        "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents",
        "summary": "With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by the Model Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation, GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novel automated code-generation pipeline to create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-art\nmultimodal agents on OSWorld-MCP show that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively low tool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents\nand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24563.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 149
        },
        "organization": {
            "_id": "67d15cca6e2cf0e062dbfb54",
            "name": "AlibabaTongyiLab",
            "fullname": "TongyiLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24698",
            "authors": [
                {
                    "_id": "69017d30646208eac0d1f3e7",
                    "name": "Baixuan Li",
                    "hidden": false
                },
                {
                    "_id": "69017d30646208eac0d1f3e8",
                    "name": "Dingchu Zhang",
                    "hidden": false
                },
                {
                    "_id": "69017d30646208eac0d1f3e9",
                    "name": "Jialong Wu",
                    "hidden": false
                },
                {
                    "_id": "69017d30646208eac0d1f3ea",
                    "name": "Wenbiao Yin",
                    "hidden": false
                },
                {
                    "_id": "69017d30646208eac0d1f3eb",
                    "name": "Zhengwei Tao",
                    "hidden": false
                },
                {
                    "_id": "69017d30646208eac0d1f3ec",
                    "user": {
                        "_id": "66e4019518a1920fb7ca19d7",
                        "avatarUrl": "/avatars/4f8be2f3244239510235c2ec6e8d38a6.svg",
                        "isPro": false,
                        "fullname": "Yida Zhao",
                        "user": "zhaoyd",
                        "type": "user"
                    },
                    "name": "Yida Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:44:30.204Z",
                    "hidden": false
                },
                {
                    "_id": "69017d30646208eac0d1f3ed",
                    "name": "Liwen Zhang",
                    "hidden": false
                },
                {
                    "_id": "69017d30646208eac0d1f3ee",
                    "name": "Haiyang Shen",
                    "hidden": false
                },
                {
                    "_id": "69017d30646208eac0d1f3ef",
                    "name": "Runnan Fang",
                    "hidden": false
                },
                {
                    "_id": "69017d30646208eac0d1f3f0",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "69017d30646208eac0d1f3f1",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "69017d30646208eac0d1f3f2",
                    "name": "Yong Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T17:51:50.000Z",
            "submittedOnDailyAt": "2025-10-29T01:04:44.983Z",
            "title": "ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking",
            "submittedOnDailyBy": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
            },
            "summary": "Parallel thinking expands exploration breadth, complementing the deep\nexploration of information-seeking (IS) agents to further enhance\nproblem-solving capability. However, conventional parallel thinking faces two\nkey challenges in this setting: inefficiency from repeatedly rolling out from\nscratch, and difficulty in integrating long-horizon reasoning trajectories\nduring answer generation, as limited context capacity prevents full\nconsideration of the reasoning process. To address these issues, we propose\nParallelMuse, a two-stage paradigm designed for deep IS agents. The first\nstage, Functionality-Specified Partial Rollout, partitions generated sequences\ninto functional regions and performs uncertainty-guided path reuse and\nbranching to enhance exploration efficiency. The second stage, Compressed\nReasoning Aggregation, exploits reasoning redundancy to losslessly compress\ninformation relevant to answer derivation and synthesize a coherent final\nanswer. Experiments across multiple open-source agents and benchmarks\ndemonstrate up to 62% performance improvement with a 10--30% reduction in\nexploratory token consumption.",
            "upvotes": 19,
            "discussionId": "69017d31646208eac0d1f3f3",
            "ai_summary": "ParallelMuse enhances problem-solving by efficiently reusing paths and compressing reasoning in deep information-seeking agents, improving performance and reducing token consumption.",
            "ai_keywords": [
                "ParallelMuse",
                "Functionality-Specified Partial Rollout",
                "uncertainty-guided path reuse",
                "branch",
                "Compressed Reasoning Aggregation",
                "reasoning redundancy",
                "information compression",
                "exploratory token consumption"
            ],
            "organization": {
                "_id": "67d15cca6e2cf0e062dbfb54",
                "name": "AlibabaTongyiLab",
                "fullname": "TongyiLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
            }
        },
        "publishedAt": "2025-10-28T13:51:50.000Z",
        "title": "ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking",
        "summary": "Parallel thinking expands exploration breadth, complementing the deep\nexploration of information-seeking (IS) agents to further enhance\nproblem-solving capability. However, conventional parallel thinking faces two\nkey challenges in this setting: inefficiency from repeatedly rolling out from\nscratch, and difficulty in integrating long-horizon reasoning trajectories\nduring answer generation, as limited context capacity prevents full\nconsideration of the reasoning process. To address these issues, we propose\nParallelMuse, a two-stage paradigm designed for deep IS agents. The first\nstage, Functionality-Specified Partial Rollout, partitions generated sequences\ninto functional regions and performs uncertainty-guided path reuse and\nbranching to enhance exploration efficiency. The second stage, Compressed\nReasoning Aggregation, exploits reasoning redundancy to losslessly compress\ninformation relevant to answer derivation and synthesize a coherent final\nanswer. Experiments across multiple open-source agents and benchmarks\ndemonstrate up to 62% performance improvement with a 10--30% reduction in\nexploratory token consumption.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24698.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "fullname": "Jialong Wu",
            "name": "callanwu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 27
        },
        "organization": {
            "_id": "67d15cca6e2cf0e062dbfb54",
            "name": "AlibabaTongyiLab",
            "fullname": "TongyiLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24697",
            "authors": [
                {
                    "_id": "69017d55646208eac0d1f400",
                    "name": "Zhengwei Tao",
                    "hidden": false
                },
                {
                    "_id": "69017d55646208eac0d1f401",
                    "name": "Haiyang Shen",
                    "hidden": false
                },
                {
                    "_id": "69017d55646208eac0d1f402",
                    "name": "Baixuan Li",
                    "hidden": false
                },
                {
                    "_id": "69017d55646208eac0d1f403",
                    "name": "Wenbiao Yin",
                    "hidden": false
                },
                {
                    "_id": "69017d55646208eac0d1f404",
                    "name": "Jialong Wu",
                    "hidden": false
                },
                {
                    "_id": "69017d55646208eac0d1f405",
                    "name": "Kuan Li",
                    "hidden": false
                },
                {
                    "_id": "69017d55646208eac0d1f406",
                    "name": "Zhongwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "69017d55646208eac0d1f407",
                    "name": "Huifeng Yin",
                    "hidden": false
                },
                {
                    "_id": "69017d55646208eac0d1f408",
                    "name": "Rui Ye",
                    "hidden": false
                },
                {
                    "_id": "69017d55646208eac0d1f409",
                    "name": "Liwen Zhang",
                    "hidden": false
                },
                {
                    "_id": "69017d55646208eac0d1f40a",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "69017d55646208eac0d1f40b",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "69017d55646208eac0d1f40c",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "69017d55646208eac0d1f40d",
                    "name": "Yong Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T17:51:42.000Z",
            "submittedOnDailyAt": "2025-10-29T01:05:29.846Z",
            "title": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling\n  Info-Rich Seeking",
            "submittedOnDailyBy": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
            },
            "summary": "Large Language Model (LLM)-based agents have emerged as a transformative\napproach for open-ended problem solving, with information seeking (IS) being a\ncore capability that enables autonomous reasoning and decision-making. While\nprior research has largely focused on improving retrieval depth, we observe\nthat current IS agents often suffer from low search efficiency, which in turn\nconstrains overall performance. A key factor underlying this inefficiency is\nthe sparsity of target entities in training tasks, which limits opportunities\nfor agents to learn and generalize efficient search behaviors. To address these\nchallenges, we propose WebLeaper, a framework for constructing high-coverage IS\ntasks and generating efficient solution trajectories. We formulate IS as a\ntree-structured reasoning problem, enabling a substantially larger set of\ntarget entities to be embedded within a constrained context. Leveraging curated\nWikipedia tables, we propose three variants for synthesizing IS tasks, Basic,\nUnion, and Reverse-Union, to systematically increase both IS efficiency and\nefficacy. Finally, we curate training trajectories by retaining only those that\nare simultaneously accurate and efficient, ensuring that the model is optimized\nfor both correctness and search performance. Extensive experiments on both\nbasic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,\nGAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method\nconsistently achieves improvements in both effectiveness and efficiency over\nstrong baselines.",
            "upvotes": 19,
            "discussionId": "69017d55646208eac0d1f40e",
            "ai_summary": "WebLeaper framework improves information seeking efficiency and effectiveness by constructing high-coverage tasks and generating efficient solution trajectories using tree-structured reasoning and curated Wikipedia tables.",
            "ai_keywords": [
                "Large Language Model (LLM)",
                "information seeking (IS)",
                "search efficiency",
                "target entities",
                "tree-structured reasoning",
                "WebLeaper",
                "Basic",
                "Union",
                "Reverse-Union",
                "training trajectories",
                "BrowserComp",
                "GAIA",
                "xbench-DeepSearch",
                "WideSearch",
                "Seal-0"
            ],
            "organization": {
                "_id": "67d15cca6e2cf0e062dbfb54",
                "name": "AlibabaTongyiLab",
                "fullname": "TongyiLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
            }
        },
        "publishedAt": "2025-10-28T13:51:42.000Z",
        "title": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling\n  Info-Rich Seeking",
        "summary": "Large Language Model (LLM)-based agents have emerged as a transformative\napproach for open-ended problem solving, with information seeking (IS) being a\ncore capability that enables autonomous reasoning and decision-making. While\nprior research has largely focused on improving retrieval depth, we observe\nthat current IS agents often suffer from low search efficiency, which in turn\nconstrains overall performance. A key factor underlying this inefficiency is\nthe sparsity of target entities in training tasks, which limits opportunities\nfor agents to learn and generalize efficient search behaviors. To address these\nchallenges, we propose WebLeaper, a framework for constructing high-coverage IS\ntasks and generating efficient solution trajectories. We formulate IS as a\ntree-structured reasoning problem, enabling a substantially larger set of\ntarget entities to be embedded within a constrained context. Leveraging curated\nWikipedia tables, we propose three variants for synthesizing IS tasks, Basic,\nUnion, and Reverse-Union, to systematically increase both IS efficiency and\nefficacy. Finally, we curate training trajectories by retaining only those that\nare simultaneously accurate and efficient, ensuring that the model is optimized\nfor both correctness and search performance. Extensive experiments on both\nbasic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,\nGAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method\nconsistently achieves improvements in both effectiveness and efficiency over\nstrong baselines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24697.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "fullname": "Jialong Wu",
            "name": "callanwu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 27
        },
        "organization": {
            "_id": "67d15cca6e2cf0e062dbfb54",
            "name": "AlibabaTongyiLab",
            "fullname": "TongyiLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24695",
            "authors": [
                {
                    "_id": "69017da8646208eac0d1f419",
                    "user": {
                        "_id": "65e6970d135c27ea806526fe",
                        "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg",
                        "isPro": false,
                        "fullname": "Xuanzhong Chen",
                        "user": "chenxz",
                        "type": "user"
                    },
                    "name": "Xuanzhong Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:44:05.057Z",
                    "hidden": false
                },
                {
                    "_id": "69017da8646208eac0d1f41a",
                    "name": "Zile Qiao",
                    "hidden": false
                },
                {
                    "_id": "69017da8646208eac0d1f41b",
                    "user": {
                        "_id": "63f06116f1a47aaea5bd497b",
                        "avatarUrl": "/avatars/7d99ffa59c4579599e852a0ffb261268.svg",
                        "isPro": false,
                        "fullname": "Guoxin Chen",
                        "user": "GuoxinChen",
                        "type": "user"
                    },
                    "name": "Guoxin Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:44:14.903Z",
                    "hidden": false
                },
                {
                    "_id": "69017da8646208eac0d1f41c",
                    "user": {
                        "_id": "677f945ea82c316db164a180",
                        "avatarUrl": "/avatars/50ec99d971564944de3b1d9c17d50cfd.svg",
                        "isPro": false,
                        "fullname": "Liangcai Su",
                        "user": "HKU-Liangcai",
                        "type": "user"
                    },
                    "name": "Liangcai Su",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:43:39.443Z",
                    "hidden": false
                },
                {
                    "_id": "69017da8646208eac0d1f41d",
                    "name": "Zhen Zhang",
                    "hidden": false
                },
                {
                    "_id": "69017da8646208eac0d1f41e",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "69017da8646208eac0d1f41f",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "69017da8646208eac0d1f420",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "69017da8646208eac0d1f421",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "69017da8646208eac0d1f422",
                    "name": "Yong Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T17:50:47.000Z",
            "submittedOnDailyAt": "2025-10-29T01:07:28.731Z",
            "title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with\n  ZPD-Guided Data Synthesis",
            "submittedOnDailyBy": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
            },
            "summary": "Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents.",
            "upvotes": 19,
            "discussionId": "69017da8646208eac0d1f423",
            "ai_summary": "A ZPD-guided data synthesis approach enhances large language model capabilities by training them on tasks just beyond their current abilities, leading to state-of-the-art performance on complex benchmarks.",
            "ai_keywords": [
                "Zone of Proximal Development (ZPD)",
                "AgentFrontier Engine",
                "high-quality",
                "multidisciplinary data",
                "continued pre-training",
                "targeted post-training",
                "ZPD Exam",
                "dynamic benchmark",
                "complex reasoning tasks",
                "AgentFrontier-30B-A3B model",
                "Humanity's Last Exam"
            ],
            "organization": {
                "_id": "67d15cca6e2cf0e062dbfb54",
                "name": "AlibabaTongyiLab",
                "fullname": "TongyiLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
            }
        },
        "publishedAt": "2025-10-28T13:50:47.000Z",
        "title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with\n  ZPD-Guided Data Synthesis",
        "summary": "Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24695.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "fullname": "Jialong Wu",
            "name": "callanwu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 27
        },
        "organization": {
            "_id": "67d15cca6e2cf0e062dbfb54",
            "name": "AlibabaTongyiLab",
            "fullname": "TongyiLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24657",
            "authors": [
                {
                    "_id": "69018a99646208eac0d1f49d",
                    "name": "Xuanpu Zhang",
                    "hidden": false
                },
                {
                    "_id": "69018a99646208eac0d1f49e",
                    "name": "Xuesong Niu",
                    "hidden": false
                },
                {
                    "_id": "69018a99646208eac0d1f49f",
                    "name": "Ruidong Chen",
                    "hidden": false
                },
                {
                    "_id": "69018a99646208eac0d1f4a0",
                    "name": "Dan Song",
                    "hidden": false
                },
                {
                    "_id": "69018a99646208eac0d1f4a1",
                    "name": "Jianhao Zeng",
                    "hidden": false
                },
                {
                    "_id": "69018a99646208eac0d1f4a2",
                    "user": {
                        "_id": "647076467fd7ecdbd0ea03b1",
                        "avatarUrl": "/avatars/6e090ea5f88977c6f70544175094c2a6.svg",
                        "isPro": false,
                        "fullname": "Penghui Du",
                        "user": "eternaldolphin",
                        "type": "user"
                    },
                    "name": "Penghui Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:41:52.279Z",
                    "hidden": false
                },
                {
                    "_id": "69018a99646208eac0d1f4a3",
                    "name": "Haoxiang Cao",
                    "hidden": false
                },
                {
                    "_id": "69018a99646208eac0d1f4a4",
                    "user": {
                        "_id": "68e741ea3edb0ff47e20084e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68e741ea3edb0ff47e20084e/OyBgFqcU4QWyPF_K58Gt5.jpeg",
                        "isPro": false,
                        "fullname": "Wu Kai",
                        "user": "KaiiWuu1993",
                        "type": "user"
                    },
                    "name": "Kai Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:41:48.100Z",
                    "hidden": false
                },
                {
                    "_id": "69018a99646208eac0d1f4a5",
                    "name": "An-an Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T17:22:44.000Z",
            "submittedOnDailyAt": "2025-10-29T02:06:38.798Z",
            "title": "Group Relative Attention Guidance for Image Editing",
            "submittedOnDailyBy": {
                "_id": "68e741ea3edb0ff47e20084e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68e741ea3edb0ff47e20084e/OyBgFqcU4QWyPF_K58Gt5.jpeg",
                "isPro": false,
                "fullname": "Wu Kai",
                "user": "KaiiWuu1993",
                "type": "user"
            },
            "summary": "Recently, image editing based on Diffusion-in-Transformer models has\nundergone rapid development. However, existing editing methods often lack\neffective control over the degree of editing, limiting their ability to achieve\nmore customized results. To address this limitation, we investigate the\nMM-Attention mechanism within the DiT model and observe that the Query and Key\ntokens share a bias vector that is only layer-dependent. We interpret this bias\nas representing the model's inherent editing behavior, while the delta between\neach token and its corresponding bias encodes the content-specific editing\nsignals. Based on this insight, we propose Group Relative Attention Guidance, a\nsimple yet effective method that reweights the delta values of different tokens\nto modulate the focus of the model on the input image relative to the editing\ninstruction, enabling continuous and fine-grained control over editing\nintensity without any tuning. Extensive experiments conducted on existing image\nediting frameworks demonstrate that GRAG can be integrated with as few as four\nlines of code, consistently enhancing editing quality. Moreover, compared to\nthe commonly used Classifier-Free Guidance, GRAG achieves smoother and more\nprecise control over the degree of editing. Our code will be released at\nhttps://github.com/little-misfit/GRAG-Image-Editing.",
            "upvotes": 19,
            "discussionId": "69018a99646208eac0d1f4a6",
            "projectPage": "https://little-misfit.github.io/GRAG-Image-Editing/",
            "githubRepo": "https://github.com/little-misfit/GRAG-Image-Editing",
            "ai_summary": "Group Relative Attention Guidance enhances image editing quality by modulating token deltas in Diffusion-in-Transformer models, providing fine-grained control over editing intensity.",
            "ai_keywords": [
                "Diffusion-in-Transformer",
                "MM-Attention",
                "Query tokens",
                "Key tokens",
                "bias vector",
                "Group Relative Attention Guidance",
                "Classifier-Free Guidance"
            ],
            "githubStars": 13,
            "organization": {
                "_id": "665f02ce9f9e5b38d0a256a8",
                "name": "Kwai-Kolors",
                "fullname": "Kolors Team, Kuaishou Technology",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f0babaef9cc6810cec02ff/sVnELkcfVo5kxg5308rkr.png"
            }
        },
        "publishedAt": "2025-10-28T13:22:44.000Z",
        "title": "Group Relative Attention Guidance for Image Editing",
        "summary": "Recently, image editing based on Diffusion-in-Transformer models has\nundergone rapid development. However, existing editing methods often lack\neffective control over the degree of editing, limiting their ability to achieve\nmore customized results. To address this limitation, we investigate the\nMM-Attention mechanism within the DiT model and observe that the Query and Key\ntokens share a bias vector that is only layer-dependent. We interpret this bias\nas representing the model's inherent editing behavior, while the delta between\neach token and its corresponding bias encodes the content-specific editing\nsignals. Based on this insight, we propose Group Relative Attention Guidance, a\nsimple yet effective method that reweights the delta values of different tokens\nto modulate the focus of the model on the input image relative to the editing\ninstruction, enabling continuous and fine-grained control over editing\nintensity without any tuning. Extensive experiments conducted on existing image\nediting frameworks demonstrate that GRAG can be integrated with as few as four\nlines of code, consistently enhancing editing quality. Moreover, compared to\nthe commonly used Classifier-Free Guidance, GRAG achieves smoother and more\nprecise control over the degree of editing. Our code will be released at\nhttps://github.com/little-misfit/GRAG-Image-Editing.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24657.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "68e741ea3edb0ff47e20084e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68e741ea3edb0ff47e20084e/OyBgFqcU4QWyPF_K58Gt5.jpeg",
            "fullname": "Wu Kai",
            "name": "KaiiWuu1993",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "665f02ce9f9e5b38d0a256a8",
            "name": "Kwai-Kolors",
            "fullname": "Kolors Team, Kuaishou Technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f0babaef9cc6810cec02ff/sVnELkcfVo5kxg5308rkr.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.23642",
            "authors": [
                {
                    "_id": "6901768a646208eac0d1f37d",
                    "user": {
                        "_id": "64de37ee5e192985054be575",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
                        "isPro": false,
                        "fullname": "Yuansheng Ni",
                        "user": "yuanshengni",
                        "type": "user"
                    },
                    "name": "Yuansheng Ni",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:45:46.613Z",
                    "hidden": false
                },
                {
                    "_id": "6901768a646208eac0d1f37e",
                    "user": {
                        "_id": "670d7aed372cb8fadbd270bb",
                        "avatarUrl": "/avatars/652c8b90d492e3e5db6c735d69ae0991.svg",
                        "isPro": false,
                        "fullname": "Songcheng Cai",
                        "user": "SongchengCai",
                        "type": "user"
                    },
                    "name": "Songcheng Cai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:45:43.616Z",
                    "hidden": false
                },
                {
                    "_id": "6901768a646208eac0d1f37f",
                    "name": "Xiangchao Chen",
                    "hidden": false
                },
                {
                    "_id": "6901768a646208eac0d1f380",
                    "name": "Jiarong Liang",
                    "hidden": false
                },
                {
                    "_id": "6901768a646208eac0d1f381",
                    "name": "Zhiheng Lyu",
                    "hidden": false
                },
                {
                    "_id": "6901768a646208eac0d1f382",
                    "name": "Jiaqi Deng",
                    "hidden": false
                },
                {
                    "_id": "6901768a646208eac0d1f383",
                    "name": "Kai Zou",
                    "hidden": false
                },
                {
                    "_id": "6901768a646208eac0d1f384",
                    "user": {
                        "_id": "65358802a920f38780b3248a",
                        "avatarUrl": "/avatars/9415510b598079973c2b0436ad12db9c.svg",
                        "isPro": false,
                        "fullname": "Ping Nie",
                        "user": "pingnieuk",
                        "type": "user"
                    },
                    "name": "Ping Nie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:45:39.951Z",
                    "hidden": false
                },
                {
                    "_id": "6901768a646208eac0d1f385",
                    "name": "Fei Yuan",
                    "hidden": false
                },
                {
                    "_id": "6901768a646208eac0d1f386",
                    "name": "Xiang Yue",
                    "hidden": false
                },
                {
                    "_id": "6901768a646208eac0d1f387",
                    "name": "Wenhu Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-24T18:03:57.000Z",
            "submittedOnDailyAt": "2025-10-29T01:48:21.691Z",
            "title": "VisCoder2: Building Multi-Language Visualization Coding Agents",
            "submittedOnDailyBy": {
                "_id": "64de37ee5e192985054be575",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
                "isPro": false,
                "fullname": "Yuansheng Ni",
                "user": "yuanshengni",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have recently enabled coding agents capable of\ngenerating, executing, and revising visualization code. However, existing\nmodels often fail in practical workflows due to limited language coverage,\nunreliable execution, and lack of iterative correction mechanisms. Progress has\nbeen constrained by narrow datasets and benchmarks that emphasize single-round\ngeneration and single-language tasks. To address these challenges, we introduce\nthree complementary resources for advancing visualization coding agents.\nVisCode-Multi-679K is a large-scale, supervised dataset containing 679K\nvalidated and executable visualization samples with multi-turn correction\ndialogues across 12 programming languages. VisPlotBench is a benchmark for\nsystematic evaluation, featuring executable tasks, rendered outputs, and\nprotocols for both initial generation and multi-round self-debug. Finally, we\npresent VisCoder2, a family of multi-language visualization models trained on\nVisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms\nstrong open-source baselines and approaches the performance of proprietary\nmodels like GPT-4.1, with further gains from iterative self-debug, reaching\n82.4% overall execution pass rate at the 32B scale, particularly in symbolic or\ncompiler-dependent languages.",
            "upvotes": 18,
            "discussionId": "6901768a646208eac0d1f388",
            "projectPage": "https://tiger-ai-lab.github.io/VisCoder2/",
            "githubRepo": "https://github.com/TIGER-AI-Lab/VisCoder2",
            "ai_summary": "VisCoder2, a family of multi-language visualization models, outperforms open-source baselines and approaches proprietary models by leveraging VisCode-Multi-679K and VisPlotBench for iterative self-debugging and multi-turn correction.",
            "ai_keywords": [
                "large language models",
                "coding agents",
                "visualization code",
                "language coverage",
                "execution reliability",
                "iterative correction mechanisms",
                "VisCode-Multi-679K",
                "VisPlotBench",
                "multi-language visualization models",
                "VisCoder2",
                "execution pass rate",
                "symbolic languages",
                "compiler-dependent languages"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "6313a90017838d05194fd282",
                "name": "TIGER-Lab",
                "fullname": "TIGER-Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/Noi3Qq3RYz8Jdq6BaFteq.png"
            }
        },
        "publishedAt": "2025-10-24T14:03:57.000Z",
        "title": "VisCoder2: Building Multi-Language Visualization Coding Agents",
        "summary": "Large language models (LLMs) have recently enabled coding agents capable of\ngenerating, executing, and revising visualization code. However, existing\nmodels often fail in practical workflows due to limited language coverage,\nunreliable execution, and lack of iterative correction mechanisms. Progress has\nbeen constrained by narrow datasets and benchmarks that emphasize single-round\ngeneration and single-language tasks. To address these challenges, we introduce\nthree complementary resources for advancing visualization coding agents.\nVisCode-Multi-679K is a large-scale, supervised dataset containing 679K\nvalidated and executable visualization samples with multi-turn correction\ndialogues across 12 programming languages. VisPlotBench is a benchmark for\nsystematic evaluation, featuring executable tasks, rendered outputs, and\nprotocols for both initial generation and multi-round self-debug. Finally, we\npresent VisCoder2, a family of multi-language visualization models trained on\nVisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms\nstrong open-source baselines and approaches the performance of proprietary\nmodels like GPT-4.1, with further gains from iterative self-debug, reaching\n82.4% overall execution pass rate at the 32B scale, particularly in symbolic or\ncompiler-dependent languages.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23642.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64de37ee5e192985054be575",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
            "fullname": "Yuansheng Ni",
            "name": "yuanshengni",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "organization": {
            "_id": "6313a90017838d05194fd282",
            "name": "TIGER-Lab",
            "fullname": "TIGER-Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/Noi3Qq3RYz8Jdq6BaFteq.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.24711",
            "authors": [
                {
                    "_id": "6901814d646208eac0d1f468",
                    "name": "Yujie Wei",
                    "hidden": false
                },
                {
                    "_id": "6901814d646208eac0d1f469",
                    "name": "Shiwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6901814d646208eac0d1f46a",
                    "name": "Hangjie Yuan",
                    "hidden": false
                },
                {
                    "_id": "6901814d646208eac0d1f46b",
                    "name": "Yujin Han",
                    "hidden": false
                },
                {
                    "_id": "6901814d646208eac0d1f46c",
                    "name": "Zhekai Chen",
                    "hidden": false
                },
                {
                    "_id": "6901814d646208eac0d1f46d",
                    "name": "Jiayu Wang",
                    "hidden": false
                },
                {
                    "_id": "6901814d646208eac0d1f46e",
                    "name": "Difan Zou",
                    "hidden": false
                },
                {
                    "_id": "6901814d646208eac0d1f46f",
                    "name": "Xihui Liu",
                    "hidden": false
                },
                {
                    "_id": "6901814d646208eac0d1f470",
                    "name": "Yingya Zhang",
                    "hidden": false
                },
                {
                    "_id": "6901814d646208eac0d1f471",
                    "name": "Yu Liu",
                    "hidden": false
                },
                {
                    "_id": "6901814d646208eac0d1f472",
                    "name": "Hongming Shan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T17:59:02.000Z",
            "submittedOnDailyAt": "2025-10-29T01:21:59.327Z",
            "title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance",
            "submittedOnDailyBy": {
                "_id": "637f70d6fab5db9101c3dfc8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f70d6fab5db9101c3dfc8/NgkYNXWLDavLbrnCby2Fl.jpeg",
                "isPro": false,
                "fullname": "Yujie Wei",
                "user": "weilllllls",
                "type": "user"
            },
            "summary": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available.",
            "upvotes": 17,
            "discussionId": "6901814e646208eac0d1f473",
            "ai_summary": "ProMoE, an MoE framework with conditional and prototypical routing, enhances expert specialization in Diffusion Transformers, achieving state-of-the-art performance on ImageNet.",
            "ai_keywords": [
                "Mixture-of-Experts",
                "MoE",
                "Diffusion Transformers",
                "DiTs",
                "ProMoE",
                "two-step router",
                "conditional routing",
                "prototypical routing",
                "learnable prototypes",
                "latent space",
                "routing contrastive loss",
                "intra-expert coherence",
                "inter-expert diversity",
                "Rectified Flow",
                "DDPM"
            ],
            "organization": {
                "_id": "67d15cca6e2cf0e062dbfb54",
                "name": "AlibabaTongyiLab",
                "fullname": "TongyiLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
            }
        },
        "publishedAt": "2025-10-28T13:59:02.000Z",
        "title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance",
        "summary": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24711.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "637f70d6fab5db9101c3dfc8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f70d6fab5db9101c3dfc8/NgkYNXWLDavLbrnCby2Fl.jpeg",
            "fullname": "Yujie Wei",
            "name": "weilllllls",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "67d15cca6e2cf0e062dbfb54",
            "name": "AlibabaTongyiLab",
            "fullname": "TongyiLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24693",
            "authors": [
                {
                    "_id": "69018af4646208eac0d1f4a8",
                    "name": "Zihan Liu",
                    "hidden": false
                },
                {
                    "_id": "69018af4646208eac0d1f4a9",
                    "name": "Zhikang Niu",
                    "hidden": false
                },
                {
                    "_id": "69018af4646208eac0d1f4aa",
                    "name": "Qiuyang Xiao",
                    "hidden": false
                },
                {
                    "_id": "69018af4646208eac0d1f4ab",
                    "name": "Zhisheng Zheng",
                    "hidden": false
                },
                {
                    "_id": "69018af4646208eac0d1f4ac",
                    "name": "Ruoqi Yuan",
                    "hidden": false
                },
                {
                    "_id": "69018af4646208eac0d1f4ad",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/kajwuVzd4pDucSPlwghxo.png",
                        "isPro": true,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:41:27.000Z",
                    "hidden": false
                },
                {
                    "_id": "69018af4646208eac0d1f4ae",
                    "name": "Yuhang Cao",
                    "hidden": false
                },
                {
                    "_id": "69018af4646208eac0d1f4af",
                    "name": "Xiaoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "69018af4646208eac0d1f4b0",
                    "name": "Jianze Liang",
                    "hidden": false
                },
                {
                    "_id": "69018af4646208eac0d1f4b1",
                    "name": "Xie Chen",
                    "hidden": false
                },
                {
                    "_id": "69018af4646208eac0d1f4b2",
                    "name": "Leilei Sun",
                    "hidden": false
                },
                {
                    "_id": "69018af4646208eac0d1f4b3",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "69018af4646208eac0d1f4b4",
                    "user": {
                        "_id": "64b4eec4faa3181a5eab9c46",
                        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
                        "isPro": true,
                        "fullname": "Jiaqi Wang",
                        "user": "myownskyW7",
                        "type": "user"
                    },
                    "name": "Jiaqi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:41:23.657Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T17:50:34.000Z",
            "submittedOnDailyAt": "2025-10-29T02:16:52.438Z",
            "title": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D\n  Intelligence",
            "submittedOnDailyBy": {
                "_id": "6454d7cb9d37c3fb33266453",
                "avatarUrl": "/avatars/4942450f4c1017ea6312d23bac48d51a.svg",
                "isPro": false,
                "fullname": "Zihan Liu",
                "user": "LiuZH-19",
                "type": "user"
            },
            "summary": "Despite rapid progress in Multi-modal Large Language Models and Large\nAudio-Language Models, existing audio benchmarks largely test semantics that\ncan be recovered from text captions, masking deficits in fine-grained\nperceptual reasoning. We formalize audio 4D intelligence that is defined as\nreasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to\nmeasure it. STAR-Bench combines a Foundational Acoustic Perception setting (six\nattributes under absolute and relative regimes) with a Holistic Spatio-Temporal\nReasoning setting that includes segment reordering for continuous and discrete\nprocesses and spatial tasks spanning static localization, multi-source\nrelations, and dynamic trajectories. Our data curation pipeline uses two\nmethods to ensure high-quality samples. For foundational tasks, we use\nprocedurally synthesized and physics-simulated audio. For holistic data, we\nfollow a four-stage process that includes human annotation and final selection\nbased on human performance. Unlike prior benchmarks where caption-only\nanswering reduces accuracy slightly, STAR-Bench induces far larger drops\n(-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically\nhard-to-describe cues. Evaluating 19 models reveals substantial gaps compared\nwith humans and a capability hierarchy: closed-source models are bottlenecked\nby fine-grained perception, while open-source models lag across perception,\nknowledge, and reasoning. Our STAR-Bench provides critical insights and a clear\npath forward for developing future models with a more robust understanding of\nthe physical world.",
            "upvotes": 17,
            "discussionId": "69018af5646208eac0d1f4b5",
            "projectPage": "https://internlm.github.io/StarBench/",
            "githubRepo": "https://github.com/InternLM/StarBench",
            "ai_summary": "STAR-Bench measures audio 4D intelligence by evaluating sound dynamics in time and 3D space, revealing gaps in fine-grained perceptual reasoning among existing models.",
            "ai_keywords": [
                "Multi-modal Large Language Models",
                "Large Audio-Language Models",
                "audio benchmarks",
                "sound dynamics",
                "3D space",
                "STAR-Bench",
                "Foundational Acoustic Perception",
                "Holistic Spatio-Temporal Reasoning",
                "segment reordering",
                "static localization",
                "multi-source relations",
                "dynamic trajectories",
                "procedurally synthesized",
                "physics-simulated audio",
                "human annotation",
                "human performance",
                "closed-source models",
                "open-source models",
                "fine-grained perception",
                "knowledge",
                "reasoning"
            ],
            "githubStars": 23,
            "organization": {
                "_id": "64a2d5fa81252883206f24c9",
                "name": "internlm",
                "fullname": "Intern Large Models",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"
            }
        },
        "publishedAt": "2025-10-28T13:50:34.000Z",
        "title": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D\n  Intelligence",
        "summary": "Despite rapid progress in Multi-modal Large Language Models and Large\nAudio-Language Models, existing audio benchmarks largely test semantics that\ncan be recovered from text captions, masking deficits in fine-grained\nperceptual reasoning. We formalize audio 4D intelligence that is defined as\nreasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to\nmeasure it. STAR-Bench combines a Foundational Acoustic Perception setting (six\nattributes under absolute and relative regimes) with a Holistic Spatio-Temporal\nReasoning setting that includes segment reordering for continuous and discrete\nprocesses and spatial tasks spanning static localization, multi-source\nrelations, and dynamic trajectories. Our data curation pipeline uses two\nmethods to ensure high-quality samples. For foundational tasks, we use\nprocedurally synthesized and physics-simulated audio. For holistic data, we\nfollow a four-stage process that includes human annotation and final selection\nbased on human performance. Unlike prior benchmarks where caption-only\nanswering reduces accuracy slightly, STAR-Bench induces far larger drops\n(-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically\nhard-to-describe cues. Evaluating 19 models reveals substantial gaps compared\nwith humans and a capability hierarchy: closed-source models are bottlenecked\nby fine-grained perception, while open-source models lag across perception,\nknowledge, and reasoning. Our STAR-Bench provides critical insights and a clear\npath forward for developing future models with a more robust understanding of\nthe physical world.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24693.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6454d7cb9d37c3fb33266453",
            "avatarUrl": "/avatars/4942450f4c1017ea6312d23bac48d51a.svg",
            "fullname": "Zihan Liu",
            "name": "LiuZH-19",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "organization": {
            "_id": "64a2d5fa81252883206f24c9",
            "name": "internlm",
            "fullname": "Intern Large Models",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24514",
            "authors": [
                {
                    "_id": "690173b7646208eac0d1f36c",
                    "name": "Huanyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "690173b7646208eac0d1f36d",
                    "name": "Wenshan Wu",
                    "hidden": false
                },
                {
                    "_id": "690173b7646208eac0d1f36e",
                    "name": "Chengzu Li",
                    "hidden": false
                },
                {
                    "_id": "690173b7646208eac0d1f36f",
                    "name": "Ning Shang",
                    "hidden": false
                },
                {
                    "_id": "690173b7646208eac0d1f370",
                    "name": "Yan Xia",
                    "hidden": false
                },
                {
                    "_id": "690173b7646208eac0d1f371",
                    "name": "Yangyu Huang",
                    "hidden": false
                },
                {
                    "_id": "690173b7646208eac0d1f372",
                    "name": "Yifan Zhang",
                    "hidden": false
                },
                {
                    "_id": "690173b7646208eac0d1f373",
                    "name": "Li Dong",
                    "hidden": false
                },
                {
                    "_id": "690173b7646208eac0d1f374",
                    "name": "Zhang Zhang",
                    "hidden": false
                },
                {
                    "_id": "690173b7646208eac0d1f375",
                    "name": "Liang Wang",
                    "hidden": false
                },
                {
                    "_id": "690173b7646208eac0d1f376",
                    "name": "Tieniu Tan",
                    "hidden": false
                },
                {
                    "_id": "690173b7646208eac0d1f377",
                    "name": "Furu Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T15:26:20.000Z",
            "submittedOnDailyAt": "2025-10-29T01:05:14.260Z",
            "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal\n  Reasoning in MLLMs",
            "submittedOnDailyBy": {
                "_id": "65e816bbcfd12cd15b052a0e",
                "avatarUrl": "/avatars/4d92da469afdba8cd7dc645b98236011.svg",
                "isPro": false,
                "fullname": "Huanyu_Zhang",
                "user": "huanyu112",
                "type": "user"
            },
            "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.",
            "upvotes": 17,
            "discussionId": "690173b8646208eac0d1f378",
            "projectPage": "https://latent-sketchpad.github.io/",
            "githubRepo": "https://github.com/hwanyu112/Latent-Sketchpad",
            "ai_summary": "Latent Sketchpad enhances Multimodal Large Language Models with an internal visual scratchpad, enabling generative visual thought and improved reasoning performance.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "Latent Sketchpad",
                "visual planning",
                "imagination",
                "sketching",
                "visual thinking",
                "internal visual scratchpad",
                "perceptual understanding",
                "generative visual thought",
                "autoregressive reasoning",
                "visual latents",
                "sketch images",
                "Context-Aware Vision Head",
                "Sketch Decoder",
                "MazePlanning",
                "Gemma3",
                "Qwen2.5-VL",
                "human-computer interaction"
            ],
            "githubStars": 15,
            "organization": {
                "_id": "5e6485f787403103f9f1055e",
                "name": "microsoft",
                "fullname": "Microsoft",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
            }
        },
        "publishedAt": "2025-10-28T11:26:20.000Z",
        "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal\n  Reasoning in MLLMs",
        "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24514.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65e816bbcfd12cd15b052a0e",
            "avatarUrl": "/avatars/4d92da469afdba8cd7dc645b98236011.svg",
            "fullname": "Huanyu_Zhang",
            "name": "huanyu112",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "5e6485f787403103f9f1055e",
            "name": "microsoft",
            "fullname": "Microsoft",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24320",
            "authors": [
                {
                    "_id": "69017878646208eac0d1f3aa",
                    "name": "Zhiheng Xi",
                    "hidden": false
                },
                {
                    "_id": "69017878646208eac0d1f3ab",
                    "name": "Jixuan Huang",
                    "hidden": false
                },
                {
                    "_id": "69017878646208eac0d1f3ac",
                    "name": "Xin Guo",
                    "hidden": false
                },
                {
                    "_id": "69017878646208eac0d1f3ad",
                    "name": "Boyang Hong",
                    "hidden": false
                },
                {
                    "_id": "69017878646208eac0d1f3ae",
                    "name": "Dingwen Yang",
                    "hidden": false
                },
                {
                    "_id": "69017878646208eac0d1f3af",
                    "name": "Xiaoran Fan",
                    "hidden": false
                },
                {
                    "_id": "69017878646208eac0d1f3b0",
                    "name": "Shuo Li",
                    "hidden": false
                },
                {
                    "_id": "69017878646208eac0d1f3b1",
                    "name": "Zehui Chen",
                    "hidden": false
                },
                {
                    "_id": "69017878646208eac0d1f3b2",
                    "user": {
                        "_id": "66384be673c2c55f2ded89fa",
                        "avatarUrl": "/avatars/1d8721074f0f51fab405f81474f2035f.svg",
                        "isPro": false,
                        "fullname": "Junjie Ye",
                        "user": "Junjie-Ye",
                        "type": "user"
                    },
                    "name": "Junjie Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:45:14.912Z",
                    "hidden": false
                },
                {
                    "_id": "69017878646208eac0d1f3b3",
                    "name": "Siyu Yuan",
                    "hidden": false
                },
                {
                    "_id": "69017878646208eac0d1f3b4",
                    "name": "Zhengyin Du",
                    "hidden": false
                },
                {
                    "_id": "69017878646208eac0d1f3b5",
                    "name": "Xuesong Yao",
                    "hidden": false
                },
                {
                    "_id": "69017878646208eac0d1f3b6",
                    "name": "Yufei Xu",
                    "hidden": false
                },
                {
                    "_id": "69017878646208eac0d1f3b7",
                    "name": "Jiecao Chen",
                    "hidden": false
                },
                {
                    "_id": "69017878646208eac0d1f3b8",
                    "name": "Rui Zheng",
                    "hidden": false
                },
                {
                    "_id": "69017878646208eac0d1f3b9",
                    "name": "Tao Gui",
                    "hidden": false
                },
                {
                    "_id": "69017878646208eac0d1f3ba",
                    "name": "Qi Zhang",
                    "hidden": false
                },
                {
                    "_id": "69017878646208eac0d1f3bb",
                    "name": "Xuanjing Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T11:37:01.000Z",
            "submittedOnDailyAt": "2025-10-29T00:47:25.750Z",
            "title": "Critique-RL: Training Language Models for Critiquing through Two-Stage\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "653a6e5cae155b92bae77b74",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg",
                "isPro": false,
                "fullname": "Zhiheng Xi",
                "user": "WooooDyy",
                "type": "user"
            },
            "summary": "Training critiquing language models to assess and provide feedback on model\noutputs is a promising way to improve LLMs for complex reasoning tasks.\nHowever, existing approaches typically rely on stronger supervisors for\nannotating critique data. To address this, we propose Critique-RL, an online RL\napproach for developing critiquing language models without stronger\nsupervision. Our approach operates on a two-player paradigm: the actor\ngenerates a response, the critic provides feedback, and the actor refines the\nresponse accordingly. We first reveal that relying solely on indirect reward\nsignals from the actor's outputs for RL optimization often leads to\nunsatisfactory critics: while their helpfulness (i.e., providing constructive\nfeedback) improves, the discriminability (i.e., determining whether a response\nis high-quality or not) remains poor, resulting in marginal performance gains.\nTo overcome this, Critique-RL adopts a two-stage optimization strategy. In\nstage I, it reinforces the discriminability of the critic with direct\nrule-based reward signals; in stage II, it introduces indirect rewards based on\nactor refinement to improve the critic's helpfulness, while maintaining its\ndiscriminability via appropriate regularization. Extensive experiments across\nvarious tasks and models show that Critique-RL delivers substantial performance\nimprovements. For example, it achieves a 9.02% gain on in-domain tasks and a\n5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.",
            "upvotes": 16,
            "discussionId": "69017878646208eac0d1f3bc",
            "githubRepo": "https://github.com/WooooDyy/Critique-RL",
            "ai_summary": "Critique-RL is an online reinforcement learning approach for developing critiquing language models without strong supervision, using a two-stage optimization strategy to improve both the critic's discriminability and helpfulness.",
            "ai_keywords": [
                "reinforcement learning",
                "RL",
                "actor",
                "critic",
                "indirect reward signals",
                "direct rule-based reward signals",
                "discriminability",
                "helpfulness",
                "Qwen2.5-7B"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "6447a7db3e7b3c11be6205f1",
                "name": "FudanNLP",
                "fullname": "Fudan NLP Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6447a74f6ffed6ece1fd0288/6P6gS4tCPu3idUj7RSrq1.jpeg"
            }
        },
        "publishedAt": "2025-10-28T07:37:01.000Z",
        "title": "Critique-RL: Training Language Models for Critiquing through Two-Stage\n  Reinforcement Learning",
        "summary": "Training critiquing language models to assess and provide feedback on model\noutputs is a promising way to improve LLMs for complex reasoning tasks.\nHowever, existing approaches typically rely on stronger supervisors for\nannotating critique data. To address this, we propose Critique-RL, an online RL\napproach for developing critiquing language models without stronger\nsupervision. Our approach operates on a two-player paradigm: the actor\ngenerates a response, the critic provides feedback, and the actor refines the\nresponse accordingly. We first reveal that relying solely on indirect reward\nsignals from the actor's outputs for RL optimization often leads to\nunsatisfactory critics: while their helpfulness (i.e., providing constructive\nfeedback) improves, the discriminability (i.e., determining whether a response\nis high-quality or not) remains poor, resulting in marginal performance gains.\nTo overcome this, Critique-RL adopts a two-stage optimization strategy. In\nstage I, it reinforces the discriminability of the critic with direct\nrule-based reward signals; in stage II, it introduces indirect rewards based on\nactor refinement to improve the critic's helpfulness, while maintaining its\ndiscriminability via appropriate regularization. Extensive experiments across\nvarious tasks and models show that Critique-RL delivers substantial performance\nimprovements. For example, it achieves a 9.02% gain on in-domain tasks and a\n5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24320.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "653a6e5cae155b92bae77b74",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg",
            "fullname": "Zhiheng Xi",
            "name": "WooooDyy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "6447a7db3e7b3c11be6205f1",
            "name": "FudanNLP",
            "fullname": "Fudan NLP Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6447a74f6ffed6ece1fd0288/6P6gS4tCPu3idUj7RSrq1.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24702",
            "authors": [
                {
                    "_id": "69022de3646208eac0d1f70a",
                    "name": "Yueqi Song",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f70b",
                    "name": "Ketan Ramaneti",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f70c",
                    "name": "Zaid Sheikh",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f70d",
                    "name": "Ziru Chen",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f70e",
                    "name": "Boyu Gou",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f70f",
                    "name": "Tianbao Xie",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f710",
                    "name": "Yiheng Xu",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f711",
                    "name": "Danyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f712",
                    "name": "Apurva Gandhi",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f713",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f714",
                    "name": "Joseph Liu",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f715",
                    "name": "Tianyue Ou",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f716",
                    "name": "Zhihao Yuan",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f717",
                    "name": "Frank Xu",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f718",
                    "name": "Shuyan Zhou",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f719",
                    "name": "Xingyao Wang",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f71a",
                    "name": "Xiang Yue",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f71b",
                    "name": "Tao Yu",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f71c",
                    "name": "Huan Sun",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f71d",
                    "name": "Yu Su",
                    "hidden": false
                },
                {
                    "_id": "69022de3646208eac0d1f71e",
                    "name": "Graham Neubig",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T17:53:13.000Z",
            "submittedOnDailyAt": "2025-10-29T13:40:58.447Z",
            "title": "Agent Data Protocol: Unifying Datasets for Diverse, Effective\n  Fine-tuning of LLM Agents",
            "submittedOnDailyBy": {
                "_id": "63e7bf7be02ee67e8e53f78d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e7bf7be02ee67e8e53f78d/z9j1PWrurEyIA2JXzpz0i.png",
                "isPro": false,
                "fullname": "Yueqi Song",
                "user": "yueqis",
                "type": "user"
            },
            "summary": "Public research results on large-scale supervised finetuning of AI agents\nremain relatively rare, since the collection of agent training data presents\nunique challenges. In this work, we argue that the bottleneck is not a lack of\nunderlying data sources, but that a large variety of data is fragmented across\nheterogeneous formats, tools, and interfaces. To this end, we introduce the\nagent data protocol (ADP), a light-weight representation language that serves\nas an \"interlingua\" between agent datasets in diverse formats and unified agent\ntraining pipelines downstream. The design of ADP is expressive enough to\ncapture a large variety of tasks, including API/tool use, browsing, coding,\nsoftware engineering, and general agentic workflows, while remaining simple to\nparse and train on without engineering at a per-dataset level. In experiments,\nwe unified a broad collection of 13 existing agent training datasets into ADP\nformat, and converted the standardized ADP data into training-ready formats for\nmultiple agent frameworks. We performed SFT on these data, and demonstrated an\naverage performance gain of ~20% over corresponding base models, and delivers\nstate-of-the-art or near-SOTA performance on standard coding, browsing, tool\nuse, and research benchmarks, without domain-specific tuning. All code and data\nare released publicly, in the hope that ADP could help lower the barrier to\nstandardized, scalable, and reproducible agent training.",
            "upvotes": 15,
            "discussionId": "69022de4646208eac0d1f71f",
            "projectPage": "https://agentdataprotocol.com/",
            "githubRepo": "https://github.com/neulab/agent-data-protocol",
            "ai_summary": "The agent data protocol (ADP) standardizes diverse agent training datasets, enabling improved performance across various tasks without domain-specific tuning.",
            "ai_keywords": [
                "agent data protocol",
                "ADP",
                "supervised finetuning",
                "AI agents",
                "agent datasets",
                "API/tool use",
                "browsing",
                "coding",
                "software engineering",
                "agentic workflows",
                "unified agent training pipelines",
                "SFT",
                "state-of-the-art",
                "near-SOTA",
                "performance gain",
                "coding benchmarks",
                "browsing benchmarks",
                "tool use benchmarks",
                "research benchmarks"
            ],
            "githubStars": 18
        },
        "publishedAt": "2025-10-28T13:53:13.000Z",
        "title": "Agent Data Protocol: Unifying Datasets for Diverse, Effective\n  Fine-tuning of LLM Agents",
        "summary": "Public research results on large-scale supervised finetuning of AI agents\nremain relatively rare, since the collection of agent training data presents\nunique challenges. In this work, we argue that the bottleneck is not a lack of\nunderlying data sources, but that a large variety of data is fragmented across\nheterogeneous formats, tools, and interfaces. To this end, we introduce the\nagent data protocol (ADP), a light-weight representation language that serves\nas an \"interlingua\" between agent datasets in diverse formats and unified agent\ntraining pipelines downstream. The design of ADP is expressive enough to\ncapture a large variety of tasks, including API/tool use, browsing, coding,\nsoftware engineering, and general agentic workflows, while remaining simple to\nparse and train on without engineering at a per-dataset level. In experiments,\nwe unified a broad collection of 13 existing agent training datasets into ADP\nformat, and converted the standardized ADP data into training-ready formats for\nmultiple agent frameworks. We performed SFT on these data, and demonstrated an\naverage performance gain of ~20% over corresponding base models, and delivers\nstate-of-the-art or near-SOTA performance on standard coding, browsing, tool\nuse, and research benchmarks, without domain-specific tuning. All code and data\nare released publicly, in the hope that ADP could help lower the barrier to\nstandardized, scalable, and reproducible agent training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24702.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63e7bf7be02ee67e8e53f78d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e7bf7be02ee67e8e53f78d/z9j1PWrurEyIA2JXzpz0i.png",
            "fullname": "Yueqi Song",
            "name": "yueqis",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.21978",
            "authors": [
                {
                    "_id": "6900dd3f646208eac0d1f072",
                    "name": "Hoang Phan",
                    "hidden": false
                },
                {
                    "_id": "6900dd3f646208eac0d1f073",
                    "name": "Xianjun Yang",
                    "hidden": false
                },
                {
                    "_id": "6900dd3f646208eac0d1f074",
                    "name": "Kevin Yao",
                    "hidden": false
                },
                {
                    "_id": "6900dd3f646208eac0d1f075",
                    "name": "Jingyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6900dd3f646208eac0d1f076",
                    "name": "Shengjie Bi",
                    "hidden": false
                },
                {
                    "_id": "6900dd3f646208eac0d1f077",
                    "name": "Xiaocheng Tang",
                    "hidden": false
                },
                {
                    "_id": "6900dd3f646208eac0d1f078",
                    "name": "Madian Khabsa",
                    "hidden": false
                },
                {
                    "_id": "6900dd3f646208eac0d1f079",
                    "name": "Lijuan Liu",
                    "hidden": false
                },
                {
                    "_id": "6900dd3f646208eac0d1f07a",
                    "user": {
                        "_id": "66c3c0cbdb33653b9041f968",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c3c0cbdb33653b9041f968/U2DEh4uyvhKnz7fNNyhyc.png",
                        "isPro": false,
                        "fullname": "Deren Lei",
                        "user": "derenlei",
                        "type": "user"
                    },
                    "name": "Deren Lei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:49:28.128Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-24T19:08:48.000Z",
            "submittedOnDailyAt": "2025-10-29T17:01:08.122Z",
            "title": "Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in\n  Large Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "66c3c0cbdb33653b9041f968",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c3c0cbdb33653b9041f968/U2DEh4uyvhKnz7fNNyhyc.png",
                "isPro": false,
                "fullname": "Deren Lei",
                "user": "derenlei",
                "type": "user"
            },
            "summary": "Reinforcement learning with verifiable rewards (RLVR) has delivered\nimpressive gains in mathematical and multimodal reasoning and has become a\nstandard post-training paradigm for contemporary language and vision-language\nmodels. However, the RLVR recipe introduces a significant risk of capability\nregression, where models forget foundational skills after prolonged training\nwithout employing regularization strategies. We empirically confirm this\nconcern, observing that open-source reasoning models suffer performance\ndegradation on core capabilities such as perception and faithfulness. While\nimposing regularization terms like KL divergence can help prevent deviation\nfrom the base model, these terms are calculated on the current task, thus they\ndo not guarantee broader knowledge. Meanwhile, commonly used experience replay\nacross heterogeneous domains makes it nontrivial to decide how much training\nfocus each objective should receive. To address this, we propose RECAP-a replay\nstrategy with dynamic objective reweighting for general knowledge preservation.\nOur reweighting mechanism adapts in an online manner using short-horizon\nsignals of convergence and instability, shifting the post-training focus away\nfrom saturated objectives and toward underperforming or volatile ones. Our\nmethod is end-to-end and readily applicable to existing RLVR pipelines without\ntraining additional models or heavy tuning. Extensive experiments on benchmarks\nbased on Qwen2.5-VL-3B and Qwen2.5-VL-7B demonstrate the effectiveness of our\nmethod, which not only preserves general capabilities but also improves\nreasoning by enabling more flexible trade-offs among in-task rewards.",
            "upvotes": 13,
            "discussionId": "6900dd3f646208eac0d1f07b",
            "ai_summary": "RECAP, a dynamic objective reweighting strategy, enhances reinforcement learning with verifiable rewards by preserving general knowledge and improving reasoning through flexible reward trade-offs.",
            "ai_keywords": [
                "reinforcement learning with verifiable rewards",
                "capability regression",
                "KL divergence",
                "experience replay",
                "dynamic objective reweighting",
                "short-horizon signals",
                "convergence",
                "instability",
                "Qwen2.5-VL-3B",
                "Qwen2.5-VL-7B"
            ],
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "publishedAt": "2025-10-24T15:08:48.000Z",
        "title": "Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in\n  Large Reasoning Models",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has delivered\nimpressive gains in mathematical and multimodal reasoning and has become a\nstandard post-training paradigm for contemporary language and vision-language\nmodels. However, the RLVR recipe introduces a significant risk of capability\nregression, where models forget foundational skills after prolonged training\nwithout employing regularization strategies. We empirically confirm this\nconcern, observing that open-source reasoning models suffer performance\ndegradation on core capabilities such as perception and faithfulness. While\nimposing regularization terms like KL divergence can help prevent deviation\nfrom the base model, these terms are calculated on the current task, thus they\ndo not guarantee broader knowledge. Meanwhile, commonly used experience replay\nacross heterogeneous domains makes it nontrivial to decide how much training\nfocus each objective should receive. To address this, we propose RECAP-a replay\nstrategy with dynamic objective reweighting for general knowledge preservation.\nOur reweighting mechanism adapts in an online manner using short-horizon\nsignals of convergence and instability, shifting the post-training focus away\nfrom saturated objectives and toward underperforming or volatile ones. Our\nmethod is end-to-end and readily applicable to existing RLVR pipelines without\ntraining additional models or heavy tuning. Extensive experiments on benchmarks\nbased on Qwen2.5-VL-3B and Qwen2.5-VL-7B demonstrate the effectiveness of our\nmethod, which not only preserves general capabilities but also improves\nreasoning by enabling more flexible trade-offs among in-task rewards.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21978.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66c3c0cbdb33653b9041f968",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c3c0cbdb33653b9041f968/U2DEh4uyvhKnz7fNNyhyc.png",
            "fullname": "Deren Lei",
            "name": "derenlei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.22037",
            "authors": [
                {
                    "_id": "69018d27646208eac0d1f4d1",
                    "name": "Shayne Longpre",
                    "hidden": false
                },
                {
                    "_id": "69018d27646208eac0d1f4d2",
                    "name": "Sneha Kudugunta",
                    "hidden": false
                },
                {
                    "_id": "69018d27646208eac0d1f4d3",
                    "name": "Niklas Muennighoff",
                    "hidden": false
                },
                {
                    "_id": "69018d27646208eac0d1f4d4",
                    "name": "I-Hung Hsu",
                    "hidden": false
                },
                {
                    "_id": "69018d27646208eac0d1f4d5",
                    "name": "Isaac Caswell",
                    "hidden": false
                },
                {
                    "_id": "69018d27646208eac0d1f4d6",
                    "name": "Alex Pentland",
                    "hidden": false
                },
                {
                    "_id": "69018d27646208eac0d1f4d7",
                    "name": "Sercan Arik",
                    "hidden": false
                },
                {
                    "_id": "69018d27646208eac0d1f4d8",
                    "name": "Chen-Yu Lee",
                    "hidden": false
                },
                {
                    "_id": "69018d27646208eac0d1f4d9",
                    "name": "Sayna Ebrahimi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-24T21:45:22.000Z",
            "submittedOnDailyAt": "2025-10-29T02:16:01.482Z",
            "title": "ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining,\n  Finetuning, and Decoding the Curse of Multilinguality",
            "submittedOnDailyBy": {
                "_id": "64dc08f9e7bc8544f9b1ac32",
                "avatarUrl": "/avatars/e2ceccaf12dbdc643396c56f9a80ab8b.svg",
                "isPro": false,
                "fullname": "I-Hung Hsu",
                "user": "alexhsu",
                "type": "user"
            },
            "summary": "Scaling laws research has focused overwhelmingly on English -- yet the most\nprominent AI models explicitly serve billions of international users. In this\nwork, we undertake the largest multilingual scaling laws study to date,\ntotaling 774 multilingual training experiments, spanning 10M-8B model\nparameters, 400+ training languages and 48 evaluation languages. We introduce\nthe Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual\npretraining, which outperforms existing scaling laws' out-of-sample\ngeneralization often by more than 0.3 R^2. Our analyses of the experiments shed\nlight on multilingual learning dynamics, transfer properties between languages,\nand the curse of multilinguality. First, we derive a cross-lingual transfer\nmatrix, empirically measuring mutual benefit scores between 38 x 38=1444\nlanguage pairs. Second, we derive a language-agnostic scaling law that reveals\nhow to optimally scale model size and data when adding languages without\nsacrificing performance. Third, we identify the computational crossover points\nfor when to pretrain from scratch versus finetune from multilingual\ncheckpoints. We hope these findings provide the scientific foundation for\ndemocratizing scaling laws across languages, and enable practitioners to\nefficiently scale models -- beyond English-first AI.",
            "upvotes": 12,
            "discussionId": "69018d27646208eac0d1f4da",
            "ai_summary": "The study introduces ATLAS, a multilingual scaling law that improves out-of-sample generalization and provides insights into cross-lingual transfer, optimal scaling, and computational crossover points for model training.",
            "ai_keywords": [
                "Adaptive Transfer Scaling Law",
                "ATLAS",
                "multilingual scaling laws",
                "cross-lingual transfer matrix",
                "language-agnostic scaling law",
                "computational crossover points",
                "pretraining",
                "finetuning",
                "multilingual checkpoints"
            ],
            "organization": {
                "_id": "5e6aca39878b8b2bf9806447",
                "name": "google",
                "fullname": "Google",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
            }
        },
        "publishedAt": "2025-10-24T17:45:22.000Z",
        "title": "ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining,\n  Finetuning, and Decoding the Curse of Multilinguality",
        "summary": "Scaling laws research has focused overwhelmingly on English -- yet the most\nprominent AI models explicitly serve billions of international users. In this\nwork, we undertake the largest multilingual scaling laws study to date,\ntotaling 774 multilingual training experiments, spanning 10M-8B model\nparameters, 400+ training languages and 48 evaluation languages. We introduce\nthe Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual\npretraining, which outperforms existing scaling laws' out-of-sample\ngeneralization often by more than 0.3 R^2. Our analyses of the experiments shed\nlight on multilingual learning dynamics, transfer properties between languages,\nand the curse of multilinguality. First, we derive a cross-lingual transfer\nmatrix, empirically measuring mutual benefit scores between 38 x 38=1444\nlanguage pairs. Second, we derive a language-agnostic scaling law that reveals\nhow to optimally scale model size and data when adding languages without\nsacrificing performance. Third, we identify the computational crossover points\nfor when to pretrain from scratch versus finetune from multilingual\ncheckpoints. We hope these findings provide the scientific foundation for\ndemocratizing scaling laws across languages, and enable practitioners to\nefficiently scale models -- beyond English-first AI.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22037.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64dc08f9e7bc8544f9b1ac32",
            "avatarUrl": "/avatars/e2ceccaf12dbdc643396c56f9a80ab8b.svg",
            "fullname": "I-Hung Hsu",
            "name": "alexhsu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "5e6aca39878b8b2bf9806447",
            "name": "google",
            "fullname": "Google",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.20661",
            "authors": [
                {
                    "_id": "68fd1ade6cdff8b857f46e34",
                    "user": {
                        "_id": "671b3b9e8750e04558d1d884",
                        "avatarUrl": "/avatars/db96dd71f46e274d2eeb6edf15c4881c.svg",
                        "isPro": false,
                        "fullname": "chen zhao",
                        "user": "zhihefang",
                        "type": "user"
                    },
                    "name": "Chen Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:50:34.860Z",
                    "hidden": false
                },
                {
                    "_id": "68fd1ade6cdff8b857f46e35",
                    "name": "En Ci",
                    "hidden": false
                },
                {
                    "_id": "68fd1ade6cdff8b857f46e36",
                    "name": "Yunzhe Xu",
                    "hidden": false
                },
                {
                    "_id": "68fd1ade6cdff8b857f46e37",
                    "name": "Tiehan Fan",
                    "hidden": false
                },
                {
                    "_id": "68fd1ade6cdff8b857f46e38",
                    "name": "Shanyan Guan",
                    "hidden": false
                },
                {
                    "_id": "68fd1ade6cdff8b857f46e39",
                    "name": "Yanhao Ge",
                    "hidden": false
                },
                {
                    "_id": "68fd1ade6cdff8b857f46e3a",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "68fd1ade6cdff8b857f46e3b",
                    "user": {
                        "_id": "65734004769f3ee9bde1af10",
                        "avatarUrl": "/avatars/d6310ed861972fd691687d8f47413f33.svg",
                        "isPro": false,
                        "fullname": "Ying Tai",
                        "user": "yingtai",
                        "type": "user"
                    },
                    "name": "Ying Tai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:50:30.685Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T15:34:53.000Z",
            "submittedOnDailyAt": "2025-10-29T11:25:23.596Z",
            "title": "UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale\n  High-Quality Dataset",
            "submittedOnDailyBy": {
                "_id": "671b3b9e8750e04558d1d884",
                "avatarUrl": "/avatars/db96dd71f46e274d2eeb6edf15c4881c.svg",
                "isPro": false,
                "fullname": "chen zhao",
                "user": "zhihefang",
                "type": "user"
            },
            "summary": "Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable\nprogress. However, two key challenges remain : 1) the absence of a large-scale\nhigh-quality UHR T2I dataset, and (2) the neglect of tailored training\nstrategies for fine-grained detail synthesis in UHR scenarios. To tackle the\nfirst challenge, we introduce UltraHR-100K, a high-quality dataset of\n100K UHR images with rich captions, offering diverse content and strong visual\nfidelity. Each image exceeds 3K resolution and is rigorously curated based on\ndetail richness, content complexity, and aesthetic quality. To tackle the\nsecond challenge, we propose a frequency-aware post-training method that\nenhances fine-detail generation in T2I diffusion models. Specifically, we\ndesign (i) Detail-Oriented Timestep Sampling (DOTS) to focus learning\non detail-critical denoising steps, and (ii) Soft-Weighting Frequency\nRegularization (SWFR), which leverages Discrete Fourier Transform (DFT) to\nsoftly constrain frequency components, encouraging high-frequency detail\npreservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks\ndemonstrate that our approach significantly improves the fine-grained detail\nquality and overall fidelity of UHR image generation. The code is available at\nhttps://github.com/NJU-PCALab/UltraHR-100k{here}.",
            "upvotes": 10,
            "discussionId": "68fd1ade6cdff8b857f46e3c",
            "ai_summary": "A new dataset and frequency-aware post-training method improve fine-grained detail synthesis in ultra-high-resolution text-to-image diffusion models.",
            "ai_keywords": [
                "UltraHR-100K",
                "frequency-aware post-training",
                "Detail-Oriented Timestep Sampling (DOTS)",
                "Soft-Weighting Frequency Regularization (SWFR)",
                "Discrete Fourier Transform (DFT)",
                "UltraHR-eval4K"
            ],
            "organization": {
                "_id": "638f70e8f1256a80d4288555",
                "name": "nanjinguniv",
                "fullname": "Nanjing University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/638f706ef1256a80d42880f9/6M6-JzwJGiLxjIJzvCflf.png"
            }
        },
        "publishedAt": "2025-10-23T11:34:53.000Z",
        "title": "UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale\n  High-Quality Dataset",
        "summary": "Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable\nprogress. However, two key challenges remain : 1) the absence of a large-scale\nhigh-quality UHR T2I dataset, and (2) the neglect of tailored training\nstrategies for fine-grained detail synthesis in UHR scenarios. To tackle the\nfirst challenge, we introduce UltraHR-100K, a high-quality dataset of\n100K UHR images with rich captions, offering diverse content and strong visual\nfidelity. Each image exceeds 3K resolution and is rigorously curated based on\ndetail richness, content complexity, and aesthetic quality. To tackle the\nsecond challenge, we propose a frequency-aware post-training method that\nenhances fine-detail generation in T2I diffusion models. Specifically, we\ndesign (i) Detail-Oriented Timestep Sampling (DOTS) to focus learning\non detail-critical denoising steps, and (ii) Soft-Weighting Frequency\nRegularization (SWFR), which leverages Discrete Fourier Transform (DFT) to\nsoftly constrain frequency components, encouraging high-frequency detail\npreservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks\ndemonstrate that our approach significantly improves the fine-grained detail\nquality and overall fidelity of UHR image generation. The code is available at\nhttps://github.com/NJU-PCALab/UltraHR-100k{here}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20661.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "671b3b9e8750e04558d1d884",
            "avatarUrl": "/avatars/db96dd71f46e274d2eeb6edf15c4881c.svg",
            "fullname": "chen zhao",
            "name": "zhihefang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "638f70e8f1256a80d4288555",
            "name": "nanjinguniv",
            "fullname": "Nanjing University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/638f706ef1256a80d42880f9/6M6-JzwJGiLxjIJzvCflf.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.17439",
            "authors": [
                {
                    "_id": "6901c2c1646208eac0d1f577",
                    "user": {
                        "_id": "68c14544a9a07d79e3e13166",
                        "avatarUrl": "/avatars/3c3f15bccb59d0a56c866b5c100cb35e.svg",
                        "isPro": false,
                        "fullname": "Zhengshen Zhang",
                        "user": "flameeee",
                        "type": "user"
                    },
                    "name": "Zhengshen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:40:44.683Z",
                    "hidden": false
                },
                {
                    "_id": "6901c2c1646208eac0d1f578",
                    "user": {
                        "_id": "667b8de7a68bf81afe668afe",
                        "avatarUrl": "/avatars/aeff10805ff858332e6f6a58735dbbd9.svg",
                        "isPro": false,
                        "fullname": "leoli",
                        "user": "lifuguan",
                        "type": "user"
                    },
                    "name": "Hao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:40:47.831Z",
                    "hidden": false
                },
                {
                    "_id": "6901c2c1646208eac0d1f579",
                    "name": "Yalun Dai",
                    "hidden": false
                },
                {
                    "_id": "6901c2c1646208eac0d1f57a",
                    "name": "Zhengbang Zhu",
                    "hidden": false
                },
                {
                    "_id": "6901c2c1646208eac0d1f57b",
                    "name": "Lei Zhou",
                    "hidden": false
                },
                {
                    "_id": "6901c2c1646208eac0d1f57c",
                    "name": "Chenchen Liu",
                    "hidden": false
                },
                {
                    "_id": "6901c2c1646208eac0d1f57d",
                    "name": "Dong Wang",
                    "hidden": false
                },
                {
                    "_id": "6901c2c1646208eac0d1f57e",
                    "name": "Francis E. H. Tay",
                    "hidden": false
                },
                {
                    "_id": "6901c2c1646208eac0d1f57f",
                    "name": "Sijin Chen",
                    "hidden": false
                },
                {
                    "_id": "6901c2c1646208eac0d1f580",
                    "name": "Ziwei Liu",
                    "hidden": false
                },
                {
                    "_id": "6901c2c1646208eac0d1f581",
                    "name": "Yuxiao Liu",
                    "hidden": false
                },
                {
                    "_id": "6901c2c1646208eac0d1f582",
                    "name": "Xinghang Li",
                    "hidden": false
                },
                {
                    "_id": "6901c2c1646208eac0d1f583",
                    "name": "Pan Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T11:26:45.000Z",
            "submittedOnDailyAt": "2025-10-29T09:41:44.387Z",
            "title": "From Spatial to Actions: Grounding Vision-Language-Action Model in\n  Spatial Foundation Priors",
            "submittedOnDailyBy": {
                "_id": "6485b08e687d9e0c759121b0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6485b08e687d9e0c759121b0/P_9F0izrQgUfEd-VEbhg8.jpeg",
                "isPro": false,
                "fullname": "sijin",
                "user": "CH3COOK",
                "type": "user"
            },
            "summary": "Existing vision-language-action (VLA) models act in 3D real-world but are\ntypically built on 2D encoders, leaving a spatial reasoning gap that limits\ngeneralization and adaptability. Recent 3D integration techniques for VLAs\neither require specialized sensors and transfer poorly across modalities, or\ninject weak cues that lack geometry and degrade vision-language alignment. In\nthis work, we introduce FALCON (From Spatial to Action), a novel paradigm that\ninjects rich 3D spatial tokens into the action head. FALCON leverages spatial\nfoundation models to deliver strong geometric priors from RGB alone, and\nincludes an Embodied Spatial Model that can optionally fuse depth, or pose for\nhigher fidelity when available, without retraining or architectural changes. To\npreserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced\nAction Head rather than being concatenated into the vision-language backbone.\nThese designs enable FALCON to address limitations in spatial representation,\nmodality transferability, and alignment. In comprehensive evaluations across\nthree simulation benchmarks and eleven real-world tasks, our proposed FALCON\nachieves state-of-the-art performance, consistently surpasses competitive\nbaselines, and remains robust under clutter, spatial-prompt conditioning, and\nvariations in object scale and height.",
            "upvotes": 10,
            "discussionId": "6901c2c2646208eac0d1f584",
            "projectPage": "https://falcon-vla.github.io/",
            "ai_summary": "FALCON enhances vision-language-action models by integrating rich 3D spatial tokens into the action head, improving spatial reasoning and modality transferability.",
            "ai_keywords": [
                "vision-language-action (VLA) models",
                "3D integration",
                "spatial tokens",
                "spatial foundation models",
                "Embodied Spatial Model",
                "Spatial-Enhanced Action Head",
                "spatial representation",
                "modality transferability",
                "vision-language alignment"
            ],
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-10-20T07:26:45.000Z",
        "title": "From Spatial to Actions: Grounding Vision-Language-Action Model in\n  Spatial Foundation Priors",
        "summary": "Existing vision-language-action (VLA) models act in 3D real-world but are\ntypically built on 2D encoders, leaving a spatial reasoning gap that limits\ngeneralization and adaptability. Recent 3D integration techniques for VLAs\neither require specialized sensors and transfer poorly across modalities, or\ninject weak cues that lack geometry and degrade vision-language alignment. In\nthis work, we introduce FALCON (From Spatial to Action), a novel paradigm that\ninjects rich 3D spatial tokens into the action head. FALCON leverages spatial\nfoundation models to deliver strong geometric priors from RGB alone, and\nincludes an Embodied Spatial Model that can optionally fuse depth, or pose for\nhigher fidelity when available, without retraining or architectural changes. To\npreserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced\nAction Head rather than being concatenated into the vision-language backbone.\nThese designs enable FALCON to address limitations in spatial representation,\nmodality transferability, and alignment. In comprehensive evaluations across\nthree simulation benchmarks and eleven real-world tasks, our proposed FALCON\nachieves state-of-the-art performance, consistently surpasses competitive\nbaselines, and remains robust under clutter, spatial-prompt conditioning, and\nvariations in object scale and height.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17439.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6485b08e687d9e0c759121b0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6485b08e687d9e0c759121b0/P_9F0izrQgUfEd-VEbhg8.jpeg",
            "fullname": "sijin",
            "name": "CH3COOK",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24684",
            "authors": [
                {
                    "_id": "69022f86646208eac0d1f730",
                    "name": "Bo Liu",
                    "hidden": false
                },
                {
                    "_id": "69022f86646208eac0d1f731",
                    "name": "Chuanyang Jin",
                    "hidden": false
                },
                {
                    "_id": "69022f86646208eac0d1f732",
                    "name": "Seungone Kim",
                    "hidden": false
                },
                {
                    "_id": "69022f86646208eac0d1f733",
                    "name": "Weizhe Yuan",
                    "hidden": false
                },
                {
                    "_id": "69022f86646208eac0d1f734",
                    "name": "Wenting Zhao",
                    "hidden": false
                },
                {
                    "_id": "69022f86646208eac0d1f735",
                    "name": "Ilia Kulikov",
                    "hidden": false
                },
                {
                    "_id": "69022f86646208eac0d1f736",
                    "name": "Xian Li",
                    "hidden": false
                },
                {
                    "_id": "69022f86646208eac0d1f737",
                    "name": "Sainbayar Sukhbaatar",
                    "hidden": false
                },
                {
                    "_id": "69022f86646208eac0d1f738",
                    "name": "Jack Lanchantin",
                    "hidden": false
                },
                {
                    "_id": "69022f86646208eac0d1f739",
                    "name": "Jason Weston",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T17:46:16.000Z",
            "submittedOnDailyAt": "2025-10-29T13:47:23.581Z",
            "title": "SPICE: Self-Play In Corpus Environments Improves Reasoning",
            "submittedOnDailyBy": {
                "_id": "62f023a36a027498eaa2f9cc",
                "avatarUrl": "/avatars/8ac1c5c74d0957e3c6cc94b3a7795c37.svg",
                "isPro": false,
                "fullname": "Jason Weston",
                "user": "spermwhale",
                "type": "user"
            },
            "summary": "Self-improving systems require environmental interaction for continuous\nadaptation. We introduce SPICE (Self-Play In Corpus Environments), a\nreinforcement learning framework where a single model acts in two roles: a\nChallenger that mines documents from a large corpus to generate diverse\nreasoning tasks, and a Reasoner that solves them. Through adversarial dynamics,\nthe Challenger creates an automatic curriculum at the frontier of the\nReasoner's capability, while corpus grounding provides the rich,\nnear-inexhaustible external signal necessary for sustained improvement. Unlike\nexisting ungrounded self-play methods that offer more limited benefits, SPICE\nachieves consistent gains across mathematical (+8.9%) and general reasoning\n(+9.8%) benchmarks on multiple model families. Our analysis reveals how\ndocument grounding is a key ingredient in SPICE to continuously generate its\nown increasingly challenging goals and achieve them, enabling sustained\nself-improvement.",
            "upvotes": 9,
            "discussionId": "69022f86646208eac0d1f73a",
            "ai_summary": "SPICE, a reinforcement learning framework, uses self-play in a corpus environment to continuously improve a model's reasoning capabilities through adversarial dynamics and document grounding.",
            "ai_keywords": [
                "reinforcement learning",
                "self-play",
                "Challenger",
                "Reasoner",
                "adversarial dynamics",
                "automatic curriculum",
                "corpus grounding",
                "mathematical benchmarks",
                "general reasoning benchmarks",
                "model families",
                "self-improvement"
            ],
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "publishedAt": "2025-10-28T13:46:16.000Z",
        "title": "SPICE: Self-Play In Corpus Environments Improves Reasoning",
        "summary": "Self-improving systems require environmental interaction for continuous\nadaptation. We introduce SPICE (Self-Play In Corpus Environments), a\nreinforcement learning framework where a single model acts in two roles: a\nChallenger that mines documents from a large corpus to generate diverse\nreasoning tasks, and a Reasoner that solves them. Through adversarial dynamics,\nthe Challenger creates an automatic curriculum at the frontier of the\nReasoner's capability, while corpus grounding provides the rich,\nnear-inexhaustible external signal necessary for sustained improvement. Unlike\nexisting ungrounded self-play methods that offer more limited benefits, SPICE\nachieves consistent gains across mathematical (+8.9%) and general reasoning\n(+9.8%) benchmarks on multiple model families. Our analysis reveals how\ndocument grounding is a key ingredient in SPICE to continuously generate its\nown increasingly challenging goals and achieve them, enabling sustained\nself-improvement.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24684.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62f023a36a027498eaa2f9cc",
            "avatarUrl": "/avatars/8ac1c5c74d0957e3c6cc94b3a7795c37.svg",
            "fullname": "Jason Weston",
            "name": "spermwhale",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 18
        },
        "organization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24081",
            "authors": [
                {
                    "_id": "69016248646208eac0d1f18f",
                    "name": "Tyler A. Chang",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f190",
                    "name": "Catherine Arnett",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f191",
                    "name": "Abdelrahman Eldesokey",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f192",
                    "name": "Abdelrahman Sadallah",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f193",
                    "name": "Abeer Kashar",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f194",
                    "name": "Abolade Daud",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f195",
                    "name": "Abosede Grace Olanihun",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f196",
                    "name": "Adamu Labaran Mohammed",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f197",
                    "name": "Adeyemi Praise",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f198",
                    "name": "Adhikarinayum Meerajita Sharma",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f199",
                    "name": "Aditi Gupta",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f19a",
                    "name": "Afitab Iyigun",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f19b",
                    "name": "Afonso Simplício",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f19c",
                    "name": "Ahmed Essouaied",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f19d",
                    "name": "Aicha Chorana",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f19e",
                    "name": "Akhil Eppa",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f19f",
                    "name": "Akintunde Oladipo",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1a0",
                    "name": "Akshay Ramesh",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1a1",
                    "name": "Aleksei Dorkin",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1a2",
                    "name": "Alfred Malengo Kondoro",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1a3",
                    "name": "Alham Fikri Aji",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1a4",
                    "name": "Ali Eren Çetintaş",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1a5",
                    "name": "Allan Hanbury",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1a6",
                    "name": "Alou Dembele",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1a7",
                    "name": "Alp Niksarli",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1a8",
                    "name": "Álvaro Arroyo",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1a9",
                    "name": "Amin Bajand",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1aa",
                    "name": "Amol Khanna",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1ab",
                    "name": "Ana Chkhaidze",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1ac",
                    "name": "Ana Condez",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1ad",
                    "name": "Andiswa Mkhonto",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1ae",
                    "name": "Andrew Hoblitzell",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1af",
                    "name": "Andrew Tran",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1b0",
                    "name": "Angelos Poulis",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1b1",
                    "name": "Anirban Majumder",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1b2",
                    "name": "Anna Vacalopoulou",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1b3",
                    "name": "Annette Kuuipolani Kanahele Wong",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1b4",
                    "name": "Annika Simonsen",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1b5",
                    "name": "Anton Kovalev",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1b6",
                    "name": "Ashvanth. S",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1b7",
                    "name": "Ayodeji Joseph Lana",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1b8",
                    "name": "Barkin Kinay",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1b9",
                    "name": "Bashar Alhafni",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1ba",
                    "name": "Benedict Cibalinda Busole",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1bb",
                    "name": "Bernard Ghanem",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1bc",
                    "name": "Bharti Nathani",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1bd",
                    "name": "Biljana Stojanovska Đurić",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1be",
                    "name": "Bola Agbonile",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1bf",
                    "name": "Bragi Bergsson",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1c0",
                    "name": "Bruce Torres Fischer",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1c1",
                    "name": "Burak Tutar",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1c2",
                    "name": "Burcu Alakuş Çınar",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1c3",
                    "name": "Cade J. Kanoniakapueo Kane",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1c4",
                    "name": "Can Udomcharoenchaikit",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1c5",
                    "name": "Catherine Arnett",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1c6",
                    "name": "Chadi Helwe",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1c7",
                    "name": "Chaithra Reddy Nerella",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1c8",
                    "name": "Chen Cecilia Liu",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1c9",
                    "name": "Chiamaka Glory Nwokolo",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1ca",
                    "name": "Cristina España-Bonet",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1cb",
                    "name": "Cynthia Amol",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1cc",
                    "name": "DaeYeop Lee",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1cd",
                    "name": "Dana Arad",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1ce",
                    "name": "Daniil Dzenhaliou",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1cf",
                    "name": "Daria Pugacheva",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1d0",
                    "name": "Dasol Choi",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1d1",
                    "name": "Daud Abolade",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1d2",
                    "name": "David Liu",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1d3",
                    "name": "David Semedo",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1d4",
                    "name": "Deborah Popoola",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1d5",
                    "name": "Deividas Mataciunas",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1d6",
                    "name": "Delphine Nyaboke",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1d7",
                    "name": "Dhyuthy Krishna Kumar",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1d8",
                    "name": "Diogo Glória-Silva",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1d9",
                    "name": "Diogo Tavares",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1da",
                    "name": "Divyanshu Goyal",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1db",
                    "name": "DongGeon Lee",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1dc",
                    "name": "Ebele Nwamaka Anajemba",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1dd",
                    "name": "Egonu Ngozi Grace",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1de",
                    "name": "Elena Mickel",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1df",
                    "name": "Elena Tutubalina",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1e0",
                    "name": "Elias Herranen",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1e1",
                    "name": "Emile Anand",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1e2",
                    "name": "Emmanuel Habumuremyi",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1e3",
                    "name": "Emuobonuvie Maria Ajiboye",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1e4",
                    "name": "Eryawan Presma Yulianrifat",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1e5",
                    "name": "Esther Adenuga",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1e6",
                    "name": "Ewa Rudnicka",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1e7",
                    "name": "Faith Olabisi Itiola",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1e8",
                    "name": "Faran Taimoor Butt",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1e9",
                    "name": "Fathima Thekkekara",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1ea",
                    "name": "Fatima Haouari",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1eb",
                    "name": "Filbert Aurelian Tjiaranata",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1ec",
                    "name": "Firas Laakom",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1ed",
                    "name": "Francesca Grasso",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1ee",
                    "name": "Francesco Orabona",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1ef",
                    "name": "Francesco Periti",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1f0",
                    "name": "Gbenga Kayode Solomon",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1f1",
                    "name": "Gia Nghia Ngo",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1f2",
                    "name": "Gloria Udhehdhe-oze",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1f3",
                    "name": "Gonçalo Martins",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1f4",
                    "name": "Gopi Naga Sai Ram Challagolla",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1f5",
                    "name": "Guijin Son",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1f6",
                    "name": "Gulnaz Abdykadyrova",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1f7",
                    "name": "Hafsteinn Einarsson",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1f8",
                    "name": "Hai Hu",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1f9",
                    "name": "Hamidreza Saffari",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1fa",
                    "name": "Hamza Zaidi",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1fb",
                    "name": "Haopeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1fc",
                    "name": "Harethah Abu Shairah",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1fd",
                    "name": "Harry Vuong",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1fe",
                    "name": "Hele-Andra Kuulmets",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f1ff",
                    "name": "Houda Bouamor",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f200",
                    "name": "Hwanjo Yu",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f201",
                    "name": "Iben Nyholm Debess",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f202",
                    "name": "İbrahim Ethem Deveci",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f203",
                    "name": "Ikhlasul Akmal Hanif",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f204",
                    "name": "Ikhyun Cho",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f205",
                    "name": "Inês Calvo",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f206",
                    "name": "Inês Vieira",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f207",
                    "name": "Isaac Manzi",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f208",
                    "name": "Ismail Daud",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f209",
                    "name": "Itay Itzhak",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f20a",
                    "name": "Iuliia",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f20b",
                    "name": "Alekseenko",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f20c",
                    "name": "Ivan Belashkin",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f20d",
                    "name": "Ivan Spada",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f20e",
                    "name": "Ivan Zhelyazkov",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f20f",
                    "name": "Jacob Brinton",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f210",
                    "name": "Jafar Isbarov",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f211",
                    "name": "Jaka Čibej",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f212",
                    "name": "Jan Čuhel",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f213",
                    "name": "Jan Kocoń",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f214",
                    "name": "Jauza Akbar Krito",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f215",
                    "name": "Jebish Purbey",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f216",
                    "name": "Jennifer Mickel",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f217",
                    "name": "Jennifer Za",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f218",
                    "name": "Jenny Kunz",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f219",
                    "name": "Jihae Jeong",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f21a",
                    "name": "Jimena Tena Dávalos",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f21b",
                    "name": "Jinu Lee",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f21c",
                    "name": "João Magalhães",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f21d",
                    "name": "John Yi",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f21e",
                    "name": "Jongin Kim",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f21f",
                    "name": "Joseph Chataignon",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f220",
                    "name": "Joseph Marvin Imperial",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f221",
                    "name": "Jubeerathan Thevakumar",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f222",
                    "name": "Judith Land",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f223",
                    "name": "Junchen Jiang",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f224",
                    "name": "Jungwhan Kim",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f225",
                    "name": "Kairit Sirts",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f226",
                    "name": "Kamesh R",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f227",
                    "name": "Kamesh V",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f228",
                    "name": "Kanda Patrick Tshinu",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f229",
                    "name": "Kätriin Kukk",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f22a",
                    "name": "Kaustubh Ponkshe",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f22b",
                    "name": "Kavsar Huseynova",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f22c",
                    "name": "Ke He",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f22d",
                    "name": "Kelly Buchanan",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f22e",
                    "name": "Kengatharaiyer Sarveswaran",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f22f",
                    "name": "Kerem Zaman",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f230",
                    "name": "Khalil Mrini",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f231",
                    "name": "Kian Kyars",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f232",
                    "name": "Krister Kruusmaa",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f233",
                    "name": "Kusum Chouhan",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f234",
                    "name": "Lainitha Krishnakumar",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f235",
                    "name": "Laura Castro Sánchez",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f236",
                    "name": "Laura Porrino Moscoso",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f237",
                    "name": "Leshem Choshen",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f238",
                    "name": "Levent Sencan",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f239",
                    "name": "Lilja Øvrelid",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f23a",
                    "name": "Lisa Alazraki",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f23b",
                    "name": "Lovina Ehimen-Ugbede",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f23c",
                    "name": "Luheerathan Thevakumar",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f23d",
                    "name": "Luxshan Thavarasa",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f23e",
                    "name": "Mahnoor Malik",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f23f",
                    "name": "Mamadou K. Keita",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f240",
                    "name": "Mansi Jangid",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f241",
                    "name": "Marco De Santis",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f242",
                    "name": "Marcos García",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f243",
                    "name": "Marek Suppa",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f244",
                    "name": "Mariam D'Ciofalo",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f245",
                    "name": "Marii Ojastu",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f246",
                    "name": "Maryam Sikander",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f247",
                    "name": "Mausami Narayan",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f248",
                    "name": "Maximos Skandalis",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f249",
                    "name": "Mehak Mehak",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f24a",
                    "name": "Mehmet İlteriş Bozkurt",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f24b",
                    "name": "Melaku Bayu Workie",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f24c",
                    "name": "Menan Velayuthan",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f24d",
                    "name": "Michael Leventhal",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f24e",
                    "name": "Michał Marcińczuk",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f24f",
                    "name": "Mirna Potočnjak",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f250",
                    "name": "Mohammadamin Shafiei",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f251",
                    "name": "Mridul Sharma",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f252",
                    "name": "Mrityunjaya Indoria",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f253",
                    "name": "Muhammad Ravi Shulthan Habibi",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f254",
                    "name": "Murat Kolić",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f255",
                    "name": "Nada Galant",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f256",
                    "name": "Naphat Permpredanun",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f257",
                    "name": "Narada Maugin",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f258",
                    "name": "Nicholas Kluge Corrêa",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f259",
                    "name": "Nikola Ljubešić",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f25a",
                    "name": "Nirmal Thomas",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f25b",
                    "name": "Nisansa de Silva",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f25c",
                    "name": "Nisheeth Joshi",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f25d",
                    "name": "Nitish Ponkshe",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f25e",
                    "name": "Nizar Habash",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f25f",
                    "name": "Nneoma C. Udeze",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f260",
                    "name": "Noel Thomas",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f261",
                    "name": "Noémi Ligeti-Nagy",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f262",
                    "name": "Nouhoum Coulibaly",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f263",
                    "name": "Nsengiyumva Faustin",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f264",
                    "name": "Odunayo Kareemat Buliaminu",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f265",
                    "name": "Odunayo Ogundepo",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f266",
                    "name": "Oghojafor Godswill Fejiro",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f267",
                    "name": "Ogundipe Blessing Funmilola",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f268",
                    "name": "Okechukwu God'spraise",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f269",
                    "name": "Olanrewaju Samuel",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f26a",
                    "name": "Olaoye Deborah Oluwaseun",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f26b",
                    "name": "Olasoji Akindejoye",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f26c",
                    "name": "Olga Popova",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f26d",
                    "name": "Olga Snissarenko",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f26e",
                    "name": "Onyinye Anulika Chiemezie",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f26f",
                    "name": "Orkun Kinay",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f270",
                    "name": "Osman Tursun",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f271",
                    "name": "Owoeye Tobiloba Moses",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f272",
                    "name": "Oyelade Oluwafemi Joshua",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f273",
                    "name": "Oyesanmi Fiyinfoluwa",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f274",
                    "name": "Pablo Gamallo",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f275",
                    "name": "Pablo Rodríguez Fernández",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f276",
                    "name": "Palak Arora",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f277",
                    "name": "Pedro Valente",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f278",
                    "name": "Peter Rupnik",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f279",
                    "name": "Philip Oghenesuowho Ekiugbo",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f27a",
                    "name": "Pramit Sahoo",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f27b",
                    "name": "Prokopis Prokopidis",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f27c",
                    "name": "Pua Niau-Puhipau",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f27d",
                    "name": "Quadri Yahya",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f27e",
                    "name": "Rachele Mignone",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f27f",
                    "name": "Raghav Singhal",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f280",
                    "name": "Ram Mohan Rao Kadiyala",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f281",
                    "name": "Raphael Merx",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f282",
                    "name": "Rapheal Afolayan",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f283",
                    "name": "Ratnavel Rajalakshmi",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f284",
                    "name": "Rishav Ghosh",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f285",
                    "name": "Romina Oji",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f286",
                    "name": "Ron Kekeha Solis",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f287",
                    "name": "Rui Guerra",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f288",
                    "name": "Rushikesh Zawar",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f289",
                    "name": "Sa'ad Nasir Bashir",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f28a",
                    "name": "Saeed Alzaabi",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f28b",
                    "name": "Sahil Sandeep",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f28c",
                    "name": "Sai Pavan Batchu",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f28d",
                    "name": "SaiSandeep Kantareddy",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f28e",
                    "name": "Salsabila Zahirah Pranida",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f28f",
                    "name": "Sam Buchanan",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f290",
                    "name": "Samuel Rutunda",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f291",
                    "name": "Sander Land",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f292",
                    "name": "Sarah Sulollari",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f293",
                    "name": "Sardar Ali",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f294",
                    "name": "Saroj Sapkota",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f295",
                    "name": "Saulius Tautvaisas",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f296",
                    "name": "Sayambhu Sen",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f297",
                    "name": "Sayantani Banerjee",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f298",
                    "name": "Sebastien Diarra",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f299",
                    "name": "SenthilNathan. M",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f29a",
                    "name": "Sewoong Lee",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f29b",
                    "name": "Shaan Shah",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f29c",
                    "name": "Shankar Venkitachalam",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f29d",
                    "name": "Sharifa Djurabaeva",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f29e",
                    "name": "Sharon Ibejih",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f29f",
                    "name": "Shivanya Shomir Dutta",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2a0",
                    "name": "Siddhant Gupta",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2a1",
                    "name": "Silvia Paniagua Suárez",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2a2",
                    "name": "Sina Ahmadi",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2a3",
                    "name": "Sivasuthan Sukumar",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2a4",
                    "name": "Siyuan Song",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2a5",
                    "name": "Snegha A.",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2a6",
                    "name": "Sokratis Sofianopoulos",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2a7",
                    "name": "Sona Elza Simon",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2a8",
                    "name": "Sonja Benčina",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2a9",
                    "name": "Sophie Gvasalia",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2aa",
                    "name": "Sphurti Kirit More",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2ab",
                    "name": "Spyros Dragazis",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2ac",
                    "name": "Stephan P. Kaufhold",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2ad",
                    "name": "Suba. S",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2ae",
                    "name": "Sultan AlRashed",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2af",
                    "name": "Surangika Ranathunga",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2b0",
                    "name": "Taiga Someya",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2b1",
                    "name": "Taja Kuzman Pungeršek",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2b2",
                    "name": "Tal Haklay",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2b3",
                    "name": "Tasi'u Jibril",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2b4",
                    "name": "Tatsuya Aoyama",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2b5",
                    "name": "Tea Abashidze",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2b6",
                    "name": "Terenz Jomar Dela Cruz",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2b7",
                    "name": "Terra Blevins",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2b8",
                    "name": "Themistoklis Nikas",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2b9",
                    "name": "Theresa Dora Idoko",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2ba",
                    "name": "Thu Mai Do",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2bb",
                    "name": "Tilek Chubakov",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2bc",
                    "name": "Tommaso Gargiani",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2bd",
                    "name": "Uma Rathore",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2be",
                    "name": "Uni Johannesen",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2bf",
                    "name": "Uwuma Doris Ugwu",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2c0",
                    "name": "Vallerie Alexandra Putra",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2c1",
                    "name": "Vanya Bannihatti Kumar",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2c2",
                    "name": "Varsha Jeyarajalingam",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2c3",
                    "name": "Varvara Arzt",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2c4",
                    "name": "Vasudevan Nedumpozhimana",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2c5",
                    "name": "Viktoria Ondrejova",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2c6",
                    "name": "Viktoryia Horbik",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2c7",
                    "name": "Vishnu Vardhan Reddy Kummitha",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2c8",
                    "name": "Vuk Dinić",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2c9",
                    "name": "Walelign Tewabe Sewunetie",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2ca",
                    "name": "Winston Wu",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2cb",
                    "name": "Xiaojing Zhao",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2cc",
                    "name": "Yacouba Diarra",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2cd",
                    "name": "Yaniv Nikankin",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2ce",
                    "name": "Yash Mathur",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2cf",
                    "name": "Yixi Chen",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2d0",
                    "name": "Yiyuan Li",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2d1",
                    "name": "Yolanda Xavier",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2d2",
                    "name": "Yonatan Belinkov",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2d3",
                    "name": "Yusuf Ismail Abayomi",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2d4",
                    "name": "Zaid Alyafeai",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2d5",
                    "name": "Zhengyang Shan",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2d6",
                    "name": "Zhi Rui Tam",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2d7",
                    "name": "Zilu Tang",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2d8",
                    "name": "Zuzana Nadova",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2d9",
                    "name": "Baber Abbasi",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2da",
                    "name": "Stella Biderman",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2db",
                    "name": "David Stap",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2dc",
                    "name": "Duygu Ataman",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2dd",
                    "name": "Fabian Schmidt",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2de",
                    "name": "Hila Gonen",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2df",
                    "name": "Jiayi Wang",
                    "hidden": false
                },
                {
                    "_id": "69016248646208eac0d1f2e0",
                    "name": "David Ifeoluwa Adelani",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6384d424ac61472e5e92d403/6BaDk0HX_j2cVDJ8TosvC.png"
            ],
            "publishedAt": "2025-10-28T05:46:25.000Z",
            "submittedOnDailyAt": "2025-10-29T14:55:06.855Z",
            "title": "Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+\n  Languages and Cultures",
            "submittedOnDailyBy": {
                "_id": "6384d424ac61472e5e92d403",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6384d424ac61472e5e92d403/RfEyLD3FmEobLcpNE9pNu.png",
                "isPro": false,
                "fullname": "Catherine Arnett",
                "user": "catherinearnett",
                "type": "user"
            },
            "summary": "To date, there exist almost no culturally-specific evaluation benchmarks for\nlarge language models (LLMs) that cover a large number of languages and\ncultures. In this paper, we present Global PIQA, a participatory commonsense\nreasoning benchmark for over 100 languages, constructed by hand by 335\nresearchers from 65 countries around the world. The 116 language varieties in\nGlobal PIQA cover five continents, 14 language families, and 23 writing\nsystems. In the non-parallel split of Global PIQA, over 50% of examples\nreference local foods, customs, traditions, or other culturally-specific\nelements. We find that state-of-the-art LLMs perform well on Global PIQA in\naggregate, but they exhibit weaker performance in lower-resource languages (up\nto a 37% accuracy gap, despite random chance at 50%). Open models generally\nperform worse than proprietary models. Global PIQA highlights that in many\nlanguages and cultures, everyday knowledge remains an area for improvement,\nalongside more widely-discussed capabilities such as complex reasoning and\nexpert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA\nprovides a glimpse into the wide diversity of cultures in which human language\nis embedded.",
            "upvotes": 8,
            "discussionId": "69016249646208eac0d1f2e1",
            "ai_summary": "Global PIQA is a multilingual commonsense reasoning benchmark that highlights performance gaps of large language models across different cultures and languages.",
            "ai_keywords": [
                "large language models",
                "commonsense reasoning",
                "Global PIQA",
                "language varieties",
                "culturally-specific elements",
                "lower-resource languages",
                "open models",
                "proprietary models"
            ]
        },
        "publishedAt": "2025-10-28T01:46:25.000Z",
        "title": "Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+\n  Languages and Cultures",
        "summary": "To date, there exist almost no culturally-specific evaluation benchmarks for\nlarge language models (LLMs) that cover a large number of languages and\ncultures. In this paper, we present Global PIQA, a participatory commonsense\nreasoning benchmark for over 100 languages, constructed by hand by 335\nresearchers from 65 countries around the world. The 116 language varieties in\nGlobal PIQA cover five continents, 14 language families, and 23 writing\nsystems. In the non-parallel split of Global PIQA, over 50% of examples\nreference local foods, customs, traditions, or other culturally-specific\nelements. We find that state-of-the-art LLMs perform well on Global PIQA in\naggregate, but they exhibit weaker performance in lower-resource languages (up\nto a 37% accuracy gap, despite random chance at 50%). Open models generally\nperform worse than proprietary models. Global PIQA highlights that in many\nlanguages and cultures, everyday knowledge remains an area for improvement,\nalongside more widely-discussed capabilities such as complex reasoning and\nexpert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA\nprovides a glimpse into the wide diversity of cultures in which human language\nis embedded.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6384d424ac61472e5e92d403/6BaDk0HX_j2cVDJ8TosvC.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24081.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6384d424ac61472e5e92d403",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6384d424ac61472e5e92d403/RfEyLD3FmEobLcpNE9pNu.png",
            "fullname": "Catherine Arnett",
            "name": "catherinearnett",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 92
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.23925",
            "authors": [
                {
                    "_id": "69022e74646208eac0d1f726",
                    "name": "Guohao Sun",
                    "hidden": false
                },
                {
                    "_id": "69022e74646208eac0d1f727",
                    "name": "Hang Hua",
                    "hidden": false
                },
                {
                    "_id": "69022e74646208eac0d1f728",
                    "name": "Jian Wang",
                    "hidden": false
                },
                {
                    "_id": "69022e74646208eac0d1f729",
                    "name": "Jiebo Luo",
                    "hidden": false
                },
                {
                    "_id": "69022e74646208eac0d1f72a",
                    "name": "Sohail Dianat",
                    "hidden": false
                },
                {
                    "_id": "69022e74646208eac0d1f72b",
                    "name": "Majid Rabbani",
                    "hidden": false
                },
                {
                    "_id": "69022e74646208eac0d1f72c",
                    "name": "Raghuveer Rao",
                    "hidden": false
                },
                {
                    "_id": "69022e74646208eac0d1f72d",
                    "name": "Zhiqiang Tao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T23:10:06.000Z",
            "submittedOnDailyAt": "2025-10-29T16:50:17.842Z",
            "title": "Latent Chain-of-Thought for Visual Reasoning",
            "submittedOnDailyBy": {
                "_id": "639f8277beb95d698de007dd",
                "avatarUrl": "/avatars/57f223ccd9d3cb03166ccf0e41361c58.svg",
                "isPro": false,
                "fullname": "HangHua",
                "user": "hhua2",
                "type": "user"
            },
            "summary": "Chain-of-thought (CoT) reasoning is critical for improving the\ninterpretability and reliability of Large Vision-Language Models (LVLMs).\nHowever, existing training algorithms such as SFT, PPO, and GRPO may not\ngeneralize well across unseen reasoning tasks and heavily rely on a biased\nreward model. To address this challenge, we reformulate reasoning in LVLMs as\nposterior inference and propose a scalable training algorithm based on\namortized variational inference. By leveraging diversity-seeking reinforcement\nlearning algorithms, we introduce a novel sparse reward function for\ntoken-level learning signals that encourage diverse, high-likelihood latent\nCoT, overcoming deterministic sampling limitations and avoiding reward hacking.\nAdditionally, we implement a Bayesian inference-scaling strategy that replaces\ncostly Best-of-N and Beam Search with a marginal likelihood to efficiently rank\noptimal rationales and answers. We empirically demonstrate that the proposed\nmethod enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in\nterms of effectiveness, generalization, and interpretability.",
            "upvotes": 8,
            "discussionId": "69022e74646208eac0d1f72e",
            "githubRepo": "https://github.com/heliossun/LaCoT",
            "ai_summary": "The proposed method reformulates reasoning in Large Vision-Language Models as posterior inference using amortized variational inference and a sparse reward function, improving effectiveness, generalization, and interpretability.",
            "ai_keywords": [
                "Chain-of-thought",
                "Large Vision-Language Models",
                "SFT",
                "PPO",
                "GRPO",
                "posterior inference",
                "amortized variational inference",
                "diversity-seeking reinforcement learning",
                "sparse reward function",
                "token-level learning signals",
                "latent CoT",
                "deterministic sampling",
                "reward hacking",
                "Bayesian inference-scaling strategy",
                "marginal likelihood",
                "reasoning benchmarks"
            ],
            "githubStars": 8
        },
        "publishedAt": "2025-10-27T19:10:06.000Z",
        "title": "Latent Chain-of-Thought for Visual Reasoning",
        "summary": "Chain-of-thought (CoT) reasoning is critical for improving the\ninterpretability and reliability of Large Vision-Language Models (LVLMs).\nHowever, existing training algorithms such as SFT, PPO, and GRPO may not\ngeneralize well across unseen reasoning tasks and heavily rely on a biased\nreward model. To address this challenge, we reformulate reasoning in LVLMs as\nposterior inference and propose a scalable training algorithm based on\namortized variational inference. By leveraging diversity-seeking reinforcement\nlearning algorithms, we introduce a novel sparse reward function for\ntoken-level learning signals that encourage diverse, high-likelihood latent\nCoT, overcoming deterministic sampling limitations and avoiding reward hacking.\nAdditionally, we implement a Bayesian inference-scaling strategy that replaces\ncostly Best-of-N and Beam Search with a marginal likelihood to efficiently rank\noptimal rationales and answers. We empirically demonstrate that the proposed\nmethod enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in\nterms of effectiveness, generalization, and interpretability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23925.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "639f8277beb95d698de007dd",
            "avatarUrl": "/avatars/57f223ccd9d3cb03166ccf0e41361c58.svg",
            "fullname": "HangHua",
            "name": "hhua2",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.22768",
            "authors": [
                {
                    "_id": "6901199c646208eac0d1f151",
                    "user": {
                        "_id": "644ed57a67a3dd3d0728d889",
                        "avatarUrl": "/avatars/2df18a497c2c107a48d312e8f79c4ff8.svg",
                        "isPro": false,
                        "fullname": "Haoyi Qiu",
                        "user": "haoyiq114",
                        "type": "user"
                    },
                    "name": "Haoyi Qiu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:47:34.998Z",
                    "hidden": false
                },
                {
                    "_id": "6901199c646208eac0d1f152",
                    "name": "Yilun Zhou",
                    "hidden": false
                },
                {
                    "_id": "6901199c646208eac0d1f153",
                    "name": "Pranav Narayanan Venkit",
                    "hidden": false
                },
                {
                    "_id": "6901199c646208eac0d1f154",
                    "user": {
                        "_id": "64d931399a6a7ae984564a3c",
                        "avatarUrl": "/avatars/4de068b843eb1bd0cd188b33cfa30fbf.svg",
                        "isPro": false,
                        "fullname": "Kung-Hsiang Huang",
                        "user": "khhuang",
                        "type": "user"
                    },
                    "name": "Kung-Hsiang Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:47:37.482Z",
                    "hidden": false
                },
                {
                    "_id": "6901199c646208eac0d1f155",
                    "name": "Jiaxin Zhang",
                    "hidden": false
                },
                {
                    "_id": "6901199c646208eac0d1f156",
                    "name": "Nanyun Peng",
                    "hidden": false
                },
                {
                    "_id": "6901199c646208eac0d1f157",
                    "name": "Chien-Sheng Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-26T17:39:21.000Z",
            "submittedOnDailyAt": "2025-10-29T15:14:44.805Z",
            "title": "MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion",
            "submittedOnDailyBy": {
                "_id": "644ed57a67a3dd3d0728d889",
                "avatarUrl": "/avatars/2df18a497c2c107a48d312e8f79c4ff8.svg",
                "isPro": false,
                "fullname": "Haoyi Qiu",
                "user": "haoyiq114",
                "type": "user"
            },
            "summary": "As Large Vision-Language Models (LVLMs) are increasingly deployed in domains\nsuch as shopping, health, and news, they are exposed to pervasive persuasive\ncontent. A critical question is how these models function as persuadees-how and\nwhy they can be influenced by persuasive multimodal inputs. Understanding both\ntheir susceptibility to persuasion and the effectiveness of different\npersuasive strategies is crucial, as overly persuadable models may adopt\nmisleading beliefs, override user preferences, or generate unethical or unsafe\noutputs when exposed to manipulative messages. We introduce MMPersuade, a\nunified framework for systematically studying multimodal persuasion dynamics in\nLVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs\nimages and videos with established persuasion principles across commercial,\nsubjective and behavioral, and adversarial contexts, and (ii) an evaluation\nframework that quantifies both persuasion effectiveness and model\nsusceptibility via third-party agreement scoring and self-estimated token\nprobabilities on conversation histories. Our study of six leading LVLMs as\npersuadees yields three key insights: (i) multimodal inputs substantially\nincrease persuasion effectiveness-and model susceptibility-compared to text\nalone, especially in misinformation scenarios; (ii) stated prior preferences\ndecrease susceptibility, yet multimodal information maintains its persuasive\nadvantage; and (iii) different strategies vary in effectiveness across\ncontexts, with reciprocity being most potent in commercial and subjective\ncontexts, and credibility and logic prevailing in adversarial contexts. By\njointly analyzing persuasion effectiveness and susceptibility, MMPersuade\nprovides a principled foundation for developing models that are robust,\npreference-consistent, and ethically aligned when engaging with persuasive\nmultimodal content.",
            "upvotes": 5,
            "discussionId": "6901199c646208eac0d1f158",
            "ai_summary": "MMPersuade is a framework for studying multimodal persuasion in Large Vision-Language Models, revealing insights into their susceptibility and the effectiveness of various persuasive strategies across different contexts.",
            "ai_keywords": [
                "Large Vision-Language Models",
                "MMPersuade",
                "multimodal dataset",
                "persuasion principles",
                "third-party agreement scoring",
                "self-estimated token probabilities",
                "conversation histories",
                "multimodal inputs",
                "misinformation scenarios",
                "reciprocity",
                "credibility",
                "logic"
            ],
            "organization": {
                "_id": "5f6d64475e78cc6b0ed31e4c",
                "name": "Salesforce",
                "fullname": "Salesforce",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
            }
        },
        "publishedAt": "2025-10-26T13:39:21.000Z",
        "title": "MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion",
        "summary": "As Large Vision-Language Models (LVLMs) are increasingly deployed in domains\nsuch as shopping, health, and news, they are exposed to pervasive persuasive\ncontent. A critical question is how these models function as persuadees-how and\nwhy they can be influenced by persuasive multimodal inputs. Understanding both\ntheir susceptibility to persuasion and the effectiveness of different\npersuasive strategies is crucial, as overly persuadable models may adopt\nmisleading beliefs, override user preferences, or generate unethical or unsafe\noutputs when exposed to manipulative messages. We introduce MMPersuade, a\nunified framework for systematically studying multimodal persuasion dynamics in\nLVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs\nimages and videos with established persuasion principles across commercial,\nsubjective and behavioral, and adversarial contexts, and (ii) an evaluation\nframework that quantifies both persuasion effectiveness and model\nsusceptibility via third-party agreement scoring and self-estimated token\nprobabilities on conversation histories. Our study of six leading LVLMs as\npersuadees yields three key insights: (i) multimodal inputs substantially\nincrease persuasion effectiveness-and model susceptibility-compared to text\nalone, especially in misinformation scenarios; (ii) stated prior preferences\ndecrease susceptibility, yet multimodal information maintains its persuasive\nadvantage; and (iii) different strategies vary in effectiveness across\ncontexts, with reciprocity being most potent in commercial and subjective\ncontexts, and credibility and logic prevailing in adversarial contexts. By\njointly analyzing persuasion effectiveness and susceptibility, MMPersuade\nprovides a principled foundation for developing models that are robust,\npreference-consistent, and ethically aligned when engaging with persuasive\nmultimodal content.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22768.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "644ed57a67a3dd3d0728d889",
            "avatarUrl": "/avatars/2df18a497c2c107a48d312e8f79c4ff8.svg",
            "fullname": "Haoyi Qiu",
            "name": "haoyiq114",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "5f6d64475e78cc6b0ed31e4c",
            "name": "Salesforce",
            "fullname": "Salesforce",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.24645",
            "authors": [
                {
                    "_id": "6901786f646208eac0d1f399",
                    "name": "Zengzhuang Xu",
                    "hidden": false
                },
                {
                    "_id": "6901786f646208eac0d1f39a",
                    "user": {
                        "_id": "6550d7f0aae66b1676131620",
                        "avatarUrl": "/avatars/0a55b7d52fc1e1655671fe5cf8a86262.svg",
                        "isPro": false,
                        "fullname": "Bingguang Hao",
                        "user": "Bingguang",
                        "type": "user"
                    },
                    "name": "Bingguang Hao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:45:37.042Z",
                    "hidden": false
                },
                {
                    "_id": "6901786f646208eac0d1f39b",
                    "name": "Zechuan Wang",
                    "hidden": false
                },
                {
                    "_id": "6901786f646208eac0d1f39c",
                    "name": "Yuntao Wen",
                    "hidden": false
                },
                {
                    "_id": "6901786f646208eac0d1f39d",
                    "name": "Maolin Wang",
                    "hidden": false
                },
                {
                    "_id": "6901786f646208eac0d1f39e",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "6901786f646208eac0d1f39f",
                    "name": "Long Chen",
                    "hidden": false
                },
                {
                    "_id": "6901786f646208eac0d1f3a0",
                    "name": "Dong Wang",
                    "hidden": false
                },
                {
                    "_id": "6901786f646208eac0d1f3a1",
                    "name": "Yicheng Chen",
                    "hidden": false
                },
                {
                    "_id": "6901786f646208eac0d1f3a2",
                    "name": "Cunyin Peng",
                    "hidden": false
                },
                {
                    "_id": "6901786f646208eac0d1f3a3",
                    "name": "Chenyi Zhuang",
                    "hidden": false
                },
                {
                    "_id": "6901786f646208eac0d1f3a4",
                    "name": "Jinjie Gu",
                    "hidden": false
                },
                {
                    "_id": "6901786f646208eac0d1f3a5",
                    "name": "Leilei Gan",
                    "hidden": false
                },
                {
                    "_id": "6901786f646208eac0d1f3a6",
                    "name": "Xiangyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "6901786f646208eac0d1f3a7",
                    "name": "Shi Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T17:15:26.000Z",
            "submittedOnDailyAt": "2025-10-29T01:15:52.315Z",
            "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in\n  Multi-Turn Function Calling",
            "submittedOnDailyBy": {
                "_id": "6550d7f0aae66b1676131620",
                "avatarUrl": "/avatars/0a55b7d52fc1e1655671fe5cf8a86262.svg",
                "isPro": false,
                "fullname": "Bingguang Hao",
                "user": "Bingguang",
                "type": "user"
            },
            "summary": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning.",
            "upvotes": 4,
            "discussionId": "6901786f646208eac0d1f3a8",
            "projectPage": "https://huggingface.co/datasets/Bingguang/FunReason-MT",
            "ai_summary": "FunReason-MT is a novel data synthesis framework that enhances multi-turn function calling in large language models by addressing challenges in environment interaction, query synthesis, and chain-of-thought generation, achieving state-of-the-art performance on the Berkeley Function-Calling Leaderboard.",
            "ai_keywords": [
                "Function calling",
                "large language models",
                "autonomous agents",
                "data synthesis",
                "Environment-API Graph Interactions",
                "Advanced Tool-Query Synthesis",
                "Guided Iterative Chain",
                "chain-of-thought generation",
                "Berkeley Function-Calling Leaderboard"
            ],
            "organization": {
                "_id": "67aea5c8f086ab0f70ed97c9",
                "name": "inclusionAI",
                "fullname": "inclusionAI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
            }
        },
        "publishedAt": "2025-10-28T13:15:26.000Z",
        "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in\n  Multi-Turn Function Calling",
        "summary": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24645.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6550d7f0aae66b1676131620",
            "avatarUrl": "/avatars/0a55b7d52fc1e1655671fe5cf8a86262.svg",
            "fullname": "Bingguang Hao",
            "name": "Bingguang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "67aea5c8f086ab0f70ed97c9",
            "name": "inclusionAI",
            "fullname": "inclusionAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.24591",
            "authors": [
                {
                    "_id": "690172c7646208eac0d1f348",
                    "name": "Christine Ye",
                    "hidden": false
                },
                {
                    "_id": "690172c7646208eac0d1f349",
                    "name": "Sihan Yuan",
                    "hidden": false
                },
                {
                    "_id": "690172c7646208eac0d1f34a",
                    "name": "Suchetha Cooray",
                    "hidden": false
                },
                {
                    "_id": "690172c7646208eac0d1f34b",
                    "name": "Steven Dillmann",
                    "hidden": false
                },
                {
                    "_id": "690172c7646208eac0d1f34c",
                    "name": "Ian L. V. Roque",
                    "hidden": false
                },
                {
                    "_id": "690172c7646208eac0d1f34d",
                    "name": "Dalya Baron",
                    "hidden": false
                },
                {
                    "_id": "690172c7646208eac0d1f34e",
                    "name": "Philipp Frank",
                    "hidden": false
                },
                {
                    "_id": "690172c7646208eac0d1f34f",
                    "name": "Sergio Martin-Alvarez",
                    "hidden": false
                },
                {
                    "_id": "690172c7646208eac0d1f350",
                    "name": "Nolan Koblischke",
                    "hidden": false
                },
                {
                    "_id": "690172c7646208eac0d1f351",
                    "name": "Frank J Qu",
                    "hidden": false
                },
                {
                    "_id": "690172c7646208eac0d1f352",
                    "name": "Diyi Yang",
                    "hidden": false
                },
                {
                    "_id": "690172c7646208eac0d1f353",
                    "name": "Risa Wechsler",
                    "hidden": false
                },
                {
                    "_id": "690172c7646208eac0d1f354",
                    "name": "Ioana Ciuca",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T16:21:19.000Z",
            "submittedOnDailyAt": "2025-10-29T00:20:08.707Z",
            "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Frontier AI agents show increasing promise as scientific research assistants,\nand may eventually be useful for extended, open-ended research workflows.\nHowever, in order to use agents for novel research, we must first assess the\nunderlying faithfulness and correctness of their work. To evaluate agents as\nresearch assistants, we introduce ReplicationBench, an evaluation framework\nthat tests whether agents can replicate entire research papers drawn from the\nastrophysics literature. Astrophysics, where research relies heavily on\narchival data and computational study while requiring little real-world\nexperimentation, is a particularly useful testbed for AI agents in scientific\nresearch. We split each paper into tasks which require agents to replicate the\npaper's core contributions, including the experimental setup, derivations, data\nanalysis, and codebase. Each task is co-developed with the original paper\nauthors and targets a key scientific result, enabling objective evaluation of\nboth faithfulness (adherence to original methods) and correctness (technical\naccuracy of results). ReplicationBench is extremely challenging for current\nfrontier language models: even the best-performing language models score under\n20%. We analyze ReplicationBench trajectories in collaboration with domain\nexperts and find a rich, diverse set of failure modes for agents in scientific\nresearch. ReplicationBench establishes the first benchmark of paper-scale,\nexpert-validated astrophysics research tasks, reveals insights about agent\nperformance generalizable to other domains of data-driven science, and provides\na scalable framework for measuring AI agents' reliability in scientific\nresearch.",
            "upvotes": 4,
            "discussionId": "690172c8646208eac0d1f355",
            "ai_summary": "ReplicationBench evaluates AI agents' ability to replicate astrophysics research papers, providing insights into their faithfulness and correctness in scientific research tasks.",
            "ai_keywords": [
                "ReplicationBench",
                "AI agents",
                "scientific research assistants",
                "faithfulness",
                "correctness",
                "research papers",
                "astrophysics",
                "experimental setup",
                "derivations",
                "data analysis",
                "codebase",
                "language models",
                "benchmark",
                "expert-validated",
                "data-driven science",
                "reliability"
            ]
        },
        "publishedAt": "2025-10-28T12:21:19.000Z",
        "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?",
        "summary": "Frontier AI agents show increasing promise as scientific research assistants,\nand may eventually be useful for extended, open-ended research workflows.\nHowever, in order to use agents for novel research, we must first assess the\nunderlying faithfulness and correctness of their work. To evaluate agents as\nresearch assistants, we introduce ReplicationBench, an evaluation framework\nthat tests whether agents can replicate entire research papers drawn from the\nastrophysics literature. Astrophysics, where research relies heavily on\narchival data and computational study while requiring little real-world\nexperimentation, is a particularly useful testbed for AI agents in scientific\nresearch. We split each paper into tasks which require agents to replicate the\npaper's core contributions, including the experimental setup, derivations, data\nanalysis, and codebase. Each task is co-developed with the original paper\nauthors and targets a key scientific result, enabling objective evaluation of\nboth faithfulness (adherence to original methods) and correctness (technical\naccuracy of results). ReplicationBench is extremely challenging for current\nfrontier language models: even the best-performing language models score under\n20%. We analyze ReplicationBench trajectories in collaboration with domain\nexperts and find a rich, diverse set of failure modes for agents in scientific\nresearch. ReplicationBench establishes the first benchmark of paper-scale,\nexpert-validated astrophysics research tasks, reveals insights about agent\nperformance generalizable to other domains of data-driven science, and provides\na scalable framework for measuring AI agents' reliability in scientific\nresearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24591.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 149
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24448",
            "authors": [
                {
                    "_id": "6901734b646208eac0d1f363",
                    "name": "Pablo Acuaviva",
                    "hidden": false
                },
                {
                    "_id": "6901734b646208eac0d1f364",
                    "name": "Aram Davtyan",
                    "hidden": false
                },
                {
                    "_id": "6901734b646208eac0d1f365",
                    "name": "Mariam Hassan",
                    "hidden": false
                },
                {
                    "_id": "6901734b646208eac0d1f366",
                    "name": "Sebastian Stapf",
                    "hidden": false
                },
                {
                    "_id": "6901734b646208eac0d1f367",
                    "name": "Ahmad Rahimi",
                    "hidden": false
                },
                {
                    "_id": "6901734b646208eac0d1f368",
                    "name": "Alexandre Alahi",
                    "hidden": false
                },
                {
                    "_id": "6901734b646208eac0d1f369",
                    "name": "Paolo Favaro",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T14:12:11.000Z",
            "submittedOnDailyAt": "2025-10-29T00:50:57.135Z",
            "title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.",
            "upvotes": 4,
            "discussionId": "6901734b646208eac0d1f36a",
            "ai_summary": "Video Diffusion Models (VDMs) show higher data efficiency than large language models across various visual tasks, suggesting video pretraining can enhance visual foundation models.",
            "ai_keywords": [
                "Video Diffusion Models",
                "VDMs",
                "large language models",
                "LLMs",
                "spatiotemporal data",
                "inductive biases",
                "ARC-AGI",
                "ConceptARC",
                "visual games",
                "route planning",
                "cellular automata",
                "visual foundation models"
            ]
        },
        "publishedAt": "2025-10-28T10:12:11.000Z",
        "title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
        "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24448.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 149
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.22876",
            "authors": [
                {
                    "_id": "6900e9a5646208eac0d1f08a",
                    "user": {
                        "_id": "63c726b18bfd6a208eb5b125",
                        "avatarUrl": "/avatars/28a3b81d8a5b1a2af379e8dbeae799bd.svg",
                        "isPro": false,
                        "fullname": "Ranran Haoran Zhang",
                        "user": "windchimeran",
                        "type": "user"
                    },
                    "name": "Ranran Haoran Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:49:25.762Z",
                    "hidden": false
                },
                {
                    "_id": "6900e9a5646208eac0d1f08b",
                    "name": "Soumik Dey",
                    "hidden": false
                },
                {
                    "_id": "6900e9a5646208eac0d1f08c",
                    "name": "Ashirbad Mishra",
                    "hidden": false
                },
                {
                    "_id": "6900e9a5646208eac0d1f08d",
                    "name": "Hansi Wu",
                    "hidden": false
                },
                {
                    "_id": "6900e9a5646208eac0d1f08e",
                    "name": "Binbin Li",
                    "hidden": false
                },
                {
                    "_id": "6900e9a5646208eac0d1f08f",
                    "name": "Rui Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63c726b18bfd6a208eb5b125/JFo9fLebG7VcbvS5urVR2.png"
            ],
            "publishedAt": "2025-10-26T23:59:23.000Z",
            "submittedOnDailyAt": "2025-10-29T19:45:45.780Z",
            "title": "Batch Speculative Decoding Done Right",
            "submittedOnDailyBy": {
                "_id": "63c726b18bfd6a208eb5b125",
                "avatarUrl": "/avatars/28a3b81d8a5b1a2af379e8dbeae799bd.svg",
                "isPro": false,
                "fullname": "Ranran Haoran Zhang",
                "user": "windchimeran",
                "type": "user"
            },
            "summary": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3times throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec.",
            "upvotes": 3,
            "discussionId": "6900e9a5646208eac0d1f090",
            "githubRepo": "https://github.com/eBay/spec_dec",
            "ai_summary": "Batch speculative decoding improves LLM inference throughput by managing ragged tensors to maintain output equivalence and reduce realignment overhead.",
            "ai_keywords": [
                "speculative decoding",
                "LLM inference",
                "draft model",
                "target model",
                "ragged tensor problem",
                "position IDs",
                "attention masks",
                "KV-cache state",
                "output equivalence",
                "autoregressive generation",
                "synchronization requirements",
                "EQSPEC",
                "EXSPEC",
                "SpecBench",
                "Vicuna-7B/68M",
                "Qwen3-8B/0.6B",
                "GLM-4-9B/0.6B"
            ],
            "githubStars": 4,
            "organization": {
                "_id": "623c72b6483fb88b35620a27",
                "name": "PennState",
                "fullname": "Pennsylvania State University"
            }
        },
        "publishedAt": "2025-10-26T19:59:23.000Z",
        "title": "Batch Speculative Decoding Done Right",
        "summary": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3times throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63c726b18bfd6a208eb5b125/JFo9fLebG7VcbvS5urVR2.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22876.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63c726b18bfd6a208eb5b125",
            "avatarUrl": "/avatars/28a3b81d8a5b1a2af379e8dbeae799bd.svg",
            "fullname": "Ranran Haoran Zhang",
            "name": "windchimeran",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "623c72b6483fb88b35620a27",
            "name": "PennState",
            "fullname": "Pennsylvania State University"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.22795",
            "authors": [
                {
                    "_id": "6900bec1646208eac0d1effa",
                    "user": {
                        "_id": "67002a13c4deb90b3fb8a040",
                        "avatarUrl": "/avatars/ba977617128ef88c75b47d5dd12c5fb0.svg",
                        "isPro": false,
                        "fullname": "Michael",
                        "user": "ungersboeck",
                        "type": "user"
                    },
                    "name": "Michael Ungersböck",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:49:30.694Z",
                    "hidden": false
                },
                {
                    "_id": "6900bec1646208eac0d1effb",
                    "name": "Florian Grötschla",
                    "hidden": false
                },
                {
                    "_id": "6900bec1646208eac0d1effc",
                    "name": "Luca A. Lanzendörfer",
                    "hidden": false
                },
                {
                    "_id": "6900bec1646208eac0d1effd",
                    "name": "June Young Yi",
                    "hidden": false
                },
                {
                    "_id": "6900bec1646208eac0d1effe",
                    "name": "Changho Choi",
                    "hidden": false
                },
                {
                    "_id": "6900bec1646208eac0d1efff",
                    "name": "Roger Wattenhofer",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67002a13c4deb90b3fb8a040/hYmA2LMoNA5nBxO3sLIkY.png"
            ],
            "publishedAt": "2025-10-26T18:57:16.000Z",
            "submittedOnDailyAt": "2025-10-29T13:55:02.667Z",
            "title": "SAO-Instruct: Free-form Audio Editing using Natural Language\n  Instructions",
            "submittedOnDailyBy": {
                "_id": "67002a13c4deb90b3fb8a040",
                "avatarUrl": "/avatars/ba977617128ef88c75b47d5dd12c5fb0.svg",
                "isPro": false,
                "fullname": "Michael",
                "user": "ungersboeck",
                "type": "user"
            },
            "summary": "Generative models have made significant progress in synthesizing\nhigh-fidelity audio from short textual descriptions. However, editing existing\naudio using natural language has remained largely underexplored. Current\napproaches either require the complete description of the edited audio or are\nconstrained to predefined edit instructions that lack flexibility. In this\nwork, we introduce SAO-Instruct, a model based on Stable Audio Open capable of\nediting audio clips using any free-form natural language instruction. To train\nour model, we create a dataset of audio editing triplets (input audio, edit\ninstruction, output audio) using Prompt-to-Prompt, DDPM inversion, and a manual\nediting pipeline. Although partially trained on synthetic data, our model\ngeneralizes well to real in-the-wild audio clips and unseen edit instructions.\nWe demonstrate that SAO-Instruct achieves competitive performance on objective\nmetrics and outperforms other audio editing approaches in a subjective\nlistening study. To encourage future research, we release our code and model\nweights.",
            "upvotes": 2,
            "discussionId": "6900bec1646208eac0d1f000",
            "projectPage": "https://eth-disco.github.io/sao-instruct/",
            "githubRepo": "https://github.com/eth-disco/sao-instruct",
            "ai_summary": "SAO-Instruct, a generative model based on Stable Audio Open, allows flexible audio editing using natural language instructions, outperforming existing methods in both objective and subjective evaluations.",
            "ai_keywords": [
                "generative models",
                "high-fidelity audio",
                "natural language",
                "audio editing",
                "free-form natural language instruction",
                "Prompt-to-Prompt",
                "DDPM inversion",
                "synthetic data",
                "real in-the-wild audio clips",
                "objective metrics",
                "subjective listening study"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "65f853c578e4139ae6001020",
                "name": "disco-eth",
                "fullname": "DISCO",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647e4df11a1fcad2fdc3e47d/9nnmQsfCbJpoQULH7LLEv.jpeg"
            }
        },
        "publishedAt": "2025-10-26T14:57:16.000Z",
        "title": "SAO-Instruct: Free-form Audio Editing using Natural Language\n  Instructions",
        "summary": "Generative models have made significant progress in synthesizing\nhigh-fidelity audio from short textual descriptions. However, editing existing\naudio using natural language has remained largely underexplored. Current\napproaches either require the complete description of the edited audio or are\nconstrained to predefined edit instructions that lack flexibility. In this\nwork, we introduce SAO-Instruct, a model based on Stable Audio Open capable of\nediting audio clips using any free-form natural language instruction. To train\nour model, we create a dataset of audio editing triplets (input audio, edit\ninstruction, output audio) using Prompt-to-Prompt, DDPM inversion, and a manual\nediting pipeline. Although partially trained on synthetic data, our model\ngeneralizes well to real in-the-wild audio clips and unseen edit instructions.\nWe demonstrate that SAO-Instruct achieves competitive performance on objective\nmetrics and outperforms other audio editing approaches in a subjective\nlistening study. To encourage future research, we release our code and model\nweights.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67002a13c4deb90b3fb8a040/hYmA2LMoNA5nBxO3sLIkY.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22795.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67002a13c4deb90b3fb8a040",
            "avatarUrl": "/avatars/ba977617128ef88c75b47d5dd12c5fb0.svg",
            "fullname": "Michael",
            "name": "ungersboeck",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "65f853c578e4139ae6001020",
            "name": "disco-eth",
            "fullname": "DISCO",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647e4df11a1fcad2fdc3e47d/9nnmQsfCbJpoQULH7LLEv.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.22590",
            "authors": [
                {
                    "_id": "6901dbff646208eac0d1f5c9",
                    "user": {
                        "_id": "658888d30100bf3373c726a0",
                        "avatarUrl": "/avatars/8d03397f6ac08f12445e9ad8a8198d6f.svg",
                        "isPro": false,
                        "fullname": "Yassir LAIRGI",
                        "user": "lairgiyassir",
                        "type": "user"
                    },
                    "name": "Yassir Lairgi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:40:41.622Z",
                    "hidden": false
                },
                {
                    "_id": "6901dbff646208eac0d1f5ca",
                    "name": "Ludovic Moncla",
                    "hidden": false
                },
                {
                    "_id": "6901dbff646208eac0d1f5cb",
                    "name": "Khalid Benabdeslem",
                    "hidden": false
                },
                {
                    "_id": "6901dbff646208eac0d1f5cc",
                    "name": "Rémy Cazabet",
                    "hidden": false
                },
                {
                    "_id": "6901dbff646208eac0d1f5cd",
                    "name": "Pierre Cléau",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-26T09:10:26.000Z",
            "submittedOnDailyAt": "2025-10-29T20:39:00.962Z",
            "title": "ATOM: AdapTive and OptiMized dynamic temporal knowledge graph\n  construction using LLMs",
            "submittedOnDailyBy": {
                "_id": "658888d30100bf3373c726a0",
                "avatarUrl": "/avatars/8d03397f6ac08f12445e9ad8a8198d6f.svg",
                "isPro": false,
                "fullname": "Yassir LAIRGI",
                "user": "lairgiyassir",
                "type": "user"
            },
            "summary": "In today's rapidly expanding data landscape, knowledge extraction from\nunstructured text is vital for real-time analytics, temporal inference, and\ndynamic memory frameworks. However, traditional static knowledge graph (KG)\nconstruction often overlooks the dynamic and time-sensitive nature of\nreal-world data, limiting adaptability to continuous changes. Moreover, recent\nzero- or few-shot approaches that avoid domain-specific fine-tuning or reliance\non prebuilt ontologies often suffer from instability across multiple runs, as\nwell as incomplete coverage of key facts. To address these challenges, we\nintroduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that\nbuilds and continuously updates Temporal Knowledge Graphs (TKGs) from\nunstructured texts. ATOM splits input documents into minimal, self-contained\n\"atomic\" facts, improving extraction exhaustivity and stability. Then, it\nconstructs atomic TKGs from these facts while employing a dual-time modeling\nthat distinguishes when information is observed from when it is valid. The\nresulting atomic TKGs are subsequently merged in parallel. Empirical\nevaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17%\nbetter stability, and over 90% latency reduction compared to baseline methods,\ndemonstrating a strong scalability potential for dynamic TKG construction.",
            "upvotes": 2,
            "discussionId": "6901dbff646208eac0d1f5ce",
            "githubRepo": "https://github.com/AuvaLab/itext2kg",
            "ai_summary": "ATOM is a few-shot, scalable approach for constructing and updating Temporal Knowledge Graphs from unstructured text, improving exhaustivity, stability, and latency.",
            "ai_keywords": [
                "Temporal Knowledge Graphs",
                "TKGs",
                "dual-time modeling",
                "atomic facts",
                "few-shot",
                "scalability",
                "exhaustivity",
                "stability",
                "latency reduction"
            ],
            "githubStars": 851
        },
        "publishedAt": "2025-10-26T05:10:26.000Z",
        "title": "ATOM: AdapTive and OptiMized dynamic temporal knowledge graph\n  construction using LLMs",
        "summary": "In today's rapidly expanding data landscape, knowledge extraction from\nunstructured text is vital for real-time analytics, temporal inference, and\ndynamic memory frameworks. However, traditional static knowledge graph (KG)\nconstruction often overlooks the dynamic and time-sensitive nature of\nreal-world data, limiting adaptability to continuous changes. Moreover, recent\nzero- or few-shot approaches that avoid domain-specific fine-tuning or reliance\non prebuilt ontologies often suffer from instability across multiple runs, as\nwell as incomplete coverage of key facts. To address these challenges, we\nintroduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that\nbuilds and continuously updates Temporal Knowledge Graphs (TKGs) from\nunstructured texts. ATOM splits input documents into minimal, self-contained\n\"atomic\" facts, improving extraction exhaustivity and stability. Then, it\nconstructs atomic TKGs from these facts while employing a dual-time modeling\nthat distinguishes when information is observed from when it is valid. The\nresulting atomic TKGs are subsequently merged in parallel. Empirical\nevaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17%\nbetter stability, and over 90% latency reduction compared to baseline methods,\ndemonstrating a strong scalability potential for dynamic TKG construction.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22590.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "658888d30100bf3373c726a0",
            "avatarUrl": "/avatars/8d03397f6ac08f12445e9ad8a8198d6f.svg",
            "fullname": "Yassir LAIRGI",
            "name": "lairgiyassir",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.22099",
            "authors": [
                {
                    "_id": "6901760b646208eac0d1f37a",
                    "name": "Xuanming Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-25T00:50:47.000Z",
            "submittedOnDailyAt": "2025-10-29T00:35:45.044Z",
            "title": "Generalization or Memorization: Dynamic Decoding for Mode Steering",
            "submittedOnDailyBy": {
                "_id": "65fc5109899083a2aad987c5",
                "avatarUrl": "/avatars/289dbb8128746d931118cff6f6871a45.svg",
                "isPro": false,
                "fullname": "XUANMING ZHANG",
                "user": "XUANMINGZHANG",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) exhibit a troubling duality, capable of both\nremarkable generalization and brittle, verbatim memorization of their training\ndata. This unpredictability undermines their reliability in high-stakes\napplications. In this work, we propose a unified framework to understand,\nidentify, and control these distinct reasoning modes. First, we introduce a\ntheoretical model based on the Information Bottleneck (IB) principle,\nformalizing generalization as the learning of a compressed, task-relevant\nrepresentation and memorization as a failure to compress. Building on this\ntheory, we develop Dynamic Mode Steering (DMS), a novel inference-time\nalgorithm which comprises two components: (1) a lightweight, causally-grounded\nlinear probe that identifies the model's instantaneous reliance on\nmemorization, and (2) a dynamic activation steering mechanism that nudges the\nmodel's computation towards pre-identified generalization circuits. We frame\nDMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning\nand faithfulness tasks demonstrate that DMS significantly improves logical\nconsistency and factual accuracy, thereby offering a principled approach to\nenhancing LLM reliability.",
            "upvotes": 2,
            "discussionId": "6901760c646208eac0d1f37b",
            "ai_summary": "A framework using the Information Bottleneck principle and Dynamic Mode Steering algorithm improves the reliability of Large Language Models by balancing generalization and memorization.",
            "ai_keywords": [
                "Information Bottleneck",
                "Dynamic Mode Steering",
                "linear probe",
                "dynamic activation steering",
                "adaptive",
                "self-contrastive decoding",
                "generalization",
                "memorization",
                "logical consistency",
                "factual accuracy"
            ],
            "organization": {
                "_id": "6112d84f8c2e1f4060908c9e",
                "name": "stanfordnlp",
                "fullname": "Stanford NLP",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1628624969199-6032802e1f993496bc14d9e3.png"
            }
        },
        "publishedAt": "2025-10-24T20:50:47.000Z",
        "title": "Generalization or Memorization: Dynamic Decoding for Mode Steering",
        "summary": "Large Language Models (LLMs) exhibit a troubling duality, capable of both\nremarkable generalization and brittle, verbatim memorization of their training\ndata. This unpredictability undermines their reliability in high-stakes\napplications. In this work, we propose a unified framework to understand,\nidentify, and control these distinct reasoning modes. First, we introduce a\ntheoretical model based on the Information Bottleneck (IB) principle,\nformalizing generalization as the learning of a compressed, task-relevant\nrepresentation and memorization as a failure to compress. Building on this\ntheory, we develop Dynamic Mode Steering (DMS), a novel inference-time\nalgorithm which comprises two components: (1) a lightweight, causally-grounded\nlinear probe that identifies the model's instantaneous reliance on\nmemorization, and (2) a dynamic activation steering mechanism that nudges the\nmodel's computation towards pre-identified generalization circuits. We frame\nDMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning\nand faithfulness tasks demonstrate that DMS significantly improves logical\nconsistency and factual accuracy, thereby offering a principled approach to\nenhancing LLM reliability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22099.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65fc5109899083a2aad987c5",
            "avatarUrl": "/avatars/289dbb8128746d931118cff6f6871a45.svg",
            "fullname": "XUANMING ZHANG",
            "name": "XUANMINGZHANG",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "6112d84f8c2e1f4060908c9e",
            "name": "stanfordnlp",
            "fullname": "Stanford NLP",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1628624969199-6032802e1f993496bc14d9e3.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.22728",
            "authors": [
                {
                    "_id": "6901537e646208eac0d1f175",
                    "name": "Khai Le-Duc",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f176",
                    "name": "Duy M. H. Nguyen",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f177",
                    "name": "Phuong T. H. Trinh",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f178",
                    "name": "Tien-Phat Nguyen",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f179",
                    "name": "Nghiem T. Diep",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f17a",
                    "name": "An Ngo",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f17b",
                    "name": "Tung Vu",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f17c",
                    "name": "Trinh Vuong",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f17d",
                    "name": "Anh-Tien Nguyen",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f17e",
                    "name": "Mau Nguyen",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f17f",
                    "name": "Van Trung Hoang",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f180",
                    "name": "Khai-Nguyen Nguyen",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f181",
                    "name": "Hy Nguyen",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f182",
                    "name": "Chris Ngo",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f183",
                    "name": "Anji Liu",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f184",
                    "name": "Nhat Ho",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f185",
                    "name": "Anne-Christin Hauschild",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f186",
                    "name": "Khanh Xuan Nguyen",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f187",
                    "name": "Thanh Nguyen-Tang",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f188",
                    "name": "Pengtao Xie",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f189",
                    "name": "Daniel Sonntag",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f18a",
                    "name": "James Zou",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f18b",
                    "name": "Mathias Niepert",
                    "hidden": false
                },
                {
                    "_id": "6901537e646208eac0d1f18c",
                    "name": "Anh Totti Nguyen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-26T15:57:14.000Z",
            "submittedOnDailyAt": "2025-10-29T20:43:13.956Z",
            "title": "S-Chain: Structured Visual Chain-of-Thought For Medicine",
            "submittedOnDailyBy": {
                "_id": "653c43909d0209606188bf6f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653c43909d0209606188bf6f/bnl8dDLLs4Tk9ODqhMrDL.jpeg",
                "isPro": false,
                "fullname": "Khai Le-Duc",
                "user": "leduckhai",
                "type": "user"
            },
            "summary": "Faithful reasoning in medical vision-language models (VLMs) requires not only\naccurate predictions but also transparent alignment between textual rationales\nand visual evidence. While Chain-of-Thought (CoT) prompting has shown promise\nin medical visual question answering (VQA), no large-scale expert-level dataset\nhas captured stepwise reasoning with precise visual grounding. We introduce\nS-Chain, the first large-scale dataset of 12,000 expert-annotated medical\nimages with bounding boxes and structured visual CoT (SV-CoT), explicitly\nlinking visual regions to reasoning steps. The dataset further supports 16\nlanguages, totaling over 700k VQA pairs for broad multilingual applicability.\nUsing S-Chain, we benchmark state-of-the-art medical VLMs (ExGra-Med,\nLLaVA-Med) and general-purpose VLMs (Qwen2.5-VL, InternVL2.5), showing that\nSV-CoT supervision significantly improves interpretability, grounding fidelity,\nand robustness. Beyond benchmarking, we study its synergy with\nretrieval-augmented generation, revealing how domain knowledge and visual\ngrounding interact during autoregressive reasoning. Finally, we propose a new\nmechanism that strengthens the alignment between visual evidence and reasoning,\nimproving both reliability and efficiency. S-Chain establishes a new benchmark\nfor grounded medical reasoning and paves the way toward more trustworthy and\nexplainable medical VLMs.",
            "upvotes": 1,
            "discussionId": "6901537e646208eac0d1f18d",
            "projectPage": "https://s-chain.github.io/",
            "githubRepo": "https://github.com/leduckhai/S-Chain",
            "ai_summary": "S-Chain, a large-scale multilingual dataset with structured visual chain-of-thought annotations, enhances the interpretability, grounding, and robustness of medical vision-language models.",
            "ai_keywords": [
                "Chain-of-Thought",
                "CoT prompting",
                "medical visual question answering",
                "VQA",
                "bounding boxes",
                "structured visual CoT",
                "SV-CoT",
                "ExGra-Med",
                "LLaVA-Med",
                "Qwen2.5-VL",
                "InternVL2.5",
                "retrieval-augmented generation",
                "autoregressive reasoning",
                "grounded medical reasoning",
                "trustworthy",
                "explainable medical VLMs"
            ],
            "githubStars": 27
        },
        "publishedAt": "2025-10-26T11:57:14.000Z",
        "title": "S-Chain: Structured Visual Chain-of-Thought For Medicine",
        "summary": "Faithful reasoning in medical vision-language models (VLMs) requires not only\naccurate predictions but also transparent alignment between textual rationales\nand visual evidence. While Chain-of-Thought (CoT) prompting has shown promise\nin medical visual question answering (VQA), no large-scale expert-level dataset\nhas captured stepwise reasoning with precise visual grounding. We introduce\nS-Chain, the first large-scale dataset of 12,000 expert-annotated medical\nimages with bounding boxes and structured visual CoT (SV-CoT), explicitly\nlinking visual regions to reasoning steps. The dataset further supports 16\nlanguages, totaling over 700k VQA pairs for broad multilingual applicability.\nUsing S-Chain, we benchmark state-of-the-art medical VLMs (ExGra-Med,\nLLaVA-Med) and general-purpose VLMs (Qwen2.5-VL, InternVL2.5), showing that\nSV-CoT supervision significantly improves interpretability, grounding fidelity,\nand robustness. Beyond benchmarking, we study its synergy with\nretrieval-augmented generation, revealing how domain knowledge and visual\ngrounding interact during autoregressive reasoning. Finally, we propose a new\nmechanism that strengthens the alignment between visual evidence and reasoning,\nimproving both reliability and efficiency. S-Chain establishes a new benchmark\nfor grounded medical reasoning and paves the way toward more trustworthy and\nexplainable medical VLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22728.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653c43909d0209606188bf6f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653c43909d0209606188bf6f/bnl8dDLLs4Tk9ODqhMrDL.jpeg",
            "fullname": "Khai Le-Duc",
            "name": "leduckhai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 23
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.23667",
            "authors": [
                {
                    "_id": "69020e82646208eac0d1f646",
                    "name": "Amin Heyrani Nobari",
                    "hidden": false
                },
                {
                    "_id": "69020e82646208eac0d1f647",
                    "name": "Lyle Regenwetter",
                    "hidden": false
                },
                {
                    "_id": "69020e82646208eac0d1f648",
                    "name": "Cyril Picard",
                    "hidden": false
                },
                {
                    "_id": "69020e82646208eac0d1f649",
                    "name": "Ligong Han",
                    "hidden": false
                },
                {
                    "_id": "69020e82646208eac0d1f64a",
                    "name": "Faez Ahmed",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-26T15:11:54.000Z",
            "submittedOnDailyAt": "2025-10-29T11:38:59.884Z",
            "title": "Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free\n  Structural Topology Optimization",
            "submittedOnDailyBy": {
                "_id": "64d516ba80d47a6b76fc1015",
                "avatarUrl": "/avatars/e520825f2ac9ff047844496ae2dad7d6.svg",
                "isPro": false,
                "fullname": "Amin Heyrani Nobari",
                "user": "ahn1376",
                "type": "user"
            },
            "summary": "Structural topology optimization (TO) is central to engineering design but\nremains computationally intensive due to complex physics and hard constraints.\nExisting deep-learning methods are limited to fixed square grids, a few\nhand-coded boundary conditions, and post-hoc optimization, preventing general\ndeployment. We introduce Optimize Any Topology (OAT), a foundation-model\nframework that directly predicts minimum-compliance layouts for arbitrary\naspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines\na resolution- and shape-agnostic autoencoder with an implicit neural-field\ndecoder and a conditional latent-diffusion model trained on OpenTO, a new\ncorpus of 2.2 million optimized structures covering 2 million unique\nboundary-condition configurations. On four public benchmarks and two\nchallenging unseen tests, OAT lowers mean compliance up to 90% relative to the\nbest prior models and delivers sub-1 second inference on a single GPU across\nresolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These\nresults establish OAT as a general, fast, and resolution-free framework for\nphysics-aware topology optimization and provide a large-scale dataset to spur\nfurther research in generative modeling for inverse design. Code & data can be\nfound at https://github.com/ahnobari/OptimizeAnyTopology.",
            "upvotes": 1,
            "discussionId": "69020e82646208eac0d1f64b",
            "ai_summary": "OAT, a deep-learning framework combining autoencoder, neural-field decoder, and latent-diffusion model, achieves fast and general topology optimization with high performance across various conditions and resolutions.",
            "ai_keywords": [
                "autoencoder",
                "implicit neural-field decoder",
                "conditional latent-diffusion model",
                "OpenTO",
                "topology optimization",
                "mean compliance",
                "inference",
                "resolution-free",
                "physics-aware"
            ],
            "organization": {
                "_id": "678bb3a2ff9242f6ac8b5896",
                "name": "OpenTO",
                "fullname": "OpenTO"
            }
        },
        "publishedAt": "2025-10-26T11:11:54.000Z",
        "title": "Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free\n  Structural Topology Optimization",
        "summary": "Structural topology optimization (TO) is central to engineering design but\nremains computationally intensive due to complex physics and hard constraints.\nExisting deep-learning methods are limited to fixed square grids, a few\nhand-coded boundary conditions, and post-hoc optimization, preventing general\ndeployment. We introduce Optimize Any Topology (OAT), a foundation-model\nframework that directly predicts minimum-compliance layouts for arbitrary\naspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines\na resolution- and shape-agnostic autoencoder with an implicit neural-field\ndecoder and a conditional latent-diffusion model trained on OpenTO, a new\ncorpus of 2.2 million optimized structures covering 2 million unique\nboundary-condition configurations. On four public benchmarks and two\nchallenging unseen tests, OAT lowers mean compliance up to 90% relative to the\nbest prior models and delivers sub-1 second inference on a single GPU across\nresolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These\nresults establish OAT as a general, fast, and resolution-free framework for\nphysics-aware topology optimization and provide a large-scale dataset to spur\nfurther research in generative modeling for inverse design. Code & data can be\nfound at https://github.com/ahnobari/OptimizeAnyTopology.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23667.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64d516ba80d47a6b76fc1015",
            "avatarUrl": "/avatars/e520825f2ac9ff047844496ae2dad7d6.svg",
            "fullname": "Amin Heyrani Nobari",
            "name": "ahn1376",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "678bb3a2ff9242f6ac8b5896",
            "name": "OpenTO",
            "fullname": "OpenTO"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.22373",
            "authors": [
                {
                    "_id": "6901cb91646208eac0d1f5a8",
                    "name": "Yupeng Xie",
                    "hidden": false
                },
                {
                    "_id": "6901cb91646208eac0d1f5a9",
                    "name": "Zhiyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6901cb91646208eac0d1f5aa",
                    "name": "Yifan Wu",
                    "hidden": false
                },
                {
                    "_id": "6901cb91646208eac0d1f5ab",
                    "name": "Sirong Lu",
                    "hidden": false
                },
                {
                    "_id": "6901cb91646208eac0d1f5ac",
                    "name": "Jiayi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6901cb91646208eac0d1f5ad",
                    "name": "Zhaoyang Yu",
                    "hidden": false
                },
                {
                    "_id": "6901cb91646208eac0d1f5ae",
                    "name": "Jinlin Wang",
                    "hidden": false
                },
                {
                    "_id": "6901cb91646208eac0d1f5af",
                    "name": "Sirui Hong",
                    "hidden": false
                },
                {
                    "_id": "6901cb91646208eac0d1f5b0",
                    "name": "Bang Liu",
                    "hidden": false
                },
                {
                    "_id": "6901cb91646208eac0d1f5b1",
                    "name": "Chenglin Wu",
                    "hidden": false
                },
                {
                    "_id": "6901cb91646208eac0d1f5b2",
                    "name": "Yuyu Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-25T17:31:02.000Z",
            "submittedOnDailyAt": "2025-10-29T06:49:03.404Z",
            "title": "VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations",
            "submittedOnDailyBy": {
                "_id": "683ebfd683a130f817d95ce8",
                "avatarUrl": "/avatars/74e8453f24b2e803a604412a32def111.svg",
                "isPro": false,
                "fullname": "xypkent",
                "user": "xypkent",
                "type": "user"
            },
            "summary": "Visualization, a domain-specific yet widely used form of imagery, is an\neffective way to turn complex datasets into intuitive insights, and its value\ndepends on whether data are faithfully represented, clearly communicated, and\naesthetically designed. However, evaluating visualization quality is\nchallenging: unlike natural images, it requires simultaneous judgment across\ndata encoding accuracy, information expressiveness, and visual aesthetics.\nAlthough multimodal large language models (MLLMs) have shown promising\nperformance in aesthetic assessment of natural images, no systematic benchmark\nexists for measuring their capabilities in evaluating visualizations. To\naddress this, we propose VisJudge-Bench, the first comprehensive benchmark for\nevaluating MLLMs' performance in assessing visualization aesthetics and\nquality. It contains 3,090 expert-annotated samples from real-world scenarios,\ncovering single visualizations, multiple visualizations, and dashboards across\n32 chart types. Systematic testing on this benchmark reveals that even the most\nadvanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human\nexperts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a\ncorrelation with human ratings of only 0.429. To address this issue, we propose\nVisJudge, a model specifically designed for visualization aesthetics and\nquality assessment. Experimental results demonstrate that VisJudge\nsignificantly narrows the gap with human judgment, reducing the MAE to 0.442 (a\n19.8% reduction) and increasing the consistency with human experts to 0.681 (a\n58.7% improvement) compared to GPT-5. The benchmark is available at\nhttps://github.com/HKUSTDial/VisJudgeBench.",
            "upvotes": 1,
            "discussionId": "6901cb91646208eac0d1f5b3",
            "githubRepo": "https://github.com/HKUSTDial/VisJudgeBench",
            "ai_summary": "VisJudge-Bench is a benchmark for evaluating MLLMs' performance in assessing visualization aesthetics and quality, revealing gaps compared to human experts and demonstrating improvements with the VisJudge model.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "VisJudge-Bench",
                "expert-annotated samples",
                "chart types",
                "Mean Absolute Error",
                "MAE",
                "correlation with human ratings",
                "VisJudge"
            ],
            "githubStars": 11
        },
        "publishedAt": "2025-10-25T13:31:02.000Z",
        "title": "VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations",
        "summary": "Visualization, a domain-specific yet widely used form of imagery, is an\neffective way to turn complex datasets into intuitive insights, and its value\ndepends on whether data are faithfully represented, clearly communicated, and\naesthetically designed. However, evaluating visualization quality is\nchallenging: unlike natural images, it requires simultaneous judgment across\ndata encoding accuracy, information expressiveness, and visual aesthetics.\nAlthough multimodal large language models (MLLMs) have shown promising\nperformance in aesthetic assessment of natural images, no systematic benchmark\nexists for measuring their capabilities in evaluating visualizations. To\naddress this, we propose VisJudge-Bench, the first comprehensive benchmark for\nevaluating MLLMs' performance in assessing visualization aesthetics and\nquality. It contains 3,090 expert-annotated samples from real-world scenarios,\ncovering single visualizations, multiple visualizations, and dashboards across\n32 chart types. Systematic testing on this benchmark reveals that even the most\nadvanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human\nexperts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a\ncorrelation with human ratings of only 0.429. To address this issue, we propose\nVisJudge, a model specifically designed for visualization aesthetics and\nquality assessment. Experimental results demonstrate that VisJudge\nsignificantly narrows the gap with human judgment, reducing the MAE to 0.442 (a\n19.8% reduction) and increasing the consistency with human experts to 0.681 (a\n58.7% improvement) compared to GPT-5. The benchmark is available at\nhttps://github.com/HKUSTDial/VisJudgeBench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22373.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "683ebfd683a130f817d95ce8",
            "avatarUrl": "/avatars/74e8453f24b2e803a604412a32def111.svg",
            "fullname": "xypkent",
            "name": "xypkent",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.22319",
            "authors": [
                {
                    "_id": "69023886646208eac0d1f746",
                    "name": "Jing Wang",
                    "hidden": false
                },
                {
                    "_id": "69023886646208eac0d1f747",
                    "name": "Jiajun Liang",
                    "hidden": false
                },
                {
                    "_id": "69023886646208eac0d1f748",
                    "name": "Jie Liu",
                    "hidden": false
                },
                {
                    "_id": "69023886646208eac0d1f749",
                    "name": "Henglin Liu",
                    "hidden": false
                },
                {
                    "_id": "69023886646208eac0d1f74a",
                    "name": "Gongye Liu",
                    "hidden": false
                },
                {
                    "_id": "69023886646208eac0d1f74b",
                    "name": "Jun Zheng",
                    "hidden": false
                },
                {
                    "_id": "69023886646208eac0d1f74c",
                    "name": "Wanyuan Pang",
                    "hidden": false
                },
                {
                    "_id": "69023886646208eac0d1f74d",
                    "name": "Ao Ma",
                    "hidden": false
                },
                {
                    "_id": "69023886646208eac0d1f74e",
                    "name": "Zhenyu Xie",
                    "hidden": false
                },
                {
                    "_id": "69023886646208eac0d1f74f",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "69023886646208eac0d1f750",
                    "name": "Meng Wang",
                    "hidden": false
                },
                {
                    "_id": "69023886646208eac0d1f751",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "69023886646208eac0d1f752",
                    "name": "Xiaodan Liang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-25T14:51:17.000Z",
            "submittedOnDailyAt": "2025-10-29T14:25:38.204Z",
            "title": "GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via\n  Regulated Clipping",
            "submittedOnDailyBy": {
                "_id": "6381847a471a4550ff298c63",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6381847a471a4550ff298c63/RTKepvX67R6pLiiUidpUO.png",
                "isPro": false,
                "fullname": "Jun",
                "user": "zxbsmk",
                "type": "user"
            },
            "summary": "Recently, GRPO-based reinforcement learning has shown remarkable progress in\noptimizing flow-matching models, effectively improving their alignment with\ntask-specific rewards. Within these frameworks, the policy update relies on\nimportance-ratio clipping to constrain overconfident positive and negative\ngradients. However, in practice, we observe a systematic shift in the\nimportance-ratio distribution-its mean falls below 1 and its variance differs\nsubstantially across timesteps. This left-shifted and inconsistent distribution\nprevents positive-advantage samples from entering the clipped region, causing\nthe mechanism to fail in constraining overconfident positive updates. As a\nresult, the policy model inevitably enters an implicit over-optimization\nstage-while the proxy reward continues to increase, essential metrics such as\nimage quality and text-prompt alignment deteriorate sharply, ultimately making\nthe learned policy impractical for real-world use. To address this issue, we\nintroduce GRPO-Guard, a simple yet effective enhancement to existing GRPO\nframeworks. Our method incorporates ratio normalization, which restores a\nbalanced and step-consistent importance ratio, ensuring that PPO clipping\nproperly constrains harmful updates across denoising timesteps. In addition, a\ngradient reweighting strategy equalizes policy gradients over noise conditions,\npreventing excessive updates from particular timestep regions. Together, these\ndesigns act as a regulated clipping mechanism, stabilizing optimization and\nsubstantially mitigating implicit over-optimization without relying on heavy KL\nregularization. Extensive experiments on multiple diffusion backbones (e.g.,\nSD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard\nsignificantly reduces over-optimization while maintaining or even improving\ngeneration quality.",
            "upvotes": 1,
            "discussionId": "69023886646208eac0d1f753",
            "projectPage": "https://jingw193.github.io/GRPO-Guard",
            "ai_summary": "GRPO-Guard enhances GRPO-based reinforcement learning by normalizing importance ratios and reweighting gradients, mitigating over-optimization in flow-matching models without heavy KL regularization.",
            "ai_keywords": [
                "GRPO",
                "reinforcement learning",
                "flow-matching models",
                "importance-ratio clipping",
                "policy update",
                "left-shifted distribution",
                "positive-advantage samples",
                "implicit over-optimization",
                "ratio normalization",
                "gradient reweighting",
                "diffusion backbones",
                "SD3.5M",
                "Flux.1-dev",
                "proxy tasks",
                "generation quality"
            ]
        },
        "publishedAt": "2025-10-25T10:51:17.000Z",
        "title": "GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via\n  Regulated Clipping",
        "summary": "Recently, GRPO-based reinforcement learning has shown remarkable progress in\noptimizing flow-matching models, effectively improving their alignment with\ntask-specific rewards. Within these frameworks, the policy update relies on\nimportance-ratio clipping to constrain overconfident positive and negative\ngradients. However, in practice, we observe a systematic shift in the\nimportance-ratio distribution-its mean falls below 1 and its variance differs\nsubstantially across timesteps. This left-shifted and inconsistent distribution\nprevents positive-advantage samples from entering the clipped region, causing\nthe mechanism to fail in constraining overconfident positive updates. As a\nresult, the policy model inevitably enters an implicit over-optimization\nstage-while the proxy reward continues to increase, essential metrics such as\nimage quality and text-prompt alignment deteriorate sharply, ultimately making\nthe learned policy impractical for real-world use. To address this issue, we\nintroduce GRPO-Guard, a simple yet effective enhancement to existing GRPO\nframeworks. Our method incorporates ratio normalization, which restores a\nbalanced and step-consistent importance ratio, ensuring that PPO clipping\nproperly constrains harmful updates across denoising timesteps. In addition, a\ngradient reweighting strategy equalizes policy gradients over noise conditions,\npreventing excessive updates from particular timestep regions. Together, these\ndesigns act as a regulated clipping mechanism, stabilizing optimization and\nsubstantially mitigating implicit over-optimization without relying on heavy KL\nregularization. Extensive experiments on multiple diffusion backbones (e.g.,\nSD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard\nsignificantly reduces over-optimization while maintaining or even improving\ngeneration quality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22319.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6381847a471a4550ff298c63",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6381847a471a4550ff298c63/RTKepvX67R6pLiiUidpUO.png",
            "fullname": "Jun",
            "name": "zxbsmk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 38
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.21323",
            "authors": [
                {
                    "_id": "6900561a22d452aac6dd4439",
                    "user": {
                        "_id": "660229531737e5cd4a6e7948",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_S1dkCnhREr1XchhdPEGM.png",
                        "isPro": false,
                        "fullname": "Shufan Shen",
                        "user": "shufanshen",
                        "type": "user"
                    },
                    "name": "Shufan Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:35:14.639Z",
                    "hidden": false
                },
                {
                    "_id": "6900561a22d452aac6dd443a",
                    "name": "Junshu Sun",
                    "hidden": false
                },
                {
                    "_id": "6900561a22d452aac6dd443b",
                    "name": "Qingming Huang",
                    "hidden": false
                },
                {
                    "_id": "6900561a22d452aac6dd443c",
                    "name": "Shuhui Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-24T10:29:31.000Z",
            "submittedOnDailyAt": "2025-10-29T00:24:20.855Z",
            "title": "VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a\n  Unified Concept Set",
            "submittedOnDailyBy": {
                "_id": "660229531737e5cd4a6e7948",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_S1dkCnhREr1XchhdPEGM.png",
                "isPro": false,
                "fullname": "Shufan Shen",
                "user": "shufanshen",
                "type": "user"
            },
            "summary": "The alignment of vision-language representations endows current\nVision-Language Models (VLMs) with strong multi-modal reasoning capabilities.\nHowever, the interpretability of the alignment component remains uninvestigated\ndue to the difficulty in mapping the semantics of multi-modal representations\ninto a unified concept set. To address this problem, we propose VL-SAE, a\nsparse autoencoder that encodes vision-language representations into its hidden\nactivations. Each neuron in its hidden layer correlates to a concept\nrepresented by semantically similar images and texts, thereby interpreting\nthese representations with a unified concept set. To establish the\nneuron-concept correlation, we encourage semantically similar representations\nto exhibit consistent neuron activations during self-supervised training.\nFirst, to measure the semantic similarity of multi-modal representations, we\nperform their alignment in an explicit form based on cosine similarity. Second,\nwe construct the VL-SAE with a distance-based encoder and two modality-specific\ndecoders to ensure the activation consistency of semantically similar\nrepresentations. Experiments across multiple VLMs (e.g., CLIP, LLaVA)\ndemonstrate the superior capability of VL-SAE in interpreting and enhancing the\nvision-language alignment. For interpretation, the alignment between vision and\nlanguage representations can be understood by comparing their semantics with\nconcepts. For enhancement, the alignment can be strengthened by aligning\nvision-language representations at the concept level, contributing to\nperformance improvements in downstream tasks, including zero-shot image\nclassification and hallucination elimination. Codes are available at\nhttps://github.com/ssfgunner/VL-SAE.",
            "upvotes": 1,
            "discussionId": "6900561b22d452aac6dd443d",
            "githubRepo": "https://github.com/ssfgunner/VL-SAE",
            "ai_summary": "VL-SAE, a sparse autoencoder, enhances vision-language alignment by correlating neurons to unified concepts, improving interpretability and performance in tasks like zero-shot image classification and hallucination elimination.",
            "ai_keywords": [
                "sparse autoencoder",
                "vision-language representations",
                "multi-modal reasoning",
                "neuron-concept correlation",
                "cosine similarity",
                "distance-based encoder",
                "modality-specific decoders",
                "zero-shot image classification",
                "hallucination elimination"
            ],
            "githubStars": 4,
            "organization": {
                "_id": "632fea4a9c9aa2bfdf5982f8",
                "name": "UCAS",
                "fullname": "ucas",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632fe99f2a6ef6fb4ad7ba08/QiMtq1UkcKsI9yy1ZCU1m.jpeg"
            }
        },
        "publishedAt": "2025-10-24T06:29:31.000Z",
        "title": "VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a\n  Unified Concept Set",
        "summary": "The alignment of vision-language representations endows current\nVision-Language Models (VLMs) with strong multi-modal reasoning capabilities.\nHowever, the interpretability of the alignment component remains uninvestigated\ndue to the difficulty in mapping the semantics of multi-modal representations\ninto a unified concept set. To address this problem, we propose VL-SAE, a\nsparse autoencoder that encodes vision-language representations into its hidden\nactivations. Each neuron in its hidden layer correlates to a concept\nrepresented by semantically similar images and texts, thereby interpreting\nthese representations with a unified concept set. To establish the\nneuron-concept correlation, we encourage semantically similar representations\nto exhibit consistent neuron activations during self-supervised training.\nFirst, to measure the semantic similarity of multi-modal representations, we\nperform their alignment in an explicit form based on cosine similarity. Second,\nwe construct the VL-SAE with a distance-based encoder and two modality-specific\ndecoders to ensure the activation consistency of semantically similar\nrepresentations. Experiments across multiple VLMs (e.g., CLIP, LLaVA)\ndemonstrate the superior capability of VL-SAE in interpreting and enhancing the\nvision-language alignment. For interpretation, the alignment between vision and\nlanguage representations can be understood by comparing their semantics with\nconcepts. For enhancement, the alignment can be strengthened by aligning\nvision-language representations at the concept level, contributing to\nperformance improvements in downstream tasks, including zero-shot image\nclassification and hallucination elimination. Codes are available at\nhttps://github.com/ssfgunner/VL-SAE.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21323.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "660229531737e5cd4a6e7948",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_S1dkCnhREr1XchhdPEGM.png",
            "fullname": "Shufan Shen",
            "name": "shufanshen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "632fea4a9c9aa2bfdf5982f8",
            "name": "UCAS",
            "fullname": "ucas",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632fe99f2a6ef6fb4ad7ba08/QiMtq1UkcKsI9yy1ZCU1m.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.20155",
            "authors": [
                {
                    "_id": "6901b7bd646208eac0d1f567",
                    "user": {
                        "_id": "6462120032083c28b408e754",
                        "avatarUrl": "/avatars/f02c6d23f03420b716d995c92f8e2213.svg",
                        "isPro": false,
                        "fullname": "Penghao Wang",
                        "user": "AuWang",
                        "type": "user"
                    },
                    "name": "Penghao Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:40:52.365Z",
                    "hidden": false
                },
                {
                    "_id": "6901b7bd646208eac0d1f568",
                    "name": "Yiyang He",
                    "hidden": false
                },
                {
                    "_id": "6901b7bd646208eac0d1f569",
                    "name": "Xin Lv",
                    "hidden": false
                },
                {
                    "_id": "6901b7bd646208eac0d1f56a",
                    "name": "Yukai Zhou",
                    "hidden": false
                },
                {
                    "_id": "6901b7bd646208eac0d1f56b",
                    "name": "Lan Xu",
                    "hidden": false
                },
                {
                    "_id": "6901b7bd646208eac0d1f56c",
                    "name": "Jingyi Yu",
                    "hidden": false
                },
                {
                    "_id": "6901b7bd646208eac0d1f56d",
                    "name": "Jiayuan Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T03:06:08.000Z",
            "submittedOnDailyAt": "2025-10-29T05:18:53.009Z",
            "title": "PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D\n  Part Understanding",
            "submittedOnDailyBy": {
                "_id": "6462120032083c28b408e754",
                "avatarUrl": "/avatars/f02c6d23f03420b716d995c92f8e2213.svg",
                "isPro": false,
                "fullname": "Penghao Wang",
                "user": "AuWang",
                "type": "user"
            },
            "summary": "Understanding objects at the level of their constituent parts is fundamental\nto advancing computer vision, graphics, and robotics. While datasets like\nPartNet have driven progress in 3D part understanding, their reliance on\nuntextured geometries and expert-dependent annotation limits scalability and\nusability. We introduce PartNeXt, a next-generation dataset addressing these\ngaps with over 23,000 high-quality, textured 3D models annotated with\nfine-grained, hierarchical part labels across 50 categories. We benchmark\nPartNeXt on two tasks: (1) class-agnostic part segmentation, where\nstate-of-the-art methods (e.g., PartField, SAMPart3D) struggle with\nfine-grained and leaf-level parts, and (2) 3D part-centric question answering,\na new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary\npart grounding. Additionally, training Point-SAM on PartNeXt yields substantial\ngains over PartNet, underscoring the dataset's superior quality and diversity.\nBy combining scalable annotation, texture-aware labels, and multi-task\nevaluation, PartNeXt opens new avenues for research in structured 3D\nunderstanding.",
            "upvotes": 1,
            "discussionId": "6901b7bd646208eac0d1f56e",
            "projectPage": "https://authoritywang.github.io/partnext/",
            "githubRepo": "https://github.com/AuthorityWang/PartNeXt",
            "ai_summary": "PartNeXt, a high-quality, textured 3D dataset with fine-grained part labels, improves performance in class-agnostic part segmentation and 3D part-centric question answering, highlighting gaps in open-vocabulary part grounding.",
            "ai_keywords": [
                "PartNeXt",
                "class-agnostic part segmentation",
                "PartField",
                "SAMPart3D",
                "3D part-centric question answering",
                "3D-LLMs",
                "open-vocabulary part grounding",
                "Point-SAM",
                "structured 3D understanding"
            ],
            "githubStars": 68
        },
        "publishedAt": "2025-10-22T23:06:08.000Z",
        "title": "PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D\n  Part Understanding",
        "summary": "Understanding objects at the level of their constituent parts is fundamental\nto advancing computer vision, graphics, and robotics. While datasets like\nPartNet have driven progress in 3D part understanding, their reliance on\nuntextured geometries and expert-dependent annotation limits scalability and\nusability. We introduce PartNeXt, a next-generation dataset addressing these\ngaps with over 23,000 high-quality, textured 3D models annotated with\nfine-grained, hierarchical part labels across 50 categories. We benchmark\nPartNeXt on two tasks: (1) class-agnostic part segmentation, where\nstate-of-the-art methods (e.g., PartField, SAMPart3D) struggle with\nfine-grained and leaf-level parts, and (2) 3D part-centric question answering,\na new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary\npart grounding. Additionally, training Point-SAM on PartNeXt yields substantial\ngains over PartNet, underscoring the dataset's superior quality and diversity.\nBy combining scalable annotation, texture-aware labels, and multi-task\nevaluation, PartNeXt opens new avenues for research in structured 3D\nunderstanding.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20155.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6462120032083c28b408e754",
            "avatarUrl": "/avatars/f02c6d23f03420b716d995c92f8e2213.svg",
            "fullname": "Penghao Wang",
            "name": "AuWang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.23828",
            "authors": [
                {
                    "_id": "6902843d72739622ee92a618",
                    "name": "Mena Attia",
                    "hidden": false
                },
                {
                    "_id": "6902843d72739622ee92a619",
                    "name": "Aashiq Muhamed",
                    "hidden": false
                },
                {
                    "_id": "6902843d72739622ee92a61a",
                    "name": "Mai Alkhamissi",
                    "hidden": false
                },
                {
                    "_id": "6902843d72739622ee92a61b",
                    "name": "Thamar Solorio",
                    "hidden": false
                },
                {
                    "_id": "6902843d72739622ee92a61c",
                    "name": "Mona Diab",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T20:13:32.000Z",
            "submittedOnDailyAt": "2025-10-29T19:47:57.342Z",
            "title": "Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural\n  Processing of Figurative Language",
            "submittedOnDailyBy": {
                "_id": "64755a83e0b188d3cb2579d8",
                "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
                "isPro": false,
                "fullname": "Aashiq Muhamed",
                "user": "aashiqmuhamed",
                "type": "user"
            },
            "summary": "We present a comprehensive evaluation of the ability of large language models\n(LLMs) to process culturally grounded language, specifically to understand and\npragmatically use figurative expressions that encode local knowledge and\ncultural nuance. Using figurative language as a proxy for cultural nuance and\nlocal knowledge, we design evaluation tasks for contextual understanding,\npragmatic use, and connotation interpretation in Arabic and English. We\nevaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms,\nmultidialectal Arabic proverbs, and English proverbs. Our results show a\nconsistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower\nthan for English proverbs, and performance for Egyptian idioms is 10.28% lower\nthan for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07%\nrelative to understanding, though providing contextual idiomatic sentences\nimproves accuracy by 10.66%. Models also struggle with connotative meaning,\nreaching at most 85.58% agreement with human annotators on idioms with 100%\ninter-annotator agreement. These findings demonstrate that figurative language\nserves as an effective diagnostic for cultural reasoning: while LLMs can often\ninterpret figurative meaning, they face challenges in using it appropriately.\nTo support future research, we release Kinayat, the first dataset of Egyptian\nArabic idioms designed for both figurative understanding and pragmatic use\nevaluation.",
            "upvotes": 0,
            "discussionId": "6902843d72739622ee92a61d",
            "organization": {
                "_id": "6474ab086d4dda6f7c6beaee",
                "name": "cmu-lti",
                "fullname": "CMU-LTI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63a4d079658851481f7e4394/HOFKzjMXriLBOGJw6S8uI.png"
            }
        },
        "publishedAt": "2025-10-27T16:13:32.000Z",
        "title": "Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural\n  Processing of Figurative Language",
        "summary": "We present a comprehensive evaluation of the ability of large language models\n(LLMs) to process culturally grounded language, specifically to understand and\npragmatically use figurative expressions that encode local knowledge and\ncultural nuance. Using figurative language as a proxy for cultural nuance and\nlocal knowledge, we design evaluation tasks for contextual understanding,\npragmatic use, and connotation interpretation in Arabic and English. We\nevaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms,\nmultidialectal Arabic proverbs, and English proverbs. Our results show a\nconsistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower\nthan for English proverbs, and performance for Egyptian idioms is 10.28% lower\nthan for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07%\nrelative to understanding, though providing contextual idiomatic sentences\nimproves accuracy by 10.66%. Models also struggle with connotative meaning,\nreaching at most 85.58% agreement with human annotators on idioms with 100%\ninter-annotator agreement. These findings demonstrate that figurative language\nserves as an effective diagnostic for cultural reasoning: while LLMs can often\ninterpret figurative meaning, they face challenges in using it appropriately.\nTo support future research, we release Kinayat, the first dataset of Egyptian\nArabic idioms designed for both figurative understanding and pragmatic use\nevaluation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23828.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64755a83e0b188d3cb2579d8",
            "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
            "fullname": "Aashiq Muhamed",
            "name": "aashiqmuhamed",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "6474ab086d4dda6f7c6beaee",
            "name": "cmu-lti",
            "fullname": "CMU-LTI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63a4d079658851481f7e4394/HOFKzjMXriLBOGJw6S8uI.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.22264",
            "authors": [
                {
                    "_id": "690099a6646208eac0d1ef97",
                    "user": {
                        "_id": "67d2f1d89c0aff2e5e498f3c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d2f1d89c0aff2e5e498f3c/5yckUjssk77GwMucCLkKx.png",
                        "isPro": false,
                        "fullname": "iliass ayaou",
                        "user": "datalyes",
                        "type": "user"
                    },
                    "name": "Iliass Ayaou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-29T12:49:32.842Z",
                    "hidden": false
                },
                {
                    "_id": "690099a6646208eac0d1ef98",
                    "name": "Denis Cavallucci",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67d2f1d89c0aff2e5e498f3c/SUI1BRCRsfmbXP_7xNM9L.jpeg"
            ],
            "publishedAt": "2025-10-25T12:01:46.000Z",
            "submittedOnDailyAt": "2025-10-29T09:22:50.679Z",
            "title": "PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text\n  Embedding",
            "submittedOnDailyBy": {
                "_id": "67d2f1d89c0aff2e5e498f3c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d2f1d89c0aff2e5e498f3c/5yckUjssk77GwMucCLkKx.png",
                "isPro": false,
                "fullname": "iliass ayaou",
                "user": "datalyes",
                "type": "user"
            },
            "summary": "Patent text embeddings enable prior art search, technology landscaping, and\npatent analysis, yet existing benchmarks inadequately capture patent-specific\nchallenges. We introduce PatenTEB, a comprehensive benchmark comprising 15\ntasks across retrieval, classification, paraphrase, and clustering, with 2.06\nmillion examples. PatenTEB employs domain-stratified splits, domain specific\nhard negative mining, and systematic coverage of asymmetric\nfragment-to-document matching scenarios absent from general embedding\nbenchmarks. We develop the patembed model family through multi-task training,\nspanning 67M to 344M parameters with context lengths up to 4096 tokens.\nExternal validation shows strong generalization: patembed-base achieves\nstate-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445\nprevious best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.\nSystematic ablations reveal that multi-task training improves external\ngeneralization despite minor benchmark costs, and that domain-pretrained\ninitialization provides consistent advantages across task families. All\nresources will be made available at https://github.com/iliass-y/patenteb.\nKeywords: patent retrieval, sentence embeddings, multi-task learning,\nasymmetric retrieval, benchmark evaluation, contrastive learning.",
            "upvotes": 0,
            "discussionId": "690099a7646208eac0d1ef99",
            "ai_summary": "PatenTEB is a comprehensive benchmark for patent text embeddings with 15 tasks, and the patembed model family demonstrates strong generalization across various patent-specific challenges.",
            "ai_keywords": [
                "domain-stratified splits",
                "domain specific hard negative mining",
                "asymmetric fragment-to-document matching",
                "multi-task training",
                "span",
                "context lengths",
                "V-measure",
                "NDCG@100",
                "domain-pretrained initialization"
            ]
        },
        "publishedAt": "2025-10-25T08:01:46.000Z",
        "title": "PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text\n  Embedding",
        "summary": "Patent text embeddings enable prior art search, technology landscaping, and\npatent analysis, yet existing benchmarks inadequately capture patent-specific\nchallenges. We introduce PatenTEB, a comprehensive benchmark comprising 15\ntasks across retrieval, classification, paraphrase, and clustering, with 2.06\nmillion examples. PatenTEB employs domain-stratified splits, domain specific\nhard negative mining, and systematic coverage of asymmetric\nfragment-to-document matching scenarios absent from general embedding\nbenchmarks. We develop the patembed model family through multi-task training,\nspanning 67M to 344M parameters with context lengths up to 4096 tokens.\nExternal validation shows strong generalization: patembed-base achieves\nstate-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445\nprevious best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.\nSystematic ablations reveal that multi-task training improves external\ngeneralization despite minor benchmark costs, and that domain-pretrained\ninitialization provides consistent advantages across task families. All\nresources will be made available at https://github.com/iliass-y/patenteb.\nKeywords: patent retrieval, sentence embeddings, multi-task learning,\nasymmetric retrieval, benchmark evaluation, contrastive learning.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67d2f1d89c0aff2e5e498f3c/SUI1BRCRsfmbXP_7xNM9L.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22264.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67d2f1d89c0aff2e5e498f3c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d2f1d89c0aff2e5e498f3c/5yckUjssk77GwMucCLkKx.png",
            "fullname": "iliass ayaou",
            "name": "datalyes",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    }
]