[
    {
        "paper": {
            "id": "2510.24411",
            "authors": [
                {
                    "_id": "6901c572646208eac0d1f58b",
                    "user": {
                        "_id": "6064a0eeb1703ddba0d458b9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
                        "isPro": false,
                        "fullname": "Qiushi",
                        "user": "QiushiSun",
                        "type": "user"
                    },
                    "name": "Qiushi Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-30T14:41:31.108Z",
                    "hidden": false
                },
                {
                    "_id": "6901c572646208eac0d1f58c",
                    "user": {
                        "_id": "624561c1939c9acec2103534",
                        "avatarUrl": "/avatars/3efdf797c53d153cea7415213fb5afc7.svg",
                        "isPro": false,
                        "fullname": "Mukai Li",
                        "user": "kiaia",
                        "type": "user"
                    },
                    "name": "Mukai Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:57:43.415Z",
                    "hidden": false
                },
                {
                    "_id": "6901c572646208eac0d1f58d",
                    "name": "Zhoumianze Liu",
                    "hidden": false
                },
                {
                    "_id": "6901c572646208eac0d1f58e",
                    "name": "Zhihui Xie",
                    "hidden": false
                },
                {
                    "_id": "6901c572646208eac0d1f58f",
                    "name": "Fangzhi Xu",
                    "hidden": false
                },
                {
                    "_id": "6901c572646208eac0d1f590",
                    "name": "Zhangyue Yin",
                    "hidden": false
                },
                {
                    "_id": "6901c572646208eac0d1f591",
                    "user": {
                        "_id": "63340dbbd92c5842ae71d1e9",
                        "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
                        "isPro": false,
                        "fullname": "Kanzhi Cheng",
                        "user": "cckevinn",
                        "type": "user"
                    },
                    "name": "Kanzhi Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:57:41.222Z",
                    "hidden": false
                },
                {
                    "_id": "6901c572646208eac0d1f592",
                    "name": "Zehao Li",
                    "hidden": false
                },
                {
                    "_id": "6901c572646208eac0d1f593",
                    "user": {
                        "_id": "642b9861bb77f8456634b048",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/VrNmmcdgX7FufQmdP5YaG.jpeg",
                        "isPro": false,
                        "fullname": "Zichen Ding",
                        "user": "heroding77",
                        "type": "user"
                    },
                    "name": "Zichen Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:57:38.971Z",
                    "hidden": false
                },
                {
                    "_id": "6901c572646208eac0d1f594",
                    "name": "Qi Liu",
                    "hidden": false
                },
                {
                    "_id": "6901c572646208eac0d1f595",
                    "name": "Zhiyong Wu",
                    "hidden": false
                },
                {
                    "_id": "6901c572646208eac0d1f596",
                    "name": "Zhuosheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6901c572646208eac0d1f597",
                    "name": "Ben Kao",
                    "hidden": false
                },
                {
                    "_id": "6901c572646208eac0d1f598",
                    "name": "Lingpeng Kong",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/bw0LkLXWMJv2phqt5NTPT.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/LzYmfXUDFU8qdRwvvP_t1.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/oYs157OjuXN_AKMQb2vrl.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/iBKBpbGwt3NyyxB-BuDX5.png"
            ],
            "publishedAt": "2025-10-28T13:22:39.000Z",
            "submittedOnDailyAt": "2025-11-03T00:39:54.915Z",
            "title": "OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid\n  Validation in Realistic Workflows",
            "submittedOnDailyBy": {
                "_id": "6064a0eeb1703ddba0d458b9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
                "isPro": false,
                "fullname": "Qiushi",
                "user": "QiushiSun",
                "type": "user"
            },
            "summary": "Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.",
            "upvotes": 60,
            "discussionId": "6901c572646208eac0d1f599",
            "githubRepo": "https://github.com/OS-Copilot/OS-Sentinel",
            "ai_summary": "A hybrid safety detection framework combining formal verification and VLM-based contextual assessment improves the detection of unsafe operations in mobile agents.",
            "ai_keywords": [
                "Vision-Language Models",
                "VLMs",
                "Formal Verifier",
                "Contextual Judge",
                "MobileRisk-Live",
                "safety detection benchmark",
                "system-level violations",
                "contextual risks",
                "autonomous mobile agents"
            ],
            "githubStars": 27,
            "organization": {
                "_id": "61bb0986699d29d369eba1b2",
                "name": "hkunlp",
                "fullname": "NLP Group of The University of Hong Kong",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1639647572687-618767e4238063b4615d042b.jpeg"
            }
        },
        "publishedAt": "2025-10-28T09:22:39.000Z",
        "title": "OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid\n  Validation in Realistic Workflows",
        "summary": "Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/bw0LkLXWMJv2phqt5NTPT.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/LzYmfXUDFU8qdRwvvP_t1.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/oYs157OjuXN_AKMQb2vrl.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/iBKBpbGwt3NyyxB-BuDX5.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24411.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6064a0eeb1703ddba0d458b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
            "fullname": "Qiushi",
            "name": "QiushiSun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 20
        },
        "organization": {
            "_id": "61bb0986699d29d369eba1b2",
            "name": "hkunlp",
            "fullname": "NLP Group of The University of Hong Kong",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1639647572687-618767e4238063b4615d042b.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.27492",
            "authors": [
                {
                    "_id": "690813a7812eca10f9cc5e01",
                    "user": {
                        "_id": "645b4819f9d4ec91fdd54852",
                        "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
                        "isPro": false,
                        "fullname": "Jiawei Gu",
                        "user": "kuvvi",
                        "type": "user"
                    },
                    "name": "Jiawei Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:52:44.478Z",
                    "hidden": false
                },
                {
                    "_id": "690813a7812eca10f9cc5e02",
                    "user": {
                        "_id": "653dd16277c2f09452ad37cd",
                        "avatarUrl": "/avatars/a95f9527722845a5414d86180c8e945d.svg",
                        "isPro": false,
                        "fullname": "Yunzhuo Hao",
                        "user": "luckychao",
                        "type": "user"
                    },
                    "name": "Yunzhuo Hao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:52:42.043Z",
                    "hidden": false
                },
                {
                    "_id": "690813a7812eca10f9cc5e03",
                    "user": {
                        "_id": "6715a16f60dd5f7f395b34cb",
                        "avatarUrl": "/avatars/f92240519132c693de30c133ca784322.svg",
                        "isPro": false,
                        "fullname": "Will Wang",
                        "user": "wangwill",
                        "type": "user"
                    },
                    "name": "Huichen Will Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:52:37.981Z",
                    "hidden": false
                },
                {
                    "_id": "690813a7812eca10f9cc5e04",
                    "name": "Linjie Li",
                    "hidden": false
                },
                {
                    "_id": "690813a7812eca10f9cc5e05",
                    "name": "Michael Qizhe Shieh",
                    "hidden": false
                },
                {
                    "_id": "690813a7812eca10f9cc5e06",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "690813a7812eca10f9cc5e07",
                    "name": "Ranjay Krishna",
                    "hidden": false
                },
                {
                    "_id": "690813a7812eca10f9cc5e08",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-30T17:51:38.000Z",
            "submittedOnDailyAt": "2025-11-03T00:17:01.674Z",
            "title": "ThinkMorph: Emergent Properties in Multimodal Interleaved\n  Chain-of-Thought Reasoning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Multimodal reasoning requires iterative coordination between language and\nvision, yet it remains unclear what constitutes a meaningful interleaved chain\nof thought. We posit that text and image thoughts should function as\ncomplementary, rather than isomorphic, modalities that mutually advance\nreasoning. Guided by this principle, we build ThinkMorph, a unified model\nfine-tuned on 24K high-quality interleaved reasoning traces spanning tasks with\nvarying visual engagement. ThinkMorph learns to generate progressive text-image\nreasoning steps that concretely manipulate visual content while maintaining\ncoherent verbal logic. It delivers large gains on vision-centric benchmarks\n(averaging 34.7% over the base model) and generalizes to out-of-domain tasks,\nmatching or surpassing larger and proprietary VLMs. Beyond performance,\nThinkMorph exhibits emergent multimodal intelligence, including unseen visual\nmanipulation skills, adaptive switching between reasoning modes, and better\ntest-time scaling through diversified multimodal thoughts.These findings\nsuggest promising directions for characterizing the emergent capabilities of\nunified models for multimodal reasoning.",
            "upvotes": 56,
            "discussionId": "690813a7812eca10f9cc5e09",
            "projectPage": "https://thinkmorph.github.io/",
            "githubRepo": "https://github.com/ThinkMorph/ThinkMorph",
            "githubStars": 42
        },
        "publishedAt": "2025-10-30T13:51:38.000Z",
        "title": "ThinkMorph: Emergent Properties in Multimodal Interleaved\n  Chain-of-Thought Reasoning",
        "summary": "Multimodal reasoning requires iterative coordination between language and\nvision, yet it remains unclear what constitutes a meaningful interleaved chain\nof thought. We posit that text and image thoughts should function as\ncomplementary, rather than isomorphic, modalities that mutually advance\nreasoning. Guided by this principle, we build ThinkMorph, a unified model\nfine-tuned on 24K high-quality interleaved reasoning traces spanning tasks with\nvarying visual engagement. ThinkMorph learns to generate progressive text-image\nreasoning steps that concretely manipulate visual content while maintaining\ncoherent verbal logic. It delivers large gains on vision-centric benchmarks\n(averaging 34.7% over the base model) and generalizes to out-of-domain tasks,\nmatching or surpassing larger and proprietary VLMs. Beyond performance,\nThinkMorph exhibits emergent multimodal intelligence, including unseen visual\nmanipulation skills, adaptive switching between reasoning modes, and better\ntest-time scaling through diversified multimodal thoughts.These findings\nsuggest promising directions for characterizing the emergent capabilities of\nunified models for multimodal reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27492.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 154
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.25602",
            "authors": [
                {
                    "_id": "69081d74812eca10f9cc5e7a",
                    "name": "Mengzhao Chen",
                    "hidden": false
                },
                {
                    "_id": "69081d74812eca10f9cc5e7b",
                    "user": {
                        "_id": "690821568b3b900d6e63c56d",
                        "avatarUrl": "/avatars/488621d29b93f4ca0a007a068dfc64be.svg",
                        "isPro": false,
                        "fullname": "Meng Wu",
                        "user": "MarlinWu",
                        "type": "user"
                    },
                    "name": "Meng Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:52:16.967Z",
                    "hidden": false
                },
                {
                    "_id": "69081d74812eca10f9cc5e7c",
                    "name": "Hui Jin",
                    "hidden": false
                },
                {
                    "_id": "69081d74812eca10f9cc5e7d",
                    "name": "Zhihang Yuan",
                    "hidden": false
                },
                {
                    "_id": "69081d74812eca10f9cc5e7e",
                    "name": "Jing Liu",
                    "hidden": false
                },
                {
                    "_id": "69081d74812eca10f9cc5e7f",
                    "name": "Chaoyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "69081d74812eca10f9cc5e80",
                    "name": "Yunshui Li",
                    "hidden": false
                },
                {
                    "_id": "69081d74812eca10f9cc5e81",
                    "name": "Jie Huang",
                    "hidden": false
                },
                {
                    "_id": "69081d74812eca10f9cc5e82",
                    "name": "Jin Ma",
                    "hidden": false
                },
                {
                    "_id": "69081d74812eca10f9cc5e83",
                    "name": "Zeyue Xue",
                    "hidden": false
                },
                {
                    "_id": "69081d74812eca10f9cc5e84",
                    "name": "Zhiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "69081d74812eca10f9cc5e85",
                    "name": "Xingyan Bin",
                    "hidden": false
                },
                {
                    "_id": "69081d74812eca10f9cc5e86",
                    "name": "Ping Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-29T15:11:53.000Z",
            "submittedOnDailyAt": "2025-11-03T00:44:27.697Z",
            "title": "INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization\n  Formats",
            "submittedOnDailyBy": {
                "_id": "64aea082704210bf815e7551",
                "avatarUrl": "/avatars/5c8dc0df57596c526b2bccea21835f53.svg",
                "isPro": false,
                "fullname": "Mengzhao Chen",
                "user": "ChenMnZ",
                "type": "user"
            },
            "summary": "Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly\nembracing low-precision floating-point (FP) formats to handle the pervasive\nactivation outliers in Large Language Models (LLMs). Despite this industry\ntrend, a unified comparison of FP and integer (INT) quantization across varying\ngranularities has been missing, leaving algorithm and hardware co-design\nwithout clear guidance. This paper fills that gap by systematically\ninvestigating the trade-offs between FP and INT formats. We reveal a critical\nperformance crossover: while FP excels in coarse-grained quantization, the\ncomparison at fine-grained (block-wise) levels is more nuanced. Our\ncomprehensive comparison demonstrates that for popular 8-bit fine-grained\nformats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart\nin both algorithmic accuracy and hardware efficiency. However, for 4-bit\nformats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we\nshow that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like\nHadamard rotation are applied. We also introduce a symmetric clipping method\nthat resolves gradient bias in fine-grained low-bit INT training, enabling\nnearly lossless performance for MXINT8 training. These findings challenge the\ncurrent hardware trajectory, demonstrating that a one-size-fits-all FP approach\nis suboptimal and advocating that fine-grained INT formats, particularly\nMXINT8, offer a better balance of accuracy, power, and efficiency for future AI\naccelerators.",
            "upvotes": 48,
            "discussionId": "69081d75812eca10f9cc5e87",
            "githubRepo": "https://github.com/ChenMnZ/INT_vs_FP",
            "githubStars": 20,
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-10-29T11:11:53.000Z",
        "title": "INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization\n  Formats",
        "summary": "Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly\nembracing low-precision floating-point (FP) formats to handle the pervasive\nactivation outliers in Large Language Models (LLMs). Despite this industry\ntrend, a unified comparison of FP and integer (INT) quantization across varying\ngranularities has been missing, leaving algorithm and hardware co-design\nwithout clear guidance. This paper fills that gap by systematically\ninvestigating the trade-offs between FP and INT formats. We reveal a critical\nperformance crossover: while FP excels in coarse-grained quantization, the\ncomparison at fine-grained (block-wise) levels is more nuanced. Our\ncomprehensive comparison demonstrates that for popular 8-bit fine-grained\nformats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart\nin both algorithmic accuracy and hardware efficiency. However, for 4-bit\nformats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we\nshow that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like\nHadamard rotation are applied. We also introduce a symmetric clipping method\nthat resolves gradient bias in fine-grained low-bit INT training, enabling\nnearly lossless performance for MXINT8 training. These findings challenge the\ncurrent hardware trajectory, demonstrating that a one-size-fits-all FP approach\nis suboptimal and advocating that fine-grained INT formats, particularly\nMXINT8, offer a better balance of accuracy, power, and efficiency for future AI\naccelerators.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25602.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64aea082704210bf815e7551",
            "avatarUrl": "/avatars/5c8dc0df57596c526b2bccea21835f53.svg",
            "fullname": "Mengzhao Chen",
            "name": "ChenMnZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 23
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.25889",
            "authors": [
                {
                    "_id": "690832af812eca10f9cc5ec0",
                    "name": "Kang Chen",
                    "hidden": false
                },
                {
                    "_id": "690832af812eca10f9cc5ec1",
                    "name": "Zhihao Liu",
                    "hidden": false
                },
                {
                    "_id": "690832af812eca10f9cc5ec2",
                    "name": "Tonghe Zhang",
                    "hidden": false
                },
                {
                    "_id": "690832af812eca10f9cc5ec3",
                    "name": "Zhen Guo",
                    "hidden": false
                },
                {
                    "_id": "690832af812eca10f9cc5ec4",
                    "name": "Si Xu",
                    "hidden": false
                },
                {
                    "_id": "690832af812eca10f9cc5ec5",
                    "name": "Hao Lin",
                    "hidden": false
                },
                {
                    "_id": "690832af812eca10f9cc5ec6",
                    "name": "Hongzhi Zang",
                    "hidden": false
                },
                {
                    "_id": "690832af812eca10f9cc5ec7",
                    "name": "Quanlu Zhang",
                    "hidden": false
                },
                {
                    "_id": "690832af812eca10f9cc5ec8",
                    "name": "Zhaofei Yu",
                    "hidden": false
                },
                {
                    "_id": "690832af812eca10f9cc5ec9",
                    "name": "Guoliang Fan",
                    "hidden": false
                },
                {
                    "_id": "690832af812eca10f9cc5eca",
                    "name": "Tiejun Huang",
                    "hidden": false
                },
                {
                    "_id": "690832af812eca10f9cc5ecb",
                    "name": "Yu Wang",
                    "hidden": false
                },
                {
                    "_id": "690832af812eca10f9cc5ecc",
                    "name": "Chao Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-29T18:37:39.000Z",
            "submittedOnDailyAt": "2025-11-03T02:15:50.813Z",
            "title": "π_RL: Online RL Fine-tuning for Flow-based\n  Vision-Language-Action Models",
            "submittedOnDailyBy": {
                "_id": "64ba0f8d842aa47891cb972b",
                "avatarUrl": "/avatars/3bcacd9b778a146e88e20887b0b00720.svg",
                "isPro": false,
                "fullname": "Chao Yu",
                "user": "zoeyuchao",
                "type": "user"
            },
            "summary": "Vision-Language-Action (VLA) models enable robots to understand and perform\ncomplex tasks from multimodal input. Although recent work explores using\nreinforcement learning (RL) to automate the laborious data collection process\nin scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based\nVLAs (e.g., pi_0, pi_{0.5}) remains challenging due to intractable action\nlog-likelihoods from iterative denoising.\n  We address this challenge with pi_{RL}, an open-source framework\nfor training flow-based VLAs in parallel simulation. pi_{RL}\nimplements two RL algorithms: (1) {Flow-Noise} models the denoising process as\na discrete-time MDP with a learnable noise network for exact log-likelihood\ncomputation. (2) {Flow-SDE} integrates denoising with agent-environment\ninteraction, formulating a two-layer MDP that employs ODE-to-SDE conversion for\nefficient RL exploration.\n  We evaluate pi_{RL} on LIBERO and ManiSkill benchmarks. On LIBERO,\npi_{RL} boosts few-shot SFT models pi_0 and pi_{0.5} from 57.6%\nto 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train\npi_{RL} in 320 parallel environments, improving pi_0 from 41.6% to\n85.7% and pi_{0.5} from 40.0% to 84.8% across 4352 pick-and-place tasks,\ndemonstrating scalable multitask RL under heterogeneous simulation.\n  Overall, pi_{RL} achieves significant performance gains and\nstronger generalization over SFT-models, validating the effectiveness of online\nRL for flow-based VLAs.",
            "upvotes": 43,
            "discussionId": "690832af812eca10f9cc5ecd",
            "projectPage": "https://rlinf.readthedocs.io/en/latest/rst_source/examples/pi0.html",
            "organization": {
                "_id": "689ea978824b212c988bc8f5",
                "name": "RLinf",
                "fullname": "RLinf",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/689ea8a1a73ecc6940dbba3d/T2RGCw18z6lYP1WfkIGJ3.jpeg"
            }
        },
        "publishedAt": "2025-10-29T14:37:39.000Z",
        "title": "π_RL: Online RL Fine-tuning for Flow-based\n  Vision-Language-Action Models",
        "summary": "Vision-Language-Action (VLA) models enable robots to understand and perform\ncomplex tasks from multimodal input. Although recent work explores using\nreinforcement learning (RL) to automate the laborious data collection process\nin scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based\nVLAs (e.g., pi_0, pi_{0.5}) remains challenging due to intractable action\nlog-likelihoods from iterative denoising.\n  We address this challenge with pi_{RL}, an open-source framework\nfor training flow-based VLAs in parallel simulation. pi_{RL}\nimplements two RL algorithms: (1) {Flow-Noise} models the denoising process as\na discrete-time MDP with a learnable noise network for exact log-likelihood\ncomputation. (2) {Flow-SDE} integrates denoising with agent-environment\ninteraction, formulating a two-layer MDP that employs ODE-to-SDE conversion for\nefficient RL exploration.\n  We evaluate pi_{RL} on LIBERO and ManiSkill benchmarks. On LIBERO,\npi_{RL} boosts few-shot SFT models pi_0 and pi_{0.5} from 57.6%\nto 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train\npi_{RL} in 320 parallel environments, improving pi_0 from 41.6% to\n85.7% and pi_{0.5} from 40.0% to 84.8% across 4352 pick-and-place tasks,\ndemonstrating scalable multitask RL under heterogeneous simulation.\n  Overall, pi_{RL} achieves significant performance gains and\nstronger generalization over SFT-models, validating the effectiveness of online\nRL for flow-based VLAs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25889.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64ba0f8d842aa47891cb972b",
            "avatarUrl": "/avatars/3bcacd9b778a146e88e20887b0b00720.svg",
            "fullname": "Chao Yu",
            "name": "zoeyuchao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "689ea978824b212c988bc8f5",
            "name": "RLinf",
            "fullname": "RLinf",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/689ea8a1a73ecc6940dbba3d/T2RGCw18z6lYP1WfkIGJ3.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.27688",
            "authors": [
                {
                    "_id": "69080ebd812eca10f9cc5df3",
                    "name": "Chenze Shao",
                    "hidden": false
                },
                {
                    "_id": "69080ebd812eca10f9cc5df4",
                    "name": "Darren Li",
                    "hidden": false
                },
                {
                    "_id": "69080ebd812eca10f9cc5df5",
                    "name": "Fandong Meng",
                    "hidden": false
                },
                {
                    "_id": "69080ebd812eca10f9cc5df6",
                    "name": "Jie Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-31T17:58:11.000Z",
            "submittedOnDailyAt": "2025-11-03T00:15:11.394Z",
            "title": "Continuous Autoregressive Language Models",
            "submittedOnDailyBy": {
                "_id": "67a42bf8dba32bb665e351ad",
                "avatarUrl": "/avatars/9c13810fe789ddcd9cefd4f2c924e4aa.svg",
                "isPro": false,
                "fullname": "Chenze Shao",
                "user": "cccczshao",
                "type": "user"
            },
            "summary": "The efficiency of large language models (LLMs) is fundamentally limited by\ntheir sequential, token-by-token generation process. We argue that overcoming\nthis bottleneck requires a new design axis for LLM scaling: increasing the\nsemantic bandwidth of each generative step. To this end, we introduce\nContinuous Autoregressive Language Models (CALM), a paradigm shift from\ndiscrete next-token prediction to continuous next-vector prediction. CALM uses\na high-fidelity autoencoder to compress a chunk of K tokens into a single\ncontinuous vector, from which the original tokens can be reconstructed with\nover 99.9\\% accuracy. This allows us to model language as a sequence of\ncontinuous vectors instead of discrete tokens, which reduces the number of\ngenerative steps by a factor of K. The paradigm shift necessitates a new\nmodeling toolkit; therefore, we develop a comprehensive likelihood-free\nframework that enables robust training, evaluation, and controllable sampling\nin the continuous domain. Experiments show that CALM significantly improves the\nperformance-compute trade-off, achieving the performance of strong discrete\nbaselines at a significantly lower computational cost. More importantly, these\nfindings establish next-vector prediction as a powerful and scalable pathway\ntowards ultra-efficient language models. Code:\nhttps://github.com/shaochenze/calm. Project:\nhttps://shaochenze.github.io/blog/2025/CALM.",
            "upvotes": 30,
            "discussionId": "69080ebd812eca10f9cc5df7",
            "projectPage": "https://shaochenze.github.io/blog/2025/CALM/",
            "githubRepo": "https://github.com/shaochenze/calm",
            "githubStars": 24,
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-10-31T13:58:11.000Z",
        "title": "Continuous Autoregressive Language Models",
        "summary": "The efficiency of large language models (LLMs) is fundamentally limited by\ntheir sequential, token-by-token generation process. We argue that overcoming\nthis bottleneck requires a new design axis for LLM scaling: increasing the\nsemantic bandwidth of each generative step. To this end, we introduce\nContinuous Autoregressive Language Models (CALM), a paradigm shift from\ndiscrete next-token prediction to continuous next-vector prediction. CALM uses\na high-fidelity autoencoder to compress a chunk of K tokens into a single\ncontinuous vector, from which the original tokens can be reconstructed with\nover 99.9\\% accuracy. This allows us to model language as a sequence of\ncontinuous vectors instead of discrete tokens, which reduces the number of\ngenerative steps by a factor of K. The paradigm shift necessitates a new\nmodeling toolkit; therefore, we develop a comprehensive likelihood-free\nframework that enables robust training, evaluation, and controllable sampling\nin the continuous domain. Experiments show that CALM significantly improves the\nperformance-compute trade-off, achieving the performance of strong discrete\nbaselines at a significantly lower computational cost. More importantly, these\nfindings establish next-vector prediction as a powerful and scalable pathway\ntowards ultra-efficient language models. Code:\nhttps://github.com/shaochenze/calm. Project:\nhttps://shaochenze.github.io/blog/2025/CALM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27688.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67a42bf8dba32bb665e351ad",
            "avatarUrl": "/avatars/9c13810fe789ddcd9cefd4f2c924e4aa.svg",
            "fullname": "Chenze Shao",
            "name": "cccczshao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.27606",
            "authors": [
                {
                    "_id": "6908284d812eca10f9cc5e92",
                    "name": "Yuhong Liu",
                    "hidden": false
                },
                {
                    "_id": "6908284d812eca10f9cc5e93",
                    "name": "Beichen Zhang",
                    "hidden": false
                },
                {
                    "_id": "6908284d812eca10f9cc5e94",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/kajwuVzd4pDucSPlwghxo.png",
                        "isPro": true,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:52:07.168Z",
                    "hidden": false
                },
                {
                    "_id": "6908284d812eca10f9cc5e95",
                    "name": "Yuhang Cao",
                    "hidden": false
                },
                {
                    "_id": "6908284d812eca10f9cc5e96",
                    "name": "Long Xing",
                    "hidden": false
                },
                {
                    "_id": "6908284d812eca10f9cc5e97",
                    "name": "Xiaoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "6908284d812eca10f9cc5e98",
                    "name": "Haodong Duan",
                    "hidden": false
                },
                {
                    "_id": "6908284d812eca10f9cc5e99",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "6908284d812eca10f9cc5e9a",
                    "name": "Jiaqi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-31T16:30:08.000Z",
            "submittedOnDailyAt": "2025-11-03T01:34:20.749Z",
            "title": "Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "63859cf3b2906edaf83af9f0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/kajwuVzd4pDucSPlwghxo.png",
                "isPro": true,
                "fullname": "Yuhang Zang",
                "user": "yuhangzang",
                "type": "user"
            },
            "summary": "Spatial understanding remains a weakness of Large Vision-Language Models\n(LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement\nlearning with verifiable rewards (RLVR) pipelines depend on costly supervision,\nspecialized tools, or constrained environments that limit scale. We introduce\nSpatial-SSRL, a self-supervised RL paradigm that derives verifiable signals\ndirectly from ordinary RGB or RGB-D images. Spatial-SSRL automatically\nformulates five pretext tasks that capture 2D and 3D spatial structure:\nshuffled patch reordering, flipped patch recognition, cropped patch inpainting,\nregional depth ordering, and relative 3D position prediction. These tasks\nprovide ground-truth answers that are easy to verify and require no human or\nLVLM annotation. Training on our tasks substantially improves spatial reasoning\nwhile preserving general visual capabilities. On seven spatial understanding\nbenchmarks in both image and video settings, Spatial-SSRL delivers average\naccuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our\nresults show that simple, intrinsic supervision enables RLVR at scale and\nprovides a practical route to stronger spatial intelligence in LVLMs.",
            "upvotes": 21,
            "discussionId": "6908284e812eca10f9cc5e9b",
            "githubRepo": "https://github.com/InternLM/Spatial-SSRL",
            "githubStars": 31,
            "organization": {
                "_id": "64a2d5fa81252883206f24c9",
                "name": "internlm",
                "fullname": "Intern Large Models",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"
            }
        },
        "publishedAt": "2025-10-31T12:30:08.000Z",
        "title": "Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised\n  Reinforcement Learning",
        "summary": "Spatial understanding remains a weakness of Large Vision-Language Models\n(LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement\nlearning with verifiable rewards (RLVR) pipelines depend on costly supervision,\nspecialized tools, or constrained environments that limit scale. We introduce\nSpatial-SSRL, a self-supervised RL paradigm that derives verifiable signals\ndirectly from ordinary RGB or RGB-D images. Spatial-SSRL automatically\nformulates five pretext tasks that capture 2D and 3D spatial structure:\nshuffled patch reordering, flipped patch recognition, cropped patch inpainting,\nregional depth ordering, and relative 3D position prediction. These tasks\nprovide ground-truth answers that are easy to verify and require no human or\nLVLM annotation. Training on our tasks substantially improves spatial reasoning\nwhile preserving general visual capabilities. On seven spatial understanding\nbenchmarks in both image and video settings, Spatial-SSRL delivers average\naccuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our\nresults show that simple, intrinsic supervision enables RLVR at scale and\nprovides a practical route to stronger spatial intelligence in LVLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27606.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/kajwuVzd4pDucSPlwghxo.png",
            "fullname": "Yuhang Zang",
            "name": "yuhangzang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "organization": {
            "_id": "64a2d5fa81252883206f24c9",
            "name": "internlm",
            "fullname": "Intern Large Models",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.27266",
            "authors": [
                {
                    "_id": "69081881812eca10f9cc5e4f",
                    "name": "Shaojie Zhang",
                    "hidden": false
                },
                {
                    "_id": "69081881812eca10f9cc5e50",
                    "name": "Pei Fu",
                    "hidden": false
                },
                {
                    "_id": "69081881812eca10f9cc5e51",
                    "name": "Ruoceng Zhang",
                    "hidden": false
                },
                {
                    "_id": "69081881812eca10f9cc5e52",
                    "name": "Jiahui Yang",
                    "hidden": false
                },
                {
                    "_id": "69081881812eca10f9cc5e53",
                    "name": "Anan Du",
                    "hidden": false
                },
                {
                    "_id": "69081881812eca10f9cc5e54",
                    "name": "Xiuwen Xi",
                    "hidden": false
                },
                {
                    "_id": "69081881812eca10f9cc5e55",
                    "name": "Shaokang Wang",
                    "hidden": false
                },
                {
                    "_id": "69081881812eca10f9cc5e56",
                    "name": "Ying Huang",
                    "hidden": false
                },
                {
                    "_id": "69081881812eca10f9cc5e57",
                    "name": "Bin Qin",
                    "hidden": false
                },
                {
                    "_id": "69081881812eca10f9cc5e58",
                    "name": "Zhenbo Luo",
                    "hidden": false
                },
                {
                    "_id": "69081881812eca10f9cc5e59",
                    "name": "Jian Luan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-31T08:07:02.000Z",
            "submittedOnDailyAt": "2025-11-03T00:20:53.322Z",
            "title": "HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Autonomous Graphical User Interface (GUI) agents rely on accurate GUI\ngrounding, which maps language instructions to on-screen coordinates, to\nexecute user commands. However, current models, whether trained via supervised\nfine-tuning (SFT) or reinforcement fine-tuning (RFT), lack self-awareness of\ntheir capability boundaries, leading to overconfidence and unreliable\npredictions. We first systematically evaluate probabilistic and verbalized\nconfidence in general and GUI-specific models, revealing a misalignment between\nconfidence and actual accuracy, which is particularly critical in dynamic GUI\nautomation tasks, where single errors can cause task failure. To address this,\nwe propose HyperClick, a novel framework that enhances reliable GUI grounding\nthrough uncertainty calibration. HyperClick introduces a dual reward mechanism,\ncombining a binary reward for correct actions with a truncated Gaussian-based\nspatial confidence modeling, calibrated using the Brier score. This approach\njointly optimizes grounding accuracy and confidence reliability, fostering\nintrospective self-criticism. Extensive experiments on seven challenge\nbenchmarks show that HyperClick achieves state-of-the-art performance while\nproviding well-calibrated confidence. By enabling explicit confidence\ncalibration and introspective self-criticism, HyperClick reduces overconfidence\nand supports more reliable GUI automation.",
            "upvotes": 15,
            "discussionId": "69081881812eca10f9cc5e5a"
        },
        "publishedAt": "2025-10-31T04:07:02.000Z",
        "title": "HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration",
        "summary": "Autonomous Graphical User Interface (GUI) agents rely on accurate GUI\ngrounding, which maps language instructions to on-screen coordinates, to\nexecute user commands. However, current models, whether trained via supervised\nfine-tuning (SFT) or reinforcement fine-tuning (RFT), lack self-awareness of\ntheir capability boundaries, leading to overconfidence and unreliable\npredictions. We first systematically evaluate probabilistic and verbalized\nconfidence in general and GUI-specific models, revealing a misalignment between\nconfidence and actual accuracy, which is particularly critical in dynamic GUI\nautomation tasks, where single errors can cause task failure. To address this,\nwe propose HyperClick, a novel framework that enhances reliable GUI grounding\nthrough uncertainty calibration. HyperClick introduces a dual reward mechanism,\ncombining a binary reward for correct actions with a truncated Gaussian-based\nspatial confidence modeling, calibrated using the Brier score. This approach\njointly optimizes grounding accuracy and confidence reliability, fostering\nintrospective self-criticism. Extensive experiments on seven challenge\nbenchmarks show that HyperClick achieves state-of-the-art performance while\nproviding well-calibrated confidence. By enabling explicit confidence\ncalibration and introspective self-criticism, HyperClick reduces overconfidence\nand supports more reliable GUI automation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27266.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 154
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.26788",
            "authors": [
                {
                    "_id": "6908202f812eca10f9cc5e89",
                    "user": {
                        "_id": "63885f1d0bebb233d8ad6e5b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Penghui Qi",
                        "user": "QPHutu",
                        "type": "user"
                    },
                    "name": "Penghui Qi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:52:13.465Z",
                    "hidden": false
                },
                {
                    "_id": "6908202f812eca10f9cc5e8a",
                    "name": "Zichen Liu",
                    "hidden": false
                },
                {
                    "_id": "6908202f812eca10f9cc5e8b",
                    "user": {
                        "_id": "66129c7b50350afe76757262",
                        "avatarUrl": "/avatars/a2f4fac076b9d658a0d904ed54960f6f.svg",
                        "isPro": false,
                        "fullname": "Xiangxin Zhou",
                        "user": "zhouxiangxin",
                        "type": "user"
                    },
                    "name": "Xiangxin Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:52:09.833Z",
                    "hidden": false
                },
                {
                    "_id": "6908202f812eca10f9cc5e8c",
                    "name": "Tianyu Pang",
                    "hidden": false
                },
                {
                    "_id": "6908202f812eca10f9cc5e8d",
                    "name": "Chao Du",
                    "hidden": false
                },
                {
                    "_id": "6908202f812eca10f9cc5e8e",
                    "name": "Wee Sun Lee",
                    "hidden": false
                },
                {
                    "_id": "6908202f812eca10f9cc5e8f",
                    "name": "Min Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-30T17:58:11.000Z",
            "submittedOnDailyAt": "2025-11-03T00:55:58.058Z",
            "title": "Defeating the Training-Inference Mismatch via FP16",
            "submittedOnDailyBy": {
                "_id": "63885f1d0bebb233d8ad6e5b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg",
                "isPro": false,
                "fullname": "Penghui Qi",
                "user": "QPHutu",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to FP16 effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.",
            "upvotes": 15,
            "discussionId": "6908202f812eca10f9cc5e90",
            "githubRepo": "https://github.com/sail-sg/Precision-RL",
            "githubStars": 94,
            "organization": {
                "_id": "61f4e841c771e23a1abb61ff",
                "name": "sail",
                "fullname": "Sea AI Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643440185801-5df833bdda6d0311fd3d5403.png"
            }
        },
        "publishedAt": "2025-10-30T13:58:11.000Z",
        "title": "Defeating the Training-Inference Mismatch via FP16",
        "summary": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to FP16 effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26788.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63885f1d0bebb233d8ad6e5b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg",
            "fullname": "Penghui Qi",
            "name": "QPHutu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "61f4e841c771e23a1abb61ff",
            "name": "sail",
            "fullname": "Sea AI Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643440185801-5df833bdda6d0311fd3d5403.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.27684",
            "authors": [
                {
                    "_id": "690817db812eca10f9cc5e44",
                    "name": "Xiangyu Fan",
                    "hidden": false
                },
                {
                    "_id": "690817db812eca10f9cc5e45",
                    "name": "Zesong Qiu",
                    "hidden": false
                },
                {
                    "_id": "690817db812eca10f9cc5e46",
                    "name": "Zhuguanyu Wu",
                    "hidden": false
                },
                {
                    "_id": "690817db812eca10f9cc5e47",
                    "name": "Fanzhou Wang",
                    "hidden": false
                },
                {
                    "_id": "690817db812eca10f9cc5e48",
                    "name": "Zhiqian Lin",
                    "hidden": false
                },
                {
                    "_id": "690817db812eca10f9cc5e49",
                    "name": "Tianxiang Ren",
                    "hidden": false
                },
                {
                    "_id": "690817db812eca10f9cc5e4a",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "690817db812eca10f9cc5e4b",
                    "name": "Ruihao Gong",
                    "hidden": false
                },
                {
                    "_id": "690817db812eca10f9cc5e4c",
                    "name": "Lei Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-31T17:55:10.000Z",
            "submittedOnDailyAt": "2025-11-03T00:26:25.051Z",
            "title": "Phased DMD: Few-step Distribution Matching Distillation via Score\n  Matching within Subintervals",
            "submittedOnDailyBy": {
                "_id": "6626a471430a124253f197c8",
                "avatarUrl": "/avatars/f5747fdbe495d1296fed9d16d8c95857.svg",
                "isPro": false,
                "fullname": "yl-1993",
                "user": "yl-1993",
                "type": "user"
            },
            "summary": "Distribution Matching Distillation (DMD) distills score-based generative\nmodels into efficient one-step generators, without requiring a one-to-one\ncorrespondence with the sampling trajectories of their teachers. However,\nlimited model capacity causes one-step distilled models underperform on complex\ngenerative tasks, e.g., synthesizing intricate object motions in text-to-video\ngeneration. Directly extending DMD to multi-step distillation increases memory\nusage and computational depth, leading to instability and reduced efficiency.\nWhile prior works propose stochastic gradient truncation as a potential\nsolution, we observe that it substantially reduces the generation diversity of\nmulti-step distilled models, bringing it down to the level of their one-step\ncounterparts. To address these limitations, we propose Phased DMD, a multi-step\ndistillation framework that bridges the idea of phase-wise distillation with\nMixture-of-Experts (MoE), reducing learning difficulty while enhancing model\ncapacity. Phased DMD is built upon two key ideas: progressive distribution\nmatching and score matching within subintervals. First, our model divides the\nSNR range into subintervals, progressively refining the model to higher SNR\nlevels, to better capture complex distributions. Next, to ensure the training\nobjective within each subinterval is accurate, we have conducted rigorous\nmathematical derivations. We validate Phased DMD by distilling state-of-the-art\nimage and video generation models, including Qwen-Image (20B parameters) and\nWan2.2 (28B parameters). Experimental results demonstrate that Phased DMD\npreserves output diversity better than DMD while retaining key generative\ncapabilities. We will release our code and models.",
            "upvotes": 14,
            "discussionId": "690817dc812eca10f9cc5e4d",
            "organization": {
                "_id": "64f0405f8a4cf3e5e6b38f9c",
                "name": "sensenova",
                "fullname": "SenseNova"
            }
        },
        "publishedAt": "2025-10-31T13:55:10.000Z",
        "title": "Phased DMD: Few-step Distribution Matching Distillation via Score\n  Matching within Subintervals",
        "summary": "Distribution Matching Distillation (DMD) distills score-based generative\nmodels into efficient one-step generators, without requiring a one-to-one\ncorrespondence with the sampling trajectories of their teachers. However,\nlimited model capacity causes one-step distilled models underperform on complex\ngenerative tasks, e.g., synthesizing intricate object motions in text-to-video\ngeneration. Directly extending DMD to multi-step distillation increases memory\nusage and computational depth, leading to instability and reduced efficiency.\nWhile prior works propose stochastic gradient truncation as a potential\nsolution, we observe that it substantially reduces the generation diversity of\nmulti-step distilled models, bringing it down to the level of their one-step\ncounterparts. To address these limitations, we propose Phased DMD, a multi-step\ndistillation framework that bridges the idea of phase-wise distillation with\nMixture-of-Experts (MoE), reducing learning difficulty while enhancing model\ncapacity. Phased DMD is built upon two key ideas: progressive distribution\nmatching and score matching within subintervals. First, our model divides the\nSNR range into subintervals, progressively refining the model to higher SNR\nlevels, to better capture complex distributions. Next, to ensure the training\nobjective within each subinterval is accurate, we have conducted rigorous\nmathematical derivations. We validate Phased DMD by distilling state-of-the-art\nimage and video generation models, including Qwen-Image (20B parameters) and\nWan2.2 (28B parameters). Experimental results demonstrate that Phased DMD\npreserves output diversity better than DMD while retaining key generative\ncapabilities. We will release our code and models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27684.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6626a471430a124253f197c8",
            "avatarUrl": "/avatars/f5747fdbe495d1296fed9d16d8c95857.svg",
            "fullname": "yl-1993",
            "name": "yl-1993",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "64f0405f8a4cf3e5e6b38f9c",
            "name": "sensenova",
            "fullname": "SenseNova"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.23095",
            "authors": [
                {
                    "_id": "6902d73072739622ee92a856",
                    "user": {
                        "_id": "6686b9f2978a8880573681e9",
                        "avatarUrl": "/avatars/965caeedb1ba847894c6b0e10025986e.svg",
                        "isPro": false,
                        "fullname": "Jie Huang",
                        "user": "JJJYmmm",
                        "type": "user"
                    },
                    "name": "Jie Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-30T14:27:21.611Z",
                    "hidden": false
                },
                {
                    "_id": "6902d73072739622ee92a857",
                    "user": {
                        "_id": "6486c50978ab6bf101afc29f",
                        "avatarUrl": "/avatars/cd101a2c5188b48a1874f20756eb8f51.svg",
                        "isPro": false,
                        "fullname": "XuejingLiu",
                        "user": "GingL",
                        "type": "user"
                    },
                    "name": "Xuejing Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-30T14:27:16.199Z",
                    "hidden": false
                },
                {
                    "_id": "6902d73072739622ee92a858",
                    "name": "Sibo Song",
                    "hidden": false
                },
                {
                    "_id": "6902d73072739622ee92a859",
                    "name": "Ruibing Hou",
                    "hidden": false
                },
                {
                    "_id": "6902d73072739622ee92a85a",
                    "name": "Hong Chang",
                    "hidden": false
                },
                {
                    "_id": "6902d73072739622ee92a85b",
                    "name": "Junyang Lin",
                    "hidden": false
                },
                {
                    "_id": "6902d73072739622ee92a85c",
                    "name": "Shuai Bai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T08:00:46.000Z",
            "submittedOnDailyAt": "2025-11-03T03:28:47.090Z",
            "title": "Revisiting Multimodal Positional Encoding in Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "6486c50978ab6bf101afc29f",
                "avatarUrl": "/avatars/cd101a2c5188b48a1874f20756eb8f51.svg",
                "isPro": false,
                "fullname": "XuejingLiu",
                "user": "GingL",
                "type": "user"
            },
            "summary": "Multimodal position encoding is essential for vision-language models, yet\nthere has been little systematic investigation into multimodal position\nencoding. We conduct a comprehensive analysis of multimodal Rotary Positional\nEmbedding (RoPE) by examining its two core components: position design and\nfrequency allocation. Through extensive experiments, we identify three key\nguidelines: positional coherence, full frequency utilization, and preservation\nof textual priors-ensuring unambiguous layout, rich representation, and\nfaithful transfer from the pre-trained LLM. Based on these insights, we propose\nMulti-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and\nplug-and-play variants that require no architectural changes. Our methods\nconsistently outperform existing approaches across diverse benchmarks, with\nsignificant improvements in both general and fine-grained multimodal\nunderstanding. Code will be avaliable at\nhttps://github.com/JJJYmmm/Multimodal-RoPEs.",
            "upvotes": 8,
            "discussionId": "6902d73072739622ee92a85d",
            "organization": {
                "_id": "64c8b5837fe12ecd0a7e92eb",
                "name": "Qwen",
                "fullname": "Qwen",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
            }
        },
        "publishedAt": "2025-10-27T04:00:46.000Z",
        "title": "Revisiting Multimodal Positional Encoding in Vision-Language Models",
        "summary": "Multimodal position encoding is essential for vision-language models, yet\nthere has been little systematic investigation into multimodal position\nencoding. We conduct a comprehensive analysis of multimodal Rotary Positional\nEmbedding (RoPE) by examining its two core components: position design and\nfrequency allocation. Through extensive experiments, we identify three key\nguidelines: positional coherence, full frequency utilization, and preservation\nof textual priors-ensuring unambiguous layout, rich representation, and\nfaithful transfer from the pre-trained LLM. Based on these insights, we propose\nMulti-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and\nplug-and-play variants that require no architectural changes. Our methods\nconsistently outperform existing approaches across diverse benchmarks, with\nsignificant improvements in both general and fine-grained multimodal\nunderstanding. Code will be avaliable at\nhttps://github.com/JJJYmmm/Multimodal-RoPEs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23095.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6486c50978ab6bf101afc29f",
            "avatarUrl": "/avatars/cd101a2c5188b48a1874f20756eb8f51.svg",
            "fullname": "XuejingLiu",
            "name": "GingL",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "64c8b5837fe12ecd0a7e92eb",
            "name": "Qwen",
            "fullname": "Qwen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.27623",
            "authors": [
                {
                    "_id": "6908174c812eca10f9cc5e38",
                    "user": {
                        "_id": "64f005d8f87b014e81716593",
                        "avatarUrl": "/avatars/cfef589b87bd424e333b2a25063528f8.svg",
                        "isPro": false,
                        "fullname": "QIUSI ZHAN",
                        "user": "qiusizhan",
                        "type": "user"
                    },
                    "name": "Qiusi Zhan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:52:27.106Z",
                    "hidden": false
                },
                {
                    "_id": "6908174c812eca10f9cc5e39",
                    "name": "Hyeonjeong Ha",
                    "hidden": false
                },
                {
                    "_id": "6908174c812eca10f9cc5e3a",
                    "user": {
                        "_id": "64d45451c34a346181b130dd",
                        "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
                        "isPro": false,
                        "fullname": "Rui Yang",
                        "user": "Ray2333",
                        "type": "user"
                    },
                    "name": "Rui Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:52:31.665Z",
                    "hidden": false
                },
                {
                    "_id": "6908174c812eca10f9cc5e3b",
                    "user": {
                        "_id": "64f17c31b4344f592fb2821e",
                        "avatarUrl": "/avatars/11c8edb0967491822277a8a0d3ff3d31.svg",
                        "isPro": false,
                        "fullname": "Sirui Xu",
                        "user": "xusirui",
                        "type": "user"
                    },
                    "name": "Sirui Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:52:24.269Z",
                    "hidden": false
                },
                {
                    "_id": "6908174c812eca10f9cc5e3c",
                    "name": "Hanyang Chen",
                    "hidden": false
                },
                {
                    "_id": "6908174c812eca10f9cc5e3d",
                    "name": "Liang-Yan Gui",
                    "hidden": false
                },
                {
                    "_id": "6908174c812eca10f9cc5e3e",
                    "name": "Yu-Xiong Wang",
                    "hidden": false
                },
                {
                    "_id": "6908174c812eca10f9cc5e3f",
                    "name": "Huan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6908174c812eca10f9cc5e40",
                    "name": "Heng Ji",
                    "hidden": false
                },
                {
                    "_id": "6908174c812eca10f9cc5e41",
                    "name": "Daniel Kang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/sKQfgvZh6v9bLrMzbPsaG.mp4"
            ],
            "publishedAt": "2025-10-31T16:50:49.000Z",
            "submittedOnDailyAt": "2025-11-03T00:15:46.836Z",
            "title": "Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive\n  Trigger Learning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Multimodal large language models (MLLMs) have advanced embodied agents by\nenabling direct perception, reasoning, and planning task-oriented actions from\nvisual inputs. However, such vision driven embodied agents open a new attack\nsurface: visual backdoor attacks, where the agent behaves normally until a\nvisual trigger appears in the scene, then persistently executes an\nattacker-specified multi-step policy. We introduce BEAT, the first framework to\ninject such visual backdoors into MLLM-based embodied agents using objects in\nthe environments as triggers. Unlike textual triggers, object triggers exhibit\nwide variation across viewpoints and lighting, making them difficult to implant\nreliably. BEAT addresses this challenge by (1) constructing a training set that\nspans diverse scenes, tasks, and trigger placements to expose agents to trigger\nvariability, and (2) introducing a two-stage training scheme that first applies\nsupervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning\n(CTL). CTL formulates trigger discrimination as preference learning between\ntrigger-present and trigger-free inputs, explicitly sharpening the decision\nboundaries to ensure precise backdoor activation. Across various embodied agent\nbenchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while\nmaintaining strong benign task performance, and generalizes reliably to\nout-of-distribution trigger placements. Notably, compared to naive SFT, CTL\nboosts backdoor activation accuracy up to 39% under limited backdoor data.\nThese findings expose a critical yet unexplored security risk in MLLM-based\nembodied agents, underscoring the need for robust defenses before real-world\ndeployment.",
            "upvotes": 7,
            "discussionId": "6908174c812eca10f9cc5e42",
            "projectPage": "https://zqs1943.github.io/BEAT/"
        },
        "publishedAt": "2025-10-31T12:50:49.000Z",
        "title": "Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive\n  Trigger Learning",
        "summary": "Multimodal large language models (MLLMs) have advanced embodied agents by\nenabling direct perception, reasoning, and planning task-oriented actions from\nvisual inputs. However, such vision driven embodied agents open a new attack\nsurface: visual backdoor attacks, where the agent behaves normally until a\nvisual trigger appears in the scene, then persistently executes an\nattacker-specified multi-step policy. We introduce BEAT, the first framework to\ninject such visual backdoors into MLLM-based embodied agents using objects in\nthe environments as triggers. Unlike textual triggers, object triggers exhibit\nwide variation across viewpoints and lighting, making them difficult to implant\nreliably. BEAT addresses this challenge by (1) constructing a training set that\nspans diverse scenes, tasks, and trigger placements to expose agents to trigger\nvariability, and (2) introducing a two-stage training scheme that first applies\nsupervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning\n(CTL). CTL formulates trigger discrimination as preference learning between\ntrigger-present and trigger-free inputs, explicitly sharpening the decision\nboundaries to ensure precise backdoor activation. Across various embodied agent\nbenchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while\nmaintaining strong benign task performance, and generalizes reliably to\nout-of-distribution trigger placements. Notably, compared to naive SFT, CTL\nboosts backdoor activation accuracy up to 39% under limited backdoor data.\nThese findings expose a critical yet unexplored security risk in MLLM-based\nembodied agents, underscoring the need for robust defenses before real-world\ndeployment.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/sKQfgvZh6v9bLrMzbPsaG.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27623.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 154
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24940",
            "authors": [
                {
                    "_id": "6908c7b7812eca10f9cc6082",
                    "name": "Yinhan He",
                    "hidden": false
                },
                {
                    "_id": "6908c7b7812eca10f9cc6083",
                    "name": "Wendy Zheng",
                    "hidden": false
                },
                {
                    "_id": "6908c7b7812eca10f9cc6084",
                    "user": {
                        "_id": "647a248bed75e95d3e98e3d6",
                        "avatarUrl": "/avatars/e10e2f8516d1451fd85e17b5a0ba978d.svg",
                        "isPro": false,
                        "fullname": "Yaochen Zhu",
                        "user": "yaochenzhu",
                        "type": "user"
                    },
                    "name": "Yaochen Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:51:08.065Z",
                    "hidden": false
                },
                {
                    "_id": "6908c7b7812eca10f9cc6085",
                    "name": "Zaiyi Zheng",
                    "hidden": false
                },
                {
                    "_id": "6908c7b7812eca10f9cc6086",
                    "name": "Lin Su",
                    "hidden": false
                },
                {
                    "_id": "6908c7b7812eca10f9cc6087",
                    "name": "Sriram Vasudevan",
                    "hidden": false
                },
                {
                    "_id": "6908c7b7812eca10f9cc6088",
                    "name": "Qi Guo",
                    "hidden": false
                },
                {
                    "_id": "6908c7b7812eca10f9cc6089",
                    "name": "Liangjie Hong",
                    "hidden": false
                },
                {
                    "_id": "6908c7b7812eca10f9cc608a",
                    "name": "Jundong Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T20:11:54.000Z",
            "submittedOnDailyAt": "2025-11-03T18:46:41.201Z",
            "title": "SemCoT: Accelerating Chain-of-Thought Reasoning through\n  Semantically-Aligned Implicit Tokens",
            "submittedOnDailyBy": {
                "_id": "647a248bed75e95d3e98e3d6",
                "avatarUrl": "/avatars/e10e2f8516d1451fd85e17b5a0ba978d.svg",
                "isPro": false,
                "fullname": "Yaochen Zhu",
                "user": "yaochenzhu",
                "type": "user"
            },
            "summary": "The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment\nin efficiency-critical applications. Recently, implicit CoT approaches have\nemerged, which encode reasoning steps within LLM's hidden embeddings (termed\n``implicit reasoning'') rather than explicit tokens. This approach accelerates\nCoT by reducing the reasoning length and bypassing some LLM components.\nHowever, existing implicit CoT methods face two significant challenges: (1)\nthey fail to preserve the semantic alignment between the implicit reasoning\n(when transformed to natural language) and the ground-truth reasoning,\nresulting in a significant CoT performance degradation, and (2) they focus on\nreducing the length of the implicit reasoning; however, they neglect the\nconsiderable time cost for an LLM to generate one individual implicit reasoning\ntoken. To tackle these challenges, we propose a novel semantically-aligned\nimplicit CoT framework termed SemCoT. In particular, for the first challenge,\nwe design a contrastively trained sentence transformer that evaluates semantic\nalignment between implicit and explicit reasoning, which is used to enforce\nsemantic preservation during implicit reasoning optimization. To address the\nsecond challenge, we introduce an efficient implicit reasoning generator by\nfinetuning a lightweight language model using knowledge distillation. This\ngenerator is guided by our sentence transformer to distill ground-truth\nreasoning into semantically aligned implicit reasoning, while also optimizing\nfor accuracy. SemCoT is the first approach that enhances CoT efficiency by\njointly optimizing token-level generation speed and preserving semantic\nalignment with ground-truth reasoning. Extensive experiments demonstrate the\nsuperior performance of SemCoT compared to state-of-the-art methods in both\nefficiency and effectiveness. Our code can be found at\nhttps://github.com/YinhanHe123/SemCoT/.",
            "upvotes": 7,
            "discussionId": "6908c7b7812eca10f9cc608b",
            "organization": {
                "_id": "6697e878d8b5b78e6e7485b7",
                "name": "LinkedIn",
                "fullname": "LinkedIn",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6697e834fd52271e0b9ce8d8/VSBDJkmYgk4-LeXgTKThN.png"
            }
        },
        "publishedAt": "2025-10-28T16:11:54.000Z",
        "title": "SemCoT: Accelerating Chain-of-Thought Reasoning through\n  Semantically-Aligned Implicit Tokens",
        "summary": "The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment\nin efficiency-critical applications. Recently, implicit CoT approaches have\nemerged, which encode reasoning steps within LLM's hidden embeddings (termed\n``implicit reasoning'') rather than explicit tokens. This approach accelerates\nCoT by reducing the reasoning length and bypassing some LLM components.\nHowever, existing implicit CoT methods face two significant challenges: (1)\nthey fail to preserve the semantic alignment between the implicit reasoning\n(when transformed to natural language) and the ground-truth reasoning,\nresulting in a significant CoT performance degradation, and (2) they focus on\nreducing the length of the implicit reasoning; however, they neglect the\nconsiderable time cost for an LLM to generate one individual implicit reasoning\ntoken. To tackle these challenges, we propose a novel semantically-aligned\nimplicit CoT framework termed SemCoT. In particular, for the first challenge,\nwe design a contrastively trained sentence transformer that evaluates semantic\nalignment between implicit and explicit reasoning, which is used to enforce\nsemantic preservation during implicit reasoning optimization. To address the\nsecond challenge, we introduce an efficient implicit reasoning generator by\nfinetuning a lightweight language model using knowledge distillation. This\ngenerator is guided by our sentence transformer to distill ground-truth\nreasoning into semantically aligned implicit reasoning, while also optimizing\nfor accuracy. SemCoT is the first approach that enhances CoT efficiency by\njointly optimizing token-level generation speed and preserving semantic\nalignment with ground-truth reasoning. Extensive experiments demonstrate the\nsuperior performance of SemCoT compared to state-of-the-art methods in both\nefficiency and effectiveness. Our code can be found at\nhttps://github.com/YinhanHe123/SemCoT/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24940.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "647a248bed75e95d3e98e3d6",
            "avatarUrl": "/avatars/e10e2f8516d1451fd85e17b5a0ba978d.svg",
            "fullname": "Yaochen Zhu",
            "name": "yaochenzhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6697e878d8b5b78e6e7485b7",
            "name": "LinkedIn",
            "fullname": "LinkedIn",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6697e834fd52271e0b9ce8d8/VSBDJkmYgk4-LeXgTKThN.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.27258",
            "authors": [
                {
                    "_id": "69081a51812eca10f9cc5e5c",
                    "user": {
                        "_id": "647bf082aba7062fe5c51ca9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
                        "isPro": false,
                        "fullname": "Yifan Zhang",
                        "user": "yifAI",
                        "type": "user"
                    },
                    "name": "Yifan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:52:21.886Z",
                    "hidden": false
                },
                {
                    "_id": "69081a51812eca10f9cc5e5d",
                    "name": "Zhen Qin",
                    "hidden": false
                },
                {
                    "_id": "69081a51812eca10f9cc5e5e",
                    "name": "Quanquan Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-31T07:54:37.000Z",
            "submittedOnDailyAt": "2025-11-03T00:36:34.235Z",
            "title": "Higher-order Linear Attention",
            "submittedOnDailyBy": {
                "_id": "647bf082aba7062fe5c51ca9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
                "isPro": false,
                "fullname": "Yifan Zhang",
                "user": "yifAI",
                "type": "user"
            },
            "summary": "The quadratic cost of scaled dot-product attention is a central obstacle to\nscaling autoregressive language models to long contexts. Linear-time attention\nand State Space Models (SSMs) provide scalable alternatives but are typically\nrestricted to first-order or kernel-based approximations, which can limit\nexpressivity. We introduce Higher-order Linear Attention (HLA), a causal,\nstreaming mechanism that realizes higher interactions via compact prefix\nsufficient statistics. In the second-order case, HLA maintains a constant-size\nstate and computes per-token outputs in linear time without materializing any\nn times n matrices. We give closed-form streaming identities, a strictly\ncausal masked variant using two additional summaries, and a chunk-parallel\ntraining scheme based on associative scans that reproduces the activations of a\nserial recurrence exactly. We further outline extensions to third and higher\norders. Collectively, these results position HLA as a principled, scalable\nbuilding block that combines attention-like, data-dependent mixing with the\nefficiency of modern recurrent architectures. Project Page:\nhttps://github.com/yifanzhang-pro/HLA.",
            "upvotes": 6,
            "discussionId": "69081a51812eca10f9cc5e5f",
            "projectPage": "https://yifanzhang-pro.github.io/HLA",
            "githubRepo": "https://github.com/yifanzhang-pro/HLA",
            "githubStars": 30,
            "organization": {
                "_id": "6779cfba64085a93fd039c0c",
                "name": "model-architectures",
                "fullname": "Model Architectures"
            }
        },
        "publishedAt": "2025-10-31T03:54:37.000Z",
        "title": "Higher-order Linear Attention",
        "summary": "The quadratic cost of scaled dot-product attention is a central obstacle to\nscaling autoregressive language models to long contexts. Linear-time attention\nand State Space Models (SSMs) provide scalable alternatives but are typically\nrestricted to first-order or kernel-based approximations, which can limit\nexpressivity. We introduce Higher-order Linear Attention (HLA), a causal,\nstreaming mechanism that realizes higher interactions via compact prefix\nsufficient statistics. In the second-order case, HLA maintains a constant-size\nstate and computes per-token outputs in linear time without materializing any\nn times n matrices. We give closed-form streaming identities, a strictly\ncausal masked variant using two additional summaries, and a chunk-parallel\ntraining scheme based on associative scans that reproduces the activations of a\nserial recurrence exactly. We further outline extensions to third and higher\norders. Collectively, these results position HLA as a principled, scalable\nbuilding block that combines attention-like, data-dependent mixing with the\nefficiency of modern recurrent architectures. Project Page:\nhttps://github.com/yifanzhang-pro/HLA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27258.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "647bf082aba7062fe5c51ca9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
            "fullname": "Yifan Zhang",
            "name": "yifAI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 16
        },
        "organization": {
            "_id": "6779cfba64085a93fd039c0c",
            "name": "model-architectures",
            "fullname": "Model Architectures"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.27607",
            "authors": [
                {
                    "_id": "690815d6812eca10f9cc5e0b",
                    "user": {
                        "_id": "660a44cc8b022f13fd706d7a",
                        "avatarUrl": "/avatars/b01d9aed87b077e4c650944b4180cb12.svg",
                        "isPro": true,
                        "fullname": "John Won",
                        "user": "periphanes",
                        "type": "user"
                    },
                    "name": "John Won",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:52:34.710Z",
                    "hidden": false
                },
                {
                    "_id": "690815d6812eca10f9cc5e0c",
                    "name": "Kyungmin Lee",
                    "hidden": false
                },
                {
                    "_id": "690815d6812eca10f9cc5e0d",
                    "name": "Huiwon Jang",
                    "hidden": false
                },
                {
                    "_id": "690815d6812eca10f9cc5e0e",
                    "name": "Dongyoung Kim",
                    "hidden": false
                },
                {
                    "_id": "690815d6812eca10f9cc5e0f",
                    "name": "Jinwoo Shin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-31T16:32:12.000Z",
            "submittedOnDailyAt": "2025-11-03T00:09:23.869Z",
            "title": "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action\n  Model",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recently, augmenting Vision-Language-Action models (VLAs) with world modeling\nhas shown promise in improving robotic policy learning. However, it remains\nchallenging to jointly predict next-state observations and action sequences\nbecause of the inherent difference between the two modalities. To address this,\nwe propose DUal-STream diffusion (DUST), a world-model augmented VLA framework\nthat handles the modality conflict and enhances the performance of VLAs across\ndiverse tasks. Specifically, we propose a multimodal diffusion transformer\narchitecture that explicitly maintains separate modality streams while still\nenabling cross-modal knowledge sharing. In addition, we introduce independent\nnoise perturbations for each modality and a decoupled flow-matching loss. This\ndesign enables the model to learn the joint distribution in a bidirectional\nmanner while avoiding the need for a unified latent space. Based on the\ndecoupling of modalities during training, we also introduce a joint sampling\nmethod that supports test-time scaling, where action and vision tokens evolve\nasynchronously at different rates. Through experiments on simulated benchmarks\nsuch as RoboCasa and GR-1, DUST achieves up to 6% gains over baseline methods,\nwhile our test-time scaling approach provides an additional 2-5% boost. On\nreal-world tasks with the Franka Research 3, DUST improves success rates by\n13%, confirming its effectiveness beyond simulation. Furthermore, pre-training\non action-free videos from BridgeV2 yields significant transfer gains on\nRoboCasa, underscoring DUST's potential for large-scale VLA pretraining.",
            "upvotes": 5,
            "discussionId": "690815d6812eca10f9cc5e10"
        },
        "publishedAt": "2025-10-31T12:32:12.000Z",
        "title": "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action\n  Model",
        "summary": "Recently, augmenting Vision-Language-Action models (VLAs) with world modeling\nhas shown promise in improving robotic policy learning. However, it remains\nchallenging to jointly predict next-state observations and action sequences\nbecause of the inherent difference between the two modalities. To address this,\nwe propose DUal-STream diffusion (DUST), a world-model augmented VLA framework\nthat handles the modality conflict and enhances the performance of VLAs across\ndiverse tasks. Specifically, we propose a multimodal diffusion transformer\narchitecture that explicitly maintains separate modality streams while still\nenabling cross-modal knowledge sharing. In addition, we introduce independent\nnoise perturbations for each modality and a decoupled flow-matching loss. This\ndesign enables the model to learn the joint distribution in a bidirectional\nmanner while avoiding the need for a unified latent space. Based on the\ndecoupling of modalities during training, we also introduce a joint sampling\nmethod that supports test-time scaling, where action and vision tokens evolve\nasynchronously at different rates. Through experiments on simulated benchmarks\nsuch as RoboCasa and GR-1, DUST achieves up to 6% gains over baseline methods,\nwhile our test-time scaling approach provides an additional 2-5% boost. On\nreal-world tasks with the Franka Research 3, DUST improves success rates by\n13%, confirming its effectiveness beyond simulation. Furthermore, pre-training\non action-free videos from BridgeV2 yields significant transfer gains on\nRoboCasa, underscoring DUST's potential for large-scale VLA pretraining.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27607.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 154
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.26887",
            "authors": [
                {
                    "_id": "6908169f812eca10f9cc5e12",
                    "name": "Francisco Villaescusa-Navarro",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e13",
                    "name": "Boris Bolliet",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e14",
                    "name": "Pablo Villanueva-Domingo",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e15",
                    "name": "Adrian E. Bayer",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e16",
                    "name": "Aidan Acquah",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e17",
                    "name": "Chetana Amancharla",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e18",
                    "name": "Almog Barzilay-Siegal",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e19",
                    "name": "Pablo Bermejo",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e1a",
                    "name": "Camille Bilodeau",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e1b",
                    "name": "Pablo Cárdenas Ramírez",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e1c",
                    "name": "Miles Cranmer",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e1d",
                    "name": "Urbano L. França",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e1e",
                    "name": "ChangHoon Hahn",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e1f",
                    "name": "Yan-Fei Jiang",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e20",
                    "name": "Raul Jimenez",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e21",
                    "name": "Jun-Young Lee",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e22",
                    "name": "Antonio Lerario",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e23",
                    "name": "Osman Mamun",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e24",
                    "name": "Thomas Meier",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e25",
                    "name": "Anupam A. Ojha",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e26",
                    "name": "Pavlos Protopapas",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e27",
                    "name": "Shimanto Roy",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e28",
                    "name": "David N. Spergel",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e29",
                    "name": "Pedro Tarancón-Álvarez",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e2a",
                    "name": "Ujjwal Tiwari",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e2b",
                    "name": "Matteo Viel",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e2c",
                    "name": "Digvijay Wadekar",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e2d",
                    "name": "Chi Wang",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e2e",
                    "name": "Bonny Y. Wang",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e2f",
                    "name": "Licong Xu",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e30",
                    "name": "Yossi Yovel",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e31",
                    "name": "Shuwen Yue",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e32",
                    "name": "Wen-Han Zhou",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e33",
                    "name": "Qiyao Zhu",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e34",
                    "name": "Jiajun Zou",
                    "hidden": false
                },
                {
                    "_id": "6908169f812eca10f9cc5e35",
                    "name": "Íñigo Zubeldia",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-30T18:00:12.000Z",
            "submittedOnDailyAt": "2025-11-03T00:12:53.238Z",
            "title": "The Denario project: Deep knowledge AI agents for scientific discovery",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present Denario, an AI multi-agent system designed to serve as a\nscientific research assistant. Denario can perform many different tasks, such\nas generating ideas, checking the literature, developing research plans,\nwriting and executing code, making plots, and drafting and reviewing a\nscientific paper. The system has a modular architecture, allowing it to handle\nspecific tasks, such as generating an idea, or carrying out end-to-end\nscientific analysis using Cmbagent as a deep-research backend. In this work, we\ndescribe in detail Denario and its modules, and illustrate its capabilities by\npresenting multiple AI-generated papers generated by it in many different\nscientific disciplines such as astrophysics, biology, biophysics, biomedical\ninformatics, chemistry, material science, mathematical physics, medicine,\nneuroscience and planetary science. Denario also excels at combining ideas from\ndifferent disciplines, and we illustrate this by showing a paper that applies\nmethods from quantum physics and machine learning to astrophysical data. We\nreport the evaluations performed on these papers by domain experts, who\nprovided both numerical scores and review-like feedback. We then highlight the\nstrengths, weaknesses, and limitations of the current system. Finally, we\ndiscuss the ethical implications of AI-driven research and reflect on how such\ntechnology relates to the philosophy of science. We publicly release the code\nat https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run\ndirectly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and\nthe full app will be deployed on the cloud.",
            "upvotes": 3,
            "discussionId": "690816a0812eca10f9cc5e36",
            "githubRepo": "https://github.com/AstroPilot-AI/Denario",
            "githubStars": 77
        },
        "publishedAt": "2025-10-30T14:00:12.000Z",
        "title": "The Denario project: Deep knowledge AI agents for scientific discovery",
        "summary": "We present Denario, an AI multi-agent system designed to serve as a\nscientific research assistant. Denario can perform many different tasks, such\nas generating ideas, checking the literature, developing research plans,\nwriting and executing code, making plots, and drafting and reviewing a\nscientific paper. The system has a modular architecture, allowing it to handle\nspecific tasks, such as generating an idea, or carrying out end-to-end\nscientific analysis using Cmbagent as a deep-research backend. In this work, we\ndescribe in detail Denario and its modules, and illustrate its capabilities by\npresenting multiple AI-generated papers generated by it in many different\nscientific disciplines such as astrophysics, biology, biophysics, biomedical\ninformatics, chemistry, material science, mathematical physics, medicine,\nneuroscience and planetary science. Denario also excels at combining ideas from\ndifferent disciplines, and we illustrate this by showing a paper that applies\nmethods from quantum physics and machine learning to astrophysical data. We\nreport the evaluations performed on these papers by domain experts, who\nprovided both numerical scores and review-like feedback. We then highlight the\nstrengths, weaknesses, and limitations of the current system. Finally, we\ndiscuss the ethical implications of AI-driven research and reflect on how such\ntechnology relates to the philosophy of science. We publicly release the code\nat https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run\ndirectly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and\nthe full app will be deployed on the cloud.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26887.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 154
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24795",
            "authors": [
                {
                    "_id": "6908bb5b812eca10f9cc6066",
                    "name": "Zhaoshu Yu",
                    "hidden": false
                },
                {
                    "_id": "6908bb5b812eca10f9cc6067",
                    "name": "Bo Wang",
                    "hidden": false
                },
                {
                    "_id": "6908bb5b812eca10f9cc6068",
                    "name": "Pengpeng Zeng",
                    "hidden": false
                },
                {
                    "_id": "6908bb5b812eca10f9cc6069",
                    "name": "Haonan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6908bb5b812eca10f9cc606a",
                    "name": "Ji Zhang",
                    "hidden": false
                },
                {
                    "_id": "6908bb5b812eca10f9cc606b",
                    "name": "Lianli Gao",
                    "hidden": false
                },
                {
                    "_id": "6908bb5b812eca10f9cc606c",
                    "name": "Jingkuan Song",
                    "hidden": false
                },
                {
                    "_id": "6908bb5b812eca10f9cc606d",
                    "name": "Nicu Sebe",
                    "hidden": false
                },
                {
                    "_id": "6908bb5b812eca10f9cc606e",
                    "name": "Heng Tao Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T17:57:33.000Z",
            "submittedOnDailyAt": "2025-11-03T11:56:33.428Z",
            "title": "A Survey on Efficient Vision-Language-Action Models",
            "submittedOnDailyBy": {
                "_id": "623437bd4f78e3acb8bd14bd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623437bd4f78e3acb8bd14bd/6ZIRpE9We4TiMtzDBtPFS.jpeg",
                "isPro": false,
                "fullname": "Haonan Zhang",
                "user": "haonanzhang",
                "type": "user"
            },
            "summary": "Vision-Language-Action models (VLAs) represent a significant frontier in\nembodied intelligence, aiming to bridge digital knowledge with physical-world\ninteraction. While these models have demonstrated remarkable generalist\ncapabilities, their deployment is severely hampered by the substantial\ncomputational and data requirements inherent to their underlying large-scale\nfoundation models. Motivated by the urgent need to address these challenges,\nthis survey presents the first comprehensive review of Efficient\nVision-Language-Action models (Efficient VLAs) across the entire\ndata-model-training process. Specifically, we introduce a unified taxonomy to\nsystematically organize the disparate efforts in this domain, categorizing\ncurrent techniques into three core pillars: (1) Efficient Model Design,\nfocusing on efficient architectures and model compression; (2) Efficient\nTraining, which reduces computational burdens during model learning; and (3)\nEfficient Data Collection, which addresses the bottlenecks in acquiring and\nutilizing robotic data. Through a critical review of state-of-the-art methods\nwithin this framework, this survey not only establishes a foundational\nreference for the community but also summarizes representative applications,\ndelineates key challenges, and charts a roadmap for future research. We\nmaintain a continuously updated project page to track our latest developments:\nhttps://evla-survey.github.io/",
            "upvotes": 3,
            "discussionId": "6908bb5b812eca10f9cc606f",
            "projectPage": "https://evla-survey.github.io/",
            "githubRepo": "https://github.com/YuZhaoshu/Efficient-VLAs-Survey",
            "githubStars": 56,
            "organization": {
                "_id": "645b8c6d9522edfc8e4f1e1f",
                "name": "Tongji",
                "fullname": "Tongji Unversity"
            }
        },
        "publishedAt": "2025-10-27T13:57:33.000Z",
        "title": "A Survey on Efficient Vision-Language-Action Models",
        "summary": "Vision-Language-Action models (VLAs) represent a significant frontier in\nembodied intelligence, aiming to bridge digital knowledge with physical-world\ninteraction. While these models have demonstrated remarkable generalist\ncapabilities, their deployment is severely hampered by the substantial\ncomputational and data requirements inherent to their underlying large-scale\nfoundation models. Motivated by the urgent need to address these challenges,\nthis survey presents the first comprehensive review of Efficient\nVision-Language-Action models (Efficient VLAs) across the entire\ndata-model-training process. Specifically, we introduce a unified taxonomy to\nsystematically organize the disparate efforts in this domain, categorizing\ncurrent techniques into three core pillars: (1) Efficient Model Design,\nfocusing on efficient architectures and model compression; (2) Efficient\nTraining, which reduces computational burdens during model learning; and (3)\nEfficient Data Collection, which addresses the bottlenecks in acquiring and\nutilizing robotic data. Through a critical review of state-of-the-art methods\nwithin this framework, this survey not only establishes a foundational\nreference for the community but also summarizes representative applications,\ndelineates key challenges, and charts a roadmap for future research. We\nmaintain a continuously updated project page to track our latest developments:\nhttps://evla-survey.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24795.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "623437bd4f78e3acb8bd14bd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623437bd4f78e3acb8bd14bd/6ZIRpE9We4TiMtzDBtPFS.jpeg",
            "fullname": "Haonan Zhang",
            "name": "haonanzhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "645b8c6d9522edfc8e4f1e1f",
            "name": "Tongji",
            "fullname": "Tongji Unversity"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.27044",
            "authors": [
                {
                    "_id": "69087c52812eca10f9cc5f7e",
                    "name": "Md Tanvirul Alam",
                    "hidden": false
                },
                {
                    "_id": "69087c52812eca10f9cc5f7f",
                    "name": "Nidhi Rastogi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-30T23:16:02.000Z",
            "submittedOnDailyAt": "2025-11-03T07:28:49.081Z",
            "title": "Limits of Generalization in RLVR: Two Case Studies in Mathematical\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "620c1a977af55d45f5519914",
                "avatarUrl": "/avatars/4d2fb7c1a5ad5dabdb8888fa2fe72e65.svg",
                "isPro": true,
                "fullname": "Tanvirul Alam",
                "user": "Tanvirul",
                "type": "user"
            },
            "summary": "Mathematical reasoning is a central challenge for large language models\n(LLMs), requiring not only correct answers but also faithful reasoning\nprocesses. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as\na promising approach for enhancing such capabilities; however, its ability to\nfoster genuine reasoning remains unclear. We investigate RLVR on two\ncombinatorial problems with fully verifiable solutions: Activity\nScheduling and the Longest Increasing Subsequence, using carefully\ncurated datasets with unique optima. Across multiple reward designs, we find\nthat RLVR improves evaluation metrics but often by reinforcing superficial\nheuristics rather than acquiring new reasoning strategies. These findings\nhighlight the limits of RLVR generalization, emphasizing the importance of\nbenchmarks that disentangle genuine mathematical reasoning from shortcut\nexploitation and provide faithful measures of progress. Code available at\nhttps://github.com/xashru/rlvr-seq-generalization.",
            "upvotes": 1,
            "discussionId": "69087c52812eca10f9cc5f80",
            "githubRepo": "https://github.com/xashru/rlvr-seq-generalization",
            "githubStars": 0
        },
        "publishedAt": "2025-10-30T19:16:02.000Z",
        "title": "Limits of Generalization in RLVR: Two Case Studies in Mathematical\n  Reasoning",
        "summary": "Mathematical reasoning is a central challenge for large language models\n(LLMs), requiring not only correct answers but also faithful reasoning\nprocesses. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as\na promising approach for enhancing such capabilities; however, its ability to\nfoster genuine reasoning remains unclear. We investigate RLVR on two\ncombinatorial problems with fully verifiable solutions: Activity\nScheduling and the Longest Increasing Subsequence, using carefully\ncurated datasets with unique optima. Across multiple reward designs, we find\nthat RLVR improves evaluation metrics but often by reinforcing superficial\nheuristics rather than acquiring new reasoning strategies. These findings\nhighlight the limits of RLVR generalization, emphasizing the importance of\nbenchmarks that disentangle genuine mathematical reasoning from shortcut\nexploitation and provide faithful measures of progress. Code available at\nhttps://github.com/xashru/rlvr-seq-generalization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27044.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "620c1a977af55d45f5519914",
            "avatarUrl": "/avatars/4d2fb7c1a5ad5dabdb8888fa2fe72e65.svg",
            "fullname": "Tanvirul Alam",
            "name": "Tanvirul",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.26707",
            "authors": [
                {
                    "_id": "69090797812eca10f9cc60f5",
                    "name": "Mehar Bhatia",
                    "hidden": false
                },
                {
                    "_id": "69090797812eca10f9cc60f6",
                    "name": "Shravan Nayak",
                    "hidden": false
                },
                {
                    "_id": "69090797812eca10f9cc60f7",
                    "name": "Gaurav Kamath",
                    "hidden": false
                },
                {
                    "_id": "69090797812eca10f9cc60f8",
                    "name": "Marius Mosbach",
                    "hidden": false
                },
                {
                    "_id": "69090797812eca10f9cc60f9",
                    "name": "Karolina Stańczak",
                    "hidden": false
                },
                {
                    "_id": "69090797812eca10f9cc60fa",
                    "name": "Vered Shwartz",
                    "hidden": false
                },
                {
                    "_id": "69090797812eca10f9cc60fb",
                    "name": "Siva Reddy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-30T17:09:09.000Z",
            "submittedOnDailyAt": "2025-11-03T17:24:02.090Z",
            "title": "Value Drifts: Tracing Value Alignment During LLM Post-Training",
            "submittedOnDailyBy": {
                "_id": "63f0546df1a47aaea5bcbae1",
                "avatarUrl": "/avatars/9d708d88574cea2f17af37b659ef6a53.svg",
                "isPro": false,
                "fullname": "Mehar Bhatia",
                "user": "MeharBhatia",
                "type": "user"
            },
            "summary": "As LLMs occupy an increasingly important role in society, they are more and\nmore confronted with questions that require them not only to draw on their\ngeneral knowledge but also to align with certain human value systems.\nTherefore, studying the alignment of LLMs with human values has become a\ncrucial field of inquiry. Prior work, however, mostly focuses on evaluating the\nalignment of fully trained models, overlooking the training dynamics by which\nmodels learn to express human values. In this work, we investigate how and at\nwhich stage value alignment arises during the course of a model's\npost-training. Our analysis disentangles the effects of post-training\nalgorithms and datasets, measuring both the magnitude and time of value drifts\nduring training. Experimenting with Llama-3 and Qwen-3 models of different\nsizes and popular supervised fine-tuning (SFT) and preference optimization\ndatasets and algorithms, we find that the SFT phase generally establishes a\nmodel's values, and subsequent preference optimization rarely re-aligns these\nvalues. Furthermore, using a synthetic preference dataset that enables\ncontrolled manipulation of values, we find that different preference\noptimization algorithms lead to different value alignment outcomes, even when\npreference data is held constant. Our findings provide actionable insights into\nhow values are learned during post-training and help to inform data curation,\nas well as the selection of models and algorithms for preference optimization\nto improve model alignment to human values.",
            "upvotes": 1,
            "discussionId": "69090797812eca10f9cc60fc",
            "organization": {
                "_id": "618cd1bfb8de35a67a79d266",
                "name": "McGill-NLP",
                "fullname": "McGill NLP Group",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1651301909677-5fa9ff3ea13e063b8b2b60cb.png"
            }
        },
        "publishedAt": "2025-10-30T13:09:09.000Z",
        "title": "Value Drifts: Tracing Value Alignment During LLM Post-Training",
        "summary": "As LLMs occupy an increasingly important role in society, they are more and\nmore confronted with questions that require them not only to draw on their\ngeneral knowledge but also to align with certain human value systems.\nTherefore, studying the alignment of LLMs with human values has become a\ncrucial field of inquiry. Prior work, however, mostly focuses on evaluating the\nalignment of fully trained models, overlooking the training dynamics by which\nmodels learn to express human values. In this work, we investigate how and at\nwhich stage value alignment arises during the course of a model's\npost-training. Our analysis disentangles the effects of post-training\nalgorithms and datasets, measuring both the magnitude and time of value drifts\nduring training. Experimenting with Llama-3 and Qwen-3 models of different\nsizes and popular supervised fine-tuning (SFT) and preference optimization\ndatasets and algorithms, we find that the SFT phase generally establishes a\nmodel's values, and subsequent preference optimization rarely re-aligns these\nvalues. Furthermore, using a synthetic preference dataset that enables\ncontrolled manipulation of values, we find that different preference\noptimization algorithms lead to different value alignment outcomes, even when\npreference data is held constant. Our findings provide actionable insights into\nhow values are learned during post-training and help to inform data curation,\nas well as the selection of models and algorithms for preference optimization\nto improve model alignment to human values.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26707.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63f0546df1a47aaea5bcbae1",
            "avatarUrl": "/avatars/9d708d88574cea2f17af37b659ef6a53.svg",
            "fullname": "Mehar Bhatia",
            "name": "MeharBhatia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "618cd1bfb8de35a67a79d266",
            "name": "McGill-NLP",
            "fullname": "McGill NLP Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1651301909677-5fa9ff3ea13e063b8b2b60cb.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.20150",
            "authors": [
                {
                    "_id": "68fad999f158a71c5a2f57fc",
                    "user": {
                        "_id": "647a248bed75e95d3e98e3d6",
                        "avatarUrl": "/avatars/e10e2f8516d1451fd85e17b5a0ba978d.svg",
                        "isPro": false,
                        "fullname": "Yaochen Zhu",
                        "user": "yaochenzhu",
                        "type": "user"
                    },
                    "name": "Yaochen Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:57:59.417Z",
                    "hidden": false
                },
                {
                    "_id": "68fad999f158a71c5a2f57fd",
                    "name": "Harald Steck",
                    "hidden": false
                },
                {
                    "_id": "68fad999f158a71c5a2f57fe",
                    "name": "Dawen Liang",
                    "hidden": false
                },
                {
                    "_id": "68fad999f158a71c5a2f57ff",
                    "name": "Yinhan He",
                    "hidden": false
                },
                {
                    "_id": "68fad999f158a71c5a2f5800",
                    "name": "Jundong Li",
                    "hidden": false
                },
                {
                    "_id": "68fad999f158a71c5a2f5801",
                    "name": "Nathan Kallus",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-23T02:56:00.000Z",
            "submittedOnDailyAt": "2025-11-03T18:44:25.509Z",
            "title": "Rank-GRPO: Training LLM-based Conversational Recommender Systems with\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "647a248bed75e95d3e98e3d6",
                "avatarUrl": "/avatars/e10e2f8516d1451fd85e17b5a0ba978d.svg",
                "isPro": false,
                "fullname": "Yaochen Zhu",
                "user": "yaochenzhu",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are reshaping the recommender system paradigm by\nenabling users to express preferences and receive recommendations through\nconversations. Yet, aligning LLMs to the recommendation task remains\nchallenging: pretrained LLMs often generate out-of-catalog items, violate\nrequired output formats, and their ranking quality degrades sharply toward the\nend of the generated list. To this end, we propose ConvRec-R1, a two-stage\nframework for end-to-end training of LLM-based conversational recommender\nsystems. In Stage 1, we construct a behavioral-cloning dataset with a\nRemap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded\ndemonstrations from powerful blackbox LLMs to warm-start the RL training. In\nStage 2, we propose Rank-GRPO, a principled extension of group relative policy\noptimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats\neach rank in the recommendation list as the unit instead of token (too\nfine-grained) or sequence (too coarse), redefining rewards to remove non-causal\ncredit assignment and introducing a rank-level importance ratio based on the\ngeometric mean of rank-wise token probabilities to stabilize policy updates.\nExperiments on the public Reddit-v2 dataset show that ConvRec-R1 converges\nfaster and achieves higher Recall and NDCG than GRPO-style baselines. Code and\ndatasets are released at https://github.com/yaochenzhu/Rank-GRPO.",
            "upvotes": 1,
            "discussionId": "68fad999f158a71c5a2f5802",
            "ai_summary": "ConvRec-R1, a two-stage framework, enhances LLM-based conversational recommender systems by using behavioral cloning and Rank-GRPO to improve recommendation quality and convergence.",
            "ai_keywords": [
                "LLMs",
                "recommender system",
                "behavioral-cloning",
                "Remap-Reflect-Adjust",
                "Rank-GRPO",
                "group relative policy optimization",
                "GRPO",
                "rank-level importance ratio",
                "Reddit-v2 dataset",
                "Recall",
                "NDCG"
            ],
            "organization": {
                "_id": "687592798586825b86976d6d",
                "name": "netflix",
                "fullname": "Netflix",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68d2e3609537cbef66612ae8/MJuH26GOdHAZYObE9QOYA.png"
            }
        },
        "publishedAt": "2025-10-22T22:56:00.000Z",
        "title": "Rank-GRPO: Training LLM-based Conversational Recommender Systems with\n  Reinforcement Learning",
        "summary": "Large language models (LLMs) are reshaping the recommender system paradigm by\nenabling users to express preferences and receive recommendations through\nconversations. Yet, aligning LLMs to the recommendation task remains\nchallenging: pretrained LLMs often generate out-of-catalog items, violate\nrequired output formats, and their ranking quality degrades sharply toward the\nend of the generated list. To this end, we propose ConvRec-R1, a two-stage\nframework for end-to-end training of LLM-based conversational recommender\nsystems. In Stage 1, we construct a behavioral-cloning dataset with a\nRemap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded\ndemonstrations from powerful blackbox LLMs to warm-start the RL training. In\nStage 2, we propose Rank-GRPO, a principled extension of group relative policy\noptimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats\neach rank in the recommendation list as the unit instead of token (too\nfine-grained) or sequence (too coarse), redefining rewards to remove non-causal\ncredit assignment and introducing a rank-level importance ratio based on the\ngeometric mean of rank-wise token probabilities to stabilize policy updates.\nExperiments on the public Reddit-v2 dataset show that ConvRec-R1 converges\nfaster and achieves higher Recall and NDCG than GRPO-style baselines. Code and\ndatasets are released at https://github.com/yaochenzhu/Rank-GRPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20150.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647a248bed75e95d3e98e3d6",
            "avatarUrl": "/avatars/e10e2f8516d1451fd85e17b5a0ba978d.svg",
            "fullname": "Yaochen Zhu",
            "name": "yaochenzhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "687592798586825b86976d6d",
            "name": "netflix",
            "fullname": "Netflix",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68d2e3609537cbef66612ae8/MJuH26GOdHAZYObE9QOYA.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.27224",
            "authors": [
                {
                    "_id": "690832bf812eca10f9cc5ecf",
                    "user": {
                        "_id": "6422eab8e2029ade06eeee2c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
                        "isPro": false,
                        "fullname": "Mahmud ElHuseyni 🇵🇸",
                        "user": "MElHuseyni",
                        "type": "user"
                    },
                    "name": "Mahmoud El Hussieni",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:52:00.430Z",
                    "hidden": false
                },
                {
                    "_id": "690832bf812eca10f9cc5ed0",
                    "name": "Bahadır K. Güntürk",
                    "hidden": false
                },
                {
                    "_id": "690832bf812eca10f9cc5ed1",
                    "name": "Hasan F. Ateş",
                    "hidden": false
                },
                {
                    "_id": "690832bf812eca10f9cc5ed2",
                    "name": "Oğuz Hanoğlu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-31T06:37:08.000Z",
            "submittedOnDailyAt": "2025-11-03T02:15:50.430Z",
            "title": "Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance\n  Segmentation and Height Classification from Satellite Imagery",
            "submittedOnDailyBy": {
                "_id": "6422eab8e2029ade06eeee2c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
                "isPro": false,
                "fullname": "Mahmud ElHuseyni 🇵🇸",
                "user": "MElHuseyni",
                "type": "user"
            },
            "summary": "Accurate building instance segmentation and height classification are\ncritical for urban planning, 3D city modeling, and infrastructure monitoring.\nThis paper presents a detailed analysis of YOLOv11, the recent advancement in\nthe YOLO series of deep learning models, focusing on its application to joint\nbuilding extraction and discrete height classification from satellite imagery.\nYOLOv11 builds on the strengths of earlier YOLO models by introducing a more\nefficient architecture that better combines features at different scales,\nimproves object localization accuracy, and enhances performance in complex\nurban scenes. Using the DFC2023 Track 2 dataset -- which includes over 125,000\nannotated buildings across 12 cities -- we evaluate YOLOv11's performance using\nmetrics such as precision, recall, F1 score, and mean average precision (mAP).\nOur findings demonstrate that YOLOv11 achieves strong instance segmentation\nperformance with 60.4\\% mAP@50 and 38.3\\% mAP@50--95 while maintaining robust\nclassification accuracy across five predefined height tiers. The model excels\nin handling occlusions, complex building shapes, and class imbalance,\nparticularly for rare high-rise structures. Comparative analysis confirms that\nYOLOv11 outperforms earlier multitask frameworks in both detection accuracy and\ninference speed, making it well-suited for real-time, large-scale urban\nmapping. This research highlights YOLOv11's potential to advance semantic urban\nreconstruction through streamlined categorical height modeling, offering\nactionable insights for future developments in remote sensing and geospatial\nintelligence.",
            "upvotes": 0,
            "discussionId": "690832c0812eca10f9cc5ed3"
        },
        "publishedAt": "2025-10-31T02:37:08.000Z",
        "title": "Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance\n  Segmentation and Height Classification from Satellite Imagery",
        "summary": "Accurate building instance segmentation and height classification are\ncritical for urban planning, 3D city modeling, and infrastructure monitoring.\nThis paper presents a detailed analysis of YOLOv11, the recent advancement in\nthe YOLO series of deep learning models, focusing on its application to joint\nbuilding extraction and discrete height classification from satellite imagery.\nYOLOv11 builds on the strengths of earlier YOLO models by introducing a more\nefficient architecture that better combines features at different scales,\nimproves object localization accuracy, and enhances performance in complex\nurban scenes. Using the DFC2023 Track 2 dataset -- which includes over 125,000\nannotated buildings across 12 cities -- we evaluate YOLOv11's performance using\nmetrics such as precision, recall, F1 score, and mean average precision (mAP).\nOur findings demonstrate that YOLOv11 achieves strong instance segmentation\nperformance with 60.4\\% mAP@50 and 38.3\\% mAP@50--95 while maintaining robust\nclassification accuracy across five predefined height tiers. The model excels\nin handling occlusions, complex building shapes, and class imbalance,\nparticularly for rare high-rise structures. Comparative analysis confirms that\nYOLOv11 outperforms earlier multitask frameworks in both detection accuracy and\ninference speed, making it well-suited for real-time, large-scale urban\nmapping. This research highlights YOLOv11's potential to advance semantic urban\nreconstruction through streamlined categorical height modeling, offering\nactionable insights for future developments in remote sensing and geospatial\nintelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27224.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6422eab8e2029ade06eeee2c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
            "fullname": "Mahmud ElHuseyni 🇵🇸",
            "name": "MElHuseyni",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 21
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.26345",
            "authors": [
                {
                    "_id": "690921df812eca10f9cc6120",
                    "name": "Mykhailo Poliakov",
                    "hidden": false
                },
                {
                    "_id": "690921df812eca10f9cc6121",
                    "name": "Nadiya Shvai",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6904d4574c1d166682536cd3/i6hYjYnEhLComPY3A0R4s.png"
            ],
            "publishedAt": "2025-10-30T10:52:43.000Z",
            "submittedOnDailyAt": "2025-11-03T19:21:07.956Z",
            "title": "MisSynth: Improving MISSCI Logical Fallacies Classification with\n  Synthetic Data",
            "submittedOnDailyBy": {
                "_id": "6904d4574c1d166682536cd3",
                "avatarUrl": "/avatars/d5bee04ac08fbfd0da70f84c535aba7d.svg",
                "isPro": false,
                "fullname": "Mykhailo Poliakov",
                "user": "mxpoliakov",
                "type": "user"
            },
            "summary": "Health-related misinformation is very prevalent and potentially harmful. It\nis difficult to identify, especially when claims distort or misinterpret\nscientific findings. We investigate the impact of synthetic data generation and\nlightweight fine-tuning techniques on the ability of large language models\n(LLMs) to recognize fallacious arguments using the MISSCI dataset and\nframework. In this work, we propose MisSynth, a pipeline that applies\nretrieval-augmented generation (RAG) to produce synthetic fallacy samples,\nwhich are then used to fine-tune an LLM model. Our results show substantial\naccuracy gains with fine-tuned models compared to vanilla baselines. For\ninstance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score\nabsolute improvement on the MISSCI test split over its vanilla baseline. We\ndemonstrate that introducing synthetic fallacy data to augment limited\nannotated resources can significantly enhance zero-shot LLM classification\nperformance on real-world scientific misinformation tasks, even with limited\ncomputational resources. The code and synthetic dataset are available on\nhttps://github.com/mxpoliakov/MisSynth.",
            "upvotes": 0,
            "discussionId": "690921df812eca10f9cc6122",
            "githubRepo": "https://github.com/mxpoliakov/MisSynth",
            "githubStars": 2
        },
        "publishedAt": "2025-10-30T06:52:43.000Z",
        "title": "MisSynth: Improving MISSCI Logical Fallacies Classification with\n  Synthetic Data",
        "summary": "Health-related misinformation is very prevalent and potentially harmful. It\nis difficult to identify, especially when claims distort or misinterpret\nscientific findings. We investigate the impact of synthetic data generation and\nlightweight fine-tuning techniques on the ability of large language models\n(LLMs) to recognize fallacious arguments using the MISSCI dataset and\nframework. In this work, we propose MisSynth, a pipeline that applies\nretrieval-augmented generation (RAG) to produce synthetic fallacy samples,\nwhich are then used to fine-tune an LLM model. Our results show substantial\naccuracy gains with fine-tuned models compared to vanilla baselines. For\ninstance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score\nabsolute improvement on the MISSCI test split over its vanilla baseline. We\ndemonstrate that introducing synthetic fallacy data to augment limited\nannotated resources can significantly enhance zero-shot LLM classification\nperformance on real-world scientific misinformation tasks, even with limited\ncomputational resources. The code and synthetic dataset are available on\nhttps://github.com/mxpoliakov/MisSynth.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6904d4574c1d166682536cd3/i6hYjYnEhLComPY3A0R4s.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26345.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6904d4574c1d166682536cd3",
            "avatarUrl": "/avatars/d5bee04ac08fbfd0da70f84c535aba7d.svg",
            "fullname": "Mykhailo Poliakov",
            "name": "mxpoliakov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.25080",
            "authors": [
                {
                    "_id": "69090e44812eca10f9cc6104",
                    "user": {
                        "_id": "6522b6330415e1b734e51fee",
                        "avatarUrl": "/avatars/cd2a2b049bc7e4b0f79965e1f54ba25c.svg",
                        "isPro": false,
                        "fullname": "William Wolf",
                        "user": "cavaunpeu",
                        "type": "user"
                    },
                    "name": "Will Wolf",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:50:38.174Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-29T01:38:19.000Z",
            "submittedOnDailyAt": "2025-11-03T18:45:40.419Z",
            "title": "Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response\n  Games",
            "submittedOnDailyBy": {
                "_id": "6522b6330415e1b734e51fee",
                "avatarUrl": "/avatars/cd2a2b049bc7e4b0f79965e1f54ba25c.svg",
                "isPro": false,
                "fullname": "William Wolf",
                "user": "cavaunpeu",
                "type": "user"
            },
            "summary": "Card games are widely used to study sequential decision-making under\nuncertainty, with real-world analogues in negotiation, finance, and\ncybersecurity. These games typically fall into three categories based on the\nflow of control: strictly sequential (players alternate single actions),\ndeterministic response (some actions trigger a fixed outcome), and unbounded\nreciprocal response (alternating counterplays are permitted). A less-explored\nbut strategically rich structure is the bounded one-sided response, where a\nplayer's action briefly transfers control to the opponent, who must satisfy a\nfixed condition through one or more moves before the turn resolves. We term\ngames featuring this mechanism Bounded One-Sided Response Games (BORGs). We\nintroduce a modified version of Monopoly Deal as a benchmark environment that\nisolates this dynamic, where a Rent action forces the opponent to choose\npayment assets. The gold-standard algorithm, Counterfactual Regret Minimization\n(CFR), converges on effective strategies without novel algorithmic extensions.\nA lightweight full-stack research platform unifies the environment, a\nparallelized CFR runtime, and a human-playable web interface. The trained CFR\nagent and source code are available at https://monopolydeal.ai.",
            "upvotes": 0,
            "discussionId": "69090e44812eca10f9cc6105"
        },
        "publishedAt": "2025-10-28T21:38:19.000Z",
        "title": "Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response\n  Games",
        "summary": "Card games are widely used to study sequential decision-making under\nuncertainty, with real-world analogues in negotiation, finance, and\ncybersecurity. These games typically fall into three categories based on the\nflow of control: strictly sequential (players alternate single actions),\ndeterministic response (some actions trigger a fixed outcome), and unbounded\nreciprocal response (alternating counterplays are permitted). A less-explored\nbut strategically rich structure is the bounded one-sided response, where a\nplayer's action briefly transfers control to the opponent, who must satisfy a\nfixed condition through one or more moves before the turn resolves. We term\ngames featuring this mechanism Bounded One-Sided Response Games (BORGs). We\nintroduce a modified version of Monopoly Deal as a benchmark environment that\nisolates this dynamic, where a Rent action forces the opponent to choose\npayment assets. The gold-standard algorithm, Counterfactual Regret Minimization\n(CFR), converges on effective strategies without novel algorithmic extensions.\nA lightweight full-stack research platform unifies the environment, a\nparallelized CFR runtime, and a human-playable web interface. The trained CFR\nagent and source code are available at https://monopolydeal.ai.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25080.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6522b6330415e1b734e51fee",
            "avatarUrl": "/avatars/cd2a2b049bc7e4b0f79965e1f54ba25c.svg",
            "fullname": "William Wolf",
            "name": "cavaunpeu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.24078",
            "authors": [
                {
                    "_id": "690919a6812eca10f9cc6110",
                    "name": "William Yang",
                    "hidden": false
                },
                {
                    "_id": "690919a6812eca10f9cc6111",
                    "name": "Xindi Wu",
                    "hidden": false
                },
                {
                    "_id": "690919a6812eca10f9cc6112",
                    "name": "Zhiwei Deng",
                    "hidden": false
                },
                {
                    "_id": "690919a6812eca10f9cc6113",
                    "name": "Esin Tureci",
                    "hidden": false
                },
                {
                    "_id": "690919a6812eca10f9cc6114",
                    "name": "Olga Russakovsky",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T05:40:14.000Z",
            "submittedOnDailyAt": "2025-11-03T18:48:39.549Z",
            "title": "Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained\n  Classification",
            "submittedOnDailyBy": {
                "_id": "66c7b4aeb5ed2bcc2706f978",
                "avatarUrl": "/avatars/ffa5dcab545715c037f4a2fa7c5ffbab.svg",
                "isPro": false,
                "fullname": "william yang",
                "user": "yang-william",
                "type": "user"
            },
            "summary": "Text-to-image (T2I) models are increasingly used for synthetic dataset\ngeneration, but generating effective synthetic training data for classification\nremains challenging. Fine-tuning a T2I model with a few real examples can help\nimprove the quality of synthetic training data; however, it may also cause\noverfitting and reduce diversity in the generated samples. We propose a\nfine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for\nfine-grained classification. Given a small set of real examples, we first\nextract class-agnostic attributes such as scene background and object pose. We\nthen explicitly condition on these attributes during fine-tuning of the T2I\nmodel and marginalize them out during generation. This design mitigates\noverfitting, preserves the T2I model's generative prior, reduces estimation\nerrors, and further minimizes unintended inter-class associations. Extensive\nexperiments across multiple T2I models, backbones, and datasets show that our\nmethod achieves state-of-the-art performance in low-shot fine-grained\nclassification when augmented with synthetic data. Concretely, BOB outperforms\nDataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning\na CLIP classifier with five real images augmented with 100 synthetic images).\nIn three of the four benchmarks, fine-tuning downstream models with 5 real\nimages augmented with BOB achieves better performance than fine-tuning with 10\nreal images. Collectively, BOB outperforms prior art in 18 of 24 experimental\nsettings, with 2+% accuracy improvements in 14 of these settings.",
            "upvotes": 0,
            "discussionId": "690919a6812eca10f9cc6115"
        },
        "publishedAt": "2025-10-28T01:40:14.000Z",
        "title": "Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained\n  Classification",
        "summary": "Text-to-image (T2I) models are increasingly used for synthetic dataset\ngeneration, but generating effective synthetic training data for classification\nremains challenging. Fine-tuning a T2I model with a few real examples can help\nimprove the quality of synthetic training data; however, it may also cause\noverfitting and reduce diversity in the generated samples. We propose a\nfine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for\nfine-grained classification. Given a small set of real examples, we first\nextract class-agnostic attributes such as scene background and object pose. We\nthen explicitly condition on these attributes during fine-tuning of the T2I\nmodel and marginalize them out during generation. This design mitigates\noverfitting, preserves the T2I model's generative prior, reduces estimation\nerrors, and further minimizes unintended inter-class associations. Extensive\nexperiments across multiple T2I models, backbones, and datasets show that our\nmethod achieves state-of-the-art performance in low-shot fine-grained\nclassification when augmented with synthetic data. Concretely, BOB outperforms\nDataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning\na CLIP classifier with five real images augmented with 100 synthetic images).\nIn three of the four benchmarks, fine-tuning downstream models with 5 real\nimages augmented with BOB achieves better performance than fine-tuning with 10\nreal images. Collectively, BOB outperforms prior art in 18 of 24 experimental\nsettings, with 2+% accuracy improvements in 14 of these settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24078.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66c7b4aeb5ed2bcc2706f978",
            "avatarUrl": "/avatars/ffa5dcab545715c037f4a2fa7c5ffbab.svg",
            "fullname": "william yang",
            "name": "yang-william",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]