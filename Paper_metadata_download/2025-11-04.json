[
    {
        "paper": {
            "id": "2510.22115",
            "authors": [
                {
                    "_id": "690977f9812eca10f9cc61c7",
                    "name": "Ling-Team",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61c8",
                    "user": {
                        "_id": "68cb6ba095d81706a4521df1",
                        "avatarUrl": "/avatars/1beabfc0e3fc3fd589d0237f7719a5f4.svg",
                        "isPro": false,
                        "fullname": "Ang Li",
                        "user": "liang23333",
                        "type": "user"
                    },
                    "name": "Ang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:25:22.669Z",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61c9",
                    "name": "Ben Liu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61ca",
                    "name": "Binbin Hu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61cb",
                    "name": "Bing Li",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61cc",
                    "name": "Bingwei Zeng",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61cd",
                    "user": {
                        "_id": "67ce9df5460575b313f5bc63",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67ce9df5460575b313f5bc63/UH1qop2BYGOOaxww_Ci7u.jpeg",
                        "isPro": false,
                        "fullname": "Borui Ye",
                        "user": "a27400",
                        "type": "user"
                    },
                    "name": "Borui Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:25:18.042Z",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61ce",
                    "name": "Caizhi Tang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61cf",
                    "name": "Changxin Tian",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d0",
                    "name": "Chao Huang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d1",
                    "name": "Chao Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d2",
                    "name": "Chen Qian",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d3",
                    "name": "Chenchen Ju",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d4",
                    "name": "Chenchen Li",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d5",
                    "name": "Chengfu Tang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d6",
                    "name": "Chili Fu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d7",
                    "name": "Chunshao Ren",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d8",
                    "name": "Chunwei Wu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d9",
                    "name": "Cong Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61da",
                    "user": {
                        "_id": "644b6dd1e03dccbdae11a098",
                        "avatarUrl": "/avatars/7f22221037afbdace1df3e77b5fedfb9.svg",
                        "isPro": false,
                        "fullname": "Cunyin Peng",
                        "user": "cherryPCY",
                        "type": "user"
                    },
                    "name": "Cunyin Peng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:25:31.382Z",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61db",
                    "name": "Dafeng Xu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61dc",
                    "name": "Daixin Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61dd",
                    "name": "Dalong Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61de",
                    "name": "Dingnan Jin",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61df",
                    "name": "Dingyuan Zhu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e0",
                    "name": "Dongke Hu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e1",
                    "name": "Fangzheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e2",
                    "user": {
                        "_id": "6909a7e34766b88b09547bfd",
                        "avatarUrl": "/avatars/616c4700d6ed65bb5c8cc1f6ac1304a0.svg",
                        "isPro": false,
                        "fullname": "Feifan Wu",
                        "user": "wff2333",
                        "type": "user"
                    },
                    "name": "Feifan Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:25:06.521Z",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e3",
                    "name": "Feng Zhu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e4",
                    "name": "Gangshan Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e5",
                    "name": "Haitao Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e6",
                    "user": {
                        "_id": "68c5887ac5692ecf2f00a590",
                        "avatarUrl": "/avatars/c02dd8bb67ec6bdce087652b610143f8.svg",
                        "isPro": false,
                        "fullname": "hailin",
                        "user": "hugzhl",
                        "type": "user"
                    },
                    "name": "Hailin Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:25:20.589Z",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e7",
                    "user": {
                        "_id": "636b54a729c325b8c0367453",
                        "avatarUrl": "/avatars/64bdcb7093bdfe5de5d9717e98d087cc.svg",
                        "isPro": false,
                        "fullname": "zhanghanxiao",
                        "user": "zhanghanxiao",
                        "type": "user"
                    },
                    "name": "Hanxiao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:25:08.976Z",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e8",
                    "user": {
                        "_id": "63f492f921eb234ab73b173f",
                        "avatarUrl": "/avatars/17beecd06505e24ae5e6fdafefd25c41.svg",
                        "isPro": false,
                        "fullname": "hanziwang",
                        "user": "hanziwang",
                        "type": "user"
                    },
                    "name": "Hanzi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:25:29.285Z",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e9",
                    "name": "Hao Qian",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61ea",
                    "name": "Haoyi Yu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61eb",
                    "name": "Heng Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61ec",
                    "name": "Hongliang Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61ed",
                    "name": "Hongzhi Luan",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61ee",
                    "name": "Huirong Dong",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61ef",
                    "name": "Huizhong Li",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f0",
                    "name": "Jia Li",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f1",
                    "name": "Jia Liu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f2",
                    "name": "Jialong Zhu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f3",
                    "name": "Jian Sha",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f4",
                    "name": "Jianping Wei",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f5",
                    "name": "Jiaolong Yang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f6",
                    "name": "Jieyue Ma",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f7",
                    "name": "Jiewei Wu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f8",
                    "name": "Jinjing Huang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f9",
                    "name": "Jingyun Tian",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61fa",
                    "name": "Jingyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61fb",
                    "name": "Jinquan Sun",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61fc",
                    "name": "Juanhui Tu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61fd",
                    "name": "Jun Liu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61fe",
                    "user": {
                        "_id": "62d898b4e2b34bed7440f69d",
                        "avatarUrl": "/avatars/19e8f28c2d0d94da251b4235815aa85f.svg",
                        "isPro": false,
                        "fullname": "xujunt",
                        "user": "xujunrt",
                        "type": "user"
                    },
                    "name": "Jun Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:25:27.042Z",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61ff",
                    "name": "Jun Zhou",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6200",
                    "name": "Junjie Ou",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6201",
                    "name": "Junpeng Fang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6202",
                    "name": "Kaihong Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6203",
                    "name": "Kaiqin Hu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6204",
                    "name": "Ke Shi",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6205",
                    "name": "Kun Tang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6206",
                    "name": "Kunlong Chen",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6207",
                    "name": "Lanyin Mei",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6208",
                    "name": "Lei Liang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6209",
                    "name": "Lei Xu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc620a",
                    "name": "Libo Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc620b",
                    "name": "Lin Ju",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc620c",
                    "name": "Lin Yuan",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc620d",
                    "name": "Ling Zhong",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc620e",
                    "name": "Lintao Ma",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc620f",
                    "user": {
                        "_id": "679c996e8174155866d1a779",
                        "avatarUrl": "/avatars/e75905929cc2783b91ab7fb397d33345.svg",
                        "isPro": false,
                        "fullname": "liulu",
                        "user": "skyliulu666",
                        "type": "user"
                    },
                    "name": "Lu Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:25:04.419Z",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6210",
                    "name": "Lu Yu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6211",
                    "name": "Lun Cai",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6212",
                    "user": {
                        "_id": "6716425c7dfe714b4210efa3",
                        "avatarUrl": "/avatars/aa5ab069976520393fb95272e5306584.svg",
                        "isPro": false,
                        "fullname": "MeiqiZhu",
                        "user": "zhumeiqi",
                        "type": "user"
                    },
                    "name": "Meiqi Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:25:15.245Z",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6213",
                    "name": "Mengying Li",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6214",
                    "name": "Min Chen",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6215",
                    "name": "Minghao Xue",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6216",
                    "name": "Minghong Cai",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6217",
                    "name": "Mingming Yin",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6218",
                    "user": {
                        "_id": "65afa634c1242ae261bc35d9",
                        "avatarUrl": "/avatars/d460aa6cc299a8080804e621875f314a.svg",
                        "isPro": false,
                        "fullname": "Peijie Jiang",
                        "user": "JZX555",
                        "type": "user"
                    },
                    "name": "Peijie Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:25:24.605Z",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6219",
                    "name": "Peilong Zhao",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc621a",
                    "name": "Pingping Liu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc621b",
                    "user": {
                        "_id": "5fde26773930f07f74aaf912",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fde26773930f07f74aaf912/SD93SVyVCRcTJbogBcf9J.jpeg",
                        "isPro": false,
                        "fullname": "Qian Zhao",
                        "user": "im0qianqian",
                        "type": "user"
                    },
                    "name": "Qian Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:25:33.457Z",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc621c",
                    "name": "Qing Cui",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc621d",
                    "name": "Qingxiang Huang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc621e",
                    "user": {
                        "_id": "67d14347480a436ad4770754",
                        "avatarUrl": "/avatars/ab4327baa7c58ec7cd0d958b0d0cd5d9.svg",
                        "isPro": false,
                        "fullname": "Qingyuan Yang",
                        "user": "yqy3214",
                        "type": "user"
                    },
                    "name": "Qingyuan Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:25:35.379Z",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc621f",
                    "name": "Quankun Yu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6220",
                    "name": "Shaowei Wei",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6221",
                    "name": "Shijie Lian",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6222",
                    "name": "Shoujian Zheng",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6223",
                    "name": "Shun Song",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6224",
                    "name": "Shungen Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6225",
                    "name": "Shuo Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6226",
                    "name": "Siyuan Li",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6227",
                    "name": "Song Liu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6228",
                    "name": "Ting Guo",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6229",
                    "name": "Tong Zhao",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc622a",
                    "name": "Wanli Gu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc622b",
                    "name": "Weichang Wu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc622c",
                    "name": "Weiguang Han",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc622d",
                    "name": "Wenjing Fang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc622e",
                    "name": "Wubin Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc622f",
                    "name": "Xiang Shu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6230",
                    "name": "Xiao Shi",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6231",
                    "name": "Xiaoshun Lan",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6232",
                    "name": "Xiaolu Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6233",
                    "name": "Xiaqing Sun",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6234",
                    "name": "Xin Zhao",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6235",
                    "name": "Xingyu Lu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6236",
                    "name": "Xiong Xu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6237",
                    "name": "Xudong Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6238",
                    "name": "Xudong Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6239",
                    "name": "Xuemin Yang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc623a",
                    "name": "Yajie Yang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc623b",
                    "name": "Yang Xiang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc623c",
                    "name": "Yanzhe Li",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc623d",
                    "name": "Yi Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc623e",
                    "name": "Yilong Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc623f",
                    "name": "Yingxue Li",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6240",
                    "name": "Yongzhen Guo",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6241",
                    "name": "Yuzhuo Fu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6242",
                    "name": "Yuanyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6243",
                    "name": "Yue Yang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6244",
                    "name": "Yue Yu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6245",
                    "name": "Yufeng Deng",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6246",
                    "name": "Yun Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6247",
                    "name": "Yunfei Xu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6248",
                    "name": "Yuqi Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6249",
                    "name": "Yuxiao He",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc624a",
                    "name": "Zengke Gui",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc624b",
                    "name": "Zhaoxin Huan",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc624c",
                    "name": "Zhaoyang Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc624d",
                    "name": "Zhibo Zhu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc624e",
                    "name": "Zhihao Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc624f",
                    "user": {
                        "_id": "677e8c5624bd3d7373584b0c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e8c5624bd3d7373584b0c/36DAI9UR_q3F4LqKZEjho.jpeg",
                        "isPro": false,
                        "fullname": "Zhang Zhiqiang",
                        "user": "zzqsmall",
                        "type": "user"
                    },
                    "name": "Zhiqiang Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:25:42.410Z",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6250",
                    "name": "Zhoufei Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6251",
                    "name": "Zihang Zeng",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6252",
                    "name": "Ziqi Liu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6253",
                    "name": "Zitao Xuan",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6254",
                    "name": "Zuoli Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-25T01:51:37.000Z",
            "submittedOnDailyAt": "2025-11-04T01:24:23.418Z",
            "title": "Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open\n  Language Foundation",
            "submittedOnDailyBy": {
                "_id": "677e8c5624bd3d7373584b0c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e8c5624bd3d7373584b0c/36DAI9UR_q3F4LqKZEjho.jpeg",
                "isPro": false,
                "fullname": "Zhang Zhiqiang",
                "user": "zzqsmall",
                "type": "user"
            },
            "summary": "We introduce Ling 2.0, a series reasoning-oriented language foundation built\nupon the principle that every activation boosts reasoning capability. Designed\nto scale from tens of billions to one trillion parameters under a unified\nMixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity,\ncross-scale consistency, and efficiency guided by empirical scaling laws. The\nseries includes three non-thinking (instruct) models - Ling-mini-2.0,\nLing-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and\nachieving up to 7-fold active-compute efficiency compared with dense\ncounterparts. Ling 2.0 integrates coordinated innovations across model\narchitecture, pre-training, post-training, and infrastructure: a high-sparsity\nMoE with MTP for efficient reasoning, reasoning-oriented data and mid-training\nCoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale\nFP8 training with fine-grained heterogeneous pipelines. At the trillion scale,\nLing-1T establishes a new Pareto frontier of reasoning accuracy versus\ncomputational efficiency, demonstrating that sparse activation, when properly\naligned with reasoning objectives, enables scalable and efficient intelligence.\nCollectively, Ling 2.0 provides a coherent, open, and efficient foundation for\nadvancing future reasoning and thinking models, including the Ring series built\nupon the same base.",
            "upvotes": 60,
            "discussionId": "690977f9812eca10f9cc6255",
            "ai_summary": "Ling 2.0, a reasoning-oriented language model series, achieves high efficiency and accuracy through a Mixture-of-Experts paradigm, sparse activation, and innovative training techniques.",
            "ai_keywords": [
                "Mixture-of-Experts (MoE)",
                "high sparsity",
                "cross-scale consistency",
                "MTP",
                "reasoning-oriented data",
                "mid-training CoT activation",
                "reinforcement-based fine-tuning (DFT",
                "Evo-CoT)",
                "full-scale FP8 training",
                "fine-grained heterogeneous pipelines",
                "sparse activation",
                "reasoning accuracy",
                "computational efficiency"
            ],
            "organization": {
                "_id": "67aea5c8f086ab0f70ed97c9",
                "name": "inclusionAI",
                "fullname": "inclusionAI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
            }
        },
        "publishedAt": "2025-10-24T21:51:37.000Z",
        "title": "Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open\n  Language Foundation",
        "summary": "We introduce Ling 2.0, a series reasoning-oriented language foundation built\nupon the principle that every activation boosts reasoning capability. Designed\nto scale from tens of billions to one trillion parameters under a unified\nMixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity,\ncross-scale consistency, and efficiency guided by empirical scaling laws. The\nseries includes three non-thinking (instruct) models - Ling-mini-2.0,\nLing-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and\nachieving up to 7-fold active-compute efficiency compared with dense\ncounterparts. Ling 2.0 integrates coordinated innovations across model\narchitecture, pre-training, post-training, and infrastructure: a high-sparsity\nMoE with MTP for efficient reasoning, reasoning-oriented data and mid-training\nCoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale\nFP8 training with fine-grained heterogeneous pipelines. At the trillion scale,\nLing-1T establishes a new Pareto frontier of reasoning accuracy versus\ncomputational efficiency, demonstrating that sparse activation, when properly\naligned with reasoning objectives, enables scalable and efficient intelligence.\nCollectively, Ling 2.0 provides a coherent, open, and efficient foundation for\nadvancing future reasoning and thinking models, including the Ring series built\nupon the same base.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22115.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "677e8c5624bd3d7373584b0c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e8c5624bd3d7373584b0c/36DAI9UR_q3F4LqKZEjho.jpeg",
            "fullname": "Zhang Zhiqiang",
            "name": "zzqsmall",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "organization": {
            "_id": "67aea5c8f086ab0f70ed97c9",
            "name": "inclusionAI",
            "fullname": "inclusionAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.00086",
            "authors": [
                {
                    "_id": "69096c6c812eca10f9cc6190",
                    "name": "Fali Wang",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6191",
                    "name": "Jihai Chen",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6192",
                    "name": "Shuhua Yang",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6193",
                    "name": "Runxue Bao",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6194",
                    "name": "Tianxiang Zhao",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6195",
                    "name": "Zhiwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6196",
                    "name": "Xianfeng Tang",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6197",
                    "name": "Hui Liu",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6198",
                    "name": "Qi He",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6199",
                    "name": "Suhang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-29T22:14:25.000Z",
            "submittedOnDailyAt": "2025-11-04T00:31:32.141Z",
            "title": "Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph",
            "submittedOnDailyBy": {
                "_id": "644a8ca97c5c68c7762906a0",
                "avatarUrl": "/avatars/c2f6507fa7dcf00fe0151462533f1c2c.svg",
                "isPro": false,
                "fullname": "Fali Wang",
                "user": "FairyFali",
                "type": "user"
            },
            "summary": "Test-Time Scaling (TTS) improves large language models (LLMs) by allocating\nadditional computation during inference, typically through parallel,\nsequential, or hybrid scaling. However, prior studies often assume fixed\ncollaboration architectures (e.g., topologies) and single-model usage,\noverlooking that optimal architectures and model combinations can vary across\ntasks. Therefore, we study the novel problem of searching for compute-optimal\nmodel combinations and architectures in TTS under a fixed budget. We formalize\nit as a multi-LLM collaboration graph, where nodes encode roles and LLM model\nassignments, and edges capture information flow. This problem is challenging\nbecause (i) the combinatorial search space is prohibitively large, and (ii)\ntask-specific requirements demand tailored designs. To address these, we\nreformulate the problem as probabilistic graph optimization and, through pilot\nexperiments, derive three empirical insights into TTS collaboration graphs.\nGuided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented\nframework that mirrors the REINFORCE pipeline by mapping\nsampling-gradient-update to sampling-feedback-update, where feedback serves as\na textual gradient to update the probabilistic graph and efficiently search for\noptimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE\noutperforms both traditional and LLM-based baselines in sample efficiency and\nsearch performance, and effectively identifies optimal graphs under joint\nobjectives of accuracy and inference latency.",
            "upvotes": 33,
            "discussionId": "69096c6c812eca10f9cc619a",
            "ai_summary": "Agent-REINFORCE optimizes multi-LLM collaboration graphs for test-time scaling, improving sample efficiency and search performance under accuracy and latency constraints.",
            "ai_keywords": [
                "Test-Time Scaling",
                "large language models",
                "parallel scaling",
                "sequential scaling",
                "hybrid scaling",
                "collaboration architectures",
                "multi-LLM collaboration graph",
                "probabilistic graph optimization",
                "Agent-REINFORCE",
                "REINFORCE pipeline",
                "textual gradient"
            ],
            "organization": {
                "_id": "623c72b6483fb88b35620a27",
                "name": "PennState",
                "fullname": "Pennsylvania State University"
            }
        },
        "publishedAt": "2025-10-29T18:14:25.000Z",
        "title": "Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph",
        "summary": "Test-Time Scaling (TTS) improves large language models (LLMs) by allocating\nadditional computation during inference, typically through parallel,\nsequential, or hybrid scaling. However, prior studies often assume fixed\ncollaboration architectures (e.g., topologies) and single-model usage,\noverlooking that optimal architectures and model combinations can vary across\ntasks. Therefore, we study the novel problem of searching for compute-optimal\nmodel combinations and architectures in TTS under a fixed budget. We formalize\nit as a multi-LLM collaboration graph, where nodes encode roles and LLM model\nassignments, and edges capture information flow. This problem is challenging\nbecause (i) the combinatorial search space is prohibitively large, and (ii)\ntask-specific requirements demand tailored designs. To address these, we\nreformulate the problem as probabilistic graph optimization and, through pilot\nexperiments, derive three empirical insights into TTS collaboration graphs.\nGuided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented\nframework that mirrors the REINFORCE pipeline by mapping\nsampling-gradient-update to sampling-feedback-update, where feedback serves as\na textual gradient to update the probabilistic graph and efficiently search for\noptimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE\noutperforms both traditional and LLM-based baselines in sample efficiency and\nsearch performance, and effectively identifies optimal graphs under joint\nobjectives of accuracy and inference latency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.00086.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "644a8ca97c5c68c7762906a0",
            "avatarUrl": "/avatars/c2f6507fa7dcf00fe0151462533f1c2c.svg",
            "fullname": "Fali Wang",
            "name": "FairyFali",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "623c72b6483fb88b35620a27",
            "name": "PennState",
            "fullname": "Pennsylvania State University"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24788",
            "authors": [
                {
                    "_id": "69096b57812eca10f9cc6185",
                    "user": {
                        "_id": "68380f4f231cf484dd4e87f4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xp34hfiSLf-DiE1DVVhHk.png",
                        "isPro": false,
                        "fullname": "Xinjian Zhao",
                        "user": "Xinjiansz",
                        "type": "user"
                    },
                    "name": "Xinjian Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:26:45.587Z",
                    "hidden": false
                },
                {
                    "_id": "69096b57812eca10f9cc6186",
                    "user": {
                        "_id": "65164444bc0631719873af81",
                        "avatarUrl": "/avatars/dab8b90db8bbd00806268fe276e3ea36.svg",
                        "isPro": false,
                        "fullname": "Wei Pang",
                        "user": "weipang142857",
                        "type": "user"
                    },
                    "name": "Wei Pang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:26:47.791Z",
                    "hidden": false
                },
                {
                    "_id": "69096b57812eca10f9cc6187",
                    "name": "Zhongkai Xue",
                    "hidden": false
                },
                {
                    "_id": "69096b57812eca10f9cc6188",
                    "user": {
                        "_id": "636865b8cca0a0a962c21f3f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mja7cpws4gb2Jmdj_foPA.png",
                        "isPro": false,
                        "fullname": "Xiangru (Edward) Jian",
                        "user": "HideOnBush",
                        "type": "user"
                    },
                    "name": "Xiangru Jian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:26:49.963Z",
                    "hidden": false
                },
                {
                    "_id": "69096b57812eca10f9cc6189",
                    "name": "Lei Zhang",
                    "hidden": false
                },
                {
                    "_id": "69096b57812eca10f9cc618a",
                    "name": "Yaoyao Xu",
                    "hidden": false
                },
                {
                    "_id": "69096b57812eca10f9cc618b",
                    "name": "Xiaozhuang Song",
                    "hidden": false
                },
                {
                    "_id": "69096b57812eca10f9cc618c",
                    "name": "Shu Wu",
                    "hidden": false
                },
                {
                    "_id": "69096b57812eca10f9cc618d",
                    "name": "Tianshu Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T05:11:44.000Z",
            "submittedOnDailyAt": "2025-11-04T02:03:48.972Z",
            "title": "The Underappreciated Power of Vision Models for Graph Structural\n  Understanding",
            "submittedOnDailyBy": {
                "_id": "68380f4f231cf484dd4e87f4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xp34hfiSLf-DiE1DVVhHk.png",
                "isPro": false,
                "fullname": "Xinjian Zhao",
                "user": "Xinjiansz",
                "type": "user"
            },
            "summary": "Graph Neural Networks operate through bottom-up message-passing,\nfundamentally differing from human visual perception, which intuitively\ncaptures global structures first. We investigate the underappreciated potential\nof vision models for graph understanding, finding they achieve performance\ncomparable to GNNs on established benchmarks while exhibiting distinctly\ndifferent learning patterns. These divergent behaviors, combined with\nlimitations of existing benchmarks that conflate domain features with\ntopological understanding, motivate our introduction of GraphAbstract. This\nbenchmark evaluates models' ability to perceive global graph properties as\nhumans do: recognizing organizational archetypes, detecting symmetry, sensing\nconnectivity strength, and identifying critical elements. Our results reveal\nthat vision models significantly outperform GNNs on tasks requiring holistic\nstructural understanding and maintain generalizability across varying graph\nscales, while GNNs struggle with global pattern abstraction and degrade with\nincreasing graph size. This work demonstrates that vision models possess\nremarkable yet underutilized capabilities for graph structural understanding,\nparticularly for problems requiring global topological awareness and\nscale-invariant reasoning. These findings open new avenues to leverage this\nunderappreciated potential for developing more effective graph foundation\nmodels for tasks dominated by holistic pattern recognition.",
            "upvotes": 31,
            "discussionId": "69096b57812eca10f9cc618e",
            "ai_summary": "Vision models outperform Graph Neural Networks on tasks requiring global structural understanding and scale-invariant reasoning, as demonstrated by the new GraphAbstract benchmark.",
            "ai_keywords": [
                "Graph Neural Networks",
                "message-passing",
                "vision models",
                "GraphAbstract",
                "global graph properties",
                "organizational archetypes",
                "symmetry",
                "connectivity strength",
                "critical elements",
                "holistic structural understanding",
                "scale-invariant reasoning"
            ]
        },
        "publishedAt": "2025-10-27T01:11:44.000Z",
        "title": "The Underappreciated Power of Vision Models for Graph Structural\n  Understanding",
        "summary": "Graph Neural Networks operate through bottom-up message-passing,\nfundamentally differing from human visual perception, which intuitively\ncaptures global structures first. We investigate the underappreciated potential\nof vision models for graph understanding, finding they achieve performance\ncomparable to GNNs on established benchmarks while exhibiting distinctly\ndifferent learning patterns. These divergent behaviors, combined with\nlimitations of existing benchmarks that conflate domain features with\ntopological understanding, motivate our introduction of GraphAbstract. This\nbenchmark evaluates models' ability to perceive global graph properties as\nhumans do: recognizing organizational archetypes, detecting symmetry, sensing\nconnectivity strength, and identifying critical elements. Our results reveal\nthat vision models significantly outperform GNNs on tasks requiring holistic\nstructural understanding and maintain generalizability across varying graph\nscales, while GNNs struggle with global pattern abstraction and degrade with\nincreasing graph size. This work demonstrates that vision models possess\nremarkable yet underutilized capabilities for graph structural understanding,\nparticularly for problems requiring global topological awareness and\nscale-invariant reasoning. These findings open new avenues to leverage this\nunderappreciated potential for developing more effective graph foundation\nmodels for tasks dominated by holistic pattern recognition.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24788.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "68380f4f231cf484dd4e87f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xp34hfiSLf-DiE1DVVhHk.png",
            "fullname": "Xinjian Zhao",
            "name": "Xinjiansz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.01678",
            "authors": [
                {
                    "_id": "6909a211812eca10f9cc63ca",
                    "name": "Ropeway Liu",
                    "hidden": false
                },
                {
                    "_id": "6909a211812eca10f9cc63cb",
                    "name": "Hangjie Yuan",
                    "hidden": false
                },
                {
                    "_id": "6909a211812eca10f9cc63cc",
                    "name": "Bo Dong",
                    "hidden": false
                },
                {
                    "_id": "6909a211812eca10f9cc63cd",
                    "name": "Jiazheng Xing",
                    "hidden": false
                },
                {
                    "_id": "6909a211812eca10f9cc63ce",
                    "name": "Jinwang Wang",
                    "hidden": false
                },
                {
                    "_id": "6909a211812eca10f9cc63cf",
                    "user": {
                        "_id": "652b83b73b5997ed71a310f2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652b83b73b5997ed71a310f2/ipCpdeHUp4-0OmRz5z8IW.png",
                        "isPro": false,
                        "fullname": "Rui Zhao",
                        "user": "ruizhaocv",
                        "type": "user"
                    },
                    "name": "Rui Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T09:24:59.454Z",
                    "hidden": false
                },
                {
                    "_id": "6909a211812eca10f9cc63d0",
                    "name": "Yan Xing",
                    "hidden": false
                },
                {
                    "_id": "6909a211812eca10f9cc63d1",
                    "name": "Weihua Chen",
                    "hidden": false
                },
                {
                    "_id": "6909a211812eca10f9cc63d2",
                    "name": "Fan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-03T15:41:41.000Z",
            "submittedOnDailyAt": "2025-11-04T04:21:00.451Z",
            "title": "UniLumos: Fast and Unified Image and Video Relighting with\n  Physics-Plausible Feedback",
            "submittedOnDailyBy": {
                "_id": "649d54b314afbb10ce2a9eeb",
                "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
                "isPro": false,
                "fullname": "Hangjie Yuan",
                "user": "JacobYuan",
                "type": "user"
            },
            "summary": "Relighting is a crucial task with both practical demand and artistic value,\nand recent diffusion models have shown strong potential by enabling rich and\ncontrollable lighting effects. However, as they are typically optimized in\nsemantic latent space, where proximity does not guarantee physical correctness\nin visual space, they often produce unrealistic results, such as overexposed\nhighlights, misaligned shadows, and incorrect occlusions. We address this with\nUniLumos, a unified relighting framework for both images and videos that brings\nRGB-space geometry feedback into a flow matching backbone. By supervising the\nmodel with depth and normal maps extracted from its outputs, we explicitly\nalign lighting effects with the scene structure, enhancing physical\nplausibility. Nevertheless, this feedback requires high-quality outputs for\nsupervision in visual space, making standard multi-step denoising\ncomputationally expensive. To mitigate this, we employ path consistency\nlearning, allowing supervision to remain effective even under few-step training\nregimes. To enable fine-grained relighting control and supervision, we design a\nstructured six-dimensional annotation protocol capturing core illumination\nattributes. Building upon this, we propose LumosBench, a disentangled\nattribute-level benchmark that evaluates lighting controllability via large\nvision-language models, enabling automatic and interpretable assessment of\nrelighting precision across individual dimensions. Extensive experiments\ndemonstrate that UniLumos achieves state-of-the-art relighting quality with\nsignificantly improved physical consistency, while delivering a 20x speedup for\nboth image and video relighting. Code is available at\nhttps://github.com/alibaba-damo-academy/Lumos-Custom.",
            "upvotes": 27,
            "discussionId": "6909a212812eca10f9cc63d3",
            "projectPage": "https://github.com/alibaba-damo-academy/Lumos-Custom",
            "githubRepo": "https://github.com/alibaba-damo-academy/Lumos-Custom",
            "ai_summary": "UniLumos, a unified relighting framework, enhances physical plausibility by integrating RGB-space geometry feedback into a flow matching backbone, achieving state-of-the-art results with improved consistency and speed.",
            "ai_keywords": [
                "diffusion models",
                "semantic latent space",
                "RGB-space",
                "flow matching",
                "depth maps",
                "normal maps",
                "path consistency learning",
                "six-dimensional annotation protocol",
                "disentangled attribute-level benchmark",
                "LumosBench",
                "large vision-language models",
                "relighting quality",
                "physical consistency"
            ],
            "githubStars": 8,
            "organization": {
                "_id": "6808e7522a4d69d5111da55f",
                "name": "Alibaba-DAMO-Academy",
                "fullname": "DAMO Academy",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
            }
        },
        "publishedAt": "2025-11-03T10:41:41.000Z",
        "title": "UniLumos: Fast and Unified Image and Video Relighting with\n  Physics-Plausible Feedback",
        "summary": "Relighting is a crucial task with both practical demand and artistic value,\nand recent diffusion models have shown strong potential by enabling rich and\ncontrollable lighting effects. However, as they are typically optimized in\nsemantic latent space, where proximity does not guarantee physical correctness\nin visual space, they often produce unrealistic results, such as overexposed\nhighlights, misaligned shadows, and incorrect occlusions. We address this with\nUniLumos, a unified relighting framework for both images and videos that brings\nRGB-space geometry feedback into a flow matching backbone. By supervising the\nmodel with depth and normal maps extracted from its outputs, we explicitly\nalign lighting effects with the scene structure, enhancing physical\nplausibility. Nevertheless, this feedback requires high-quality outputs for\nsupervision in visual space, making standard multi-step denoising\ncomputationally expensive. To mitigate this, we employ path consistency\nlearning, allowing supervision to remain effective even under few-step training\nregimes. To enable fine-grained relighting control and supervision, we design a\nstructured six-dimensional annotation protocol capturing core illumination\nattributes. Building upon this, we propose LumosBench, a disentangled\nattribute-level benchmark that evaluates lighting controllability via large\nvision-language models, enabling automatic and interpretable assessment of\nrelighting precision across individual dimensions. Extensive experiments\ndemonstrate that UniLumos achieves state-of-the-art relighting quality with\nsignificantly improved physical consistency, while delivering a 20x speedup for\nboth image and video relighting. Code is available at\nhttps://github.com/alibaba-damo-academy/Lumos-Custom.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01678.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "fullname": "Hangjie Yuan",
            "name": "JacobYuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "6808e7522a4d69d5111da55f",
            "name": "Alibaba-DAMO-Academy",
            "fullname": "DAMO Academy",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.01163",
            "authors": [
                {
                    "_id": "690986f4812eca10f9cc6384",
                    "name": "Yongyuan Liang",
                    "hidden": false
                },
                {
                    "_id": "690986f4812eca10f9cc6385",
                    "name": "Wei Chow",
                    "hidden": false
                },
                {
                    "_id": "690986f4812eca10f9cc6386",
                    "name": "Feng Li",
                    "hidden": false
                },
                {
                    "_id": "690986f4812eca10f9cc6387",
                    "user": {
                        "_id": "630cfc45b66f088d547b2768",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630cfc45b66f088d547b2768/9-dMts2xFVbPmPHJGGqBx.png",
                        "isPro": true,
                        "fullname": "Martin Ziqiao Ma",
                        "user": "marstin",
                        "type": "user"
                    },
                    "name": "Ziqiao Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:24:35.876Z",
                    "hidden": false
                },
                {
                    "_id": "690986f4812eca10f9cc6388",
                    "user": {
                        "_id": "655fed9fdef5905d38b84af3",
                        "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
                        "isPro": false,
                        "fullname": "Xiyao Wang",
                        "user": "russwang",
                        "type": "user"
                    },
                    "name": "Xiyao Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T19:50:50.234Z",
                    "hidden": false
                },
                {
                    "_id": "690986f4812eca10f9cc6389",
                    "name": "Jiageng Mao",
                    "hidden": false
                },
                {
                    "_id": "690986f4812eca10f9cc638a",
                    "name": "Jiuhai Chen",
                    "hidden": false
                },
                {
                    "_id": "690986f4812eca10f9cc638b",
                    "name": "Jiatao Gu",
                    "hidden": false
                },
                {
                    "_id": "690986f4812eca10f9cc638c",
                    "name": "Yue Wang",
                    "hidden": false
                },
                {
                    "_id": "690986f4812eca10f9cc638d",
                    "name": "Furong Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-03T02:27:46.000Z",
            "submittedOnDailyAt": "2025-11-04T02:28:41.642Z",
            "title": "ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal\n  Generation",
            "submittedOnDailyBy": {
                "_id": "6646d5819bb34d2b6b7455d3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JFH3ZTPvlaVSg4RJJBb6L.jpeg",
                "isPro": false,
                "fullname": "Yongyuan Liang",
                "user": "cheryyunl",
                "type": "user"
            },
            "summary": "Unified multimodal models (UMMs) have emerged as a powerful paradigm for\nseamlessly unifying text and image understanding and generation. However,\nprevailing evaluations treat these abilities in isolation, such that tasks with\nmultimodal inputs and outputs are scored primarily through unimodal reasoning,\ni.e., textual benchmarks emphasize language-based reasoning, while visual\nbenchmarks emphasize reasoning outcomes manifested in the pixels. We introduce\nROVER to address this pressing need to test reciprocal cross-modal reasoning,\nthe use of one modality to guide, verify, or refine outputs in the other, an\nability central to the vision of unified multimodal intelligence. ROVER is a\nhuman-annotated benchmark that explicitly targets reciprocal cross-modal\nreasoning, which contains 1312 tasks grounded in 1876 images, spanning two\ncomplementary settings. Verbally-augmented reasoning for visual generation\nevaluates whether models can use verbal prompts and reasoning chains to guide\nfaithful image synthesis. Visually-augmented reasoning for verbal generation\nevaluates whether models can generate intermediate visualizations that\nstrengthen their own reasoning processes for question answering. Experiments on\n17 unified models reveal two key findings: (i) Cross-modal reasoning determines\nvisual generation quality, with interleaved models significantly outperforming\nnon-interleaved ones; notably, combining strong unimodal models fails to\nachieve comparable reasoning. (ii) Models show dissociation between physical\nand symbolic reasoning: they succeed at interpreting perceptual concepts\nliterally but fail to construct visual abstractions for symbolic tasks, where\nfaulty reasoning harms performance. These results highlight reciprocal\ncross-modal reasoning as a critical frontier for enabling true omnimodal\ngeneration.",
            "upvotes": 24,
            "discussionId": "690986f4812eca10f9cc638e",
            "ai_summary": "ROVER is a benchmark that evaluates reciprocal cross-modal reasoning in unified multimodal models, showing that cross-modal interactions significantly impact visual generation quality and that models struggle with symbolic reasoning tasks.",
            "ai_keywords": [
                "unified multimodal models",
                "UMMs",
                "reciprocal cross-modal reasoning",
                "ROVER",
                "verbal prompts",
                "reasoning chains",
                "image synthesis",
                "intermediate visualizations",
                "question answering",
                "cross-modal reasoning",
                "visual generation quality",
                "interleaved models",
                "non-interleaved models",
                "unimodal models",
                "physical reasoning",
                "symbolic reasoning",
                "perceptual concepts",
                "visual abstractions"
            ]
        },
        "publishedAt": "2025-11-02T21:27:46.000Z",
        "title": "ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal\n  Generation",
        "summary": "Unified multimodal models (UMMs) have emerged as a powerful paradigm for\nseamlessly unifying text and image understanding and generation. However,\nprevailing evaluations treat these abilities in isolation, such that tasks with\nmultimodal inputs and outputs are scored primarily through unimodal reasoning,\ni.e., textual benchmarks emphasize language-based reasoning, while visual\nbenchmarks emphasize reasoning outcomes manifested in the pixels. We introduce\nROVER to address this pressing need to test reciprocal cross-modal reasoning,\nthe use of one modality to guide, verify, or refine outputs in the other, an\nability central to the vision of unified multimodal intelligence. ROVER is a\nhuman-annotated benchmark that explicitly targets reciprocal cross-modal\nreasoning, which contains 1312 tasks grounded in 1876 images, spanning two\ncomplementary settings. Verbally-augmented reasoning for visual generation\nevaluates whether models can use verbal prompts and reasoning chains to guide\nfaithful image synthesis. Visually-augmented reasoning for verbal generation\nevaluates whether models can generate intermediate visualizations that\nstrengthen their own reasoning processes for question answering. Experiments on\n17 unified models reveal two key findings: (i) Cross-modal reasoning determines\nvisual generation quality, with interleaved models significantly outperforming\nnon-interleaved ones; notably, combining strong unimodal models fails to\nachieve comparable reasoning. (ii) Models show dissociation between physical\nand symbolic reasoning: they succeed at interpreting perceptual concepts\nliterally but fail to construct visual abstractions for symbolic tasks, where\nfaulty reasoning harms performance. These results highlight reciprocal\ncross-modal reasoning as a critical frontier for enabling true omnimodal\ngeneration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01163.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6646d5819bb34d2b6b7455d3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JFH3ZTPvlaVSg4RJJBb6L.jpeg",
            "fullname": "Yongyuan Liang",
            "name": "cheryyunl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.26236",
            "authors": [
                {
                    "_id": "6904bbe336711a7e91ed4d94",
                    "user": {
                        "_id": "68f621035356f4ab64fa04a2",
                        "avatarUrl": "/avatars/29fbe7a537a71160b2c301efe7c830d1.svg",
                        "isPro": false,
                        "fullname": "Kyungmin Lee",
                        "user": "Kyungminn",
                        "type": "user"
                    },
                    "name": "Kyungmin Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:54:32.642Z",
                    "hidden": false
                },
                {
                    "_id": "6904bbe336711a7e91ed4d95",
                    "name": "Sibeen Kim",
                    "hidden": false
                },
                {
                    "_id": "6904bbe336711a7e91ed4d96",
                    "name": "Minho Park",
                    "hidden": false
                },
                {
                    "_id": "6904bbe336711a7e91ed4d97",
                    "name": "Hyunseung Kim",
                    "hidden": false
                },
                {
                    "_id": "6904bbe336711a7e91ed4d98",
                    "name": "Dongyoon Hwang",
                    "hidden": false
                },
                {
                    "_id": "6904bbe336711a7e91ed4d99",
                    "name": "Hojoon Lee",
                    "hidden": false
                },
                {
                    "_id": "6904bbe336711a7e91ed4d9a",
                    "name": "Jaegul Choo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/68f621035356f4ab64fa04a2/gVSPzM9IOvUGTSsjcfZkk.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/68f621035356f4ab64fa04a2/sZNMyVxYxz2QqZ9yq4t1t.mp4"
            ],
            "publishedAt": "2025-10-30T08:13:12.000Z",
            "submittedOnDailyAt": "2025-11-04T04:38:29.362Z",
            "title": "PHUMA: Physically-Grounded Humanoid Locomotion Dataset",
            "submittedOnDailyBy": {
                "_id": "68f621035356f4ab64fa04a2",
                "avatarUrl": "/avatars/29fbe7a537a71160b2c301efe7c830d1.svg",
                "isPro": false,
                "fullname": "Kyungmin Lee",
                "user": "Kyungminn",
                "type": "user"
            },
            "summary": "Motion imitation is a promising approach for humanoid locomotion, enabling\nagents to acquire humanlike behaviors. Existing methods typically rely on\nhigh-quality motion capture datasets such as AMASS, but these are scarce and\nexpensive, limiting scalability and diversity. Recent studies attempt to scale\ndata collection by converting large-scale internet videos, exemplified by\nHumanoid-X. However, they often introduce physical artifacts such as floating,\npenetration, and foot skating, which hinder stable imitation. In response, we\nintroduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that\nleverages human video at scale, while addressing physical artifacts through\ncareful data curation and physics-constrained retargeting. PHUMA enforces joint\nlimits, ensures ground contact, and eliminates foot skating, producing motions\nthat are both large-scale and physically reliable. We evaluated PHUMA in two\nsets of conditions: (i) imitation of unseen motion from self-recorded test\nvideos and (ii) path following with pelvis-only guidance. In both cases,\nPHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant\ngains in imitating diverse motions. The code is available at\nhttps://davian-robotics.github.io/PHUMA.",
            "upvotes": 23,
            "discussionId": "6904bbe336711a7e91ed4d9b",
            "projectPage": "https://davian-robotics.github.io/PHUMA/",
            "githubRepo": "https://github.com/davian-robotics/PHUMA",
            "ai_summary": "PHUMA, a large-scale and physically reliable humanoid locomotion dataset, improves motion imitation by addressing physical artifacts in human video data.",
            "ai_keywords": [
                "motion imitation",
                "humanoid locomotion",
                "motion capture datasets",
                "AMASS",
                "Humanoid-X",
                "physical artifacts",
                "floating",
                "penetration",
                "foot skating",
                "data curation",
                "physics-constrained retargeting",
                "joint limits",
                "ground contact",
                "path following",
                "pelvis-only guidance"
            ],
            "githubStars": 85,
            "organization": {
                "_id": "68f61ab10a6265597402e1b1",
                "name": "DAVIAN-Robotics",
                "fullname": "DAVIAN Robotics",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630461624ec2dfa82a5ad7e7/0NA6xGIRlNalvgIxvZIXD.png"
            }
        },
        "publishedAt": "2025-10-30T04:13:12.000Z",
        "title": "PHUMA: Physically-Grounded Humanoid Locomotion Dataset",
        "summary": "Motion imitation is a promising approach for humanoid locomotion, enabling\nagents to acquire humanlike behaviors. Existing methods typically rely on\nhigh-quality motion capture datasets such as AMASS, but these are scarce and\nexpensive, limiting scalability and diversity. Recent studies attempt to scale\ndata collection by converting large-scale internet videos, exemplified by\nHumanoid-X. However, they often introduce physical artifacts such as floating,\npenetration, and foot skating, which hinder stable imitation. In response, we\nintroduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that\nleverages human video at scale, while addressing physical artifacts through\ncareful data curation and physics-constrained retargeting. PHUMA enforces joint\nlimits, ensures ground contact, and eliminates foot skating, producing motions\nthat are both large-scale and physically reliable. We evaluated PHUMA in two\nsets of conditions: (i) imitation of unseen motion from self-recorded test\nvideos and (ii) path following with pelvis-only guidance. In both cases,\nPHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant\ngains in imitating diverse motions. The code is available at\nhttps://davian-robotics.github.io/PHUMA.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/68f621035356f4ab64fa04a2/gVSPzM9IOvUGTSsjcfZkk.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/68f621035356f4ab64fa04a2/sZNMyVxYxz2QqZ9yq4t1t.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26236.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "68f621035356f4ab64fa04a2",
            "avatarUrl": "/avatars/29fbe7a537a71160b2c301efe7c830d1.svg",
            "fullname": "Kyungmin Lee",
            "name": "Kyungminn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "68f61ab10a6265597402e1b1",
            "name": "DAVIAN-Robotics",
            "fullname": "DAVIAN Robotics",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630461624ec2dfa82a5ad7e7/0NA6xGIRlNalvgIxvZIXD.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.01295",
            "authors": [
                {
                    "_id": "69097a04812eca10f9cc62fe",
                    "name": "Feng Han",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc62ff",
                    "user": {
                        "_id": "654c6845bac6e6e49895a5b5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KXQaAxulqr8jNBSpEaYM4.png",
                        "isPro": false,
                        "fullname": "SII-Yibin Wang",
                        "user": "CodeGoat24",
                        "type": "user"
                    },
                    "name": "Yibin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:24:43.308Z",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6300",
                    "name": "Chenglin Li",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6301",
                    "name": "Zheming Liang",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6302",
                    "name": "Dianyi Wang",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6303",
                    "name": "Yang Jiao",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6304",
                    "name": "Zhipeng Wei",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6305",
                    "name": "Chao Gong",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6306",
                    "name": "Cheng Jin",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6307",
                    "name": "Jingjing Chen",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6308",
                    "name": "Jiaqi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-03T07:24:57.000Z",
            "submittedOnDailyAt": "2025-11-04T01:29:52.436Z",
            "title": "UniREditBench: A Unified Reasoning-based Image Editing Benchmark",
            "submittedOnDailyBy": {
                "_id": "654c6845bac6e6e49895a5b5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KXQaAxulqr8jNBSpEaYM4.png",
                "isPro": false,
                "fullname": "SII-Yibin Wang",
                "user": "CodeGoat24",
                "type": "user"
            },
            "summary": "Recent advances in multi-modal generative models have driven substantial\nimprovements in image editing. However, current generative models still\nstruggle with handling diverse and complex image editing tasks that require\nimplicit reasoning, underscoring the need for a comprehensive benchmark to\nsystematically assess their performance across various reasoning scenarios.\nExisting benchmarks primarily focus on single-object attribute transformation\nin realistic scenarios, which, while effective, encounter two key challenges:\n(1) they largely overlook multi-object interactions as well as game-world\nscenarios that involve human-defined rules, which are common in real-life\napplications; (2) they only rely on textual references to evaluate the\ngenerated images, potentially leading to systematic misjudgments, especially in\ncomplex reasoning scenarios. To this end, this work proposes UniREditBench, a\nunified benchmark for reasoning-based image editing evaluation. It comprises\n2,700 meticulously curated samples, covering both real- and game-world\nscenarios across 8 primary dimensions and 18 sub-dimensions. To improve\nevaluation reliability, we introduce multimodal dual-reference evaluation,\nproviding both textual and ground-truth image references for each sample\nassessment. Furthermore, we design an automated multi-scenario data synthesis\npipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with\nhigh-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel\non this dataset and develop UniREdit-Bagel, demonstrating substantial\nimprovements in both in-domain and out-of-distribution settings. Through\nthorough benchmarking of both open-source and closed-source image editing\nmodels, we reveal their strengths and weaknesses across various aspects.",
            "upvotes": 22,
            "discussionId": "69097a04812eca10f9cc6309",
            "projectPage": "https://maplebb.github.io/UniREditBench/",
            "githubRepo": "https://github.com/Maplebb/UniREditBench",
            "ai_summary": "UniREditBench is a unified benchmark for reasoning-based image editing that addresses limitations in existing benchmarks by including multi-object interactions, game-world scenarios, and multimodal dual-reference evaluation.",
            "ai_keywords": [
                "multi-modal generative models",
                "image editing",
                "implicit reasoning",
                "benchmark",
                "single-object attribute transformation",
                "real-world scenarios",
                "game-world scenarios",
                "human-defined rules",
                "multimodal dual-reference evaluation",
                "automated multi-scenario data synthesis",
                "chain-of-thought reasoning",
                "fine-tuning",
                "UniREdit-Data-100K",
                "UniREdit-Bagel",
                "in-domain",
                "out-of-distribution"
            ],
            "githubStars": 27,
            "organization": {
                "_id": "643cb0625fcffe09fb6ca688",
                "name": "Fudan-University",
                "fullname": "Fudan University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"
            }
        },
        "publishedAt": "2025-11-03T02:24:57.000Z",
        "title": "UniREditBench: A Unified Reasoning-based Image Editing Benchmark",
        "summary": "Recent advances in multi-modal generative models have driven substantial\nimprovements in image editing. However, current generative models still\nstruggle with handling diverse and complex image editing tasks that require\nimplicit reasoning, underscoring the need for a comprehensive benchmark to\nsystematically assess their performance across various reasoning scenarios.\nExisting benchmarks primarily focus on single-object attribute transformation\nin realistic scenarios, which, while effective, encounter two key challenges:\n(1) they largely overlook multi-object interactions as well as game-world\nscenarios that involve human-defined rules, which are common in real-life\napplications; (2) they only rely on textual references to evaluate the\ngenerated images, potentially leading to systematic misjudgments, especially in\ncomplex reasoning scenarios. To this end, this work proposes UniREditBench, a\nunified benchmark for reasoning-based image editing evaluation. It comprises\n2,700 meticulously curated samples, covering both real- and game-world\nscenarios across 8 primary dimensions and 18 sub-dimensions. To improve\nevaluation reliability, we introduce multimodal dual-reference evaluation,\nproviding both textual and ground-truth image references for each sample\nassessment. Furthermore, we design an automated multi-scenario data synthesis\npipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with\nhigh-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel\non this dataset and develop UniREdit-Bagel, demonstrating substantial\nimprovements in both in-domain and out-of-distribution settings. Through\nthorough benchmarking of both open-source and closed-source image editing\nmodels, we reveal their strengths and weaknesses across various aspects.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01295.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "654c6845bac6e6e49895a5b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KXQaAxulqr8jNBSpEaYM4.png",
            "fullname": "SII-Yibin Wang",
            "name": "CodeGoat24",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "organization": {
            "_id": "643cb0625fcffe09fb6ca688",
            "name": "Fudan-University",
            "fullname": "Fudan University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.00062",
            "authors": [
                {
                    "_id": "69097a8f812eca10f9cc630b",
                    "name": "NVIDIA",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc630d",
                    "name": "Arslan Ali",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc630e",
                    "name": "Junjie Bai",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc630f",
                    "name": "Maciej Bala",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6310",
                    "name": "Yogesh Balaji",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6311",
                    "name": "Aaron Blakeman",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6312",
                    "name": "Tiffany Cai",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6313",
                    "name": "Jiaxin Cao",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6314",
                    "name": "Tianshi Cao",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6315",
                    "name": "Elizabeth Cha",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6316",
                    "name": "Yu-Wei Chao",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6317",
                    "name": "Prithvijit Chattopadhyay",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6318",
                    "name": "Mike Chen",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6319",
                    "name": "Yongxin Chen",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc631a",
                    "name": "Yu Chen",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc631b",
                    "name": "Shuai Cheng",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc631c",
                    "name": "Yin Cui",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc631d",
                    "name": "Jenna Diamond",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc631e",
                    "name": "Yifan Ding",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc631f",
                    "name": "Jiaojiao Fan",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6320",
                    "name": "Linxi Fan",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6321",
                    "name": "Liang Feng",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6322",
                    "name": "Francesco Ferroni",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6323",
                    "name": "Sanja Fidler",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6324",
                    "name": "Xiao Fu",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6325",
                    "name": "Ruiyuan Gao",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6326",
                    "name": "Yunhao Ge",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6327",
                    "name": "Jinwei Gu",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6328",
                    "name": "Aryaman Gupta",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6329",
                    "name": "Siddharth Gururani",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc632a",
                    "name": "Imad El Hanafi",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc632b",
                    "name": "Ali Hassani",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc632c",
                    "name": "Zekun Hao",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc632d",
                    "name": "Jacob Huffman",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc632e",
                    "name": "Joel Jang",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc632f",
                    "name": "Pooya Jannaty",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6330",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6331",
                    "name": "Grace Lam",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6332",
                    "name": "Xuan Li",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6333",
                    "name": "Zhaoshuo Li",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6334",
                    "name": "Maosheng Liao",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6335",
                    "name": "Chen-Hsuan Lin",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6336",
                    "name": "Tsung-Yi Lin",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6337",
                    "name": "Yen-Chen Lin",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6338",
                    "name": "Huan Ling",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6339",
                    "name": "Ming-Yu Liu",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc633a",
                    "name": "Xian Liu",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc633b",
                    "name": "Yifan Lu",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc633c",
                    "name": "Alice Luo",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc633d",
                    "name": "Qianli Ma",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc633e",
                    "name": "Hanzi Mao",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc633f",
                    "user": {
                        "_id": "633a0e55ddf7f9c46d6697ec",
                        "avatarUrl": "/avatars/aff6df6a6b9d9e415b7f1931dd270a38.svg",
                        "isPro": false,
                        "fullname": "Kaichun Mo",
                        "user": "daerduomkch",
                        "type": "user"
                    },
                    "name": "Kaichun Mo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:24:41.033Z",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6340",
                    "name": "Seungjun Nah",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6341",
                    "name": "Yashraj Narang",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6342",
                    "name": "Abhijeet Panaskar",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6343",
                    "name": "Lindsey Pavao",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6344",
                    "name": "Trung Pham",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6345",
                    "name": "Morteza Ramezanali",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6346",
                    "name": "Fitsum Reda",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6347",
                    "name": "Scott Reed",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6348",
                    "name": "Xuanchi Ren",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6349",
                    "name": "Haonan Shao",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc634a",
                    "name": "Yue Shen",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc634b",
                    "name": "Stella Shi",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc634c",
                    "name": "Shuran Song",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc634d",
                    "name": "Bartosz Stefaniak",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc634e",
                    "name": "Shangkun Sun",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc634f",
                    "name": "Shitao Tang",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6350",
                    "name": "Sameena Tasmeen",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6351",
                    "name": "Lyne Tchapmi",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6352",
                    "name": "Wei-Cheng Tseng",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6353",
                    "name": "Jibin Varghese",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6354",
                    "name": "Andrew Z. Wang",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6355",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6356",
                    "name": "Haoxiang Wang",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6357",
                    "name": "Heng Wang",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6358",
                    "name": "Ting-Chun Wang",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6359",
                    "name": "Fangyin Wei",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc635a",
                    "name": "Jiashu Xu",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc635b",
                    "name": "Dinghao Yang",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc635c",
                    "name": "Xiaodong Yang",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc635d",
                    "name": "Haotian Ye",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc635e",
                    "name": "Seonghyeon Ye",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc635f",
                    "name": "Xiaohui Zeng",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6360",
                    "name": "Jing Zhang",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6361",
                    "name": "Qinsheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6362",
                    "name": "Kaiwen Zheng",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6363",
                    "name": "Andrew Zhu",
                    "hidden": false
                },
                {
                    "_id": "69097a8f812eca10f9cc6364",
                    "name": "Yuke Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T22:44:13.000Z",
            "submittedOnDailyAt": "2025-11-04T01:31:46.720Z",
            "title": "World Simulation with Video Foundation Models for Physical AI",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World\nFoundation Models for Physical AI. Built on a flow-based architecture,\n[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation\nin a single model and leverages [Cosmos-Reason1], a Physical AI vision-language\nmodel, to provide richer text grounding and finer control of world simulation.\nTrained on 200M curated video clips and refined with reinforcement\nlearning-based post-training, [Cosmos-Predict2.5] achieves substantial\nimprovements over [Cosmos-Predict1] in video quality and instruction alignment,\nwith models released at 2B and 14B scales. These capabilities enable more\nreliable synthetic data generation, policy evaluation, and closed-loop\nsimulation for robotics and autonomous systems. We further extend the family\nwith [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and\nReal2Real world translation. Despite being 3.5times smaller than\n[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video\ngeneration. Together, these advances establish [Cosmos-Predict2.5] and\n[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To\naccelerate research and deployment in Physical AI, we release source code,\npretrained checkpoints, and curated benchmarks under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-predict2.5 and\nhttps://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open\nresources lower the barrier to adoption and foster innovation in building the\nnext generation of embodied intelligence.",
            "upvotes": 20,
            "discussionId": "69097a90812eca10f9cc6365",
            "ai_summary": "Cosmos-Predict2.5 and Cosmos-Transfer2.5 are advanced Physical AI models that unify text, image, and video generation, improve video quality and instruction alignment, and enable Sim2Real and Real2Real world translation with higher fidelity.",
            "ai_keywords": [
                "flow-based architecture",
                "Text2World",
                "Image2World",
                "Video2World",
                "Physical AI",
                "vision-language model",
                "reinforcement learning",
                "synthetic data generation",
                "policy evaluation",
                "closed-loop simulation",
                "robotics",
                "autonomous systems",
                "control-net",
                "Sim2Real",
                "Real2Real",
                "video generation",
                "embodied intelligence"
            ]
        },
        "publishedAt": "2025-10-28T18:44:13.000Z",
        "title": "World Simulation with Video Foundation Models for Physical AI",
        "summary": "We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World\nFoundation Models for Physical AI. Built on a flow-based architecture,\n[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation\nin a single model and leverages [Cosmos-Reason1], a Physical AI vision-language\nmodel, to provide richer text grounding and finer control of world simulation.\nTrained on 200M curated video clips and refined with reinforcement\nlearning-based post-training, [Cosmos-Predict2.5] achieves substantial\nimprovements over [Cosmos-Predict1] in video quality and instruction alignment,\nwith models released at 2B and 14B scales. These capabilities enable more\nreliable synthetic data generation, policy evaluation, and closed-loop\nsimulation for robotics and autonomous systems. We further extend the family\nwith [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and\nReal2Real world translation. Despite being 3.5times smaller than\n[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video\ngeneration. Together, these advances establish [Cosmos-Predict2.5] and\n[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To\naccelerate research and deployment in Physical AI, we release source code,\npretrained checkpoints, and curated benchmarks under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-predict2.5 and\nhttps://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open\nresources lower the barrier to adoption and foster innovation in building the\nnext generation of embodied intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.00062.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 154
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.27363",
            "authors": [
                {
                    "_id": "6908501d812eca10f9cc5efe",
                    "user": {
                        "_id": "65db23d1f386d08eb0d1cec5",
                        "avatarUrl": "/avatars/b495ec5b35b15fea245ef490b83d1856.svg",
                        "isPro": false,
                        "fullname": "Mengjie Deng",
                        "user": "MengjieDeng",
                        "type": "user"
                    },
                    "name": "Mengjie Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:51:52.660Z",
                    "hidden": false
                },
                {
                    "_id": "6908501d812eca10f9cc5eff",
                    "name": "Guanting Dong",
                    "hidden": false
                },
                {
                    "_id": "6908501d812eca10f9cc5f00",
                    "name": "Zhicheng Dou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-31T10:51:27.000Z",
            "submittedOnDailyAt": "2025-11-04T04:26:55.171Z",
            "title": "ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool\n  Use",
            "submittedOnDailyBy": {
                "_id": "65db23d1f386d08eb0d1cec5",
                "avatarUrl": "/avatars/b495ec5b35b15fea245ef490b83d1856.svg",
                "isPro": false,
                "fullname": "Mengjie Deng",
                "user": "MengjieDeng",
                "type": "user"
            },
            "summary": "Recently, large language models (LLMs) have demonstrated remarkable\nproblem-solving capabilities by autonomously integrating with external tools\nfor collaborative reasoning. However, due to the inherently complex and diverse\nnature of multimodal information, enabling multimodal large language models\n(MLLMs) to flexibly and efficiently utilize external tools during reasoning\nremains an underexplored challenge. In this work, we introduce ToolScope, an\nagentic framework designed to unify global planning with local multimodal\nperception, adopting a specialized Perceive tool to mitigates visual context\ndegradation in long-horizon VQA task. ToolScope comprises three primary\ncomponents: the Global Navigator, the Agentic Executor, and the Response\nSynthesizer. The Global Navigator functions as a \"telescope\", offering\nhigh-level strategic guidance. The Agentic Executor operates iteratively to\naugment MLLM with local perception through the integration of external\ntools-Search, Code, and Perceive. Finally, the Response Synthesizer\nconsolidates and organizes the reasoning process into a coherent, user-friendly\noutput. We evaluate ToolScope on four VQA benchmarks across diverse domains,\nincluding VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong\ngeneralization capabilities, achieving an average performance improvement of up\nto +6.69% across all datasets.",
            "upvotes": 17,
            "discussionId": "6908501e812eca10f9cc5f01",
            "githubRepo": "https://github.com/dengmengjie/ToolScope",
            "ai_summary": "ToolScope, an agentic framework for multimodal large language models, enhances visual question answering by integrating external tools and achieving significant performance improvements across various benchmarks.",
            "ai_keywords": [
                "large language models",
                "multimodal large language models",
                "ToolScope",
                "Global Navigator",
                "Agentic Executor",
                "Response Synthesizer",
                "VQA",
                "visual context degradation",
                "long-horizon VQA",
                "Search",
                "Code",
                "Perceive",
                "VQA 2.0",
                "ScienceQA",
                "MAT-Search",
                "MathVista"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-10-31T06:51:27.000Z",
        "title": "ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool\n  Use",
        "summary": "Recently, large language models (LLMs) have demonstrated remarkable\nproblem-solving capabilities by autonomously integrating with external tools\nfor collaborative reasoning. However, due to the inherently complex and diverse\nnature of multimodal information, enabling multimodal large language models\n(MLLMs) to flexibly and efficiently utilize external tools during reasoning\nremains an underexplored challenge. In this work, we introduce ToolScope, an\nagentic framework designed to unify global planning with local multimodal\nperception, adopting a specialized Perceive tool to mitigates visual context\ndegradation in long-horizon VQA task. ToolScope comprises three primary\ncomponents: the Global Navigator, the Agentic Executor, and the Response\nSynthesizer. The Global Navigator functions as a \"telescope\", offering\nhigh-level strategic guidance. The Agentic Executor operates iteratively to\naugment MLLM with local perception through the integration of external\ntools-Search, Code, and Perceive. Finally, the Response Synthesizer\nconsolidates and organizes the reasoning process into a coherent, user-friendly\noutput. We evaluate ToolScope on four VQA benchmarks across diverse domains,\nincluding VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong\ngeneralization capabilities, achieving an average performance improvement of up\nto +6.69% across all datasets.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27363.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65db23d1f386d08eb0d1cec5",
            "avatarUrl": "/avatars/b495ec5b35b15fea245ef490b83d1856.svg",
            "fullname": "Mengjie Deng",
            "name": "MengjieDeng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.00602",
            "authors": [
                {
                    "_id": "6909a672812eca10f9cc63f7",
                    "name": "Wai-Chung Kwan",
                    "hidden": false
                },
                {
                    "_id": "6909a672812eca10f9cc63f8",
                    "name": "Joshua Ong Jun Leang",
                    "hidden": false
                },
                {
                    "_id": "6909a672812eca10f9cc63f9",
                    "name": "Pavlos Vougiouklis",
                    "hidden": false
                },
                {
                    "_id": "6909a672812eca10f9cc63fa",
                    "name": "Jeff Z. Pan",
                    "hidden": false
                },
                {
                    "_id": "6909a672812eca10f9cc63fb",
                    "name": "Marco Valentino",
                    "hidden": false
                },
                {
                    "_id": "6909a672812eca10f9cc63fc",
                    "user": {
                        "_id": "61001311e043e15c13412d30",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61001311e043e15c13412d30/6yAbTweYR16XtxMBEyOWl.png",
                        "isPro": false,
                        "fullname": "Pasquale Minervini",
                        "user": "pminervini",
                        "type": "user"
                    },
                    "name": "Pasquale Minervini",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T09:24:52.430Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-01T16:08:28.000Z",
            "submittedOnDailyAt": "2025-11-04T04:40:55.345Z",
            "title": "OpenSIR: Open-Ended Self-Improving Reasoner",
            "submittedOnDailyBy": {
                "_id": "61001311e043e15c13412d30",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61001311e043e15c13412d30/6yAbTweYR16XtxMBEyOWl.png",
                "isPro": false,
                "fullname": "Pasquale Minervini",
                "user": "pminervini",
                "type": "user"
            },
            "summary": "Recent advances in large language model (LLM) reasoning through reinforcement\nlearning rely on annotated datasets for verifiable rewards, which may limit\nmodels' ability to surpass human-level performance. While self-play offers a\npromising alternative, existing approaches depend on external verifiers or\ncannot learn open-endedly. We present Open-Ended Self-Improving Reasoner\n(OpenSIR), a self-play framework where an LLM learns to generate and solve\nnovel problems by alternating teacher and student roles without external\nsupervision. To generate novel problems, OpenSIR optimises for both difficulty\nand diversity, rewarding problems that challenge appropriately while exploring\ndistinct concepts, enabling open-ended mathematical discovery. Starting from a\nsingle trivial seed problem, OpenSIR substantially improves instruction models:\nLlama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to\n34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on\nGSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through\nco-evolving teacher-student roles that adaptively calibrate difficulty and\ndrive diverse exploration, progressing autonomously from basic to advanced\nmathematics.",
            "upvotes": 14,
            "discussionId": "6909a672812eca10f9cc63fd",
            "githubRepo": "https://github.com/EdinburghNLP/OpenSIR",
            "ai_summary": "OpenSIR is a self-play framework that enables large language models to improve their reasoning abilities through open-ended problem generation and solving without external supervision.",
            "ai_keywords": [
                "large language model",
                "reinforcement learning",
                "annotated datasets",
                "self-play",
                "Open-Ended Self-Improving Reasoner",
                "OpenSIR",
                "teacher-student roles",
                "open-ended learning",
                "mathematical discovery",
                "GSM8K",
                "College Math",
                "Gemma-2-2B-Instruct",
                "Llama-3.2-3B-Instruct"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "652e72b5fd5e3a357cf6f844",
                "name": "EdinburghNLP",
                "fullname": "EdinburghNLP - Natural Language Processing Group at the University of Edinburgh",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5fbfd09ee366524fe8e97cd3/KBva4SboTuDXRdYqWZsCX.png"
            }
        },
        "publishedAt": "2025-11-01T12:08:28.000Z",
        "title": "OpenSIR: Open-Ended Self-Improving Reasoner",
        "summary": "Recent advances in large language model (LLM) reasoning through reinforcement\nlearning rely on annotated datasets for verifiable rewards, which may limit\nmodels' ability to surpass human-level performance. While self-play offers a\npromising alternative, existing approaches depend on external verifiers or\ncannot learn open-endedly. We present Open-Ended Self-Improving Reasoner\n(OpenSIR), a self-play framework where an LLM learns to generate and solve\nnovel problems by alternating teacher and student roles without external\nsupervision. To generate novel problems, OpenSIR optimises for both difficulty\nand diversity, rewarding problems that challenge appropriately while exploring\ndistinct concepts, enabling open-ended mathematical discovery. Starting from a\nsingle trivial seed problem, OpenSIR substantially improves instruction models:\nLlama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to\n34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on\nGSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through\nco-evolving teacher-student roles that adaptively calibrate difficulty and\ndrive diverse exploration, progressing autonomously from basic to advanced\nmathematics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.00602.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "61001311e043e15c13412d30",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61001311e043e15c13412d30/6yAbTweYR16XtxMBEyOWl.png",
            "fullname": "Pasquale Minervini",
            "name": "pminervini",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 45
        },
        "organization": {
            "_id": "652e72b5fd5e3a357cf6f844",
            "name": "EdinburghNLP",
            "fullname": "EdinburghNLP - Natural Language Processing Group at the University of Edinburgh",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5fbfd09ee366524fe8e97cd3/KBva4SboTuDXRdYqWZsCX.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.27545",
            "authors": [
                {
                    "_id": "690a1d7dd70e173c84528e6a",
                    "name": "Travis Davies",
                    "hidden": false
                },
                {
                    "_id": "690a1d7dd70e173c84528e6b",
                    "name": "Yiqi Huang",
                    "hidden": false
                },
                {
                    "_id": "690a1d7dd70e173c84528e6c",
                    "name": "Alexi Gladstone",
                    "hidden": false
                },
                {
                    "_id": "690a1d7dd70e173c84528e6d",
                    "name": "Yunxin Liu",
                    "hidden": false
                },
                {
                    "_id": "690a1d7dd70e173c84528e6e",
                    "name": "Xiang Chen",
                    "hidden": false
                },
                {
                    "_id": "690a1d7dd70e173c84528e6f",
                    "name": "Heng Ji",
                    "hidden": false
                },
                {
                    "_id": "690a1d7dd70e173c84528e70",
                    "name": "Huxian Liu",
                    "hidden": false
                },
                {
                    "_id": "690a1d7dd70e173c84528e71",
                    "name": "Luhui Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-31T15:21:05.000Z",
            "submittedOnDailyAt": "2025-11-04T13:09:04.225Z",
            "title": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities",
            "submittedOnDailyBy": {
                "_id": "63fbbff41b4b1bd4e7042fd1",
                "avatarUrl": "/avatars/d186d0859046b35723fe1156d8f68917.svg",
                "isPro": false,
                "fullname": "Alexi Gladstone",
                "user": "alexiglad",
                "type": "user"
            },
            "summary": "Implicit policies parameterized by generative models, such as Diffusion\nPolicy, have become the standard for policy learning and Vision-Language-Action\n(VLA) models in robotics. However, these approaches often suffer from high\ncomputational cost, exposure bias, and unstable inference dynamics, which lead\nto divergence under distribution shifts. Energy-Based Models (EBMs) address\nthese issues by learning energy landscapes end-to-end and modeling equilibrium\ndynamics, offering improved robustness and reduced exposure bias. Yet, policies\nparameterized by EBMs have historically struggled to scale effectively. Recent\nwork on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs\nto high-dimensional spaces, but their potential for solving core challenges in\nphysically embodied models remains underexplored. We introduce a new\nenergy-based architecture, EBT-Policy, that solves core issues in robotic and\nreal-world settings. Across simulated and real-world tasks, EBT-Policy\nconsistently outperforms diffusion-based policies, while requiring less\ntraining and inference computation. Remarkably, on some tasks it converges\nwithin just two inference steps, a 50x reduction compared to Diffusion Policy's\n100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior\nmodels, such as zero-shot recovery from failed action sequences using only\nbehavior cloning and without explicit retry training. By leveraging its scalar\nenergy for uncertainty-aware inference and dynamic compute allocation,\nEBT-Policy offers a promising path toward robust, generalizable robot behavior\nunder distribution shifts.",
            "upvotes": 14,
            "discussionId": "690a1d7dd70e173c84528e72",
            "projectPage": "https://energy-based-transformers.github.io/ebt-policy/",
            "ai_summary": "EBT-Policy, an energy-based architecture, outperforms diffusion-based policies in robotic tasks by offering improved robustness, reduced computational cost, and emergent zero-shot recovery capabilities.",
            "ai_keywords": [
                "Diffusion Policy",
                "Energy-Based Models",
                "EBMs",
                "Energy-Based Transformers",
                "EBTs",
                "EBT-Policy",
                "energy landscapes",
                "equilibrium dynamics",
                "distribution shifts",
                "zero-shot recovery",
                "behavior cloning",
                "uncertainty-aware inference",
                "dynamic compute allocation"
            ]
        },
        "publishedAt": "2025-10-31T11:21:05.000Z",
        "title": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities",
        "summary": "Implicit policies parameterized by generative models, such as Diffusion\nPolicy, have become the standard for policy learning and Vision-Language-Action\n(VLA) models in robotics. However, these approaches often suffer from high\ncomputational cost, exposure bias, and unstable inference dynamics, which lead\nto divergence under distribution shifts. Energy-Based Models (EBMs) address\nthese issues by learning energy landscapes end-to-end and modeling equilibrium\ndynamics, offering improved robustness and reduced exposure bias. Yet, policies\nparameterized by EBMs have historically struggled to scale effectively. Recent\nwork on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs\nto high-dimensional spaces, but their potential for solving core challenges in\nphysically embodied models remains underexplored. We introduce a new\nenergy-based architecture, EBT-Policy, that solves core issues in robotic and\nreal-world settings. Across simulated and real-world tasks, EBT-Policy\nconsistently outperforms diffusion-based policies, while requiring less\ntraining and inference computation. Remarkably, on some tasks it converges\nwithin just two inference steps, a 50x reduction compared to Diffusion Policy's\n100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior\nmodels, such as zero-shot recovery from failed action sequences using only\nbehavior cloning and without explicit retry training. By leveraging its scalar\nenergy for uncertainty-aware inference and dynamic compute allocation,\nEBT-Policy offers a promising path toward robust, generalizable robot behavior\nunder distribution shifts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27545.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63fbbff41b4b1bd4e7042fd1",
            "avatarUrl": "/avatars/d186d0859046b35723fe1156d8f68917.svg",
            "fullname": "Alexi Gladstone",
            "name": "alexiglad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.24794",
            "authors": [
                {
                    "_id": "69081d1b812eca10f9cc5e6c",
                    "user": {
                        "_id": "66e4221d99e55519e9be00b6",
                        "avatarUrl": "/avatars/aa821e632ce314cb636c3a1469d61017.svg",
                        "isPro": false,
                        "fullname": "Wang",
                        "user": "PeterKKQ",
                        "type": "user"
                    },
                    "name": "Xinming Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:52:19.280Z",
                    "hidden": false
                },
                {
                    "_id": "69081d1b812eca10f9cc5e6d",
                    "name": "Jian Xu",
                    "hidden": false
                },
                {
                    "_id": "69081d1b812eca10f9cc5e6e",
                    "name": "Bin Yu",
                    "hidden": false
                },
                {
                    "_id": "69081d1b812eca10f9cc5e6f",
                    "name": "Sheng Lian",
                    "hidden": false
                },
                {
                    "_id": "69081d1b812eca10f9cc5e70",
                    "user": {
                        "_id": "641eec9973cfc036ddb9a22e",
                        "avatarUrl": "/avatars/b17b0293bc47ffc65585732e32db66f8.svg",
                        "isPro": false,
                        "fullname": "",
                        "user": "hongzhuyi",
                        "type": "user"
                    },
                    "name": "Hongzhu Yi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:27:09.213Z",
                    "hidden": false
                },
                {
                    "_id": "69081d1b812eca10f9cc5e71",
                    "name": "Yi Chen",
                    "hidden": false
                },
                {
                    "_id": "69081d1b812eca10f9cc5e72",
                    "name": "Yingjian Zhu",
                    "hidden": false
                },
                {
                    "_id": "69081d1b812eca10f9cc5e73",
                    "name": "Boran Wang",
                    "hidden": false
                },
                {
                    "_id": "69081d1b812eca10f9cc5e74",
                    "name": "Hongming Yang",
                    "hidden": false
                },
                {
                    "_id": "69081d1b812eca10f9cc5e75",
                    "name": "Han Hu",
                    "hidden": false
                },
                {
                    "_id": "69081d1b812eca10f9cc5e76",
                    "name": "Xu-Yao Zhang",
                    "hidden": false
                },
                {
                    "_id": "69081d1b812eca10f9cc5e77",
                    "name": "Cheng-Lin Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T15:00:54.000Z",
            "submittedOnDailyAt": "2025-11-04T12:05:19.529Z",
            "title": "MR-Align: Meta-Reasoning Informed Factuality Alignment for Large\n  Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "66e4221d99e55519e9be00b6",
                "avatarUrl": "/avatars/aa821e632ce314cb636c3a1469d61017.svg",
                "isPro": false,
                "fullname": "Wang",
                "user": "PeterKKQ",
                "type": "user"
            },
            "summary": "Large reasoning models (LRMs) show strong capabilities in complex reasoning,\nyet their marginal gains on evidence-dependent factual questions are limited.\nWe find this limitation is partially attributable to a reasoning-answer hit\ngap, where the model identifies the correct facts during reasoning but fails to\nincorporate them into the final response, thereby reducing factual fidelity. To\naddress this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment\nframework that enhances factuality without relying on external verifiers.\nMR-ALIGN quantifies state transition probabilities along the model's thinking\nprocess and constructs a transition-aware implicit reward that reinforces\nbeneficial reasoning patterns while suppressing defective ones at the atomic\nthinking segments. This re-weighting reshapes token-level signals into\nprobability-aware segment scores, encouraging coherent reasoning trajectories\nthat are more conducive to factual correctness. Empirical evaluations across\nfour factual QA datasets and one long-form factuality benchmark show that\nMR-ALIGN consistently improves accuracy and truthfulness while reducing\nmisleading reasoning. These results highlight that aligning the reasoning\nprocess itself, rather than merely the outputs, is pivotal for advancing\nfactuality in LRMs.",
            "upvotes": 14,
            "discussionId": "69081d1c812eca10f9cc5e78",
            "ai_summary": "MR-ALIGN, a Meta-Reasoning informed alignment framework, enhances the factuality of large reasoning models by aligning their reasoning process, improving accuracy and reducing misleading reasoning.",
            "ai_keywords": [
                "Large reasoning models",
                "reasoning-answer hit gap",
                "Meta-Reasoning",
                "MR-ALIGN",
                "state transition probabilities",
                "transition-aware implicit reward",
                "token-level signals",
                "probability-aware segment scores",
                "coherent reasoning trajectories",
                "factual QA datasets",
                "long-form factuality benchmark"
            ]
        },
        "publishedAt": "2025-10-27T11:00:54.000Z",
        "title": "MR-Align: Meta-Reasoning Informed Factuality Alignment for Large\n  Reasoning Models",
        "summary": "Large reasoning models (LRMs) show strong capabilities in complex reasoning,\nyet their marginal gains on evidence-dependent factual questions are limited.\nWe find this limitation is partially attributable to a reasoning-answer hit\ngap, where the model identifies the correct facts during reasoning but fails to\nincorporate them into the final response, thereby reducing factual fidelity. To\naddress this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment\nframework that enhances factuality without relying on external verifiers.\nMR-ALIGN quantifies state transition probabilities along the model's thinking\nprocess and constructs a transition-aware implicit reward that reinforces\nbeneficial reasoning patterns while suppressing defective ones at the atomic\nthinking segments. This re-weighting reshapes token-level signals into\nprobability-aware segment scores, encouraging coherent reasoning trajectories\nthat are more conducive to factual correctness. Empirical evaluations across\nfour factual QA datasets and one long-form factuality benchmark show that\nMR-ALIGN consistently improves accuracy and truthfulness while reducing\nmisleading reasoning. These results highlight that aligning the reasoning\nprocess itself, rather than merely the outputs, is pivotal for advancing\nfactuality in LRMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24794.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66e4221d99e55519e9be00b6",
            "avatarUrl": "/avatars/aa821e632ce314cb636c3a1469d61017.svg",
            "fullname": "Wang",
            "name": "PeterKKQ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.27571",
            "authors": [
                {
                    "_id": "6908681a812eca10f9cc5f5a",
                    "user": {
                        "_id": "6746b1e2224b22ef67fbff11",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6746b1e2224b22ef67fbff11/q6sN8lQjtLGCgen_41VxU.jpeg",
                        "isPro": false,
                        "fullname": "Zhuoning Guo",
                        "user": "Zhuoning",
                        "type": "user"
                    },
                    "name": "Zhuoning Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:27:05.194Z",
                    "hidden": false
                },
                {
                    "_id": "6908681a812eca10f9cc5f5b",
                    "name": "Mingxin Li",
                    "hidden": false
                },
                {
                    "_id": "6908681a812eca10f9cc5f5c",
                    "name": "Yanzhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6908681a812eca10f9cc5f5d",
                    "name": "Dingkun Long",
                    "hidden": false
                },
                {
                    "_id": "6908681a812eca10f9cc5f5e",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "6908681a812eca10f9cc5f5f",
                    "name": "Xiaowen Chu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-31T15:54:48.000Z",
            "submittedOnDailyAt": "2025-11-04T01:06:37.116Z",
            "title": "Towards Universal Video Retrieval: Generalizing Video Embedding via\n  Synthesized Multimodal Pyramid Curriculum",
            "submittedOnDailyBy": {
                "_id": "64dc29d9b5d625e0e9a6ecb9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
                "isPro": false,
                "fullname": "Tingyu Song",
                "user": "songtingyu",
                "type": "user"
            },
            "summary": "The prevailing video retrieval paradigm is structurally misaligned, as narrow\nbenchmarks incentivize correspondingly limited data and single-task training.\nTherefore, universal capability is suppressed due to the absence of a\ndiagnostic evaluation that defines and demands multi-dimensional\ngeneralization. To break this cycle, we introduce a framework built on the\nco-design of evaluation, data, and modeling. First, we establish the Universal\nVideo Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to\nmeasure performance but also to diagnose critical capability gaps across tasks\nand domains. Second, guided by UVRB's diagnostics, we introduce a scalable\nsynthesis workflow that generates 1.55 million high-quality pairs to populate\nthe semantic space required for universality. Finally, we devise the Modality\nPyramid, a curriculum that trains our General Video Embedder (GVE) by\nexplicitly leveraging the latent interconnections within our diverse data.\nExtensive experiments show GVE achieves state-of-the-art zero-shot\ngeneralization on UVRB. In particular, our analysis reveals that popular\nbenchmarks are poor predictors of general ability and that partially relevant\nretrieval is a dominant but overlooked scenario. Overall, our co-designed\nframework provides a practical path to escape the limited scope and advance\ntoward truly universal video retrieval.",
            "upvotes": 13,
            "discussionId": "6908681b812eca10f9cc5f60",
            "projectPage": "https://gzn00417.github.io/GVE/",
            "ai_summary": "A framework combining a diagnostic benchmark, data synthesis, and a modality pyramid curriculum achieves state-of-the-art zero-shot generalization in video retrieval.",
            "ai_keywords": [
                "Universal Video Retrieval Benchmark",
                "UVRB",
                "scalable synthesis workflow",
                "Modality Pyramid",
                "General Video Embedder",
                "GVE",
                "zero-shot generalization"
            ],
            "organization": {
                "_id": "661f98de142a51d630dbbcc4",
                "name": "Alibaba-NLP",
                "fullname": "Alibaba-NLP",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/dfd_EcIfylvu3sdc2WMqX.png"
            }
        },
        "publishedAt": "2025-10-31T11:54:48.000Z",
        "title": "Towards Universal Video Retrieval: Generalizing Video Embedding via\n  Synthesized Multimodal Pyramid Curriculum",
        "summary": "The prevailing video retrieval paradigm is structurally misaligned, as narrow\nbenchmarks incentivize correspondingly limited data and single-task training.\nTherefore, universal capability is suppressed due to the absence of a\ndiagnostic evaluation that defines and demands multi-dimensional\ngeneralization. To break this cycle, we introduce a framework built on the\nco-design of evaluation, data, and modeling. First, we establish the Universal\nVideo Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to\nmeasure performance but also to diagnose critical capability gaps across tasks\nand domains. Second, guided by UVRB's diagnostics, we introduce a scalable\nsynthesis workflow that generates 1.55 million high-quality pairs to populate\nthe semantic space required for universality. Finally, we devise the Modality\nPyramid, a curriculum that trains our General Video Embedder (GVE) by\nexplicitly leveraging the latent interconnections within our diverse data.\nExtensive experiments show GVE achieves state-of-the-art zero-shot\ngeneralization on UVRB. In particular, our analysis reveals that popular\nbenchmarks are poor predictors of general ability and that partially relevant\nretrieval is a dominant but overlooked scenario. Overall, our co-designed\nframework provides a practical path to escape the limited scope and advance\ntoward truly universal video retrieval.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27571.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64dc29d9b5d625e0e9a6ecb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
            "fullname": "Tingyu Song",
            "name": "songtingyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "661f98de142a51d630dbbcc4",
            "name": "Alibaba-NLP",
            "fullname": "Alibaba-NLP",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/dfd_EcIfylvu3sdc2WMqX.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.01833",
            "authors": [
                {
                    "_id": "6909788f812eca10f9cc6257",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "6909788f812eca10f9cc6258",
                    "name": "Jike Zhong",
                    "hidden": false
                },
                {
                    "_id": "6909788f812eca10f9cc6259",
                    "user": {
                        "_id": "62c66504031996c36c86976a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png",
                        "isPro": false,
                        "fullname": "steve z",
                        "user": "stzhao",
                        "type": "user"
                    },
                    "name": "Shitian Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:25:00.119Z",
                    "hidden": false
                },
                {
                    "_id": "6909788f812eca10f9cc625a",
                    "user": {
                        "_id": "67ff7f687351095d4b606b84",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67ff7f687351095d4b606b84/KhNPmbBC3zghuP5h1MK-c.png",
                        "isPro": false,
                        "fullname": "Haoquan Zhang",
                        "user": "haoquan03",
                        "type": "user"
                    },
                    "name": "Haoquan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:25:02.529Z",
                    "hidden": false
                },
                {
                    "_id": "6909788f812eca10f9cc625b",
                    "name": "Shaoheng Lin",
                    "hidden": false
                },
                {
                    "_id": "6909788f812eca10f9cc625c",
                    "name": "Yuxiang Lai",
                    "hidden": false
                },
                {
                    "_id": "6909788f812eca10f9cc625d",
                    "name": "Wei Chen",
                    "hidden": false
                },
                {
                    "_id": "6909788f812eca10f9cc625e",
                    "name": "Konstantinos Psounis",
                    "hidden": false
                },
                {
                    "_id": "6909788f812eca10f9cc625f",
                    "name": "Kaipeng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-03T18:40:17.000Z",
            "submittedOnDailyAt": "2025-11-04T01:22:59.659Z",
            "title": "TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The frontier of visual reasoning is shifting toward models like OpenAI o3,\nwhich can intelligently create and operate tools to transform images for\nproblem-solving, also known as thinking-with-images in\nchain-of-thought. Yet existing benchmarks fail to fully capture this advanced\ncapability. Even Visual Search, the most common benchmark for current\nthinking-with-images methods, tests only basic operations such as\nlocalization and cropping, offering little insight into more complex, dynamic,\nand tool-dependent reasoning. We introduce TIR-Bench, a comprehensive\nbenchmark for evaluating agentic thinking-with-images across 13 diverse tasks,\neach requiring novel tool use for image processing and manipulation in\nchain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from\nleading open-sourced and proprietary models to those with explicit tool-use\naugmentation. Results show that TIR-Bench is universally challenging, and\nstrong performance requires genuine thinking-with-images capabilities. Finally,\nwe present a pilot study comparing direct versus agentic fine-tuning.",
            "upvotes": 12,
            "discussionId": "6909788f812eca10f9cc6260",
            "githubRepo": "https://github.com/agents-x-project/TIR-Bench",
            "ai_summary": "TIR-Bench evaluates advanced visual reasoning capabilities in multimodal models through diverse tasks requiring tool use and chain-of-thought, demonstrating the need for genuine thinking-with-images.",
            "ai_keywords": [
                "OpenAI o3",
                "thinking-with-images",
                "chain-of-thought",
                "TIR-Bench",
                "multimodal large language models",
                "MLLMs",
                "tool-use augmentation",
                "direct fine-tuning",
                "agentic fine-tuning"
            ],
            "githubStars": 14
        },
        "publishedAt": "2025-11-03T13:40:17.000Z",
        "title": "TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images\n  Reasoning",
        "summary": "The frontier of visual reasoning is shifting toward models like OpenAI o3,\nwhich can intelligently create and operate tools to transform images for\nproblem-solving, also known as thinking-with-images in\nchain-of-thought. Yet existing benchmarks fail to fully capture this advanced\ncapability. Even Visual Search, the most common benchmark for current\nthinking-with-images methods, tests only basic operations such as\nlocalization and cropping, offering little insight into more complex, dynamic,\nand tool-dependent reasoning. We introduce TIR-Bench, a comprehensive\nbenchmark for evaluating agentic thinking-with-images across 13 diverse tasks,\neach requiring novel tool use for image processing and manipulation in\nchain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from\nleading open-sourced and proprietary models to those with explicit tool-use\naugmentation. Results show that TIR-Bench is universally challenging, and\nstrong performance requires genuine thinking-with-images capabilities. Finally,\nwe present a pilot study comparing direct versus agentic fine-tuning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01833.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 154
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.00279",
            "authors": [
                {
                    "_id": "690979c6812eca10f9cc6278",
                    "name": "Meituan LongCat Team",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6279",
                    "name": "Bairui Wang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc627a",
                    "name": "Bayan",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc627b",
                    "name": "Bin Xiao",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc627c",
                    "name": "Bo Zhang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc627d",
                    "name": "Bolin Rong",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc627e",
                    "name": "Borun Chen",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc627f",
                    "name": "Chang Wan",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6280",
                    "name": "Chao Zhang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6281",
                    "name": "Chen Huang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6282",
                    "name": "Chen Chen",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6283",
                    "name": "Chen Chen",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6284",
                    "name": "Chengxu Yang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6285",
                    "name": "Chengzuo Yang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6286",
                    "name": "Cong Han",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6287",
                    "name": "Dandan Peng",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6288",
                    "name": "Delian Ruan",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6289",
                    "name": "Detai Xin",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc628a",
                    "name": "Disong Wang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc628b",
                    "name": "Dongchao Yang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc628c",
                    "name": "Fanfan Liu",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc628d",
                    "name": "Fengjiao Chen",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc628e",
                    "name": "Fengyu Yang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc628f",
                    "name": "Gan Dong",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6290",
                    "name": "Gang Huang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6291",
                    "name": "Gang Xu",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6292",
                    "name": "Guanglu Wan",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6293",
                    "name": "Guoqiang Tan",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6294",
                    "name": "Guoqiao Yu",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6295",
                    "name": "Haibo Qiu",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6296",
                    "name": "Hao Lu",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6297",
                    "name": "Hongbo Liu",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6298",
                    "name": "Hongyu Xiang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc6299",
                    "name": "Jiaheng Wu",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc629a",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc629b",
                    "name": "Jiaxing Liu",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc629c",
                    "name": "Jing Huang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc629d",
                    "name": "Jingang Wang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc629e",
                    "name": "Jinrui Ding",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc629f",
                    "name": "Juchao Jiang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62a0",
                    "name": "Jun Kuang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62a1",
                    "name": "Jun Wang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62a2",
                    "name": "Junhui Mei",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62a3",
                    "name": "Ke Ding",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62a4",
                    "name": "Kefeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62a5",
                    "name": "Lei Chen",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62a6",
                    "name": "Liang Shi",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62a7",
                    "name": "Limeng Qiao",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62a8",
                    "name": "Liming Zheng",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62a9",
                    "name": "Lin Ma",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62aa",
                    "name": "Liuyang Guo",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62ab",
                    "name": "Liya Ma",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62ac",
                    "name": "Luying Sun",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62ad",
                    "name": "Man Gao",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62ae",
                    "name": "Mengshen Zhu",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62af",
                    "name": "Miao Cao",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62b0",
                    "name": "Minliang Lin",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62b1",
                    "name": "Nuo Xu",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62b2",
                    "name": "Peng Shi",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62b3",
                    "name": "Qi Zhang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62b4",
                    "name": "Qian Fang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62b5",
                    "name": "Qian Wang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62b6",
                    "name": "Qian Yang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62b7",
                    "name": "Quanxiu Wang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62b8",
                    "name": "Rongxiang Weng",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62b9",
                    "name": "Rongxin Guo",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62ba",
                    "name": "Ruoxuan Liang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62bb",
                    "name": "Senbin Yang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62bc",
                    "name": "Shanbo Xu",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62bd",
                    "name": "Shanglin Lei",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62be",
                    "name": "Shengze Ye",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62bf",
                    "name": "Shimin Chen",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62c0",
                    "name": "Shuaiqi Chen",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62c1",
                    "name": "Shujie Hu",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62c2",
                    "name": "Shuo Li",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62c3",
                    "name": "Siqi Yang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62c4",
                    "name": "Siyu Xu",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62c5",
                    "name": "Siyu Ren",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62c6",
                    "name": "Song Li",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62c7",
                    "name": "Songxiang Liu",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62c8",
                    "name": "Tianhao Bai",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62c9",
                    "name": "Tianye Dai",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62ca",
                    "name": "Wei Hong",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62cb",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62cc",
                    "name": "Weixiao Zhao",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62cd",
                    "name": "Wengang Cao",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62ce",
                    "name": "Wenlong Zhu",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62cf",
                    "name": "Wenlong He",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62d0",
                    "name": "Xi Su",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62d1",
                    "name": "Xi Nan",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62d2",
                    "name": "Xiaohan Zhao",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62d3",
                    "name": "Xiaohao Wang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62d4",
                    "name": "Xiaoyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62d5",
                    "name": "Xiaoyu Wang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62d6",
                    "name": "Xiaoyu Li",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62d7",
                    "name": "Xin Pan",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62d8",
                    "name": "Xin Chen",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62d9",
                    "name": "Xiusong Sun",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62da",
                    "name": "Xu Xiang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62db",
                    "name": "Xudong Xing",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62dc",
                    "name": "Xuezhi Cao",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62dd",
                    "name": "Xunliang Cai",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62de",
                    "name": "Yang Yang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62df",
                    "name": "Yanli Tan",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62e0",
                    "name": "Yao Yao",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62e1",
                    "name": "Yerui Sun",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62e2",
                    "name": "Yi Chen",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62e3",
                    "name": "Yifan Lu",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62e4",
                    "name": "Yin Gong",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62e5",
                    "name": "Yining Zhang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62e6",
                    "name": "Yitian Chen",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62e7",
                    "name": "Yiyang Gan",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62e8",
                    "name": "Yuchen Tang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62e9",
                    "name": "Yuchen Xie",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62ea",
                    "name": "Yueqian Wang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62eb",
                    "name": "Yuewen Zheng",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62ec",
                    "name": "Yufei Zhang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62ed",
                    "name": "Yufeng Zhong",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62ee",
                    "name": "Yulei Qian",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62ef",
                    "name": "Yuqi Peng",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62f0",
                    "name": "Yuwei Jiang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62f1",
                    "name": "Zeyang Hu",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62f2",
                    "name": "Zheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62f3",
                    "name": "Zhengkun Tian",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62f4",
                    "name": "Zhiqing Hong",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62f5",
                    "name": "Zhixiong Zeng",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62f6",
                    "name": "Zhuqi Mi",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62f7",
                    "name": "Ziran Li",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62f8",
                    "name": "Ziwen Wang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62f9",
                    "name": "Ziyi Zhao",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62fa",
                    "name": "Ziyuan Zhuang",
                    "hidden": false
                },
                {
                    "_id": "690979c6812eca10f9cc62fb",
                    "name": "Zizhe Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-31T21:58:15.000Z",
            "submittedOnDailyAt": "2025-11-04T01:28:08.212Z",
            "title": "LongCat-Flash-Omni Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal\nmodel with 560 billion parameters, excelling at real-time audio-visual\ninteraction. By adopting a curriculum-inspired progressive training strategy\nthat transitions from simpler to increasingly complex modality sequence\nmodeling tasks, LongCat-Flash-Omni attains comprehensive multimodal\ncapabilities while maintaining strong unimodal capability. Building upon\nLongCat-Flash, which adopts a high-performance Shortcut-connected\nMixture-of-Experts (MoE) architecture with zero-computation experts,\nLongCat-Flash-Omni integrates efficient multimodal perception and speech\nreconstruction modules. Despite its immense size of 560B parameters (with 27B\nactivated), LongCat-Flash-Omni achieves low-latency real-time audio-visual\ninteraction. For training infrastructure, we developed a modality-decoupled\nparallelism scheme specifically designed to manage the data and model\nheterogeneity inherent in large-scale multimodal training. This innovative\napproach demonstrates exceptional efficiency by sustaining over 90% of the\nthroughput achieved by text-only training. Extensive evaluations show that\nLongCat-Flash-Omni achieves state-of-the-art performance on omni-modal\nbenchmarks among open-source models. Furthermore, it delivers highly\ncompetitive results across a wide range of modality-specific tasks, including\ntext, image, and video understanding, as well as audio understanding and\ngeneration. We provide a comprehensive overview of the model architecture\ndesign, training procedures, and data strategies, and open-source the model to\nfoster future research and development in the community.",
            "upvotes": 12,
            "discussionId": "690979c6812eca10f9cc62fc",
            "ai_summary": "LongCat-Flash-Omni, a 560 billion parameter omni-modal model, achieves real-time audio-visual interaction through curriculum-inspired training and modality-decoupled parallelism.",
            "ai_keywords": [
                "curriculum-inspired progressive training",
                "Shortcut-connected Mixture-of-Experts (MoE)",
                "zero-computation experts",
                "multimodal perception",
                "speech reconstruction",
                "modality-decoupled parallelism",
                "throughput",
                "omni-modal benchmarks",
                "text understanding",
                "image understanding",
                "video understanding",
                "audio understanding",
                "audio generation"
            ]
        },
        "publishedAt": "2025-10-31T17:58:15.000Z",
        "title": "LongCat-Flash-Omni Technical Report",
        "summary": "We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal\nmodel with 560 billion parameters, excelling at real-time audio-visual\ninteraction. By adopting a curriculum-inspired progressive training strategy\nthat transitions from simpler to increasingly complex modality sequence\nmodeling tasks, LongCat-Flash-Omni attains comprehensive multimodal\ncapabilities while maintaining strong unimodal capability. Building upon\nLongCat-Flash, which adopts a high-performance Shortcut-connected\nMixture-of-Experts (MoE) architecture with zero-computation experts,\nLongCat-Flash-Omni integrates efficient multimodal perception and speech\nreconstruction modules. Despite its immense size of 560B parameters (with 27B\nactivated), LongCat-Flash-Omni achieves low-latency real-time audio-visual\ninteraction. For training infrastructure, we developed a modality-decoupled\nparallelism scheme specifically designed to manage the data and model\nheterogeneity inherent in large-scale multimodal training. This innovative\napproach demonstrates exceptional efficiency by sustaining over 90% of the\nthroughput achieved by text-only training. Extensive evaluations show that\nLongCat-Flash-Omni achieves state-of-the-art performance on omni-modal\nbenchmarks among open-source models. Furthermore, it delivers highly\ncompetitive results across a wide range of modality-specific tasks, including\ntext, image, and video understanding, as well as audio understanding and\ngeneration. We provide a comprehensive overview of the model architecture\ndesign, training procedures, and data strategies, and open-source the model to\nfoster future research and development in the community.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.00279.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 154
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.26909",
            "authors": [
                {
                    "_id": "69088fb9812eca10f9cc5f9c",
                    "user": {
                        "_id": "68381e8849895d52fe3363d3",
                        "avatarUrl": "/avatars/301b2e581dfa6a4dea29af1aae4a837e.svg",
                        "isPro": false,
                        "fullname": "Tim Windecker",
                        "user": "TimWindecker",
                        "type": "user"
                    },
                    "name": "Tim Windecker",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:51:32.197Z",
                    "hidden": false
                },
                {
                    "_id": "69088fb9812eca10f9cc5f9d",
                    "user": {
                        "_id": "66fa8b357aa2514b7fb320f3",
                        "avatarUrl": "/avatars/40e6aee914ce624703d26fc61aafb119.svg",
                        "isPro": false,
                        "fullname": "Manthan Patel",
                        "user": "manthanhp",
                        "type": "user"
                    },
                    "name": "Manthan Patel",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:27:03.103Z",
                    "hidden": false
                },
                {
                    "_id": "69088fb9812eca10f9cc5f9e",
                    "user": {
                        "_id": "650af93422ce64f22b619549",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/RBybzGRphbiV2MXYHrDoc.png",
                        "isPro": false,
                        "fullname": "Moritz Reuss",
                        "user": "mbreuss",
                        "type": "user"
                    },
                    "name": "Moritz Reuss",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:27:01.242Z",
                    "hidden": false
                },
                {
                    "_id": "69088fb9812eca10f9cc5f9f",
                    "name": "Richard Schwarzkopf",
                    "hidden": false
                },
                {
                    "_id": "69088fb9812eca10f9cc5fa0",
                    "name": "Cesar Cadena",
                    "hidden": false
                },
                {
                    "_id": "69088fb9812eca10f9cc5fa1",
                    "name": "Rudolf Lioutikov",
                    "hidden": false
                },
                {
                    "_id": "69088fb9812eca10f9cc5fa2",
                    "name": "Marco Hutter",
                    "hidden": false
                },
                {
                    "_id": "69088fb9812eca10f9cc5fa3",
                    "name": "Jonas Frey",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/650af93422ce64f22b619549/rjqBs2smbUo9MUUrrlj3R.png"
            ],
            "publishedAt": "2025-10-30T18:16:32.000Z",
            "submittedOnDailyAt": "2025-11-04T06:38:35.240Z",
            "title": "NaviTrace: Evaluating Embodied Navigation of Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "650af93422ce64f22b619549",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/RBybzGRphbiV2MXYHrDoc.png",
                "isPro": false,
                "fullname": "Moritz Reuss",
                "user": "mbreuss",
                "type": "user"
            },
            "summary": "Vision-language models demonstrate unprecedented performance and\ngeneralization across a wide range of tasks and scenarios. Integrating these\nfoundation models into robotic navigation systems opens pathways toward\nbuilding general-purpose robots. Yet, evaluating these models' navigation\ncapabilities remains constrained by costly real-world trials, overly simplified\nsimulations, and limited benchmarks. We introduce NaviTrace, a high-quality\nVisual Question Answering benchmark where a model receives an instruction and\nembodiment type (human, legged robot, wheeled robot, bicycle) and must output a\n2D navigation trace in image space. Across 1000 scenarios and more than 3000\nexpert traces, we systematically evaluate eight state-of-the-art VLMs using a\nnewly introduced semantic-aware trace score. This metric combines Dynamic Time\nWarping distance, goal endpoint error, and embodiment-conditioned penalties\nderived from per-pixel semantics and correlates with human preferences. Our\nevaluation reveals consistent gap to human performance caused by poor spatial\ngrounding and goal localization. NaviTrace establishes a scalable and\nreproducible benchmark for real-world robotic navigation. The benchmark and\nleaderboard can be found at\nhttps://leggedrobotics.github.io/navitrace_webpage/.",
            "upvotes": 10,
            "discussionId": "69088fb9812eca10f9cc5fa4",
            "projectPage": "https://leggedrobotics.github.io/navitrace_webpage/",
            "githubRepo": "https://github.com/leggedrobotics/navitrace_evaluation",
            "ai_summary": "NaviTrace is a Visual Question Answering benchmark for evaluating robotic navigation capabilities using a semantic-aware trace score across various scenarios and embodiment types.",
            "ai_keywords": [
                "vision-language models",
                "Visual Question Answering",
                "NaviTrace",
                "semantic-aware trace score",
                "Dynamic Time Warping distance",
                "goal endpoint error",
                "embodiment-conditioned penalties",
                "per-pixel semantics",
                "robotic navigation"
            ],
            "githubStars": 6,
            "organization": {
                "_id": "6798bc47495916be7c624bc2",
                "name": "leggedrobotics",
                "fullname": "Robotic Systems Lab - ETH Zrich",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b85828324dd1598bad739f/NxcyiB-gpFlU5sLTnepe2.png"
            }
        },
        "publishedAt": "2025-10-30T14:16:32.000Z",
        "title": "NaviTrace: Evaluating Embodied Navigation of Vision-Language Models",
        "summary": "Vision-language models demonstrate unprecedented performance and\ngeneralization across a wide range of tasks and scenarios. Integrating these\nfoundation models into robotic navigation systems opens pathways toward\nbuilding general-purpose robots. Yet, evaluating these models' navigation\ncapabilities remains constrained by costly real-world trials, overly simplified\nsimulations, and limited benchmarks. We introduce NaviTrace, a high-quality\nVisual Question Answering benchmark where a model receives an instruction and\nembodiment type (human, legged robot, wheeled robot, bicycle) and must output a\n2D navigation trace in image space. Across 1000 scenarios and more than 3000\nexpert traces, we systematically evaluate eight state-of-the-art VLMs using a\nnewly introduced semantic-aware trace score. This metric combines Dynamic Time\nWarping distance, goal endpoint error, and embodiment-conditioned penalties\nderived from per-pixel semantics and correlates with human preferences. Our\nevaluation reveals consistent gap to human performance caused by poor spatial\ngrounding and goal localization. NaviTrace establishes a scalable and\nreproducible benchmark for real-world robotic navigation. The benchmark and\nleaderboard can be found at\nhttps://leggedrobotics.github.io/navitrace_webpage/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/650af93422ce64f22b619549/rjqBs2smbUo9MUUrrlj3R.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26909.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "650af93422ce64f22b619549",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/RBybzGRphbiV2MXYHrDoc.png",
            "fullname": "Moritz Reuss",
            "name": "mbreuss",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "organization": {
            "_id": "6798bc47495916be7c624bc2",
            "name": "leggedrobotics",
            "fullname": "Robotic Systems Lab - ETH Zrich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b85828324dd1598bad739f/NxcyiB-gpFlU5sLTnepe2.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.01340",
            "authors": [
                {
                    "_id": "690a3af8d70e173c84528eda",
                    "name": "Trishanu Das",
                    "hidden": false
                },
                {
                    "_id": "690a3af8d70e173c84528edb",
                    "name": "Abhilash Nandy",
                    "hidden": false
                },
                {
                    "_id": "690a3af8d70e173c84528edc",
                    "name": "Khush Bajaj",
                    "hidden": false
                },
                {
                    "_id": "690a3af8d70e173c84528edd",
                    "name": "Deepiha S",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-03T08:42:59.000Z",
            "submittedOnDailyAt": "2025-11-04T15:19:08.189Z",
            "title": "left|,circlearrowright,text{BUS},right|: A Large and\n  Diverse Multimodal Benchmark for evaluating the ability of Vision-Language\n  Models to understand Rebus Puzzles",
            "submittedOnDailyBy": {
                "_id": "5f89da6c5d083370c711f37c",
                "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
                "isPro": false,
                "fullname": "Abhilash Nandy",
                "user": "abhi1nandy2",
                "type": "user"
            },
            "summary": "Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters\nto represent words or phrases creatively) requires a variety of skills such as\nimage recognition, cognitive skills, commonsense reasoning, multi-step\nreasoning, image-based wordplay, etc., making this a challenging task for even\ncurrent Vision-Language Models. In this paper, we present\nleft|,circlearrowright,text{BUS},right|, a large and diverse\nbenchmark of 1,333 English Rebus Puzzles containing different artistic styles\nand levels of difficulty, spread across 18 categories such as food, idioms,\nsports, finance, entertainment, etc. We also propose RebusDescProgICE, a\nmodel-agnostic framework which uses a combination of an unstructured\ndescription and code-based, structured reasoning, along with better,\nreasoning-based in-context example selection, improving the performance of\nVision-Language Models on\nleft|,circlearrowright,text{BUS},right| by 2.1-4.1% and\n20-30% using closed-source and open-source models respectively compared to\nChain-of-Thought Reasoning.",
            "upvotes": 9,
            "discussionId": "690a3af8d70e173c84528ede",
            "ai_summary": "A benchmark and framework improve Vision-Language Models' performance on Rebus Puzzles through structured reasoning and example selection.",
            "ai_keywords": [
                "Vision-Language Models",
                "Rebus Puzzles",
                "Chain-of-Thought Reasoning",
                "RebusDescProgICE",
                "in-context example selection"
            ]
        },
        "publishedAt": "2025-11-03T03:42:59.000Z",
        "title": "left|,circlearrowright,text{BUS},right|: A Large and\n  Diverse Multimodal Benchmark for evaluating the ability of Vision-Language\n  Models to understand Rebus Puzzles",
        "summary": "Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters\nto represent words or phrases creatively) requires a variety of skills such as\nimage recognition, cognitive skills, commonsense reasoning, multi-step\nreasoning, image-based wordplay, etc., making this a challenging task for even\ncurrent Vision-Language Models. In this paper, we present\nleft|,circlearrowright,text{BUS},right|, a large and diverse\nbenchmark of 1,333 English Rebus Puzzles containing different artistic styles\nand levels of difficulty, spread across 18 categories such as food, idioms,\nsports, finance, entertainment, etc. We also propose RebusDescProgICE, a\nmodel-agnostic framework which uses a combination of an unstructured\ndescription and code-based, structured reasoning, along with better,\nreasoning-based in-context example selection, improving the performance of\nVision-Language Models on\nleft|,circlearrowright,text{BUS},right| by 2.1-4.1% and\n20-30% using closed-source and open-source models respectively compared to\nChain-of-Thought Reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01340.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5f89da6c5d083370c711f37c",
            "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
            "fullname": "Abhilash Nandy",
            "name": "abhi1nandy2",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.26865",
            "authors": [
                {
                    "_id": "69080eb6812eca10f9cc5de7",
                    "name": "Fenfen Lin",
                    "hidden": false
                },
                {
                    "_id": "69080eb6812eca10f9cc5de8",
                    "name": "Yesheng Liu",
                    "hidden": false
                },
                {
                    "_id": "69080eb6812eca10f9cc5de9",
                    "user": {
                        "_id": "68b97ed277d4f089dfded859",
                        "avatarUrl": "/avatars/ff20a41afea3bd5fd310c3e050862843.svg",
                        "isPro": false,
                        "fullname": "Xu Haiyu",
                        "user": "lindaxu525",
                        "type": "user"
                    },
                    "name": "Haiyu Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:27:11.380Z",
                    "hidden": false
                },
                {
                    "_id": "69080eb6812eca10f9cc5dea",
                    "name": "Chen Yue",
                    "hidden": false
                },
                {
                    "_id": "69080eb6812eca10f9cc5deb",
                    "user": {
                        "_id": "65b21047f5d76208991e463e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b21047f5d76208991e463e/6ML4lLz-vUr1HdWR3Jo-L.jpeg",
                        "isPro": false,
                        "fullname": "Zheqi He",
                        "user": "philokey",
                        "type": "user"
                    },
                    "name": "Zheqi He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:52:48.573Z",
                    "hidden": false
                },
                {
                    "_id": "69080eb6812eca10f9cc5dec",
                    "name": "Mingxuan Zhao",
                    "hidden": false
                },
                {
                    "_id": "69080eb6812eca10f9cc5ded",
                    "name": "Miguel Hu Chen",
                    "hidden": false
                },
                {
                    "_id": "69080eb6812eca10f9cc5dee",
                    "name": "Jiakang Liu",
                    "hidden": false
                },
                {
                    "_id": "69080eb6812eca10f9cc5def",
                    "name": "JG Yao",
                    "hidden": false
                },
                {
                    "_id": "69080eb6812eca10f9cc5df0",
                    "name": "Xi Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-30T17:20:51.000Z",
            "submittedOnDailyAt": "2025-11-04T04:03:17.287Z",
            "title": "Do Vision-Language Models Measure Up? Benchmarking Visual Measurement\n  Reading with MeasureBench",
            "submittedOnDailyBy": {
                "_id": "65b21047f5d76208991e463e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b21047f5d76208991e463e/6ML4lLz-vUr1HdWR3Jo-L.jpeg",
                "isPro": false,
                "fullname": "Zheqi He",
                "user": "philokey",
                "type": "user"
            },
            "summary": "Reading measurement instruments is effortless for humans and requires\nrelatively little domain expertise, yet it remains surprisingly challenging for\ncurrent vision-language models (VLMs) as we find in preliminary evaluation. In\nthis work, we introduce MeasureBench, a benchmark on visual measurement reading\ncovering both real-world and synthesized images of various types of\nmeasurements, along with an extensible pipeline for data synthesis. Our\npipeline procedurally generates a specified type of gauge with controllable\nvisual appearance, enabling scalable variation in key details such as pointers,\nscales, fonts, lighting, and clutter. Evaluation on popular proprietary and\nopen-weight VLMs shows that even the strongest frontier VLMs struggle\nmeasurement reading in general. A consistent failure mode is indicator\nlocalization: models can read digits or labels but misidentify the key\npositions of pointers or alignments, leading to big numeric errors despite\nplausible textual reasoning. We have also conducted preliminary experiments\nwith reinforcement learning over synthetic data, and find encouraging results\non in-domain synthetic subset but less promising for real-world images. Our\nanalysis highlights a fundamental limitation of current VLMs in fine-grained\nspatial grounding. We hope this resource can help future advances on visually\ngrounded numeracy and precise spatial perception of VLMs, bridging the gap\nbetween recognizing numbers and measuring the world.",
            "upvotes": 9,
            "discussionId": "69080eb6812eca10f9cc5df1",
            "projectPage": "https://flageval-baai.github.io/MeasureBenchPage/",
            "githubRepo": "https://github.com/flageval-baai/MeasureBench",
            "ai_summary": "MeasureBench evaluates vision-language models on reading measurements from images, revealing challenges in indicator localization and fine-grained spatial grounding.",
            "ai_keywords": [
                "vision-language models",
                "VLMs",
                "MeasureBench",
                "data synthesis",
                "procedural generation",
                "gauge",
                "pointers",
                "scales",
                "fonts",
                "lighting",
                "clutter",
                "reinforcement learning",
                "fine-grained spatial grounding",
                "visually grounded numeracy"
            ],
            "githubStars": 5,
            "organization": {
                "_id": "61be9739d2f9358e24ca0a4f",
                "name": "BAAI",
                "fullname": "Beijing Academy of Artificial Intelligence",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
            }
        },
        "publishedAt": "2025-10-30T13:20:51.000Z",
        "title": "Do Vision-Language Models Measure Up? Benchmarking Visual Measurement\n  Reading with MeasureBench",
        "summary": "Reading measurement instruments is effortless for humans and requires\nrelatively little domain expertise, yet it remains surprisingly challenging for\ncurrent vision-language models (VLMs) as we find in preliminary evaluation. In\nthis work, we introduce MeasureBench, a benchmark on visual measurement reading\ncovering both real-world and synthesized images of various types of\nmeasurements, along with an extensible pipeline for data synthesis. Our\npipeline procedurally generates a specified type of gauge with controllable\nvisual appearance, enabling scalable variation in key details such as pointers,\nscales, fonts, lighting, and clutter. Evaluation on popular proprietary and\nopen-weight VLMs shows that even the strongest frontier VLMs struggle\nmeasurement reading in general. A consistent failure mode is indicator\nlocalization: models can read digits or labels but misidentify the key\npositions of pointers or alignments, leading to big numeric errors despite\nplausible textual reasoning. We have also conducted preliminary experiments\nwith reinforcement learning over synthetic data, and find encouraging results\non in-domain synthetic subset but less promising for real-world images. Our\nanalysis highlights a fundamental limitation of current VLMs in fine-grained\nspatial grounding. We hope this resource can help future advances on visually\ngrounded numeracy and precise spatial perception of VLMs, bridging the gap\nbetween recognizing numbers and measuring the world.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26865.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65b21047f5d76208991e463e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b21047f5d76208991e463e/6ML4lLz-vUr1HdWR3Jo-L.jpeg",
            "fullname": "Zheqi He",
            "name": "philokey",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "61be9739d2f9358e24ca0a4f",
            "name": "BAAI",
            "fullname": "Beijing Academy of Artificial Intelligence",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.01857",
            "authors": [
                {
                    "_id": "69098243812eca10f9cc637f",
                    "name": "Reza Esfandiarpoor",
                    "hidden": false
                },
                {
                    "_id": "69098243812eca10f9cc6380",
                    "user": {
                        "_id": "64fbb457c7f04f7cee8624e0",
                        "avatarUrl": "/avatars/f0512561780625d9be43f00dfd5cd46d.svg",
                        "isPro": false,
                        "fullname": "Max Zuo",
                        "user": "zuom",
                        "type": "user"
                    },
                    "name": "Max Zuo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:24:39.013Z",
                    "hidden": false
                },
                {
                    "_id": "69098243812eca10f9cc6381",
                    "name": "Stephen H. Bach",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-03T18:59:57.000Z",
            "submittedOnDailyAt": "2025-11-04T02:05:54.867Z",
            "title": "Trove: A Flexible Toolkit for Dense Retrieval",
            "submittedOnDailyBy": {
                "_id": "64fbb457c7f04f7cee8624e0",
                "avatarUrl": "/avatars/f0512561780625d9be43f00dfd5cd46d.svg",
                "isPro": false,
                "fullname": "Max Zuo",
                "user": "zuom",
                "type": "user"
            },
            "summary": "We introduce Trove, an easy-to-use open-source retrieval toolkit that\nsimplifies research experiments without sacrificing flexibility or speed. For\nthe first time, we introduce efficient data management features that load and\nprocess (filter, select, transform, and combine) retrieval datasets on the fly,\nwith just a few lines of code. This gives users the flexibility to easily\nexperiment with different dataset configurations without the need to compute\nand store multiple copies of large datasets. Trove is highly customizable: in\naddition to many built-in options, it allows users to freely modify existing\ncomponents or replace them entirely with user-defined objects. It also provides\na low-code and unified pipeline for evaluation and hard negative mining, which\nsupports multi-node execution without any code changes. Trove's data management\nfeatures reduce memory consumption by a factor of 2.6. Moreover, Trove's\neasy-to-use inference pipeline incurs no overhead, and inference times decrease\nlinearly with the number of available nodes. Most importantly, we demonstrate\nhow Trove simplifies retrieval experiments and allows for arbitrary\ncustomizations, thus facilitating exploratory research.",
            "upvotes": 8,
            "discussionId": "69098243812eca10f9cc6382",
            "projectPage": "https://ir-trove.dev/",
            "githubRepo": "https://github.com/BatsResearch/trove",
            "ai_summary": "Trove is an open-source retrieval toolkit that streamlines data management and experimentation with efficient on-the-fly processing and customizable components.",
            "ai_keywords": [
                "retrieval toolkit",
                "data management",
                "dataset configurations",
                "hard negative mining",
                "multi-node execution",
                "inference pipeline"
            ],
            "githubStars": 35,
            "organization": {
                "_id": "650d8bcd5085c0ce1f286c12",
                "name": "BatsResearch",
                "fullname": "Bats Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/608849cadf398c3b285ce95b/oFrkKbxGUKIW2W8FFGUje.png"
            }
        },
        "publishedAt": "2025-11-03T13:59:57.000Z",
        "title": "Trove: A Flexible Toolkit for Dense Retrieval",
        "summary": "We introduce Trove, an easy-to-use open-source retrieval toolkit that\nsimplifies research experiments without sacrificing flexibility or speed. For\nthe first time, we introduce efficient data management features that load and\nprocess (filter, select, transform, and combine) retrieval datasets on the fly,\nwith just a few lines of code. This gives users the flexibility to easily\nexperiment with different dataset configurations without the need to compute\nand store multiple copies of large datasets. Trove is highly customizable: in\naddition to many built-in options, it allows users to freely modify existing\ncomponents or replace them entirely with user-defined objects. It also provides\na low-code and unified pipeline for evaluation and hard negative mining, which\nsupports multi-node execution without any code changes. Trove's data management\nfeatures reduce memory consumption by a factor of 2.6. Moreover, Trove's\neasy-to-use inference pipeline incurs no overhead, and inference times decrease\nlinearly with the number of available nodes. Most importantly, we demonstrate\nhow Trove simplifies retrieval experiments and allows for arbitrary\ncustomizations, thus facilitating exploratory research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01857.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64fbb457c7f04f7cee8624e0",
            "avatarUrl": "/avatars/f0512561780625d9be43f00dfd5cd46d.svg",
            "fullname": "Max Zuo",
            "name": "zuom",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "650d8bcd5085c0ce1f286c12",
            "name": "BatsResearch",
            "fullname": "Bats Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/608849cadf398c3b285ce95b/oFrkKbxGUKIW2W8FFGUje.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.01618",
            "authors": [
                {
                    "_id": "6909a0a0812eca10f9cc63ba",
                    "name": "Xiaoyu Zhan",
                    "hidden": false
                },
                {
                    "_id": "6909a0a0812eca10f9cc63bb",
                    "user": {
                        "_id": "67dc162ec8c00778e8689f42",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png",
                        "isPro": false,
                        "fullname": "Wenxuan Huang",
                        "user": "Osilly",
                        "type": "user"
                    },
                    "name": "Wenxuan Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T09:25:01.262Z",
                    "hidden": false
                },
                {
                    "_id": "6909a0a0812eca10f9cc63bc",
                    "name": "Hao Sun",
                    "hidden": false
                },
                {
                    "_id": "6909a0a0812eca10f9cc63bd",
                    "name": "Xinyu Fu",
                    "hidden": false
                },
                {
                    "_id": "6909a0a0812eca10f9cc63be",
                    "name": "Changfeng Ma",
                    "hidden": false
                },
                {
                    "_id": "6909a0a0812eca10f9cc63bf",
                    "name": "Shaosheng Cao",
                    "hidden": false
                },
                {
                    "_id": "6909a0a0812eca10f9cc63c0",
                    "name": "Bohan Jia",
                    "hidden": false
                },
                {
                    "_id": "6909a0a0812eca10f9cc63c1",
                    "name": "Shaohui Lin",
                    "hidden": false
                },
                {
                    "_id": "6909a0a0812eca10f9cc63c2",
                    "name": "Zhenfei Yin",
                    "hidden": false
                },
                {
                    "_id": "6909a0a0812eca10f9cc63c3",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "6909a0a0812eca10f9cc63c4",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "6909a0a0812eca10f9cc63c5",
                    "name": "Yuanqi Li",
                    "hidden": false
                },
                {
                    "_id": "6909a0a0812eca10f9cc63c6",
                    "name": "Jie Guo",
                    "hidden": false
                },
                {
                    "_id": "6909a0a0812eca10f9cc63c7",
                    "name": "Yanwen Guo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-03T14:27:00.000Z",
            "submittedOnDailyAt": "2025-11-04T04:15:47.258Z",
            "title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "67dc162ec8c00778e8689f42",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png",
                "isPro": false,
                "fullname": "Wenxuan Huang",
                "user": "Osilly",
                "type": "user"
            },
            "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly improved 2D visual understanding, prompting interest in their\napplication to complex 3D reasoning tasks. However, it remains unclear whether\nthese models can effectively capture the detailed spatial information required\nfor robust real-world performance, especially cross-view consistency, a key\nrequirement for accurate 3D reasoning. Considering this issue, we introduce\nViewpoint Learning, a task designed to evaluate and improve the spatial\nreasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,\nconsisting of 100K object-centric image pairs with diverse viewpoints and\ncorresponding question-answer pairs. Our approach employs a two-stage\nfine-tuning strategy: first, foundational knowledge is injected to the baseline\nMLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in\nsignificant improvements across multiple tasks; second, generalization is\nenhanced through Reinforcement Learning using the Group Relative Policy\nOptimization (GRPO) algorithm on a broader set of questions. Additionally, we\nintroduce a hybrid cold-start initialization method designed to simultaneously\nlearn viewpoint representations and maintain coherent reasoning thinking.\nExperimental results show that our approach significantly activates the spatial\nreasoning ability of MLLM, improving performance on both in-domain and\nout-of-domain reasoning tasks. Our findings highlight the value of developing\nfoundational spatial skills in MLLMs, supporting future progress in robotics,\nautonomous systems, and 3D scene understanding.",
            "upvotes": 8,
            "discussionId": "6909a0a0812eca10f9cc63c8",
            "ai_summary": "Viewpoint Learning enhances the spatial reasoning capabilities of Multimodal Large Language Models using a two-stage fine-tuning strategy and a hybrid cold-start initialization method, improving performance on both in-domain and out-of-domain 3D reasoning tasks.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "Viewpoint Learning",
                "Viewpoint-100K dataset",
                "Supervised Fine-Tuning",
                "Reinforcement Learning",
                "Group Relative Policy Optimization",
                "hybrid cold-start initialization",
                "spatial reasoning"
            ]
        },
        "publishedAt": "2025-11-03T09:27:00.000Z",
        "title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language\n  Models",
        "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly improved 2D visual understanding, prompting interest in their\napplication to complex 3D reasoning tasks. However, it remains unclear whether\nthese models can effectively capture the detailed spatial information required\nfor robust real-world performance, especially cross-view consistency, a key\nrequirement for accurate 3D reasoning. Considering this issue, we introduce\nViewpoint Learning, a task designed to evaluate and improve the spatial\nreasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,\nconsisting of 100K object-centric image pairs with diverse viewpoints and\ncorresponding question-answer pairs. Our approach employs a two-stage\nfine-tuning strategy: first, foundational knowledge is injected to the baseline\nMLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in\nsignificant improvements across multiple tasks; second, generalization is\nenhanced through Reinforcement Learning using the Group Relative Policy\nOptimization (GRPO) algorithm on a broader set of questions. Additionally, we\nintroduce a hybrid cold-start initialization method designed to simultaneously\nlearn viewpoint representations and maintain coherent reasoning thinking.\nExperimental results show that our approach significantly activates the spatial\nreasoning ability of MLLM, improving performance on both in-domain and\nout-of-domain reasoning tasks. Our findings highlight the value of developing\nfoundational spatial skills in MLLMs, supporting future progress in robotics,\nautonomous systems, and 3D scene understanding.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01618.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67dc162ec8c00778e8689f42",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png",
            "fullname": "Wenxuan Huang",
            "name": "Osilly",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.01846",
            "authors": [
                {
                    "_id": "6909790d812eca10f9cc6262",
                    "name": "Thang Luong",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc6263",
                    "name": "Dawsen Hwang",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc6264",
                    "name": "Hoang H. Nguyen",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc6265",
                    "name": "Golnaz Ghiasi",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc6266",
                    "name": "Yuri Chervonyi",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc6267",
                    "name": "Insuk Seo",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc6268",
                    "name": "Junsu Kim",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc6269",
                    "name": "Garrett Bingham",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc626a",
                    "name": "Jonathan Lee",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc626b",
                    "name": "Swaroop Mishra",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc626c",
                    "name": "Alex Zhai",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc626d",
                    "name": "Clara Huiyi Hu",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc626e",
                    "name": "Henryk Michalewski",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc626f",
                    "name": "Jimin Kim",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc6270",
                    "name": "Jeonghyun Ahn",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc6271",
                    "name": "Junhwi Bae",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc6272",
                    "name": "Xingyou Song",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc6273",
                    "name": "Trieu H. Trinh",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc6274",
                    "name": "Quoc V. Le",
                    "hidden": false
                },
                {
                    "_id": "6909790d812eca10f9cc6275",
                    "name": "Junehyuk Jung",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-03T18:53:02.000Z",
            "submittedOnDailyAt": "2025-11-04T01:25:07.006Z",
            "title": "Towards Robust Mathematical Reasoning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Finding the right north-star metrics is highly critical for advancing the\nmathematical reasoning capabilities of foundation models, especially given that\nexisting evaluations are either too easy or only focus on getting correct short\nanswers. To address these issues, we present IMO-Bench, a suite of advanced\nreasoning benchmarks, vetted by a panel of top specialists and that\nspecifically targets the level of the International Mathematical Olympiad\n(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench\nfirst tests models on 400 diverse Olympiad problems with verifiable short\nanswers. IMO-Proof Bench is the next-level evaluation for proof-writing\ncapabilities, which includes both basic and advanced IMO level problems as well\nas detailed grading guidelines to facilitate automatic grading. These\nbenchmarks played a crucial role in our historic achievement of the gold-level\nperformance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our\nmodel achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof\nBench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%\nrespectively. We also showed that autograders built with Gemini reasoning\ncorrelate well with human evaluations and construct IMO-GradingBench, with 1000\nhuman gradings on proofs, to enable further progress in automatic evaluation of\nlong-form answers. We hope that IMO-Bench will help the community towards\nadvancing robust mathematical reasoning and release it at\nhttps://imobench.github.io/.",
            "upvotes": 6,
            "discussionId": "6909790d812eca10f9cc6276",
            "projectPage": "https://imobench.github.io/",
            "ai_summary": "IMO-Bench, a suite of advanced reasoning benchmarks, evaluates mathematical reasoning capabilities of foundation models using IMO-level problems and detailed grading guidelines, achieving gold-level performance with Gemini Deep Think.",
            "ai_keywords": [
                "IMO-Bench",
                "IMO-AnswerBench",
                "IMO-Proof Bench",
                "IMO-GradingBench",
                "mathematical reasoning",
                "foundation models",
                "autograders",
                "human evaluations"
            ]
        },
        "publishedAt": "2025-11-03T13:53:02.000Z",
        "title": "Towards Robust Mathematical Reasoning",
        "summary": "Finding the right north-star metrics is highly critical for advancing the\nmathematical reasoning capabilities of foundation models, especially given that\nexisting evaluations are either too easy or only focus on getting correct short\nanswers. To address these issues, we present IMO-Bench, a suite of advanced\nreasoning benchmarks, vetted by a panel of top specialists and that\nspecifically targets the level of the International Mathematical Olympiad\n(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench\nfirst tests models on 400 diverse Olympiad problems with verifiable short\nanswers. IMO-Proof Bench is the next-level evaluation for proof-writing\ncapabilities, which includes both basic and advanced IMO level problems as well\nas detailed grading guidelines to facilitate automatic grading. These\nbenchmarks played a crucial role in our historic achievement of the gold-level\nperformance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our\nmodel achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof\nBench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%\nrespectively. We also showed that autograders built with Gemini reasoning\ncorrelate well with human evaluations and construct IMO-GradingBench, with 1000\nhuman gradings on proofs, to enable further progress in automatic evaluation of\nlong-form answers. We hope that IMO-Bench will help the community towards\nadvancing robust mathematical reasoning and release it at\nhttps://imobench.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01846.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 154
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.26491",
            "authors": [
                {
                    "_id": "6905baa1b26ae8b17699fd58",
                    "user": {
                        "_id": "6421c749eaad1bcb28b11206",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6421c749eaad1bcb28b11206/jvaBUG7Z5fv6lJMoH301q.jpeg",
                        "isPro": false,
                        "fullname": "Erle Zhu",
                        "user": "lez3f",
                        "type": "user"
                    },
                    "name": "Erle Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-03T20:53:52.262Z",
                    "hidden": false
                },
                {
                    "_id": "6905baa1b26ae8b17699fd59",
                    "name": "Dazhi Jiang",
                    "hidden": false
                },
                {
                    "_id": "6905baa1b26ae8b17699fd5a",
                    "user": {
                        "_id": "6864ef64d4dde09fbabad95a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sbEmJxiJg4-8rrXDqUuFy.jpeg",
                        "isPro": false,
                        "fullname": "Yuan Wang",
                        "user": "traveler2333",
                        "type": "user"
                    },
                    "name": "Yuan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:27:15.733Z",
                    "hidden": false
                },
                {
                    "_id": "6905baa1b26ae8b17699fd5b",
                    "name": "Xujun Li",
                    "hidden": false
                },
                {
                    "_id": "6905baa1b26ae8b17699fd5c",
                    "name": "Jiale Cheng",
                    "hidden": false
                },
                {
                    "_id": "6905baa1b26ae8b17699fd5d",
                    "name": "Yuxian Gu",
                    "hidden": false
                },
                {
                    "_id": "6905baa1b26ae8b17699fd5e",
                    "name": "Yilin Niu",
                    "hidden": false
                },
                {
                    "_id": "6905baa1b26ae8b17699fd5f",
                    "name": "Aohan Zeng",
                    "hidden": false
                },
                {
                    "_id": "6905baa1b26ae8b17699fd60",
                    "name": "Jie Tang",
                    "hidden": false
                },
                {
                    "_id": "6905baa1b26ae8b17699fd61",
                    "name": "Minlie Huang",
                    "hidden": false
                },
                {
                    "_id": "6905baa1b26ae8b17699fd62",
                    "name": "Hongning Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-30T13:40:52.000Z",
            "submittedOnDailyAt": "2025-11-04T02:09:51.474Z",
            "title": "Data-Efficient RLVR via Off-Policy Influence Guidance",
            "submittedOnDailyBy": {
                "_id": "6421c749eaad1bcb28b11206",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6421c749eaad1bcb28b11206/jvaBUG7Z5fv6lJMoH301q.jpeg",
                "isPro": false,
                "fullname": "Erle Zhu",
                "user": "lez3f",
                "type": "user"
            },
            "summary": "Data selection is a critical aspect of Reinforcement Learning with Verifiable\nRewards (RLVR) for enhancing the reasoning capabilities of large language\nmodels (LLMs). Current data selection methods are largely heuristic-based,\nlacking theoretical guarantees and generalizability. This work proposes a\ntheoretically-grounded approach using influence functions to estimate the\ncontribution of each data point to the learning objective. To overcome the\nprohibitive computational cost of policy rollouts required for online influence\nestimation, we introduce an off-policy influence estimation method that\nefficiently approximates data influence using pre-collected offline\ntrajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we\nemploy sparse random projection to reduce dimensionality and improve storage\nand computation efficiency. Leveraging these techniques, we develop\nCurriculum RL with Off-Policy\nInfluence guidance (CROPI), a multi-stage RL framework that\niteratively selects the most influential data for the current policy.\nExperiments on models up to 7B parameters demonstrate that CROPI significantly\naccelerates training. On a 1.5B model, it achieves a 2.66x step-level\nacceleration while using only 10\\% of the data per stage compared to\nfull-dataset training. Our results highlight the substantial potential of\ninfluence-based data selection for efficient RLVR.",
            "upvotes": 6,
            "discussionId": "6905baa1b26ae8b17699fd63",
            "ai_summary": "Influence functions and off-policy estimation improve data selection in RLVR, accelerating training and reducing data usage for large language models.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards (RLVR)",
                "influence functions",
                "policy rollouts",
                "off-policy influence estimation",
                "sparse random projection",
                "Curriculum RL with Off-Policy Influence guidance (CROPI)",
                "high-dimensional gradients",
                "multi-stage RL framework"
            ]
        },
        "publishedAt": "2025-10-30T09:40:52.000Z",
        "title": "Data-Efficient RLVR via Off-Policy Influence Guidance",
        "summary": "Data selection is a critical aspect of Reinforcement Learning with Verifiable\nRewards (RLVR) for enhancing the reasoning capabilities of large language\nmodels (LLMs). Current data selection methods are largely heuristic-based,\nlacking theoretical guarantees and generalizability. This work proposes a\ntheoretically-grounded approach using influence functions to estimate the\ncontribution of each data point to the learning objective. To overcome the\nprohibitive computational cost of policy rollouts required for online influence\nestimation, we introduce an off-policy influence estimation method that\nefficiently approximates data influence using pre-collected offline\ntrajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we\nemploy sparse random projection to reduce dimensionality and improve storage\nand computation efficiency. Leveraging these techniques, we develop\nCurriculum RL with Off-Policy\nInfluence guidance (CROPI), a multi-stage RL framework that\niteratively selects the most influential data for the current policy.\nExperiments on models up to 7B parameters demonstrate that CROPI significantly\naccelerates training. On a 1.5B model, it achieves a 2.66x step-level\nacceleration while using only 10\\% of the data per stage compared to\nfull-dataset training. Our results highlight the substantial potential of\ninfluence-based data selection for efficient RLVR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26491.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6421c749eaad1bcb28b11206",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6421c749eaad1bcb28b11206/jvaBUG7Z5fv6lJMoH301q.jpeg",
            "fullname": "Erle Zhu",
            "name": "lez3f",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.01775",
            "authors": [
                {
                    "_id": "69097bc4812eca10f9cc6367",
                    "name": "Zhen Chen",
                    "hidden": false
                },
                {
                    "_id": "69097bc4812eca10f9cc6368",
                    "name": "Qing Xu",
                    "hidden": false
                },
                {
                    "_id": "69097bc4812eca10f9cc6369",
                    "name": "Jinlin Wu",
                    "hidden": false
                },
                {
                    "_id": "69097bc4812eca10f9cc636a",
                    "name": "Biao Yang",
                    "hidden": false
                },
                {
                    "_id": "69097bc4812eca10f9cc636b",
                    "name": "Yuhao Zhai",
                    "hidden": false
                },
                {
                    "_id": "69097bc4812eca10f9cc636c",
                    "name": "Geng Guo",
                    "hidden": false
                },
                {
                    "_id": "69097bc4812eca10f9cc636d",
                    "name": "Jing Zhang",
                    "hidden": false
                },
                {
                    "_id": "69097bc4812eca10f9cc636e",
                    "name": "Yinlu Ding",
                    "hidden": false
                },
                {
                    "_id": "69097bc4812eca10f9cc636f",
                    "name": "Nassir Navab",
                    "hidden": false
                },
                {
                    "_id": "69097bc4812eca10f9cc6370",
                    "name": "Jiebo Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-03T17:28:54.000Z",
            "submittedOnDailyAt": "2025-11-04T01:36:35.253Z",
            "title": "How Far Are Surgeons from Surgical World Models? A Pilot Study on\n  Zero-shot Surgical Video Generation with Expert Assessment",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Foundation models in video generation are demonstrating remarkable\ncapabilities as potential world models for simulating the physical world.\nHowever, their application in high-stakes domains like surgery, which demand\ndeep, specialized causal knowledge rather than general physical rules, remains\na critical unexplored gap. To systematically address this challenge, we present\nSurgVeo, the first expert-curated benchmark for video generation model\nevaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,\nfour-tiered framework tailored to assess model outputs from basic appearance to\ncomplex surgical strategy. On the basis of the SurgVeo benchmark, we task the\nadvanced Veo-3 model with a zero-shot prediction task on surgical clips from\nlaparoscopic and neurosurgical procedures. A panel of four board-certified\nsurgeons evaluates the generated videos according to the SPP. Our results\nreveal a distinct \"plausibility gap\": while Veo-3 achieves exceptional Visual\nPerceptual Plausibility, it fails critically at higher levels of the SPP,\nincluding Instrument Operation Plausibility, Environment Feedback Plausibility,\nand Surgical Intent Plausibility. This work provides the first quantitative\nevidence of the chasm between visually convincing mimicry and causal\nunderstanding in surgical AI. Our findings from SurgVeo and the SPP establish a\ncrucial foundation and roadmap for developing future models capable of\nnavigating the complexities of specialized, real-world healthcare domains.",
            "upvotes": 5,
            "discussionId": "69097bc4812eca10f9cc6371",
            "ai_summary": "SurgVeo, a benchmark for video generation in surgery, and the Surgical Plausibility Pyramid reveal a gap between visual plausibility and causal understanding in surgical AI models.",
            "ai_keywords": [
                "Foundation models",
                "video generation",
                "world models",
                "SurgVeo",
                "Surgical Plausibility Pyramid",
                "Veo-3",
                "zero-shot prediction",
                "Visual Perceptual Plausibility",
                "Instrument Operation Plausibility",
                "Environment Feedback Plausibility",
                "Surgical Intent Plausibility"
            ]
        },
        "publishedAt": "2025-11-03T12:28:54.000Z",
        "title": "How Far Are Surgeons from Surgical World Models? A Pilot Study on\n  Zero-shot Surgical Video Generation with Expert Assessment",
        "summary": "Foundation models in video generation are demonstrating remarkable\ncapabilities as potential world models for simulating the physical world.\nHowever, their application in high-stakes domains like surgery, which demand\ndeep, specialized causal knowledge rather than general physical rules, remains\na critical unexplored gap. To systematically address this challenge, we present\nSurgVeo, the first expert-curated benchmark for video generation model\nevaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,\nfour-tiered framework tailored to assess model outputs from basic appearance to\ncomplex surgical strategy. On the basis of the SurgVeo benchmark, we task the\nadvanced Veo-3 model with a zero-shot prediction task on surgical clips from\nlaparoscopic and neurosurgical procedures. A panel of four board-certified\nsurgeons evaluates the generated videos according to the SPP. Our results\nreveal a distinct \"plausibility gap\": while Veo-3 achieves exceptional Visual\nPerceptual Plausibility, it fails critically at higher levels of the SPP,\nincluding Instrument Operation Plausibility, Environment Feedback Plausibility,\nand Surgical Intent Plausibility. This work provides the first quantitative\nevidence of the chasm between visually convincing mimicry and causal\nunderstanding in surgical AI. Our findings from SurgVeo and the SPP establish a\ncrucial foundation and roadmap for developing future models capable of\nnavigating the complexities of specialized, real-world healthcare domains.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01775.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 154
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.01266",
            "authors": [
                {
                    "_id": "6909a508812eca10f9cc63de",
                    "user": {
                        "_id": "631074d895c34b95407945f0",
                        "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
                        "isPro": false,
                        "fullname": "Joonghyuk Shin",
                        "user": "alex4727",
                        "type": "user"
                    },
                    "name": "Joonghyuk Shin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T09:24:54.969Z",
                    "hidden": false
                },
                {
                    "_id": "6909a508812eca10f9cc63df",
                    "name": "Zhengqi Li",
                    "hidden": false
                },
                {
                    "_id": "6909a508812eca10f9cc63e0",
                    "name": "Richard Zhang",
                    "hidden": false
                },
                {
                    "_id": "6909a508812eca10f9cc63e1",
                    "name": "Jun-Yan Zhu",
                    "hidden": false
                },
                {
                    "_id": "6909a508812eca10f9cc63e2",
                    "name": "Jaesik Park",
                    "hidden": false
                },
                {
                    "_id": "6909a508812eca10f9cc63e3",
                    "name": "Eli Schechtman",
                    "hidden": false
                },
                {
                    "_id": "6909a508812eca10f9cc63e4",
                    "name": "Xun Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-03T06:37:53.000Z",
            "submittedOnDailyAt": "2025-11-04T04:33:51.430Z",
            "title": "MotionStream: Real-Time Video Generation with Interactive Motion\n  Controls",
            "submittedOnDailyBy": {
                "_id": "631074d895c34b95407945f0",
                "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
                "isPro": false,
                "fullname": "Joonghyuk Shin",
                "user": "alex4727",
                "type": "user"
            },
            "summary": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience.",
            "upvotes": 5,
            "discussionId": "6909a509812eca10f9cc63e5",
            "ai_summary": "MotionStream enables real-time video generation with sub-second latency and up to 29 FPS by distilling a text-to-video model with motion control into a causal student using Self Forcing with Distribution Matching Distillation and sliding-window causal attention with attention sinks.",
            "ai_keywords": [
                "motion-conditioned video generation",
                "MotionStream",
                "text-to-video model",
                "motion control",
                "Self Forcing with Distribution Matching Distillation",
                "causal student",
                "sliding-window causal attention",
                "attention sinks",
                "self-rollout",
                "KV cache rolling",
                "motion following",
                "video quality",
                "real-time interaction"
            ],
            "organization": {
                "_id": "61e5d14f77496de0a6d95c6b",
                "name": "adobe",
                "fullname": "Adobe",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
            }
        },
        "publishedAt": "2025-11-03T01:37:53.000Z",
        "title": "MotionStream: Real-Time Video Generation with Interactive Motion\n  Controls",
        "summary": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01266.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "631074d895c34b95407945f0",
            "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
            "fullname": "Joonghyuk Shin",
            "name": "alex4727",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "61e5d14f77496de0a6d95c6b",
            "name": "adobe",
            "fullname": "Adobe",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.01718",
            "authors": [
                {
                    "_id": "69096c89812eca10f9cc619c",
                    "user": {
                        "_id": "661e3bc34273a439e171d806",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661e3bc34273a439e171d806/-OFdIK39yQD702NFsdALE.jpeg",
                        "isPro": false,
                        "fullname": "jiayi chen",
                        "user": "chenpyyy",
                        "type": "user"
                    },
                    "name": "Jiayi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:26:42.597Z",
                    "hidden": false
                },
                {
                    "_id": "69096c89812eca10f9cc619d",
                    "name": "Wenxuan Song",
                    "hidden": false
                },
                {
                    "_id": "69096c89812eca10f9cc619e",
                    "name": "Pengxiang Ding",
                    "hidden": false
                },
                {
                    "_id": "69096c89812eca10f9cc619f",
                    "name": "Ziyang Zhou",
                    "hidden": false
                },
                {
                    "_id": "69096c89812eca10f9cc61a0",
                    "name": "Han Zhao",
                    "hidden": false
                },
                {
                    "_id": "69096c89812eca10f9cc61a1",
                    "name": "Feilong Tang",
                    "hidden": false
                },
                {
                    "_id": "69096c89812eca10f9cc61a2",
                    "name": "Donglin Wang",
                    "hidden": false
                },
                {
                    "_id": "69096c89812eca10f9cc61a3",
                    "name": "Haoang Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-03T16:26:54.000Z",
            "submittedOnDailyAt": "2025-11-04T00:35:20.084Z",
            "title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete\n  Denoising Diffusion Process",
            "submittedOnDailyBy": {
                "_id": "66a0a3405c5e2a42a214c70f",
                "avatarUrl": "/avatars/52b9ee7f899ee5431ed37fd1db378d9e.svg",
                "isPro": false,
                "fullname": "Wenxuan Song",
                "user": "Wenxuan123",
                "type": "user"
            },
            "summary": "Vision-language-action (VLA) models aim to understand natural language\ninstructions and visual observations and to execute corresponding actions as an\nembodied agent. Recent work integrates future images into the\nunderstanding-acting loop, yielding unified VLAs that jointly understand,\ngenerate, and act -- reading text and images and producing future images and\nactions. However, these models either rely on external experts for modality\nunification or treat image generation and action prediction as separate\nprocesses, limiting the benefits of direct synergy between these tasks. Our\ncore philosophy is to optimize generation and action jointly through a\nsynchronous denoising process, where the iterative refinement enables actions\nto evolve from initialization, under constant and sufficient visual guidance.\nWe ground this philosophy in our proposed Unified Diffusion VLA and Joint\nDiscrete Denoising Diffusion Process (JD3P), which is a joint diffusion process\nthat integrates multiple modalities into a single denoising trajectory to serve\nas the key mechanism enabling understanding, generation, and acting to be\nintrinsically synergistic. Our model and theory are built on a unified\ntokenized space of all modalities and a hybrid attention mechanism. We further\npropose a two-stage training pipeline and several inference-time techniques\nthat optimize performance and efficiency. Our approach achieves\nstate-of-the-art performance on benchmarks such as CALVIN, LIBERO, and\nSimplerEnv with 4times faster inference than autoregressive methods, and we\ndemonstrate its effectiveness through in-depth analysis and real-world\nevaluations. Our project page is available at\nhttps://irpn-eai.github.io/UD-VLA.github.io/.",
            "upvotes": 4,
            "discussionId": "69096c8a812eca10f9cc61a4",
            "projectPage": "https://irpn-eai.github.io/UD-VLA.github.io/",
            "githubRepo": "https://github.com/OpenHelix-Team/UD-VLA",
            "ai_summary": "A Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P) model integrates multiple modalities through a synchronous denoising process, achieving state-of-the-art performance in vision-language-action tasks with faster inference.",
            "ai_keywords": [
                "Unified Diffusion VLA",
                "Joint Discrete Denoising Diffusion Process",
                "JD3P",
                "synchronous denoising process",
                "unified tokenized space",
                "hybrid attention mechanism",
                "two-stage training pipeline",
                "inference-time techniques",
                "CALVIN",
                "LIBERO",
                "SimplerEnv"
            ],
            "githubStars": 20,
            "organization": {
                "_id": "65ad19cac14c3cf579ad9b68",
                "name": "HKUSTGZ",
                "fullname": "HKUSTGZ"
            }
        },
        "publishedAt": "2025-11-03T11:26:54.000Z",
        "title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete\n  Denoising Diffusion Process",
        "summary": "Vision-language-action (VLA) models aim to understand natural language\ninstructions and visual observations and to execute corresponding actions as an\nembodied agent. Recent work integrates future images into the\nunderstanding-acting loop, yielding unified VLAs that jointly understand,\ngenerate, and act -- reading text and images and producing future images and\nactions. However, these models either rely on external experts for modality\nunification or treat image generation and action prediction as separate\nprocesses, limiting the benefits of direct synergy between these tasks. Our\ncore philosophy is to optimize generation and action jointly through a\nsynchronous denoising process, where the iterative refinement enables actions\nto evolve from initialization, under constant and sufficient visual guidance.\nWe ground this philosophy in our proposed Unified Diffusion VLA and Joint\nDiscrete Denoising Diffusion Process (JD3P), which is a joint diffusion process\nthat integrates multiple modalities into a single denoising trajectory to serve\nas the key mechanism enabling understanding, generation, and acting to be\nintrinsically synergistic. Our model and theory are built on a unified\ntokenized space of all modalities and a hybrid attention mechanism. We further\npropose a two-stage training pipeline and several inference-time techniques\nthat optimize performance and efficiency. Our approach achieves\nstate-of-the-art performance on benchmarks such as CALVIN, LIBERO, and\nSimplerEnv with 4times faster inference than autoregressive methods, and we\ndemonstrate its effectiveness through in-depth analysis and real-world\nevaluations. Our project page is available at\nhttps://irpn-eai.github.io/UD-VLA.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01718.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66a0a3405c5e2a42a214c70f",
            "avatarUrl": "/avatars/52b9ee7f899ee5431ed37fd1db378d9e.svg",
            "fullname": "Wenxuan Song",
            "name": "Wenxuan123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "65ad19cac14c3cf579ad9b68",
            "name": "HKUSTGZ",
            "fullname": "HKUSTGZ"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.00405",
            "authors": [
                {
                    "_id": "69097f4d812eca10f9cc6373",
                    "name": "Zhibin Lan",
                    "hidden": false
                },
                {
                    "_id": "69097f4d812eca10f9cc6374",
                    "name": "Liqiang Niu",
                    "hidden": false
                },
                {
                    "_id": "69097f4d812eca10f9cc6375",
                    "name": "Fandong Meng",
                    "hidden": false
                },
                {
                    "_id": "69097f4d812eca10f9cc6376",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "69097f4d812eca10f9cc6377",
                    "name": "Jinsong Su",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-01T05:04:23.000Z",
            "submittedOnDailyAt": "2025-11-04T02:10:14.506Z",
            "title": "UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings",
            "submittedOnDailyBy": {
                "_id": "6626449503e1f561573d30e9",
                "avatarUrl": "/avatars/e7f9720ccd01bae32d0a03a1b0dacab5.svg",
                "isPro": false,
                "fullname": "Zhibin Lan",
                "user": "zhibinlan",
                "type": "user"
            },
            "summary": "The remarkable success of multimodal large language models (MLLMs) has driven\nadvances in multimodal embeddings, yet existing models remain inherently\ndiscriminative, limiting their ability to benefit from reasoning-driven\ngeneration paradigm. In this work, we pioneer the exploration of generative\nembeddings, unifying embedding tasks within a generative paradigm. We propose\nUME-R1, a universal multimodal embedding framework consisting of a two-stage\ntraining strategy: a cold-start supervised fine-tuning equips the model with\nreasoning capabilities and enables it to generate both discriminative and\ngenerative embeddings; a subsequent reinforcement learning enhances reasoning\nand further optimizes generative embedding quality. This pioneering work\nreveals four key insights: 1) generative embeddings unlock substantial\nperformance gains over conventional discriminative embeddings by leveraging the\npowerful generative reasoning capabilities of MLLMs; 2) discriminative and\ngenerative embeddings are complementary, whose combined oracle performance far\nexceeding that of either alone; 3) RL can effectively enhance generative\nembeddings, establishing a scalable optimization paradigm.; 4) repeated\nsampling at inference boosts downstream task coverage (pass@k), highlighting\nthe inference-time scalability potential of generative embeddings. Evaluated on\nthe MMEB-V2 benchmark across 78 tasks spanning video, image, and visual\ndocuments, UME-R1 significantly outperforms conventional discriminative\nembedding models and offers a foundation for more interpretable,\nreasoning-driven generative multimodal embeddings. Our code, models, and\ndatasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.",
            "upvotes": 4,
            "discussionId": "69097f4d812eca10f9cc6378",
            "githubRepo": "https://github.com/XMUDeepLIT/UME-R1",
            "ai_summary": "UME-R1, a generative multimodal embedding framework, enhances performance through reasoning-driven generation and reinforcement learning, outperforming conventional discriminative models.",
            "ai_keywords": [
                "multimodal large language models",
                "multimodal embeddings",
                "generative embeddings",
                "discriminative embeddings",
                "two-stage training strategy",
                "cold-start supervised fine-tuning",
                "reinforcement learning",
                "MMEB-V2 benchmark",
                "pass@k",
                "inference-time scalability"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-11-01T01:04:23.000Z",
        "title": "UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings",
        "summary": "The remarkable success of multimodal large language models (MLLMs) has driven\nadvances in multimodal embeddings, yet existing models remain inherently\ndiscriminative, limiting their ability to benefit from reasoning-driven\ngeneration paradigm. In this work, we pioneer the exploration of generative\nembeddings, unifying embedding tasks within a generative paradigm. We propose\nUME-R1, a universal multimodal embedding framework consisting of a two-stage\ntraining strategy: a cold-start supervised fine-tuning equips the model with\nreasoning capabilities and enables it to generate both discriminative and\ngenerative embeddings; a subsequent reinforcement learning enhances reasoning\nand further optimizes generative embedding quality. This pioneering work\nreveals four key insights: 1) generative embeddings unlock substantial\nperformance gains over conventional discriminative embeddings by leveraging the\npowerful generative reasoning capabilities of MLLMs; 2) discriminative and\ngenerative embeddings are complementary, whose combined oracle performance far\nexceeding that of either alone; 3) RL can effectively enhance generative\nembeddings, establishing a scalable optimization paradigm.; 4) repeated\nsampling at inference boosts downstream task coverage (pass@k), highlighting\nthe inference-time scalability potential of generative embeddings. Evaluated on\nthe MMEB-V2 benchmark across 78 tasks spanning video, image, and visual\ndocuments, UME-R1 significantly outperforms conventional discriminative\nembedding models and offers a foundation for more interpretable,\nreasoning-driven generative multimodal embeddings. Our code, models, and\ndatasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.00405.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6626449503e1f561573d30e9",
            "avatarUrl": "/avatars/e7f9720ccd01bae32d0a03a1b0dacab5.svg",
            "fullname": "Zhibin Lan",
            "name": "zhibinlan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.01617",
            "authors": [
                {
                    "_id": "6909c945812eca10f9cc6471",
                    "user": {
                        "_id": "6331c242e092098b57bd8e58",
                        "avatarUrl": "/avatars/8bcaf3cb3482a002ded96d3206b04947.svg",
                        "isPro": false,
                        "fullname": "Mohamed Eltahir",
                        "user": "mohammad2012191",
                        "type": "user"
                    },
                    "name": "Mohamed Eltahir",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T10:49:03.133Z",
                    "hidden": false
                },
                {
                    "_id": "6909c945812eca10f9cc6472",
                    "name": "Ali Habibullah",
                    "hidden": false
                },
                {
                    "_id": "6909c945812eca10f9cc6473",
                    "name": "Lama Ayash",
                    "hidden": false
                },
                {
                    "_id": "6909c945812eca10f9cc6474",
                    "name": "Tanveer Hussain",
                    "hidden": false
                },
                {
                    "_id": "6909c945812eca10f9cc6475",
                    "name": "Naeemullah Khan",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6331c242e092098b57bd8e58/e9xdg-oRnVHlyjFGHzl_4.png"
            ],
            "publishedAt": "2025-11-03T14:25:12.000Z",
            "submittedOnDailyAt": "2025-11-04T09:09:03.747Z",
            "title": "Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers",
            "submittedOnDailyBy": {
                "_id": "6331c242e092098b57bd8e58",
                "avatarUrl": "/avatars/8bcaf3cb3482a002ded96d3206b04947.svg",
                "isPro": false,
                "fullname": "Mohamed Eltahir",
                "user": "mohammad2012191",
                "type": "user"
            },
            "summary": "In the retrieval domain, candidates' fusion from heterogeneous retrievers is\na long-standing challenge, particularly for complex, multi-modal data such as\nvideos. While typical fusion techniques are training-free, they rely solely on\nrank or score signals, disregarding candidates' representations. This work\nintroduces Vote-in-Context (ViC), a generalized, training-free framework that\nre-thinks list-wise reranking and fusion as a zero-shot reasoning task for a\nVision-Language Model (VLM). The core insight is to serialize both content\nevidence and retriever metadata directly within the VLM's prompt, allowing the\nmodel to adaptively weigh retriever consensus against visual-linguistic\ncontent. We demonstrate the generality of this framework by applying it to the\nchallenging domain of cross-modal video retrieval. To this end, we introduce\nthe S-Grid, a compact serialization map that represents each video as an image\ngrid, optionally paired with subtitles to enable list-wise reasoning over video\ncandidates. ViC is evaluated both as a single-list reranker, where it\ndramatically improves the precision of individual retrievers, and as an\nensemble fuser, where it consistently outperforms strong baselines like\nCombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the\nframework establishes new state-of-the-art zero-shot retrieval performance,\ndemonstrating its effectiveness in handling complex visual and temporal signals\nalongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%\n(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive\ngains of up to +40 Recall@1 over previous state-of-the-art baselines. We\npresent ViC as a simple, reproducible, and highly effective recipe for turning\nmodern VLMs into powerful zero-shot rerankers and fusers. Code and resources\nare publicly available at: https://github.com/mohammad2012191/ViC",
            "upvotes": 2,
            "discussionId": "6909c945812eca10f9cc6476",
            "githubRepo": "https://github.com/mohammad2012191/ViC",
            "ai_summary": "Vote-in-Context (ViC) is a training-free framework that leverages Vision-Language Models (VLMs) for zero-shot reranking and fusion in cross-modal video retrieval, achieving state-of-the-art performance.",
            "ai_keywords": [
                "Vote-in-Context",
                "ViC",
                "Vision-Language Model",
                "VLM",
                "list-wise reranking",
                "fusion",
                "zero-shot reasoning",
                "S-Grid",
                "image grid",
                "subtitles",
                "list-wise reasoning",
                "ensemble fuser",
                "CombSUM",
                "ActivityNet",
                "VATEX",
                "Recall@1",
                "zero-shot retrieval"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "642bf38ba208ae9adcebe075",
                "name": "KAUST",
                "fullname": "King Abdullah University of Science and Technology",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6315fb0b29411a6864b05b35/6egitr9tcwgl5i6ikCbkY.jpeg"
            }
        },
        "publishedAt": "2025-11-03T09:25:12.000Z",
        "title": "Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers",
        "summary": "In the retrieval domain, candidates' fusion from heterogeneous retrievers is\na long-standing challenge, particularly for complex, multi-modal data such as\nvideos. While typical fusion techniques are training-free, they rely solely on\nrank or score signals, disregarding candidates' representations. This work\nintroduces Vote-in-Context (ViC), a generalized, training-free framework that\nre-thinks list-wise reranking and fusion as a zero-shot reasoning task for a\nVision-Language Model (VLM). The core insight is to serialize both content\nevidence and retriever metadata directly within the VLM's prompt, allowing the\nmodel to adaptively weigh retriever consensus against visual-linguistic\ncontent. We demonstrate the generality of this framework by applying it to the\nchallenging domain of cross-modal video retrieval. To this end, we introduce\nthe S-Grid, a compact serialization map that represents each video as an image\ngrid, optionally paired with subtitles to enable list-wise reasoning over video\ncandidates. ViC is evaluated both as a single-list reranker, where it\ndramatically improves the precision of individual retrievers, and as an\nensemble fuser, where it consistently outperforms strong baselines like\nCombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the\nframework establishes new state-of-the-art zero-shot retrieval performance,\ndemonstrating its effectiveness in handling complex visual and temporal signals\nalongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%\n(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive\ngains of up to +40 Recall@1 over previous state-of-the-art baselines. We\npresent ViC as a simple, reproducible, and highly effective recipe for turning\nmodern VLMs into powerful zero-shot rerankers and fusers. Code and resources\nare publicly available at: https://github.com/mohammad2012191/ViC",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6331c242e092098b57bd8e58/e9xdg-oRnVHlyjFGHzl_4.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01617.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6331c242e092098b57bd8e58",
            "avatarUrl": "/avatars/8bcaf3cb3482a002ded96d3206b04947.svg",
            "fullname": "Mohamed Eltahir",
            "name": "mohammad2012191",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "642bf38ba208ae9adcebe075",
            "name": "KAUST",
            "fullname": "King Abdullah University of Science and Technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6315fb0b29411a6864b05b35/6egitr9tcwgl5i6ikCbkY.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.00810",
            "authors": [
                {
                    "_id": "690a6cb3d70e173c84528f0c",
                    "name": "Shijie Zhou",
                    "hidden": false
                },
                {
                    "_id": "690a6cb3d70e173c84528f0d",
                    "name": "Viet Dac Lai",
                    "hidden": false
                },
                {
                    "_id": "690a6cb3d70e173c84528f0e",
                    "name": "Hao Tan",
                    "hidden": false
                },
                {
                    "_id": "690a6cb3d70e173c84528f0f",
                    "name": "Jihyung Kil",
                    "hidden": false
                },
                {
                    "_id": "690a6cb3d70e173c84528f10",
                    "name": "Wanrong Zhu",
                    "hidden": false
                },
                {
                    "_id": "690a6cb3d70e173c84528f11",
                    "name": "Changyou Chen",
                    "hidden": false
                },
                {
                    "_id": "690a6cb3d70e173c84528f12",
                    "name": "Ruiyi Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-02T05:34:21.000Z",
            "submittedOnDailyAt": "2025-11-04T18:47:37.610Z",
            "title": "GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor\n  for GUI Grounding",
            "submittedOnDailyBy": {
                "_id": "64668f982da1abc242355cbb",
                "avatarUrl": "/avatars/c9a248b8d70b1a8f7b78265c98690570.svg",
                "isPro": false,
                "fullname": "Ruiyi Zhang",
                "user": "zhangry868",
                "type": "user"
            },
            "summary": "Graphical user interface (GUI) grounding is a key function of computer-use\nagents, which maps natural-language instructions to actionable screen regions.\nExisting approaches based on Multimodal Large Language Models (MLLMs) typically\nformulate it as a text-based coordinate generation task, yet directly\ngenerating precise coordinates from visual inputs remains challenging and\ncomputationally intensive. An intuitive way to implement GUI grounding is to\nfirst select visual patches relevant to the instructions and then determine the\nprecise click location within those patches. Based on the observations that\ngeneral MLLMs have some native grounding capability, nested within their\nattentions, we propose GUI-AIMA, an attention-based and coordinate-free\nsupervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns\nthe intrinsic multimodal attention of MLLMs with patch-wise grounding signals.\nThese signals are calculated adaptively for diverse user instructions by\nmulti-head aggregation on simplified query-visual attention matrices. Besides,\nits coordinate-free manner can easily integrate a plug-and-play zoom-in stage.\nGUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional\ndata efficiency and verifying that light training can trigger the native\ngrounding capability of MLLMs. It achieves state-of-the-art performance among\n3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%\non OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA",
            "upvotes": 2,
            "discussionId": "690a6cb3d70e173c84528f13",
            "projectPage": "https://github.com/sjz5202/GUI-AIMA",
            "githubRepo": "https://github.com/sjz5202/GUI-AIMA",
            "ai_summary": "GUI-AIMA, an attention-based and coordinate-free framework, enhances GUI grounding by aligning MLLM attention with patch-wise signals, achieving state-of-the-art performance with minimal training data.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "MLLMs",
                "GUI grounding",
                "attention-based",
                "coordinate-free",
                "supervised fine-tuning",
                "patch-wise grounding",
                "multi-head aggregation",
                "query-visual attention matrices",
                "data efficiency",
                "native grounding capability",
                "ScreenSpot-Pro",
                "OSWorld-G"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-11-02T01:34:21.000Z",
        "title": "GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor\n  for GUI Grounding",
        "summary": "Graphical user interface (GUI) grounding is a key function of computer-use\nagents, which maps natural-language instructions to actionable screen regions.\nExisting approaches based on Multimodal Large Language Models (MLLMs) typically\nformulate it as a text-based coordinate generation task, yet directly\ngenerating precise coordinates from visual inputs remains challenging and\ncomputationally intensive. An intuitive way to implement GUI grounding is to\nfirst select visual patches relevant to the instructions and then determine the\nprecise click location within those patches. Based on the observations that\ngeneral MLLMs have some native grounding capability, nested within their\nattentions, we propose GUI-AIMA, an attention-based and coordinate-free\nsupervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns\nthe intrinsic multimodal attention of MLLMs with patch-wise grounding signals.\nThese signals are calculated adaptively for diverse user instructions by\nmulti-head aggregation on simplified query-visual attention matrices. Besides,\nits coordinate-free manner can easily integrate a plug-and-play zoom-in stage.\nGUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional\ndata efficiency and verifying that light training can trigger the native\ngrounding capability of MLLMs. It achieves state-of-the-art performance among\n3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%\non OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.00810.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64668f982da1abc242355cbb",
            "avatarUrl": "/avatars/c9a248b8d70b1a8f7b78265c98690570.svg",
            "fullname": "Ruiyi Zhang",
            "name": "zhangry868",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.01706",
            "authors": [
                {
                    "_id": "690a048fd70e173c84528ddf",
                    "name": "Sekh Mainul Islam",
                    "hidden": false
                },
                {
                    "_id": "690a048fd70e173c84528de0",
                    "name": "Pepa Atanasova",
                    "hidden": false
                },
                {
                    "_id": "690a048fd70e173c84528de1",
                    "name": "Isabelle Augenstein",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-03T16:15:06.000Z",
            "submittedOnDailyAt": "2025-11-04T11:21:33.682Z",
            "title": "Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace\n  Disentanglement",
            "submittedOnDailyBy": {
                "_id": "637390430938c0754238276c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637390430938c0754238276c/O7IIJdZsS9yjznh780XwA.jpeg",
                "isPro": false,
                "fullname": "Sekh Mainul Islam",
                "user": "sekhcopenlu",
                "type": "user"
            },
            "summary": "Natural Language Explanations (NLEs) describe how Large Language Models\n(LLMs) make decisions, drawing on both external Context Knowledge (CK) and\nParametric Knowledge (PK) stored in model weights. Understanding their\ninteraction is key to assessing the grounding of NLEs, yet it remains\nunderexplored. Prior work has largely examined only single-step generation,\ntypically the final answer, and has modelled PK and CK interaction only as a\nbinary choice in a rank-1 subspace. This overlooks richer forms of interaction,\nsuch as complementary or supportive knowledge. We propose a novel rank-2\nprojection subspace that disentangles PK and CK contributions more accurately\nand use it for the first multi-step analysis of knowledge interactions across\nlonger NLE sequences. Experiments on four QA datasets and three open-weight\ninstruction-tuned LLMs show that diverse knowledge interactions are poorly\nrepresented in a rank-1 subspace but are effectively captured in our rank-2\nformulation. Our multi-step analysis reveals that hallucinated NLEs align\nstrongly with the PK direction, context-faithful ones balance PK and CK, and\nChain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing\nPK reliance. This work provides the first framework for systematic studies of\nmulti-step knowledge interactions in LLMs through a richer rank-2 subspace\ndisentanglement. Code and data:\nhttps://github.com/copenlu/pk-ck-knowledge-disentanglement.",
            "upvotes": 1,
            "discussionId": "690a048fd70e173c84528de2",
            "githubRepo": "https://github.com/copenlu/pk-ck-knowledge-disentanglement",
            "ai_summary": "A novel rank-2 projection subspace is proposed to analyze multi-step knowledge interactions in Large Language Models, revealing how Parametric Knowledge and Context Knowledge contribute to Natural Language Explanations.",
            "ai_keywords": [
                "Large Language Models",
                "Natural Language Explanations",
                "Context Knowledge",
                "Parametric Knowledge",
                "rank-2 projection subspace",
                "multi-step analysis",
                "knowledge interactions",
                "hallucinated NLEs",
                "context-faithful NLEs",
                "Chain-of-Thought prompting"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "63f951039bf98d3ed29a0d32",
                "name": "UCPH",
                "fullname": "University of Copenhagen",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1677283576670-63a5f0a028af3c9aa2079e6f.png"
            }
        },
        "publishedAt": "2025-11-03T11:15:06.000Z",
        "title": "Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace\n  Disentanglement",
        "summary": "Natural Language Explanations (NLEs) describe how Large Language Models\n(LLMs) make decisions, drawing on both external Context Knowledge (CK) and\nParametric Knowledge (PK) stored in model weights. Understanding their\ninteraction is key to assessing the grounding of NLEs, yet it remains\nunderexplored. Prior work has largely examined only single-step generation,\ntypically the final answer, and has modelled PK and CK interaction only as a\nbinary choice in a rank-1 subspace. This overlooks richer forms of interaction,\nsuch as complementary or supportive knowledge. We propose a novel rank-2\nprojection subspace that disentangles PK and CK contributions more accurately\nand use it for the first multi-step analysis of knowledge interactions across\nlonger NLE sequences. Experiments on four QA datasets and three open-weight\ninstruction-tuned LLMs show that diverse knowledge interactions are poorly\nrepresented in a rank-1 subspace but are effectively captured in our rank-2\nformulation. Our multi-step analysis reveals that hallucinated NLEs align\nstrongly with the PK direction, context-faithful ones balance PK and CK, and\nChain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing\nPK reliance. This work provides the first framework for systematic studies of\nmulti-step knowledge interactions in LLMs through a richer rank-2 subspace\ndisentanglement. Code and data:\nhttps://github.com/copenlu/pk-ck-knowledge-disentanglement.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01706.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "637390430938c0754238276c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637390430938c0754238276c/O7IIJdZsS9yjznh780XwA.jpeg",
            "fullname": "Sekh Mainul Islam",
            "name": "sekhcopenlu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "63f951039bf98d3ed29a0d32",
            "name": "UCPH",
            "fullname": "University of Copenhagen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1677283576670-63a5f0a028af3c9aa2079e6f.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.01144",
            "authors": [
                {
                    "_id": "690a90ebd70e173c84528f36",
                    "name": "Md Tanvirul Alam",
                    "hidden": false
                },
                {
                    "_id": "690a90ebd70e173c84528f37",
                    "name": "Dipkamal Bhusal",
                    "hidden": false
                },
                {
                    "_id": "690a90ebd70e173c84528f38",
                    "name": "Salman Ahmad",
                    "hidden": false
                },
                {
                    "_id": "690a90ebd70e173c84528f39",
                    "name": "Nidhi Rastogi",
                    "hidden": false
                },
                {
                    "_id": "690a90ebd70e173c84528f3a",
                    "name": "Peter Worth",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-03T01:45:29.000Z",
            "submittedOnDailyAt": "2025-11-04T22:35:54.692Z",
            "title": "AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat\n  Intelligence",
            "submittedOnDailyBy": {
                "_id": "620c1a977af55d45f5519914",
                "avatarUrl": "/avatars/4d2fb7c1a5ad5dabdb8888fa2fe72e65.svg",
                "isPro": true,
                "fullname": "Tanvirul Alam",
                "user": "Tanvirul",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in natural\nlanguage reasoning, yet their application to Cyber Threat Intelligence (CTI)\nremains limited. CTI analysis involves distilling large volumes of unstructured\nreports into actionable knowledge, a process where LLMs could substantially\nreduce analyst workload. CTIBench introduced a comprehensive benchmark for\nevaluating LLMs across multiple CTI tasks. In this work, we extend CTIBench by\ndeveloping AthenaBench, an enhanced benchmark that includes an improved dataset\ncreation pipeline, duplicate removal, refined evaluation metrics, and a new\ntask focused on risk mitigation strategies. We evaluate twelve LLMs, including\nstate-of-the-art proprietary models such as GPT-5 and Gemini-2.5 Pro, alongside\nseven open-source models from the LLaMA and Qwen families. While proprietary\nLLMs achieve stronger results overall, their performance remains subpar on\nreasoning-intensive tasks, such as threat actor attribution and risk\nmitigation, with open-source models trailing even further behind. These\nfindings highlight fundamental limitations in the reasoning capabilities of\ncurrent LLMs and underscore the need for models explicitly tailored to CTI\nworkflows and automation.",
            "upvotes": 1,
            "discussionId": "690a90ecd70e173c84528f3b",
            "githubRepo": "https://github.com/Athena-Software-Group/athenabench",
            "ai_summary": "AthenaBench, an enhanced benchmark for evaluating LLMs in CTI, reveals limitations in reasoning capabilities of current models, especially for tasks like threat actor attribution and risk mitigation.",
            "ai_keywords": [
                "Large Language Models",
                "CTI",
                "Cyber Threat Intelligence",
                "CTIBench",
                "AthenaBench",
                "dataset creation pipeline",
                "duplicate removal",
                "evaluation metrics",
                "risk mitigation strategies",
                "GPT-5",
                "Gemini-2.5 Pro",
                "LLaMA",
                "Qwen"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-11-02T20:45:29.000Z",
        "title": "AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat\n  Intelligence",
        "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in natural\nlanguage reasoning, yet their application to Cyber Threat Intelligence (CTI)\nremains limited. CTI analysis involves distilling large volumes of unstructured\nreports into actionable knowledge, a process where LLMs could substantially\nreduce analyst workload. CTIBench introduced a comprehensive benchmark for\nevaluating LLMs across multiple CTI tasks. In this work, we extend CTIBench by\ndeveloping AthenaBench, an enhanced benchmark that includes an improved dataset\ncreation pipeline, duplicate removal, refined evaluation metrics, and a new\ntask focused on risk mitigation strategies. We evaluate twelve LLMs, including\nstate-of-the-art proprietary models such as GPT-5 and Gemini-2.5 Pro, alongside\nseven open-source models from the LLaMA and Qwen families. While proprietary\nLLMs achieve stronger results overall, their performance remains subpar on\nreasoning-intensive tasks, such as threat actor attribution and risk\nmitigation, with open-source models trailing even further behind. These\nfindings highlight fundamental limitations in the reasoning capabilities of\ncurrent LLMs and underscore the need for models explicitly tailored to CTI\nworkflows and automation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01144.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "620c1a977af55d45f5519914",
            "avatarUrl": "/avatars/4d2fb7c1a5ad5dabdb8888fa2fe72e65.svg",
            "fullname": "Tanvirul Alam",
            "name": "Tanvirul",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]