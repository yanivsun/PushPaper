[
    {
        "paper": {
            "id": "2510.25616",
            "authors": [
                {
                    "_id": "690b2b3ad70e173c8452913b",
                    "name": "Nikita Kachaev",
                    "hidden": false
                },
                {
                    "_id": "690b2b3ad70e173c8452913c",
                    "name": "Mikhail Kolosov",
                    "hidden": false
                },
                {
                    "_id": "690b2b3ad70e173c8452913d",
                    "user": {
                        "_id": "67583d59b75044e5c970876e",
                        "avatarUrl": "/avatars/85d5797197bc89775922fccd6c4db3ce.svg",
                        "isPro": false,
                        "fullname": "Daniil Zelezetsky",
                        "user": "zelezetsky",
                        "type": "user"
                    },
                    "name": "Daniil Zelezetsky",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-05T16:24:10.935Z",
                    "hidden": false
                },
                {
                    "_id": "690b2b3ad70e173c8452913e",
                    "name": "Alexey K. Kovalev",
                    "hidden": false
                },
                {
                    "_id": "690b2b3ad70e173c8452913f",
                    "name": "Aleksandr I. Panov",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64705ef84be5cf1f3348e283/3uiRVwV1O4F9igfnAheL3.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64705ef84be5cf1f3348e283/3EDhUUSvXsBEwjQLyeqOv.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64705ef84be5cf1f3348e283/B5JJL2VMVgeT6LOGiQskw.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64705ef84be5cf1f3348e283/NNuvVzsX7v3oJcBcWEBXd.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64705ef84be5cf1f3348e283/Bb4JGxQJlaaip8CsMmsRI.png"
            ],
            "publishedAt": "2025-10-29T15:20:10.000Z",
            "submittedOnDailyAt": "2025-11-05T08:17:48.318Z",
            "title": "Don't Blind Your VLA: Aligning Visual Representations for OOD\n  Generalization",
            "submittedOnDailyBy": {
                "_id": "64705ef84be5cf1f3348e283",
                "avatarUrl": "/avatars/915875e7c4118098ab460831d5e8ef0e.svg",
                "isPro": false,
                "fullname": "Nikita",
                "user": "tttonyalpha",
                "type": "user"
            },
            "summary": "The growing success of Vision-Language-Action (VLA) models stems from the\npromise that pretrained Vision-Language Models (VLMs) can endow agents with\ntransferable world knowledge and vision-language (VL) grounding, laying a\nfoundation for action models with broader generalization. Yet when these VLMs\nare adapted to the action modality, it remains unclear to what extent their\noriginal VL representations and knowledge are preserved. In this work, we\nconduct a systematic study of representation retention during VLA fine-tuning,\nshowing that naive action fine-tuning leads to degradation of visual\nrepresentations. To characterize and measure these effects, we probe VLA's\nhidden representations and analyze attention maps, further, we design a set of\ntargeted tasks and methods that contrast VLA models with their counterpart\nVLMs, isolating changes in VL capabilities induced by action fine-tuning. We\nfurther evaluate a range of strategies for aligning visual representations and\nintroduce a simple yet effective method that mitigates degradation and yields\nimproved generalization to out-of-distribution (OOD) scenarios. Taken together,\nour analysis clarifies the trade-off between action fine-tuning and the\ndegradation of VL representations and highlights practical approaches to\nrecover inherited VL capabilities. Code is publicly available:\nhttps://blind-vla-paper.github.io",
            "upvotes": 70,
            "discussionId": "690b2b3ad70e173c84529140",
            "projectPage": "https://blind-vla-paper.github.io",
            "ai_summary": "Systematic study reveals that naive action fine-tuning degrades visual representations in Vision-Language-Action models, but targeted strategies can mitigate this and improve generalization.",
            "ai_keywords": [
                "Vision-Language-Action models",
                "Vision-Language Models",
                "VLA fine-tuning",
                "visual representations",
                "attention maps",
                "out-of-distribution scenarios"
            ],
            "githubStars": 10
        },
        "publishedAt": "2025-10-29T11:20:10.000Z",
        "title": "Don't Blind Your VLA: Aligning Visual Representations for OOD\n  Generalization",
        "summary": "The growing success of Vision-Language-Action (VLA) models stems from the\npromise that pretrained Vision-Language Models (VLMs) can endow agents with\ntransferable world knowledge and vision-language (VL) grounding, laying a\nfoundation for action models with broader generalization. Yet when these VLMs\nare adapted to the action modality, it remains unclear to what extent their\noriginal VL representations and knowledge are preserved. In this work, we\nconduct a systematic study of representation retention during VLA fine-tuning,\nshowing that naive action fine-tuning leads to degradation of visual\nrepresentations. To characterize and measure these effects, we probe VLA's\nhidden representations and analyze attention maps, further, we design a set of\ntargeted tasks and methods that contrast VLA models with their counterpart\nVLMs, isolating changes in VL capabilities induced by action fine-tuning. We\nfurther evaluate a range of strategies for aligning visual representations and\nintroduce a simple yet effective method that mitigates degradation and yields\nimproved generalization to out-of-distribution (OOD) scenarios. Taken together,\nour analysis clarifies the trade-off between action fine-tuning and the\ndegradation of VL representations and highlights practical approaches to\nrecover inherited VL capabilities. Code is publicly available:\nhttps://blind-vla-paper.github.io",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64705ef84be5cf1f3348e283/3uiRVwV1O4F9igfnAheL3.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64705ef84be5cf1f3348e283/3EDhUUSvXsBEwjQLyeqOv.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64705ef84be5cf1f3348e283/B5JJL2VMVgeT6LOGiQskw.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64705ef84be5cf1f3348e283/NNuvVzsX7v3oJcBcWEBXd.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64705ef84be5cf1f3348e283/Bb4JGxQJlaaip8CsMmsRI.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25616.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64705ef84be5cf1f3348e283",
            "avatarUrl": "/avatars/915875e7c4118098ab460831d5e8ef0e.svg",
            "fullname": "Nikita",
            "name": "tttonyalpha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.02778",
            "authors": [
                {
                    "_id": "690aba32d70e173c84528fa7",
                    "user": {
                        "_id": "64440be5af034cdfd69ca3a7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
                        "isPro": false,
                        "fullname": "Qinghong (Kevin) Lin",
                        "user": "KevinQHLin",
                        "type": "user"
                    },
                    "name": "Kevin Qinghong Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-05T16:24:42.232Z",
                    "hidden": false
                },
                {
                    "_id": "690aba32d70e173c84528fa8",
                    "name": "Yuhao Zheng",
                    "hidden": false
                },
                {
                    "_id": "690aba32d70e173c84528fa9",
                    "user": {
                        "_id": "67cf130b3bec454b361a6b70",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/XMcVRwcKJtUfY_WAvTsM6.jpeg",
                        "isPro": false,
                        "fullname": "Hangyu Ran",
                        "user": "hangyuran",
                        "type": "user"
                    },
                    "name": "Hangyu Ran",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-05T16:24:40.244Z",
                    "hidden": false
                },
                {
                    "_id": "690aba32d70e173c84528faa",
                    "name": "Dantong Zhu",
                    "hidden": false
                },
                {
                    "_id": "690aba32d70e173c84528fab",
                    "name": "Dongxing Mao",
                    "hidden": false
                },
                {
                    "_id": "690aba32d70e173c84528fac",
                    "name": "Linjie Li",
                    "hidden": false
                },
                {
                    "_id": "690aba32d70e173c84528fad",
                    "name": "Philip Torr",
                    "hidden": false
                },
                {
                    "_id": "690aba32d70e173c84528fae",
                    "name": "Alex Jinpeng Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-04T18:00:18.000Z",
            "submittedOnDailyAt": "2025-11-05T00:18:37.196Z",
            "title": "VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual\n  Representation",
            "submittedOnDailyBy": {
                "_id": "64440be5af034cdfd69ca3a7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
                "isPro": false,
                "fullname": "Qinghong (Kevin) Lin",
                "user": "KevinQHLin",
                "type": "user"
            },
            "summary": "Code has emerged as a precise and executable medium for reasoning and action\nin the agent era. Yet, progress has largely focused on language-centric tasks\nsuch as program synthesis and debugging, leaving visual-centric coding\nunderexplored. Inspired by how humans reason over sketches, we advocate SVG\ncode as a compact, interpretable, and executable visual representation. We\nintroduce VCode, a benchmark that reframes multimodal understanding as code\ngeneration: given an image, a model must produce SVG that preserves symbolic\nmeaning for downstream reasoning. VCode covers three domains - general\ncommonsense (MM-Vet), professional disciplines (MMMU), and visual-centric\nperception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel\nevaluation protocol in which a policy model answers questions over rendered\nSVGs; correct answers indicate faithful symbolic preservation. Empirically,\nfrontier VLMs struggle to generate faithful SVGs, revealing a persistent gap\nbetween language-centric and visual-centric coding. To close this gap, we\nintroduce VCoder, an agentic framework that augments VLMs along two axes: (i)\nThinking with Revision, which iteratively analyzes discrepancies and refines\nSVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply\nstructured cues such as objects, shapes, and text beyond the model's intrinsic\ncapacity. Across benchmarks, frontier VLMs with strong reasoning capabilities\nscore well overall yet remain limited in professional knowledge and 3D\nreasoning. VCoder delivers a 12.3-point overall gain over the top-performing\nClaude-4-Opus. Human studies show that both humans and VLMs perform worse on\nrendered SVGs, their consistency reveals the promise of symbolic visual\nrepresentation. The benchmark and code are available at\nhttps://github.com/CSU-JPG/VCode.",
            "upvotes": 64,
            "discussionId": "690aba33d70e173c84528faf",
            "projectPage": "https://csu-jpg.github.io/VCode/",
            "githubRepo": "https://github.com/CSU-JPG/VCode/tree/main",
            "ai_summary": "VCode introduces a benchmark for generating SVG code from images to preserve symbolic meaning, highlighting gaps in visual-centric coding and proposing VCoder to improve performance.",
            "ai_keywords": [
                "SVG",
                "VCode",
                "multimodal understanding",
                "code generation",
                "CodeVQA",
                "VLMs",
                "Thinking with Revision",
                "Acting with Visual Tools",
                "professional knowledge",
                "3D reasoning",
                "VCoder"
            ],
            "githubStars": 50,
            "organization": {
                "_id": "67ab7720792eebb05080c926",
                "name": "CSU-JPG",
                "fullname": "Jinpeng Group",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62333a88fd7bb4a39b92d387/MHfLrhVz0KqH6ydx1UrOc.jpeg"
            }
        },
        "publishedAt": "2025-11-04T13:00:18.000Z",
        "title": "VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual\n  Representation",
        "summary": "Code has emerged as a precise and executable medium for reasoning and action\nin the agent era. Yet, progress has largely focused on language-centric tasks\nsuch as program synthesis and debugging, leaving visual-centric coding\nunderexplored. Inspired by how humans reason over sketches, we advocate SVG\ncode as a compact, interpretable, and executable visual representation. We\nintroduce VCode, a benchmark that reframes multimodal understanding as code\ngeneration: given an image, a model must produce SVG that preserves symbolic\nmeaning for downstream reasoning. VCode covers three domains - general\ncommonsense (MM-Vet), professional disciplines (MMMU), and visual-centric\nperception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel\nevaluation protocol in which a policy model answers questions over rendered\nSVGs; correct answers indicate faithful symbolic preservation. Empirically,\nfrontier VLMs struggle to generate faithful SVGs, revealing a persistent gap\nbetween language-centric and visual-centric coding. To close this gap, we\nintroduce VCoder, an agentic framework that augments VLMs along two axes: (i)\nThinking with Revision, which iteratively analyzes discrepancies and refines\nSVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply\nstructured cues such as objects, shapes, and text beyond the model's intrinsic\ncapacity. Across benchmarks, frontier VLMs with strong reasoning capabilities\nscore well overall yet remain limited in professional knowledge and 3D\nreasoning. VCoder delivers a 12.3-point overall gain over the top-performing\nClaude-4-Opus. Human studies show that both humans and VLMs perform worse on\nrendered SVGs, their consistency reveals the promise of symbolic visual\nrepresentation. The benchmark and code are available at\nhttps://github.com/CSU-JPG/VCode.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02778.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64440be5af034cdfd69ca3a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
            "fullname": "Qinghong (Kevin) Lin",
            "name": "KevinQHLin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 38
        },
        "organization": {
            "_id": "67ab7720792eebb05080c926",
            "name": "CSU-JPG",
            "fullname": "Jinpeng Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62333a88fd7bb4a39b92d387/MHfLrhVz0KqH6ydx1UrOc.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.02779",
            "authors": [
                {
                    "_id": "690adcf8d70e173c84529049",
                    "name": "Yiyang Zhou",
                    "hidden": false
                },
                {
                    "_id": "690adcf8d70e173c8452904a",
                    "name": "Haoqin Tu",
                    "hidden": false
                },
                {
                    "_id": "690adcf8d70e173c8452904b",
                    "name": "Zijun Wang",
                    "hidden": false
                },
                {
                    "_id": "690adcf8d70e173c8452904c",
                    "name": "Zeyu Wang",
                    "hidden": false
                },
                {
                    "_id": "690adcf8d70e173c8452904d",
                    "name": "Niklas Muennighoff",
                    "hidden": false
                },
                {
                    "_id": "690adcf8d70e173c8452904e",
                    "name": "Fan Nie",
                    "hidden": false
                },
                {
                    "_id": "690adcf8d70e173c8452904f",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "690adcf8d70e173c84529050",
                    "name": "James Zou",
                    "hidden": false
                },
                {
                    "_id": "690adcf8d70e173c84529051",
                    "name": "Chaorui Deng",
                    "hidden": false
                },
                {
                    "_id": "690adcf8d70e173c84529052",
                    "name": "Shen Yan",
                    "hidden": false
                },
                {
                    "_id": "690adcf8d70e173c84529053",
                    "name": "Haoqi Fan",
                    "hidden": false
                },
                {
                    "_id": "690adcf8d70e173c84529054",
                    "name": "Cihang Xie",
                    "hidden": false
                },
                {
                    "_id": "690adcf8d70e173c84529055",
                    "name": "Huaxiu Yao",
                    "hidden": false
                },
                {
                    "_id": "690adcf8d70e173c84529056",
                    "name": "Qinghao Ye",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-04T18:00:51.000Z",
            "submittedOnDailyAt": "2025-11-05T02:45:55.873Z",
            "title": "When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for\n  Visual Chain-of-Thought",
            "submittedOnDailyBy": {
                "_id": "6433722c5277e3b24ef49055",
                "avatarUrl": "/avatars/f3c5560d500c699e452986a6a45ba3ee.svg",
                "isPro": false,
                "fullname": "Yiyang Zhou",
                "user": "YiyangAiLab",
                "type": "user"
            },
            "summary": "We propose MIRA, a new benchmark designed to evaluate models in scenarios\nwhere generating intermediate visual images is essential for successful\nreasoning. Unlike traditional CoT methods that rely solely on text, tasks in\nMIRA require models to generate and utilize intermediate images - such as\nsketches, structural diagrams, or path drawings - to guide their reasoning\nprocess. This setup closely mirrors how humans solve complex problems through\n\"drawing to think\". To solve this, MIRA focuses on tasks that are intrinsically\nchallenging and involve complex structures, spatial relationships, or reasoning\nsteps that are difficult to express through language alone. To ensure that our\nevaluation data is of high-quality, we include 546 multimodal problems,\nannotated with intermediate visual images and final answers. We also propose a\nunified evaluation protocol for MIRA that spans three levels of evaluation\ninput: direct input with image and question only, text-only CoT input with\nimage and thinking prompts, and Visual-CoT input with both annotated image\nclues and textual thinking prompts. To probe the upper bound of model capacity\non our benchmark, we also report pass@k and majority voting accuracies under\ndifferent k settings. Experimental results show that existing multimodal large\nlanguage models, including strongest private models as well as strong\nopen-weight models, perform poorly when relying solely on textual prompts.\nHowever, when intermediate visual cues are provided, model performance improves\nconsistently, yielding an average relative gain of 33.7% across all models and\ntasks. We also probe the upper bound by expanding the search space and\ndesigning textual prompts aligned with Visual-CoT, but both yield only limited\nimprovements compared to our Visual-CoT setting. These results underscore the\ncritical role of imagined visual information in enabling successful reasoning\non MIRA.",
            "upvotes": 42,
            "discussionId": "690adcf8d70e173c84529057",
            "projectPage": "https://mira-benchmark.github.io/",
            "ai_summary": "MIRA is a benchmark that evaluates models using intermediate visual images to enhance reasoning, showing significant performance improvements over text-only methods.",
            "ai_keywords": [
                "CoT methods",
                "intermediate visual images",
                "sketches",
                "structural diagrams",
                "path drawings",
                "multimodal problems",
                "evaluation protocol",
                "pass@k",
                "majority voting accuracies",
                "multimodal large language models",
                "Visual-CoT input"
            ],
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-11-04T13:00:51.000Z",
        "title": "When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for\n  Visual Chain-of-Thought",
        "summary": "We propose MIRA, a new benchmark designed to evaluate models in scenarios\nwhere generating intermediate visual images is essential for successful\nreasoning. Unlike traditional CoT methods that rely solely on text, tasks in\nMIRA require models to generate and utilize intermediate images - such as\nsketches, structural diagrams, or path drawings - to guide their reasoning\nprocess. This setup closely mirrors how humans solve complex problems through\n\"drawing to think\". To solve this, MIRA focuses on tasks that are intrinsically\nchallenging and involve complex structures, spatial relationships, or reasoning\nsteps that are difficult to express through language alone. To ensure that our\nevaluation data is of high-quality, we include 546 multimodal problems,\nannotated with intermediate visual images and final answers. We also propose a\nunified evaluation protocol for MIRA that spans three levels of evaluation\ninput: direct input with image and question only, text-only CoT input with\nimage and thinking prompts, and Visual-CoT input with both annotated image\nclues and textual thinking prompts. To probe the upper bound of model capacity\non our benchmark, we also report pass@k and majority voting accuracies under\ndifferent k settings. Experimental results show that existing multimodal large\nlanguage models, including strongest private models as well as strong\nopen-weight models, perform poorly when relying solely on textual prompts.\nHowever, when intermediate visual cues are provided, model performance improves\nconsistently, yielding an average relative gain of 33.7% across all models and\ntasks. We also probe the upper bound by expanding the search space and\ndesigning textual prompts aligned with Visual-CoT, but both yield only limited\nimprovements compared to our Visual-CoT setting. These results underscore the\ncritical role of imagined visual information in enabling successful reasoning\non MIRA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02779.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6433722c5277e3b24ef49055",
            "avatarUrl": "/avatars/f3c5560d500c699e452986a6a45ba3ee.svg",
            "fullname": "Yiyang Zhou",
            "name": "YiyangAiLab",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.02243",
            "authors": [
                {
                    "_id": "690ae419d70e173c8452906b",
                    "name": "Zhuoran Zhang",
                    "hidden": false
                },
                {
                    "_id": "690ae419d70e173c8452906c",
                    "name": "Tengyue Wang",
                    "hidden": false
                },
                {
                    "_id": "690ae419d70e173c8452906d",
                    "name": "Xilin Gong",
                    "hidden": false
                },
                {
                    "_id": "690ae419d70e173c8452906e",
                    "user": {
                        "_id": "673c7319d11b1c2e246ead9c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
                        "isPro": false,
                        "fullname": "Yang Shi",
                        "user": "DogNeverSleep",
                        "type": "user"
                    },
                    "name": "Yang Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-05T16:24:28.669Z",
                    "hidden": false
                },
                {
                    "_id": "690ae419d70e173c8452906f",
                    "name": "Haotian Wang",
                    "hidden": false
                },
                {
                    "_id": "690ae419d70e173c84529070",
                    "name": "Di Wang",
                    "hidden": false
                },
                {
                    "_id": "690ae419d70e173c84529071",
                    "name": "Lijie Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-04T04:11:31.000Z",
            "submittedOnDailyAt": "2025-11-05T03:14:18.767Z",
            "title": "When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs\n  Preference Dynamics in MLLMs",
            "submittedOnDailyBy": {
                "_id": "673c7319d11b1c2e246ead9c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
                "isPro": false,
                "fullname": "Yang Shi",
                "user": "DogNeverSleep",
                "type": "user"
            },
            "summary": "Multimodal large language models (MLLMs) must resolve conflicts when\ndifferent modalities provide contradictory information, a process we term\nmodality following. Prior work measured this behavior only with coarse\ndataset-level statistics, overlooking the influence of model's confidence in\nunimodal reasoning. In this paper, we introduce a new framework that decomposes\nmodality following into two fundamental factors: relative reasoning uncertainty\n(the case-specific confidence gap between unimodal predictions) and inherent\nmodality preference( a model's stable bias when uncertainties are balanced). To\nvalidate this framework, we construct a controllable dataset that\nsystematically varies the reasoning difficulty of visual and textual inputs.\nUsing entropy as a fine-grained uncertainty metric, we uncover a universal law:\nthe probability of following a modality decreases monotonically as its relative\nuncertainty increases. At the relative difficulty level where the model tends\nto follow both modalities with comparable probability what we call the balance\npoint, a practical indicator of the model's inherent preference. Unlike\ntraditional macro-level ratios, this measure offers a more principled and less\nconfounded way to characterize modality bias, disentangling it from unimodal\ncapabilities and dataset artifacts. Further, by probing layer-wise predictions,\nwe reveal the internal mechanism of oscillation: in ambiguous regions near the\nbalance point, models vacillate between modalities across layers, explaining\nexternally observed indecision. Together, these findings establish relative\nuncertainty and inherent preference as the two governing principles of modality\nfollowing, offering both a quantitative framework and mechanistic insight into\nhow MLLMs resolve conflicting information.",
            "upvotes": 20,
            "discussionId": "690ae41ad70e173c84529072",
            "ai_summary": "A framework decomposes modality following in multimodal large language models into relative reasoning uncertainty and inherent modality preference, providing insights into how models resolve conflicting information.",
            "ai_keywords": [
                "multimodal large language models",
                "modality following",
                "relative reasoning uncertainty",
                "inherent modality preference",
                "entropy",
                "balance point",
                "layer-wise predictions"
            ]
        },
        "publishedAt": "2025-11-03T23:11:31.000Z",
        "title": "When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs\n  Preference Dynamics in MLLMs",
        "summary": "Multimodal large language models (MLLMs) must resolve conflicts when\ndifferent modalities provide contradictory information, a process we term\nmodality following. Prior work measured this behavior only with coarse\ndataset-level statistics, overlooking the influence of model's confidence in\nunimodal reasoning. In this paper, we introduce a new framework that decomposes\nmodality following into two fundamental factors: relative reasoning uncertainty\n(the case-specific confidence gap between unimodal predictions) and inherent\nmodality preference( a model's stable bias when uncertainties are balanced). To\nvalidate this framework, we construct a controllable dataset that\nsystematically varies the reasoning difficulty of visual and textual inputs.\nUsing entropy as a fine-grained uncertainty metric, we uncover a universal law:\nthe probability of following a modality decreases monotonically as its relative\nuncertainty increases. At the relative difficulty level where the model tends\nto follow both modalities with comparable probability what we call the balance\npoint, a practical indicator of the model's inherent preference. Unlike\ntraditional macro-level ratios, this measure offers a more principled and less\nconfounded way to characterize modality bias, disentangling it from unimodal\ncapabilities and dataset artifacts. Further, by probing layer-wise predictions,\nwe reveal the internal mechanism of oscillation: in ambiguous regions near the\nbalance point, models vacillate between modalities across layers, explaining\nexternally observed indecision. Together, these findings establish relative\nuncertainty and inherent preference as the two governing principles of modality\nfollowing, offering both a quantitative framework and mechanistic insight into\nhow MLLMs resolve conflicting information.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02243.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "673c7319d11b1c2e246ead9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
            "fullname": "Yang Shi",
            "name": "DogNeverSleep",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.02687",
            "authors": [
                {
                    "_id": "690b32b260494e4fa76754e7",
                    "name": "Tim R. Davidson",
                    "hidden": false
                },
                {
                    "_id": "690b32b260494e4fa76754e8",
                    "name": "Adam Fourney",
                    "hidden": false
                },
                {
                    "_id": "690b32b260494e4fa76754e9",
                    "name": "Saleema Amershi",
                    "hidden": false
                },
                {
                    "_id": "690b32b260494e4fa76754ea",
                    "name": "Robert West",
                    "hidden": false
                },
                {
                    "_id": "690b32b260494e4fa76754eb",
                    "name": "Eric Horvitz",
                    "hidden": false
                },
                {
                    "_id": "690b32b260494e4fa76754ec",
                    "name": "Ece Kamar",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65c9e65bd6bed59d801a5eb3/PvNYO5wJOYOlzLGMbmEC7.png",
                "https://cdn-uploads.huggingface.co/production/uploads/65c9e65bd6bed59d801a5eb3/hKEjgHqgs0Av0-B7d0UNA.png",
                "https://cdn-uploads.huggingface.co/production/uploads/65c9e65bd6bed59d801a5eb3/7plWfwxsNpRuF2ZQgPWdb.png",
                "https://cdn-uploads.huggingface.co/production/uploads/65c9e65bd6bed59d801a5eb3/xv8WA8kwqOuUvAfmJOrHi.png",
                "https://cdn-uploads.huggingface.co/production/uploads/65c9e65bd6bed59d801a5eb3/e8iEZ3a1zsM4tllwRtOFW.png",
                "https://cdn-uploads.huggingface.co/production/uploads/65c9e65bd6bed59d801a5eb3/eD_qdOXGgci7Fdh_QWPJy.png"
            ],
            "publishedAt": "2025-11-04T16:10:57.000Z",
            "submittedOnDailyAt": "2025-11-05T09:56:48.389Z",
            "title": "The Collaboration Gap",
            "submittedOnDailyBy": {
                "_id": "65c9e65bd6bed59d801a5eb3",
                "avatarUrl": "/avatars/431bc46350cbbe996bb058e03ccc2fd0.svg",
                "isPro": false,
                "fullname": "Tim Davidson",
                "user": "trdavidson",
                "type": "user"
            },
            "summary": "The trajectory of AI development suggests that we will increasingly rely on\nagent-based systems composed of independently developed agents with different\ninformation, privileges, and tools. The success of these systems will\ncritically depend on effective collaboration among these heterogeneous agents,\neven under partial observability. Despite intense interest, few empirical\nstudies have evaluated such agent-agent collaboration at scale. We propose a\ncollaborative maze-solving benchmark that (i) isolates collaborative\ncapabilities, (ii) modulates problem complexity, (iii) enables scalable\nautomated grading, and (iv) imposes no output-format constraints, preserving\necological plausibility. Using this framework, we evaluate 32 leading open- and\nclosed-source models in solo, homogeneous, and heterogeneous pairings. Our\nresults reveal a \"collaboration gap\": models that perform well solo often\ndegrade substantially when required to collaborate. Collaboration can break\ndown dramatically; for instance, small distilled models that solve mazes well\nalone may fail almost completely in certain pairings. We find that starting\nwith the stronger agent often improves outcomes, motivating a \"relay inference\"\napproach where the stronger agent leads before handing off to the weaker one,\nclosing much of the gap. Our findings argue for (1) collaboration-aware\nevaluation, (2) training strategies developed to enhance collaborative\ncapabilities, and (3) interaction design that reliably elicits agents' latent\nskills, guidance that applies to AI-AI and human-AI collaboration.",
            "upvotes": 10,
            "discussionId": "690b32b360494e4fa76754ed",
            "ai_summary": "Evaluation of agent-based systems reveals a collaboration gap where solo-performing models degrade in pairings, suggesting the need for collaboration-aware evaluation and training strategies.",
            "ai_keywords": [
                "agent-based systems",
                "collaborative maze-solving benchmark",
                "collaboration gap",
                "relay inference",
                "collaboration-aware evaluation",
                "training strategies",
                "interaction design"
            ],
            "organization": {
                "_id": "68151d0f51add3813f3f7d1b",
                "name": "MicrosoftResearch",
                "fullname": "Microsoft Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
            }
        },
        "publishedAt": "2025-11-04T11:10:57.000Z",
        "title": "The Collaboration Gap",
        "summary": "The trajectory of AI development suggests that we will increasingly rely on\nagent-based systems composed of independently developed agents with different\ninformation, privileges, and tools. The success of these systems will\ncritically depend on effective collaboration among these heterogeneous agents,\neven under partial observability. Despite intense interest, few empirical\nstudies have evaluated such agent-agent collaboration at scale. We propose a\ncollaborative maze-solving benchmark that (i) isolates collaborative\ncapabilities, (ii) modulates problem complexity, (iii) enables scalable\nautomated grading, and (iv) imposes no output-format constraints, preserving\necological plausibility. Using this framework, we evaluate 32 leading open- and\nclosed-source models in solo, homogeneous, and heterogeneous pairings. Our\nresults reveal a \"collaboration gap\": models that perform well solo often\ndegrade substantially when required to collaborate. Collaboration can break\ndown dramatically; for instance, small distilled models that solve mazes well\nalone may fail almost completely in certain pairings. We find that starting\nwith the stronger agent often improves outcomes, motivating a \"relay inference\"\napproach where the stronger agent leads before handing off to the weaker one,\nclosing much of the gap. Our findings argue for (1) collaboration-aware\nevaluation, (2) training strategies developed to enhance collaborative\ncapabilities, and (3) interaction design that reliably elicits agents' latent\nskills, guidance that applies to AI-AI and human-AI collaboration.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65c9e65bd6bed59d801a5eb3/PvNYO5wJOYOlzLGMbmEC7.png",
            "https://cdn-uploads.huggingface.co/production/uploads/65c9e65bd6bed59d801a5eb3/hKEjgHqgs0Av0-B7d0UNA.png",
            "https://cdn-uploads.huggingface.co/production/uploads/65c9e65bd6bed59d801a5eb3/7plWfwxsNpRuF2ZQgPWdb.png",
            "https://cdn-uploads.huggingface.co/production/uploads/65c9e65bd6bed59d801a5eb3/xv8WA8kwqOuUvAfmJOrHi.png",
            "https://cdn-uploads.huggingface.co/production/uploads/65c9e65bd6bed59d801a5eb3/e8iEZ3a1zsM4tllwRtOFW.png",
            "https://cdn-uploads.huggingface.co/production/uploads/65c9e65bd6bed59d801a5eb3/eD_qdOXGgci7Fdh_QWPJy.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02687.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65c9e65bd6bed59d801a5eb3",
            "avatarUrl": "/avatars/431bc46350cbbe996bb058e03ccc2fd0.svg",
            "fullname": "Tim Davidson",
            "name": "trdavidson",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "68151d0f51add3813f3f7d1b",
            "name": "MicrosoftResearch",
            "fullname": "Microsoft Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.25976",
            "authors": [
                {
                    "_id": "6905a52db26ae8b17699fd22",
                    "user": {
                        "_id": "6698688dd723c35c0ea9f16d",
                        "avatarUrl": "/avatars/0748274382d0814bb52b9491bb0a329c.svg",
                        "isPro": false,
                        "fullname": "Roman Beliy",
                        "user": "RomanBeliy",
                        "type": "user"
                    },
                    "name": "Roman Beliy",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-05T16:25:16.961Z",
                    "hidden": false
                },
                {
                    "_id": "6905a52db26ae8b17699fd23",
                    "name": "Amit Zalcher",
                    "hidden": false
                },
                {
                    "_id": "6905a52db26ae8b17699fd24",
                    "name": "Jonathan Kogman",
                    "hidden": false
                },
                {
                    "_id": "6905a52db26ae8b17699fd25",
                    "name": "Navve Wasserman",
                    "hidden": false
                },
                {
                    "_id": "6905a52db26ae8b17699fd26",
                    "name": "Michal Irani",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-29T21:21:54.000Z",
            "submittedOnDailyAt": "2025-11-05T03:33:58.960Z",
            "title": "Brain-IT: Image Reconstruction from fMRI via Brain-Interaction\n  Transformer",
            "submittedOnDailyBy": {
                "_id": "67d844b5e31360c75fa7410d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QgryioCNwCGDx_tdlHeym.png",
                "isPro": false,
                "fullname": "Zalcher",
                "user": "Amitz244",
                "type": "user"
            },
            "summary": "Reconstructing images seen by people from their fMRI brain recordings\nprovides a non-invasive window into the human brain. Despite recent progress\nenabled by diffusion models, current methods often lack faithfulness to the\nactual seen images. We present \"Brain-IT\", a brain-inspired approach that\naddresses this challenge through a Brain Interaction Transformer (BIT),\nallowing effective interactions between clusters of functionally-similar\nbrain-voxels. These functional-clusters are shared by all subjects, serving as\nbuilding blocks for integrating information both within and across brains. All\nmodel components are shared by all clusters & subjects, allowing efficient\ntraining with a limited amount of data. To guide the image reconstruction, BIT\npredicts two complementary localized patch-level image features: (i)high-level\nsemantic features which steer the diffusion model toward the correct semantic\ncontent of the image; and (ii)low-level structural features which help to\ninitialize the diffusion process with the correct coarse layout of the image.\nBIT's design enables direct flow of information from brain-voxel clusters to\nlocalized image features. Through these principles, our method achieves image\nreconstructions from fMRI that faithfully reconstruct the seen images, and\nsurpass current SotA approaches both visually and by standard objective\nmetrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve\nresults comparable to current methods trained on full 40-hour recordings.",
            "upvotes": 8,
            "discussionId": "6905a52db26ae8b17699fd27",
            "projectPage": "https://amitzalcher.github.io/Brain-IT/",
            "ai_summary": "Brain-IT uses a Brain Interaction Transformer to reconstruct images from fMRI data with high fidelity, surpassing current methods and requiring less training data.",
            "ai_keywords": [
                "diffusion models",
                "Brain Interaction Transformer",
                "BIT",
                "brain-voxels",
                "functional-clusters",
                "high-level semantic features",
                "low-level structural features",
                "image reconstruction",
                "fMRI",
                "standard objective metrics"
            ],
            "organization": {
                "_id": "62e28e39555a866437a78225",
                "name": "weizmannscience",
                "fullname": "Weizmann Institute of Science",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1659014711791-624bebf604abc7ebb01789af.jpeg"
            }
        },
        "publishedAt": "2025-10-29T17:21:54.000Z",
        "title": "Brain-IT: Image Reconstruction from fMRI via Brain-Interaction\n  Transformer",
        "summary": "Reconstructing images seen by people from their fMRI brain recordings\nprovides a non-invasive window into the human brain. Despite recent progress\nenabled by diffusion models, current methods often lack faithfulness to the\nactual seen images. We present \"Brain-IT\", a brain-inspired approach that\naddresses this challenge through a Brain Interaction Transformer (BIT),\nallowing effective interactions between clusters of functionally-similar\nbrain-voxels. These functional-clusters are shared by all subjects, serving as\nbuilding blocks for integrating information both within and across brains. All\nmodel components are shared by all clusters & subjects, allowing efficient\ntraining with a limited amount of data. To guide the image reconstruction, BIT\npredicts two complementary localized patch-level image features: (i)high-level\nsemantic features which steer the diffusion model toward the correct semantic\ncontent of the image; and (ii)low-level structural features which help to\ninitialize the diffusion process with the correct coarse layout of the image.\nBIT's design enables direct flow of information from brain-voxel clusters to\nlocalized image features. Through these principles, our method achieves image\nreconstructions from fMRI that faithfully reconstruct the seen images, and\nsurpass current SotA approaches both visually and by standard objective\nmetrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve\nresults comparable to current methods trained on full 40-hour recordings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25976.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "67d844b5e31360c75fa7410d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QgryioCNwCGDx_tdlHeym.png",
            "fullname": "Zalcher",
            "name": "Amitz244",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "62e28e39555a866437a78225",
            "name": "weizmannscience",
            "fullname": "Weizmann Institute of Science",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1659014711791-624bebf604abc7ebb01789af.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.02650",
            "authors": [
                {
                    "_id": "690aeddcd70e173c84529080",
                    "name": "Tianfan Peng",
                    "hidden": false
                },
                {
                    "_id": "690aeddcd70e173c84529081",
                    "name": "Yuntao Du",
                    "hidden": false
                },
                {
                    "_id": "690aeddcd70e173c84529082",
                    "name": "Pengzhou Ji",
                    "hidden": false
                },
                {
                    "_id": "690aeddcd70e173c84529083",
                    "name": "Shijie Dong",
                    "hidden": false
                },
                {
                    "_id": "690aeddcd70e173c84529084",
                    "user": {
                        "_id": "65745569839aa08899ea5d27",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
                        "isPro": false,
                        "fullname": "Kailin Jiang, ",
                        "user": "kailinjiang",
                        "type": "user"
                    },
                    "name": "Kailin Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-05T16:24:24.713Z",
                    "hidden": false
                },
                {
                    "_id": "690aeddcd70e173c84529085",
                    "name": "Mingchuan Ma",
                    "hidden": false
                },
                {
                    "_id": "690aeddcd70e173c84529086",
                    "name": "Yijun Tian",
                    "hidden": false
                },
                {
                    "_id": "690aeddcd70e173c84529087",
                    "name": "Jinhe Bi",
                    "hidden": false
                },
                {
                    "_id": "690aeddcd70e173c84529088",
                    "name": "Qian Li",
                    "hidden": false
                },
                {
                    "_id": "690aeddcd70e173c84529089",
                    "name": "Wei Du",
                    "hidden": false
                },
                {
                    "_id": "690aeddcd70e173c8452908a",
                    "name": "Feng Xiao",
                    "hidden": false
                },
                {
                    "_id": "690aeddcd70e173c8452908b",
                    "name": "Lizhen Cui",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-04T15:17:06.000Z",
            "submittedOnDailyAt": "2025-11-05T04:06:26.868Z",
            "title": "Can Visual Input Be Compressed? A Visual Token Compression Benchmark for\n  Large Multimodal Models",
            "submittedOnDailyBy": {
                "_id": "65745569839aa08899ea5d27",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
                "isPro": false,
                "fullname": "Kailin Jiang, ",
                "user": "kailinjiang",
                "type": "user"
            },
            "summary": "Large multimodal models (LMMs) often suffer from severe inference\ninefficiency due to the large number of visual tokens introduced by image\nencoders. While recent token compression methods, such as pruning and merging,\nhave shown promise in reducing redundancy, their evaluation remains fragmented\nand inconsistent. In this work, we present UniPruneBench, a unified and\nextensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench\nprovides standardized protocols across six ability dimensions and ten datasets,\ncovering ten representative compression algorithms and three families of LMMs\n(LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates\nsystem-level metrics such as runtime and prefilling latency to provide a\nholistic view. Our experiments uncover several key findings: (1) random pruning\nis a surprisingly strong baseline, (2) no single method consistently\noutperforms others across scenarios, (3) pruning sensitivity varies\nsignificantly across tasks, with OCR being most vulnerable, and (4) pruning\nratio is the dominant factor governing performance degradation. We believe\nUniPruneBench will serve as a reliable foundation for future research on\nefficient multimodal modeling.",
            "upvotes": 7,
            "discussionId": "690aeddcd70e173c8452908c",
            "projectPage": "https://uniprunebench-lmm.github.io/",
            "githubRepo": "https://github.com/TianfanPeng/VLMUniPruneBench",
            "ai_summary": "UniPruneBench is a unified benchmark for evaluating visual token pruning in multimodal LLMs, providing standardized protocols and system-level metrics to assess performance across various tasks and models.",
            "ai_keywords": [
                "multimodal models",
                "visual tokens",
                "image encoders",
                "token compression",
                "pruning",
                "merging",
                "UniPruneBench",
                "ability dimensions",
                "datasets",
                "compression algorithms",
                "LLaVA-v1.5",
                "Intern-VL3",
                "Qwen2.5-VL",
                "runtime",
                "prefilling latency",
                "OCR",
                "pruning sensitivity",
                "pruning ratio"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-11-04T10:17:06.000Z",
        "title": "Can Visual Input Be Compressed? A Visual Token Compression Benchmark for\n  Large Multimodal Models",
        "summary": "Large multimodal models (LMMs) often suffer from severe inference\ninefficiency due to the large number of visual tokens introduced by image\nencoders. While recent token compression methods, such as pruning and merging,\nhave shown promise in reducing redundancy, their evaluation remains fragmented\nand inconsistent. In this work, we present UniPruneBench, a unified and\nextensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench\nprovides standardized protocols across six ability dimensions and ten datasets,\ncovering ten representative compression algorithms and three families of LMMs\n(LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates\nsystem-level metrics such as runtime and prefilling latency to provide a\nholistic view. Our experiments uncover several key findings: (1) random pruning\nis a surprisingly strong baseline, (2) no single method consistently\noutperforms others across scenarios, (3) pruning sensitivity varies\nsignificantly across tasks, with OCR being most vulnerable, and (4) pruning\nratio is the dominant factor governing performance degradation. We believe\nUniPruneBench will serve as a reliable foundation for future research on\nefficient multimodal modeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02650.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65745569839aa08899ea5d27",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
            "fullname": "Kailin Jiang, ",
            "name": "kailinjiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.02347",
            "authors": [
                {
                    "_id": "690ad8c4d70e173c8452902c",
                    "name": "Liuhao Lin",
                    "hidden": false
                },
                {
                    "_id": "690ad8c4d70e173c8452902d",
                    "name": "Ke Li",
                    "hidden": false
                },
                {
                    "_id": "690ad8c4d70e173c8452902e",
                    "name": "Zihan Xu",
                    "hidden": false
                },
                {
                    "_id": "690ad8c4d70e173c8452902f",
                    "name": "Yuchen Shi",
                    "hidden": false
                },
                {
                    "_id": "690ad8c4d70e173c84529030",
                    "user": {
                        "_id": "6390525c00fb8ec4a424e0ff",
                        "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
                        "isPro": false,
                        "fullname": "Yulei Qin",
                        "user": "yolay",
                        "type": "user"
                    },
                    "name": "Yulei Qin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-05T16:24:35.496Z",
                    "hidden": false
                },
                {
                    "_id": "690ad8c4d70e173c84529031",
                    "name": "Yan Zhang",
                    "hidden": false
                },
                {
                    "_id": "690ad8c4d70e173c84529032",
                    "name": "Xing Sun",
                    "hidden": false
                },
                {
                    "_id": "690ad8c4d70e173c84529033",
                    "name": "Rongrong Ji",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-04T08:11:23.000Z",
            "submittedOnDailyAt": "2025-11-05T02:35:20.480Z",
            "title": "LTD-Bench: Evaluating Large Language Models by Letting Them Draw",
            "submittedOnDailyBy": {
                "_id": "63280915eeee4dd858083092",
                "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg",
                "isPro": false,
                "fullname": "Ke Li",
                "user": "tristanli",
                "type": "user"
            },
            "summary": "Current evaluation paradigms for large language models (LLMs) represent a\ncritical blind spot in AI research--relying on opaque numerical metrics that\nconceal fundamental limitations in spatial reasoning while providing no\nintuitive understanding of model capabilities. This deficiency creates a\ndangerous disconnect between reported performance and practical abilities,\nparticularly for applications requiring physical world understanding. We\nintroduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation\nfrom abstract scores to directly observable visual outputs by requiring models\nto generate drawings through dot matrices or executable code. This approach\nmakes spatial reasoning limitations immediately apparent even to non-experts,\nbridging the fundamental gap between statistical performance and intuitive\nassessment. LTD-Bench implements a comprehensive methodology with complementary\ngeneration tasks (testing spatial imagination) and recognition tasks (assessing\nspatial perception) across three progressively challenging difficulty levels,\nmethodically evaluating both directions of the critical language-spatial\nmapping. Our extensive experiments with state-of-the-art models expose an\nalarming capability gap: even LLMs achieving impressive results on traditional\nbenchmarks demonstrate profound deficiencies in establishing bidirectional\nmappings between language and spatial concept--a fundamental limitation that\nundermines their potential as genuine world models. Furthermore, LTD-Bench's\nvisual outputs enable powerful diagnostic analysis, offering a potential\napproach to investigate model similarity.",
            "upvotes": 6,
            "discussionId": "690ad8c4d70e173c84529034",
            "ai_summary": "LTD-Bench evaluates large language models' spatial reasoning by requiring them to generate visual outputs, revealing significant limitations in their ability to map language to spatial concepts.",
            "ai_keywords": [
                "LTD-Bench",
                "large language models",
                "spatial reasoning",
                "dot matrices",
                "executable code",
                "spatial imagination",
                "spatial perception",
                "bidirectional mappings",
                "language-spatial mapping",
                "world models",
                "diagnostic analysis"
            ],
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-11-04T03:11:23.000Z",
        "title": "LTD-Bench: Evaluating Large Language Models by Letting Them Draw",
        "summary": "Current evaluation paradigms for large language models (LLMs) represent a\ncritical blind spot in AI research--relying on opaque numerical metrics that\nconceal fundamental limitations in spatial reasoning while providing no\nintuitive understanding of model capabilities. This deficiency creates a\ndangerous disconnect between reported performance and practical abilities,\nparticularly for applications requiring physical world understanding. We\nintroduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation\nfrom abstract scores to directly observable visual outputs by requiring models\nto generate drawings through dot matrices or executable code. This approach\nmakes spatial reasoning limitations immediately apparent even to non-experts,\nbridging the fundamental gap between statistical performance and intuitive\nassessment. LTD-Bench implements a comprehensive methodology with complementary\ngeneration tasks (testing spatial imagination) and recognition tasks (assessing\nspatial perception) across three progressively challenging difficulty levels,\nmethodically evaluating both directions of the critical language-spatial\nmapping. Our extensive experiments with state-of-the-art models expose an\nalarming capability gap: even LLMs achieving impressive results on traditional\nbenchmarks demonstrate profound deficiencies in establishing bidirectional\nmappings between language and spatial concept--a fundamental limitation that\nundermines their potential as genuine world models. Furthermore, LTD-Bench's\nvisual outputs enable powerful diagnostic analysis, offering a potential\napproach to investigate model similarity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02347.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63280915eeee4dd858083092",
            "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg",
            "fullname": "Ke Li",
            "name": "tristanli",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.01937",
            "authors": [
                {
                    "_id": "690ae1c8d70e173c84529059",
                    "user": {
                        "_id": "6380e53efb49cd1c12052c17",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6380e53efb49cd1c12052c17/b5CweexfrVn-W_xto2agR.jpeg",
                        "isPro": false,
                        "fullname": "Abdelaziz Bounhar",
                        "user": "BounharAbdelaziz",
                        "type": "user"
                    },
                    "name": "Abdelaziz Bounhar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-05T16:24:33.298Z",
                    "hidden": false
                },
                {
                    "_id": "690ae1c8d70e173c8452905a",
                    "name": "Hadi Abdine",
                    "hidden": false
                },
                {
                    "_id": "690ae1c8d70e173c8452905b",
                    "name": "Evan Dufraisse",
                    "hidden": false
                },
                {
                    "_id": "690ae1c8d70e173c8452905c",
                    "name": "Ahmad Chamma",
                    "hidden": false
                },
                {
                    "_id": "690ae1c8d70e173c8452905d",
                    "name": "Amr Mohamed",
                    "hidden": false
                },
                {
                    "_id": "690ae1c8d70e173c8452905e",
                    "name": "Dani Bouch",
                    "hidden": false
                },
                {
                    "_id": "690ae1c8d70e173c8452905f",
                    "name": "Michalis Vazirgiannis",
                    "hidden": false
                },
                {
                    "_id": "690ae1c8d70e173c84529060",
                    "user": {
                        "_id": "6087e598e2b7cc3a117b0dc5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6087e598e2b7cc3a117b0dc5/Ctz_W-uo1gOQRBHXalD1P.png",
                        "isPro": false,
                        "fullname": "Guokan Shang",
                        "user": "guokan-shang",
                        "type": "user"
                    },
                    "name": "Guokan Shang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-05T16:24:31.211Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-02T17:29:16.000Z",
            "submittedOnDailyAt": "2025-11-05T03:15:19.121Z",
            "title": "Shorter but not Worse: Frugal Reasoning via Easy Samples as Length\n  Regularizers in Math RLVR",
            "submittedOnDailyBy": {
                "_id": "6380e53efb49cd1c12052c17",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6380e53efb49cd1c12052c17/b5CweexfrVn-W_xto2agR.jpeg",
                "isPro": false,
                "fullname": "Abdelaziz Bounhar",
                "user": "BounharAbdelaziz",
                "type": "user"
            },
            "summary": "Large language models (LLMs) trained for step-by-step reasoning often become\nexcessively verbose, raising inference cost. Standard Reinforcement Learning\nwith Verifiable Rewards (RLVR) pipelines filter out ``easy'' problems for\ntraining efficiency, leaving the model to train primarily on harder problems\nthat require longer reasoning chains. This skews the output length distribution\nupward, resulting in a model that conflates ``thinking longer'' with\n``thinking better''. In this work, we show that retaining and modestly\nup-weighting moderately easy problems acts as an implicit length regularizer.\nExposing the model to solvable short-chain tasks constrains its output\ndistribution and prevents runaway verbosity. The result is\n\\emph{emergent brevity for free}: the model learns to solve harder\nproblems without inflating the output length,  despite the absence of\nany explicit length penalization. RLVR experiments using this approach on\nQwen3-4B-Thinking-2507 (with a 16k token limit) achieve baseline\npass@1 AIME25 accuracy while generating solutions that are, on average, nearly\ntwice as short. The code is available at\nhttps://github.com/MBZUAI-Paris/Frugal-AI{GitHub}, with datasets and\nmodels on\nhttps://huggingface.co/collections/MBZUAI-Paris/k2-think-mini-68dcfa8b114686a4bd3dc2bc{Hugging\nFace}.",
            "upvotes": 6,
            "discussionId": "690ae1c8d70e173c84529061",
            "githubRepo": "https://github.com/MBZUAI-Paris/Frugal-AI-Math",
            "ai_summary": "Retaining and up-weighting moderately easy problems in RLVR pipelines for LLMs reduces output verbosity without explicit length penalization.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards (RLVR)",
                "Large language models (LLMs)",
                "step-by-step reasoning",
                "inference cost",
                "output length distribution",
                "thinking longer",
                "thinking better",
                "emergent brevity",
                "Qwen3-4B-Thinking-2507",
                "pass@1 AIME25 accuracy"
            ],
            "githubStars": 7,
            "organization": {
                "_id": "6656df18bfefce0a724e65d6",
                "name": "MBZUAI-Paris",
                "fullname": "MBZUAI-IFM Paris Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6087e598e2b7cc3a117b0dc5/JUiDkClou70ZdFQaiZ3hK.png"
            }
        },
        "publishedAt": "2025-11-02T12:29:16.000Z",
        "title": "Shorter but not Worse: Frugal Reasoning via Easy Samples as Length\n  Regularizers in Math RLVR",
        "summary": "Large language models (LLMs) trained for step-by-step reasoning often become\nexcessively verbose, raising inference cost. Standard Reinforcement Learning\nwith Verifiable Rewards (RLVR) pipelines filter out ``easy'' problems for\ntraining efficiency, leaving the model to train primarily on harder problems\nthat require longer reasoning chains. This skews the output length distribution\nupward, resulting in a model that conflates ``thinking longer'' with\n``thinking better''. In this work, we show that retaining and modestly\nup-weighting moderately easy problems acts as an implicit length regularizer.\nExposing the model to solvable short-chain tasks constrains its output\ndistribution and prevents runaway verbosity. The result is\n\\emph{emergent brevity for free}: the model learns to solve harder\nproblems without inflating the output length,  despite the absence of\nany explicit length penalization. RLVR experiments using this approach on\nQwen3-4B-Thinking-2507 (with a 16k token limit) achieve baseline\npass@1 AIME25 accuracy while generating solutions that are, on average, nearly\ntwice as short. The code is available at\nhttps://github.com/MBZUAI-Paris/Frugal-AI{GitHub}, with datasets and\nmodels on\nhttps://huggingface.co/collections/MBZUAI-Paris/k2-think-mini-68dcfa8b114686a4bd3dc2bc{Hugging\nFace}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01937.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6380e53efb49cd1c12052c17",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6380e53efb49cd1c12052c17/b5CweexfrVn-W_xto2agR.jpeg",
            "fullname": "Abdelaziz Bounhar",
            "name": "BounharAbdelaziz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 59
        },
        "organization": {
            "_id": "6656df18bfefce0a724e65d6",
            "name": "MBZUAI-Paris",
            "fullname": "MBZUAI-IFM Paris Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6087e598e2b7cc3a117b0dc5/JUiDkClou70ZdFQaiZ3hK.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.00839",
            "authors": [
                {
                    "_id": "690b07cdd70e173c845290c4",
                    "user": {
                        "_id": "677c2e5032582d07139f8af1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ZxdbXnPAFq_O20FAZuD4v.png",
                        "isPro": false,
                        "fullname": "John",
                        "user": "john-b-yang",
                        "type": "user"
                    },
                    "name": "John Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-05T09:30:15.256Z",
                    "hidden": false
                },
                {
                    "_id": "690b07cdd70e173c845290c5",
                    "name": "Kilian Lieret",
                    "hidden": false
                },
                {
                    "_id": "690b07cdd70e173c845290c6",
                    "name": "Joyce Yang",
                    "hidden": false
                },
                {
                    "_id": "690b07cdd70e173c845290c7",
                    "name": "Carlos E. Jimenez",
                    "hidden": false
                },
                {
                    "_id": "690b07cdd70e173c845290c8",
                    "name": "Ofir Press",
                    "hidden": false
                },
                {
                    "_id": "690b07cdd70e173c845290c9",
                    "name": "Ludwig Schmidt",
                    "hidden": false
                },
                {
                    "_id": "690b07cdd70e173c845290ca",
                    "name": "Diyi Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-02T07:42:51.000Z",
            "submittedOnDailyAt": "2025-11-05T05:57:29.478Z",
            "title": "CodeClash: Benchmarking Goal-Oriented Software Engineering",
            "submittedOnDailyBy": {
                "_id": "677c2e5032582d07139f8af1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ZxdbXnPAFq_O20FAZuD4v.png",
                "isPro": false,
                "fullname": "John",
                "user": "john-b-yang",
                "type": "user"
            },
            "summary": "Current benchmarks for coding evaluate language models (LMs) on concrete,\nwell-specified tasks such as fixing specific bugs or writing targeted tests.\nHowever, human programmers do not spend all day incessantly addressing isolated\ntasks. Instead, real-world software development is grounded in the pursuit of\nhigh-level goals, like improving user retention or reducing costs. Evaluating\nwhether LMs can also iteratively develop code to better accomplish open-ended\nobjectives without any explicit guidance remains an open challenge. To address\nthis, we introduce CodeClash, a benchmark where LMs compete in multi-round\ntournaments to build the best codebase for achieving a competitive objective.\nEach round proceeds in two phases: agents edit their code, then their codebases\ncompete head-to-head in a code arena that determines winners based on\nobjectives like score maximization, resource acquisition, or survival. Whether\nit's writing notes, scrutinizing documentation, analyzing competition logs, or\ncreating test suites, models must decide for themselves how to improve their\ncodebases both absolutely and against their opponents. We run 1680 tournaments\n(25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal\nthat while models exhibit diverse development styles, they share fundamental\nlimitations in strategic reasoning. Models also struggle with long-term\ncodebase maintenance, as repositories become progressively messy and redundant.\nThese limitations are stark: top models lose every round against expert human\nprogrammers. We open-source CodeClash to advance the study of autonomous,\ngoal-oriented code development.",
            "upvotes": 4,
            "discussionId": "690b07cdd70e173c845290cb",
            "ai_summary": "CodeClash evaluates language models' ability to iteratively develop code for open-ended objectives through competitive multi-round tournaments.",
            "ai_keywords": [
                "language models",
                "CodeClash",
                "multi-round tournaments",
                "codebase",
                "strategic reasoning",
                "long-term codebase maintenance"
            ],
            "organization": {
                "_id": "6112d84f8c2e1f4060908c9e",
                "name": "stanfordnlp",
                "fullname": "Stanford NLP",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1628624969199-6032802e1f993496bc14d9e3.png"
            }
        },
        "publishedAt": "2025-11-02T02:42:51.000Z",
        "title": "CodeClash: Benchmarking Goal-Oriented Software Engineering",
        "summary": "Current benchmarks for coding evaluate language models (LMs) on concrete,\nwell-specified tasks such as fixing specific bugs or writing targeted tests.\nHowever, human programmers do not spend all day incessantly addressing isolated\ntasks. Instead, real-world software development is grounded in the pursuit of\nhigh-level goals, like improving user retention or reducing costs. Evaluating\nwhether LMs can also iteratively develop code to better accomplish open-ended\nobjectives without any explicit guidance remains an open challenge. To address\nthis, we introduce CodeClash, a benchmark where LMs compete in multi-round\ntournaments to build the best codebase for achieving a competitive objective.\nEach round proceeds in two phases: agents edit their code, then their codebases\ncompete head-to-head in a code arena that determines winners based on\nobjectives like score maximization, resource acquisition, or survival. Whether\nit's writing notes, scrutinizing documentation, analyzing competition logs, or\ncreating test suites, models must decide for themselves how to improve their\ncodebases both absolutely and against their opponents. We run 1680 tournaments\n(25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal\nthat while models exhibit diverse development styles, they share fundamental\nlimitations in strategic reasoning. Models also struggle with long-term\ncodebase maintenance, as repositories become progressively messy and redundant.\nThese limitations are stark: top models lose every round against expert human\nprogrammers. We open-source CodeClash to advance the study of autonomous,\ngoal-oriented code development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.00839.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "677c2e5032582d07139f8af1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ZxdbXnPAFq_O20FAZuD4v.png",
            "fullname": "John",
            "name": "john-b-yang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "6112d84f8c2e1f4060908c9e",
            "name": "stanfordnlp",
            "fullname": "Stanford NLP",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1628624969199-6032802e1f993496bc14d9e3.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.02832",
            "authors": [
                {
                    "_id": "690abfa3d70e173c84528fe6",
                    "name": "Yanjie Ze",
                    "hidden": false
                },
                {
                    "_id": "690abfa3d70e173c84528fe7",
                    "name": "Siheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "690abfa3d70e173c84528fe8",
                    "name": "Weizhuo Wang",
                    "hidden": false
                },
                {
                    "_id": "690abfa3d70e173c84528fe9",
                    "name": "Angjoo Kanazawa",
                    "hidden": false
                },
                {
                    "_id": "690abfa3d70e173c84528fea",
                    "name": "Rocky Duan",
                    "hidden": false
                },
                {
                    "_id": "690abfa3d70e173c84528feb",
                    "name": "Pieter Abbeel",
                    "hidden": false
                },
                {
                    "_id": "690abfa3d70e173c84528fec",
                    "name": "Guanya Shi",
                    "hidden": false
                },
                {
                    "_id": "690abfa3d70e173c84528fed",
                    "name": "Jiajun Wu",
                    "hidden": false
                },
                {
                    "_id": "690abfa3d70e173c84528fee",
                    "name": "C. Karen Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-04T18:58:35.000Z",
            "submittedOnDailyAt": "2025-11-05T00:38:36.561Z",
            "title": "TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Large-scale data has driven breakthroughs in robotics, from language models\nto vision-language-action models in bimanual manipulation. However, humanoid\nrobotics lacks equally effective data collection frameworks. Existing humanoid\nteleoperation systems either use decoupled control or depend on expensive\nmotion capture setups. We introduce TWIST2, a portable, mocap-free humanoid\nteleoperation and data collection system that preserves full whole-body control\nwhile advancing scalability. Our system leverages PICO4U VR for obtaining\nreal-time whole-body human motions, with a custom 2-DoF robot neck (cost around\n$250) for egocentric vision, enabling holistic human-to-humanoid control. We\ndemonstrate long-horizon dexterous and mobile humanoid skills and we can\ncollect 100 demonstrations in 15 minutes with an almost 100% success rate.\nBuilding on this pipeline, we propose a hierarchical visuomotor policy\nframework that autonomously controls the full humanoid body based on egocentric\nvision. Our visuomotor policy successfully demonstrates whole-body dexterous\nmanipulation and dynamic kicking tasks. The entire system is fully reproducible\nand open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also\nopen-sourced at https://twist-data.github.io .",
            "upvotes": 3,
            "discussionId": "690abfa3d70e173c84528fef",
            "projectPage": "https://yanjieze.com/TWIST2/",
            "ai_summary": "TWIST2, a portable mocap-free system, enables efficient data collection and hierarchical visuomotor policy control for humanoid robots.",
            "ai_keywords": [
                "humanoid teleoperation",
                "PICO4U VR",
                "egocentric vision",
                "hierarchical visuomotor policy",
                "whole-body control",
                "dexterous manipulation",
                "dynamic kicking tasks"
            ]
        },
        "publishedAt": "2025-11-04T13:58:35.000Z",
        "title": "TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System",
        "summary": "Large-scale data has driven breakthroughs in robotics, from language models\nto vision-language-action models in bimanual manipulation. However, humanoid\nrobotics lacks equally effective data collection frameworks. Existing humanoid\nteleoperation systems either use decoupled control or depend on expensive\nmotion capture setups. We introduce TWIST2, a portable, mocap-free humanoid\nteleoperation and data collection system that preserves full whole-body control\nwhile advancing scalability. Our system leverages PICO4U VR for obtaining\nreal-time whole-body human motions, with a custom 2-DoF robot neck (cost around\n$250) for egocentric vision, enabling holistic human-to-humanoid control. We\ndemonstrate long-horizon dexterous and mobile humanoid skills and we can\ncollect 100 demonstrations in 15 minutes with an almost 100% success rate.\nBuilding on this pipeline, we propose a hierarchical visuomotor policy\nframework that autonomously controls the full humanoid body based on egocentric\nvision. Our visuomotor policy successfully demonstrates whole-body dexterous\nmanipulation and dynamic kicking tasks. The entire system is fully reproducible\nand open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also\nopen-sourced at https://twist-data.github.io .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02832.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 154
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.01914",
            "authors": [
                {
                    "_id": "690abf69d70e173c84528fde",
                    "name": "Yuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "690abf69d70e173c84528fdf",
                    "name": "Chenyu Xue",
                    "hidden": false
                },
                {
                    "_id": "690abf69d70e173c84528fe0",
                    "name": "Wenjie Xu",
                    "hidden": false
                },
                {
                    "_id": "690abf69d70e173c84528fe1",
                    "name": "Chao Ji",
                    "hidden": false
                },
                {
                    "_id": "690abf69d70e173c84528fe2",
                    "name": "Jiajia wu",
                    "hidden": false
                },
                {
                    "_id": "690abf69d70e173c84528fe3",
                    "name": "Jia Pan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-01T06:24:56.000Z",
            "submittedOnDailyAt": "2025-11-05T00:37:39.223Z",
            "title": "iFlyBot-VLA Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model\ntrained under a novel framework. The main contributions are listed as follows:\n(1) a latent action model thoroughly trained on large-scale human and robotic\nmanipulation videos; (2) a dual-level action representation framework that\njointly supervises both the Vision-Language Model (VLM) and the action expert\nduring training; (3) a mixed training strategy that combines robot trajectory\ndata with general QA and spatial QA datasets, effectively enhancing the 3D\nperceptual and reasoning capabilities of the VLM backbone. Specifically, the\nVLM is trained to predict two complementary forms of actions: latent actions,\nderived from our latent action model pretrained on cross-embodiment\nmanipulation data, which capture implicit high-level intentions; and structured\ndiscrete action tokens, obtained through frequency-domain transformations of\ncontinuous control signals, which encode explicit low-level dynamics. This dual\nsupervision aligns the representation spaces of language, vision, and action,\nenabling the VLM to directly contribute to action generation. Experimental\nresults on the LIBERO Franka benchmark demonstrate the superiority of our\nframe-work, while real-world evaluations further show that iFlyBot-VLA achieves\ncompetitive success rates across diverse and challenging manipulation tasks.\nFurthermore, we plan to open-source a portion of our self-constructed dataset\nto support future research in the community",
            "upvotes": 3,
            "discussionId": "690abf69d70e173c84528fe4",
            "projectPage": "https://xuwenjie401.github.io/iFlyBot-VLA.github.io/",
            "ai_summary": "iFlyBot-VLA, a large-scale VLA model, uses a latent action model and dual-level action representation to enhance 3D perceptual and reasoning capabilities, achieving superior performance in manipulation tasks.",
            "ai_keywords": [
                "latent action model",
                "dual-level action representation",
                "Vision-Language Model (VLM)",
                "action expert",
                "mixed training strategy",
                "robot trajectory data",
                "general QA",
                "spatial QA",
                "latent actions",
                "structured discrete action tokens",
                "frequency-domain transformations",
                "continuous control signals",
                "LIBERO Franka benchmark"
            ]
        },
        "publishedAt": "2025-11-01T02:24:56.000Z",
        "title": "iFlyBot-VLA Technical Report",
        "summary": "We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model\ntrained under a novel framework. The main contributions are listed as follows:\n(1) a latent action model thoroughly trained on large-scale human and robotic\nmanipulation videos; (2) a dual-level action representation framework that\njointly supervises both the Vision-Language Model (VLM) and the action expert\nduring training; (3) a mixed training strategy that combines robot trajectory\ndata with general QA and spatial QA datasets, effectively enhancing the 3D\nperceptual and reasoning capabilities of the VLM backbone. Specifically, the\nVLM is trained to predict two complementary forms of actions: latent actions,\nderived from our latent action model pretrained on cross-embodiment\nmanipulation data, which capture implicit high-level intentions; and structured\ndiscrete action tokens, obtained through frequency-domain transformations of\ncontinuous control signals, which encode explicit low-level dynamics. This dual\nsupervision aligns the representation spaces of language, vision, and action,\nenabling the VLM to directly contribute to action generation. Experimental\nresults on the LIBERO Franka benchmark demonstrate the superiority of our\nframe-work, while real-world evaluations further show that iFlyBot-VLA achieves\ncompetitive success rates across diverse and challenging manipulation tasks.\nFurthermore, we plan to open-source a portion of our self-constructed dataset\nto support future research in the community",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01914.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 154
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.02490",
            "authors": [
                {
                    "_id": "690ae401d70e173c84529063",
                    "name": "Rajan Das Gupta",
                    "hidden": false
                },
                {
                    "_id": "690ae401d70e173c84529064",
                    "name": "Md Kishor Morol",
                    "hidden": false
                },
                {
                    "_id": "690ae401d70e173c84529065",
                    "name": "Nafiz Fahad",
                    "hidden": false
                },
                {
                    "_id": "690ae401d70e173c84529066",
                    "name": "Md Tanzib Hosain",
                    "hidden": false
                },
                {
                    "_id": "690ae401d70e173c84529067",
                    "name": "Sumaya Binte Zilani Choya",
                    "hidden": false
                },
                {
                    "_id": "690ae401d70e173c84529068",
                    "name": "Md Jakir Hossen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/GOjumJN8KFk2GcHqyer5G.jpeg"
            ],
            "publishedAt": "2025-11-04T11:27:03.000Z",
            "submittedOnDailyAt": "2025-11-05T03:19:16.438Z",
            "title": "BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and\n  Monitoring",
            "submittedOnDailyBy": {
                "_id": "67a3002c637d195f3c4bf371",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a3002c637d195f3c4bf371/-9G1DvjGpODiq5vwjLANF.png",
                "isPro": false,
                "fullname": "Rajan Das Gupta",
                "user": "rajandasgupta",
                "type": "user"
            },
            "summary": "As the global burden of Alzheimer's disease (AD) continues to grow, early and\naccurate detection has become increasingly critical, especially in regions with\nlimited access to advanced diagnostic tools. We propose BRAINS (Biomedical\nRetrieval-Augmented Intelligence for Neurodegeneration Screening) to address\nthis challenge. This novel system harnesses the powerful reasoning capabilities\nof Large Language Models (LLMs) for Alzheimer's detection and monitoring.\nBRAINS features a dual-module architecture: a cognitive diagnostic module and a\ncase-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on\ncognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain\nvolume metrics -- to perform structured assessments of Alzheimer's risk.\nMeanwhile, the Case Retrieval Module encodes patient profiles into latent\nrepresentations and retrieves similar cases from a curated knowledge base.\nThese auxiliary cases are fused with the input profile via a Case Fusion Layer\nto enhance contextual understanding. The combined representation is then\nprocessed with clinical prompts for inference. Evaluations on real-world\ndatasets demonstrate BRAINS effectiveness in classifying disease severity and\nidentifying early signs of cognitive decline. This system not only shows strong\npotential as an assistive tool for scalable, explainable, and early-stage\nAlzheimer's disease detection, but also offers hope for future applications in\nthe field.",
            "upvotes": 2,
            "discussionId": "690ae402d70e173c84529069",
            "ai_summary": "BRAINS, a system using Large Language Models, effectively detects and monitors Alzheimer's disease by integrating cognitive assessments and case retrieval.",
            "ai_keywords": [
                "Large Language Models",
                "cognitive diagnostic module",
                "case-retrieval module",
                "latent representations",
                "Case Fusion Layer",
                "clinical prompts",
                "Alzheimer's risk",
                "disease severity",
                "cognitive decline"
            ],
            "organization": {
                "_id": "68b97f8992f0e22420e79325",
                "name": "eliteresearch",
                "fullname": "ELITE Research Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b97676948b70f005d923fe/2QCfcp6rhnh8GI6AVJ_yQ.png"
            }
        },
        "publishedAt": "2025-11-04T06:27:03.000Z",
        "title": "BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and\n  Monitoring",
        "summary": "As the global burden of Alzheimer's disease (AD) continues to grow, early and\naccurate detection has become increasingly critical, especially in regions with\nlimited access to advanced diagnostic tools. We propose BRAINS (Biomedical\nRetrieval-Augmented Intelligence for Neurodegeneration Screening) to address\nthis challenge. This novel system harnesses the powerful reasoning capabilities\nof Large Language Models (LLMs) for Alzheimer's detection and monitoring.\nBRAINS features a dual-module architecture: a cognitive diagnostic module and a\ncase-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on\ncognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain\nvolume metrics -- to perform structured assessments of Alzheimer's risk.\nMeanwhile, the Case Retrieval Module encodes patient profiles into latent\nrepresentations and retrieves similar cases from a curated knowledge base.\nThese auxiliary cases are fused with the input profile via a Case Fusion Layer\nto enhance contextual understanding. The combined representation is then\nprocessed with clinical prompts for inference. Evaluations on real-world\ndatasets demonstrate BRAINS effectiveness in classifying disease severity and\nidentifying early signs of cognitive decline. This system not only shows strong\npotential as an assistive tool for scalable, explainable, and early-stage\nAlzheimer's disease detection, but also offers hope for future applications in\nthe field.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/GOjumJN8KFk2GcHqyer5G.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02490.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67a3002c637d195f3c4bf371",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a3002c637d195f3c4bf371/-9G1DvjGpODiq5vwjLANF.png",
            "fullname": "Rajan Das Gupta",
            "name": "rajandasgupta",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "68b97f8992f0e22420e79325",
            "name": "eliteresearch",
            "fullname": "ELITE Research Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b97676948b70f005d923fe/2QCfcp6rhnh8GI6AVJ_yQ.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.02415",
            "authors": [
                {
                    "_id": "690ac0bbd70e173c84528ff1",
                    "name": "Duo Xu",
                    "hidden": false
                },
                {
                    "_id": "690ac0bbd70e173c84528ff2",
                    "name": "Hao Cheng",
                    "hidden": false
                },
                {
                    "_id": "690ac0bbd70e173c84528ff3",
                    "name": "Xin Lin",
                    "hidden": false
                },
                {
                    "_id": "690ac0bbd70e173c84528ff4",
                    "name": "Zhen Xie",
                    "hidden": false
                },
                {
                    "_id": "690ac0bbd70e173c84528ff5",
                    "name": "Hao Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-04T09:45:34.000Z",
            "submittedOnDailyAt": "2025-11-05T00:43:20.949Z",
            "title": "ChartM^3: A Multi-Stage Code-Driven Pipeline for Constructing\n  Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Complex chart understanding tasks demand advanced visual recognition and\nreasoning capabilities from multimodal large language models (MLLMs). However,\ncurrent research provides limited coverage of complex chart scenarios and\ncomputation-intensive reasoning tasks prevalent in real-world applications.\nThis study proposes an automated multi-stage code-driven pipeline for\nsystematically generating visual reasoning datasets to address these\nlimitations. The pipeline integrates retrieval-augmented generation (RAG) to\nretrieve professional chart templates and employs chain-of-thought (CoT)\nstrategies to generate reasoning codes that simulate real data distributions,\nthereby driving chart rendering and question-related statistical computations.\nThrough model-based evaluation, the pipeline enhances chart diversity and data\nquality. Using this framework, we construct ChartM^3, a multi-dimensional and\nmulti-step dataset containing 38K charts and 142K Q&A pairs for training, along\nwith 2,871 high-quality evaluation samples for enabling practical performance\nassessment. Supervised fine-tuning (SFT) and reinforcement learning (RL)\nexperiments demonstrate that our dataset significantly improves reasoning\ncapabilities and cross-domain generalization performance, enabling smaller\nmodels to achieve performance comparable to larger-scale models in complex\nchart comprehension.",
            "upvotes": 2,
            "discussionId": "690ac0bbd70e173c84528ff6",
            "ai_summary": "An automated pipeline using retrieval-augmented generation and chain-of-thought strategies creates a diverse dataset to enhance reasoning capabilities in complex chart understanding tasks.",
            "ai_keywords": [
                "retrieval-augmented generation",
                "chain-of-thought",
                "visual reasoning datasets",
                "chart rendering",
                "statistical computations",
                "model-based evaluation",
                "supervised fine-tuning",
                "reinforcement learning",
                "cross-domain generalization"
            ]
        },
        "publishedAt": "2025-11-04T04:45:34.000Z",
        "title": "ChartM^3: A Multi-Stage Code-Driven Pipeline for Constructing\n  Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension",
        "summary": "Complex chart understanding tasks demand advanced visual recognition and\nreasoning capabilities from multimodal large language models (MLLMs). However,\ncurrent research provides limited coverage of complex chart scenarios and\ncomputation-intensive reasoning tasks prevalent in real-world applications.\nThis study proposes an automated multi-stage code-driven pipeline for\nsystematically generating visual reasoning datasets to address these\nlimitations. The pipeline integrates retrieval-augmented generation (RAG) to\nretrieve professional chart templates and employs chain-of-thought (CoT)\nstrategies to generate reasoning codes that simulate real data distributions,\nthereby driving chart rendering and question-related statistical computations.\nThrough model-based evaluation, the pipeline enhances chart diversity and data\nquality. Using this framework, we construct ChartM^3, a multi-dimensional and\nmulti-step dataset containing 38K charts and 142K Q&A pairs for training, along\nwith 2,871 high-quality evaluation samples for enabling practical performance\nassessment. Supervised fine-tuning (SFT) and reinforcement learning (RL)\nexperiments demonstrate that our dataset significantly improves reasoning\ncapabilities and cross-domain generalization performance, enabling smaller\nmodels to achieve performance comparable to larger-scale models in complex\nchart comprehension.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02415.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 154
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.17950",
            "authors": [
                {
                    "_id": "6902d14272739622ee92a825",
                    "user": {
                        "_id": "63a369d98c0c89dcae3b8329",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
                        "isPro": true,
                        "fullname": "Adina Yakefu",
                        "user": "AdinaY",
                        "type": "user"
                    },
                    "name": "Adina Yakefu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-05T15:56:08.834Z",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a826",
                    "name": "Bin Xie",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a827",
                    "name": "Chongyang Xu",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a828",
                    "name": "Enwen Zhang",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a829",
                    "user": {
                        "_id": "63f5bca33aa49d8cb97ff689",
                        "avatarUrl": "/avatars/d6649c9413f32dbf4477126d0610432e.svg",
                        "isPro": false,
                        "fullname": "Erjin Zhou",
                        "user": "zej",
                        "type": "user"
                    },
                    "name": "Erjin Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-05T16:33:52.794Z",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a82a",
                    "name": "Fan Jia",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a82b",
                    "name": "Haitao Yang",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a82c",
                    "name": "Haoqiang Fan",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a82d",
                    "name": "Haowei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a82e",
                    "user": {
                        "_id": "66b5a9bb8d9afb7a9d6dc9d3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XsfkoY58DhcY-YxzbuKKn.png",
                        "isPro": false,
                        "fullname": "Hongyang Peng",
                        "user": "LoveEatCandy",
                        "type": "user"
                    },
                    "name": "Hongyang Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-05T16:34:53.102Z",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a82f",
                    "name": "Jing Tan",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a830",
                    "user": {
                        "_id": "65672681e7be83d499809c98",
                        "avatarUrl": "/avatars/efcde7a86a47a7f7a2b26847f41c214d.svg",
                        "isPro": false,
                        "fullname": "Junwen Huang",
                        "user": "ssdqpbss",
                        "type": "user"
                    },
                    "name": "Junwen Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-05T16:35:03.363Z",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a831",
                    "name": "Kai Liu",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a832",
                    "user": {
                        "_id": "6375e0d73b13bf69cb3b0ec9",
                        "avatarUrl": "/avatars/99993f27d18448d72e4fef5e2ce80529.svg",
                        "isPro": false,
                        "fullname": "Kaixin Liu",
                        "user": "smrset",
                        "type": "user"
                    },
                    "name": "Kaixin Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-05T16:35:10.805Z",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a833",
                    "name": "Kefan Gu",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a834",
                    "name": "Qinglun Zhang",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a835",
                    "user": {
                        "_id": "6548d99db4c5d07feb83cfd8",
                        "avatarUrl": "/avatars/a34a20efda0695bdb830ea4733c2bc73.svg",
                        "isPro": false,
                        "fullname": "Ruitao Zhang",
                        "user": "Ruitao123",
                        "type": "user"
                    },
                    "name": "Ruitao Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-05T16:35:29.397Z",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a836",
                    "name": "Saike Huang",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a837",
                    "name": "Shen Cheng",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a838",
                    "name": "Shuaicheng Liu",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a839",
                    "name": "Tiancai Wang",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a83a",
                    "user": {
                        "_id": "62d22496c58f969c152bcefd",
                        "avatarUrl": "/avatars/76c3b70e312f25e1e610473475553c5c.svg",
                        "isPro": false,
                        "fullname": "Tiezhen WANG",
                        "user": "xianbao",
                        "type": "user"
                    },
                    "name": "Tiezhen Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-05T16:35:51.928Z",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a83b",
                    "name": "Wei Sun",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a83c",
                    "name": "Wenbin Tang",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a83d",
                    "name": "Yajun Wei",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a83e",
                    "name": "Yang Chen",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a83f",
                    "name": "Youqiang Gui",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a840",
                    "name": "Yucheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a841",
                    "name": "Yunchao Ma",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a842",
                    "name": "Yunfei Wei",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a843",
                    "name": "Yunhuan Yang",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a844",
                    "name": "Yutong Guo",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a845",
                    "name": "Ze Chen",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a846",
                    "name": "Zhengyuan Du",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a847",
                    "name": "Ziheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a848",
                    "name": "Ziming Liu",
                    "hidden": false
                },
                {
                    "_id": "6902d14272739622ee92a849",
                    "name": "Ziwei Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-20T17:59:14.000Z",
            "submittedOnDailyAt": "2025-11-05T14:03:32.812Z",
            "title": "RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies",
            "submittedOnDailyBy": {
                "_id": "63a369d98c0c89dcae3b8329",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
                "isPro": true,
                "fullname": "Adina Yakefu",
                "user": "AdinaY",
                "type": "user"
            },
            "summary": "Testing on real machines is indispensable for robotic control algorithms. In\nthe context of learning-based algorithms, especially VLA models, demand for\nlarge-scale evaluation, i.e. testing a large number of models on a large number\nof tasks, is becoming increasingly urgent. However, doing this right is highly\nnon-trivial, especially when scalability and reproducibility is taken into\naccount. In this report, we describe our methodology for constructing\nRoboChallenge, an online evaluation system to test robotic control algorithms,\nand our survey of recent state-of-the-art VLA models using our initial\nbenchmark Table30.",
            "upvotes": 2,
            "discussionId": "6902d14272739622ee92a84a",
            "ai_summary": "RoboChallenge is an online evaluation system for robotic control algorithms, particularly VLA models, that addresses the need for large-scale testing with scalability and reproducibility.",
            "ai_keywords": [
                "VLA models",
                "RoboChallenge",
                "online evaluation system",
                "robotic control algorithms",
                "large-scale testing",
                "scalability",
                "reproducibility",
                "benchmark Table30"
            ],
            "organization": {
                "_id": "68e9f2ce161aa15d7432a5d3",
                "name": "RoboChallenge",
                "fullname": "RoboChallenge.ai",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68d9ff7b646abb43021b3e7d/ueC9pjiYd8lrzptIbUVFs.jpeg"
            }
        },
        "publishedAt": "2025-10-20T13:59:14.000Z",
        "title": "RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies",
        "summary": "Testing on real machines is indispensable for robotic control algorithms. In\nthe context of learning-based algorithms, especially VLA models, demand for\nlarge-scale evaluation, i.e. testing a large number of models on a large number\nof tasks, is becoming increasingly urgent. However, doing this right is highly\nnon-trivial, especially when scalability and reproducibility is taken into\naccount. In this report, we describe our methodology for constructing\nRoboChallenge, an online evaluation system to test robotic control algorithms,\nand our survey of recent state-of-the-art VLA models using our initial\nbenchmark Table30.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17950.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a369d98c0c89dcae3b8329",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
            "fullname": "Adina Yakefu",
            "name": "AdinaY",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1168
        },
        "organization": {
            "_id": "68e9f2ce161aa15d7432a5d3",
            "name": "RoboChallenge",
            "fullname": "RoboChallenge.ai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68d9ff7b646abb43021b3e7d/ueC9pjiYd8lrzptIbUVFs.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.02712",
            "authors": [
                {
                    "_id": "690abebbd70e173c84528fd5",
                    "name": "Zhicheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "690abebbd70e173c84528fd6",
                    "name": "Weicheng Wang",
                    "hidden": false
                },
                {
                    "_id": "690abebbd70e173c84528fd7",
                    "name": "Yongjie Zhu",
                    "hidden": false
                },
                {
                    "_id": "690abebbd70e173c84528fd8",
                    "name": "Wenyu Qin",
                    "hidden": false
                },
                {
                    "_id": "690abebbd70e173c84528fd9",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "690abebbd70e173c84528fda",
                    "name": "Di Zhang",
                    "hidden": false
                },
                {
                    "_id": "690abebbd70e173c84528fdb",
                    "name": "Jufeng Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-04T16:31:09.000Z",
            "submittedOnDailyAt": "2025-11-05T00:34:58.030Z",
            "title": "VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation\n  Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Understanding and predicting emotion from videos has gathered significant\nattention in recent studies, driven by advancements in video large language\nmodels (VideoLLMs). While advanced methods have made progress in video emotion\nanalysis, the intrinsic nature of emotions poses significant challenges.\nEmotions are characterized by dynamic and cues-dependent properties, making it\ndifficult to understand complex and evolving emotional states with reasonable\nrationale. To tackle these challenges, we propose a novel affective cues-guided\nreasoning framework that unifies fundamental attribute perception, expression\nanalysis, and high-level emotional understanding in a stage-wise manner. At the\ncore of our approach is a family of video emotion foundation models (VidEmo),\nspecifically designed for emotion reasoning and instruction-following. These\nmodels undergo a two-stage tuning process: first, curriculum emotion learning\nfor injecting emotion knowledge, followed by affective-tree reinforcement\nlearning for emotion reasoning. Moreover, we establish a foundational data\ninfrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG)\nconsisting of 2.1M diverse instruction-based samples. Emo-CFG includes\nexplainable emotional question-answering, fine-grained captions, and associated\nrationales, providing essential resources for advancing emotion understanding\ntasks. Experimental results demonstrate that our approach achieves competitive\nperformance, setting a new milestone across 15 face perception tasks.",
            "upvotes": 1,
            "discussionId": "690abebbd70e173c84528fdc",
            "projectPage": "https://zzcheng.top/VidEmo",
            "ai_summary": "A novel affective cues-guided reasoning framework using video emotion foundation models and a fine-grained dataset achieves competitive performance in emotion understanding tasks.",
            "ai_keywords": [
                "video large language models",
                "VideoLLMs",
                "video emotion analysis",
                "affective cues-guided reasoning",
                "video emotion foundation models",
                "VidEmo",
                "curriculum emotion learning",
                "affective-tree reinforcement learning",
                "emotion-centric fine-grained dataset",
                "Emo-CFG",
                "explainable emotional question-answering",
                "fine-grained captions",
                "face perception tasks"
            ]
        },
        "publishedAt": "2025-11-04T11:31:09.000Z",
        "title": "VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation\n  Models",
        "summary": "Understanding and predicting emotion from videos has gathered significant\nattention in recent studies, driven by advancements in video large language\nmodels (VideoLLMs). While advanced methods have made progress in video emotion\nanalysis, the intrinsic nature of emotions poses significant challenges.\nEmotions are characterized by dynamic and cues-dependent properties, making it\ndifficult to understand complex and evolving emotional states with reasonable\nrationale. To tackle these challenges, we propose a novel affective cues-guided\nreasoning framework that unifies fundamental attribute perception, expression\nanalysis, and high-level emotional understanding in a stage-wise manner. At the\ncore of our approach is a family of video emotion foundation models (VidEmo),\nspecifically designed for emotion reasoning and instruction-following. These\nmodels undergo a two-stage tuning process: first, curriculum emotion learning\nfor injecting emotion knowledge, followed by affective-tree reinforcement\nlearning for emotion reasoning. Moreover, we establish a foundational data\ninfrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG)\nconsisting of 2.1M diverse instruction-based samples. Emo-CFG includes\nexplainable emotional question-answering, fine-grained captions, and associated\nrationales, providing essential resources for advancing emotion understanding\ntasks. Experimental results demonstrate that our approach achieves competitive\nperformance, setting a new milestone across 15 face perception tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02712.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 154
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.02374",
            "authors": [
                {
                    "_id": "690b9bbf60494e4fa767558b",
                    "name": "Mohd Nauman",
                    "hidden": false
                },
                {
                    "_id": "690b9bbf60494e4fa767558c",
                    "name": "Sravan Gvm",
                    "hidden": false
                },
                {
                    "_id": "690b9bbf60494e4fa767558d",
                    "user": {
                        "_id": "64d22cc68f912d78587f396f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d22cc68f912d78587f396f/g50XH8IbGu5uG3X02CDrM.jpeg",
                        "isPro": false,
                        "fullname": "Vijay Devane",
                        "user": "vjdevane",
                        "type": "user"
                    },
                    "name": "Vijay Devane",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-05T19:56:53.345Z",
                    "hidden": false
                },
                {
                    "_id": "690b9bbf60494e4fa767558e",
                    "name": "Shyam Pawar",
                    "hidden": false
                },
                {
                    "_id": "690b9bbf60494e4fa767558f",
                    "name": "Viraj Thakur",
                    "hidden": false
                },
                {
                    "_id": "690b9bbf60494e4fa7675590",
                    "name": "Kundeshwar Pundalik",
                    "hidden": false
                },
                {
                    "_id": "690b9bbf60494e4fa7675591",
                    "name": "Piyush Sawarkar",
                    "hidden": false
                },
                {
                    "_id": "690b9bbf60494e4fa7675592",
                    "name": "Rohit Saluja",
                    "hidden": false
                },
                {
                    "_id": "690b9bbf60494e4fa7675593",
                    "name": "Maunendra Desarkar",
                    "hidden": false
                },
                {
                    "_id": "690b9bbf60494e4fa7675594",
                    "name": "Ganesh Ramakrishnan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-04T08:53:21.000Z",
            "submittedOnDailyAt": "2025-11-05T16:18:24.109Z",
            "title": "AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda",
            "submittedOnDailyBy": {
                "_id": "64d22cc68f912d78587f396f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d22cc68f912d78587f396f/g50XH8IbGu5uG3X02CDrM.jpeg",
                "isPro": false,
                "fullname": "Vijay Devane",
                "user": "vjdevane",
                "type": "user"
            },
            "summary": "Current large language models excel at broad, general-purpose tasks, but\nconsistently underperform when exposed to highly specialized domains that\nrequire deep cultural, linguistic, and subject-matter expertise. In particular,\ntraditional medical systems such as Ayurveda embody centuries of nuanced\ntextual and clinical knowledge that mainstream LLMs fail to accurately\ninterpret or apply. We introduce AyurParam-2.9B, a domain-specialized,\nbilingual language model fine-tuned from Param-1-2.9B using an extensive,\nexpertly curated Ayurveda dataset spanning classical texts and clinical\nguidance. AyurParam's dataset incorporates context-aware, reasoning, and\nobjective-style Q&A in both English and Hindi, with rigorous annotation\nprotocols for factual precision and instructional clarity. Benchmarked on\nBhashaBench-Ayur, AyurParam not only surpasses all open-source\ninstruction-tuned models in its size class (1.5--3B parameters), but also\ndemonstrates competitive or superior performance compared to much larger\nmodels. The results from AyurParam highlight the necessity for authentic domain\nadaptation and high-quality supervision in delivering reliable, culturally\ncongruent AI for specialized medical knowledge.",
            "upvotes": 1,
            "discussionId": "690b9bc060494e4fa7675595",
            "ai_summary": "AyurParam-2.9B, a domain-specialized bilingual language model fine-tuned for Ayurveda, outperforms other models in its size class and demonstrates competitive performance on specialized medical knowledge tasks.",
            "ai_keywords": [
                "domain-specialized",
                "bilingual language model",
                "fine-tuned",
                "Ayurveda dataset",
                "context-aware",
                "reasoning",
                "objective-style Q&A",
                "BhashaBench-Ayur",
                "domain adaptation",
                "high-quality supervision"
            ],
            "organization": {
                "_id": "67b473e74dd7ea0538ef5d5f",
                "name": "bharatgenai",
                "fullname": "BharatGen AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67b462a1f4f414c2b3e2bc2f/EnVeNWEIeZ6yF6ueZ7E3Y.jpeg"
            }
        },
        "publishedAt": "2025-11-04T03:53:21.000Z",
        "title": "AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda",
        "summary": "Current large language models excel at broad, general-purpose tasks, but\nconsistently underperform when exposed to highly specialized domains that\nrequire deep cultural, linguistic, and subject-matter expertise. In particular,\ntraditional medical systems such as Ayurveda embody centuries of nuanced\ntextual and clinical knowledge that mainstream LLMs fail to accurately\ninterpret or apply. We introduce AyurParam-2.9B, a domain-specialized,\nbilingual language model fine-tuned from Param-1-2.9B using an extensive,\nexpertly curated Ayurveda dataset spanning classical texts and clinical\nguidance. AyurParam's dataset incorporates context-aware, reasoning, and\nobjective-style Q&A in both English and Hindi, with rigorous annotation\nprotocols for factual precision and instructional clarity. Benchmarked on\nBhashaBench-Ayur, AyurParam not only surpasses all open-source\ninstruction-tuned models in its size class (1.5--3B parameters), but also\ndemonstrates competitive or superior performance compared to much larger\nmodels. The results from AyurParam highlight the necessity for authentic domain\nadaptation and high-quality supervision in delivering reliable, culturally\ncongruent AI for specialized medical knowledge.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02374.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64d22cc68f912d78587f396f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d22cc68f912d78587f396f/g50XH8IbGu5uG3X02CDrM.jpeg",
            "fullname": "Vijay Devane",
            "name": "vjdevane",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "67b473e74dd7ea0538ef5d5f",
            "name": "bharatgenai",
            "fullname": "BharatGen AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67b462a1f4f414c2b3e2bc2f/EnVeNWEIeZ6yF6ueZ7E3Y.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.02219",
            "authors": [
                {
                    "_id": "690b05add70e173c845290bd",
                    "user": {
                        "_id": "652fc2605615e57807e3db19",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fc2605615e57807e3db19/kbRcpR0YFQnU3IlziqCHf.png",
                        "isPro": false,
                        "fullname": "Changjiang Jiang",
                        "user": "arnodjiang",
                        "type": "user"
                    },
                    "name": "Changjiang Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-05T16:24:21.778Z",
                    "hidden": false
                },
                {
                    "_id": "690b05add70e173c845290be",
                    "name": "Fengchang Yu",
                    "hidden": false
                },
                {
                    "_id": "690b05add70e173c845290bf",
                    "name": "Haihua Chen",
                    "hidden": false
                },
                {
                    "_id": "690b05add70e173c845290c0",
                    "name": "Wei Lu",
                    "hidden": false
                },
                {
                    "_id": "690b05add70e173c845290c1",
                    "name": "Jin Zeng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-04T03:13:02.000Z",
            "submittedOnDailyAt": "2025-11-05T05:38:18.554Z",
            "title": "TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning\n  in Tabular Data",
            "submittedOnDailyBy": {
                "_id": "652fc2605615e57807e3db19",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fc2605615e57807e3db19/kbRcpR0YFQnU3IlziqCHf.png",
                "isPro": false,
                "fullname": "Changjiang Jiang",
                "user": "arnodjiang",
                "type": "user"
            },
            "summary": "Complex reasoning over tabular data is crucial in real-world data analysis,\nyet large language models (LLMs) often underperform due to complex queries,\nnoisy data, and limited numerical capabilities. To address these issues, we\npropose \\method, a framework consisting of: (1) a query decomposer that breaks\ndown complex questions, (2) a table sanitizer that cleans and filters noisy\ntables, and (3) a program-of-thoughts (PoT)-based reasoner that generates\nexecutable code to derive the final answer from the sanitized table. To ensure\nunbiased evaluation and mitigate data leakage, we introduce a new dataset,\nCalTab151, specifically designed for complex numerical reasoning over tables.\nExperimental results demonstrate that \\method consistently outperforms existing\nmethods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and\n19.87% accuracy improvement on TAT-QA, TableBench, and \\method, respectively.\nMoreover, our framework integrates seamlessly with mainstream LLMs, providing a\nrobust solution for complex tabular numerical reasoning. These findings\nhighlight the effectiveness of our framework in enhancing LLM performance for\ncomplex tabular numerical reasoning. Data and code are available upon request.",
            "upvotes": 1,
            "discussionId": "690b05aed70e173c845290c2",
            "ai_summary": "A framework combining query decomposition, table sanitization, and program-of-thoughts reasoning improves large language models' performance on complex tabular numerical reasoning tasks.",
            "ai_keywords": [
                "query decomposer",
                "table sanitizer",
                "program-of-thoughts",
                "CalTab151",
                "TAT-QA",
                "TableBench"
            ]
        },
        "publishedAt": "2025-11-03T22:13:02.000Z",
        "title": "TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning\n  in Tabular Data",
        "summary": "Complex reasoning over tabular data is crucial in real-world data analysis,\nyet large language models (LLMs) often underperform due to complex queries,\nnoisy data, and limited numerical capabilities. To address these issues, we\npropose \\method, a framework consisting of: (1) a query decomposer that breaks\ndown complex questions, (2) a table sanitizer that cleans and filters noisy\ntables, and (3) a program-of-thoughts (PoT)-based reasoner that generates\nexecutable code to derive the final answer from the sanitized table. To ensure\nunbiased evaluation and mitigate data leakage, we introduce a new dataset,\nCalTab151, specifically designed for complex numerical reasoning over tables.\nExperimental results demonstrate that \\method consistently outperforms existing\nmethods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and\n19.87% accuracy improvement on TAT-QA, TableBench, and \\method, respectively.\nMoreover, our framework integrates seamlessly with mainstream LLMs, providing a\nrobust solution for complex tabular numerical reasoning. These findings\nhighlight the effectiveness of our framework in enhancing LLM performance for\ncomplex tabular numerical reasoning. Data and code are available upon request.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02219.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "652fc2605615e57807e3db19",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fc2605615e57807e3db19/kbRcpR0YFQnU3IlziqCHf.png",
            "fullname": "Changjiang Jiang",
            "name": "arnodjiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.01450",
            "authors": [
                {
                    "_id": "690b7e0a60494e4fa7675560",
                    "user": {
                        "_id": "674ec1c465d9c6670de9fee1",
                        "avatarUrl": "/avatars/564338e32bd475dd0265f417fd5c7632.svg",
                        "isPro": false,
                        "fullname": "dujie",
                        "user": "dujielvtqs",
                        "type": "user"
                    },
                    "name": "Jie Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-05T16:46:05.265Z",
                    "hidden": false
                },
                {
                    "_id": "690b7e0a60494e4fa7675561",
                    "name": "Xinyu Gong",
                    "hidden": false
                },
                {
                    "_id": "690b7e0a60494e4fa7675562",
                    "name": "Qingshan Tan",
                    "hidden": false
                },
                {
                    "_id": "690b7e0a60494e4fa7675563",
                    "name": "Wen Li",
                    "hidden": false
                },
                {
                    "_id": "690b7e0a60494e4fa7675564",
                    "name": "Yangming Cheng",
                    "hidden": false
                },
                {
                    "_id": "690b7e0a60494e4fa7675565",
                    "name": "Weitao Wang",
                    "hidden": false
                },
                {
                    "_id": "690b7e0a60494e4fa7675566",
                    "name": "Chenlu Zhan",
                    "hidden": false
                },
                {
                    "_id": "690b7e0a60494e4fa7675567",
                    "name": "Suhui Wu",
                    "hidden": false
                },
                {
                    "_id": "690b7e0a60494e4fa7675568",
                    "name": "Hao Zhang",
                    "hidden": false
                },
                {
                    "_id": "690b7e0a60494e4fa7675569",
                    "name": "Jun Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-03T11:04:22.000Z",
            "submittedOnDailyAt": "2025-11-05T14:26:42.114Z",
            "title": "Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for\n  Improving Video Generation",
            "submittedOnDailyBy": {
                "_id": "674ec1c465d9c6670de9fee1",
                "avatarUrl": "/avatars/564338e32bd475dd0265f417fd5c7632.svg",
                "isPro": false,
                "fullname": "dujie",
                "user": "dujielvtqs",
                "type": "user"
            },
            "summary": "Recent studies have identified Direct Preference Optimization (DPO) as an\nefficient and reward-free approach to improving video generation quality.\nHowever, existing methods largely follow image-domain paradigms and are mainly\ndeveloped on small-scale models (approximately 2B parameters), limiting their\nability to address the unique challenges of video tasks, such as costly data\nconstruction, unstable training, and heavy memory consumption. To overcome\nthese limitations, we introduce a GT-Pair that automatically builds\nhigh-quality preference pairs by using real videos as positives and\nmodel-generated videos as negatives, eliminating the need for any external\nannotation. We further present Reg-DPO, which incorporates the SFT loss as a\nregularization term into the DPO objective to enhance training stability and\ngeneration fidelity. Additionally, by combining the FSDP framework with\nmultiple memory optimization techniques, our approach achieves nearly three\ntimes higher training capacity than using FSDP alone. Extensive experiments on\nboth I2V and T2V tasks across multiple datasets demonstrate that our method\nconsistently outperforms existing approaches, delivering superior video\ngeneration quality.",
            "upvotes": 1,
            "discussionId": "690b7e0a60494e4fa767556a",
            "githubRepo": "https://github.com/JieDuTQS/Reg-DPO",
            "ai_summary": "A novel approach combining GT-Pair and Reg-DPO enhances video generation quality by addressing data construction, training stability, and memory consumption challenges.",
            "ai_keywords": [
                "Direct Preference Optimization",
                "DPO",
                "GT-Pair",
                "Reg-DPO",
                "SFT loss",
                "FSDP framework",
                "I2V tasks",
                "T2V tasks"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "publishedAt": "2025-11-03T06:04:22.000Z",
        "title": "Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for\n  Improving Video Generation",
        "summary": "Recent studies have identified Direct Preference Optimization (DPO) as an\nefficient and reward-free approach to improving video generation quality.\nHowever, existing methods largely follow image-domain paradigms and are mainly\ndeveloped on small-scale models (approximately 2B parameters), limiting their\nability to address the unique challenges of video tasks, such as costly data\nconstruction, unstable training, and heavy memory consumption. To overcome\nthese limitations, we introduce a GT-Pair that automatically builds\nhigh-quality preference pairs by using real videos as positives and\nmodel-generated videos as negatives, eliminating the need for any external\nannotation. We further present Reg-DPO, which incorporates the SFT loss as a\nregularization term into the DPO objective to enhance training stability and\ngeneration fidelity. Additionally, by combining the FSDP framework with\nmultiple memory optimization techniques, our approach achieves nearly three\ntimes higher training capacity than using FSDP alone. Extensive experiments on\nboth I2V and T2V tasks across multiple datasets demonstrate that our method\nconsistently outperforms existing approaches, delivering superior video\ngeneration quality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01450.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "674ec1c465d9c6670de9fee1",
            "avatarUrl": "/avatars/564338e32bd475dd0265f417fd5c7632.svg",
            "fullname": "dujie",
            "name": "dujielvtqs",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.24932",
            "authors": [
                {
                    "_id": "6908db77812eca10f9cc6095",
                    "user": {
                        "_id": "66fc42b48bf64649c684b479",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66fc42b48bf64649c684b479/7ja7AWVPELZmHMUmIhaPP.jpeg",
                        "isPro": false,
                        "fullname": "Deepon",
                        "user": "deeponh",
                        "type": "user"
                    },
                    "name": "Deepon Halder",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-05T16:25:09.006Z",
                    "hidden": false
                },
                {
                    "_id": "6908db77812eca10f9cc6096",
                    "name": "Alan Saji",
                    "hidden": false
                },
                {
                    "_id": "6908db77812eca10f9cc6097",
                    "name": "Thanmay Jayakumar",
                    "hidden": false
                },
                {
                    "_id": "6908db77812eca10f9cc6098",
                    "name": "Ratish Puduppully",
                    "hidden": false
                },
                {
                    "_id": "6908db77812eca10f9cc6099",
                    "name": "Anoop Kunchukuttan",
                    "hidden": false
                },
                {
                    "_id": "6908db77812eca10f9cc609a",
                    "name": "Raj Dabre",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T19:58:24.000Z",
            "submittedOnDailyAt": "2025-11-05T13:44:19.912Z",
            "title": "RiddleBench: A New Generative Reasoning Benchmark for LLMs",
            "submittedOnDailyBy": {
                "_id": "66fc42b48bf64649c684b479",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66fc42b48bf64649c684b479/7ja7AWVPELZmHMUmIhaPP.jpeg",
                "isPro": false,
                "fullname": "Deepon",
                "user": "deeponh",
                "type": "user"
            },
            "summary": "Large Language Models have demonstrated strong performance on many\nestablished reasoning benchmarks. However, these benchmarks primarily evaluate\nstructured skills like quantitative problem-solving, leaving a gap in assessing\nflexible, multifaceted reasoning abilities that are central to human\nintelligence. These abilities require integrating logical deduction with\nspatial awareness and constraint satisfaction, which current evaluations do not\nmeasure well. To address this, we introduce RiddleBench, a benchmark of 1,737\nchallenging puzzles in English designed to probe these core reasoning\ncapabilities. Evaluation of state-of-the-art models on RiddleBench shows\nfundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3,\nand Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and\n63.16%). Analysis further reveals deep failures, including hallucination\ncascades (accepting flawed reasoning from other models) and poor\nself-correction due to a strong self-confirmation bias. Their reasoning is also\nfragile, with performance degrading significantly when constraints are\nreordered or irrelevant information is introduced. RiddleBench functions as a\ndiagnostic tool for these issues and as a resource for guiding the development\nof more robust and reliable language models.",
            "upvotes": 1,
            "discussionId": "6908db77812eca10f9cc609b",
            "ai_summary": "RiddleBench, a benchmark of 1,737 puzzles, reveals fundamental weaknesses in state-of-the-art language models, including hallucination cascades and poor self-correction, highlighting the need for more robust reasoning capabilities.",
            "ai_keywords": [
                "Large Language Models",
                "reasoning benchmarks",
                "structured skills",
                "quantitative problem-solving",
                "flexible reasoning",
                "multifaceted reasoning",
                "logical deduction",
                "spatial awareness",
                "constraint satisfaction",
                "RiddleBench",
                "hallucination cascades",
                "self-confirmation bias"
            ],
            "organization": {
                "_id": "5f566a838bf55658acfed290",
                "name": "ai4bharat",
                "fullname": "AI4Bharat",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645373988267-5fbfc7a6e366524fe8e97ccf.png"
            }
        },
        "publishedAt": "2025-10-28T15:58:24.000Z",
        "title": "RiddleBench: A New Generative Reasoning Benchmark for LLMs",
        "summary": "Large Language Models have demonstrated strong performance on many\nestablished reasoning benchmarks. However, these benchmarks primarily evaluate\nstructured skills like quantitative problem-solving, leaving a gap in assessing\nflexible, multifaceted reasoning abilities that are central to human\nintelligence. These abilities require integrating logical deduction with\nspatial awareness and constraint satisfaction, which current evaluations do not\nmeasure well. To address this, we introduce RiddleBench, a benchmark of 1,737\nchallenging puzzles in English designed to probe these core reasoning\ncapabilities. Evaluation of state-of-the-art models on RiddleBench shows\nfundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3,\nand Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and\n63.16%). Analysis further reveals deep failures, including hallucination\ncascades (accepting flawed reasoning from other models) and poor\nself-correction due to a strong self-confirmation bias. Their reasoning is also\nfragile, with performance degrading significantly when constraints are\nreordered or irrelevant information is introduced. RiddleBench functions as a\ndiagnostic tool for these issues and as a resource for guiding the development\nof more robust and reliable language models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24932.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66fc42b48bf64649c684b479",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66fc42b48bf64649c684b479/7ja7AWVPELZmHMUmIhaPP.jpeg",
            "fullname": "Deepon",
            "name": "deeponh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "5f566a838bf55658acfed290",
            "name": "ai4bharat",
            "fullname": "AI4Bharat",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645373988267-5fbfc7a6e366524fe8e97ccf.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.19278",
            "authors": [
                {
                    "_id": "69095f1c812eca10f9cc6163",
                    "user": {
                        "_id": "69095e06b8c21a27dbc0d2b6",
                        "avatarUrl": "/avatars/3c5f5c1140024c084bf8c8d935dd3d90.svg",
                        "isPro": false,
                        "fullname": "Nobline Yoo",
                        "user": "n-yoo",
                        "type": "user"
                    },
                    "name": "Nobline Yoo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:26:53.098Z",
                    "hidden": false
                },
                {
                    "_id": "69095f1c812eca10f9cc6164",
                    "name": "Olga Russakovsky",
                    "hidden": false
                },
                {
                    "_id": "69095f1c812eca10f9cc6165",
                    "name": "Ye Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T06:27:05.000Z",
            "submittedOnDailyAt": "2025-11-05T00:11:33.890Z",
            "title": "D2D: Detector-to-Differentiable Critic for Improved Numeracy in\n  Text-to-Image Generation",
            "submittedOnDailyBy": {
                "_id": "69095e06b8c21a27dbc0d2b6",
                "avatarUrl": "/avatars/3c5f5c1140024c084bf8c8d935dd3d90.svg",
                "isPro": false,
                "fullname": "Nobline Yoo",
                "user": "n-yoo",
                "type": "user"
            },
            "summary": "Text-to-image (T2I) diffusion models have achieved strong performance in\nsemantic alignment, yet they still struggle with generating the correct number\nof objects specified in prompts. Existing approaches typically incorporate\nauxiliary counting networks as external critics to enhance numeracy. However,\nsince these critics must provide gradient guidance during generation, they are\nrestricted to regression-based models that are inherently differentiable, thus\nexcluding detector-based models with superior counting ability, whose\ncount-via-enumeration nature is non-differentiable. To overcome this\nlimitation, we propose Detector-to-Differentiable (D2D), a novel framework that\ntransforms non-differentiable detection models into differentiable critics,\nthereby leveraging their superior counting ability to guide numeracy\ngeneration. Specifically, we design custom activation functions to convert\ndetector logits into soft binary indicators, which are then used to optimize\nthe noise prior at inference time with pre-trained T2I models. Our extensive\nexperiments on SDXL-Turbo, SD-Turbo, and Pixart-DMD across four benchmarks of\nvarying complexity (low-density, high-density, and multi-object scenarios)\ndemonstrate consistent and substantial improvements in object counting accuracy\n(e.g., boosting up to 13.7% on D2D-Small, a 400-prompt, low-density benchmark),\nwith minimal degradation in overall image quality and computational overhead.",
            "upvotes": 1,
            "discussionId": "69095f1d812eca10f9cc6166",
            "ai_summary": "A novel framework, Detector-to-Differentiable (D2D), transforms non-differentiable detection models into differentiable critics to improve object counting accuracy in text-to-image diffusion models with minimal impact on image quality.",
            "ai_keywords": [
                "text-to-image (T2I) diffusion models",
                "semantic alignment",
                "auxiliary counting networks",
                "gradient guidance",
                "detector-based models",
                "non-differentiable",
                "Detector-to-Differentiable (D2D)",
                "custom activation functions",
                "soft binary indicators",
                "noise prior",
                "SDXL-Turbo",
                "SD-Turbo",
                "Pixart-DMD",
                "object counting accuracy",
                "image quality",
                "computational overhead"
            ]
        },
        "publishedAt": "2025-10-22T02:27:05.000Z",
        "title": "D2D: Detector-to-Differentiable Critic for Improved Numeracy in\n  Text-to-Image Generation",
        "summary": "Text-to-image (T2I) diffusion models have achieved strong performance in\nsemantic alignment, yet they still struggle with generating the correct number\nof objects specified in prompts. Existing approaches typically incorporate\nauxiliary counting networks as external critics to enhance numeracy. However,\nsince these critics must provide gradient guidance during generation, they are\nrestricted to regression-based models that are inherently differentiable, thus\nexcluding detector-based models with superior counting ability, whose\ncount-via-enumeration nature is non-differentiable. To overcome this\nlimitation, we propose Detector-to-Differentiable (D2D), a novel framework that\ntransforms non-differentiable detection models into differentiable critics,\nthereby leveraging their superior counting ability to guide numeracy\ngeneration. Specifically, we design custom activation functions to convert\ndetector logits into soft binary indicators, which are then used to optimize\nthe noise prior at inference time with pre-trained T2I models. Our extensive\nexperiments on SDXL-Turbo, SD-Turbo, and Pixart-DMD across four benchmarks of\nvarying complexity (low-density, high-density, and multi-object scenarios)\ndemonstrate consistent and substantial improvements in object counting accuracy\n(e.g., boosting up to 13.7% on D2D-Small, a 400-prompt, low-density benchmark),\nwith minimal degradation in overall image quality and computational overhead.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19278.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "69095e06b8c21a27dbc0d2b6",
            "avatarUrl": "/avatars/3c5f5c1140024c084bf8c8d935dd3d90.svg",
            "fullname": "Nobline Yoo",
            "name": "n-yoo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.02366",
            "authors": [
                {
                    "_id": "690abdafd70e173c84528fc1",
                    "name": "Yudong Li",
                    "hidden": false
                },
                {
                    "_id": "690abdafd70e173c84528fc2",
                    "name": "Zhongliang Yang",
                    "hidden": false
                },
                {
                    "_id": "690abdafd70e173c84528fc3",
                    "name": "Kejiang Chen",
                    "hidden": false
                },
                {
                    "_id": "690abdafd70e173c84528fc4",
                    "name": "Wenxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "690abdafd70e173c84528fc5",
                    "name": "Tianxin Zhang",
                    "hidden": false
                },
                {
                    "_id": "690abdafd70e173c84528fc6",
                    "name": "Sifang Wan",
                    "hidden": false
                },
                {
                    "_id": "690abdafd70e173c84528fc7",
                    "name": "Kecheng Wang",
                    "hidden": false
                },
                {
                    "_id": "690abdafd70e173c84528fc8",
                    "name": "Haitian Li",
                    "hidden": false
                },
                {
                    "_id": "690abdafd70e173c84528fc9",
                    "name": "Xu Wang",
                    "hidden": false
                },
                {
                    "_id": "690abdafd70e173c84528fca",
                    "name": "Lefan Cheng",
                    "hidden": false
                },
                {
                    "_id": "690abdafd70e173c84528fcb",
                    "name": "Youdan Yang",
                    "hidden": false
                },
                {
                    "_id": "690abdafd70e173c84528fcc",
                    "name": "Baocheng Chen",
                    "hidden": false
                },
                {
                    "_id": "690abdafd70e173c84528fcd",
                    "name": "Ziyu Liu",
                    "hidden": false
                },
                {
                    "_id": "690abdafd70e173c84528fce",
                    "name": "Yufei Sun",
                    "hidden": false
                },
                {
                    "_id": "690abdafd70e173c84528fcf",
                    "name": "Liyan Wu",
                    "hidden": false
                },
                {
                    "_id": "690abdafd70e173c84528fd0",
                    "name": "Wenya Wen",
                    "hidden": false
                },
                {
                    "_id": "690abdafd70e173c84528fd1",
                    "name": "Xingchi Gu",
                    "hidden": false
                },
                {
                    "_id": "690abdafd70e173c84528fd2",
                    "name": "Peiru Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-04T08:44:09.000Z",
            "submittedOnDailyAt": "2025-11-05T00:30:20.539Z",
            "title": "LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for\n  LLMs in Chinese Context",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "In this work, we propose LiveSecBench, a dynamic and continuously updated\nsafety benchmark specifically for Chinese-language LLM application scenarios.\nLiveSecBench evaluates models across six critical dimensions (Legality, Ethics,\nFactuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in\nthe Chinese legal and social frameworks. This benchmark maintains relevance\nthrough a dynamic update schedule that incorporates new threat vectors, such as\nthe planned inclusion of Text-to-Image Generation Safety and Agentic Safety in\nthe next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs,\nproviding a landscape of AI safety in the context of Chinese language. The\nleaderboard is publicly accessible at https://livesecbench.intokentech.cn/.",
            "upvotes": 0,
            "discussionId": "690abdb0d70e173c84528fd3",
            "projectPage": "https://livesecbench.intokentech.cn/",
            "ai_summary": "LiveSecBench is a continuously updated safety benchmark for Chinese-language LLMs, evaluating them across six critical dimensions including legality, ethics, factuality, privacy, adversarial robustness, and reasoning safety.",
            "ai_keywords": [
                "LLM",
                "LiveSecBench",
                "Text-to-Image Generation Safety",
                "Agentic Safety"
            ]
        },
        "publishedAt": "2025-11-04T03:44:09.000Z",
        "title": "LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for\n  LLMs in Chinese Context",
        "summary": "In this work, we propose LiveSecBench, a dynamic and continuously updated\nsafety benchmark specifically for Chinese-language LLM application scenarios.\nLiveSecBench evaluates models across six critical dimensions (Legality, Ethics,\nFactuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in\nthe Chinese legal and social frameworks. This benchmark maintains relevance\nthrough a dynamic update schedule that incorporates new threat vectors, such as\nthe planned inclusion of Text-to-Image Generation Safety and Agentic Safety in\nthe next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs,\nproviding a landscape of AI safety in the context of Chinese language. The\nleaderboard is publicly accessible at https://livesecbench.intokentech.cn/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02366.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 154
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.01502",
            "authors": [
                {
                    "_id": "690ab99cd70e173c84528f9c",
                    "name": "Mengtan Zhang",
                    "hidden": false
                },
                {
                    "_id": "690ab99cd70e173c84528f9d",
                    "name": "Zizhan Guo",
                    "hidden": false
                },
                {
                    "_id": "690ab99cd70e173c84528f9e",
                    "name": "Hongbo Zhao",
                    "hidden": false
                },
                {
                    "_id": "690ab99cd70e173c84528f9f",
                    "name": "Yi Feng",
                    "hidden": false
                },
                {
                    "_id": "690ab99cd70e173c84528fa0",
                    "name": "Zuyi Xiong",
                    "hidden": false
                },
                {
                    "_id": "690ab99cd70e173c84528fa1",
                    "name": "Yue Wang",
                    "hidden": false
                },
                {
                    "_id": "690ab99cd70e173c84528fa2",
                    "name": "Shaoyi Du",
                    "hidden": false
                },
                {
                    "_id": "690ab99cd70e173c84528fa3",
                    "name": "Hanli Wang",
                    "hidden": false
                },
                {
                    "_id": "690ab99cd70e173c84528fa4",
                    "name": "Rui Fan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-03T12:14:52.000Z",
            "submittedOnDailyAt": "2025-11-05T01:00:57.758Z",
            "title": "Discriminately Treating Motion Components Evolves Joint Depth and\n  Ego-Motion Learning",
            "submittedOnDailyBy": {
                "_id": "69098f192b98bb91436f24f0",
                "avatarUrl": "/avatars/103c790078d5e8c6f7e141de42c8e261.svg",
                "isPro": false,
                "fullname": "Mengtan Zhang",
                "user": "mengtanZ",
                "type": "user"
            },
            "summary": "Unsupervised learning of depth and ego-motion, two fundamental 3D perception\ntasks, has made significant strides in recent years. However, most methods\ntreat ego-motion as an auxiliary task, either mixing all motion types or\nexcluding depth-independent rotational motions in supervision. Such designs\nlimit the incorporation of strong geometric constraints, reducing reliability\nand robustness under diverse conditions. This study introduces a discriminative\ntreatment of motion components, leveraging the geometric regularities of their\nrespective rigid flows to benefit both depth and ego-motion estimation. Given\nconsecutive video frames, network outputs first align the optical axes and\nimaging planes of the source and target cameras. Optical flows between frames\nare transformed through these alignments, and deviations are quantified to\nimpose geometric constraints individually on each ego-motion component,\nenabling more targeted refinement. These alignments further reformulate the\njoint learning process into coaxial and coplanar forms, where depth and each\ntranslation component can be mutually derived through closed-form geometric\nrelationships, introducing complementary constraints that improve depth\nrobustness. DiMoDE, a general depth and ego-motion joint learning framework\nincorporating these designs, achieves state-of-the-art performance on multiple\npublic datasets and a newly collected diverse real-world dataset, particularly\nunder challenging conditions. Our source code will be publicly available at\nmias.group/DiMoDE upon publication.",
            "upvotes": 0,
            "discussionId": "690ab99dd70e173c84528fa5",
            "ai_summary": "A discriminative approach to depth and ego-motion estimation leverages geometric constraints to improve performance and robustness in 3D perception tasks.",
            "ai_keywords": [
                "unsupervised learning",
                "depth",
                "ego-motion",
                "3D perception",
                "optical axes",
                "imaging planes",
                "optical flows",
                "geometric constraints",
                "coaxial",
                "coplanar",
                "closed-form geometric relationships",
                "DiMoDE"
            ]
        },
        "publishedAt": "2025-11-03T07:14:52.000Z",
        "title": "Discriminately Treating Motion Components Evolves Joint Depth and\n  Ego-Motion Learning",
        "summary": "Unsupervised learning of depth and ego-motion, two fundamental 3D perception\ntasks, has made significant strides in recent years. However, most methods\ntreat ego-motion as an auxiliary task, either mixing all motion types or\nexcluding depth-independent rotational motions in supervision. Such designs\nlimit the incorporation of strong geometric constraints, reducing reliability\nand robustness under diverse conditions. This study introduces a discriminative\ntreatment of motion components, leveraging the geometric regularities of their\nrespective rigid flows to benefit both depth and ego-motion estimation. Given\nconsecutive video frames, network outputs first align the optical axes and\nimaging planes of the source and target cameras. Optical flows between frames\nare transformed through these alignments, and deviations are quantified to\nimpose geometric constraints individually on each ego-motion component,\nenabling more targeted refinement. These alignments further reformulate the\njoint learning process into coaxial and coplanar forms, where depth and each\ntranslation component can be mutually derived through closed-form geometric\nrelationships, introducing complementary constraints that improve depth\nrobustness. DiMoDE, a general depth and ego-motion joint learning framework\nincorporating these designs, achieves state-of-the-art performance on multiple\npublic datasets and a newly collected diverse real-world dataset, particularly\nunder challenging conditions. Our source code will be publicly available at\nmias.group/DiMoDE upon publication.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01502.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "69098f192b98bb91436f24f0",
            "avatarUrl": "/avatars/103c790078d5e8c6f7e141de42c8e261.svg",
            "fullname": "Mengtan Zhang",
            "name": "mengtanZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]