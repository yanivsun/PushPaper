[
    {
        "paper": {
            "id": "2511.04570",
            "authors": [
                {
                    "_id": "690d5b51ad2597bf6c464ca9",
                    "name": "Jingqi Tong",
                    "hidden": false
                },
                {
                    "_id": "690d5b51ad2597bf6c464caa",
                    "name": "Yurong Mou",
                    "hidden": false
                },
                {
                    "_id": "690d5b51ad2597bf6c464cab",
                    "name": "Hangcheng Li",
                    "hidden": false
                },
                {
                    "_id": "690d5b51ad2597bf6c464cac",
                    "user": {
                        "_id": "65e4134472e748aae53e24f3",
                        "avatarUrl": "/avatars/3346f4f4cdbffc4c51276be01a6c5f10.svg",
                        "isPro": false,
                        "fullname": "Mingzhe Li",
                        "user": "Mubuky",
                        "type": "user"
                    },
                    "name": "Mingzhe Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-07T10:28:47.508Z",
                    "hidden": false
                },
                {
                    "_id": "690d5b51ad2597bf6c464cad",
                    "name": "Yongzhuo Yang",
                    "hidden": false
                },
                {
                    "_id": "690d5b51ad2597bf6c464cae",
                    "name": "Ming Zhang",
                    "hidden": false
                },
                {
                    "_id": "690d5b51ad2597bf6c464caf",
                    "name": "Qiguang Chen",
                    "hidden": false
                },
                {
                    "_id": "690d5b51ad2597bf6c464cb0",
                    "name": "Tianyi Liang",
                    "hidden": false
                },
                {
                    "_id": "690d5b51ad2597bf6c464cb1",
                    "name": "Xiaomeng Hu",
                    "hidden": false
                },
                {
                    "_id": "690d5b51ad2597bf6c464cb2",
                    "name": "Yining Zheng",
                    "hidden": false
                },
                {
                    "_id": "690d5b51ad2597bf6c464cb3",
                    "name": "Xinchi Chen",
                    "hidden": false
                },
                {
                    "_id": "690d5b51ad2597bf6c464cb4",
                    "name": "Jun Zhao",
                    "hidden": false
                },
                {
                    "_id": "690d5b51ad2597bf6c464cb5",
                    "name": "Xuanjing Huang",
                    "hidden": false
                },
                {
                    "_id": "690d5b51ad2597bf6c464cb6",
                    "name": "Xipeng Qiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-06T17:25:23.000Z",
            "submittedOnDailyAt": "2025-11-07T00:18:17.549Z",
            "title": "Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm",
            "submittedOnDailyBy": {
                "_id": "6690e13ccbcaf7ab0ec1c971",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/e8KDV6J29tviXlIpLZPq6.png",
                "isPro": false,
                "fullname": "Tony.Li",
                "user": "lkdhy",
                "type": "user"
            },
            "summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leverages video generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing\nPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind that self-consistency and in-context learning can improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm.",
            "upvotes": 127,
            "discussionId": "690d5b51ad2597bf6c464cb7",
            "projectPage": "https://thinking-with-video.github.io/",
            "githubRepo": "https://github.com/tongjingqi/Thinking-with-Video",
            "ai_summary": "The \"Thinking with Video\" paradigm enhances multimodal reasoning by integrating video generation models, demonstrated through the Video Thinking Benchmark and improved performance on both vision and text tasks.",
            "ai_keywords": [
                "Thinking with Text",
                "Thinking with Images",
                "large language models",
                "Vision Language Models",
                "Thinking with Video",
                "video generation models",
                "Video Thinking Benchmark",
                "vision-centric tasks",
                "text-centric tasks",
                "Eyeballing Puzzles",
                "GSM8K",
                "MMMU",
                "self-consistency",
                "in-context learning"
            ],
            "githubStars": 94,
            "organization": {
                "_id": "613b0dee83ec35d460684607",
                "name": "OpenMOSS-Team",
                "fullname": "OpenMOSS",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
            }
        },
        "publishedAt": "2025-11-06T12:25:23.000Z",
        "title": "Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm",
        "summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leverages video generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing\nPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind that self-consistency and in-context learning can improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04570.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6690e13ccbcaf7ab0ec1c971",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/e8KDV6J29tviXlIpLZPq6.png",
            "fullname": "Tony.Li",
            "name": "lkdhy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "613b0dee83ec35d460684607",
            "name": "OpenMOSS-Team",
            "fullname": "OpenMOSS",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.04460",
            "authors": [
                {
                    "_id": "690d5b2aad2597bf6c464c9a",
                    "name": "Runqi Qiao",
                    "hidden": false
                },
                {
                    "_id": "690d5b2aad2597bf6c464c9b",
                    "name": "Qiuna Tan",
                    "hidden": false
                },
                {
                    "_id": "690d5b2aad2597bf6c464c9c",
                    "name": "Minghan Yang",
                    "hidden": false
                },
                {
                    "_id": "690d5b2aad2597bf6c464c9d",
                    "name": "Guanting Dong",
                    "hidden": false
                },
                {
                    "_id": "690d5b2aad2597bf6c464c9e",
                    "name": "Peiqing Yang",
                    "hidden": false
                },
                {
                    "_id": "690d5b2aad2597bf6c464c9f",
                    "name": "Shiqiang Lang",
                    "hidden": false
                },
                {
                    "_id": "690d5b2aad2597bf6c464ca0",
                    "name": "Enhui Wan",
                    "hidden": false
                },
                {
                    "_id": "690d5b2aad2597bf6c464ca1",
                    "name": "Xiaowan Wang",
                    "hidden": false
                },
                {
                    "_id": "690d5b2aad2597bf6c464ca2",
                    "name": "Yida Xu",
                    "hidden": false
                },
                {
                    "_id": "690d5b2aad2597bf6c464ca3",
                    "name": "Lan Yang",
                    "hidden": false
                },
                {
                    "_id": "690d5b2aad2597bf6c464ca4",
                    "name": "Chong Sun",
                    "hidden": false
                },
                {
                    "_id": "690d5b2aad2597bf6c464ca5",
                    "name": "Chen Li",
                    "hidden": false
                },
                {
                    "_id": "690d5b2aad2597bf6c464ca6",
                    "name": "Honggang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-06T15:32:29.000Z",
            "submittedOnDailyAt": "2025-11-07T00:09:41.943Z",
            "title": "V-Thinker: Interactive Thinking with Images",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Empowering Large Multimodal Models (LMMs) to deeply integrate image\ninteraction with long-horizon reasoning capabilities remains a long-standing\nchallenge in this field. Recent advances in vision-centric reasoning explore a\npromising \"Thinking with Images\" paradigm for LMMs, marking a shift from\nimage-assisted reasoning to image-interactive thinking. While this milestone\nenables models to focus on fine-grained image regions, progress remains\nconstrained by limited visual tool spaces and task-specific workflow designs.\nTo bridge this gap, we present V-Thinker, a general-purpose multimodal\nreasoning assistant that enables interactive, vision-centric thinking through\nend-to-end reinforcement learning. V-Thinker comprises two key components: (1)\na Data Evolution Flywheel that automatically synthesizes, evolves, and verifies\ninteractive reasoning datasets across three dimensions-diversity, quality, and\ndifficulty; and (2) a Visual Progressive Training Curriculum that first aligns\nperception via point-level supervision, then integrates interactive reasoning\nthrough a two-stage reinforcement learning framework. Furthermore, we introduce\nVTBench, an expert-verified benchmark targeting vision-centric interactive\nreasoning tasks. Extensive experiments demonstrate that V-Thinker consistently\noutperforms strong LMM-based baselines in both general and interactive\nreasoning scenarios, providing valuable insights for advancing\nimage-interactive reasoning applications.",
            "upvotes": 65,
            "discussionId": "690d5b2aad2597bf6c464ca7",
            "githubRepo": "https://github.com/We-Math/V-Thinker",
            "ai_summary": "V-Thinker, a multimodal reasoning assistant using reinforcement learning, enhances image-interactive thinking by synthesizing datasets and aligning perception for improved performance in vision-centric tasks.",
            "ai_keywords": [
                "multimodal models",
                "image interaction",
                "long-horizon reasoning",
                "Thinking with Images",
                "image-interactive thinking",
                "end-to-end reinforcement learning",
                "Data Evolution Flywheel",
                "Visual Progressive Training Curriculum",
                "point-level supervision",
                "two-stage reinforcement learning",
                "VTBench",
                "vision-centric interactive reasoning"
            ],
            "githubStars": 56
        },
        "publishedAt": "2025-11-06T10:32:29.000Z",
        "title": "V-Thinker: Interactive Thinking with Images",
        "summary": "Empowering Large Multimodal Models (LMMs) to deeply integrate image\ninteraction with long-horizon reasoning capabilities remains a long-standing\nchallenge in this field. Recent advances in vision-centric reasoning explore a\npromising \"Thinking with Images\" paradigm for LMMs, marking a shift from\nimage-assisted reasoning to image-interactive thinking. While this milestone\nenables models to focus on fine-grained image regions, progress remains\nconstrained by limited visual tool spaces and task-specific workflow designs.\nTo bridge this gap, we present V-Thinker, a general-purpose multimodal\nreasoning assistant that enables interactive, vision-centric thinking through\nend-to-end reinforcement learning. V-Thinker comprises two key components: (1)\na Data Evolution Flywheel that automatically synthesizes, evolves, and verifies\ninteractive reasoning datasets across three dimensions-diversity, quality, and\ndifficulty; and (2) a Visual Progressive Training Curriculum that first aligns\nperception via point-level supervision, then integrates interactive reasoning\nthrough a two-stage reinforcement learning framework. Furthermore, we introduce\nVTBench, an expert-verified benchmark targeting vision-centric interactive\nreasoning tasks. Extensive experiments demonstrate that V-Thinker consistently\noutperforms strong LMM-based baselines in both general and interactive\nreasoning scenarios, providing valuable insights for advancing\nimage-interactive reasoning applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04460.png",
        "numComments": 7,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 156
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.03773",
            "authors": [
                {
                    "_id": "690d51e4ad2597bf6c464c52",
                    "name": "Zhaorun Chen",
                    "hidden": false
                },
                {
                    "_id": "690d51e4ad2597bf6c464c53",
                    "name": "Zhuokai Zhao",
                    "hidden": false
                },
                {
                    "_id": "690d51e4ad2597bf6c464c54",
                    "user": {
                        "_id": "63e0a50242591dda0b9dca5c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e0a50242591dda0b9dca5c/c7cBPEBWQDFYimfGnO_SI.png",
                        "isPro": false,
                        "fullname": "Kai Zhang",
                        "user": "drogozhang",
                        "type": "user"
                    },
                    "name": "Kai Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-07T10:30:52.478Z",
                    "hidden": false
                },
                {
                    "_id": "690d51e4ad2597bf6c464c55",
                    "user": {
                        "_id": "635e3a76106f984574c36409",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
                        "isPro": false,
                        "fullname": "Bo Liu",
                        "user": "Benjamin-eecs",
                        "type": "user"
                    },
                    "name": "Bo Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-07T10:30:56.570Z",
                    "hidden": false
                },
                {
                    "_id": "690d51e4ad2597bf6c464c56",
                    "name": "Qi Qi",
                    "hidden": false
                },
                {
                    "_id": "690d51e4ad2597bf6c464c57",
                    "user": {
                        "_id": "64641a2938083255f6769953",
                        "avatarUrl": "/avatars/a4117357703607bd7b290dc2975acbef.svg",
                        "isPro": false,
                        "fullname": "Yifan Wu",
                        "user": "yfwu",
                        "type": "user"
                    },
                    "name": "Yifan Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-07T10:30:54.351Z",
                    "hidden": false
                },
                {
                    "_id": "690d51e4ad2597bf6c464c58",
                    "name": "Tarun Kalluri",
                    "hidden": false
                },
                {
                    "_id": "690d51e4ad2597bf6c464c59",
                    "name": "Sara Cao",
                    "hidden": false
                },
                {
                    "_id": "690d51e4ad2597bf6c464c5a",
                    "name": "Yuanhao Xiong",
                    "hidden": false
                },
                {
                    "_id": "690d51e4ad2597bf6c464c5b",
                    "name": "Haibo Tong",
                    "hidden": false
                },
                {
                    "_id": "690d51e4ad2597bf6c464c5c",
                    "name": "Huaxiu Yao",
                    "hidden": false
                },
                {
                    "_id": "690d51e4ad2597bf6c464c5d",
                    "name": "Hengduo Li",
                    "hidden": false
                },
                {
                    "_id": "690d51e4ad2597bf6c464c5e",
                    "name": "Jiacheng Zhu",
                    "hidden": false
                },
                {
                    "_id": "690d51e4ad2597bf6c464c5f",
                    "name": "Xian Li",
                    "hidden": false
                },
                {
                    "_id": "690d51e4ad2597bf6c464c60",
                    "name": "Dawn Song",
                    "hidden": false
                },
                {
                    "_id": "690d51e4ad2597bf6c464c61",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "690d51e4ad2597bf6c464c62",
                    "name": "Jason Weston",
                    "hidden": false
                },
                {
                    "_id": "690d51e4ad2597bf6c464c63",
                    "name": "Dat Huynh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-05T18:58:48.000Z",
            "submittedOnDailyAt": "2025-11-07T00:37:12.582Z",
            "title": "Scaling Agent Learning via Experience Synthesis",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "While reinforcement learning (RL) can empower large language model (LLM)\nagents by enabling self-improvement through interaction, its practical adoption\nremains challenging due to costly rollouts, limited task diversity, unreliable\nreward signals, and infrastructure complexity, all of which obstruct the\ncollection of scalable experience data. To address these challenges, we\nintroduce DreamGym, the first unified framework designed to synthesize diverse\nexperiences with scalability in mind to enable effective online RL training for\nautonomous agents. Rather than relying on expensive real-environment rollouts,\nDreamGym distills environment dynamics into a reasoning-based experience model\nthat derives consistent state transitions and feedback signals through\nstep-by-step reasoning, enabling scalable agent rollout collection for RL. To\nimprove the stability and quality of transitions, DreamGym leverages an\nexperience replay buffer initialized with offline real-world data and\ncontinuously enriched with fresh interactions to actively support agent\ntraining. To improve knowledge acquisition, DreamGym adaptively generates new\ntasks that challenge the current agent policy, enabling more effective online\ncurriculum learning. Experiments across diverse environments and agent\nbackbones demonstrate that DreamGym substantially improves RL training, both in\nfully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready\ntasks like WebArena, DreamGym outperforms all baselines by over 30%. And in\nRL-ready but costly settings, it matches GRPO and PPO performance using only\nsynthetic interactions. When transferring a policy trained purely on synthetic\nexperiences to real-environment RL, DreamGym yields significant additional\nperformance gains while requiring far fewer real-world interactions, providing\na scalable warm-start strategy for general-purpose RL.",
            "upvotes": 50,
            "discussionId": "690d51e5ad2597bf6c464c64",
            "ai_summary": "DreamGym is a unified framework that synthesizes diverse experiences for scalable online RL training, improving agent performance and reducing real-world interactions.",
            "ai_keywords": [
                "reinforcement learning",
                "large language model",
                "self-improvement",
                "real-environment rollouts",
                "experience model",
                "state transitions",
                "feedback signals",
                "experience replay buffer",
                "offline real-world data",
                "adaptive task generation",
                "curriculum learning",
                "sim-to-real transfer",
                "WebArena",
                "GRPO",
                "PPO",
                "synthetic interactions"
            ],
            "organization": {
                "_id": "66b54027408752ae16404b05",
                "name": "metaresearch",
                "fullname": "Meta Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
            }
        },
        "publishedAt": "2025-11-05T13:58:48.000Z",
        "title": "Scaling Agent Learning via Experience Synthesis",
        "summary": "While reinforcement learning (RL) can empower large language model (LLM)\nagents by enabling self-improvement through interaction, its practical adoption\nremains challenging due to costly rollouts, limited task diversity, unreliable\nreward signals, and infrastructure complexity, all of which obstruct the\ncollection of scalable experience data. To address these challenges, we\nintroduce DreamGym, the first unified framework designed to synthesize diverse\nexperiences with scalability in mind to enable effective online RL training for\nautonomous agents. Rather than relying on expensive real-environment rollouts,\nDreamGym distills environment dynamics into a reasoning-based experience model\nthat derives consistent state transitions and feedback signals through\nstep-by-step reasoning, enabling scalable agent rollout collection for RL. To\nimprove the stability and quality of transitions, DreamGym leverages an\nexperience replay buffer initialized with offline real-world data and\ncontinuously enriched with fresh interactions to actively support agent\ntraining. To improve knowledge acquisition, DreamGym adaptively generates new\ntasks that challenge the current agent policy, enabling more effective online\ncurriculum learning. Experiments across diverse environments and agent\nbackbones demonstrate that DreamGym substantially improves RL training, both in\nfully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready\ntasks like WebArena, DreamGym outperforms all baselines by over 30%. And in\nRL-ready but costly settings, it matches GRPO and PPO performance using only\nsynthetic interactions. When transferring a policy trained purely on synthetic\nexperiences to real-environment RL, DreamGym yields significant additional\nperformance gains while requiring far fewer real-world interactions, providing\na scalable warm-start strategy for general-purpose RL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03773.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 156
        },
        "organization": {
            "_id": "66b54027408752ae16404b05",
            "name": "metaresearch",
            "fullname": "Meta Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.04670",
            "authors": [
                {
                    "_id": "690d5b7aad2597bf6c464cb9",
                    "user": {
                        "_id": "627ccf058b4e56cfc2716425",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1652346592327-noauth.jpeg",
                        "isPro": true,
                        "fullname": "Shusheng Yang",
                        "user": "ShushengYang",
                        "type": "user"
                    },
                    "name": "Shusheng Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-07T10:28:45.693Z",
                    "hidden": false
                },
                {
                    "_id": "690d5b7aad2597bf6c464cba",
                    "name": "Jihan Yang",
                    "hidden": false
                },
                {
                    "_id": "690d5b7aad2597bf6c464cbb",
                    "name": "Pinzhi Huang",
                    "hidden": false
                },
                {
                    "_id": "690d5b7aad2597bf6c464cbc",
                    "name": "Ellis Brown",
                    "hidden": false
                },
                {
                    "_id": "690d5b7aad2597bf6c464cbd",
                    "name": "Zihao Yang",
                    "hidden": false
                },
                {
                    "_id": "690d5b7aad2597bf6c464cbe",
                    "name": "Yue Yu",
                    "hidden": false
                },
                {
                    "_id": "690d5b7aad2597bf6c464cbf",
                    "name": "Shengbang Tong",
                    "hidden": false
                },
                {
                    "_id": "690d5b7aad2597bf6c464cc0",
                    "name": "Zihan Zheng",
                    "hidden": false
                },
                {
                    "_id": "690d5b7aad2597bf6c464cc1",
                    "name": "Yifan Xu",
                    "hidden": false
                },
                {
                    "_id": "690d5b7aad2597bf6c464cc2",
                    "name": "Muhan Wang",
                    "hidden": false
                },
                {
                    "_id": "690d5b7aad2597bf6c464cc3",
                    "name": "Daohan Lu",
                    "hidden": false
                },
                {
                    "_id": "690d5b7aad2597bf6c464cc4",
                    "name": "Rob Fergus",
                    "hidden": false
                },
                {
                    "_id": "690d5b7aad2597bf6c464cc5",
                    "name": "Yann LeCun",
                    "hidden": false
                },
                {
                    "_id": "690d5b7aad2597bf6c464cc6",
                    "name": "Li Fei-Fei",
                    "hidden": false
                },
                {
                    "_id": "690d5b7aad2597bf6c464cc7",
                    "name": "Saining Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-06T18:55:17.000Z",
            "submittedOnDailyAt": "2025-11-07T00:07:55.850Z",
            "title": "Cambrian-S: Towards Spatial Supersensing in Video",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We argue that progress in true multimodal intelligence calls for a shift from\nreactive, task-driven systems and brute-force long context towards a broader\nparadigm of supersensing. We frame spatial supersensing as four stages beyond\nlinguistic-only understanding: semantic perception (naming what is seen),\nstreaming event cognition (maintaining memory across continuous experiences),\nimplicit 3D spatial cognition (inferring the world behind pixels), and\npredictive world modeling (creating internal models that filter and organize\ninformation). Current benchmarks largely test only the early stages, offering\nnarrow coverage of spatial cognition and rarely challenging models in ways that\nrequire true world modeling. To drive progress in spatial supersensing, we\npresent VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial\nrecall) and VSC (continual visual spatial counting). These tasks require\narbitrarily long video inputs yet are resistant to brute-force context\nexpansion. We then test data scaling limits by curating VSI-590K and training\nCambrian-S, achieving +30% absolute improvement on VSI-Bench without\nsacrificing general capabilities. Yet performance on VSI-SUPER remains limited,\nindicating that scale alone is insufficient for spatial supersensing. We\npropose predictive sensing as a path forward, presenting a proof-of-concept in\nwhich a self-supervised next-latent-frame predictor leverages surprise\n(prediction error) to drive memory and event segmentation. On VSI-SUPER, this\napproach substantially outperforms leading proprietary baselines, showing that\nspatial supersensing requires models that not only see but also anticipate,\nselect, and organize experience.",
            "upvotes": 17,
            "discussionId": "690d5b7aad2597bf6c464cc8",
            "projectPage": "https://cambrian-mllm.github.io/",
            "ai_summary": "Progress in multimodal intelligence requires a shift to supersensing, including semantic perception, event cognition, spatial cognition, and predictive modeling, demonstrated through VSI-SUPER benchmarks and a self-supervised predictive sensing approach.",
            "ai_keywords": [
                "supersensing",
                "semantic perception",
                "streaming event cognition",
                "implicit 3D spatial cognition",
                "predictive world modeling",
                "VSI-SUPER",
                "VSR",
                "VSC",
                "VSI-590K",
                "Cambrian-S",
                "VSI-Bench",
                "predictive sensing",
                "self-supervised next-latent-frame predictor",
                "surprise",
                "prediction error",
                "memory",
                "event segmentation"
            ]
        },
        "publishedAt": "2025-11-06T13:55:17.000Z",
        "title": "Cambrian-S: Towards Spatial Supersensing in Video",
        "summary": "We argue that progress in true multimodal intelligence calls for a shift from\nreactive, task-driven systems and brute-force long context towards a broader\nparadigm of supersensing. We frame spatial supersensing as four stages beyond\nlinguistic-only understanding: semantic perception (naming what is seen),\nstreaming event cognition (maintaining memory across continuous experiences),\nimplicit 3D spatial cognition (inferring the world behind pixels), and\npredictive world modeling (creating internal models that filter and organize\ninformation). Current benchmarks largely test only the early stages, offering\nnarrow coverage of spatial cognition and rarely challenging models in ways that\nrequire true world modeling. To drive progress in spatial supersensing, we\npresent VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial\nrecall) and VSC (continual visual spatial counting). These tasks require\narbitrarily long video inputs yet are resistant to brute-force context\nexpansion. We then test data scaling limits by curating VSI-590K and training\nCambrian-S, achieving +30% absolute improvement on VSI-Bench without\nsacrificing general capabilities. Yet performance on VSI-SUPER remains limited,\nindicating that scale alone is insufficient for spatial supersensing. We\npropose predictive sensing as a path forward, presenting a proof-of-concept in\nwhich a self-supervised next-latent-frame predictor leverages surprise\n(prediction error) to drive memory and event segmentation. On VSI-SUPER, this\napproach substantially outperforms leading proprietary baselines, showing that\nspatial supersensing requires models that not only see but also anticipate,\nselect, and organize experience.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04670.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 156
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.04307",
            "authors": [
                {
                    "_id": "690d5db4ad2597bf6c464d4b",
                    "name": "Jian Mu",
                    "hidden": false
                },
                {
                    "_id": "690d5db4ad2597bf6c464d4c",
                    "name": "Chaoyun Zhang",
                    "hidden": false
                },
                {
                    "_id": "690d5db4ad2597bf6c464d4d",
                    "name": "Chiming Ni",
                    "hidden": false
                },
                {
                    "_id": "690d5db4ad2597bf6c464d4e",
                    "name": "Lu Wang",
                    "hidden": false
                },
                {
                    "_id": "690d5db4ad2597bf6c464d4f",
                    "name": "Bo Qiao",
                    "hidden": false
                },
                {
                    "_id": "690d5db4ad2597bf6c464d50",
                    "name": "Kartik Mathur",
                    "hidden": false
                },
                {
                    "_id": "690d5db4ad2597bf6c464d51",
                    "name": "Qianhui Wu",
                    "hidden": false
                },
                {
                    "_id": "690d5db4ad2597bf6c464d52",
                    "name": "Yuhang Xie",
                    "hidden": false
                },
                {
                    "_id": "690d5db4ad2597bf6c464d53",
                    "name": "Xiaojun Ma",
                    "hidden": false
                },
                {
                    "_id": "690d5db4ad2597bf6c464d54",
                    "name": "Mengyu Zhou",
                    "hidden": false
                },
                {
                    "_id": "690d5db4ad2597bf6c464d55",
                    "name": "Si Qin",
                    "hidden": false
                },
                {
                    "_id": "690d5db4ad2597bf6c464d56",
                    "name": "Liqun Li",
                    "hidden": false
                },
                {
                    "_id": "690d5db4ad2597bf6c464d57",
                    "name": "Yu Kang",
                    "hidden": false
                },
                {
                    "_id": "690d5db4ad2597bf6c464d58",
                    "name": "Minghua Ma",
                    "hidden": false
                },
                {
                    "_id": "690d5db4ad2597bf6c464d59",
                    "name": "Qingwei Lin",
                    "hidden": false
                },
                {
                    "_id": "690d5db4ad2597bf6c464d5a",
                    "name": "Saravan Rajmohan",
                    "hidden": false
                },
                {
                    "_id": "690d5db4ad2597bf6c464d5b",
                    "name": "Dongmei Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-06T12:19:02.000Z",
            "submittedOnDailyAt": "2025-11-07T00:19:55.977Z",
            "title": "GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents",
            "submittedOnDailyBy": {
                "_id": "654dbac9938fbf1e696be8aa",
                "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
                "isPro": true,
                "fullname": "Chaoyun Zhang",
                "user": "vyokky",
                "type": "user"
            },
            "summary": "We introduce GUI-360^circ, a large-scale, comprehensive dataset and\nbenchmark suite designed to advance computer-using agents (CUAs). CUAs present\nunique challenges and is constrained by three persistent gaps: a scarcity of\nreal-world CUA tasks, the lack of automated collection-and-annotation pipelines\nfor multi-modal trajectories, and the absence of a unified benchmark that\njointly evaluates GUI grounding, screen parsing, and action prediction.\n  GUI-360^circ addresses these gaps with an LLM-augmented, largely automated\npipeline for query sourcing, environment-template construction, task\ninstantiation, batched execution, and LLM-driven quality filtering. The\nreleased corpus contains over 1.2M executed action steps across thousands of\ntrajectories in popular Windows office applications, and includes\nfull-resolution screenshots, accessibility metadata when available,\ninstantiated goals, intermediate reasoning traces, and both successful and\nfailed action trajectories. The dataset supports three canonical tasks, GUI\ngrounding, screen parsing, and action prediction, and a hybrid GUI+API action\nspace that reflects modern agent designs. Benchmarking state-of-the-art\nvision--language models on GUI-360^circ reveals substantial out-of-the-box\nshortcomings in grounding and action prediction; supervised fine-tuning and\nreinforcement learning yield significant gains but do not close the gap to\nhuman-level reliability. We release GUI-360^circ and accompanying code to\nfacilitate reproducible research and accelerate progress on robust desktop\nCUAs.\n  The full dataset has been made public on\nhttps://huggingface.co/datasets/vyokky/GUI-360.",
            "upvotes": 9,
            "discussionId": "690d5db4ad2597bf6c464d5c",
            "ai_summary": "GUI-360Â° is a large-scale dataset and benchmark suite for computer-using agents, addressing gaps in real-world tasks, automated data collection, and unified evaluation of GUI grounding, screen parsing, and action prediction.",
            "ai_keywords": [
                "LLM-augmented",
                "GUI grounding",
                "screen parsing",
                "action prediction",
                "hybrid GUI+API action space",
                "vision--language models",
                "supervised fine-tuning",
                "reinforcement learning"
            ],
            "organization": {
                "_id": "5e6485f787403103f9f1055e",
                "name": "microsoft",
                "fullname": "Microsoft",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
            }
        },
        "publishedAt": "2025-11-06T07:19:02.000Z",
        "title": "GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents",
        "summary": "We introduce GUI-360^circ, a large-scale, comprehensive dataset and\nbenchmark suite designed to advance computer-using agents (CUAs). CUAs present\nunique challenges and is constrained by three persistent gaps: a scarcity of\nreal-world CUA tasks, the lack of automated collection-and-annotation pipelines\nfor multi-modal trajectories, and the absence of a unified benchmark that\njointly evaluates GUI grounding, screen parsing, and action prediction.\n  GUI-360^circ addresses these gaps with an LLM-augmented, largely automated\npipeline for query sourcing, environment-template construction, task\ninstantiation, batched execution, and LLM-driven quality filtering. The\nreleased corpus contains over 1.2M executed action steps across thousands of\ntrajectories in popular Windows office applications, and includes\nfull-resolution screenshots, accessibility metadata when available,\ninstantiated goals, intermediate reasoning traces, and both successful and\nfailed action trajectories. The dataset supports three canonical tasks, GUI\ngrounding, screen parsing, and action prediction, and a hybrid GUI+API action\nspace that reflects modern agent designs. Benchmarking state-of-the-art\nvision--language models on GUI-360^circ reveals substantial out-of-the-box\nshortcomings in grounding and action prediction; supervised fine-tuning and\nreinforcement learning yield significant gains but do not close the gap to\nhuman-level reliability. We release GUI-360^circ and accompanying code to\nfacilitate reproducible research and accelerate progress on robust desktop\nCUAs.\n  The full dataset has been made public on\nhttps://huggingface.co/datasets/vyokky/GUI-360.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04307.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "654dbac9938fbf1e696be8aa",
            "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
            "fullname": "Chaoyun Zhang",
            "name": "vyokky",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "5e6485f787403103f9f1055e",
            "name": "microsoft",
            "fullname": "Microsoft",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.03929",
            "authors": [
                {
                    "_id": "690d5c72ad2597bf6c464cca",
                    "name": "NVIDIA",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464ccc",
                    "name": "Amala Sanjay Deshmukh",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464ccd",
                    "name": "Kateryna Chumachenko",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cce",
                    "name": "Tuomas Rintamaki",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464ccf",
                    "name": "Matthieu Le",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cd0",
                    "name": "Tyler Poon",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cd1",
                    "name": "Danial Mohseni Taheri",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cd2",
                    "name": "Ilia Karmanov",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cd3",
                    "name": "Guilin Liu",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cd4",
                    "name": "Jarno Seppanen",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cd5",
                    "name": "Guo Chen",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cd6",
                    "name": "Karan Sapra",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cd7",
                    "name": "Zhiding Yu",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cd8",
                    "name": "Adi Renduchintala",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cd9",
                    "name": "Charles Wang",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cda",
                    "name": "Peter Jin",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cdb",
                    "name": "Arushi Goel",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cdc",
                    "name": "Mike Ranzinger",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cdd",
                    "name": "Lukas Voegtle",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cde",
                    "name": "Philipp Fischer",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cdf",
                    "name": "Timo Roman",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464ce0",
                    "name": "Wei Ping",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464ce1",
                    "name": "Boxin Wang",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464ce2",
                    "name": "Zhuolin Yang",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464ce3",
                    "name": "Nayeon Lee",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464ce4",
                    "name": "Shaokun Zhang",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464ce5",
                    "name": "Fuxiao Liu",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464ce6",
                    "name": "Zhiqi Li",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464ce7",
                    "name": "Di Zhang",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464ce8",
                    "name": "Greg Heinrich",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464ce9",
                    "name": "Hongxu",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cea",
                    "name": "Yin",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464ceb",
                    "name": "Song Han",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cec",
                    "name": "Pavlo Molchanov",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464ced",
                    "name": "Parth Mannan",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cee",
                    "name": "Yao Xu",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cef",
                    "name": "Jane Polak Scowcroft",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cf0",
                    "name": "Tom Balough",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cf1",
                    "name": "Subhashree Radhakrishnan",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cf2",
                    "name": "Paris Zhang",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cf3",
                    "name": "Sean Cha",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cf4",
                    "name": "Ratnesh Kumar",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cf5",
                    "name": "Zaid Pervaiz Bhat",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cf6",
                    "name": "Jian Zhang",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cf7",
                    "name": "Darragh Hanley",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cf8",
                    "name": "Pritam Biswas",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cf9",
                    "name": "Jesse Oliver",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cfa",
                    "name": "Kevin Vasques",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cfb",
                    "name": "Roger Waleffe",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cfc",
                    "name": "Duncan Riach",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cfd",
                    "name": "Oluwatobi Olabiyi",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cfe",
                    "name": "Ameya Sunil Mahabaleshwarkar",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464cff",
                    "name": "Bilal Kartal",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d00",
                    "name": "Pritam Gundecha",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d01",
                    "name": "Khanh Nguyen",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d02",
                    "name": "Alexandre Milesi",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d03",
                    "name": "Eugene Khvedchenia",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d04",
                    "name": "Ran Zilberstein",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d05",
                    "name": "Ofri Masad",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d06",
                    "name": "Natan Bagrov",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d07",
                    "name": "Nave Assaf",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d08",
                    "name": "Tomer Asida",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d09",
                    "name": "Daniel Afrimi",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d0a",
                    "name": "Amit Zuker",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d0b",
                    "name": "Netanel Haber",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d0c",
                    "name": "Zhiyu Cheng",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d0d",
                    "name": "Jingyu",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d0e",
                    "name": "Xin",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d0f",
                    "name": "Di",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d10",
                    "name": "Wu",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d11",
                    "name": "Nik Spirin",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d12",
                    "name": "Maryam Moosaei",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d13",
                    "name": "Roman Ageev",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d14",
                    "name": "Vanshil Atul Shah",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d15",
                    "name": "Yuting Wu",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d16",
                    "name": "Daniel Korzekwa",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d17",
                    "name": "Unnikrishnan Kizhakkemadam Sreekumar",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d18",
                    "name": "Wanli Jiang",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d19",
                    "name": "Padmavathy Subramanian",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d1a",
                    "name": "Alejandra Rico",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d1b",
                    "name": "Sandip Bhaskar",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d1c",
                    "name": "Saeid Motiian",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d1d",
                    "name": "Kedi Wu",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d1e",
                    "name": "Annie Surla",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d1f",
                    "name": "Chia-Chih Chen",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d20",
                    "name": "Hayden Wolff",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d21",
                    "name": "Matthew Feinberg",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d22",
                    "name": "Melissa Corpuz",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d23",
                    "name": "Marek Wawrzos",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d24",
                    "name": "Eileen Long",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d25",
                    "name": "Aastha Jhunjhunwala",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d26",
                    "name": "Paul Hendricks",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d27",
                    "name": "Farzan Memarian",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d28",
                    "name": "Benika Hall",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d29",
                    "name": "Xin-Yu Wang",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d2a",
                    "name": "David Mosallanezhad",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d2b",
                    "name": "Soumye Singhal",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d2c",
                    "name": "Luis Vega",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d2d",
                    "name": "Katherine Cheung",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d2e",
                    "name": "Krzysztof Pawelec",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d2f",
                    "name": "Michael Evans",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d30",
                    "name": "Katherine Luna",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d31",
                    "name": "Jie Lou",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d32",
                    "name": "Erick Galinkin",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d33",
                    "name": "Akshay Hazare",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d34",
                    "name": "Kaustubh Purandare",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d35",
                    "name": "Ann Guan",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d36",
                    "name": "Anna Warno",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d37",
                    "name": "Chen Cui",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d38",
                    "name": "Yoshi Suhara",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d39",
                    "name": "Shibani Likhite",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d3a",
                    "name": "Seph Mard",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d3b",
                    "name": "Meredith Price",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d3c",
                    "name": "Laya Sleiman",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d3d",
                    "name": "Saori Kaji",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d3e",
                    "name": "Udi Karpas",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d3f",
                    "name": "Kari Briski",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d40",
                    "name": "Joey Conway",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d41",
                    "name": "Michael Lightstone",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d42",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d43",
                    "name": "Mohammad Shoeybi",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d44",
                    "name": "Mostofa Patwary",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d45",
                    "name": "Jonathen Cohen",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d46",
                    "name": "Oleksii Kuchaiev",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d47",
                    "name": "Andrew Tao",
                    "hidden": false
                },
                {
                    "_id": "690d5c72ad2597bf6c464d48",
                    "name": "Bryan Catanzaro",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-06T00:10:19.000Z",
            "submittedOnDailyAt": "2025-11-07T00:12:03.113Z",
            "title": "NVIDIA Nemotron Nano V2 VL",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce Nemotron Nano V2 VL, the latest model of the Nemotron\nvision-language series designed for strong real-world document understanding,\nlong video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers\nsignificant improvements over our previous model,\nLlama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major\nenhancements in model architecture, datasets, and training recipes. Nemotron\nNano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and\ninnovative token reduction techniques to achieve higher inference throughput in\nlong document and video scenarios. We are releasing model checkpoints in BF16,\nFP8, and FP4 formats and sharing large parts of our datasets, recipes and\ntraining code.",
            "upvotes": 8,
            "discussionId": "690d5c72ad2597bf6c464d49",
            "ai_summary": "Nemotron Nano V2 VL, a hybrid Mamba-Transformer LLM, improves document and video understanding through enhanced architecture and token reduction techniques.",
            "ai_keywords": [
                "Mamba-Transformer",
                "token reduction techniques",
                "BF16",
                "FP8",
                "FP4"
            ],
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-11-05T19:10:19.000Z",
        "title": "NVIDIA Nemotron Nano V2 VL",
        "summary": "We introduce Nemotron Nano V2 VL, the latest model of the Nemotron\nvision-language series designed for strong real-world document understanding,\nlong video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers\nsignificant improvements over our previous model,\nLlama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major\nenhancements in model architecture, datasets, and training recipes. Nemotron\nNano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and\ninnovative token reduction techniques to achieve higher inference throughput in\nlong document and video scenarios. We are releasing model checkpoints in BF16,\nFP8, and FP4 formats and sharing large parts of our datasets, recipes and\ntraining code.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03929.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 156
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.03774",
            "authors": [
                {
                    "_id": "690d8af0ad2597bf6c464f4f",
                    "user": {
                        "_id": "670f5267d1b58394145c1ca3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/t-YchgvZCbDW-plR8DZbA.png",
                        "isPro": false,
                        "fullname": "Jaden Park",
                        "user": "jpark677",
                        "type": "user"
                    },
                    "name": "Jaden Park",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-07T10:28:27.216Z",
                    "hidden": false
                },
                {
                    "_id": "690d8af0ad2597bf6c464f50",
                    "name": "Mu Cai",
                    "hidden": false
                },
                {
                    "_id": "690d8af0ad2597bf6c464f51",
                    "name": "Feng Yao",
                    "hidden": false
                },
                {
                    "_id": "690d8af0ad2597bf6c464f52",
                    "name": "Jingbo Shang",
                    "hidden": false
                },
                {
                    "_id": "690d8af0ad2597bf6c464f53",
                    "name": "Soochahn Lee",
                    "hidden": false
                },
                {
                    "_id": "690d8af0ad2597bf6c464f54",
                    "name": "Yong Jae Lee",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63b7b2c6bd2d153522821766/TbM07ib9NWtYgwcY726R8.png"
            ],
            "publishedAt": "2025-11-05T18:59:52.000Z",
            "submittedOnDailyAt": "2025-11-07T04:06:05.354Z",
            "title": "Contamination Detection for VLMs using Multi-Modal Semantic Perturbation",
            "submittedOnDailyBy": {
                "_id": "63b7b2c6bd2d153522821766",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b7b2c6bd2d153522821766/aHtga-_OUdOrg_TRrXO08.jpeg",
                "isPro": false,
                "fullname": "Mu Cai",
                "user": "mucai",
                "type": "user"
            },
            "summary": "Recent advances in Vision-Language Models (VLMs) have achieved\nstate-of-the-art performance on numerous benchmark tasks. However, the use of\ninternet-scale, often proprietary, pretraining corpora raises a critical\nconcern for both practitioners and users: inflated performance due to test-set\nleakage. While prior works have proposed mitigation strategies such as\ndecontamination of pretraining data and benchmark redesign for LLMs, the\ncomplementary direction of developing detection methods for contaminated VLMs\nremains underexplored. To address this gap, we deliberately contaminate\nopen-source VLMs on popular benchmarks and show that existing detection\napproaches either fail outright or exhibit inconsistent behavior. We then\npropose a novel simple yet effective detection method based on multi-modal\nsemantic perturbation, demonstrating that contaminated models fail to\ngeneralize under controlled perturbations. Finally, we validate our approach\nacross multiple realistic contamination strategies, confirming its robustness\nand effectiveness. The code and perturbed dataset will be released publicly.",
            "upvotes": 8,
            "discussionId": "690d8af1ad2597bf6c464f55",
            "ai_summary": "A novel detection method based on multi-modal semantic perturbation is proposed to identify contaminated Vision-Language Models, demonstrating robustness across various contamination strategies.",
            "ai_keywords": [
                "Vision-Language Models",
                "VLMs",
                "pretraining corpora",
                "test-set leakage",
                "decontamination",
                "benchmark redesign",
                "detection methods",
                "multi-modal semantic perturbation"
            ],
            "organization": {
                "_id": "61d090ec03bc10eb8e1c2970",
                "name": "uw-madison",
                "fullname": "University of Wisconsin - Madison",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"
            }
        },
        "publishedAt": "2025-11-05T13:59:52.000Z",
        "title": "Contamination Detection for VLMs using Multi-Modal Semantic Perturbation",
        "summary": "Recent advances in Vision-Language Models (VLMs) have achieved\nstate-of-the-art performance on numerous benchmark tasks. However, the use of\ninternet-scale, often proprietary, pretraining corpora raises a critical\nconcern for both practitioners and users: inflated performance due to test-set\nleakage. While prior works have proposed mitigation strategies such as\ndecontamination of pretraining data and benchmark redesign for LLMs, the\ncomplementary direction of developing detection methods for contaminated VLMs\nremains underexplored. To address this gap, we deliberately contaminate\nopen-source VLMs on popular benchmarks and show that existing detection\napproaches either fail outright or exhibit inconsistent behavior. We then\npropose a novel simple yet effective detection method based on multi-modal\nsemantic perturbation, demonstrating that contaminated models fail to\ngeneralize under controlled perturbations. Finally, we validate our approach\nacross multiple realistic contamination strategies, confirming its robustness\nand effectiveness. The code and perturbed dataset will be released publicly.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63b7b2c6bd2d153522821766/TbM07ib9NWtYgwcY726R8.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03774.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63b7b2c6bd2d153522821766",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b7b2c6bd2d153522821766/aHtga-_OUdOrg_TRrXO08.jpeg",
            "fullname": "Mu Cai",
            "name": "mucai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "61d090ec03bc10eb8e1c2970",
            "name": "uw-madison",
            "fullname": "University of Wisconsin - Madison",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.04217",
            "authors": [
                {
                    "_id": "690d5a51ad2597bf6c464c92",
                    "user": {
                        "_id": "676a62adaeb3bcd701e23c1b",
                        "avatarUrl": "/avatars/47afd9ed11fafd2c70f6fa8c570d1038.svg",
                        "isPro": false,
                        "fullname": "Hikari Otsuka",
                        "user": "h-otsuka",
                        "type": "user"
                    },
                    "name": "Hikari Otsuka",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-07T10:28:50.332Z",
                    "hidden": false
                },
                {
                    "_id": "690d5a51ad2597bf6c464c93",
                    "name": "Daiki Chijiwa",
                    "hidden": false
                },
                {
                    "_id": "690d5a51ad2597bf6c464c94",
                    "name": "Yasuyuki Okoshi",
                    "hidden": false
                },
                {
                    "_id": "690d5a51ad2597bf6c464c95",
                    "name": "Daichi Fujiki",
                    "hidden": false
                },
                {
                    "_id": "690d5a51ad2597bf6c464c96",
                    "name": "Susumu Takeuchi",
                    "hidden": false
                },
                {
                    "_id": "690d5a51ad2597bf6c464c97",
                    "name": "Masato Motomura",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-06T09:29:58.000Z",
            "submittedOnDailyAt": "2025-11-07T00:04:48.015Z",
            "title": "The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms",
            "submittedOnDailyBy": {
                "_id": "676a62adaeb3bcd701e23c1b",
                "avatarUrl": "/avatars/47afd9ed11fafd2c70f6fa8c570d1038.svg",
                "isPro": false,
                "fullname": "Hikari Otsuka",
                "user": "h-otsuka",
                "type": "user"
            },
            "summary": "The strong lottery ticket hypothesis (SLTH) conjectures that high-performing\nsubnetworks, called strong lottery tickets (SLTs), are hidden in randomly\ninitialized neural networks. Although recent theoretical studies have\nestablished the SLTH across various neural architectures, the SLTH for\ntransformer architectures still lacks theoretical understanding. In particular,\nthe current theory of the SLTH does not yet account for the multi-head\nattention (MHA) mechanism, a core component of transformers. To address this\ngap, we introduce a theoretical analysis of the existence of SLTs within MHAs.\nWe prove that, if a randomly initialized MHA of H heads and input dimension\nd has the hidden dimension O(dlog(Hd^{3/2})) for the key and value, it\ncontains an SLT that approximates an arbitrary MHA with the same input\ndimension with high probability. Furthermore, by leveraging this theory for\nMHAs, we extend the SLTH to transformers without normalization layers. We\nempirically validate our theoretical findings, demonstrating that the\napproximation error between the SLT within a source model (MHA and transformer)\nand an approximate target counterpart decreases exponentially by increasing the\nhidden dimension of the source model.",
            "upvotes": 6,
            "discussionId": "690d5a51ad2597bf6c464c98",
            "ai_summary": "Theoretical analysis proves the existence of strong lottery tickets within multi-head attention mechanisms and extends the strong lottery ticket hypothesis to transformers without normalization layers.",
            "ai_keywords": [
                "strong lottery ticket hypothesis",
                "strong lottery tickets",
                "multi-head attention",
                "transformers",
                "normalization layers"
            ]
        },
        "publishedAt": "2025-11-06T04:29:58.000Z",
        "title": "The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms",
        "summary": "The strong lottery ticket hypothesis (SLTH) conjectures that high-performing\nsubnetworks, called strong lottery tickets (SLTs), are hidden in randomly\ninitialized neural networks. Although recent theoretical studies have\nestablished the SLTH across various neural architectures, the SLTH for\ntransformer architectures still lacks theoretical understanding. In particular,\nthe current theory of the SLTH does not yet account for the multi-head\nattention (MHA) mechanism, a core component of transformers. To address this\ngap, we introduce a theoretical analysis of the existence of SLTs within MHAs.\nWe prove that, if a randomly initialized MHA of H heads and input dimension\nd has the hidden dimension O(dlog(Hd^{3/2})) for the key and value, it\ncontains an SLT that approximates an arbitrary MHA with the same input\ndimension with high probability. Furthermore, by leveraging this theory for\nMHAs, we extend the SLTH to transformers without normalization layers. We\nempirically validate our theoretical findings, demonstrating that the\napproximation error between the SLT within a source model (MHA and transformer)\nand an approximate target counterpart decreases exponentially by increasing the\nhidden dimension of the source model.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04217.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "676a62adaeb3bcd701e23c1b",
            "avatarUrl": "/avatars/47afd9ed11fafd2c70f6fa8c570d1038.svg",
            "fullname": "Hikari Otsuka",
            "name": "h-otsuka",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.04655",
            "authors": [
                {
                    "_id": "690d627fad2597bf6c464d77",
                    "name": "Ellis Brown",
                    "hidden": false
                },
                {
                    "_id": "690d627fad2597bf6c464d78",
                    "name": "Jihan Yang",
                    "hidden": false
                },
                {
                    "_id": "690d627fad2597bf6c464d79",
                    "name": "Shusheng Yang",
                    "hidden": false
                },
                {
                    "_id": "690d627fad2597bf6c464d7a",
                    "name": "Rob Fergus",
                    "hidden": false
                },
                {
                    "_id": "690d627fad2597bf6c464d7b",
                    "name": "Saining Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-06T18:43:21.000Z",
            "submittedOnDailyAt": "2025-11-07T00:38:49.237Z",
            "title": "Benchmark Designers Should \"Train on the Test Set\" to Expose Exploitable\n  Non-Visual Shortcuts",
            "submittedOnDailyBy": {
                "_id": "6304baf041387c7f1177a5d2",
                "avatarUrl": "/avatars/795c63f2394080eec78ca7981d4a1f78.svg",
                "isPro": true,
                "fullname": "Jihan Yang",
                "user": "jihanyang",
                "type": "user"
            },
            "summary": "Robust benchmarks are crucial for evaluating Multimodal Large Language Models\n(MLLMs). Yet we find that models can ace many multimodal benchmarks without\nstrong visual understanding, instead exploiting biases, linguistic priors, and\nsuperficial patterns. This is especially problematic for vision-centric\nbenchmarks that are meant to require visual inputs. We adopt a diagnostic\nprinciple for benchmark design: if a benchmark can be gamed, it will be.\nDesigners should therefore try to ``game'' their own benchmarks first, using\ndiagnostic and debiasing procedures to systematically identify and mitigate\nnon-visual biases. Effective diagnosis requires directly ``training on the test\nset'' -- probing the released test set for its intrinsic, exploitable patterns.\n  We operationalize this standard with two components. First, we diagnose\nbenchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.\nOur primary diagnostic tool involves fine-tuning a powerful Large Language\nModel via k-fold cross-validation on exclusively the non-visual, textual\ninputs of the test set to reveal shortcut performance and assign each sample a\nbias score s(x). We complement this with a lightweight Random Forest-based\ndiagnostic operating on hand-crafted features for fast, interpretable auditing.\nSecond, we debias benchmarks by filtering high-bias samples using an\n``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four\nbenchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive\nnon-visual biases. As a case study, we apply our full framework to create\nVSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider\nvision-blind performance gap than the original.",
            "upvotes": 5,
            "discussionId": "690d6280ad2597bf6c464d7c",
            "projectPage": "https://cambrian-mllm.github.io/",
            "ai_summary": "A framework for diagnosing and debiasing multimodal benchmarks reveals and mitigates non-visual biases, improving the robustness of Multimodal Large Language Models.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "vision-centric benchmarks",
                "diagnostic principle",
                "test-set stress-test",
                "TsT",
                "fine-tuning",
                "k-fold cross-validation",
                "bias score",
                "random forest",
                "iterative bias pruning",
                "IBP",
                "VSI-Bench",
                "CV-Bench",
                "MMMU",
                "VideoMME",
                "VSI-Bench-Debiased"
            ],
            "organization": {
                "_id": "662741612ada5b77e310d171",
                "name": "nyu-visionx",
                "fullname": "NYU VisionX",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"
            }
        },
        "publishedAt": "2025-11-06T13:43:21.000Z",
        "title": "Benchmark Designers Should \"Train on the Test Set\" to Expose Exploitable\n  Non-Visual Shortcuts",
        "summary": "Robust benchmarks are crucial for evaluating Multimodal Large Language Models\n(MLLMs). Yet we find that models can ace many multimodal benchmarks without\nstrong visual understanding, instead exploiting biases, linguistic priors, and\nsuperficial patterns. This is especially problematic for vision-centric\nbenchmarks that are meant to require visual inputs. We adopt a diagnostic\nprinciple for benchmark design: if a benchmark can be gamed, it will be.\nDesigners should therefore try to ``game'' their own benchmarks first, using\ndiagnostic and debiasing procedures to systematically identify and mitigate\nnon-visual biases. Effective diagnosis requires directly ``training on the test\nset'' -- probing the released test set for its intrinsic, exploitable patterns.\n  We operationalize this standard with two components. First, we diagnose\nbenchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.\nOur primary diagnostic tool involves fine-tuning a powerful Large Language\nModel via k-fold cross-validation on exclusively the non-visual, textual\ninputs of the test set to reveal shortcut performance and assign each sample a\nbias score s(x). We complement this with a lightweight Random Forest-based\ndiagnostic operating on hand-crafted features for fast, interpretable auditing.\nSecond, we debias benchmarks by filtering high-bias samples using an\n``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four\nbenchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive\nnon-visual biases. As a case study, we apply our full framework to create\nVSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider\nvision-blind performance gap than the original.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04655.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6304baf041387c7f1177a5d2",
            "avatarUrl": "/avatars/795c63f2394080eec78ca7981d4a1f78.svg",
            "fullname": "Jihan Yang",
            "name": "jihanyang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "662741612ada5b77e310d171",
            "name": "nyu-visionx",
            "fullname": "NYU VisionX",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.03996",
            "authors": [
                {
                    "_id": "690d62f9ad2597bf6c464d7e",
                    "name": "Yushi Wang",
                    "hidden": false
                },
                {
                    "_id": "690d62f9ad2597bf6c464d7f",
                    "name": "Changsheng Luo",
                    "hidden": false
                },
                {
                    "_id": "690d62f9ad2597bf6c464d80",
                    "name": "Penghui Chen",
                    "hidden": false
                },
                {
                    "_id": "690d62f9ad2597bf6c464d81",
                    "name": "Jianran Liu",
                    "hidden": false
                },
                {
                    "_id": "690d62f9ad2597bf6c464d82",
                    "name": "Weijian Sun",
                    "hidden": false
                },
                {
                    "_id": "690d62f9ad2597bf6c464d83",
                    "name": "Tong Guo",
                    "hidden": false
                },
                {
                    "_id": "690d62f9ad2597bf6c464d84",
                    "name": "Kechang Yang",
                    "hidden": false
                },
                {
                    "_id": "690d62f9ad2597bf6c464d85",
                    "name": "Biao Hu",
                    "hidden": false
                },
                {
                    "_id": "690d62f9ad2597bf6c464d86",
                    "name": "Yangang Zhang",
                    "hidden": false
                },
                {
                    "_id": "690d62f9ad2597bf6c464d87",
                    "name": "Mingguo Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-06T02:40:48.000Z",
            "submittedOnDailyAt": "2025-11-07T00:40:00.286Z",
            "title": "Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Humanoid soccer poses a representative challenge for embodied intelligence,\nrequiring robots to operate within a tightly coupled perception-action loop.\nHowever, existing systems typically rely on decoupled modules, resulting in\ndelayed responses and incoherent behaviors in dynamic environments, while\nreal-world perceptual limitations further exacerbate these issues. In this\nwork, we present a unified reinforcement learning-based controller that enables\nhumanoid robots to acquire reactive soccer skills through the direct\nintegration of visual perception and motion control. Our approach extends\nAdversarial Motion Priors to perceptual settings in real-world dynamic\nenvironments, bridging motion imitation and visually grounded dynamic control.\nWe introduce an encoder-decoder architecture combined with a virtual perception\nsystem that models real-world visual characteristics, allowing the policy to\nrecover privileged states from imperfect observations and establish active\ncoordination between perception and action. The resulting controller\ndemonstrates strong reactivity, consistently executing coherent and robust\nsoccer behaviors across various scenarios, including real RoboCup matches.",
            "upvotes": 3,
            "discussionId": "690d62f9ad2597bf6c464d88",
            "projectPage": "https://humanoid-kick.github.io/",
            "ai_summary": "A unified reinforcement learning controller integrates visual perception and motion control for humanoid robots in soccer, using Adversarial Motion Priors and an encoder-decoder architecture to achieve reactive and coherent behaviors.",
            "ai_keywords": [
                "reinforcement learning",
                "Adversarial Motion Priors",
                "encoder-decoder architecture",
                "virtual perception system",
                "privileged states",
                "real-world visual characteristics",
                "motion imitation",
                "visually grounded dynamic control"
            ],
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-11-05T21:40:48.000Z",
        "title": "Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots",
        "summary": "Humanoid soccer poses a representative challenge for embodied intelligence,\nrequiring robots to operate within a tightly coupled perception-action loop.\nHowever, existing systems typically rely on decoupled modules, resulting in\ndelayed responses and incoherent behaviors in dynamic environments, while\nreal-world perceptual limitations further exacerbate these issues. In this\nwork, we present a unified reinforcement learning-based controller that enables\nhumanoid robots to acquire reactive soccer skills through the direct\nintegration of visual perception and motion control. Our approach extends\nAdversarial Motion Priors to perceptual settings in real-world dynamic\nenvironments, bridging motion imitation and visually grounded dynamic control.\nWe introduce an encoder-decoder architecture combined with a virtual perception\nsystem that models real-world visual characteristics, allowing the policy to\nrecover privileged states from imperfect observations and establish active\ncoordination between perception and action. The resulting controller\ndemonstrates strong reactivity, consistently executing coherent and robust\nsoccer behaviors across various scenarios, including real RoboCup matches.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03996.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 156
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.03295",
            "authors": [
                {
                    "_id": "690dac66ad2597bf6c464f5c",
                    "name": "Mauro Cettolo",
                    "hidden": false
                },
                {
                    "_id": "690dac66ad2597bf6c464f5d",
                    "name": "Marco Gaido",
                    "hidden": false
                },
                {
                    "_id": "690dac66ad2597bf6c464f5e",
                    "name": "Matteo Negri",
                    "hidden": false
                },
                {
                    "_id": "690dac66ad2597bf6c464f5f",
                    "user": {
                        "_id": "66309b3833ccd9e68c5d5171",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg",
                        "isPro": false,
                        "fullname": "Sara Papi",
                        "user": "spapi",
                        "type": "user"
                    },
                    "name": "Sara Papi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-07T10:28:25.127Z",
                    "hidden": false
                },
                {
                    "_id": "690dac66ad2597bf6c464f60",
                    "name": "Luisa Bentivogli",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-05T08:49:22.000Z",
            "submittedOnDailyAt": "2025-11-07T05:59:42.593Z",
            "title": "How to Evaluate Speech Translation with Source-Aware Neural MT Metrics",
            "submittedOnDailyBy": {
                "_id": "66309b3833ccd9e68c5d5171",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg",
                "isPro": false,
                "fullname": "Sara Papi",
                "user": "spapi",
                "type": "user"
            },
            "summary": "Automatic evaluation of speech-to-text translation (ST) systems is typically\nperformed by comparing translation hypotheses with one or more reference\ntranslations. While effective to some extent, this approach inherits the\nlimitation of reference-based evaluation that ignores valuable information from\nthe source input. In machine translation (MT), recent progress has shown that\nneural metrics incorporating the source text achieve stronger correlation with\nhuman judgments. Extending this idea to ST, however, is not trivial because the\nsource is audio rather than text, and reliable transcripts or alignments\nbetween source and references are often unavailable. In this work, we conduct\nthe first systematic study of source-aware metrics for ST, with a particular\nfocus on real-world operating conditions where source transcripts are not\navailable. We explore two complementary strategies for generating textual\nproxies of the input audio, automatic speech recognition (ASR) transcripts, and\nback-translations of the reference translation, and introduce a novel two-step\ncross-lingual re-segmentation algorithm to address the alignment mismatch\nbetween synthetic sources and reference translations. Our experiments, carried\nout on two ST benchmarks covering 79 language pairs and six ST systems with\ndiverse architectures and performance levels, show that ASR transcripts\nconstitute a more reliable synthetic source than back-translations when word\nerror rate is below 20%, while back-translations always represent a\ncomputationally cheaper but still effective alternative. Furthermore, our\ncross-lingual re-segmentation algorithm enables robust use of source-aware MT\nmetrics in ST evaluation, paving the way toward more accurate and principled\nevaluation methodologies for speech translation.",
            "upvotes": 3,
            "discussionId": "690dac66ad2597bf6c464f61",
            "ai_summary": "Source-aware metrics using ASR transcripts and back-translations improve speech-to-text evaluation by addressing alignment issues and incorporating source information.",
            "ai_keywords": [
                "automatic speech recognition",
                "ASR transcripts",
                "back-translations",
                "cross-lingual re-segmentation",
                "source-aware metrics",
                "speech-to-text",
                "machine translation",
                "word error rate"
            ]
        },
        "publishedAt": "2025-11-05T03:49:22.000Z",
        "title": "How to Evaluate Speech Translation with Source-Aware Neural MT Metrics",
        "summary": "Automatic evaluation of speech-to-text translation (ST) systems is typically\nperformed by comparing translation hypotheses with one or more reference\ntranslations. While effective to some extent, this approach inherits the\nlimitation of reference-based evaluation that ignores valuable information from\nthe source input. In machine translation (MT), recent progress has shown that\nneural metrics incorporating the source text achieve stronger correlation with\nhuman judgments. Extending this idea to ST, however, is not trivial because the\nsource is audio rather than text, and reliable transcripts or alignments\nbetween source and references are often unavailable. In this work, we conduct\nthe first systematic study of source-aware metrics for ST, with a particular\nfocus on real-world operating conditions where source transcripts are not\navailable. We explore two complementary strategies for generating textual\nproxies of the input audio, automatic speech recognition (ASR) transcripts, and\nback-translations of the reference translation, and introduce a novel two-step\ncross-lingual re-segmentation algorithm to address the alignment mismatch\nbetween synthetic sources and reference translations. Our experiments, carried\nout on two ST benchmarks covering 79 language pairs and six ST systems with\ndiverse architectures and performance levels, show that ASR transcripts\nconstitute a more reliable synthetic source than back-translations when word\nerror rate is below 20%, while back-translations always represent a\ncomputationally cheaper but still effective alternative. Furthermore, our\ncross-lingual re-segmentation algorithm enables robust use of source-aware MT\nmetrics in ST evaluation, paving the way toward more accurate and principled\nevaluation methodologies for speech translation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03295.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66309b3833ccd9e68c5d5171",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg",
            "fullname": "Sara Papi",
            "name": "spapi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.04668",
            "authors": [
                {
                    "_id": "690df354ad2597bf6c465057",
                    "name": "Ellis Brown",
                    "hidden": false
                },
                {
                    "_id": "690df354ad2597bf6c465058",
                    "name": "Arijit Ray",
                    "hidden": false
                },
                {
                    "_id": "690df354ad2597bf6c465059",
                    "name": "Ranjay Krishna",
                    "hidden": false
                },
                {
                    "_id": "690df354ad2597bf6c46505a",
                    "name": "Ross Girshick",
                    "hidden": false
                },
                {
                    "_id": "690df354ad2597bf6c46505b",
                    "name": "Rob Fergus",
                    "hidden": false
                },
                {
                    "_id": "690df354ad2597bf6c46505c",
                    "name": "Saining Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-06T18:53:31.000Z",
            "submittedOnDailyAt": "2025-11-07T10:57:23.376Z",
            "title": "SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding",
            "submittedOnDailyBy": {
                "_id": "626dc5105f7327906f0b2a4e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626dc5105f7327906f0b2a4e/QCSzuwYqsv8ozRnusVb-F.jpeg",
                "isPro": true,
                "fullname": "Ellis Brown",
                "user": "ellisbrown",
                "type": "user"
            },
            "summary": "Despite impressive high-level video comprehension, multimodal language models\nstruggle with spatial reasoning across time and space. While current spatial\ntraining approaches rely on real-world video data, obtaining diverse footage\nwith precise spatial annotations remains a bottleneck. To alleviate this\nbottleneck, we present SIMS-V -- a systematic data-generation framework that\nleverages the privileged information of 3D simulators to create spatially-rich\nvideo training data for multimodal language models. Using this framework, we\ninvestigate which properties of simulated data drive effective real-world\ntransfer through systematic ablations of question types, mixes, and scales. We\nidentify a minimal set of three question categories (metric measurement,\nperspective-dependent reasoning, and temporal tracking) that prove most\neffective for developing transferable spatial intelligence, outperforming\ncomprehensive coverage despite using fewer question types. These insights\nenable highly efficient training: our 7B-parameter video LLM fine-tuned on just\n25K simulated examples outperforms the larger 72B baseline and achieves\ncompetitive performance with proprietary models on rigorous real-world spatial\nreasoning benchmarks. Our approach demonstrates robust generalization,\nmaintaining performance on general video understanding while showing\nsubstantial improvements on embodied and real-world spatial tasks.",
            "upvotes": 2,
            "discussionId": "690df354ad2597bf6c46505d",
            "ai_summary": "A data-generation framework using 3D simulators improves spatial reasoning in multimodal language models with efficient training on simulated data.",
            "ai_keywords": [
                "multimodal language models",
                "spatial reasoning",
                "3D simulators",
                "video training data",
                "systematic ablations",
                "question types",
                "metric measurement",
                "perspective-dependent reasoning",
                "temporal tracking",
                "video LLM",
                "fine-tuning",
                "real-world spatial reasoning benchmarks",
                "embodied tasks"
            ],
            "organization": {
                "_id": "662741612ada5b77e310d171",
                "name": "nyu-visionx",
                "fullname": "NYU VisionX",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"
            }
        },
        "publishedAt": "2025-11-06T13:53:31.000Z",
        "title": "SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding",
        "summary": "Despite impressive high-level video comprehension, multimodal language models\nstruggle with spatial reasoning across time and space. While current spatial\ntraining approaches rely on real-world video data, obtaining diverse footage\nwith precise spatial annotations remains a bottleneck. To alleviate this\nbottleneck, we present SIMS-V -- a systematic data-generation framework that\nleverages the privileged information of 3D simulators to create spatially-rich\nvideo training data for multimodal language models. Using this framework, we\ninvestigate which properties of simulated data drive effective real-world\ntransfer through systematic ablations of question types, mixes, and scales. We\nidentify a minimal set of three question categories (metric measurement,\nperspective-dependent reasoning, and temporal tracking) that prove most\neffective for developing transferable spatial intelligence, outperforming\ncomprehensive coverage despite using fewer question types. These insights\nenable highly efficient training: our 7B-parameter video LLM fine-tuned on just\n25K simulated examples outperforms the larger 72B baseline and achieves\ncompetitive performance with proprietary models on rigorous real-world spatial\nreasoning benchmarks. Our approach demonstrates robust generalization,\nmaintaining performance on general video understanding while showing\nsubstantial improvements on embodied and real-world spatial tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04668.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "626dc5105f7327906f0b2a4e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626dc5105f7327906f0b2a4e/QCSzuwYqsv8ozRnusVb-F.jpeg",
            "fullname": "Ellis Brown",
            "name": "ellisbrown",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "organization": {
            "_id": "662741612ada5b77e310d171",
            "name": "nyu-visionx",
            "fullname": "NYU VisionX",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2510.27656",
            "authors": [
                {
                    "_id": "6908364f812eca10f9cc5edc",
                    "name": "Nandor Licker",
                    "hidden": false
                },
                {
                    "_id": "6908364f812eca10f9cc5edd",
                    "name": "Kevin Hu",
                    "hidden": false
                },
                {
                    "_id": "6908364f812eca10f9cc5ede",
                    "name": "Vladimir Zaytsev",
                    "hidden": false
                },
                {
                    "_id": "6908364f812eca10f9cc5edf",
                    "name": "Lequn Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-31T17:28:22.000Z",
            "submittedOnDailyAt": "2025-11-07T07:25:13.747Z",
            "title": "RDMA Point-to-Point Communication for LLM Systems",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": true,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "Emerging Large Language Model (LLM) system patterns, such as disaggregated\ninference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement\nfine-tuning, require flexible point-to-point communication beyond simple\ncollectives. Existing implementations are locked to specific Network Interface\nControllers (NICs), hindering integration into inference engines and\nportability across hardware providers. We present TransferEngine, which bridges\nthe functionality of common NICs to expose a uniform interface. TransferEngine\nexposes one-sided WriteImm operations with a ImmCounter primitive for\ncompletion notification, without ordering assumptions of network transport,\ntransparently managing multiple NICs per GPU. We demonstrate peak throughput of\n400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We\nshowcase TransferEngine through three production systems: (1) KvCache transfer\nfor disaggregated inference with dynamic scaling, (2) RL weight updates\nachieving 1.3 seconds for trillion-parameter models, and (3) MoE\ndispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,\nwith the first viable latencies on EFA. We demonstrate that our portable\npoint-to-point communication complements collectives while avoiding lock-in.",
            "upvotes": 2,
            "discussionId": "6908364f812eca10f9cc5ee0",
            "ai_summary": "TransferEngine provides a uniform interface for flexible point-to-point communication in large language models, supporting disaggregated inference, reinforcement learning, and Mixture-of-Experts routing across different hardware.",
            "ai_keywords": [
                "disaggregated inference",
                "Mixture-of-Experts (MoE)",
                "asynchronous reinforcement fine-tuning",
                "Network Interface Controllers (NICs)",
                "TransferEngine",
                "WriteImm operations",
                "ImmCounter",
                "peak throughput",
                "NVIDIA ConnectX-7",
                "AWS Elastic Fabric Adapter (EFA)",
                "KvCache transfer",
                "RL weight updates",
                "MoE dispatch/combine",
                "DeepEP decode latency"
            ],
            "organization": {
                "_id": "66c4e0d3e16a16b7a489a87b",
                "name": "perplexity-ai",
                "fullname": "Perplexity",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b89bf66b5ee8c38859cbd6/l_27fD52uFMZUXdFdY9fR.png"
            }
        },
        "publishedAt": "2025-10-31T13:28:22.000Z",
        "title": "RDMA Point-to-Point Communication for LLM Systems",
        "summary": "Emerging Large Language Model (LLM) system patterns, such as disaggregated\ninference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement\nfine-tuning, require flexible point-to-point communication beyond simple\ncollectives. Existing implementations are locked to specific Network Interface\nControllers (NICs), hindering integration into inference engines and\nportability across hardware providers. We present TransferEngine, which bridges\nthe functionality of common NICs to expose a uniform interface. TransferEngine\nexposes one-sided WriteImm operations with a ImmCounter primitive for\ncompletion notification, without ordering assumptions of network transport,\ntransparently managing multiple NICs per GPU. We demonstrate peak throughput of\n400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We\nshowcase TransferEngine through three production systems: (1) KvCache transfer\nfor disaggregated inference with dynamic scaling, (2) RL weight updates\nachieving 1.3 seconds for trillion-parameter models, and (3) MoE\ndispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,\nwith the first viable latencies on EFA. We demonstrate that our portable\npoint-to-point communication complements collectives while avoiding lock-in.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27656.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1031
        },
        "organization": {
            "_id": "66c4e0d3e16a16b7a489a87b",
            "name": "perplexity-ai",
            "fullname": "Perplexity",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b89bf66b5ee8c38859cbd6/l_27fD52uFMZUXdFdY9fR.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.02280",
            "authors": [
                {
                    "_id": "690c18d960494e4fa7675677",
                    "name": "Fangxun Shu",
                    "hidden": false
                },
                {
                    "_id": "690c18d960494e4fa7675678",
                    "name": "Yongjie Ye",
                    "hidden": false
                },
                {
                    "_id": "690c18d960494e4fa7675679",
                    "name": "Yue Liao",
                    "hidden": false
                },
                {
                    "_id": "690c18d960494e4fa767567a",
                    "name": "Zijian Kang",
                    "hidden": false
                },
                {
                    "_id": "690c18d960494e4fa767567b",
                    "name": "Weijie Yin",
                    "hidden": false
                },
                {
                    "_id": "690c18d960494e4fa767567c",
                    "name": "Jiacong Wang",
                    "hidden": false
                },
                {
                    "_id": "690c18d960494e4fa767567d",
                    "name": "Xiao Liang",
                    "hidden": false
                },
                {
                    "_id": "690c18d960494e4fa767567e",
                    "name": "Shuicheng Yan",
                    "hidden": false
                },
                {
                    "_id": "690c18d960494e4fa767567f",
                    "name": "Chao Feng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-04T05:34:06.000Z",
            "submittedOnDailyAt": "2025-11-07T09:54:20.529Z",
            "title": "SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL\n  Tuning",
            "submittedOnDailyBy": {
                "_id": "6491b1b2c1741666238f8a0f",
                "avatarUrl": "/avatars/9246c9ef06d80bd8628426375c95d4be.svg",
                "isPro": false,
                "fullname": "JackShu",
                "user": "Shuhuhuhu",
                "type": "user"
            },
            "summary": "We introduce SAIL-RL, a reinforcement learning (RL) post-training framework\nthat enhances the reasoning capabilities of multimodal large language models\n(MLLMs) by teaching them when and how to think. Existing approaches are limited\nby outcome-only supervision, which rewards correct answers without ensuring\nsound reasoning, and by uniform thinking strategies, which often lead to\noverthinking on simple tasks and underthinking on complex ones. SAIL-RL\naddresses these challenges with a dual reward system: the Thinking Reward,\nwhich evaluates reasoning quality through factual grounding, logical coherence,\nand answer consistency, and the Judging Reward, which adaptively determines\nwhether deep reasoning or direct answering is appropriate. Experiments on the\nstate-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal\nunderstanding benchmarks at both 4B and 8B scales, achieving competitive\nperformance against commercial closed-source models such as GPT-4o, and\nsubstantially reduces hallucinations, establishing it as a principled framework\nfor building more reliable and adaptive MLLMs. The code will be available at\nhttps://github.com/BytedanceDouyinContent/SAIL-RL.",
            "upvotes": 0,
            "discussionId": "690c18da60494e4fa7675680",
            "ai_summary": "SAIL-RL enhances multimodal large language models' reasoning capabilities through a dual reward system, improving benchmarks and reducing hallucinations.",
            "ai_keywords": [
                "reinforcement learning",
                "multimodal large language models",
                "outcome-only supervision",
                "Thinking Reward",
                "Judging Reward",
                "factual grounding",
                "logical coherence",
                "answer consistency",
                "deep reasoning",
                "direct answering",
                "hallucinations"
            ],
            "organization": {
                "_id": "675ab4ac340e393548c43609",
                "name": "BytedanceDouyinContent",
                "fullname": "BytedanceDouyinContent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/675a9a9972aadf30cae14112/FRutS5CvFUHGdun8uybJ5.jpeg"
            }
        },
        "publishedAt": "2025-11-04T00:34:06.000Z",
        "title": "SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL\n  Tuning",
        "summary": "We introduce SAIL-RL, a reinforcement learning (RL) post-training framework\nthat enhances the reasoning capabilities of multimodal large language models\n(MLLMs) by teaching them when and how to think. Existing approaches are limited\nby outcome-only supervision, which rewards correct answers without ensuring\nsound reasoning, and by uniform thinking strategies, which often lead to\noverthinking on simple tasks and underthinking on complex ones. SAIL-RL\naddresses these challenges with a dual reward system: the Thinking Reward,\nwhich evaluates reasoning quality through factual grounding, logical coherence,\nand answer consistency, and the Judging Reward, which adaptively determines\nwhether deep reasoning or direct answering is appropriate. Experiments on the\nstate-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal\nunderstanding benchmarks at both 4B and 8B scales, achieving competitive\nperformance against commercial closed-source models such as GPT-4o, and\nsubstantially reduces hallucinations, establishing it as a principled framework\nfor building more reliable and adaptive MLLMs. The code will be available at\nhttps://github.com/BytedanceDouyinContent/SAIL-RL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02280.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6491b1b2c1741666238f8a0f",
            "avatarUrl": "/avatars/9246c9ef06d80bd8628426375c95d4be.svg",
            "fullname": "JackShu",
            "name": "Shuhuhuhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "675ab4ac340e393548c43609",
            "name": "BytedanceDouyinContent",
            "fullname": "BytedanceDouyinContent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/675a9a9972aadf30cae14112/FRutS5CvFUHGdun8uybJ5.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.00956",
            "authors": [
                {
                    "_id": "69097488812eca10f9cc61bb",
                    "user": {
                        "_id": "670fb4273231cb3ef3b970d2",
                        "avatarUrl": "/avatars/14bb9a7bd887e358a3a447ea2847e07a.svg",
                        "isPro": false,
                        "fullname": "liuzhuozheng Li",
                        "user": "Jyugatsu",
                        "type": "user"
                    },
                    "name": "Liuzhuozheng Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T20:26:36.730Z",
                    "hidden": false
                },
                {
                    "_id": "69097488812eca10f9cc61bc",
                    "name": "Yue Gong",
                    "hidden": false
                },
                {
                    "_id": "69097488812eca10f9cc61bd",
                    "name": "Shanyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "69097488812eca10f9cc61be",
                    "name": "Bo Cheng",
                    "hidden": false
                },
                {
                    "_id": "69097488812eca10f9cc61bf",
                    "name": "Yuhang Ma",
                    "hidden": false
                },
                {
                    "_id": "69097488812eca10f9cc61c0",
                    "name": "Liebucha Wu",
                    "hidden": false
                },
                {
                    "_id": "69097488812eca10f9cc61c1",
                    "name": "Dengyang Jiang",
                    "hidden": false
                },
                {
                    "_id": "69097488812eca10f9cc61c2",
                    "name": "Zanyi Wang",
                    "hidden": false
                },
                {
                    "_id": "69097488812eca10f9cc61c3",
                    "name": "Dawei Leng",
                    "hidden": false
                },
                {
                    "_id": "69097488812eca10f9cc61c4",
                    "name": "Yuhui Yin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-02T14:32:31.000Z",
            "submittedOnDailyAt": "2025-11-07T16:18:26.952Z",
            "title": "EVTAR: End-to-End Try on with Additional Unpaired Visual Reference",
            "submittedOnDailyBy": {
                "_id": "638de818ded2c11511f5db22",
                "avatarUrl": "/avatars/5795283123829a42e0687df3b4f5689c.svg",
                "isPro": false,
                "fullname": "lsy",
                "user": "liushanyuan18",
                "type": "user"
            },
            "summary": "We propose EVTAR, an End-to-End Virtual Try-on model with Additional\nReference, that directly fits the target garment onto the person image while\nincorporating reference images to enhance try-on accuracy. Most existing\nvirtual try-on approaches rely on complex inputs such as agnostic person\nimages, human pose, densepose, or body keypoints, making them labor-intensive\nand impractical for real-world applications. In contrast, EVTAR adopts a\ntwo-stage training strategy, enabling simple inference with only the source\nimage and the target garment inputs. Our model generates try-on results without\nmasks, densepose, or segmentation maps. Moreover, EVTAR leverages additional\nreference images of different individuals wearing the same clothes to preserve\ngarment texture and fine-grained details better. This mechanism is analogous to\nhow humans consider reference models when choosing outfits, thereby simulating\na more realistic and high-quality dressing effect. We enrich the training data\nwith supplementary references and unpaired person images to support these\ncapabilities. We evaluate EVTAR on two widely used benchmarks and diverse\ntasks, and the results consistently validate the effectiveness of our approach.",
            "upvotes": 0,
            "discussionId": "69097488812eca10f9cc61c5",
            "ai_summary": "EVTAR is an end-to-end virtual try-on model that enhances accuracy by using reference images, simplifying the inference process and improving garment texture and detail preservation.",
            "ai_keywords": [
                "End-to-End Virtual Try-on",
                "agnostic person images",
                "human pose",
                "densepose",
                "body keypoints",
                "two-stage training strategy",
                "garment texture",
                "fine-grained details",
                "supplementary references",
                "unpaired person images"
            ],
            "organization": {
                "_id": "6606990280543d0b74d38438",
                "name": "qihoo360",
                "fullname": "åäº¬å¥èç§ææéå¬å¸",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660697054c0882ce77fe3160/fVsqY-Q0DDrme0zyN_zLa.png"
            }
        },
        "publishedAt": "2025-11-02T09:32:31.000Z",
        "title": "EVTAR: End-to-End Try on with Additional Unpaired Visual Reference",
        "summary": "We propose EVTAR, an End-to-End Virtual Try-on model with Additional\nReference, that directly fits the target garment onto the person image while\nincorporating reference images to enhance try-on accuracy. Most existing\nvirtual try-on approaches rely on complex inputs such as agnostic person\nimages, human pose, densepose, or body keypoints, making them labor-intensive\nand impractical for real-world applications. In contrast, EVTAR adopts a\ntwo-stage training strategy, enabling simple inference with only the source\nimage and the target garment inputs. Our model generates try-on results without\nmasks, densepose, or segmentation maps. Moreover, EVTAR leverages additional\nreference images of different individuals wearing the same clothes to preserve\ngarment texture and fine-grained details better. This mechanism is analogous to\nhow humans consider reference models when choosing outfits, thereby simulating\na more realistic and high-quality dressing effect. We enrich the training data\nwith supplementary references and unpaired person images to support these\ncapabilities. We evaluate EVTAR on two widely used benchmarks and diverse\ntasks, and the results consistently validate the effectiveness of our approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.00956.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638de818ded2c11511f5db22",
            "avatarUrl": "/avatars/5795283123829a42e0687df3b4f5689c.svg",
            "fullname": "lsy",
            "name": "liushanyuan18",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "6606990280543d0b74d38438",
            "name": "qihoo360",
            "fullname": "åäº¬å¥èç§ææéå¬å¸",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660697054c0882ce77fe3160/fVsqY-Q0DDrme0zyN_zLa.png"
        },
        "isAuthorParticipating": true
    }
]