[
    {
        "paper": {
            "id": "2511.04962",
            "authors": [
                {
                    "_id": "69114faf830c57fd4c795fe7",
                    "name": "Zihao Yi",
                    "hidden": false
                },
                {
                    "_id": "69114faf830c57fd4c795fe8",
                    "name": "Qingxuan Jiang",
                    "hidden": false
                },
                {
                    "_id": "69114faf830c57fd4c795fe9",
                    "name": "Ruotian Ma",
                    "hidden": false
                },
                {
                    "_id": "69114faf830c57fd4c795fea",
                    "name": "Xingyu Chen",
                    "hidden": false
                },
                {
                    "_id": "69114faf830c57fd4c795feb",
                    "name": "Qu Yang",
                    "hidden": false
                },
                {
                    "_id": "69114faf830c57fd4c795fec",
                    "name": "Mengru Wang",
                    "hidden": false
                },
                {
                    "_id": "69114faf830c57fd4c795fed",
                    "name": "Fanghua Ye",
                    "hidden": false
                },
                {
                    "_id": "69114faf830c57fd4c795fee",
                    "name": "Ying Shen",
                    "hidden": false
                },
                {
                    "_id": "69114faf830c57fd4c795fef",
                    "user": {
                        "_id": "67485743561b1e6f9579389f",
                        "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
                        "isPro": false,
                        "fullname": "Zhaopeng Tu",
                        "user": "zptu",
                        "type": "user"
                    },
                    "name": "Zhaopeng Tu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-10T21:03:03.510Z",
                    "hidden": false
                },
                {
                    "_id": "69114faf830c57fd4c795ff0",
                    "name": "Xiaolong Li",
                    "hidden": false
                },
                {
                    "_id": "69114faf830c57fd4c795ff1",
                    "name": "Linus",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-07T03:50:52.000Z",
            "submittedOnDailyAt": "2025-11-10T00:19:32.643Z",
            "title": "Too Good to be Bad: On the Failure of LLMs to Role-Play Villains",
            "submittedOnDailyBy": {
                "_id": "65f7ad5526f86cf3378a59f2",
                "avatarUrl": "/avatars/752aa3c072c71ece41ea786371574777.svg",
                "isPro": false,
                "fullname": "Zihao Yi",
                "user": "Zihao1",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) are increasingly tasked with creative\ngeneration, including the simulation of fictional characters. However, their\nability to portray non-prosocial, antagonistic personas remains largely\nunexamined. We hypothesize that the safety alignment of modern LLMs creates a\nfundamental conflict with the task of authentically role-playing morally\nambiguous or villainous characters. To investigate this, we introduce the Moral\nRolePlay benchmark, a new dataset featuring a four-level moral alignment scale\nand a balanced test set for rigorous evaluation. We task state-of-the-art LLMs\nwith role-playing characters from moral paragons to pure villains. Our\nlarge-scale evaluation reveals a consistent, monotonic decline in role-playing\nfidelity as character morality decreases. We find that models struggle most\nwith traits directly antithetical to safety principles, such as ``Deceitful''\nand ``Manipulative'', often substituting nuanced malevolence with superficial\naggression. Furthermore, we demonstrate that general chatbot proficiency is a\npoor predictor of villain role-playing ability, with highly safety-aligned\nmodels performing particularly poorly. Our work provides the first systematic\nevidence of this critical limitation, highlighting a key tension between model\nsafety and creative fidelity. Our benchmark and findings pave the way for\ndeveloping more nuanced, context-aware alignment methods.",
            "upvotes": 32,
            "discussionId": "69114fb0830c57fd4c795ff2",
            "projectPage": "https://github.com/Tencent/digitalhuman/tree/main/RolePlay_Villain",
            "githubRepo": "https://github.com/Tencent/digitalhuman/tree/main/RolePlay_Villain",
            "ai_summary": "LLMs struggle to authentically portray morally ambiguous or villainous characters due to safety alignment, as evidenced by the Moral RolePlay benchmark.",
            "ai_keywords": [
                "Large Language Models",
                "Moral RolePlay benchmark",
                "moral alignment scale",
                "role-playing fidelity",
                "safety principles",
                "Deceitful",
                "Manipulative",
                "chatbot proficiency",
                "model safety",
                "creative fidelity"
            ],
            "githubStars": 161,
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-11-06T22:50:52.000Z",
        "title": "Too Good to be Bad: On the Failure of LLMs to Role-Play Villains",
        "summary": "Large Language Models (LLMs) are increasingly tasked with creative\ngeneration, including the simulation of fictional characters. However, their\nability to portray non-prosocial, antagonistic personas remains largely\nunexamined. We hypothesize that the safety alignment of modern LLMs creates a\nfundamental conflict with the task of authentically role-playing morally\nambiguous or villainous characters. To investigate this, we introduce the Moral\nRolePlay benchmark, a new dataset featuring a four-level moral alignment scale\nand a balanced test set for rigorous evaluation. We task state-of-the-art LLMs\nwith role-playing characters from moral paragons to pure villains. Our\nlarge-scale evaluation reveals a consistent, monotonic decline in role-playing\nfidelity as character morality decreases. We find that models struggle most\nwith traits directly antithetical to safety principles, such as ``Deceitful''\nand ``Manipulative'', often substituting nuanced malevolence with superficial\naggression. Furthermore, we demonstrate that general chatbot proficiency is a\npoor predictor of villain role-playing ability, with highly safety-aligned\nmodels performing particularly poorly. Our work provides the first systematic\nevidence of this critical limitation, highlighting a key tension between model\nsafety and creative fidelity. Our benchmark and findings pave the way for\ndeveloping more nuanced, context-aware alignment methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04962.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "65f7ad5526f86cf3378a59f2",
            "avatarUrl": "/avatars/752aa3c072c71ece41ea786371574777.svg",
            "fullname": "Zihao Yi",
            "name": "Zihao1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.05271",
            "authors": [
                {
                    "_id": "69114e31830c57fd4c795fa3",
                    "name": "Jack Hong",
                    "hidden": false
                },
                {
                    "_id": "69114e31830c57fd4c795fa4",
                    "name": "Chenxiao Zhao",
                    "hidden": false
                },
                {
                    "_id": "69114e31830c57fd4c795fa5",
                    "name": "ChengLin Zhu",
                    "hidden": false
                },
                {
                    "_id": "69114e31830c57fd4c795fa6",
                    "name": "Weiheng Lu",
                    "hidden": false
                },
                {
                    "_id": "69114e31830c57fd4c795fa7",
                    "name": "Guohai Xu",
                    "hidden": false
                },
                {
                    "_id": "69114e31830c57fd4c795fa8",
                    "name": "Xing Yu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/k6AJxw6TBApwg-wnk8q80.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/rqGT59socM7p28aV0vajF.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/fasAZzZ_zxN4f-S6Pvd5t.gif"
            ],
            "publishedAt": "2025-11-07T14:31:20.000Z",
            "submittedOnDailyAt": "2025-11-10T00:00:34.439Z",
            "title": "DeepEyesV2: Toward Agentic Multimodal Model",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Agentic multimodal models should not only comprehend text and images, but\nalso actively invoke external tools, such as code execution environments and\nweb search, and integrate these operations into reasoning. In this work, we\nintroduce DeepEyesV2 and explore how to build an agentic multimodal model from\nthe perspectives of data construction, training methods, and model evaluation.\nWe observe that direct reinforcement learning alone fails to induce robust\ntool-use behavior. This phenomenon motivates a two-stage training pipeline: a\ncold-start stage to establish tool-use patterns, and reinforcement learning\nstage to further refine tool invocation. We curate a diverse, moderately\nchallenging training dataset, specifically including examples where tool use is\nbeneficial. We further introduce RealX-Bench, a comprehensive benchmark\ndesigned to evaluate real-world multimodal reasoning, which inherently requires\nthe integration of multiple capabilities, including perception, search, and\nreasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative\nbenchmarks, demonstrating its effectiveness across real-world understanding,\nmathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2\nexhibits task-adaptive tool invocation, tending to use image operations for\nperception tasks and numerical computations for reasoning tasks. Reinforcement\nlearning further enables complex tool combinations and allows model to\nselectively invoke tools based on context. We hope our study can provide\nguidance for community in developing agentic multimodal models.",
            "upvotes": 29,
            "discussionId": "69114e31830c57fd4c795fa9",
            "projectPage": "https://visual-agent.github.io/",
            "githubRepo": "https://github.com/Visual-Agent/DeepEyes",
            "ai_summary": "DeepEyesV2, an agentic multimodal model, uses a two-stage training pipeline to effectively integrate tool use, demonstrating robust performance across real-world reasoning tasks.",
            "ai_keywords": [
                "DeepEyesV2",
                "reinforcement learning",
                "cold-start stage",
                "RealX-Bench",
                "multimodal reasoning",
                "tool invocation",
                "task-adaptive",
                "tool combinations"
            ],
            "githubStars": 941
        },
        "publishedAt": "2025-11-07T09:31:20.000Z",
        "title": "DeepEyesV2: Toward Agentic Multimodal Model",
        "summary": "Agentic multimodal models should not only comprehend text and images, but\nalso actively invoke external tools, such as code execution environments and\nweb search, and integrate these operations into reasoning. In this work, we\nintroduce DeepEyesV2 and explore how to build an agentic multimodal model from\nthe perspectives of data construction, training methods, and model evaluation.\nWe observe that direct reinforcement learning alone fails to induce robust\ntool-use behavior. This phenomenon motivates a two-stage training pipeline: a\ncold-start stage to establish tool-use patterns, and reinforcement learning\nstage to further refine tool invocation. We curate a diverse, moderately\nchallenging training dataset, specifically including examples where tool use is\nbeneficial. We further introduce RealX-Bench, a comprehensive benchmark\ndesigned to evaluate real-world multimodal reasoning, which inherently requires\nthe integration of multiple capabilities, including perception, search, and\nreasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative\nbenchmarks, demonstrating its effectiveness across real-world understanding,\nmathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2\nexhibits task-adaptive tool invocation, tending to use image operations for\nperception tasks and numerical computations for reasoning tasks. Reinforcement\nlearning further enables complex tool combinations and allows model to\nselectively invoke tools based on context. We hope our study can provide\nguidance for community in developing agentic multimodal models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/k6AJxw6TBApwg-wnk8q80.gif",
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/rqGT59socM7p28aV0vajF.gif",
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/fasAZzZ_zxN4f-S6Pvd5t.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.05271.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 157
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.05491",
            "authors": [
                {
                    "_id": "69115163830c57fd4c795fff",
                    "user": {
                        "_id": "641d6c27c5f150c3af0bf879",
                        "avatarUrl": "/avatars/39808793703406b4fe997e351b6a6bfb.svg",
                        "isPro": false,
                        "fullname": "Ray Yang",
                        "user": "rayruiyang",
                        "type": "user"
                    },
                    "name": "Rui Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-10T08:44:47.523Z",
                    "hidden": false
                },
                {
                    "_id": "69115163830c57fd4c796000",
                    "name": "Ziyu Zhu",
                    "hidden": false
                },
                {
                    "_id": "69115163830c57fd4c796001",
                    "user": {
                        "_id": "643ff78cdc984afcbbbc3b1a",
                        "avatarUrl": "/avatars/eec5198ce88aaf8156840bec0d190a7f.svg",
                        "isPro": false,
                        "fullname": "Yanwei Li",
                        "user": "YanweiLi",
                        "type": "user"
                    },
                    "name": "Yanwei Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-10T08:44:49.860Z",
                    "hidden": false
                },
                {
                    "_id": "69115163830c57fd4c796002",
                    "name": "Jingjia Huang",
                    "hidden": false
                },
                {
                    "_id": "69115163830c57fd4c796003",
                    "name": "Shen Yan",
                    "hidden": false
                },
                {
                    "_id": "69115163830c57fd4c796004",
                    "name": "Siyuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "69115163830c57fd4c796005",
                    "name": "Zhe Liu",
                    "hidden": false
                },
                {
                    "_id": "69115163830c57fd4c796006",
                    "name": "Xiangtai Li",
                    "hidden": false
                },
                {
                    "_id": "69115163830c57fd4c796007",
                    "name": "Shuangye Li",
                    "hidden": false
                },
                {
                    "_id": "69115163830c57fd4c796008",
                    "name": "Wenqian Wang",
                    "hidden": false
                },
                {
                    "_id": "69115163830c57fd4c796009",
                    "name": "Yi Lin",
                    "hidden": false
                },
                {
                    "_id": "69115163830c57fd4c79600a",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-07T18:59:16.000Z",
            "submittedOnDailyAt": "2025-11-10T00:14:02.211Z",
            "title": "Visual Spatial Tuning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Capturing spatial relationships from visual inputs is a cornerstone of\nhuman-like general intelligence. Several previous studies have tried to enhance\nthe spatial awareness of Vision-Language Models (VLMs) by adding extra expert\nencoders, which brings extra overhead and usually harms general capabilities.\nTo enhance the spatial ability in general architectures, we introduce Visual\nSpatial Tuning (VST), a comprehensive framework to cultivate VLMs with\nhuman-like visuospatial abilities, from spatial perception to reasoning. We\nfirst attempt to enhance spatial perception in VLMs by constructing a\nlarge-scale dataset termed VST-P, which comprises 4.1 million samples spanning\n19 skills across single views, multiple images, and videos. Then, we present\nVST-R, a curated dataset with 135K samples that instruct models to reason in\nspace. In particular, we adopt a progressive training pipeline: supervised\nfine-tuning to build foundational spatial knowledge, followed by reinforcement\nlearning to further improve spatial reasoning abilities. Without the\nside-effect to general capabilities, the proposed VST consistently achieves\nstate-of-the-art results on several spatial benchmarks, including 34.8% on\nMMSI-Bench and 61.2% on VSIBench. It turns out that the\nVision-Language-Action models can be significantly enhanced with the proposed\nspatial tuning paradigm, paving the way for more physically grounded AI.",
            "upvotes": 28,
            "discussionId": "69115163830c57fd4c79600b",
            "projectPage": "https://yangr116.github.io/vst_project/",
            "githubRepo": "https://github.com/Yangr116/VST",
            "ai_summary": "A framework called Visual Spatial Tuning (VST) enhances the spatial abilities of Vision-Language Models (VLMs) through progressive training with specialized datasets, achieving state-of-the-art results on spatial benchmarks.",
            "ai_keywords": [
                "Visual Spatial Tuning",
                "VST",
                "Vision-Language Models",
                "VLMs",
                "spatial perception",
                "spatial reasoning",
                "VST-P",
                "VST-R",
                "supervised fine-tuning",
                "reinforcement learning",
                "MMSI-Bench",
                "VSIBench",
                "Vision-Language-Action models"
            ],
            "githubStars": 51,
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-11-07T13:59:16.000Z",
        "title": "Visual Spatial Tuning",
        "summary": "Capturing spatial relationships from visual inputs is a cornerstone of\nhuman-like general intelligence. Several previous studies have tried to enhance\nthe spatial awareness of Vision-Language Models (VLMs) by adding extra expert\nencoders, which brings extra overhead and usually harms general capabilities.\nTo enhance the spatial ability in general architectures, we introduce Visual\nSpatial Tuning (VST), a comprehensive framework to cultivate VLMs with\nhuman-like visuospatial abilities, from spatial perception to reasoning. We\nfirst attempt to enhance spatial perception in VLMs by constructing a\nlarge-scale dataset termed VST-P, which comprises 4.1 million samples spanning\n19 skills across single views, multiple images, and videos. Then, we present\nVST-R, a curated dataset with 135K samples that instruct models to reason in\nspace. In particular, we adopt a progressive training pipeline: supervised\nfine-tuning to build foundational spatial knowledge, followed by reinforcement\nlearning to further improve spatial reasoning abilities. Without the\nside-effect to general capabilities, the proposed VST consistently achieves\nstate-of-the-art results on several spatial benchmarks, including 34.8% on\nMMSI-Bench and 61.2% on VSIBench. It turns out that the\nVision-Language-Action models can be significantly enhanced with the proposed\nspatial tuning paradigm, paving the way for more physically grounded AI.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.05491.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 157
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.04662",
            "authors": [
                {
                    "_id": "690e232f41eeb14959b21d01",
                    "name": "Yu Feng",
                    "hidden": false
                },
                {
                    "_id": "690e232f41eeb14959b21d02",
                    "name": "Nathaniel Weir",
                    "hidden": false
                },
                {
                    "_id": "690e232f41eeb14959b21d03",
                    "name": "Kaj Bostrom",
                    "hidden": false
                },
                {
                    "_id": "690e232f41eeb14959b21d04",
                    "name": "Sam Bayless",
                    "hidden": false
                },
                {
                    "_id": "690e232f41eeb14959b21d05",
                    "name": "Darion Cassel",
                    "hidden": false
                },
                {
                    "_id": "690e232f41eeb14959b21d06",
                    "name": "Sapana Chaudhary",
                    "hidden": false
                },
                {
                    "_id": "690e232f41eeb14959b21d07",
                    "name": "Benjamin Kiesl-Reiter",
                    "hidden": false
                },
                {
                    "_id": "690e232f41eeb14959b21d08",
                    "name": "Huzefa Rangwala",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-06T18:50:08.000Z",
            "submittedOnDailyAt": "2025-11-10T14:52:17.876Z",
            "title": "VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical\n  Consistency Checks",
            "submittedOnDailyBy": {
                "_id": "641fb016f8c8b04c0bad1df5",
                "avatarUrl": "/avatars/e66bfff9befb964ba3a58d05ebb9180b.svg",
                "isPro": false,
                "fullname": "Yu Feng",
                "user": "AnnieFeng",
                "type": "user"
            },
            "summary": "LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but\nthey cannot reliably verify their own logic. Even when they reach correct\nanswers, the underlying reasoning may be flawed, undermining trust in\nhigh-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a\nneuro-symbolic method that extracts and verifies formal logical arguments from\nCoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order\nlogic and identifies premises that ground the argument in source context,\ncommonsense knowledge, or prior reasoning steps. The symbolic representation\nenables automated solvers to verify logical validity while the NL premises\nallow humans and systems to identify ungrounded or fallacious reasoning steps.\nExperiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT\neffectively identifies flawed reasoning, and serves as a strong predictor of\nfinal answer correctness. We also leverage VeriCoT's verification signal for\n(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on\nVeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct\npreference optimization (DPO) using verification-based pairwise rewards,\nfurther improving reasoning validity and accuracy.",
            "upvotes": 24,
            "discussionId": "690e232f41eeb14959b21d09",
            "ai_summary": "VeriCoT, a neuro-symbolic method, formalizes and verifies logical arguments in Chain-of-Thought reasoning to improve the reliability and accuracy of LLMs.",
            "ai_keywords": [
                "Chain-of-Thought",
                "VeriCoT",
                "neuro-symbolic method",
                "first-order logic",
                "formal logical arguments",
                "source context",
                "commonsense knowledge",
                "automated solvers",
                "inference-time self-reflection",
                "supervised fine-tuning",
                "preference fine-tuning",
                "direct preference optimization",
                "verification-based pairwise rewards"
            ],
            "organization": {
                "_id": "6058ec29102f61b42f65ae35",
                "name": "AWS",
                "fullname": "Amazon Web Services",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/wtuzZxWzijQ3do3zYoOFH.png"
            }
        },
        "publishedAt": "2025-11-06T13:50:08.000Z",
        "title": "VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical\n  Consistency Checks",
        "summary": "LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but\nthey cannot reliably verify their own logic. Even when they reach correct\nanswers, the underlying reasoning may be flawed, undermining trust in\nhigh-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a\nneuro-symbolic method that extracts and verifies formal logical arguments from\nCoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order\nlogic and identifies premises that ground the argument in source context,\ncommonsense knowledge, or prior reasoning steps. The symbolic representation\nenables automated solvers to verify logical validity while the NL premises\nallow humans and systems to identify ungrounded or fallacious reasoning steps.\nExperiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT\neffectively identifies flawed reasoning, and serves as a strong predictor of\nfinal answer correctness. We also leverage VeriCoT's verification signal for\n(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on\nVeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct\npreference optimization (DPO) using verification-based pairwise rewards,\nfurther improving reasoning validity and accuracy.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04662.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "641fb016f8c8b04c0bad1df5",
            "avatarUrl": "/avatars/e66bfff9befb964ba3a58d05ebb9180b.svg",
            "fullname": "Yu Feng",
            "name": "AnnieFeng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6058ec29102f61b42f65ae35",
            "name": "AWS",
            "fullname": "Amazon Web Services",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/wtuzZxWzijQ3do3zYoOFH.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.05369",
            "authors": [
                {
                    "_id": "69114e51830c57fd4c795fab",
                    "user": {
                        "_id": "6457920bcd935d48a4789cef",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AswyuYQah0noLTYkpAmRT.png",
                        "isPro": false,
                        "fullname": "Shiyao Xu",
                        "user": "Xusy2333",
                        "type": "user"
                    },
                    "name": "Shiyao Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-10T21:03:08.576Z",
                    "hidden": false
                },
                {
                    "_id": "69114e51830c57fd4c795fac",
                    "name": "Benedetta Liberatori",
                    "hidden": false
                },
                {
                    "_id": "69114e51830c57fd4c795fad",
                    "name": "GÃ¼l Varol",
                    "hidden": false
                },
                {
                    "_id": "69114e51830c57fd4c795fae",
                    "name": "Paolo Rota",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/zdBDRZop6ZikRVtG2iUMD.png"
            ],
            "publishedAt": "2025-11-07T15:55:10.000Z",
            "submittedOnDailyAt": "2025-11-10T00:00:57.635Z",
            "title": "Dense Motion Captioning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent advances in 3D human motion and language integration have primarily\nfocused on text-to-motion generation, leaving the task of motion understanding\nrelatively unexplored. We introduce Dense Motion Captioning, a novel task that\naims to temporally localize and caption actions within 3D human motion\nsequences. Current datasets fall short in providing detailed temporal\nannotations and predominantly consist of short sequences featuring few actions.\nTo overcome these limitations, we present the Complex Motion Dataset (CompMo),\nthe first large-scale dataset featuring richly annotated, complex motion\nsequences with precise temporal boundaries. Built through a carefully designed\ndata generation pipeline, CompMo includes 60,000 motion sequences, each\ncomposed of multiple actions ranging from at least two to ten, accurately\nannotated with their temporal extents. We further present DEMO, a model that\nintegrates a large language model with a simple motion adapter, trained to\ngenerate dense, temporally grounded captions. Our experiments show that DEMO\nsubstantially outperforms existing methods on CompMo as well as on adapted\nbenchmarks, establishing a robust baseline for future research in 3D motion\nunderstanding and captioning.",
            "upvotes": 5,
            "discussionId": "69114e51830c57fd4c795faf",
            "projectPage": "https://xusy2333.com/demo/",
            "githubRepo": "https://github.com/41xu/DEMO",
            "ai_summary": "A new task, Dense Motion Captioning, is introduced with a large-scale dataset, CompMo, and a model, DEMO, that integrates a language model with a motion adapter to generate detailed, temporally grounded captions for 3D human motion sequences.",
            "ai_keywords": [
                "Dense Motion Captioning",
                "Complex Motion Dataset",
                "CompMo",
                "DEMO",
                "large language model",
                "motion adapter",
                "temporal localization",
                "temporal boundaries",
                "motion sequences",
                "action captioning"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-11-07T10:55:10.000Z",
        "title": "Dense Motion Captioning",
        "summary": "Recent advances in 3D human motion and language integration have primarily\nfocused on text-to-motion generation, leaving the task of motion understanding\nrelatively unexplored. We introduce Dense Motion Captioning, a novel task that\naims to temporally localize and caption actions within 3D human motion\nsequences. Current datasets fall short in providing detailed temporal\nannotations and predominantly consist of short sequences featuring few actions.\nTo overcome these limitations, we present the Complex Motion Dataset (CompMo),\nthe first large-scale dataset featuring richly annotated, complex motion\nsequences with precise temporal boundaries. Built through a carefully designed\ndata generation pipeline, CompMo includes 60,000 motion sequences, each\ncomposed of multiple actions ranging from at least two to ten, accurately\nannotated with their temporal extents. We further present DEMO, a model that\nintegrates a large language model with a simple motion adapter, trained to\ngenerate dense, temporally grounded captions. Our experiments show that DEMO\nsubstantially outperforms existing methods on CompMo as well as on adapted\nbenchmarks, establishing a robust baseline for future research in 3D motion\nunderstanding and captioning.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/zdBDRZop6ZikRVtG2iUMD.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.05369.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 157
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.05017",
            "authors": [
                {
                    "_id": "69114f2f830c57fd4c795fdd",
                    "name": "Aakriti Agrawal",
                    "hidden": false
                },
                {
                    "_id": "69114f2f830c57fd4c795fde",
                    "name": "Gouthaman KV",
                    "hidden": false
                },
                {
                    "_id": "69114f2f830c57fd4c795fdf",
                    "name": "Rohith Aralikatti",
                    "hidden": false
                },
                {
                    "_id": "69114f2f830c57fd4c795fe0",
                    "name": "Gauri Jagatap",
                    "hidden": false
                },
                {
                    "_id": "69114f2f830c57fd4c795fe1",
                    "name": "Jiaxin Yuan",
                    "hidden": false
                },
                {
                    "_id": "69114f2f830c57fd4c795fe2",
                    "name": "Vijay Kamarshi",
                    "hidden": false
                },
                {
                    "_id": "69114f2f830c57fd4c795fe3",
                    "name": "Andrea Fanelli",
                    "hidden": false
                },
                {
                    "_id": "69114f2f830c57fd4c795fe4",
                    "name": "Furong Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-07T06:39:54.000Z",
            "submittedOnDailyAt": "2025-11-10T00:04:30.999Z",
            "title": "Towards Mitigating Hallucinations in Large Vision-Language Models by\n  Refining Textual Embeddings",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "In this work, we identify an inherent bias in prevailing LVLM architectures\ntoward the language modality, largely resulting from the common practice of\nsimply appending visual embeddings to the input text sequence. To address this,\nwe propose a simple yet effective method that refines textual embeddings by\nintegrating average-pooled visual features. Our approach demonstrably improves\nvisual grounding and significantly reduces hallucinations on established\nbenchmarks. While average pooling offers a straightforward, robust, and\nefficient means of incorporating visual information, we believe that more\nsophisticated fusion methods could further enhance visual grounding and\ncross-modal alignment. Given that the primary focus of this work is to\nhighlight the modality imbalance and its impact on hallucinations -- and to\nshow that refining textual embeddings with visual information mitigates this\nissue -- we leave exploration of advanced fusion strategies for future work.",
            "upvotes": 4,
            "discussionId": "69114f2f830c57fd4c795fe5",
            "ai_summary": "Refining textual embeddings with average-pooled visual features improves visual grounding and reduces hallucinations in LVLM architectures.",
            "ai_keywords": [
                "LVLM architectures",
                "language modality",
                "visual embeddings",
                "textual embeddings",
                "average-pooled visual features",
                "visual grounding",
                "hallucinations",
                "cross-modal alignment"
            ]
        },
        "publishedAt": "2025-11-07T01:39:54.000Z",
        "title": "Towards Mitigating Hallucinations in Large Vision-Language Models by\n  Refining Textual Embeddings",
        "summary": "In this work, we identify an inherent bias in prevailing LVLM architectures\ntoward the language modality, largely resulting from the common practice of\nsimply appending visual embeddings to the input text sequence. To address this,\nwe propose a simple yet effective method that refines textual embeddings by\nintegrating average-pooled visual features. Our approach demonstrably improves\nvisual grounding and significantly reduces hallucinations on established\nbenchmarks. While average pooling offers a straightforward, robust, and\nefficient means of incorporating visual information, we believe that more\nsophisticated fusion methods could further enhance visual grounding and\ncross-modal alignment. Given that the primary focus of this work is to\nhighlight the modality imbalance and its impact on hallucinations -- and to\nshow that refining textual embeddings with visual information mitigates this\nissue -- we leave exploration of advanced fusion strategies for future work.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.05017.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 157
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.04898",
            "authors": [
                {
                    "_id": "691261e9a644ba07c499c69d",
                    "name": "Yule Wen",
                    "hidden": false
                },
                {
                    "_id": "691261e9a644ba07c499c69e",
                    "name": "Yixin Ye",
                    "hidden": false
                },
                {
                    "_id": "691261e9a644ba07c499c69f",
                    "name": "Yanzhe Zhang",
                    "hidden": false
                },
                {
                    "_id": "691261e9a644ba07c499c6a0",
                    "name": "Diyi Yang",
                    "hidden": false
                },
                {
                    "_id": "691261e9a644ba07c499c6a1",
                    "name": "Hao Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-07T00:51:02.000Z",
            "submittedOnDailyAt": "2025-11-10T20:05:59.763Z",
            "title": "Real-Time Reasoning Agents in Evolving Environments",
            "submittedOnDailyBy": {
                "_id": "61aa376688c20eebf1e8deb3",
                "avatarUrl": "/avatars/7c11dcb232c73547d7d87834be287822.svg",
                "isPro": false,
                "fullname": "Hao Zhu",
                "user": "ProKil",
                "type": "user"
            },
            "summary": "Agents in the real world must make not only logical but also timely\njudgments. This requires continuous awareness of the dynamic environment:\nhazards emerge, opportunities arise, and other agents act, while the agent's\nreasoning is still unfolding. Despite advances in language model reasoning,\nexisting approaches fail to account for this dynamic nature. We introduce\nreal-time reasoning as a new problem formulation for agents in evolving\nenvironments and build Real-Time Reasoning Gym to demonstrate it. We study two\nparadigms for deploying language models in agents: (1) reactive agents, which\nemploy language models with bounded reasoning computation for rapid responses,\nand (2) planning agents, which allow extended reasoning computation for complex\nproblems. Our experiments show that even state-of-the-art models struggle with\nmaking logical and timely judgments in either paradigm. To address this\nlimitation, we propose AgileThinker, which simultaneously engages both\nreasoning paradigms. AgileThinker consistently outperforms agents engaging only\none reasoning paradigm as the task difficulty and time pressure rise,\neffectively balancing reasoning depth and response latency. Our work\nestablishes real-time reasoning as a critical testbed for developing practical\nagents and provides a foundation for research in temporally constrained AI\nsystems, highlighting a path toward real-time capable agents.",
            "upvotes": 2,
            "discussionId": "691261e9a644ba07c499c6a2",
            "projectPage": "https://realtimegym.saltlab.stanford.edu",
            "githubRepo": "https://github.com/SALT-NLP/RealtimeGym",
            "ai_summary": "Real-Time Reasoning Gym demonstrates the challenges of deploying language models in dynamic environments, introducing AgileThinker to balance reasoning depth and response latency.",
            "ai_keywords": [
                "real-time reasoning",
                "Reactive agents",
                "Planning agents",
                "AgileThinker",
                "temporally constrained AI systems"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "63213141145cfa4c04ce8f5f",
                "name": "SALT-NLP",
                "fullname": "Social And Language Technology Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632116accafe12f481a473cb/6s98xP2tbaxg5uHpV66m7.png"
            }
        },
        "publishedAt": "2025-11-06T19:51:02.000Z",
        "title": "Real-Time Reasoning Agents in Evolving Environments",
        "summary": "Agents in the real world must make not only logical but also timely\njudgments. This requires continuous awareness of the dynamic environment:\nhazards emerge, opportunities arise, and other agents act, while the agent's\nreasoning is still unfolding. Despite advances in language model reasoning,\nexisting approaches fail to account for this dynamic nature. We introduce\nreal-time reasoning as a new problem formulation for agents in evolving\nenvironments and build Real-Time Reasoning Gym to demonstrate it. We study two\nparadigms for deploying language models in agents: (1) reactive agents, which\nemploy language models with bounded reasoning computation for rapid responses,\nand (2) planning agents, which allow extended reasoning computation for complex\nproblems. Our experiments show that even state-of-the-art models struggle with\nmaking logical and timely judgments in either paradigm. To address this\nlimitation, we propose AgileThinker, which simultaneously engages both\nreasoning paradigms. AgileThinker consistently outperforms agents engaging only\none reasoning paradigm as the task difficulty and time pressure rise,\neffectively balancing reasoning depth and response latency. Our work\nestablishes real-time reasoning as a critical testbed for developing practical\nagents and provides a foundation for research in temporally constrained AI\nsystems, highlighting a path toward real-time capable agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04898.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "61aa376688c20eebf1e8deb3",
            "avatarUrl": "/avatars/7c11dcb232c73547d7d87834be287822.svg",
            "fullname": "Hao Zhu",
            "name": "ProKil",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "organization": {
            "_id": "63213141145cfa4c04ce8f5f",
            "name": "SALT-NLP",
            "fullname": "Social And Language Technology Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632116accafe12f481a473cb/6s98xP2tbaxg5uHpV66m7.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.04707",
            "authors": [
                {
                    "_id": "69114ff0830c57fd4c795ff4",
                    "name": "Rishi Rajesh Shah",
                    "hidden": false
                },
                {
                    "_id": "69114ff0830c57fd4c795ff5",
                    "name": "Chen Henry Wu",
                    "hidden": false
                },
                {
                    "_id": "69114ff0830c57fd4c795ff6",
                    "name": "Shashwat Saxena",
                    "hidden": false
                },
                {
                    "_id": "69114ff0830c57fd4c795ff7",
                    "name": "Ziqian Zhong",
                    "hidden": false
                },
                {
                    "_id": "69114ff0830c57fd4c795ff8",
                    "name": "Alexander Robey",
                    "hidden": false
                },
                {
                    "_id": "69114ff0830c57fd4c795ff9",
                    "name": "Aditi Raghunathan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-05T01:12:50.000Z",
            "submittedOnDailyAt": "2025-11-10T00:07:54.603Z",
            "title": "Jailbreaking in the Haystack",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent advances in long-context language models (LMs) have enabled\nmillion-token inputs, expanding their capabilities across complex tasks like\ncomputer-use agents. Yet, the safety implications of these extended contexts\nremain unclear. To bridge this gap, we introduce NINJA (short for\nNeedle-in-haystack jailbreak attack), a method that jailbreaks aligned LMs by\nappending benign, model-generated content to harmful user goals. Critical to\nour method is the observation that the position of harmful goals play an\nimportant role in safety. Experiments on standard safety benchmark, HarmBench,\nshow that NINJA significantly increases attack success rates across\nstate-of-the-art open and proprietary models, including LLaMA, Qwen, Mistral,\nand Gemini. Unlike prior jailbreaking methods, our approach is low-resource,\ntransferable, and less detectable. Moreover, we show that NINJA is\ncompute-optimal -- under a fixed compute budget, increasing context length can\noutperform increasing the number of trials in best-of-N jailbreak. These\nfindings reveal that even benign long contexts -- when crafted with careful\ngoal positioning -- introduce fundamental vulnerabilities in modern LMs.",
            "upvotes": 1,
            "discussionId": "69114ff0830c57fd4c795ffa",
            "projectPage": "https://ar-forum.github.io/ninjaattackweb/",
            "githubRepo": "https://github.com/AR-FORUM/NINJA_Attack",
            "ai_summary": "NINJA, a jailbreak attack method, appends benign content to harmful goals in long-context language models, increasing attack success rates and revealing vulnerabilities in these models.",
            "ai_keywords": [
                "long-context language models",
                "HarmBench",
                "NINJA",
                "jailbreak attack",
                "model-generated content",
                "position of harmful goals",
                "compute-optimal",
                "best-of-N jailbreak"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-11-04T20:12:50.000Z",
        "title": "Jailbreaking in the Haystack",
        "summary": "Recent advances in long-context language models (LMs) have enabled\nmillion-token inputs, expanding their capabilities across complex tasks like\ncomputer-use agents. Yet, the safety implications of these extended contexts\nremain unclear. To bridge this gap, we introduce NINJA (short for\nNeedle-in-haystack jailbreak attack), a method that jailbreaks aligned LMs by\nappending benign, model-generated content to harmful user goals. Critical to\nour method is the observation that the position of harmful goals play an\nimportant role in safety. Experiments on standard safety benchmark, HarmBench,\nshow that NINJA significantly increases attack success rates across\nstate-of-the-art open and proprietary models, including LLaMA, Qwen, Mistral,\nand Gemini. Unlike prior jailbreaking methods, our approach is low-resource,\ntransferable, and less detectable. Moreover, we show that NINJA is\ncompute-optimal -- under a fixed compute budget, increasing context length can\noutperform increasing the number of trials in best-of-N jailbreak. These\nfindings reveal that even benign long contexts -- when crafted with careful\ngoal positioning -- introduce fundamental vulnerabilities in modern LMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04707.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 157
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.01047",
            "authors": [
                {
                    "_id": "6910e22d830c57fd4c795f4c",
                    "name": "Yu Shi",
                    "hidden": false
                },
                {
                    "_id": "6910e22d830c57fd4c795f4d",
                    "user": {
                        "_id": "62b4f3b7464e664268bf4e85",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b4f3b7464e664268bf4e85/atFIscmB37aur0a1zuQ2o.jpeg",
                        "isPro": false,
                        "fullname": "Leo",
                        "user": "hao-li",
                        "type": "user"
                    },
                    "name": "Hao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-10T21:03:14.600Z",
                    "hidden": false
                },
                {
                    "_id": "6910e22d830c57fd4c795f4e",
                    "name": "Bram Adams",
                    "hidden": false
                },
                {
                    "_id": "6910e22d830c57fd4c795f4f",
                    "name": "Ahmed E. Hassan",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62b4f3b7464e664268bf4e85/pFfZuolMGXV58xEv2ZjPo.png"
            ],
            "publishedAt": "2025-11-02T18:45:34.000Z",
            "submittedOnDailyAt": "2025-11-10T13:09:00.046Z",
            "title": "HAFixAgent: History-Aware Automated Program Repair Agent",
            "submittedOnDailyBy": {
                "_id": "62b4f3b7464e664268bf4e85",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b4f3b7464e664268bf4e85/atFIscmB37aur0a1zuQ2o.jpeg",
                "isPro": false,
                "fullname": "Leo",
                "user": "hao-li",
                "type": "user"
            },
            "summary": "Automated program repair (APR) has recently shifted toward large language\nmodels and agent-based systems, yet most systems rely on local snapshot\ncontext, overlooking repository history. Prior work shows that repository\nhistory helps repair single-line bugs, since the last commit touching the buggy\nline is often the bug-introducing one. In this paper, we investigate whether\nrepository history can also improve agentic APR systems at scale, especially\nfor complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing\nAgent that injects blame-derived repository heuristics into its repair loop. A\npreliminary study of all 854 real-world bugs from Defects4J motivates our\ndesign, showing that bug-relevant history is both widely available and highly\nconcentrated. Empirical comparison of HAFixAgent with two state-of-the-art\nbaselines shows: (1) Effectiveness: HAFixAgent significantly improves over the\nagent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2)\nEfficiency: history does not significantly increase agent steps and keeps token\ncosts comparable, with notably lower median costs for complex\nmulti-file-multi-hunk bugs. (3) Practicality: combining different historical\nheuristics repairs more bugs, offering a clear cost-benefit trade-off.\nHAFixAgent offers a practical recipe for history-aware agentic APR: ground the\nagent in version control history, prioritize diff-based historical context, and\nintegrate complementary heuristics when needed.",
            "upvotes": 1,
            "discussionId": "6910e22e830c57fd4c795f50",
            "ai_summary": "HAFixAgent, a history-aware bug-fixing agent, improves automated program repair by incorporating repository history, enhancing effectiveness and efficiency for complex multi-hunk bugs.",
            "ai_keywords": [
                "automated program repair",
                "large language models",
                "agent-based systems",
                "repository history",
                "blame-derived repository heuristics",
                "Defects4J",
                "multi-hunk bugs",
                "diff-based historical context"
            ]
        },
        "publishedAt": "2025-11-02T13:45:34.000Z",
        "title": "HAFixAgent: History-Aware Automated Program Repair Agent",
        "summary": "Automated program repair (APR) has recently shifted toward large language\nmodels and agent-based systems, yet most systems rely on local snapshot\ncontext, overlooking repository history. Prior work shows that repository\nhistory helps repair single-line bugs, since the last commit touching the buggy\nline is often the bug-introducing one. In this paper, we investigate whether\nrepository history can also improve agentic APR systems at scale, especially\nfor complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing\nAgent that injects blame-derived repository heuristics into its repair loop. A\npreliminary study of all 854 real-world bugs from Defects4J motivates our\ndesign, showing that bug-relevant history is both widely available and highly\nconcentrated. Empirical comparison of HAFixAgent with two state-of-the-art\nbaselines shows: (1) Effectiveness: HAFixAgent significantly improves over the\nagent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2)\nEfficiency: history does not significantly increase agent steps and keeps token\ncosts comparable, with notably lower median costs for complex\nmulti-file-multi-hunk bugs. (3) Practicality: combining different historical\nheuristics repairs more bugs, offering a clear cost-benefit trade-off.\nHAFixAgent offers a practical recipe for history-aware agentic APR: ground the\nagent in version control history, prioritize diff-based historical context, and\nintegrate complementary heuristics when needed.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62b4f3b7464e664268bf4e85/pFfZuolMGXV58xEv2ZjPo.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01047.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62b4f3b7464e664268bf4e85",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b4f3b7464e664268bf4e85/atFIscmB37aur0a1zuQ2o.jpeg",
            "fullname": "Leo",
            "name": "hao-li",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2510.24505",
            "authors": [
                {
                    "_id": "6911c8bc830c57fd4c796124",
                    "name": "Qing Zong",
                    "hidden": false
                },
                {
                    "_id": "6911c8bc830c57fd4c796125",
                    "user": {
                        "_id": "66783baec3f824dde8f783ac",
                        "avatarUrl": "/avatars/eb9a0441986be50274c5e661f7039e2c.svg",
                        "isPro": false,
                        "fullname": "Jeff",
                        "user": "JiayuJeff",
                        "type": "user"
                    },
                    "name": "Jiayu Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-10T21:02:59.143Z",
                    "hidden": false
                },
                {
                    "_id": "6911c8bc830c57fd4c796126",
                    "name": "Tianshi Zheng",
                    "hidden": false
                },
                {
                    "_id": "6911c8bc830c57fd4c796127",
                    "name": "Chunyang Li",
                    "hidden": false
                },
                {
                    "_id": "6911c8bc830c57fd4c796128",
                    "name": "Baixuan Xu",
                    "hidden": false
                },
                {
                    "_id": "6911c8bc830c57fd4c796129",
                    "name": "Haochen Shi",
                    "hidden": false
                },
                {
                    "_id": "6911c8bc830c57fd4c79612a",
                    "name": "Weiqi Wang",
                    "hidden": false
                },
                {
                    "_id": "6911c8bc830c57fd4c79612b",
                    "name": "Zhaowei Wang",
                    "hidden": false
                },
                {
                    "_id": "6911c8bc830c57fd4c79612c",
                    "name": "Chunkit Chan",
                    "hidden": false
                },
                {
                    "_id": "6911c8bc830c57fd4c79612d",
                    "name": "Yangqiu Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-28T15:16:06.000Z",
            "submittedOnDailyAt": "2025-11-10T08:44:50.533Z",
            "title": "CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?",
            "submittedOnDailyBy": {
                "_id": "66783baec3f824dde8f783ac",
                "avatarUrl": "/avatars/eb9a0441986be50274c5e661f7039e2c.svg",
                "isPro": false,
                "fullname": "Jeff",
                "user": "JiayuJeff",
                "type": "user"
            },
            "summary": "Accurate confidence calibration in Large Language Models (LLMs) is critical\nfor safe use in high-stakes domains, where clear verbalized confidence enhances\nuser trust. Traditional methods that mimic reference confidence expressions\noften fail to capture the reasoning needed for accurate confidence assessment.\nWe propose natural language critiques as a solution, ideally suited for\nconfidence calibration, as precise gold confidence labels are hard to obtain\nand often require multiple generations. This paper studies how natural language\ncritiques can enhance verbalized confidence, addressing: (1) What to critique:\nuncertainty (question-focused) or confidence (answer-specific)? Analysis shows\nconfidence suits multiple-choice tasks, while uncertainty excels in open-ended\nscenarios. (2) How to critique: self-critique or critique calibration training?\nWe propose Self-Critique, enabling LLMs to critique and optimize their\nconfidence beyond mere accuracy, and CritiCal, a novel Critique Calibration\ntraining method that leverages natural language critiques to improve confidence\ncalibration, moving beyond direct numerical optimization. Experiments show that\nCritiCal significantly outperforms Self-Critique and other competitive\nbaselines, even surpassing its teacher model, GPT-4o, in complex reasoning\ntasks. CritiCal also shows robust generalization in out-of-distribution\nsettings, advancing LLM's reliability.",
            "upvotes": 1,
            "discussionId": "6911c8bc830c57fd4c79612e",
            "ai_summary": "Natural language critiques improve confidence calibration in LLMs, with CritiCal training method outperforming other approaches and enhancing reliability.",
            "ai_keywords": [
                "Large Language Models",
                "confidence calibration",
                "natural language critiques",
                "self-critique",
                "CritiCal",
                "confidence assessment",
                "uncertainty",
                "multiple-choice tasks",
                "open-ended scenarios",
                "complex reasoning tasks",
                "out-of-distribution settings"
            ]
        },
        "publishedAt": "2025-10-28T11:16:06.000Z",
        "title": "CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?",
        "summary": "Accurate confidence calibration in Large Language Models (LLMs) is critical\nfor safe use in high-stakes domains, where clear verbalized confidence enhances\nuser trust. Traditional methods that mimic reference confidence expressions\noften fail to capture the reasoning needed for accurate confidence assessment.\nWe propose natural language critiques as a solution, ideally suited for\nconfidence calibration, as precise gold confidence labels are hard to obtain\nand often require multiple generations. This paper studies how natural language\ncritiques can enhance verbalized confidence, addressing: (1) What to critique:\nuncertainty (question-focused) or confidence (answer-specific)? Analysis shows\nconfidence suits multiple-choice tasks, while uncertainty excels in open-ended\nscenarios. (2) How to critique: self-critique or critique calibration training?\nWe propose Self-Critique, enabling LLMs to critique and optimize their\nconfidence beyond mere accuracy, and CritiCal, a novel Critique Calibration\ntraining method that leverages natural language critiques to improve confidence\ncalibration, moving beyond direct numerical optimization. Experiments show that\nCritiCal significantly outperforms Self-Critique and other competitive\nbaselines, even surpassing its teacher model, GPT-4o, in complex reasoning\ntasks. CritiCal also shows robust generalization in out-of-distribution\nsettings, advancing LLM's reliability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24505.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66783baec3f824dde8f783ac",
            "avatarUrl": "/avatars/eb9a0441986be50274c5e661f7039e2c.svg",
            "fullname": "Jeff",
            "name": "JiayuJeff",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]