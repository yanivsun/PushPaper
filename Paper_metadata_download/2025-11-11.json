[
    {
        "paper": {
            "id": "2511.03506",
            "authors": [
                {
                    "_id": "690c081d60494e4fa7675618",
                    "user": {
                        "_id": "64e18e9ec20c27fcc8df384e",
                        "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
                        "isPro": false,
                        "fullname": "Ding Chen",
                        "user": "Hush-cd",
                        "type": "user"
                    },
                    "name": "Ding Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:51:36.105Z",
                    "hidden": false
                },
                {
                    "_id": "690c081d60494e4fa7675619",
                    "name": "Simin Niu",
                    "hidden": false
                },
                {
                    "_id": "690c081d60494e4fa767561a",
                    "name": "Kehang Li",
                    "hidden": false
                },
                {
                    "_id": "690c081d60494e4fa767561b",
                    "name": "Peng Liu",
                    "hidden": false
                },
                {
                    "_id": "690c081d60494e4fa767561c",
                    "name": "Xiangping Zheng",
                    "hidden": false
                },
                {
                    "_id": "690c081d60494e4fa767561d",
                    "name": "Bo Tang",
                    "hidden": false
                },
                {
                    "_id": "690c081d60494e4fa767561e",
                    "name": "Xinchi Li",
                    "hidden": false
                },
                {
                    "_id": "690c081d60494e4fa767561f",
                    "name": "Feiyu Xiong",
                    "hidden": false
                },
                {
                    "_id": "690c081d60494e4fa7675620",
                    "name": "Zhiyu Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-05T14:37:34.000Z",
            "submittedOnDailyAt": "2025-11-11T00:42:05.247Z",
            "title": "HaluMem: Evaluating Hallucinations in Memory Systems of Agents",
            "submittedOnDailyBy": {
                "_id": "64e18e9ec20c27fcc8df384e",
                "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
                "isPro": false,
                "fullname": "Ding Chen",
                "user": "Hush-cd",
                "type": "user"
            },
            "summary": "Memory systems are key components that enable AI systems such as LLMs and AI\nagents to achieve long-term learning and sustained interaction. However, during\nmemory storage and retrieval, these systems frequently exhibit memory\nhallucinations, including fabrication, errors, conflicts, and omissions.\nExisting evaluations of memory hallucinations are primarily end-to-end question\nanswering, which makes it difficult to localize the operational stage within\nthe memory system where hallucinations arise. To address this, we introduce the\nHallucination in Memory Benchmark (HaluMem), the first operation level\nhallucination evaluation benchmark tailored to memory systems. HaluMem defines\nthree evaluation tasks (memory extraction, memory updating, and memory question\nanswering) to comprehensively reveal hallucination behaviors across different\noperational stages of interaction. To support evaluation, we construct\nuser-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and\nHaluMem-Long. Both include about 15k memory points and 3.5k multi-type\nquestions. The average dialogue length per user reaches 1.5k and 2.6k turns,\nwith context lengths exceeding 1M tokens, enabling evaluation of hallucinations\nacross different context scales and task complexities. Empirical studies based\non HaluMem show that existing memory systems tend to generate and accumulate\nhallucinations during the extraction and updating stages, which subsequently\npropagate errors to the question answering stage. Future research should focus\non developing interpretable and constrained memory operation mechanisms that\nsystematically suppress hallucinations and improve memory reliability.",
            "upvotes": 72,
            "discussionId": "690c081d60494e4fa7675621",
            "githubRepo": "https://github.com/MemTensor/HaluMem",
            "ai_summary": "HaluMem, a benchmark for evaluating memory hallucinations in AI systems, identifies and analyzes hallucinations across memory extraction, updating, and question answering stages using large-scale human-AI interaction datasets.",
            "ai_keywords": [
                "memory systems",
                "LLMs",
                "AI agents",
                "memory hallucinations",
                "fabrication",
                "errors",
                "conflicts",
                "omissions",
                "HaluMem",
                "operation level hallucination evaluation",
                "memory extraction",
                "memory updating",
                "memory question answering",
                "user-centric",
                "multi-turn human-AI interaction datasets",
                "HaluMem-Medium",
                "HaluMem-Long",
                "dialogue length",
                "context lengths",
                "token",
                "hallucinations",
                "memory reliability"
            ],
            "githubStars": 46,
            "organization": {
                "_id": "684d4f8e0bb9b6d7621cd53b",
                "name": "MemTensor",
                "fullname": "MemTensor",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62a155e615eeab266b2f2243/2mVH99TFqle9MJVb95aDC.jpeg"
            }
        },
        "publishedAt": "2025-11-05T09:37:34.000Z",
        "title": "HaluMem: Evaluating Hallucinations in Memory Systems of Agents",
        "summary": "Memory systems are key components that enable AI systems such as LLMs and AI\nagents to achieve long-term learning and sustained interaction. However, during\nmemory storage and retrieval, these systems frequently exhibit memory\nhallucinations, including fabrication, errors, conflicts, and omissions.\nExisting evaluations of memory hallucinations are primarily end-to-end question\nanswering, which makes it difficult to localize the operational stage within\nthe memory system where hallucinations arise. To address this, we introduce the\nHallucination in Memory Benchmark (HaluMem), the first operation level\nhallucination evaluation benchmark tailored to memory systems. HaluMem defines\nthree evaluation tasks (memory extraction, memory updating, and memory question\nanswering) to comprehensively reveal hallucination behaviors across different\noperational stages of interaction. To support evaluation, we construct\nuser-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and\nHaluMem-Long. Both include about 15k memory points and 3.5k multi-type\nquestions. The average dialogue length per user reaches 1.5k and 2.6k turns,\nwith context lengths exceeding 1M tokens, enabling evaluation of hallucinations\nacross different context scales and task complexities. Empirical studies based\non HaluMem show that existing memory systems tend to generate and accumulate\nhallucinations during the extraction and updating stages, which subsequently\npropagate errors to the question answering stage. Future research should focus\non developing interpretable and constrained memory operation mechanisms that\nsystematically suppress hallucinations and improve memory reliability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03506.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e18e9ec20c27fcc8df384e",
            "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
            "fullname": "Ding Chen",
            "name": "Hush-cd",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "684d4f8e0bb9b6d7621cd53b",
            "name": "MemTensor",
            "fullname": "MemTensor",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62a155e615eeab266b2f2243/2mVH99TFqle9MJVb95aDC.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.07327",
            "authors": [
                {
                    "_id": "6912b5eda644ba07c499c740",
                    "user": {
                        "_id": "63f06116f1a47aaea5bd497b",
                        "avatarUrl": "/avatars/7d99ffa59c4579599e852a0ffb261268.svg",
                        "isPro": false,
                        "fullname": "Guoxin Chen",
                        "user": "GuoxinChen",
                        "type": "user"
                    },
                    "name": "Guoxin Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:44:10.134Z",
                    "hidden": false
                },
                {
                    "_id": "6912b5eda644ba07c499c741",
                    "name": "Zile Qiao",
                    "hidden": false
                },
                {
                    "_id": "6912b5eda644ba07c499c742",
                    "user": {
                        "_id": "65e6970d135c27ea806526fe",
                        "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg",
                        "isPro": false,
                        "fullname": "Xuanzhong Chen",
                        "user": "chenxz",
                        "type": "user"
                    },
                    "name": "Xuanzhong Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:44:08.086Z",
                    "hidden": false
                },
                {
                    "_id": "6912b5eda644ba07c499c743",
                    "name": "Donglei Yu",
                    "hidden": false
                },
                {
                    "_id": "6912b5eda644ba07c499c744",
                    "name": "Haotian Xu",
                    "hidden": false
                },
                {
                    "_id": "6912b5eda644ba07c499c745",
                    "name": "Wayne Xin Zhao",
                    "hidden": false
                },
                {
                    "_id": "6912b5eda644ba07c499c746",
                    "name": "Ruihua Song",
                    "hidden": false
                },
                {
                    "_id": "6912b5eda644ba07c499c747",
                    "name": "Wenbiao Yin",
                    "hidden": false
                },
                {
                    "_id": "6912b5eda644ba07c499c748",
                    "name": "Huifeng Yin",
                    "hidden": false
                },
                {
                    "_id": "6912b5eda644ba07c499c749",
                    "name": "Liwen Zhang",
                    "hidden": false
                },
                {
                    "_id": "6912b5eda644ba07c499c74a",
                    "name": "Kuan Li",
                    "hidden": false
                },
                {
                    "_id": "6912b5eda644ba07c499c74b",
                    "name": "Minpeng Liao",
                    "hidden": false
                },
                {
                    "_id": "6912b5eda644ba07c499c74c",
                    "name": "Yong Jiang",
                    "hidden": false
                },
                {
                    "_id": "6912b5eda644ba07c499c74d",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "6912b5eda644ba07c499c74e",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "6912b5eda644ba07c499c74f",
                    "name": "Jingren Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T17:30:08.000Z",
            "submittedOnDailyAt": "2025-11-11T01:39:02.557Z",
            "title": "IterResearch: Rethinking Long-Horizon Agents via Markovian State\n  Reconstruction",
            "submittedOnDailyBy": {
                "_id": "63f06116f1a47aaea5bd497b",
                "avatarUrl": "/avatars/7d99ffa59c4579599e852a0ffb261268.svg",
                "isPro": false,
                "fullname": "Guoxin Chen",
                "user": "GuoxinChen",
                "type": "user"
            },
            "summary": "Recent advances in deep-research agents have shown promise for autonomous\nknowledge construction through dynamic reasoning over external sources.\nHowever, existing approaches rely on a mono-contextual paradigm that\naccumulates all information in a single, expanding context window, leading to\ncontext suffocation and noise contamination that limit their effectiveness on\nlong-horizon tasks. We introduce IterResearch, a novel iterative deep-research\nparadigm that reformulates long-horizon research as a Markov Decision Process\nwith strategic workspace reconstruction. By maintaining an evolving report as\nmemory and periodically synthesizing insights, our approach preserves\nconsistent reasoning capacity across arbitrary exploration depths. We further\ndevelop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning\nframework that incentivizes efficient exploration through geometric reward\ndiscounting and enables stable distributed training via adaptive downsampling.\nExtensive experiments demonstrate that IterResearch achieves substantial\nimprovements over existing open-source agents with average +14.5pp across six\nbenchmarks and narrows the gap with frontier proprietary systems. Remarkably,\nour paradigm exhibits unprecedented interaction scaling, extending to 2048\ninteractions with dramatic performance gains (from 3.5\\% to 42.5\\%), and serves\nas an effective prompting strategy, improving frontier models by up to 19.2pp\nover ReAct on long-horizon tasks. These findings position IterResearch as a\nversatile solution for long-horizon reasoning, effective both as a trained\nagent and as a prompting paradigm for frontier models.",
            "upvotes": 58,
            "discussionId": "6912b5eda644ba07c499c750",
            "ai_summary": "IterResearch, an iterative deep-research paradigm, improves long-horizon reasoning by reformulating it as a Markov Decision Process with strategic workspace reconstruction and Efficiency-Aware Policy Optimization, achieving better performance and interaction scaling compared to existing agents.",
            "ai_keywords": [
                "deep-research agents",
                "dynamic reasoning",
                "mono-contextual paradigm",
                "context suffocation",
                "noise contamination",
                "long-horizon tasks",
                "IterResearch",
                "Markov Decision Process",
                "strategic workspace reconstruction",
                "evolving report",
                "Efficiency-Aware Policy Optimization",
                "reinforcement learning",
                "geometric reward discounting",
                "adaptive downsampling",
                "prompting strategy",
                "ReAct"
            ]
        },
        "publishedAt": "2025-11-10T12:30:08.000Z",
        "title": "IterResearch: Rethinking Long-Horizon Agents via Markovian State\n  Reconstruction",
        "summary": "Recent advances in deep-research agents have shown promise for autonomous\nknowledge construction through dynamic reasoning over external sources.\nHowever, existing approaches rely on a mono-contextual paradigm that\naccumulates all information in a single, expanding context window, leading to\ncontext suffocation and noise contamination that limit their effectiveness on\nlong-horizon tasks. We introduce IterResearch, a novel iterative deep-research\nparadigm that reformulates long-horizon research as a Markov Decision Process\nwith strategic workspace reconstruction. By maintaining an evolving report as\nmemory and periodically synthesizing insights, our approach preserves\nconsistent reasoning capacity across arbitrary exploration depths. We further\ndevelop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning\nframework that incentivizes efficient exploration through geometric reward\ndiscounting and enables stable distributed training via adaptive downsampling.\nExtensive experiments demonstrate that IterResearch achieves substantial\nimprovements over existing open-source agents with average +14.5pp across six\nbenchmarks and narrows the gap with frontier proprietary systems. Remarkably,\nour paradigm exhibits unprecedented interaction scaling, extending to 2048\ninteractions with dramatic performance gains (from 3.5\\% to 42.5\\%), and serves\nas an effective prompting strategy, improving frontier models by up to 19.2pp\nover ReAct on long-horizon tasks. These findings position IterResearch as a\nversatile solution for long-horizon reasoning, effective both as a trained\nagent and as a prompting paradigm for frontier models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07327.png",
        "numComments": 6,
        "submittedBy": {
            "_id": "63f06116f1a47aaea5bd497b",
            "avatarUrl": "/avatars/7d99ffa59c4579599e852a0ffb261268.svg",
            "fullname": "Guoxin Chen",
            "name": "GuoxinChen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.06307",
            "authors": [
                {
                    "_id": "6912c5b6a644ba07c499c7ce",
                    "name": "Speed Zhu",
                    "hidden": false
                },
                {
                    "_id": "6912c5b6a644ba07c499c7cf",
                    "name": "Jianwei Cai",
                    "hidden": false
                },
                {
                    "_id": "6912c5b6a644ba07c499c7d0",
                    "user": {
                        "_id": "624445ed9fdefb55a0b9070b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/MS8EoLxeEXZnBoXEbMEnn.png",
                        "isPro": false,
                        "fullname": "GuangChen",
                        "user": "Guang2666",
                        "type": "user"
                    },
                    "name": "Guang Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:43:37.073Z",
                    "hidden": false
                },
                {
                    "_id": "6912c5b6a644ba07c499c7d1",
                    "name": "Lulu Wu",
                    "hidden": false
                },
                {
                    "_id": "6912c5b6a644ba07c499c7d2",
                    "name": "Saiyong Yang",
                    "hidden": false
                },
                {
                    "_id": "6912c5b6a644ba07c499c7d3",
                    "name": "Wiggin Zhou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/GA0zCucYiKF2baXLaOhE7.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/I8i9Vyp-OdQbeBiKmq9c8.png"
            ],
            "publishedAt": "2025-11-09T10:11:28.000Z",
            "submittedOnDailyAt": "2025-11-11T02:43:52.695Z",
            "title": "DRIVE: Data Curation Best Practices for Reinforcement Learning with\n  Verifiable Reward in Competitive Code Generation",
            "submittedOnDailyBy": {
                "_id": "64b74b906ab5d14ca7f289cd",
                "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
                "isPro": false,
                "fullname": "Chenchen Zhang",
                "user": "xxzcc",
                "type": "user"
            },
            "summary": "Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a\nresurgence of interest in RLVR. Nevertheless, advances are dominated by\nmathematics (e.g., AIME), with competitive-programming code generation\nunderexplored and data curation receiving less attention than RL algorithm\ndesign. We investigate how to construct RLVR datasets (i.e., RL prompts) and\npresent practical training techniques that yield strong performance on\ncompetitive-programming code generation. Our pipeline begins with supervised\nfine-tuning (SFT) distilled from strong open-source models, augmented with\ngeneral-purpose and reasoning-intensive data. RL then follows a two-stage\nprocess with executable, testcase-driven rewards: first, training on a large,\nuniformly distributed set of competitive-programming problems using Group\nRelative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively\nshort response-generation window (e.g., 32k during SFT and 24k in this stage)\nto expand entropy and mitigate repetition and truncation; second, we perform\nPre-GRPO: updating on a small, high-quality set of challenging\nproblems with a large rollout budget (64 rollouts per prompt) under a\nhard-focus curriculum that continuously retains the most difficult instances\nthroughout training. We implement our method on Qwen2.5-32B and evaluate on\nLeetCode and Codeforces weekly contests to avoid data leakage. The resulting\nmodel achieves state-of-the-art performance among models of similar scale and\nis comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking.\nWe also examine scaling trends and observe strong RL scaling on an internal\nlarge-scale MoE model. Our study distills concise best practices for data\ncuration, entropy expansion, and curriculum design in RLVR for\ncompetitive-programming code generation.",
            "upvotes": 41,
            "discussionId": "6912c5b6a644ba07c499c7d4",
            "ai_summary": "The study presents a two-stage reinforcement learning approach for competitive-programming code generation, achieving state-of-the-art performance using Group Relative Policy Optimization and a hard-focus curriculum.",
            "ai_keywords": [
                "RLVR",
                "supervised fine-tuning",
                "Group Relative Policy Optimization",
                "Pre-GRPO",
                "entropy expansion",
                "curriculum design",
                "competitive-programming code generation",
                "Qwen2.5-32B",
                "LeetCode",
                "Codeforces",
                "MoE model"
            ],
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-11-09T05:11:28.000Z",
        "title": "DRIVE: Data Curation Best Practices for Reinforcement Learning with\n  Verifiable Reward in Competitive Code Generation",
        "summary": "Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a\nresurgence of interest in RLVR. Nevertheless, advances are dominated by\nmathematics (e.g., AIME), with competitive-programming code generation\nunderexplored and data curation receiving less attention than RL algorithm\ndesign. We investigate how to construct RLVR datasets (i.e., RL prompts) and\npresent practical training techniques that yield strong performance on\ncompetitive-programming code generation. Our pipeline begins with supervised\nfine-tuning (SFT) distilled from strong open-source models, augmented with\ngeneral-purpose and reasoning-intensive data. RL then follows a two-stage\nprocess with executable, testcase-driven rewards: first, training on a large,\nuniformly distributed set of competitive-programming problems using Group\nRelative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively\nshort response-generation window (e.g., 32k during SFT and 24k in this stage)\nto expand entropy and mitigate repetition and truncation; second, we perform\nPre-GRPO: updating on a small, high-quality set of challenging\nproblems with a large rollout budget (64 rollouts per prompt) under a\nhard-focus curriculum that continuously retains the most difficult instances\nthroughout training. We implement our method on Qwen2.5-32B and evaluate on\nLeetCode and Codeforces weekly contests to avoid data leakage. The resulting\nmodel achieves state-of-the-art performance among models of similar scale and\nis comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking.\nWe also examine scaling trends and observe strong RL scaling on an internal\nlarge-scale MoE model. Our study distills concise best practices for data\ncuration, entropy expansion, and curriculum design in RLVR for\ncompetitive-programming code generation.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/GA0zCucYiKF2baXLaOhE7.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/I8i9Vyp-OdQbeBiKmq9c8.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06307.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "64b74b906ab5d14ca7f289cd",
            "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
            "fullname": "Chenchen Zhang",
            "name": "xxzcc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.06309",
            "authors": [
                {
                    "_id": "6912b989a644ba07c499c773",
                    "user": {
                        "_id": "67cef8b7d9f3ce4930069e10",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67cef8b7d9f3ce4930069e10/1o2rYNrmLPozUDaUo_nVE.png",
                        "isPro": false,
                        "fullname": "Sephen Chung",
                        "user": "stephenchungmh",
                        "type": "user"
                    },
                    "name": "Stephen Chung",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:43:48.700Z",
                    "hidden": false
                },
                {
                    "_id": "6912b989a644ba07c499c774",
                    "user": {
                        "_id": "624c3d2ca19f20b197761ba9",
                        "avatarUrl": "/avatars/7a64b81c29f4f6700fa18effc5616865.svg",
                        "isPro": false,
                        "fullname": "Wenyu Du",
                        "user": "wydu",
                        "type": "user"
                    },
                    "name": "Wenyu Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:43:46.514Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-09T10:13:00.000Z",
            "submittedOnDailyAt": "2025-11-11T01:50:33.244Z",
            "title": "The Station: An Open-World Environment for AI-Driven Discovery",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce the STATION, an open-world multi-agent environment that models a\nminiature scientific ecosystem. Leveraging their extended context windows,\nagents in the Station can engage in long scientific journeys that include\nreading papers from peers, formulating hypotheses, submitting code, performing\nanalyses, and publishing results. Importantly, there is no centralized system\ncoordinating their activities - agents are free to choose their own actions and\ndevelop their own narratives within the Station. Experiments demonstrate that\nAI agents in the Station achieve new state-of-the-art performance on a wide\nrange of benchmarks, spanning from mathematics to computational biology to\nmachine learning, notably surpassing AlphaEvolve in circle packing. A rich\ntapestry of narratives emerges as agents pursue independent research, interact\nwith peers, and build upon a cumulative history. From these emergent\nnarratives, novel methods arise organically, such as a new density-adaptive\nalgorithm for scRNA-seq batch integration. The Station marks a first step\ntowards autonomous scientific discovery driven by emergent behavior in an\nopen-world environment, representing a new paradigm that moves beyond rigid\noptimization.",
            "upvotes": 28,
            "discussionId": "6912b989a644ba07c499c775",
            "githubRepo": "https://github.com/dualverse-ai/station",
            "ai_summary": "AI agents in the STATION environment achieve state-of-the-art performance across various benchmarks through autonomous scientific discovery and emergent behavior.",
            "ai_keywords": [
                "STATION",
                "multi-agent environment",
                "scientific ecosystem",
                "context windows",
                "long scientific journeys",
                "reading papers",
                "formulating hypotheses",
                "submitting code",
                "performing analyses",
                "publishing results",
                "AlphaEvolve",
                "circle packing",
                "density-adaptive algorithm",
                "scRNA-seq batch integration",
                "autonomous scientific discovery",
                "emergent behavior",
                "open-world environment"
            ],
            "githubStars": 12
        },
        "publishedAt": "2025-11-09T05:13:00.000Z",
        "title": "The Station: An Open-World Environment for AI-Driven Discovery",
        "summary": "We introduce the STATION, an open-world multi-agent environment that models a\nminiature scientific ecosystem. Leveraging their extended context windows,\nagents in the Station can engage in long scientific journeys that include\nreading papers from peers, formulating hypotheses, submitting code, performing\nanalyses, and publishing results. Importantly, there is no centralized system\ncoordinating their activities - agents are free to choose their own actions and\ndevelop their own narratives within the Station. Experiments demonstrate that\nAI agents in the Station achieve new state-of-the-art performance on a wide\nrange of benchmarks, spanning from mathematics to computational biology to\nmachine learning, notably surpassing AlphaEvolve in circle packing. A rich\ntapestry of narratives emerges as agents pursue independent research, interact\nwith peers, and build upon a cumulative history. From these emergent\nnarratives, novel methods arise organically, such as a new density-adaptive\nalgorithm for scRNA-seq batch integration. The Station marks a first step\ntowards autonomous scientific discovery driven by emergent behavior in an\nopen-world environment, representing a new paradigm that moves beyond rigid\noptimization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06309.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 159
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.07250",
            "authors": [
                {
                    "_id": "6912b9b7a644ba07c499c777",
                    "name": "Tianhao Peng",
                    "hidden": false
                },
                {
                    "_id": "6912b9b7a644ba07c499c778",
                    "user": {
                        "_id": "6499809cf19fc795e7724e43",
                        "avatarUrl": "/avatars/4b3adce8c85e2f3ef05318ded6c89c3e.svg",
                        "isPro": false,
                        "fullname": "HaochenWang",
                        "user": "HaochenWang",
                        "type": "user"
                    },
                    "name": "Haochen Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:43:43.690Z",
                    "hidden": false
                },
                {
                    "_id": "6912b9b7a644ba07c499c779",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "6912b9b7a644ba07c499c77a",
                    "name": "Zekun Wang",
                    "hidden": false
                },
                {
                    "_id": "6912b9b7a644ba07c499c77b",
                    "name": "Zili Wang",
                    "hidden": false
                },
                {
                    "_id": "6912b9b7a644ba07c499c77c",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "6912b9b7a644ba07c499c77d",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "6912b9b7a644ba07c499c77e",
                    "name": "Shihao Li",
                    "hidden": false
                },
                {
                    "_id": "6912b9b7a644ba07c499c77f",
                    "name": "Yanghai Wang",
                    "hidden": false
                },
                {
                    "_id": "6912b9b7a644ba07c499c780",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "6912b9b7a644ba07c499c781",
                    "name": "Houyi Li",
                    "hidden": false
                },
                {
                    "_id": "6912b9b7a644ba07c499c782",
                    "name": "Wei Ji",
                    "hidden": false
                },
                {
                    "_id": "6912b9b7a644ba07c499c783",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "6912b9b7a644ba07c499c784",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "6912b9b7a644ba07c499c785",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6912b9b7a644ba07c499c786",
                    "name": "Jiaheng Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T16:02:33.000Z",
            "submittedOnDailyAt": "2025-11-11T01:52:05.353Z",
            "title": "MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal\n  LLMs",
            "submittedOnDailyBy": {
                "_id": "65377c30e48353201e6fdda0",
                "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
                "isPro": false,
                "fullname": "Jiaheng Liu",
                "user": "CheeryLJH",
                "type": "user"
            },
            "summary": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI\ncapabilities to visual modalities, yet existing evaluation benchmarks remain\nlimited to single-video understanding, overlooking the critical need for\nmulti-video understanding in real-world scenarios (e.g., sports analytics and\nautonomous driving). To address this significant gap, we introduce MVU-Eval,\nthe first comprehensive benchmark for evaluating Multi-Video Understanding for\nMLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies\nthrough 1,824 meticulously curated question-answer pairs spanning 4,959 videos\nfrom diverse domains, addressing both fundamental perception tasks and\nhigh-order reasoning tasks. These capabilities are rigorously aligned with\nreal-world applications such as multi-sensor synthesis in autonomous systems\nand cross-angle sports analytics. Through extensive evaluation of\nstate-of-the-art open-source and closed-source models, we reveal significant\nperformance discrepancies and limitations in current MLLMs' ability to perform\nunderstanding across multiple videos. The benchmark will be made publicly\navailable to foster future research.",
            "upvotes": 17,
            "discussionId": "6912b9b8a644ba07c499c787",
            "projectPage": "https://huggingface.co/datasets/MVU-Eval-Team/MVU-Eval-Data",
            "githubRepo": "https://github.com/NJU-LINK/MVU-Eval",
            "ai_summary": "MVU-Eval is a comprehensive benchmark for evaluating multi-video understanding in Multimodal Large Language Models, addressing gaps in existing single-video benchmarks and highlighting performance discrepancies in real-world applications.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "MVU-Eval",
                "Multi-Video Understanding",
                "question-answer pairs",
                "multi-sensor synthesis",
                "cross-angle sports analytics"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "68edc767abe005ac1b354573",
                "name": "NJU-LINK",
                "fullname": "NJU-LINK Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"
            }
        },
        "publishedAt": "2025-11-10T11:02:33.000Z",
        "title": "MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal\n  LLMs",
        "summary": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI\ncapabilities to visual modalities, yet existing evaluation benchmarks remain\nlimited to single-video understanding, overlooking the critical need for\nmulti-video understanding in real-world scenarios (e.g., sports analytics and\nautonomous driving). To address this significant gap, we introduce MVU-Eval,\nthe first comprehensive benchmark for evaluating Multi-Video Understanding for\nMLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies\nthrough 1,824 meticulously curated question-answer pairs spanning 4,959 videos\nfrom diverse domains, addressing both fundamental perception tasks and\nhigh-order reasoning tasks. These capabilities are rigorously aligned with\nreal-world applications such as multi-sensor synthesis in autonomous systems\nand cross-angle sports analytics. Through extensive evaluation of\nstate-of-the-art open-source and closed-source models, we reveal significant\nperformance discrepancies and limitations in current MLLMs' ability to perform\nunderstanding across multiple videos. The benchmark will be made publicly\navailable to foster future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07250.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65377c30e48353201e6fdda0",
            "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
            "fullname": "Jiaheng Liu",
            "name": "CheeryLJH",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "organization": {
            "_id": "68edc767abe005ac1b354573",
            "name": "NJU-LINK",
            "fullname": "NJU-LINK Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.07419",
            "authors": [
                {
                    "_id": "6912af1ca644ba07c499c71a",
                    "name": "Zhongyang Li",
                    "hidden": false
                },
                {
                    "_id": "6912af1ca644ba07c499c71b",
                    "name": "Ziyue Li",
                    "hidden": false
                },
                {
                    "_id": "6912af1ca644ba07c499c71c",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:44:12.162Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T18:59:53.000Z",
            "submittedOnDailyAt": "2025-11-11T01:21:40.975Z",
            "title": "Routing Manifold Alignment Improves Generalization of Mixture-of-Experts\n  LLMs",
            "submittedOnDailyBy": {
                "_id": "647f5af5b0e96764589f3b2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                "isPro": false,
                "fullname": "Tianyi Zhou",
                "user": "zhoutianyi",
                "type": "user"
            },
            "summary": "Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large\nlanguage models since it can efficiently scale up the model capability without\nincreasing the inference cost. However, evaluations on broad downstream tasks\nreveal a consistent suboptimality of the routers in existing MoE LLMs, which\nresults in a severe performance gap (e.g., 10-20% in accuracy) to the optimal\nrouting. In this paper, we show that aligning the manifold of routing weights\nwith that of task embedding can effectively reduce the gap and improve MoE\nLLMs' generalization performance. Our method, \"Routing Manifold Alignment\n(RoMA)\", introduces an additional manifold regularization term in the\npost-training objective and only requires lightweight finetuning of routers\n(with other parameters frozen). Specifically, the regularization encourages the\nrouting weights of each sample to be close to those of its successful neighbors\n(whose routing weights lead to correct answers) in a task embedding space.\nConsequently, samples targeting similar tasks will share similar expert choices\nacross layers. Building such bindings between tasks and experts over different\nsamples is essential to achieve better generalization. Moreover, RoMA\ndemonstrates the advantage of unifying the task understanding (by embedding\nmodels) with solution generation (by MoE LLMs). In experiments, we finetune\nrouters in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse\nbenchmarks and extensive comparisons with baselines show the substantial\nimprovement brought by RoMA.",
            "upvotes": 16,
            "discussionId": "6912af1da644ba07c499c71d",
            "githubRepo": "https://github.com/tianyi-lab/RoMA",
            "ai_summary": "Aligning routing weights with task embeddings in Sparse Mixture-of-Experts (MoE) models improves generalization and reduces performance gaps in large language models.",
            "ai_keywords": [
                "Sparse Mixture-of-Experts",
                "MoE",
                "routing weights",
                "task embedding",
                "manifold regularization",
                "RoMA",
                "OLMoE",
                "DeepSeekMoE",
                "Qwen3-MoE"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-11-10T13:59:53.000Z",
        "title": "Routing Manifold Alignment Improves Generalization of Mixture-of-Experts\n  LLMs",
        "summary": "Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large\nlanguage models since it can efficiently scale up the model capability without\nincreasing the inference cost. However, evaluations on broad downstream tasks\nreveal a consistent suboptimality of the routers in existing MoE LLMs, which\nresults in a severe performance gap (e.g., 10-20% in accuracy) to the optimal\nrouting. In this paper, we show that aligning the manifold of routing weights\nwith that of task embedding can effectively reduce the gap and improve MoE\nLLMs' generalization performance. Our method, \"Routing Manifold Alignment\n(RoMA)\", introduces an additional manifold regularization term in the\npost-training objective and only requires lightweight finetuning of routers\n(with other parameters frozen). Specifically, the regularization encourages the\nrouting weights of each sample to be close to those of its successful neighbors\n(whose routing weights lead to correct answers) in a task embedding space.\nConsequently, samples targeting similar tasks will share similar expert choices\nacross layers. Building such bindings between tasks and experts over different\nsamples is essential to achieve better generalization. Moreover, RoMA\ndemonstrates the advantage of unifying the task understanding (by embedding\nmodels) with solution generation (by MoE LLMs). In experiments, we finetune\nrouters in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse\nbenchmarks and extensive comparisons with baselines show the substantial\nimprovement brought by RoMA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07419.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "fullname": "Tianyi Zhou",
            "name": "zhoutianyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 18
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.07070",
            "authors": [
                {
                    "_id": "6912afc3a644ba07c499c72b",
                    "name": "Fei Zhao",
                    "hidden": false
                },
                {
                    "_id": "6912afc3a644ba07c499c72c",
                    "name": "Chonggang Lu",
                    "hidden": false
                },
                {
                    "_id": "6912afc3a644ba07c499c72d",
                    "name": "Haofu Qian",
                    "hidden": false
                },
                {
                    "_id": "6912afc3a644ba07c499c72e",
                    "name": "Fangcheng Shi",
                    "hidden": false
                },
                {
                    "_id": "6912afc3a644ba07c499c72f",
                    "name": "Zijie Meng",
                    "hidden": false
                },
                {
                    "_id": "6912afc3a644ba07c499c730",
                    "name": "Jianzhao Huang",
                    "hidden": false
                },
                {
                    "_id": "6912afc3a644ba07c499c731",
                    "name": "Xu Tang",
                    "hidden": false
                },
                {
                    "_id": "6912afc3a644ba07c499c732",
                    "name": "Zheyong Xie",
                    "hidden": false
                },
                {
                    "_id": "6912afc3a644ba07c499c733",
                    "name": "Zheyu Ye",
                    "hidden": false
                },
                {
                    "_id": "6912afc3a644ba07c499c734",
                    "name": "Zhe Xu",
                    "hidden": false
                },
                {
                    "_id": "6912afc3a644ba07c499c735",
                    "name": "Yao Hu",
                    "hidden": false
                },
                {
                    "_id": "6912afc3a644ba07c499c736",
                    "name": "Shaosheng Cao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T13:04:34.000Z",
            "submittedOnDailyAt": "2025-11-11T01:24:20.752Z",
            "title": "RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social\n  Networking Services",
            "submittedOnDailyBy": {
                "_id": "65328aa39326d6da5ff19b52",
                "avatarUrl": "/avatars/5c3de984cd6eba69616bb608796865c5.svg",
                "isPro": false,
                "fullname": "Fei Zhao",
                "user": "Hiiamein",
                "type": "user"
            },
            "summary": "As a key medium for human interaction and information exchange, social\nnetworking services (SNS) pose unique challenges for large language models\n(LLMs): heterogeneous workloads, fast-shifting norms and slang, and\nmultilingual, culturally diverse corpora that induce sharp distribution shift.\nSupervised fine-tuning (SFT) can specialize models but often triggers a\n``seesaw'' between in-distribution gains and out-of-distribution robustness,\nespecially for smaller models. To address these challenges, we introduce RedOne\n2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized\npost-training paradigm designed for rapid and stable adaptation. The pipeline\nconsist in three stages: (1) Exploratory Learning on curated SNS corpora to\nestablish initial alignment and identify systematic weaknesses; (2) Targeted\nFine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a\nsmall fraction of general data to mitigate forgetting; and (3) Refinement\nLearning that re-applies RL with SNS-centric signals to consolidate\nimprovements and harmonize trade-offs across tasks. Across various tasks\nspanning three categories, our 4B scale model delivers an average improvements\nabout 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves\naverage performance lift about 8.74 from the base model with less than half the\ndata required by SFT-centric method RedOne, evidencing superior data efficiency\nand stability at compact scales. Overall, RedOne 2.0 establishes a competitive,\ncost-effective baseline for domain-specific LLMs in SNS scenario, advancing\ncapability without sacrificing robustness.",
            "upvotes": 14,
            "discussionId": "6912afc4a644ba07c499c737",
            "ai_summary": "RedOne 2.0, a social networking service-oriented LLM, uses a progressive, RL-prioritized post-training paradigm to achieve rapid and stable adaptation, delivering improvements over larger baselines with less data.",
            "ai_keywords": [
                "supervised fine-tuning",
                "RL-prioritized",
                "exploratory learning",
                "targeted fine-tuning",
                "refinement learning",
                "domain-specific LLMs"
            ]
        },
        "publishedAt": "2025-11-10T08:04:34.000Z",
        "title": "RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social\n  Networking Services",
        "summary": "As a key medium for human interaction and information exchange, social\nnetworking services (SNS) pose unique challenges for large language models\n(LLMs): heterogeneous workloads, fast-shifting norms and slang, and\nmultilingual, culturally diverse corpora that induce sharp distribution shift.\nSupervised fine-tuning (SFT) can specialize models but often triggers a\n``seesaw'' between in-distribution gains and out-of-distribution robustness,\nespecially for smaller models. To address these challenges, we introduce RedOne\n2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized\npost-training paradigm designed for rapid and stable adaptation. The pipeline\nconsist in three stages: (1) Exploratory Learning on curated SNS corpora to\nestablish initial alignment and identify systematic weaknesses; (2) Targeted\nFine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a\nsmall fraction of general data to mitigate forgetting; and (3) Refinement\nLearning that re-applies RL with SNS-centric signals to consolidate\nimprovements and harmonize trade-offs across tasks. Across various tasks\nspanning three categories, our 4B scale model delivers an average improvements\nabout 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves\naverage performance lift about 8.74 from the base model with less than half the\ndata required by SFT-centric method RedOne, evidencing superior data efficiency\nand stability at compact scales. Overall, RedOne 2.0 establishes a competitive,\ncost-effective baseline for domain-specific LLMs in SNS scenario, advancing\ncapability without sacrificing robustness.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07070.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65328aa39326d6da5ff19b52",
            "avatarUrl": "/avatars/5c3de984cd6eba69616bb608796865c5.svg",
            "fullname": "Fei Zhao",
            "name": "Hiiamein",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.06411",
            "authors": [
                {
                    "_id": "6912a010a644ba07c499c6d6",
                    "user": {
                        "_id": "67a1d21e33e92b4a1183f3bb",
                        "avatarUrl": "/avatars/43f9dd3fcb7d58ddc69562fd1fc12957.svg",
                        "isPro": false,
                        "fullname": "Zhi Zheng",
                        "user": "zz1358m",
                        "type": "user"
                    },
                    "name": "Zhi Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:44:36.291Z",
                    "hidden": false
                },
                {
                    "_id": "6912a010a644ba07c499c6d7",
                    "name": "Wee Sun Lee",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67a1d21e33e92b4a1183f3bb/CKtkMN0gQnaBFvadEA7gD.png",
                "https://cdn-uploads.huggingface.co/production/uploads/67a1d21e33e92b4a1183f3bb/1oIZCE3feLsvZOJjMFR3t.png",
                "https://cdn-uploads.huggingface.co/production/uploads/67a1d21e33e92b4a1183f3bb/hveVcu19PMsF7cj-_B_z0.png"
            ],
            "publishedAt": "2025-11-09T14:55:50.000Z",
            "submittedOnDailyAt": "2025-11-11T02:47:22.154Z",
            "title": "SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via\n  Gumbel-Reparameterized Soft-Thinking Policy Optimization",
            "submittedOnDailyBy": {
                "_id": "67a1d21e33e92b4a1183f3bb",
                "avatarUrl": "/avatars/43f9dd3fcb7d58ddc69562fd1fc12957.svg",
                "isPro": false,
                "fullname": "Zhi Zheng",
                "user": "zz1358m",
                "type": "user"
            },
            "summary": "The soft-thinking paradigm for Large Language Model (LLM) reasoning can\noutperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in\nsome scenarios, underscoring its research and application value. However, while\nthe discrete-token CoT reasoning pattern can be reinforced through policy\noptimization algorithms such as group relative policy optimization (GRPO),\nextending the soft-thinking pattern with Reinforcement Learning (RL) remains\nchallenging. This difficulty stems from the complexities of injecting\nstochasticity into soft-thinking tokens and updating soft-thinking policies\naccordingly. As a result, previous attempts to combine soft-thinking with GRPO\ntypically underperform their discrete-token GRPO counterparts. To fully unlock\nthe potential of soft-thinking, this paper presents a novel policy optimization\nalgorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning\npattern. SofT-GRPO injects the Gumbel noise into logits, employs the\nGumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained\nembedding space, and leverages the reparameterization trick in policy gradient.\nWe conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and\nresults demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly\noutperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while\nexhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes\nand weights are available on https://github.com/zz1358m/SofT-GRPO-master",
            "upvotes": 13,
            "discussionId": "6912a010a644ba07c499c6d8",
            "githubRepo": "https://github.com/zz1358m/SofT-GRPO-master/tree/main",
            "ai_summary": "A novel policy optimization algorithm, SofT-GRPO, enhances soft-thinking in Large Language Models by integrating Gumbel noise and the Gumbel-Softmax technique, leading to improved performance over discrete-token methods.",
            "ai_keywords": [
                "soft-thinking",
                "Large Language Model (LLM)",
                "Chain-of-Thought (CoT)",
                "policy optimization",
                "group relative policy optimization (GRPO)",
                "Reinforcement Learning (RL)",
                "Gumbel noise",
                "Gumbel-Softmax",
                "reparameterization trick",
                "policy gradient"
            ],
            "githubStars": 23,
            "organization": {
                "_id": "6508ab2b349930913196378b",
                "name": "NationalUniversityofSingapore",
                "fullname": "National University of Singapore",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
            }
        },
        "publishedAt": "2025-11-09T09:55:50.000Z",
        "title": "SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via\n  Gumbel-Reparameterized Soft-Thinking Policy Optimization",
        "summary": "The soft-thinking paradigm for Large Language Model (LLM) reasoning can\noutperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in\nsome scenarios, underscoring its research and application value. However, while\nthe discrete-token CoT reasoning pattern can be reinforced through policy\noptimization algorithms such as group relative policy optimization (GRPO),\nextending the soft-thinking pattern with Reinforcement Learning (RL) remains\nchallenging. This difficulty stems from the complexities of injecting\nstochasticity into soft-thinking tokens and updating soft-thinking policies\naccordingly. As a result, previous attempts to combine soft-thinking with GRPO\ntypically underperform their discrete-token GRPO counterparts. To fully unlock\nthe potential of soft-thinking, this paper presents a novel policy optimization\nalgorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning\npattern. SofT-GRPO injects the Gumbel noise into logits, employs the\nGumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained\nembedding space, and leverages the reparameterization trick in policy gradient.\nWe conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and\nresults demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly\noutperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while\nexhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes\nand weights are available on https://github.com/zz1358m/SofT-GRPO-master",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67a1d21e33e92b4a1183f3bb/CKtkMN0gQnaBFvadEA7gD.png",
            "https://cdn-uploads.huggingface.co/production/uploads/67a1d21e33e92b4a1183f3bb/1oIZCE3feLsvZOJjMFR3t.png",
            "https://cdn-uploads.huggingface.co/production/uploads/67a1d21e33e92b4a1183f3bb/hveVcu19PMsF7cj-_B_z0.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06411.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67a1d21e33e92b4a1183f3bb",
            "avatarUrl": "/avatars/43f9dd3fcb7d58ddc69562fd1fc12957.svg",
            "fullname": "Zhi Zheng",
            "name": "zz1358m",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6508ab2b349930913196378b",
            "name": "NationalUniversityofSingapore",
            "fullname": "National University of Singapore",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.06209",
            "authors": [
                {
                    "_id": "6912f325a644ba07c499c854",
                    "name": "Jingwei Ni",
                    "hidden": false
                },
                {
                    "_id": "6912f325a644ba07c499c855",
                    "name": "Ekaterina Fadeeva",
                    "hidden": false
                },
                {
                    "_id": "6912f325a644ba07c499c856",
                    "name": "Tianyi Wu",
                    "hidden": false
                },
                {
                    "_id": "6912f325a644ba07c499c857",
                    "name": "Mubashara Akhtar",
                    "hidden": false
                },
                {
                    "_id": "6912f325a644ba07c499c858",
                    "name": "Jiaheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6912f325a644ba07c499c859",
                    "name": "Elliott Ash",
                    "hidden": false
                },
                {
                    "_id": "6912f325a644ba07c499c85a",
                    "name": "Markus Leippold",
                    "hidden": false
                },
                {
                    "_id": "6912f325a644ba07c499c85b",
                    "name": "Timothy Baldwin",
                    "hidden": false
                },
                {
                    "_id": "6912f325a644ba07c499c85c",
                    "name": "See-Kiong Ng",
                    "hidden": false
                },
                {
                    "_id": "6912f325a644ba07c499c85d",
                    "name": "Artem Shelmanov",
                    "hidden": false
                },
                {
                    "_id": "6912f325a644ba07c499c85e",
                    "name": "Mrinmaya Sachan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-09T03:38:29.000Z",
            "submittedOnDailyAt": "2025-11-11T06:01:25.860Z",
            "title": "Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps\n  via Uncertainty Heads",
            "submittedOnDailyBy": {
                "_id": "685b9b62e896e7627649bd2f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/YFxwvCmegiZHTaWif-xd1.png",
                "isPro": false,
                "fullname": "Tianyi Wu",
                "user": "tianyiwuhaha",
                "type": "user"
            },
            "summary": "Solving complex tasks usually requires LLMs to generate long multi-step\nreasoning chains. Previous work has shown that verifying the correctness of\nindividual reasoning steps can further improve the performance and efficiency\nof LLMs on such tasks and enhance solution interpretability. However, existing\nverification approaches, such as Process Reward Models (PRMs), are either\ncomputationally expensive, limited to specific domains, or require large-scale\nhuman or model-generated annotations. Thus, we propose a lightweight\nalternative for step-level reasoning verification based on data-driven\nuncertainty scores. We train transformer-based uncertainty quantification heads\n(UHeads) that use the internal states of a frozen LLM to estimate the\nuncertainty of its reasoning steps during generation. The approach is fully\nautomatic: target labels are generated either by another larger LLM (e.g.,\nDeepSeek R1) or in a self-supervised manner by the original model itself.\nUHeads are both effective and lightweight, containing less than 10M parameters.\nAcross multiple domains, including mathematics, planning, and general knowledge\nquestion answering, they match or even surpass the performance of PRMs that are\nup to 810x larger. Our findings suggest that the internal states of LLMs encode\ntheir uncertainty and can serve as reliable signals for reasoning verification,\noffering a promising direction toward scalable and generalizable introspective\nLLMs.",
            "upvotes": 12,
            "discussionId": "6912f326a644ba07c499c85f",
            "ai_summary": "Transformer-based uncertainty quantification heads improve step-level reasoning verification in LLMs by estimating uncertainty from internal states, offering a lightweight and scalable alternative to existing methods.",
            "ai_keywords": [
                "LLMs",
                "reasoning chains",
                "verification",
                "Process Reward Models",
                "uncertainty scores",
                "transformer-based",
                "uncertainty quantification heads",
                "UHeads",
                "self-supervised",
                "generalizable introspective LLMs"
            ]
        },
        "publishedAt": "2025-11-08T22:38:29.000Z",
        "title": "Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps\n  via Uncertainty Heads",
        "summary": "Solving complex tasks usually requires LLMs to generate long multi-step\nreasoning chains. Previous work has shown that verifying the correctness of\nindividual reasoning steps can further improve the performance and efficiency\nof LLMs on such tasks and enhance solution interpretability. However, existing\nverification approaches, such as Process Reward Models (PRMs), are either\ncomputationally expensive, limited to specific domains, or require large-scale\nhuman or model-generated annotations. Thus, we propose a lightweight\nalternative for step-level reasoning verification based on data-driven\nuncertainty scores. We train transformer-based uncertainty quantification heads\n(UHeads) that use the internal states of a frozen LLM to estimate the\nuncertainty of its reasoning steps during generation. The approach is fully\nautomatic: target labels are generated either by another larger LLM (e.g.,\nDeepSeek R1) or in a self-supervised manner by the original model itself.\nUHeads are both effective and lightweight, containing less than 10M parameters.\nAcross multiple domains, including mathematics, planning, and general knowledge\nquestion answering, they match or even surpass the performance of PRMs that are\nup to 810x larger. Our findings suggest that the internal states of LLMs encode\ntheir uncertainty and can serve as reliable signals for reasoning verification,\noffering a promising direction toward scalable and generalizable introspective\nLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06209.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "685b9b62e896e7627649bd2f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/YFxwvCmegiZHTaWif-xd1.png",
            "fullname": "Tianyi Wu",
            "name": "tianyiwuhaha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.07384",
            "authors": [
                {
                    "_id": "691312e3a644ba07c499c89d",
                    "user": {
                        "_id": "65255f1073a043e50d043641",
                        "avatarUrl": "/avatars/257085f01c439d7c84787a4e6d085b3d.svg",
                        "isPro": true,
                        "fullname": "Sean McLeish",
                        "user": "smcleish",
                        "type": "user"
                    },
                    "name": "Sean McLeish",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:41:40.749Z",
                    "hidden": false
                },
                {
                    "_id": "691312e3a644ba07c499c89e",
                    "user": {
                        "_id": "64a1b18b98fad0c8a5b04e3a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8EvZSqz_GumooxK0uHkyR.png",
                        "isPro": true,
                        "fullname": "Leon Li",
                        "user": "leonli66",
                        "type": "user"
                    },
                    "name": "Ang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:41:37.955Z",
                    "hidden": false
                },
                {
                    "_id": "691312e3a644ba07c499c89f",
                    "name": "John Kirchenbauer",
                    "hidden": false
                },
                {
                    "_id": "691312e3a644ba07c499c8a0",
                    "name": "Dayal Singh Kalra",
                    "hidden": false
                },
                {
                    "_id": "691312e3a644ba07c499c8a1",
                    "name": "Brian R. Bartoldson",
                    "hidden": false
                },
                {
                    "_id": "691312e3a644ba07c499c8a2",
                    "name": "Bhavya Kailkhura",
                    "hidden": false
                },
                {
                    "_id": "691312e3a644ba07c499c8a3",
                    "name": "Avi Schwarzschild",
                    "hidden": false
                },
                {
                    "_id": "691312e3a644ba07c499c8a4",
                    "name": "Jonas Geiping",
                    "hidden": false
                },
                {
                    "_id": "691312e3a644ba07c499c8a5",
                    "name": "Tom Goldstein",
                    "hidden": false
                },
                {
                    "_id": "691312e3a644ba07c499c8a6",
                    "name": "Micah Goldblum",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T18:43:07.000Z",
            "submittedOnDailyAt": "2025-11-11T09:48:30.712Z",
            "title": "Teaching Pretrained Language Models to Think Deeper with Retrofitted\n  Recurrence",
            "submittedOnDailyBy": {
                "_id": "65255f1073a043e50d043641",
                "avatarUrl": "/avatars/257085f01c439d7c84787a4e6d085b3d.svg",
                "isPro": true,
                "fullname": "Sean McLeish",
                "user": "smcleish",
                "type": "user"
            },
            "summary": "Recent advances in depth-recurrent language models show that recurrence can\ndecouple train-time compute and parameter count from test-time compute. In this\nwork, we study how to convert existing pretrained non-recurrent language models\ninto depth-recurrent models. We find that using a curriculum of recurrences to\nincrease the effective depth of the model over the course of training preserves\nperformance while reducing total computational cost. In our experiments, on\nmathematics, we observe that converting pretrained models to recurrent ones\nresults in better performance at a given compute budget than simply\npost-training the original non-recurrent language model.",
            "upvotes": 9,
            "discussionId": "691312e3a644ba07c499c8a7",
            "githubRepo": "https://github.com/mcleish7/retrofitting-recurrence",
            "ai_summary": "Converting pretrained non-recurrent language models to depth-recurrent models improves performance at a given compute budget using a curriculum of recurrences.",
            "ai_keywords": [
                "depth-recurrent language models",
                "recurrence",
                "curriculum of recurrences",
                "effective depth",
                "computational cost",
                "mathematics",
                "post-training"
            ],
            "githubStars": 14,
            "organization": {
                "_id": "63dbddbe06e5ca38798321bd",
                "name": "tomg-group-umd",
                "fullname": "Tom Goldstein's Lab at University of Maryland, College Park",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1675353480936-63d98af1897746d6496177df.png"
            }
        },
        "publishedAt": "2025-11-10T13:43:07.000Z",
        "title": "Teaching Pretrained Language Models to Think Deeper with Retrofitted\n  Recurrence",
        "summary": "Recent advances in depth-recurrent language models show that recurrence can\ndecouple train-time compute and parameter count from test-time compute. In this\nwork, we study how to convert existing pretrained non-recurrent language models\ninto depth-recurrent models. We find that using a curriculum of recurrences to\nincrease the effective depth of the model over the course of training preserves\nperformance while reducing total computational cost. In our experiments, on\nmathematics, we observe that converting pretrained models to recurrent ones\nresults in better performance at a given compute budget than simply\npost-training the original non-recurrent language model.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07384.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65255f1073a043e50d043641",
            "avatarUrl": "/avatars/257085f01c439d7c84787a4e6d085b3d.svg",
            "fullname": "Sean McLeish",
            "name": "smcleish",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "organization": {
            "_id": "63dbddbe06e5ca38798321bd",
            "name": "tomg-group-umd",
            "fullname": "Tom Goldstein's Lab at University of Maryland, College Park",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1675353480936-63d98af1897746d6496177df.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.07416",
            "authors": [
                {
                    "_id": "6912b819a644ba07c499c765",
                    "name": "Jiageng Mao",
                    "hidden": false
                },
                {
                    "_id": "6912b819a644ba07c499c766",
                    "name": "Sicheng He",
                    "hidden": false
                },
                {
                    "_id": "6912b819a644ba07c499c767",
                    "name": "Hao-Ning Wu",
                    "hidden": false
                },
                {
                    "_id": "6912b819a644ba07c499c768",
                    "name": "Yang You",
                    "hidden": false
                },
                {
                    "_id": "6912b819a644ba07c499c769",
                    "name": "Shuyang Sun",
                    "hidden": false
                },
                {
                    "_id": "6912b819a644ba07c499c76a",
                    "name": "Zhicheng Wang",
                    "hidden": false
                },
                {
                    "_id": "6912b819a644ba07c499c76b",
                    "name": "Yanan Bao",
                    "hidden": false
                },
                {
                    "_id": "6912b819a644ba07c499c76c",
                    "name": "Huizhong Chen",
                    "hidden": false
                },
                {
                    "_id": "6912b819a644ba07c499c76d",
                    "name": "Leonidas Guibas",
                    "hidden": false
                },
                {
                    "_id": "6912b819a644ba07c499c76e",
                    "name": "Vitor Guizilini",
                    "hidden": false
                },
                {
                    "_id": "6912b819a644ba07c499c76f",
                    "name": "Howard Zhou",
                    "hidden": false
                },
                {
                    "_id": "6912b819a644ba07c499c770",
                    "name": "Yue Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/f_PxBT_5OAmBdsChxO_cM.mp4"
            ],
            "publishedAt": "2025-11-10T18:59:07.000Z",
            "submittedOnDailyAt": "2025-11-11T01:44:22.734Z",
            "title": "Robot Learning from a Physical World Model",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce PhysWorld, a framework that enables robot learning from video\ngeneration through physical world modeling. Recent video generation models can\nsynthesize photorealistic visual demonstrations from language commands and\nimages, offering a powerful yet underexplored source of training signals for\nrobotics. However, directly retargeting pixel motions from generated videos to\nrobots neglects physics, often resulting in inaccurate manipulations. PhysWorld\naddresses this limitation by coupling video generation with physical world\nreconstruction. Given a single image and a task command, our method generates\ntask-conditioned videos and reconstructs the underlying physical world from the\nvideos, and the generated video motions are grounded into physically accurate\nactions through object-centric residual reinforcement learning with the\nphysical world model. This synergy transforms implicit visual guidance into\nphysically executable robotic trajectories, eliminating the need for real robot\ndata collection and enabling zero-shot generalizable robotic manipulation.\nExperiments on diverse real-world tasks demonstrate that PhysWorld\nsubstantially improves manipulation accuracy compared to previous approaches.\nVisit https://pointscoder.github.io/PhysWorld_Web/{the project webpage}\nfor details.",
            "upvotes": 8,
            "discussionId": "6912b819a644ba07c499c771",
            "projectPage": "https://pointscoder.github.io/PhysWorld_Web/",
            "githubRepo": "https://github.com/PointsCoder/OpenReal2Sim",
            "ai_summary": "PhysWorld integrates video generation and physical world modeling to enable accurate robotic manipulation from visual demonstrations without real robot data.",
            "ai_keywords": [
                "video generation",
                "physical world modeling",
                "task-conditioned videos",
                "object-centric residual reinforcement learning",
                "physically executable robotic trajectories",
                "zero-shot generalizable robotic manipulation"
            ],
            "githubStars": 24,
            "organization": {
                "_id": "60f6cbb2852126bac698c89e",
                "name": "deepmind",
                "fullname": "Deepmind",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
            }
        },
        "publishedAt": "2025-11-10T13:59:07.000Z",
        "title": "Robot Learning from a Physical World Model",
        "summary": "We introduce PhysWorld, a framework that enables robot learning from video\ngeneration through physical world modeling. Recent video generation models can\nsynthesize photorealistic visual demonstrations from language commands and\nimages, offering a powerful yet underexplored source of training signals for\nrobotics. However, directly retargeting pixel motions from generated videos to\nrobots neglects physics, often resulting in inaccurate manipulations. PhysWorld\naddresses this limitation by coupling video generation with physical world\nreconstruction. Given a single image and a task command, our method generates\ntask-conditioned videos and reconstructs the underlying physical world from the\nvideos, and the generated video motions are grounded into physically accurate\nactions through object-centric residual reinforcement learning with the\nphysical world model. This synergy transforms implicit visual guidance into\nphysically executable robotic trajectories, eliminating the need for real robot\ndata collection and enabling zero-shot generalizable robotic manipulation.\nExperiments on diverse real-world tasks demonstrate that PhysWorld\nsubstantially improves manipulation accuracy compared to previous approaches.\nVisit https://pointscoder.github.io/PhysWorld_Web/{the project webpage}\nfor details.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/f_PxBT_5OAmBdsChxO_cM.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07416.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 159
        },
        "organization": {
            "_id": "60f6cbb2852126bac698c89e",
            "name": "deepmind",
            "fullname": "Deepmind",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.06194",
            "authors": [
                {
                    "_id": "69131472a644ba07c499c8a9",
                    "user": {
                        "_id": "6623d8266046dd4b8575906f",
                        "avatarUrl": "/avatars/0ca98a2b24f598ce02bf23ddbc6a7ecf.svg",
                        "isPro": false,
                        "fullname": "Muhammad Usama",
                        "user": "MUsama100",
                        "type": "user"
                    },
                    "name": "Muhammad Usama",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:41:35.777Z",
                    "hidden": false
                },
                {
                    "_id": "69131472a644ba07c499c8aa",
                    "user": {
                        "_id": "65e07fb40d364b871d406648",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e07fb40d364b871d406648/3UAWI6jWhmjexzxLboEd4.webp",
                        "isPro": false,
                        "fullname": "Mohammad Sadil Khan",
                        "user": "SadilKhan",
                        "type": "user"
                    },
                    "name": "Mohammad Sadil Khan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:41:33.445Z",
                    "hidden": false
                },
                {
                    "_id": "69131472a644ba07c499c8ab",
                    "name": "Didier Stricker",
                    "hidden": false
                },
                {
                    "_id": "69131472a644ba07c499c8ac",
                    "name": "Muhammad Zeshan Afzal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-09T02:45:12.000Z",
            "submittedOnDailyAt": "2025-11-11T08:19:22.497Z",
            "title": "NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS\n  Modeling",
            "submittedOnDailyBy": {
                "_id": "65e07fb40d364b871d406648",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e07fb40d364b871d406648/3UAWI6jWhmjexzxLboEd4.webp",
                "isPro": false,
                "fullname": "Mohammad Sadil Khan",
                "user": "SadilKhan",
                "type": "user"
            },
            "summary": "Generating editable 3D CAD models from natural language remains challenging,\nas existing text-to-CAD systems either produce meshes or rely on scarce\ndesign-history data. We present NURBGen, the first framework to generate\nhigh-fidelity 3D CAD models directly from text using Non-Uniform Rational\nB-Splines (NURBS). To achieve this, we fine-tune a large language model (LLM)\nto translate free-form texts into JSON representations containing NURBS surface\nparameters (i.e, control points, knot vectors, degrees, and rational\nweights) which can be directly converted into BRep format using Python. We\nfurther propose a hybrid representation that combines untrimmed NURBS with\nanalytic primitives to handle trimmed surfaces and degenerate regions more\nrobustly, while reducing token complexity. Additionally, we introduce partABC,\na curated subset of the ABC dataset consisting of individual CAD components,\nannotated with detailed captions using an automated annotation pipeline.\nNURBGen demonstrates strong performance on diverse prompts, surpassing prior\nmethods in geometric fidelity and dimensional accuracy, as confirmed by expert\nevaluations. Code and dataset will be released publicly.",
            "upvotes": 6,
            "discussionId": "69131472a644ba07c499c8ad",
            "ai_summary": "NURBGen generates high-fidelity 3D CAD models from text using Non-Uniform Rational B-Splines, outperforming existing methods in geometric fidelity and dimensional accuracy.",
            "ai_keywords": [
                "Non-Uniform Rational B-Splines",
                "NURBS",
                "large language model",
                "LLM",
                "JSON representations",
                "BRep format",
                "hybrid representation",
                "analytic primitives",
                "trimmed surfaces",
                "degenerate regions",
                "partABC",
                "ABC dataset",
                "automated annotation pipeline"
            ],
            "organization": {
                "_id": "68e3c1b372607eeeeeaa9662",
                "name": "dfki-av",
                "fullname": "Augmented Vision",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64da1dd841f0e6c0e9638b67/k24siR3YWnto5sMIRAwu9.png"
            }
        },
        "publishedAt": "2025-11-08T21:45:12.000Z",
        "title": "NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS\n  Modeling",
        "summary": "Generating editable 3D CAD models from natural language remains challenging,\nas existing text-to-CAD systems either produce meshes or rely on scarce\ndesign-history data. We present NURBGen, the first framework to generate\nhigh-fidelity 3D CAD models directly from text using Non-Uniform Rational\nB-Splines (NURBS). To achieve this, we fine-tune a large language model (LLM)\nto translate free-form texts into JSON representations containing NURBS surface\nparameters (i.e, control points, knot vectors, degrees, and rational\nweights) which can be directly converted into BRep format using Python. We\nfurther propose a hybrid representation that combines untrimmed NURBS with\nanalytic primitives to handle trimmed surfaces and degenerate regions more\nrobustly, while reducing token complexity. Additionally, we introduce partABC,\na curated subset of the ABC dataset consisting of individual CAD components,\nannotated with detailed captions using an automated annotation pipeline.\nNURBGen demonstrates strong performance on diverse prompts, surpassing prior\nmethods in geometric fidelity and dimensional accuracy, as confirmed by expert\nevaluations. Code and dataset will be released publicly.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06194.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e07fb40d364b871d406648",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e07fb40d364b871d406648/3UAWI6jWhmjexzxLboEd4.webp",
            "fullname": "Mohammad Sadil Khan",
            "name": "SadilKhan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "organization": {
            "_id": "68e3c1b372607eeeeeaa9662",
            "name": "dfki-av",
            "fullname": "Augmented Vision",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64da1dd841f0e6c0e9638b67/k24siR3YWnto5sMIRAwu9.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.07413",
            "authors": [
                {
                    "_id": "6912bc52a644ba07c499c7b0",
                    "user": {
                        "_id": "665e28741d30854dbbf62046",
                        "avatarUrl": "/avatars/78b7291107eff7180031094103bd0a8b.svg",
                        "isPro": false,
                        "fullname": "Yuxuan Sun",
                        "user": "yuxuans",
                        "type": "user"
                    },
                    "name": "Yuxuan Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:43:39.339Z",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7b1",
                    "name": "Manchen Wang",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7b2",
                    "user": {
                        "_id": "6445dc7e219dc6142a6a6861",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445dc7e219dc6142a6a6861/Ak_YV8V1NZpkF5AQnJVlJ.png",
                        "isPro": false,
                        "fullname": "Shengyi Qian",
                        "user": "shengyi-qian",
                        "type": "user"
                    },
                    "name": "Shengyi Qian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:54:33.507Z",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7b3",
                    "name": "William R. Wong",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7b4",
                    "name": "Eric Gan",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7b5",
                    "name": "Pierluca D'Oro",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7b6",
                    "name": "Alejandro Castillejo Munoz",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7b7",
                    "name": "Sneha Silwal",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7b8",
                    "name": "Pedro Matias",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7b9",
                    "name": "Nitin Kamra",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7ba",
                    "name": "Satwik Kottur",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7bb",
                    "name": "Nick Raines",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7bc",
                    "name": "Xuanyi Zhao",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7bd",
                    "name": "Joy Chen",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7be",
                    "name": "Joseph Greer",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7bf",
                    "name": "Andrea Madotto",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7c0",
                    "name": "Allen Bolourchi",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7c1",
                    "name": "James Valori",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7c2",
                    "name": "Kevin Carlberg",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7c3",
                    "name": "Karl Ridgeway",
                    "hidden": false
                },
                {
                    "_id": "6912bc52a644ba07c499c7c4",
                    "name": "Joseph Tighe",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T18:57:35.000Z",
            "submittedOnDailyAt": "2025-11-11T02:03:02.991Z",
            "title": "DigiData: Training and Evaluating General-Purpose Mobile Control Agents",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "AI agents capable of controlling user interfaces have the potential to\ntransform human interaction with digital devices. To accelerate this\ntransformation, two fundamental building blocks are essential: high-quality\ndatasets that enable agents to achieve complex and human-relevant goals, and\nrobust evaluation methods that allow researchers and practitioners to rapidly\nenhance agent performance. In this paper, we introduce DigiData, a large-scale,\nhigh-quality, diverse, multi-modal dataset designed for training mobile control\nagents. Unlike existing datasets, which derive goals from unstructured\ninteractions, DigiData is meticulously constructed through comprehensive\nexploration of app features, resulting in greater diversity and higher goal\ncomplexity. Additionally, we present DigiData-Bench, a benchmark for evaluating\nmobile control agents on real-world complex tasks. We demonstrate that the\ncommonly used step-accuracy metric falls short in reliably assessing mobile\ncontrol agents and, to address this, we propose dynamic evaluation protocols\nand AI-powered evaluations as rigorous alternatives for agent assessment. Our\ncontributions aim to significantly advance the development of mobile control\nagents, paving the way for more intuitive and effective human-device\ninteractions.",
            "upvotes": 4,
            "discussionId": "6912bc53a644ba07c499c7c5",
            "projectPage": "https://facebookresearch.github.io/DigiData/",
            "githubRepo": "https://github.com/facebookresearch/digidata",
            "ai_summary": "DigiData and DigiData-Bench advance mobile control agents by providing a diverse, high-quality dataset and dynamic evaluation protocols, respectively.",
            "ai_keywords": [
                "mobile control agents",
                "DigiData",
                "DigiData-Bench",
                "step-accuracy metric",
                "dynamic evaluation protocols",
                "AI-powered evaluations"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "publishedAt": "2025-11-10T13:57:35.000Z",
        "title": "DigiData: Training and Evaluating General-Purpose Mobile Control Agents",
        "summary": "AI agents capable of controlling user interfaces have the potential to\ntransform human interaction with digital devices. To accelerate this\ntransformation, two fundamental building blocks are essential: high-quality\ndatasets that enable agents to achieve complex and human-relevant goals, and\nrobust evaluation methods that allow researchers and practitioners to rapidly\nenhance agent performance. In this paper, we introduce DigiData, a large-scale,\nhigh-quality, diverse, multi-modal dataset designed for training mobile control\nagents. Unlike existing datasets, which derive goals from unstructured\ninteractions, DigiData is meticulously constructed through comprehensive\nexploration of app features, resulting in greater diversity and higher goal\ncomplexity. Additionally, we present DigiData-Bench, a benchmark for evaluating\nmobile control agents on real-world complex tasks. We demonstrate that the\ncommonly used step-accuracy metric falls short in reliably assessing mobile\ncontrol agents and, to address this, we propose dynamic evaluation protocols\nand AI-powered evaluations as rigorous alternatives for agent assessment. Our\ncontributions aim to significantly advance the development of mobile control\nagents, paving the way for more intuitive and effective human-device\ninteractions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07413.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 159
        },
        "organization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.07137",
            "authors": [
                {
                    "_id": "6912d8eaa644ba07c499c808",
                    "user": {
                        "_id": "670765da8d82328304a11503",
                        "avatarUrl": "/avatars/2cce012febe32dc195eabb168b58efeb.svg",
                        "isPro": false,
                        "fullname": "Shiqi Jiang",
                        "user": "Eriiiiiin",
                        "type": "user"
                    },
                    "name": "Shiqi Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:43:23.985Z",
                    "hidden": false
                },
                {
                    "_id": "6912d8eaa644ba07c499c809",
                    "user": {
                        "_id": "62c14609ac1b639c2d87192c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png",
                        "isPro": false,
                        "fullname": "SII-liangtianyi",
                        "user": "tianyilt",
                        "type": "user"
                    },
                    "name": "Tianyi Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:43:20.468Z",
                    "hidden": false
                },
                {
                    "_id": "6912d8eaa644ba07c499c80a",
                    "name": "Changbo Wang",
                    "hidden": false
                },
                {
                    "_id": "6912d8eaa644ba07c499c80b",
                    "name": "Chenhui Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T14:18:27.000Z",
            "submittedOnDailyAt": "2025-11-11T05:19:32.338Z",
            "title": "MPJudge: Towards Perceptual Assessment of Music-Induced Paintings",
            "submittedOnDailyBy": {
                "_id": "62c14609ac1b639c2d87192c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png",
                "isPro": false,
                "fullname": "SII-liangtianyi",
                "user": "tianyilt",
                "type": "user"
            },
            "summary": "Music induced painting is a unique artistic practice, where visual artworks\nare created under the influence of music. Evaluating whether a painting\nfaithfully reflects the music that inspired it poses a challenging perceptual\nassessment task. Existing methods primarily rely on emotion recognition models\nto assess the similarity between music and painting, but such models introduce\nconsiderable noise and overlook broader perceptual cues beyond emotion. To\naddress these limitations, we propose a novel framework for music induced\npainting assessment that directly models perceptual coherence between music and\nvisual art. We introduce MPD, the first large scale dataset of music painting\npairs annotated by domain experts based on perceptual coherence. To better\nhandle ambiguous cases, we further collect pairwise preference annotations.\nBuilding on this dataset, we present MPJudge, a model that integrates music\nfeatures into a visual encoder via a modulation based fusion mechanism. To\neffectively learn from ambiguous cases, we adopt Direct Preference Optimization\nfor training. Extensive experiments demonstrate that our method outperforms\nexisting approaches. Qualitative results further show that our model more\naccurately identifies music relevant regions in paintings.",
            "upvotes": 4,
            "discussionId": "6912d8eaa644ba07c499c80c",
            "ai_summary": "A novel framework MPJudge assesses music-induced paintings by integrating music features into a visual encoder using a modulation-based fusion mechanism, outperforming existing emotion recognition models.",
            "ai_keywords": [
                "perceptual coherence",
                "MPD",
                "MPJudge",
                "modulation-based fusion mechanism",
                "Direct Preference Optimization"
            ],
            "organization": {
                "_id": "62c1482aac1b639c2d873235",
                "name": "ECNU",
                "fullname": "East China Normal University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1656834075081-62c14609ac1b639c2d87192c.png"
            }
        },
        "publishedAt": "2025-11-10T09:18:27.000Z",
        "title": "MPJudge: Towards Perceptual Assessment of Music-Induced Paintings",
        "summary": "Music induced painting is a unique artistic practice, where visual artworks\nare created under the influence of music. Evaluating whether a painting\nfaithfully reflects the music that inspired it poses a challenging perceptual\nassessment task. Existing methods primarily rely on emotion recognition models\nto assess the similarity between music and painting, but such models introduce\nconsiderable noise and overlook broader perceptual cues beyond emotion. To\naddress these limitations, we propose a novel framework for music induced\npainting assessment that directly models perceptual coherence between music and\nvisual art. We introduce MPD, the first large scale dataset of music painting\npairs annotated by domain experts based on perceptual coherence. To better\nhandle ambiguous cases, we further collect pairwise preference annotations.\nBuilding on this dataset, we present MPJudge, a model that integrates music\nfeatures into a visual encoder via a modulation based fusion mechanism. To\neffectively learn from ambiguous cases, we adopt Direct Preference Optimization\nfor training. Extensive experiments demonstrate that our method outperforms\nexisting approaches. Qualitative results further show that our model more\naccurately identifies music relevant regions in paintings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07137.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c14609ac1b639c2d87192c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png",
            "fullname": "SII-liangtianyi",
            "name": "tianyilt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "62c1482aac1b639c2d873235",
            "name": "ECNU",
            "fullname": "East China Normal University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1656834075081-62c14609ac1b639c2d87192c.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.07025",
            "authors": [
                {
                    "_id": "69131915a644ba07c499c8af",
                    "user": {
                        "_id": "643f884da649e1a499446c11",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643f884da649e1a499446c11/4lmRszXeD1S62sfwUHg2D.jpeg",
                        "isPro": false,
                        "fullname": "Yauhen Babakhin",
                        "user": "ybabakhin",
                        "type": "user"
                    },
                    "name": "Yauhen Babakhin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:41:30.752Z",
                    "hidden": false
                },
                {
                    "_id": "69131915a644ba07c499c8b0",
                    "name": "Radek Osmulski",
                    "hidden": false
                },
                {
                    "_id": "69131915a644ba07c499c8b1",
                    "user": {
                        "_id": "6814af6d59c3d8d6f9b09f2b",
                        "avatarUrl": "/avatars/d9fa8404f258c96df1c500cffd10752f.svg",
                        "isPro": false,
                        "fullname": "Ronay Ak",
                        "user": "ronay-nv",
                        "type": "user"
                    },
                    "name": "Ronay Ak",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:41:27.473Z",
                    "hidden": false
                },
                {
                    "_id": "69131915a644ba07c499c8b2",
                    "name": "Gabriel Moreira",
                    "hidden": false
                },
                {
                    "_id": "69131915a644ba07c499c8b3",
                    "name": "Mengyao Xu",
                    "hidden": false
                },
                {
                    "_id": "69131915a644ba07c499c8b4",
                    "name": "Benedikt Schifferer",
                    "hidden": false
                },
                {
                    "_id": "69131915a644ba07c499c8b5",
                    "name": "Bo Liu",
                    "hidden": false
                },
                {
                    "_id": "69131915a644ba07c499c8b6",
                    "name": "Even Oldridge",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T12:13:16.000Z",
            "submittedOnDailyAt": "2025-11-11T12:57:47.039Z",
            "title": "Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for\n  Multilingual and Cross-Lingual Tasks",
            "submittedOnDailyBy": {
                "_id": "643f884da649e1a499446c11",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643f884da649e1a499446c11/4lmRszXeD1S62sfwUHg2D.jpeg",
                "isPro": false,
                "fullname": "Yauhen Babakhin",
                "user": "ybabakhin",
                "type": "user"
            },
            "summary": "We introduce llama-embed-nemotron-8b, an open-weights text embedding model\nthat achieves state-of-the-art performance on the Multilingual Massive Text\nEmbedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent\nmodels show strong performance, their training data or methodologies are often\nnot fully disclosed. We aim to address this by developing a fully open-source\nmodel, publicly releasing its weights and detailed ablation studies, and\nplanning to share the curated training datasets. Our model demonstrates\nsuperior performance across all major embedding tasks -- including retrieval,\nclassification and semantic textual similarity (STS) -- and excels in\nchallenging multilingual scenarios, such as low-resource languages and\ncross-lingual setups. This state-of-the-art performance is driven by a novel\ndata mix of 16.1 million query-document pairs, split between 7.7 million\nsamples from public datasets and 8.4 million synthetically generated examples\nfrom various open-weight LLMs. One of our key contributions is a detailed\nablation study analyzing core design choices, including a comparison of\ncontrastive loss implementations, an evaluation of synthetic data generation\n(SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b\nis an instruction-aware model, supporting user-defined instructions to enhance\nperformance for specific use-cases. This combination of top-tier performance,\nbroad applicability, and user-driven flexibility enables it to serve as a\nuniversal text embedding solution.",
            "upvotes": 4,
            "discussionId": "69131916a644ba07c499c8b7",
            "ai_summary": "A fully open-source text embedding model achieves state-of-the-art performance across embedding tasks, particularly in multilingual scenarios, through a novel data mix and detailed ablation studies.",
            "ai_keywords": [
                "text embedding model",
                "Multilingual Massive Text Embedding Benchmark (MMTEB)",
                "open-source",
                "ablation studies",
                "retrieval",
                "classification",
                "semantic textual similarity (STS)",
                "low-resource languages",
                "cross-lingual setups",
                "contrastive loss",
                "synthetic data generation (SDG)",
                "model merging",
                "instruction-aware model"
            ],
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-11-10T07:13:16.000Z",
        "title": "Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for\n  Multilingual and Cross-Lingual Tasks",
        "summary": "We introduce llama-embed-nemotron-8b, an open-weights text embedding model\nthat achieves state-of-the-art performance on the Multilingual Massive Text\nEmbedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent\nmodels show strong performance, their training data or methodologies are often\nnot fully disclosed. We aim to address this by developing a fully open-source\nmodel, publicly releasing its weights and detailed ablation studies, and\nplanning to share the curated training datasets. Our model demonstrates\nsuperior performance across all major embedding tasks -- including retrieval,\nclassification and semantic textual similarity (STS) -- and excels in\nchallenging multilingual scenarios, such as low-resource languages and\ncross-lingual setups. This state-of-the-art performance is driven by a novel\ndata mix of 16.1 million query-document pairs, split between 7.7 million\nsamples from public datasets and 8.4 million synthetically generated examples\nfrom various open-weight LLMs. One of our key contributions is a detailed\nablation study analyzing core design choices, including a comparison of\ncontrastive loss implementations, an evaluation of synthetic data generation\n(SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b\nis an instruction-aware model, supporting user-defined instructions to enhance\nperformance for specific use-cases. This combination of top-tier performance,\nbroad applicability, and user-driven flexibility enables it to serve as a\nuniversal text embedding solution.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07025.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643f884da649e1a499446c11",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643f884da649e1a499446c11/4lmRszXeD1S62sfwUHg2D.jpeg",
            "fullname": "Yauhen Babakhin",
            "name": "ybabakhin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.05705",
            "authors": [
                {
                    "_id": "6913700aa644ba07c499c928",
                    "name": "David Acuna",
                    "hidden": false
                },
                {
                    "_id": "6913700aa644ba07c499c929",
                    "name": "Chao-Han Huck Yang",
                    "hidden": false
                },
                {
                    "_id": "6913700aa644ba07c499c92a",
                    "name": "Yuntian Deng",
                    "hidden": false
                },
                {
                    "_id": "6913700aa644ba07c499c92b",
                    "user": {
                        "_id": "61703fa3dff0ef663e421ab5",
                        "avatarUrl": "/avatars/96172e2782e218bbbddfdf47f96c1ad4.svg",
                        "isPro": false,
                        "fullname": "Jaehun Jung",
                        "user": "Jaehun",
                        "type": "user"
                    },
                    "name": "Jaehun Jung",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:41:00.690Z",
                    "hidden": false
                },
                {
                    "_id": "6913700aa644ba07c499c92c",
                    "name": "Ximing Lu",
                    "hidden": false
                },
                {
                    "_id": "6913700aa644ba07c499c92d",
                    "name": "Prithviraj Ammanabrolu",
                    "hidden": false
                },
                {
                    "_id": "6913700aa644ba07c499c92e",
                    "name": "Hyunwoo Kim",
                    "hidden": false
                },
                {
                    "_id": "6913700aa644ba07c499c92f",
                    "name": "Yuan-Hong Liao",
                    "hidden": false
                },
                {
                    "_id": "6913700aa644ba07c499c930",
                    "name": "Yejin Choi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/629e1b71bb6419817ed7566c/O-fcFejizuqy5q8tYQhdx.png"
            ],
            "publishedAt": "2025-11-07T20:50:54.000Z",
            "submittedOnDailyAt": "2025-11-11T14:52:07.954Z",
            "title": "Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale",
            "submittedOnDailyBy": {
                "_id": "629e1b71bb6419817ed7566c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629e1b71bb6419817ed7566c/0ZCt-11eQtRDCOk9AozOp.jpeg",
                "isPro": false,
                "fullname": "Huck Yang",
                "user": "huckiyang",
                "type": "user"
            },
            "summary": "Recent progress in multimodal reasoning has been driven largely by undisclosed datasets and proprietary data synthesis recipes, leaving open questions about how to systematically build large-scale, vision-centric reasoning datasets, particularly for tasks that go beyond visual math. In this work, we introduce a new reasoning data generation framework spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions. The dataset also includes preference data and instruction prompts supporting both offline and online RL. Our synthesis framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning traces are then synthesized through a two-stage process that leverages VLMs and reasoning LLMs, producing CoT traces for VLMs that capture the richness and diverse cognitive behaviors found in frontier reasoning models. Remarkably, we show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data baselines across all evaluated vision-centric benchmarks, and even surpasses strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning (MMAU), demonstrating its effectiveness. Similarly, despite not containing videos or embodied visual data, we observe notable gains when evaluating on a single-evidence embodied QA benchmark (NiEH). Finally, we use our data to analyze the entire VLM post-training pipeline. Our empirical analysis highlights that (i) SFT on high-quality data with non-linear reasoning traces is essential for effective online RL, (ii) staged offline RL matches online RL's performance while reducing compute demands, and (iii) careful SFT on high quality data can substantially improve out-of-domain, cross-modality transfer.",
            "upvotes": 4,
            "discussionId": "6913700aa644ba07c499c931",
            "ai_summary": "A new reasoning data generation framework creates a large-scale vision-centric dataset with over 1M synthetic questions, enhancing performance across various benchmarks and improving cross-modality transfer.",
            "ai_keywords": [
                "multimodal reasoning",
                "reasoning data generation framework",
                "synthetic vision-centric questions",
                "preference data",
                "instruction prompts",
                "offline RL",
                "online RL",
                "VLMs",
                "reasoning LLMs",
                "CoT traces",
                "finetuning",
                "Qwen2.5-VL-7B",
                "MiMo-VL-7B-RL",
                "V* Bench",
                "CV-Bench",
                "MMStar-V",
                "MMLU-Pro",
                "MMAU",
                "NiEH",
                "SFT",
                "staged offline RL",
                "out-of-domain",
                "cross-modality transfer"
            ],
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-11-07T15:50:54.000Z",
        "title": "Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale",
        "summary": "Recent progress in multimodal reasoning has been driven largely by undisclosed datasets and proprietary data synthesis recipes, leaving open questions about how to systematically build large-scale, vision-centric reasoning datasets, particularly for tasks that go beyond visual math. In this work, we introduce a new reasoning data generation framework spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions. The dataset also includes preference data and instruction prompts supporting both offline and online RL. Our synthesis framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning traces are then synthesized through a two-stage process that leverages VLMs and reasoning LLMs, producing CoT traces for VLMs that capture the richness and diverse cognitive behaviors found in frontier reasoning models. Remarkably, we show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data baselines across all evaluated vision-centric benchmarks, and even surpasses strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning (MMAU), demonstrating its effectiveness. Similarly, despite not containing videos or embodied visual data, we observe notable gains when evaluating on a single-evidence embodied QA benchmark (NiEH). Finally, we use our data to analyze the entire VLM post-training pipeline. Our empirical analysis highlights that (i) SFT on high-quality data with non-linear reasoning traces is essential for effective online RL, (ii) staged offline RL matches online RL's performance while reducing compute demands, and (iii) careful SFT on high quality data can substantially improve out-of-domain, cross-modality transfer.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/629e1b71bb6419817ed7566c/O-fcFejizuqy5q8tYQhdx.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.05705.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "629e1b71bb6419817ed7566c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629e1b71bb6419817ed7566c/0ZCt-11eQtRDCOk9AozOp.jpeg",
            "fullname": "Huck Yang",
            "name": "huckiyang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.04285",
            "authors": [
                {
                    "_id": "6912a610a644ba07c499c6ed",
                    "name": "Zeng Zhiyuan",
                    "hidden": false
                },
                {
                    "_id": "6912a610a644ba07c499c6ee",
                    "name": "Jiashuo Liu",
                    "hidden": false
                },
                {
                    "_id": "6912a610a644ba07c499c6ef",
                    "name": "Zhangyue Yin",
                    "hidden": false
                },
                {
                    "_id": "6912a610a644ba07c499c6f0",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "6912a610a644ba07c499c6f1",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "6912a610a644ba07c499c6f2",
                    "name": "Xipeng Qiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-06T11:27:16.000Z",
            "submittedOnDailyAt": "2025-11-11T00:40:21.089Z",
            "title": "RLoop: An Self-Improving Framework for Reinforcement Learning with\n  Iterative Policy Initialization",
            "submittedOnDailyBy": {
                "_id": "61c351f0e58df959bde67301",
                "avatarUrl": "/avatars/30c1082060590db8ca7ee98a5dd3b83d.svg",
                "isPro": false,
                "fullname": "zhiyuan zeng",
                "user": "zyzeng",
                "type": "user"
            },
            "summary": "While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for\ntraining large reasoning models, its training dynamics harbor a critical\nchallenge: RL overfitting, where models gain training rewards but lose\ngeneralization. Our analysis reveals this is driven by policy\nover-specialization and catastrophic forgetting of diverse solutions generated\nduring training. Standard optimization discards this valuable inter-step policy\ndiversity. To address this, we introduce RLoop, a self-improving framework\nbuilt on iterative policy initialization. RLoop transforms the standard\ntraining process into a virtuous cycle: it first uses RL to explore the\nsolution space from a given policy, then filters the successful trajectories to\ncreate an expert dataset. This dataset is used via Rejection-sampling\nFine-Tuning (RFT) to refine the initial policy, creating a superior starting\npoint for the next iteration. This loop of exploration and exploitation via\niterative re-initialization effectively converts transient policy variations\ninto robust performance gains. Our experiments show RLoop mitigates forgetting\nand substantially improves generalization, boosting average accuracy by 9% and\npass@32 by over 15% compared to vanilla RL.",
            "upvotes": 4,
            "discussionId": "6912a610a644ba07c499c6f3",
            "ai_summary": "RLoop, a self-improving framework using iterative policy initialization and Rejection-sampling Fine-Tuning, mitigates overfitting and enhances generalization in Reinforcement Learning for Verifiable Rewards.",
            "ai_keywords": [
                "Reinforcement Learning for Verifiable Rewards",
                "RLVR",
                "RL overfitting",
                "policy over-specialization",
                "catastrophic forgetting",
                "iterative policy initialization",
                "RLoop",
                "Rejection-sampling Fine-Tuning",
                "RFT",
                "exploration",
                "exploitation",
                "transient policy variations",
                "robust performance gains",
                "pass@32"
            ]
        },
        "publishedAt": "2025-11-06T06:27:16.000Z",
        "title": "RLoop: An Self-Improving Framework for Reinforcement Learning with\n  Iterative Policy Initialization",
        "summary": "While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for\ntraining large reasoning models, its training dynamics harbor a critical\nchallenge: RL overfitting, where models gain training rewards but lose\ngeneralization. Our analysis reveals this is driven by policy\nover-specialization and catastrophic forgetting of diverse solutions generated\nduring training. Standard optimization discards this valuable inter-step policy\ndiversity. To address this, we introduce RLoop, a self-improving framework\nbuilt on iterative policy initialization. RLoop transforms the standard\ntraining process into a virtuous cycle: it first uses RL to explore the\nsolution space from a given policy, then filters the successful trajectories to\ncreate an expert dataset. This dataset is used via Rejection-sampling\nFine-Tuning (RFT) to refine the initial policy, creating a superior starting\npoint for the next iteration. This loop of exploration and exploitation via\niterative re-initialization effectively converts transient policy variations\ninto robust performance gains. Our experiments show RLoop mitigates forgetting\nand substantially improves generalization, boosting average accuracy by 9% and\npass@32 by over 15% compared to vanilla RL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04285.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61c351f0e58df959bde67301",
            "avatarUrl": "/avatars/30c1082060590db8ca7ee98a5dd3b83d.svg",
            "fullname": "zhiyuan zeng",
            "name": "zyzeng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.00710",
            "authors": [
                {
                    "_id": "690ba1d560494e4fa767559d",
                    "name": "Minghe Shen",
                    "hidden": false
                },
                {
                    "_id": "690ba1d560494e4fa767559e",
                    "name": "Zhuo Zhi",
                    "hidden": false
                },
                {
                    "_id": "690ba1d560494e4fa767559f",
                    "name": "Chonghan Liu",
                    "hidden": false
                },
                {
                    "_id": "690ba1d560494e4fa76755a0",
                    "name": "Shuo Xing",
                    "hidden": false
                },
                {
                    "_id": "690ba1d560494e4fa76755a1",
                    "name": "Zhengzhong Tu",
                    "hidden": false
                },
                {
                    "_id": "690ba1d560494e4fa76755a2",
                    "name": "Che Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-01T21:19:41.000Z",
            "submittedOnDailyAt": "2025-11-11T07:34:06.779Z",
            "title": "Ariadne: A Controllable Framework for Probing and Extending VLM\n  Reasoning Boundaries",
            "submittedOnDailyBy": {
                "_id": "631b9ff5824f2502e3557c7e",
                "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
                "isPro": true,
                "fullname": "liu",
                "user": "che111",
                "type": "user"
            },
            "summary": "While Vision-Language Models (VLMs) post-trained with Reinforcement Learning\n(RL) show impressive general reasoning, their evaluation is often confined to\nlanguage-dominant tasks (e.g., math). This raises a critical question: can RL\npost-training truly extend the inherent capability boundary of a base VLM,\nparticularly for visual-centric spatial tasks where it initially fails? To\ninvestigate this, we introduce Ariadne, a framework utilizing synthetic mazes\nfor multi-step spatial reasoning where task difficulty (e.g., path length,\nturns) is precisely controlled. We leverage this controllable environment to\ntrain VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a\ndifficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves\nover 50% accuracy on a problem set where the base model scored 0%,\ndemonstrating that our approach expands the model's initial capability\nboundary. To assess real-world viability, we evaluate out-of-distribution (OOD)\ngeneralization on practical benchmarks. Despite training only on synthetic maze\nsamples, Ariadne achieves significant zero-shot improvements, averaging 16% on\nMapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer\ntasks). These results confirm that our method not only broadens the model's\nfundamental limits but also enhances its generalization to real-world spatial\nreasoning. We acknowledge our study is limited to the post-training phase,\ngiven the opaqueness of pre-training data, and hope our research motivates\nfurther work on specialized, capability-extending alignment.",
            "upvotes": 4,
            "discussionId": "690ba1d560494e4fa76755a3",
            "projectPage": "https://mingheshen.github.io/Ariadne/",
            "ai_summary": "Ariadne, a framework using synthetic mazes and RLVR, expands VLMs' capability in visual-centric spatial reasoning and improves zero-shot generalization on real-world benchmarks.",
            "ai_keywords": [
                "Vision-Language Models",
                "Reinforcement Learning",
                "RLVR",
                "synthetic mazes",
                "multi-step spatial reasoning",
                "difficulty-aware curriculum",
                "out-of-distribution generalization",
                "MapBench",
                "ReasonMap"
            ]
        },
        "publishedAt": "2025-11-01T17:19:41.000Z",
        "title": "Ariadne: A Controllable Framework for Probing and Extending VLM\n  Reasoning Boundaries",
        "summary": "While Vision-Language Models (VLMs) post-trained with Reinforcement Learning\n(RL) show impressive general reasoning, their evaluation is often confined to\nlanguage-dominant tasks (e.g., math). This raises a critical question: can RL\npost-training truly extend the inherent capability boundary of a base VLM,\nparticularly for visual-centric spatial tasks where it initially fails? To\ninvestigate this, we introduce Ariadne, a framework utilizing synthetic mazes\nfor multi-step spatial reasoning where task difficulty (e.g., path length,\nturns) is precisely controlled. We leverage this controllable environment to\ntrain VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a\ndifficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves\nover 50% accuracy on a problem set where the base model scored 0%,\ndemonstrating that our approach expands the model's initial capability\nboundary. To assess real-world viability, we evaluate out-of-distribution (OOD)\ngeneralization on practical benchmarks. Despite training only on synthetic maze\nsamples, Ariadne achieves significant zero-shot improvements, averaging 16% on\nMapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer\ntasks). These results confirm that our method not only broadens the model's\nfundamental limits but also enhances its generalization to real-world spatial\nreasoning. We acknowledge our study is limited to the post-training phase,\ngiven the opaqueness of pre-training data, and hope our research motivates\nfurther work on specialized, capability-extending alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.00710.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631b9ff5824f2502e3557c7e",
            "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
            "fullname": "liu",
            "name": "che111",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.07317",
            "authors": [
                {
                    "_id": "6912bb2da644ba07c499c793",
                    "user": {
                        "_id": "64a85e23b6512b8328f9d9e2",
                        "avatarUrl": "/avatars/4a6b35752d3f76cb03278f52b3b43426.svg",
                        "isPro": false,
                        "fullname": "Zhiyuan Zeng",
                        "user": "ZhiyuanZeng",
                        "type": "user"
                    },
                    "name": "Zhiyuan Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:43:41.584Z",
                    "hidden": false
                },
                {
                    "_id": "6912bb2da644ba07c499c794",
                    "name": "Hamish Ivison",
                    "hidden": false
                },
                {
                    "_id": "6912bb2da644ba07c499c795",
                    "name": "Yiping Wang",
                    "hidden": false
                },
                {
                    "_id": "6912bb2da644ba07c499c796",
                    "name": "Lifan Yuan",
                    "hidden": false
                },
                {
                    "_id": "6912bb2da644ba07c499c797",
                    "name": "Shuyue Stella Li",
                    "hidden": false
                },
                {
                    "_id": "6912bb2da644ba07c499c798",
                    "name": "Zhuorui Ye",
                    "hidden": false
                },
                {
                    "_id": "6912bb2da644ba07c499c799",
                    "name": "Siting Li",
                    "hidden": false
                },
                {
                    "_id": "6912bb2da644ba07c499c79a",
                    "name": "Jacqueline He",
                    "hidden": false
                },
                {
                    "_id": "6912bb2da644ba07c499c79b",
                    "name": "Runlong Zhou",
                    "hidden": false
                },
                {
                    "_id": "6912bb2da644ba07c499c79c",
                    "name": "Tong Chen",
                    "hidden": false
                },
                {
                    "_id": "6912bb2da644ba07c499c79d",
                    "name": "Chenyang Zhao",
                    "hidden": false
                },
                {
                    "_id": "6912bb2da644ba07c499c79e",
                    "name": "Yulia Tsvetkov",
                    "hidden": false
                },
                {
                    "_id": "6912bb2da644ba07c499c79f",
                    "name": "Simon Shaolei Du",
                    "hidden": false
                },
                {
                    "_id": "6912bb2da644ba07c499c7a0",
                    "name": "Natasha Jaques",
                    "hidden": false
                },
                {
                    "_id": "6912bb2da644ba07c499c7a1",
                    "name": "Hao Peng",
                    "hidden": false
                },
                {
                    "_id": "6912bb2da644ba07c499c7a2",
                    "name": "Pang Wei Koh",
                    "hidden": false
                },
                {
                    "_id": "6912bb2da644ba07c499c7a3",
                    "name": "Hannaneh Hajishirzi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T17:18:35.000Z",
            "submittedOnDailyAt": "2025-11-11T10:47:10.684Z",
            "title": "RLVE: Scaling Up Reinforcement Learning for Language Models with\n  Adaptive Verifiable Environments",
            "submittedOnDailyBy": {
                "_id": "64a85e23b6512b8328f9d9e2",
                "avatarUrl": "/avatars/4a6b35752d3f76cb03278f52b3b43426.svg",
                "isPro": false,
                "fullname": "Zhiyuan Zeng",
                "user": "ZhiyuanZeng",
                "type": "user"
            },
            "summary": "We introduce Reinforcement Learning (RL) with Adaptive Verifiable\nEnvironments (RLVE), an approach using verifiable environments that\nprocedurally generate problems and provide algorithmically verifiable rewards,\nto scale up RL for language models (LMs). RLVE enables each verifiable\nenvironment to dynamically adapt its problem difficulty distribution to the\npolicy model's capabilities as training progresses. In contrast, static data\ndistributions often lead to vanishing learning signals when problems are either\ntoo easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a\nlarge-scale suite of 400 verifiable environments carefully developed through\nmanual environment engineering. Using RLVE-Gym, we show that environment\nscaling, i.e., expanding the collection of training environments, consistently\nimproves generalizable reasoning capabilities. RLVE with joint training across\nall 400 environments in RLVE-Gym yields a 3.37% absolute average improvement\nacross six reasoning benchmarks, starting from one of the strongest 1.5B\nreasoning LMs. By comparison, continuing this LM's original RL training yields\nonly a 0.49% average absolute gain despite using over 3x more compute. We\nrelease our code publicly.",
            "upvotes": 3,
            "discussionId": "6912bb2da644ba07c499c7a4",
            "githubRepo": "https://github.com/Zhiyuan-Zeng/RLVE",
            "ai_summary": "Reinforcement Learning with Adaptive Verifiable Environments (RLVE) improves language model reasoning by dynamically adjusting problem difficulty, outperforming static environments and traditional RL training.",
            "ai_keywords": [
                "Reinforcement Learning (RL)",
                "Adaptive Verifiable Environments (RLVE)",
                "verifiable environments",
                "procedural generation",
                "algorithmically verifiable rewards",
                "policy model",
                "problem difficulty distribution",
                "static data distributions",
                "vanishing learning signals",
                "RLVE-Gym",
                "environment scaling",
                "generalizable reasoning capabilities",
                "reasoning benchmarks",
                "language models (LMs)"
            ],
            "githubStars": 16
        },
        "publishedAt": "2025-11-10T12:18:35.000Z",
        "title": "RLVE: Scaling Up Reinforcement Learning for Language Models with\n  Adaptive Verifiable Environments",
        "summary": "We introduce Reinforcement Learning (RL) with Adaptive Verifiable\nEnvironments (RLVE), an approach using verifiable environments that\nprocedurally generate problems and provide algorithmically verifiable rewards,\nto scale up RL for language models (LMs). RLVE enables each verifiable\nenvironment to dynamically adapt its problem difficulty distribution to the\npolicy model's capabilities as training progresses. In contrast, static data\ndistributions often lead to vanishing learning signals when problems are either\ntoo easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a\nlarge-scale suite of 400 verifiable environments carefully developed through\nmanual environment engineering. Using RLVE-Gym, we show that environment\nscaling, i.e., expanding the collection of training environments, consistently\nimproves generalizable reasoning capabilities. RLVE with joint training across\nall 400 environments in RLVE-Gym yields a 3.37% absolute average improvement\nacross six reasoning benchmarks, starting from one of the strongest 1.5B\nreasoning LMs. By comparison, continuing this LM's original RL training yields\nonly a 0.49% average absolute gain despite using over 3x more compute. We\nrelease our code publicly.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07317.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a85e23b6512b8328f9d9e2",
            "avatarUrl": "/avatars/4a6b35752d3f76cb03278f52b3b43426.svg",
            "fullname": "Zhiyuan Zeng",
            "name": "ZhiyuanZeng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 0
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.07061",
            "authors": [
                {
                    "_id": "6912e3f3a644ba07c499c83c",
                    "user": {
                        "_id": "681227983a64768db17175ca",
                        "avatarUrl": "/avatars/2229fbf20cf8a03b8dda875da613905e.svg",
                        "isPro": false,
                        "fullname": "LiXinran",
                        "user": "LiXinran1",
                        "type": "user"
                    },
                    "name": "Xinran Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:43:15.742Z",
                    "hidden": false
                },
                {
                    "_id": "6912e3f3a644ba07c499c83d",
                    "name": "Xiujuan Xu",
                    "hidden": false
                },
                {
                    "_id": "6912e3f3a644ba07c499c83e",
                    "name": "Jiaqi Qiao",
                    "hidden": false
                },
                {
                    "_id": "6912e3f3a644ba07c499c83f",
                    "name": "Yu Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T12:52:11.000Z",
            "submittedOnDailyAt": "2025-11-11T04:52:11.193Z",
            "title": "Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and\n  Curriculum Learning",
            "submittedOnDailyBy": {
                "_id": "681227983a64768db17175ca",
                "avatarUrl": "/avatars/2229fbf20cf8a03b8dda875da613905e.svg",
                "isPro": false,
                "fullname": "LiXinran",
                "user": "LiXinran1",
                "type": "user"
            },
            "summary": "Emotion Recognition in Conversation (ERC) is a crucial task for understanding\nhuman emotions and enabling natural human-computer interaction. Although Large\nLanguage Models (LLMs) have recently shown great potential in this field, their\nability to capture the intrinsic connections between explicit and implicit\nemotions remains limited. We propose a novel ERC training framework, PRC-Emo,\nwhich integrates Prompt engineering, demonstration Retrieval, and Curriculum\nlearning, with the goal of exploring whether LLMs can effectively perceive\nemotions in conversational contexts. Specifically, we design emotion-sensitive\nprompt templates based on both explicit and implicit emotional cues to better\nguide the model in understanding the speaker's psychological states. We\nconstruct the first dedicated demonstration retrieval repository for ERC, which\nincludes training samples from widely used datasets, as well as high-quality\ndialogue examples generated by LLMs and manually verified. Moreover, we\nintroduce a curriculum learning strategy into the LoRA fine-tuning process,\nincorporating weighted emotional shifts between same-speaker and\ndifferent-speaker utterances to assign difficulty levels to dialogue samples,\nwhich are then organized in an easy-to-hard training sequence. Experimental\nresults on two benchmark datasets-- IEMOCAP and MELD --show that our method\nachieves new state-of-the-art (SOTA) performance, demonstrating the\neffectiveness and generalizability of our approach in improving LLM-based\nemotional understanding.",
            "upvotes": 3,
            "discussionId": "6912e3f4a644ba07c499c840",
            "githubRepo": "https://github.com/LiXinran6/PRC-Emo",
            "ai_summary": "A novel ERC training framework, PRC-Emo, integrates prompt engineering, demonstration retrieval, and curriculum learning to enhance LLMs' ability to perceive emotions in conversations, achieving state-of-the-art performance on benchmark datasets.",
            "ai_keywords": [
                "Prompt engineering",
                "demonstration retrieval",
                "curriculum learning",
                "LoRA fine-tuning",
                "emotion-sensitive prompt templates",
                "IEMOCAP",
                "MELD"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "68b830782b6b404b8318fe8e",
                "name": "dalian-university-of-technology",
                "fullname": "DaLian University of Technology",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b82cf0116141793335f750/N5laKTgqcFB6x_i8kzdTY.webp"
            }
        },
        "publishedAt": "2025-11-10T07:52:11.000Z",
        "title": "Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and\n  Curriculum Learning",
        "summary": "Emotion Recognition in Conversation (ERC) is a crucial task for understanding\nhuman emotions and enabling natural human-computer interaction. Although Large\nLanguage Models (LLMs) have recently shown great potential in this field, their\nability to capture the intrinsic connections between explicit and implicit\nemotions remains limited. We propose a novel ERC training framework, PRC-Emo,\nwhich integrates Prompt engineering, demonstration Retrieval, and Curriculum\nlearning, with the goal of exploring whether LLMs can effectively perceive\nemotions in conversational contexts. Specifically, we design emotion-sensitive\nprompt templates based on both explicit and implicit emotional cues to better\nguide the model in understanding the speaker's psychological states. We\nconstruct the first dedicated demonstration retrieval repository for ERC, which\nincludes training samples from widely used datasets, as well as high-quality\ndialogue examples generated by LLMs and manually verified. Moreover, we\nintroduce a curriculum learning strategy into the LoRA fine-tuning process,\nincorporating weighted emotional shifts between same-speaker and\ndifferent-speaker utterances to assign difficulty levels to dialogue samples,\nwhich are then organized in an easy-to-hard training sequence. Experimental\nresults on two benchmark datasets-- IEMOCAP and MELD --show that our method\nachieves new state-of-the-art (SOTA) performance, demonstrating the\neffectiveness and generalizability of our approach in improving LLM-based\nemotional understanding.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07061.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "681227983a64768db17175ca",
            "avatarUrl": "/avatars/2229fbf20cf8a03b8dda875da613905e.svg",
            "fullname": "LiXinran",
            "name": "LiXinran1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "68b830782b6b404b8318fe8e",
            "name": "dalian-university-of-technology",
            "fullname": "DaLian University of Technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b82cf0116141793335f750/N5laKTgqcFB6x_i8kzdTY.webp"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.06090",
            "authors": [
                {
                    "_id": "6912b9f9a644ba07c499c789",
                    "name": "Jeffrey Jian Ma",
                    "hidden": false
                },
                {
                    "_id": "6912b9f9a644ba07c499c78a",
                    "name": "Milad Hashemi",
                    "hidden": false
                },
                {
                    "_id": "6912b9f9a644ba07c499c78b",
                    "name": "Amir Yazdanbakhsh",
                    "hidden": false
                },
                {
                    "_id": "6912b9f9a644ba07c499c78c",
                    "name": "Kevin Swersky",
                    "hidden": false
                },
                {
                    "_id": "6912b9f9a644ba07c499c78d",
                    "name": "Ofir Press",
                    "hidden": false
                },
                {
                    "_id": "6912b9f9a644ba07c499c78e",
                    "name": "Enhui Li",
                    "hidden": false
                },
                {
                    "_id": "6912b9f9a644ba07c499c78f",
                    "name": "Vijay Janapa Reddi",
                    "hidden": false
                },
                {
                    "_id": "6912b9f9a644ba07c499c790",
                    "name": "Parthasarathy Ranganathan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-08T17:55:09.000Z",
            "submittedOnDailyAt": "2025-11-11T01:52:35.484Z",
            "title": "SWE-fficiency: Can Language Models Optimize Real-World Repositories on\n  Real Workloads?",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Optimizing the performance of large-scale software repositories demands\nexpertise in code reasoning and software engineering (SWE) to reduce runtime\nwhile preserving program correctness. However, most benchmarks emphasize what\nto fix rather than how to fix code. We introduce SWE-fficiency, a\nbenchmark for evaluating repository-level performance optimization on real\nworkloads. Our suite contains 498 tasks across nine widely used data-science,\nmachine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a\ncomplete codebase and a slow workload, an agent must investigate code\nsemantics, localize bottlenecks and relevant tests, and produce a patch that\nmatches or exceeds expert speedup while passing the same unit tests. To enable\nthis how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests\nfor performance-improving edits, combining keyword filtering, static analysis,\ncoverage tooling, and execution validation to both confirm expert speedup\nbaselines and identify relevant repository unit tests. Empirical evaluation of\nstate-of-the-art agents reveals significant underperformance. On average,\nagents achieve less than 0.15x the expert speedup: agents struggle in\nlocalizing optimization opportunities, reasoning about execution across\nfunctions, and maintaining correctness in proposed edits. We release the\nbenchmark and accompanying data pipeline to facilitate research on automated\nperformance engineering and long-horizon software reasoning.",
            "upvotes": 3,
            "discussionId": "6912b9faa644ba07c499c791",
            "projectPage": "https://swefficiency.com/"
        },
        "publishedAt": "2025-11-08T12:55:09.000Z",
        "title": "SWE-fficiency: Can Language Models Optimize Real-World Repositories on\n  Real Workloads?",
        "summary": "Optimizing the performance of large-scale software repositories demands\nexpertise in code reasoning and software engineering (SWE) to reduce runtime\nwhile preserving program correctness. However, most benchmarks emphasize what\nto fix rather than how to fix code. We introduce SWE-fficiency, a\nbenchmark for evaluating repository-level performance optimization on real\nworkloads. Our suite contains 498 tasks across nine widely used data-science,\nmachine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a\ncomplete codebase and a slow workload, an agent must investigate code\nsemantics, localize bottlenecks and relevant tests, and produce a patch that\nmatches or exceeds expert speedup while passing the same unit tests. To enable\nthis how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests\nfor performance-improving edits, combining keyword filtering, static analysis,\ncoverage tooling, and execution validation to both confirm expert speedup\nbaselines and identify relevant repository unit tests. Empirical evaluation of\nstate-of-the-art agents reveals significant underperformance. On average,\nagents achieve less than 0.15x the expert speedup: agents struggle in\nlocalizing optimization opportunities, reasoning about execution across\nfunctions, and maintaining correctness in proposed edits. We release the\nbenchmark and accompanying data pipeline to facilitate research on automated\nperformance engineering and long-horizon software reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06090.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 159
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.05936",
            "authors": [
                {
                    "_id": "6912db4ea644ba07c499c80e",
                    "name": "Soujanya Poria",
                    "hidden": false
                },
                {
                    "_id": "6912db4ea644ba07c499c80f",
                    "name": "Navonil Majumder",
                    "hidden": false
                },
                {
                    "_id": "6912db4ea644ba07c499c810",
                    "name": "Chia-Yu Hung",
                    "hidden": false
                },
                {
                    "_id": "6912db4ea644ba07c499c811",
                    "name": "Amir Ali Bagherzadeh",
                    "hidden": false
                },
                {
                    "_id": "6912db4ea644ba07c499c812",
                    "name": "Chuan Li",
                    "hidden": false
                },
                {
                    "_id": "6912db4ea644ba07c499c813",
                    "name": "Kenneth Kwok",
                    "hidden": false
                },
                {
                    "_id": "6912db4ea644ba07c499c814",
                    "name": "Ziwei Wang",
                    "hidden": false
                },
                {
                    "_id": "6912db4ea644ba07c499c815",
                    "name": "Cheston Tan",
                    "hidden": false
                },
                {
                    "_id": "6912db4ea644ba07c499c816",
                    "name": "Jiajun Wu",
                    "hidden": false
                },
                {
                    "_id": "6912db4ea644ba07c499c817",
                    "name": "David Hsu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-08T09:02:13.000Z",
            "submittedOnDailyAt": "2025-11-11T04:49:33.265Z",
            "title": "10 Open Challenges Steering the Future of Vision-Language-Action Models",
            "submittedOnDailyBy": {
                "_id": "626b626405fe1cb65725aca1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/ZVSbhynzpQhVGq9kGywW6.png",
                "isPro": false,
                "fullname": "Soujanya Poria",
                "user": "soujanyaporia",
                "type": "user"
            },
            "summary": "Due to their ability of follow natural language instructions,\nvision-language-action (VLA) models are increasingly prevalent in the embodied\nAI arena, following the widespread success of their precursors -- LLMs and\nVLMs. In this paper, we discuss 10 principal milestones in the ongoing\ndevelopment of VLA models -- multimodality, reasoning, data, evaluation,\ncross-robot action generalization, efficiency, whole-body coordination, safety,\nagents, and coordination with humans. Furthermore, we discuss the emerging\ntrends of using spatial understanding, modeling world dynamics, post training,\nand data synthesis -- all aiming to reach these milestones. Through these\ndiscussions, we hope to bring attention to the research avenues that may\naccelerate the development of VLA models into wider acceptability.",
            "upvotes": 3,
            "discussionId": "6912db4fa644ba07c499c818",
            "ai_summary": "VLA models, combining vision, language, and action, are advancing through milestones like multimodality, reasoning, and safety, with trends focusing on spatial understanding and human coordination.",
            "ai_keywords": [
                "vision-language-action (VLA) models",
                "embodied AI",
                "multimodality",
                "reasoning",
                "data",
                "evaluation",
                "cross-robot action generalization",
                "efficiency",
                "whole-body coordination",
                "safety",
                "agents",
                "coordination with humans",
                "spatial understanding",
                "modeling world dynamics",
                "post training",
                "data synthesis"
            ],
            "organization": {
                "_id": "626ab9dac804c432c1b27a48",
                "name": "declare-lab",
                "fullname": "Deep Cognition and Language Research (DeCLaRe) Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/grq3rj2uj0WRjjPjAtR1I.png"
            }
        },
        "publishedAt": "2025-11-08T04:02:13.000Z",
        "title": "10 Open Challenges Steering the Future of Vision-Language-Action Models",
        "summary": "Due to their ability of follow natural language instructions,\nvision-language-action (VLA) models are increasingly prevalent in the embodied\nAI arena, following the widespread success of their precursors -- LLMs and\nVLMs. In this paper, we discuss 10 principal milestones in the ongoing\ndevelopment of VLA models -- multimodality, reasoning, data, evaluation,\ncross-robot action generalization, efficiency, whole-body coordination, safety,\nagents, and coordination with humans. Furthermore, we discuss the emerging\ntrends of using spatial understanding, modeling world dynamics, post training,\nand data synthesis -- all aiming to reach these milestones. Through these\ndiscussions, we hope to bring attention to the research avenues that may\naccelerate the development of VLA models into wider acceptability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.05936.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "626b626405fe1cb65725aca1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/ZVSbhynzpQhVGq9kGywW6.png",
            "fullname": "Soujanya Poria",
            "name": "soujanyaporia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "626ab9dac804c432c1b27a48",
            "name": "declare-lab",
            "fullname": "Deep Cognition and Language Research (DeCLaRe) Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/grq3rj2uj0WRjjPjAtR1I.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.03317",
            "authors": [
                {
                    "_id": "6912d73fa644ba07c499c7fa",
                    "name": "Minghao Fu",
                    "hidden": false
                },
                {
                    "_id": "6912d73fa644ba07c499c7fb",
                    "user": {
                        "_id": "636f4c6b5d2050767e4a1491",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636f4c6b5d2050767e4a1491/qErBLQnYzL35EkbDAhvqN.jpeg",
                        "isPro": false,
                        "fullname": "Guo-Hua Wang",
                        "user": "Flourish",
                        "type": "user"
                    },
                    "name": "Guo-Hua Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:43:26.420Z",
                    "hidden": false
                },
                {
                    "_id": "6912d73fa644ba07c499c7fc",
                    "name": "Tianyu Cui",
                    "hidden": false
                },
                {
                    "_id": "6912d73fa644ba07c499c7fd",
                    "name": "Qing-Guo Chen",
                    "hidden": false
                },
                {
                    "_id": "6912d73fa644ba07c499c7fe",
                    "name": "Zhao Xu",
                    "hidden": false
                },
                {
                    "_id": "6912d73fa644ba07c499c7ff",
                    "name": "Weihua Luo",
                    "hidden": false
                },
                {
                    "_id": "6912d73fa644ba07c499c800",
                    "name": "Kaifu Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-05T09:30:49.000Z",
            "submittedOnDailyAt": "2025-11-11T03:58:56.878Z",
            "title": "Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion\n  Models",
            "submittedOnDailyBy": {
                "_id": "636f4c6b5d2050767e4a1491",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636f4c6b5d2050767e4a1491/qErBLQnYzL35EkbDAhvqN.jpeg",
                "isPro": false,
                "fullname": "Guo-Hua Wang",
                "user": "Flourish",
                "type": "user"
            },
            "summary": "Text-to-image diffusion models deliver high-quality images, yet aligning them\nwith human preferences remains challenging. We revisit diffusion-based Direct\nPreference Optimization (DPO) for these models and identify a critical\npathology: enlarging the preference margin does not necessarily improve\ngeneration quality. In particular, the standard Diffusion-DPO objective can\nincrease the reconstruction error of both winner and loser branches.\nConsequently, degradation of the less-preferred outputs can become sufficiently\nsevere that the preferred branch is also adversely affected even as the margin\ngrows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule\nthat preserves the winner by adaptively scaling the loser gradient according to\nits alignment with the winner gradient. A first-order analysis yields a\nclosed-form scaling coefficient that guarantees the error of the preferred\noutput is non-increasing at each optimization step. Our method is simple,\nmodel-agnostic, broadly compatible with existing DPO-style alignment frameworks\nand adds only marginal computational overhead. Across standard text-to-image\nbenchmarks, Diffusion-SDPO delivers consistent gains over preference-learning\nbaselines on automated preference, aesthetic, and prompt alignment metrics.\nCode is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.",
            "upvotes": 3,
            "discussionId": "6912d73fa644ba07c499c801",
            "githubRepo": "https://github.com/AIDC-AI/Diffusion-SDPO",
            "ai_summary": "Diffusion-SDPO improves text-to-image generation quality by adaptively scaling the loser gradient in preference optimization, ensuring the preferred output's error does not increase.",
            "ai_keywords": [
                "diffusion models",
                "Direct Preference Optimization (DPO)",
                "preference margin",
                "reconstruction error",
                "winner branch",
                "loser branch",
                "Diffusion-SDPO",
                "safeguarded update rule",
                "adaptive scaling",
                "winner gradient",
                "loser gradient",
                "alignment",
                "closed-form scaling coefficient",
                "automated preference",
                "aesthetic",
                "prompt alignment metrics"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-11-05T04:30:49.000Z",
        "title": "Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion\n  Models",
        "summary": "Text-to-image diffusion models deliver high-quality images, yet aligning them\nwith human preferences remains challenging. We revisit diffusion-based Direct\nPreference Optimization (DPO) for these models and identify a critical\npathology: enlarging the preference margin does not necessarily improve\ngeneration quality. In particular, the standard Diffusion-DPO objective can\nincrease the reconstruction error of both winner and loser branches.\nConsequently, degradation of the less-preferred outputs can become sufficiently\nsevere that the preferred branch is also adversely affected even as the margin\ngrows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule\nthat preserves the winner by adaptively scaling the loser gradient according to\nits alignment with the winner gradient. A first-order analysis yields a\nclosed-form scaling coefficient that guarantees the error of the preferred\noutput is non-increasing at each optimization step. Our method is simple,\nmodel-agnostic, broadly compatible with existing DPO-style alignment frameworks\nand adds only marginal computational overhead. Across standard text-to-image\nbenchmarks, Diffusion-SDPO delivers consistent gains over preference-learning\nbaselines on automated preference, aesthetic, and prompt alignment metrics.\nCode is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03317.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636f4c6b5d2050767e4a1491",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636f4c6b5d2050767e4a1491/qErBLQnYzL35EkbDAhvqN.jpeg",
            "fullname": "Guo-Hua Wang",
            "name": "Flourish",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.07409",
            "authors": [
                {
                    "_id": "6912b1b4a644ba07c499c739",
                    "name": "Linzhan Mou",
                    "hidden": false
                },
                {
                    "_id": "6912b1b4a644ba07c499c73a",
                    "name": "Jiahui Lei",
                    "hidden": false
                },
                {
                    "_id": "6912b1b4a644ba07c499c73b",
                    "name": "Chen Wang",
                    "hidden": false
                },
                {
                    "_id": "6912b1b4a644ba07c499c73c",
                    "name": "Lingjie Liu",
                    "hidden": false
                },
                {
                    "_id": "6912b1b4a644ba07c499c73d",
                    "name": "Kostas Daniilidis",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/LtgESO51ssdDHDp7543DQ.mp4"
            ],
            "publishedAt": "2025-11-10T18:56:49.000Z",
            "submittedOnDailyAt": "2025-11-11T01:18:21.336Z",
            "title": "DIMO: Diverse 3D Motion Generation for Arbitrary Objects",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present DIMO, a generative approach capable of generating diverse 3D\nmotions for arbitrary objects from a single image. The core idea of our work is\nto leverage the rich priors in well-trained video models to extract the common\nmotion patterns and then embed them into a shared low-dimensional latent space.\nSpecifically, we first generate multiple videos of the same object with diverse\nmotions. We then embed each motion into a latent vector and train a shared\nmotion decoder to learn the distribution of motions represented by a structured\nand compact motion representation, i.e., neural key point trajectories. The\ncanonical 3D Gaussians are then driven by these key points and fused to model\nthe geometry and appearance. During inference time with learned latent space,\nwe can instantly sample diverse 3D motions in a single-forward pass and support\nseveral interesting applications including 3D motion interpolation and\nlanguage-guided motion generation. Our project page is available at\nhttps://linzhanm.github.io/dimo.",
            "upvotes": 2,
            "discussionId": "6912b1efa644ba07c499c73e",
            "projectPage": "https://linzhanm.github.io/dimo",
            "ai_summary": "A generative approach extracts motion patterns from video models, embeds them into a latent space, and uses neural key point trajectories to generate diverse 3D motions from a single image.",
            "ai_keywords": [
                "generative approach",
                "3D motions",
                "video models",
                "latent space",
                "motion decoder",
                "neural key point trajectories",
                "canonical 3D Gaussians",
                "3D motion interpolation",
                "language-guided motion generation"
            ]
        },
        "publishedAt": "2025-11-10T13:56:49.000Z",
        "title": "DIMO: Diverse 3D Motion Generation for Arbitrary Objects",
        "summary": "We present DIMO, a generative approach capable of generating diverse 3D\nmotions for arbitrary objects from a single image. The core idea of our work is\nto leverage the rich priors in well-trained video models to extract the common\nmotion patterns and then embed them into a shared low-dimensional latent space.\nSpecifically, we first generate multiple videos of the same object with diverse\nmotions. We then embed each motion into a latent vector and train a shared\nmotion decoder to learn the distribution of motions represented by a structured\nand compact motion representation, i.e., neural key point trajectories. The\ncanonical 3D Gaussians are then driven by these key points and fused to model\nthe geometry and appearance. During inference time with learned latent space,\nwe can instantly sample diverse 3D motions in a single-forward pass and support\nseveral interesting applications including 3D motion interpolation and\nlanguage-guided motion generation. Our project page is available at\nhttps://linzhanm.github.io/dimo.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/LtgESO51ssdDHDp7543DQ.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07409.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 159
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.06876",
            "authors": [
                {
                    "_id": "691305eca644ba07c499c878",
                    "name": "Eyal Gutflaish",
                    "hidden": false
                },
                {
                    "_id": "691305eca644ba07c499c879",
                    "name": "Eliran Kachlon",
                    "hidden": false
                },
                {
                    "_id": "691305eca644ba07c499c87a",
                    "user": {
                        "_id": "6488b7aa5cf73a16e7758b74",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6488b7aa5cf73a16e7758b74/MZ1pZ_ukGwabTMkaNv3OJ.jpeg",
                        "isPro": false,
                        "fullname": "Hezi Zisman",
                        "user": "H3ZI",
                        "type": "user"
                    },
                    "name": "Hezi Zisman",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:41:45.565Z",
                    "hidden": false
                },
                {
                    "_id": "691305eca644ba07c499c87b",
                    "name": "Tal Hacham",
                    "hidden": false
                },
                {
                    "_id": "691305eca644ba07c499c87c",
                    "name": "Nimrod Sarid",
                    "hidden": false
                },
                {
                    "_id": "691305eca644ba07c499c87d",
                    "name": "Alexander Visheratin",
                    "hidden": false
                },
                {
                    "_id": "691305eca644ba07c499c87e",
                    "name": "Saar Huberman",
                    "hidden": false
                },
                {
                    "_id": "691305eca644ba07c499c87f",
                    "name": "Gal Davidi",
                    "hidden": false
                },
                {
                    "_id": "691305eca644ba07c499c880",
                    "name": "Guy Bukchin",
                    "hidden": false
                },
                {
                    "_id": "691305eca644ba07c499c881",
                    "name": "Kfir Goldberg",
                    "hidden": false
                },
                {
                    "_id": "691305eca644ba07c499c882",
                    "user": {
                        "_id": "61647fc7d2a4f6328e734621",
                        "avatarUrl": "/avatars/5111133a889ffe4a7f33f1cf391fe4a9.svg",
                        "isPro": false,
                        "fullname": "Ron Mokady",
                        "user": "rmokady",
                        "type": "user"
                    },
                    "name": "Ron Mokady",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:41:43.302Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/61647fc7d2a4f6328e734621/WBA53ONGtaQS8xsDjeQ7H.jpeg"
            ],
            "publishedAt": "2025-11-10T09:25:25.000Z",
            "submittedOnDailyAt": "2025-11-11T17:37:17.989Z",
            "title": "Generating an Image From 1,000 Words: Enhancing Text-to-Image With\n  Structured Captions",
            "submittedOnDailyBy": {
                "_id": "61647fc7d2a4f6328e734621",
                "avatarUrl": "/avatars/5111133a889ffe4a7f33f1cf391fe4a9.svg",
                "isPro": false,
                "fullname": "Ron Mokady",
                "user": "rmokady",
                "type": "user"
            },
            "summary": "Text-to-image models have rapidly evolved from casual creative tools to\nprofessional-grade systems, achieving unprecedented levels of image quality and\nrealism. Yet, most models are trained to map short prompts into detailed\nimages, creating a gap between sparse textual input and rich visual outputs.\nThis mismatch reduces controllability, as models often fill in missing details\narbitrarily, biasing toward average user preferences and limiting precision for\nprofessional use. We address this limitation by training the first open-source\ntext-to-image model on long structured captions, where every training sample is\nannotated with the same set of fine-grained attributes. This design maximizes\nexpressive coverage and enables disentangled control over visual factors. To\nprocess long captions efficiently, we propose DimFusion, a fusion mechanism\nthat integrates intermediate tokens from a lightweight LLM without increasing\ntoken length. We also introduce the Text-as-a-Bottleneck Reconstruction (TaBR)\nevaluation protocol. By assessing how well real images can be reconstructed\nthrough a captioning-generation loop, TaBR directly measures controllability\nand expressiveness, even for very long captions where existing evaluation\nmethods fail. Finally, we demonstrate our contributions by training the\nlarge-scale model FIBO, achieving state-of-the-art prompt alignment among\nopen-source models. Model weights are publicly available at\nhttps://huggingface.co/briaai/FIBO",
            "upvotes": 2,
            "discussionId": "691305eda644ba07c499c883",
            "ai_summary": "A text-to-image model trained on long structured captions with DimFusion fusion mechanism and TaBR evaluation protocol achieves state-of-the-art prompt alignment and improved controllability.",
            "ai_keywords": [
                "text-to-image models",
                "long structured captions",
                "fine-grained attributes",
                "disentangled control",
                "DimFusion",
                "fusion mechanism",
                "lightweight LLM",
                "Text-as-a-Bottleneck Reconstruction",
                "TaBR evaluation protocol",
                "prompt alignment",
                "FIBO"
            ],
            "organization": {
                "_id": "65659a951cf463a71d953231",
                "name": "briaai",
                "fullname": "BRIA AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65659985cfbe8a857070d950/1HTn-HmGDwK53SSJ5dEYt.png"
            }
        },
        "publishedAt": "2025-11-10T04:25:25.000Z",
        "title": "Generating an Image From 1,000 Words: Enhancing Text-to-Image With\n  Structured Captions",
        "summary": "Text-to-image models have rapidly evolved from casual creative tools to\nprofessional-grade systems, achieving unprecedented levels of image quality and\nrealism. Yet, most models are trained to map short prompts into detailed\nimages, creating a gap between sparse textual input and rich visual outputs.\nThis mismatch reduces controllability, as models often fill in missing details\narbitrarily, biasing toward average user preferences and limiting precision for\nprofessional use. We address this limitation by training the first open-source\ntext-to-image model on long structured captions, where every training sample is\nannotated with the same set of fine-grained attributes. This design maximizes\nexpressive coverage and enables disentangled control over visual factors. To\nprocess long captions efficiently, we propose DimFusion, a fusion mechanism\nthat integrates intermediate tokens from a lightweight LLM without increasing\ntoken length. We also introduce the Text-as-a-Bottleneck Reconstruction (TaBR)\nevaluation protocol. By assessing how well real images can be reconstructed\nthrough a captioning-generation loop, TaBR directly measures controllability\nand expressiveness, even for very long captions where existing evaluation\nmethods fail. Finally, we demonstrate our contributions by training the\nlarge-scale model FIBO, achieving state-of-the-art prompt alignment among\nopen-source models. Model weights are publicly available at\nhttps://huggingface.co/briaai/FIBO",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/61647fc7d2a4f6328e734621/WBA53ONGtaQS8xsDjeQ7H.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06876.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61647fc7d2a4f6328e734621",
            "avatarUrl": "/avatars/5111133a889ffe4a7f33f1cf391fe4a9.svg",
            "fullname": "Ron Mokady",
            "name": "rmokady",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "65659a951cf463a71d953231",
            "name": "briaai",
            "fullname": "BRIA AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65659985cfbe8a857070d950/1HTn-HmGDwK53SSJ5dEYt.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.07299",
            "authors": [
                {
                    "_id": "6912dcc8a644ba07c499c81a",
                    "name": "Ying Cheng",
                    "hidden": false
                },
                {
                    "_id": "6912dcc8a644ba07c499c81b",
                    "name": "Yu-Ho Lin",
                    "hidden": false
                },
                {
                    "_id": "6912dcc8a644ba07c499c81c",
                    "user": {
                        "_id": "64ae22dd1aee69ece065cdcd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                        "isPro": false,
                        "fullname": "Min-Hung Chen",
                        "user": "cmhungsteve",
                        "type": "user"
                    },
                    "name": "Min-Hung Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:43:17.952Z",
                    "hidden": false
                },
                {
                    "_id": "6912dcc8a644ba07c499c81d",
                    "name": "Fu-En Yang",
                    "hidden": false
                },
                {
                    "_id": "6912dcc8a644ba07c499c81e",
                    "name": "Shang-Hong Lai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T16:56:11.000Z",
            "submittedOnDailyAt": "2025-11-11T04:21:57.296Z",
            "title": "VADER: Towards Causal Video Anomaly Understanding with Relation-Aware\n  Large Language Models",
            "submittedOnDailyBy": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
            },
            "summary": "Video anomaly understanding (VAU) aims to provide detailed interpretation and\nsemantic comprehension of anomalous events within videos, addressing\nlimitations of traditional methods that focus solely on detecting and\nlocalizing anomalies. However, existing approaches often neglect the deeper\ncausal relationships and interactions between objects, which are critical for\nunderstanding anomalous behaviors. In this paper, we propose VADER, an\nLLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe\nobject Relation features with visual cues to enhance anomaly comprehension from\nvideo. Specifically, VADER first applies an Anomaly Scorer to assign per-frame\nanomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture\nthe causal context of each anomalous event. A Relation Feature Extractor and a\nCOntrastive Relation Encoder (CORE) jointly model dynamic object interactions,\nproducing compact relational representations for downstream reasoning. These\nvisual and relational cues are integrated with LLMs to generate detailed,\ncausally grounded descriptions and support robust anomaly-related question\nanswering. Experiments on multiple real-world VAU benchmarks demonstrate that\nVADER achieves strong results across anomaly description, explanation, and\ncausal reasoning tasks, advancing the frontier of explainable video anomaly\nanalysis.",
            "upvotes": 1,
            "discussionId": "6912dcc8a644ba07c499c81f",
            "ai_summary": "VADER, an LLM-driven framework, enhances video anomaly understanding by integrating keyframe object relations and visual cues to provide detailed, causally grounded descriptions and robust question answering.",
            "ai_keywords": [
                "Anomaly Scorer",
                "Context-AwarE Sampling",
                "Relation Feature Extractor",
                "COntrastive Relation Encoder",
                "LLMs",
                "anomaly description",
                "explanation",
                "causal reasoning"
            ],
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-11-10T11:56:11.000Z",
        "title": "VADER: Towards Causal Video Anomaly Understanding with Relation-Aware\n  Large Language Models",
        "summary": "Video anomaly understanding (VAU) aims to provide detailed interpretation and\nsemantic comprehension of anomalous events within videos, addressing\nlimitations of traditional methods that focus solely on detecting and\nlocalizing anomalies. However, existing approaches often neglect the deeper\ncausal relationships and interactions between objects, which are critical for\nunderstanding anomalous behaviors. In this paper, we propose VADER, an\nLLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe\nobject Relation features with visual cues to enhance anomaly comprehension from\nvideo. Specifically, VADER first applies an Anomaly Scorer to assign per-frame\nanomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture\nthe causal context of each anomalous event. A Relation Feature Extractor and a\nCOntrastive Relation Encoder (CORE) jointly model dynamic object interactions,\nproducing compact relational representations for downstream reasoning. These\nvisual and relational cues are integrated with LLMs to generate detailed,\ncausally grounded descriptions and support robust anomaly-related question\nanswering. Experiments on multiple real-world VAU benchmarks demonstrate that\nVADER achieves strong results across anomaly description, explanation, and\ncausal reasoning tasks, advancing the frontier of explainable video anomaly\nanalysis.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07299.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ae22dd1aee69ece065cdcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
            "fullname": "Min-Hung Chen",
            "name": "cmhungsteve",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.07253",
            "authors": [
                {
                    "_id": "6912f39fa644ba07c499c861",
                    "user": {
                        "_id": "64903f017b630c141867877f",
                        "avatarUrl": "/avatars/3c7b248baed446ecc2b6adc2c444320d.svg",
                        "isPro": false,
                        "fullname": "Umberto Cappellazzo",
                        "user": "hisoka94",
                        "type": "user"
                    },
                    "name": "Umberto Cappellazzo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:43:13.094Z",
                    "hidden": false
                },
                {
                    "_id": "6912f39fa644ba07c499c862",
                    "name": "Xubo Liu",
                    "hidden": false
                },
                {
                    "_id": "6912f39fa644ba07c499c863",
                    "name": "Pingchuan Ma",
                    "hidden": false
                },
                {
                    "_id": "6912f39fa644ba07c499c864",
                    "name": "Stavros Petridis",
                    "hidden": false
                },
                {
                    "_id": "6912f39fa644ba07c499c865",
                    "name": "Maja Pantic",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T16:03:44.000Z",
            "submittedOnDailyAt": "2025-11-11T05:59:10.323Z",
            "title": "Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "64903f017b630c141867877f",
                "avatarUrl": "/avatars/3c7b248baed446ecc2b6adc2c444320d.svg",
                "isPro": false,
                "fullname": "Umberto Cappellazzo",
                "user": "hisoka94",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have recently achieved impressive results in\nspeech recognition across multiple modalities, including Auditory Speech\nRecognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech\nRecognition (AVSR). Despite this progress, current LLM-based approaches\ntypically address each task independently, training separate models that raise\ncomputational and deployment resource use while missing potential cross-task\nsynergies. They also rely on fixed-rate token compression, which restricts\nflexibility in balancing accuracy with efficiency. These limitations highlight\nthe need for a unified framework that can support ASR, VSR, and AVSR while\nenabling elastic inference. To this end, we present Omni-AVSR, a unified\naudio-visual LLM that combines efficient multi-granularity training with\nparameter-efficient adaptation. Specifically, we adapt the matryoshka\nrepresentation learning paradigm to efficiently train across multiple audio and\nvisual granularities, reducing its inherent training resource use. Furthermore,\nwe explore three LoRA-based strategies for adapting the backbone LLM, balancing\nshared and task-specific specialization. Experiments on LRS2 and LRS3 show that\nOmni-AVSR achieves comparable or superior accuracy to state-of-the-art\nbaselines while training a single model at substantially lower training and\ndeployment resource use. The model also remains robust under acoustic noise,\nand we analyze its scaling behavior as LLM size increases, providing insights\ninto the trade-off between performance and efficiency.",
            "upvotes": 1,
            "discussionId": "6912f39fa644ba07c499c866",
            "projectPage": "https://umbertocappellazzo.github.io/Omni-AVSR",
            "githubRepo": "https://github.com/umbertocappellazzo/Omni-AVSR",
            "ai_summary": "Omni-AVSR is a unified audio-visual LLM that efficiently supports ASR, VSR, and AVSR through multi-granularity training and parameter-efficient adaptation, achieving high accuracy with reduced resource use.",
            "ai_keywords": [
                "LLMs",
                "Auditory Speech Recognition (ASR)",
                "Visual Speech Recognition (VSR)",
                "Audio-Visual Speech Recognition (AVSR)",
                "matryoshka representation learning",
                "multi-granularity training",
                "parameter-efficient adaptation",
                "LoRA",
                "LRS2",
                "LRS3",
                "acoustic noise"
            ],
            "githubStars": 9,
            "organization": {
                "_id": "650987fc2feb9570c5137ac2",
                "name": "ImperialCollegeLondon",
                "fullname": "Imperial College London",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/u6ceSXXV6ldtt0qZOOMJw.jpeg"
            }
        },
        "publishedAt": "2025-11-10T11:03:44.000Z",
        "title": "Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large\n  Language Models",
        "summary": "Large language models (LLMs) have recently achieved impressive results in\nspeech recognition across multiple modalities, including Auditory Speech\nRecognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech\nRecognition (AVSR). Despite this progress, current LLM-based approaches\ntypically address each task independently, training separate models that raise\ncomputational and deployment resource use while missing potential cross-task\nsynergies. They also rely on fixed-rate token compression, which restricts\nflexibility in balancing accuracy with efficiency. These limitations highlight\nthe need for a unified framework that can support ASR, VSR, and AVSR while\nenabling elastic inference. To this end, we present Omni-AVSR, a unified\naudio-visual LLM that combines efficient multi-granularity training with\nparameter-efficient adaptation. Specifically, we adapt the matryoshka\nrepresentation learning paradigm to efficiently train across multiple audio and\nvisual granularities, reducing its inherent training resource use. Furthermore,\nwe explore three LoRA-based strategies for adapting the backbone LLM, balancing\nshared and task-specific specialization. Experiments on LRS2 and LRS3 show that\nOmni-AVSR achieves comparable or superior accuracy to state-of-the-art\nbaselines while training a single model at substantially lower training and\ndeployment resource use. The model also remains robust under acoustic noise,\nand we analyze its scaling behavior as LLM size increases, providing insights\ninto the trade-off between performance and efficiency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07253.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64903f017b630c141867877f",
            "avatarUrl": "/avatars/3c7b248baed446ecc2b6adc2c444320d.svg",
            "fullname": "Umberto Cappellazzo",
            "name": "hisoka94",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "650987fc2feb9570c5137ac2",
            "name": "ImperialCollegeLondon",
            "fullname": "Imperial College London",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/u6ceSXXV6ldtt0qZOOMJw.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.06174",
            "authors": [
                {
                    "_id": "6912be89a644ba07c499c7c7",
                    "name": "Zifan He",
                    "hidden": false
                },
                {
                    "_id": "6912be89a644ba07c499c7c8",
                    "name": "Shengyu Ye",
                    "hidden": false
                },
                {
                    "_id": "6912be89a644ba07c499c7c9",
                    "name": "Rui Ma",
                    "hidden": false
                },
                {
                    "_id": "6912be89a644ba07c499c7ca",
                    "name": "Yang Wang",
                    "hidden": false
                },
                {
                    "_id": "6912be89a644ba07c499c7cb",
                    "name": "Jason Cong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-09T01:17:08.000Z",
            "submittedOnDailyAt": "2025-11-11T02:12:48.170Z",
            "title": "LUT-LLM: Efficient Large Language Model Inference with Memory-based\n  Computations on FPGAs",
            "submittedOnDailyBy": {
                "_id": "6542bbc0bcc3731b0debc628",
                "avatarUrl": "/avatars/7bcbff0738cf7efb9fbff73084446ea6.svg",
                "isPro": false,
                "fullname": "Zifan He",
                "user": "OswaldHe123",
                "type": "user"
            },
            "summary": "The rapid progress of large language models (LLMs) has advanced numerous\napplications, yet efficient single-batch inference remains vital for on-device\nintelligence. While FPGAs offer fine-grained data control and high energy\nefficiency, recent GPU optimizations have narrowed their advantage, especially\nunder arithmetic-based computation. To overcome this, we leverage FPGAs'\nabundant on-chip memory to shift LLM inference from arithmetic- to memory-based\ncomputation through table lookups. We present LUT-LLM, the first FPGA\naccelerator enabling 1B+ LLM inference via vector-quantized memory operations.\nOur analysis identifies activation-weight co-quantization as the most effective\nscheme, supported by (1) bandwidth-aware parallel centroid search, (2)\nefficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing\ndata caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B\nmodel, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher\nenergy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency\ngain over A100.",
            "upvotes": 1,
            "discussionId": "6912be89a644ba07c499c7cc",
            "ai_summary": "LUT-LLM, an FPGA accelerator, improves LLM inference efficiency by shifting computation to memory-based operations, achieving lower latency and higher energy efficiency compared to GPUs.",
            "ai_keywords": [
                "LLMs",
                "FPGAs",
                "memory-based computation",
                "table lookups",
                "LUT-LLM",
                "vector-quantized memory operations",
                "activation-weight co-quantization",
                "bandwidth-aware parallel centroid search",
                "efficient 2D table lookups",
                "spatial-temporal hybrid design",
                "Qwen 3 1.7B",
                "AMD V80 FPGA",
                "AMD MI210",
                "NVIDIA A100"
            ]
        },
        "publishedAt": "2025-11-08T20:17:08.000Z",
        "title": "LUT-LLM: Efficient Large Language Model Inference with Memory-based\n  Computations on FPGAs",
        "summary": "The rapid progress of large language models (LLMs) has advanced numerous\napplications, yet efficient single-batch inference remains vital for on-device\nintelligence. While FPGAs offer fine-grained data control and high energy\nefficiency, recent GPU optimizations have narrowed their advantage, especially\nunder arithmetic-based computation. To overcome this, we leverage FPGAs'\nabundant on-chip memory to shift LLM inference from arithmetic- to memory-based\ncomputation through table lookups. We present LUT-LLM, the first FPGA\naccelerator enabling 1B+ LLM inference via vector-quantized memory operations.\nOur analysis identifies activation-weight co-quantization as the most effective\nscheme, supported by (1) bandwidth-aware parallel centroid search, (2)\nefficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing\ndata caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B\nmodel, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher\nenergy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency\ngain over A100.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06174.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6542bbc0bcc3731b0debc628",
            "avatarUrl": "/avatars/7bcbff0738cf7efb9fbff73084446ea6.svg",
            "fullname": "Zifan He",
            "name": "OswaldHe123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.05933",
            "authors": [
                {
                    "_id": "69137104a644ba07c499c933",
                    "name": "Renfei Zhang",
                    "hidden": false
                },
                {
                    "_id": "69137104a644ba07c499c934",
                    "name": "Manasa Kaniselvan",
                    "hidden": false
                },
                {
                    "_id": "69137104a644ba07c499c935",
                    "user": {
                        "_id": "642154542cc2b3c39e7f47e2",
                        "avatarUrl": "/avatars/78fa026109ff7cc96726e6e3cd581587.svg",
                        "isPro": false,
                        "fullname": "Niloofar  Mireshghallah",
                        "user": "niloofarm",
                        "type": "user"
                    },
                    "name": "Niloofar Mireshghallah",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:40:58.231Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/642154542cc2b3c39e7f47e2/35LxxuhN_5tNfPZBXyXV5.png"
            ],
            "publishedAt": "2025-11-08T08:56:29.000Z",
            "submittedOnDailyAt": "2025-11-11T15:11:01.281Z",
            "title": "Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs",
            "submittedOnDailyBy": {
                "_id": "642154542cc2b3c39e7f47e2",
                "avatarUrl": "/avatars/78fa026109ff7cc96726e6e3cd581587.svg",
                "isPro": false,
                "fullname": "Niloofar  Mireshghallah",
                "user": "niloofarm",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement \"code 57.95 refers to urinary infection\") maintain high cosine similarity between SFT and RL models, query representations (e.g., \"what is code 57.95\") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.",
            "upvotes": 1,
            "discussionId": "69137104a644ba07c499c936",
            "ai_summary": "Reinforcement learning enhances language models' ability to recall hierarchical knowledge without degrading memorized facts, as evidenced by improved performance on structured prompting and deep-retrieval tasks.",
            "ai_keywords": [
                "reinforcement learning",
                "language model",
                "procedural skills",
                "hierarchical knowledge",
                "structured prompting",
                "MedConceptsQA",
                "DeepSeek-V3/R1",
                "internal activation analysis",
                "factual representations",
                "query representations"
            ],
            "organization": {
                "_id": "655ab47d3f3e05685d569899",
                "name": "AI-at-Meta",
                "fullname": "Meta AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6451e763c5d273f95483d623/lMhwff5zPwW1IAltjDewc.png"
            }
        },
        "publishedAt": "2025-11-08T03:56:29.000Z",
        "title": "Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs",
        "summary": "Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement \"code 57.95 refers to urinary infection\") maintain high cosine similarity between SFT and RL models, query representations (e.g., \"what is code 57.95\") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/642154542cc2b3c39e7f47e2/35LxxuhN_5tNfPZBXyXV5.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.05933.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642154542cc2b3c39e7f47e2",
            "avatarUrl": "/avatars/78fa026109ff7cc96726e6e3cd581587.svg",
            "fullname": "Niloofar  Mireshghallah",
            "name": "niloofarm",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "655ab47d3f3e05685d569899",
            "name": "AI-at-Meta",
            "fullname": "Meta AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6451e763c5d273f95483d623/lMhwff5zPwW1IAltjDewc.png"
        },
        "isAuthorParticipating": true
    }
]