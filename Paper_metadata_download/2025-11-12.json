[
    {
        "paper": {
            "id": "2511.07332",
            "authors": [
                {
                    "_id": "6912b70ea644ba07c499c752",
                    "user": {
                        "_id": "62e98e784fa4bc6de6c0f65b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e98e784fa4bc6de6c0f65b/V8M3vr8yC4d5x4o2MfIhX.jpeg",
                        "isPro": false,
                        "fullname": "Aarash Feizi",
                        "user": "aarashfeizi",
                        "type": "user"
                    },
                    "name": "Aarash Feizi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:44:00.927Z",
                    "hidden": false
                },
                {
                    "_id": "6912b70ea644ba07c499c753",
                    "name": "Shravan Nayak",
                    "hidden": false
                },
                {
                    "_id": "6912b70ea644ba07c499c754",
                    "user": {
                        "_id": "636865b8cca0a0a962c21f3f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mja7cpws4gb2Jmdj_foPA.png",
                        "isPro": false,
                        "fullname": "Xiangru (Edward) Jian",
                        "user": "HideOnBush",
                        "type": "user"
                    },
                    "name": "Xiangru Jian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:44:03.406Z",
                    "hidden": false
                },
                {
                    "_id": "6912b70ea644ba07c499c755",
                    "user": {
                        "_id": "64440be5af034cdfd69ca3a7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
                        "isPro": false,
                        "fullname": "Qinghong (Kevin) Lin",
                        "user": "KevinQHLin",
                        "type": "user"
                    },
                    "name": "Kevin Qinghong Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:44:05.862Z",
                    "hidden": false
                },
                {
                    "_id": "6912b70ea644ba07c499c756",
                    "name": "Kaixin Li",
                    "hidden": false
                },
                {
                    "_id": "6912b70ea644ba07c499c757",
                    "user": {
                        "_id": "60edf5e03203a5daf7d3912e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60edf5e03203a5daf7d3912e/fq1AzGutKI_qojhevc03P.jpeg",
                        "isPro": false,
                        "fullname": "Rabiul Awal",
                        "user": "rabiulawal",
                        "type": "user"
                    },
                    "name": "Rabiul Awal",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:43:58.411Z",
                    "hidden": false
                },
                {
                    "_id": "6912b70ea644ba07c499c758",
                    "user": {
                        "_id": "5fa9ff3ea13e063b8b2b60cb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
                        "isPro": false,
                        "fullname": "Xing Han Lù",
                        "user": "xhluca",
                        "type": "user"
                    },
                    "name": "Xing Han Lù",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:43:50.993Z",
                    "hidden": false
                },
                {
                    "_id": "6912b70ea644ba07c499c759",
                    "user": {
                        "_id": "6478f30ea68454566353ef95",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478f30ea68454566353ef95/hX5NiVPyQbK8TIBKnT4J1.jpeg",
                        "isPro": false,
                        "fullname": "Johan Samir Obando Ceron",
                        "user": "johanobandoc",
                        "type": "user"
                    },
                    "name": "Johan Obando-Ceron",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:43:55.983Z",
                    "hidden": false
                },
                {
                    "_id": "6912b70ea644ba07c499c75a",
                    "name": "Juan A. Rodriguez",
                    "hidden": false
                },
                {
                    "_id": "6912b70ea644ba07c499c75b",
                    "name": "Nicolas Chapados",
                    "hidden": false
                },
                {
                    "_id": "6912b70ea644ba07c499c75c",
                    "name": "David Vazquez",
                    "hidden": false
                },
                {
                    "_id": "6912b70ea644ba07c499c75d",
                    "name": "Adriana Romero-Soriano",
                    "hidden": false
                },
                {
                    "_id": "6912b70ea644ba07c499c75e",
                    "name": "Reihaneh Rabbany",
                    "hidden": false
                },
                {
                    "_id": "6912b70ea644ba07c499c75f",
                    "name": "Perouz Taslakian",
                    "hidden": false
                },
                {
                    "_id": "6912b70ea644ba07c499c760",
                    "name": "Christopher Pal",
                    "hidden": false
                },
                {
                    "_id": "6912b70ea644ba07c499c761",
                    "name": "Spandana Gella",
                    "hidden": false
                },
                {
                    "_id": "6912b70ea644ba07c499c762",
                    "name": "Sai Rajeswar",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T17:35:21.000Z",
            "submittedOnDailyAt": "2025-11-12T01:32:27.759Z",
            "title": "Grounding Computer Use Agents on Human Demonstrations",
            "submittedOnDailyBy": {
                "_id": "62e98e784fa4bc6de6c0f65b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e98e784fa4bc6de6c0f65b/V8M3vr8yC4d5x4o2MfIhX.jpeg",
                "isPro": false,
                "fullname": "Aarash Feizi",
                "user": "aarashfeizi",
                "type": "user"
            },
            "summary": "Building reliable computer-use agents requires grounding: accurately\nconnecting natural language instructions to the correct on-screen elements.\nWhile large datasets exist for web and mobile interactions, high-quality\nresources for desktop environments are limited. To address this gap, we\nintroduce GroundCUA, a large-scale desktop grounding dataset built from expert\nhuman demonstrations. It covers 87 applications across 12 categories and\nincludes 56K screenshots, with every on-screen element carefully annotated for\na total of over 3.56M human-verified annotations. From these demonstrations, we\ngenerate diverse instructions that capture a wide range of real-world tasks,\nproviding high-quality data for model training. Using GroundCUA, we develop the\nGroundNext family of models that map instructions to their target UI elements.\nAt both 3B and 7B scales, GroundNext achieves state-of-the-art results across\nfive benchmarks using supervised fine-tuning, while requiring less than\none-tenth the training data of prior work. Reinforcement learning post-training\nfurther improves performance, and when evaluated in an agentic setting on the\nOSWorld benchmark using o3 as planner, GroundNext attains comparable or\nsuperior results to models trained with substantially more data,. These results\ndemonstrate the critical role of high-quality, expert-driven datasets in\nadvancing general-purpose computer-use agents.",
            "upvotes": 77,
            "discussionId": "6912b70fa644ba07c499c763",
            "projectPage": "https://groundcua.github.io/",
            "githubRepo": "https://github.com/ServiceNow/GroundCUA/",
            "ai_summary": "GroundCUA, a large-scale desktop grounding dataset, enables the development of GroundNext models that achieve state-of-the-art performance in mapping instructions to UI elements with less training data.",
            "ai_keywords": [
                "grounding",
                "natural language instructions",
                "on-screen elements",
                "desktop environments",
                "GroundCUA",
                "expert human demonstrations",
                "screenshots",
                "human-verified annotations",
                "GroundNext",
                "supervised fine-tuning",
                "reinforcement learning",
                "OSWorld benchmark",
                "o3 planner"
            ],
            "githubStars": 27,
            "organization": {
                "_id": "633497b475bed993246ff763",
                "name": "ServiceNow",
                "fullname": "ServiceNow",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664391313869-62f6691f329d4d014d1b4087.png"
            }
        },
        "publishedAt": "2025-11-10T12:35:21.000Z",
        "title": "Grounding Computer Use Agents on Human Demonstrations",
        "summary": "Building reliable computer-use agents requires grounding: accurately\nconnecting natural language instructions to the correct on-screen elements.\nWhile large datasets exist for web and mobile interactions, high-quality\nresources for desktop environments are limited. To address this gap, we\nintroduce GroundCUA, a large-scale desktop grounding dataset built from expert\nhuman demonstrations. It covers 87 applications across 12 categories and\nincludes 56K screenshots, with every on-screen element carefully annotated for\na total of over 3.56M human-verified annotations. From these demonstrations, we\ngenerate diverse instructions that capture a wide range of real-world tasks,\nproviding high-quality data for model training. Using GroundCUA, we develop the\nGroundNext family of models that map instructions to their target UI elements.\nAt both 3B and 7B scales, GroundNext achieves state-of-the-art results across\nfive benchmarks using supervised fine-tuning, while requiring less than\none-tenth the training data of prior work. Reinforcement learning post-training\nfurther improves performance, and when evaluated in an agentic setting on the\nOSWorld benchmark using o3 as planner, GroundNext attains comparable or\nsuperior results to models trained with substantially more data,. These results\ndemonstrate the critical role of high-quality, expert-driven datasets in\nadvancing general-purpose computer-use agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07332.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62e98e784fa4bc6de6c0f65b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e98e784fa4bc6de6c0f65b/V8M3vr8yC4d5x4o2MfIhX.jpeg",
            "fullname": "Aarash Feizi",
            "name": "aarashfeizi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "633497b475bed993246ff763",
            "name": "ServiceNow",
            "fullname": "ServiceNow",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664391313869-62f6691f329d4d014d1b4087.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.06221",
            "authors": [
                {
                    "_id": "6912a1c7a644ba07c499c6e1",
                    "user": {
                        "_id": "67486775ed2e4d9e50fc9117",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67486775ed2e4d9e50fc9117/WrCtPqY9X67ASbkUeloDF.jpeg",
                        "isPro": false,
                        "fullname": "Sen Xu",
                        "user": "SenXbjtu",
                        "type": "user"
                    },
                    "name": "Sen Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:44:27.021Z",
                    "hidden": false
                },
                {
                    "_id": "6912a1c7a644ba07c499c6e2",
                    "user": {
                        "_id": "6406991ec3ab325efa9b6732",
                        "avatarUrl": "/avatars/5ba29d9e25820c1172b5a98b078e416f.svg",
                        "isPro": false,
                        "fullname": "DenseHub",
                        "user": "YiZhouDenseHub",
                        "type": "user"
                    },
                    "name": "Yi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:44:32.245Z",
                    "hidden": false
                },
                {
                    "_id": "6912a1c7a644ba07c499c6e3",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "6912a1c7a644ba07c499c6e4",
                    "user": {
                        "_id": "6646fa14b096df5b522fd5f9",
                        "avatarUrl": "/avatars/98f67b6f8cb9776575916a2ba027f738.svg",
                        "isPro": false,
                        "fullname": "MIN JIXIN",
                        "user": "JIXIN0121",
                        "type": "user"
                    },
                    "name": "Jixin Min",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-12T12:21:33.041Z",
                    "hidden": false
                },
                {
                    "_id": "6912a1c7a644ba07c499c6e5",
                    "user": {
                        "_id": "64d1faaa1ed6649d70d1fa2f",
                        "avatarUrl": "/avatars/388ba18df077eaa8e16a89e59bf852fa.svg",
                        "isPro": false,
                        "fullname": "YinZhiBin",
                        "user": "YinZhiBin",
                        "type": "user"
                    },
                    "name": "Zhibin Yin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:44:34.446Z",
                    "hidden": false
                },
                {
                    "_id": "6912a1c7a644ba07c499c6e6",
                    "name": "Yingwei Dai",
                    "hidden": false
                },
                {
                    "_id": "6912a1c7a644ba07c499c6e7",
                    "name": "Shixi Liu",
                    "hidden": false
                },
                {
                    "_id": "6912a1c7a644ba07c499c6e8",
                    "user": {
                        "_id": "636b5433d524b220830e7a61",
                        "avatarUrl": "/avatars/09e9b392a189195aa09b93fc17b70cc3.svg",
                        "isPro": false,
                        "fullname": "pangly",
                        "user": "pangly",
                        "type": "user"
                    },
                    "name": "Lianyu Pang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-12T12:21:31.027Z",
                    "hidden": false
                },
                {
                    "_id": "6912a1c7a644ba07c499c6e9",
                    "name": "Yirong Chen",
                    "hidden": false
                },
                {
                    "_id": "6912a1c7a644ba07c499c6ea",
                    "user": {
                        "_id": "668b5090101353874ced73d0",
                        "avatarUrl": "/avatars/b2ec34a321890140e97ddd69884132a8.svg",
                        "isPro": false,
                        "fullname": "junlin zhang",
                        "user": "junlinzhang",
                        "type": "user"
                    },
                    "name": "Junlin Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:44:24.601Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6406991ec3ab325efa9b6732/4yVJjn-Y2UONHvzebYMkU.png"
            ],
            "publishedAt": "2025-11-09T04:37:36.000Z",
            "submittedOnDailyAt": "2025-11-12T00:53:44.764Z",
            "title": "Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model\n  Reasoning Ability in VibeThinker-1.5B",
            "submittedOnDailyBy": {
                "_id": "6406991ec3ab325efa9b6732",
                "avatarUrl": "/avatars/5ba29d9e25820c1172b5a98b078e416f.svg",
                "isPro": false,
                "fullname": "DenseHub",
                "user": "YiZhouDenseHub",
                "type": "user"
            },
            "summary": "Challenging the prevailing consensus that small models inherently lack robust\nreasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense\nmodel developed via our Spectrum-to-Signal Principle (SSP). This challenges the\nprevailing approach of scaling model parameters to enhance capabilities, as\nseen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework\nfirst employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a\nbroad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL)\nto amplify the correct signal. With a total training cost of only $7,800,\nVibeThinker-1.5B demonstrates superior reasoning capabilities compared to\nclosed-source models like Magistral Medium and Claude Opus 4, and performs on\npar with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses\nthe 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8),\nAIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial\nimprovement over its base model (6.7, 4.3, and 0.6, respectively). On\nLiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its\nbase model's 0.0. These findings demonstrate that small models can achieve\nreasoning capabilities comparable to large models, drastically reducing\ntraining and inference costs and thereby democratizing advanced AI research.",
            "upvotes": 61,
            "discussionId": "6912a1c7a644ba07c499c6eb",
            "projectPage": "https://github.com/WeiboAI/VibeThinker",
            "githubRepo": "https://github.com/WeiboAI/VibeThinker",
            "ai_summary": "VibeThinker-1.5B, a 1.5B-parameter model using the Spectrum-to-Signal Principle, achieves superior reasoning capabilities compared to larger models at a significantly lower cost.",
            "ai_keywords": [
                "Spectrum-to-Signal Principle",
                "Two-Stage Diversity-Exploring Distillation",
                "MaxEnt-Guided Policy Optimization",
                "VibeThinker-1.5B",
                "DeepSeek R1",
                "Kimi k2",
                "Magistral Medium",
                "Claude Opus 4",
                "GPT OSS-20B Medium",
                "AIME24",
                "AIME25",
                "HMMT25",
                "LiveCodeBench V6"
            ],
            "githubStars": 58,
            "organization": {
                "_id": "68c8059479c43cfa50f36156",
                "name": "WeiboAI",
                "fullname": "WeiboAI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64d1faaa1ed6649d70d1fa2f/lZVm6Yuiif9cdr5KsnfZr.png"
            }
        },
        "publishedAt": "2025-11-08T23:37:36.000Z",
        "title": "Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model\n  Reasoning Ability in VibeThinker-1.5B",
        "summary": "Challenging the prevailing consensus that small models inherently lack robust\nreasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense\nmodel developed via our Spectrum-to-Signal Principle (SSP). This challenges the\nprevailing approach of scaling model parameters to enhance capabilities, as\nseen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework\nfirst employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a\nbroad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL)\nto amplify the correct signal. With a total training cost of only $7,800,\nVibeThinker-1.5B demonstrates superior reasoning capabilities compared to\nclosed-source models like Magistral Medium and Claude Opus 4, and performs on\npar with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses\nthe 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8),\nAIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial\nimprovement over its base model (6.7, 4.3, and 0.6, respectively). On\nLiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its\nbase model's 0.0. These findings demonstrate that small models can achieve\nreasoning capabilities comparable to large models, drastically reducing\ntraining and inference costs and thereby democratizing advanced AI research.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6406991ec3ab325efa9b6732/4yVJjn-Y2UONHvzebYMkU.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06221.png",
        "numComments": 6,
        "submittedBy": {
            "_id": "6406991ec3ab325efa9b6732",
            "avatarUrl": "/avatars/5ba29d9e25820c1172b5a98b078e416f.svg",
            "fullname": "DenseHub",
            "name": "YiZhouDenseHub",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "68c8059479c43cfa50f36156",
            "name": "WeiboAI",
            "fullname": "WeiboAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64d1faaa1ed6649d70d1fa2f/lZVm6Yuiif9cdr5KsnfZr.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.08319",
            "authors": [
                {
                    "_id": "691416b6ac231a572657202a",
                    "name": "Soyeong Jeong",
                    "hidden": false
                },
                {
                    "_id": "691416b6ac231a572657202b",
                    "name": "Aparna Elangovan",
                    "hidden": false
                },
                {
                    "_id": "691416b6ac231a572657202c",
                    "name": "Emine Yilmaz",
                    "hidden": false
                },
                {
                    "_id": "691416b6ac231a572657202d",
                    "name": "Oleg Rokhlenko",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-11T14:48:34.000Z",
            "submittedOnDailyAt": "2025-11-12T02:44:47.353Z",
            "title": "Adaptive Multi-Agent Response Refinement in Conversational Systems",
            "submittedOnDailyBy": {
                "_id": "64e5a1cd4c20016ec9020ec8",
                "avatarUrl": "/avatars/d7ffe7fbbe39c0a013375357457c57b3.svg",
                "isPro": false,
                "fullname": "Soyeong",
                "user": "starsuzi",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.",
            "upvotes": 34,
            "discussionId": "691416b6ac231a572657202e",
            "ai_summary": "A multi-agent framework refines conversational responses by addressing factuality, personalization, and coherence, outperforming single-agent methods on challenging datasets.",
            "ai_keywords": [
                "Large Language Models",
                "conversational systems",
                "multi-agent framework",
                "factuality",
                "personalization",
                "coherence",
                "dynamic communication strategy"
            ],
            "organization": {
                "_id": "5ffdfbadbba2ae614d771970",
                "name": "amazon",
                "fullname": "Amazon",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
            }
        },
        "publishedAt": "2025-11-11T09:48:34.000Z",
        "title": "Adaptive Multi-Agent Response Refinement in Conversational Systems",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.08319.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e5a1cd4c20016ec9020ec8",
            "avatarUrl": "/avatars/d7ffe7fbbe39c0a013375357457c57b3.svg",
            "fullname": "Soyeong",
            "name": "starsuzi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "organization": {
            "_id": "5ffdfbadbba2ae614d771970",
            "name": "amazon",
            "fullname": "Amazon",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.07080",
            "authors": [
                {
                    "_id": "6912c6cba644ba07c499c7dd",
                    "user": {
                        "_id": "65276c7911a8a521c91bc10f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
                        "isPro": false,
                        "fullname": "Khalil Hennara",
                        "user": "Hennara",
                        "type": "user"
                    },
                    "name": "Khalil Hennara",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:43:31.158Z",
                    "hidden": false
                },
                {
                    "_id": "6912c6cba644ba07c499c7de",
                    "name": "Ahmad Bastati",
                    "hidden": false
                },
                {
                    "_id": "6912c6cba644ba07c499c7df",
                    "user": {
                        "_id": "6496df4b3c64d75523a11973",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6496df4b3c64d75523a11973/I_Qn5-3Czngle-NsGmabO.jpeg",
                        "isPro": false,
                        "fullname": "Muhammad Hreden",
                        "user": "muhammad0-0hreden",
                        "type": "user"
                    },
                    "name": "Muhammad Hreden",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-12T12:21:09.582Z",
                    "hidden": false
                },
                {
                    "_id": "6912c6cba644ba07c499c7e0",
                    "name": "Mohamed Motasim Hamed",
                    "hidden": false
                },
                {
                    "_id": "6912c6cba644ba07c499c7e1",
                    "user": {
                        "_id": "65704741e1cfce1764ce652e",
                        "avatarUrl": "/avatars/9189aaf417426af4ebe381ed364a6c0e.svg",
                        "isPro": false,
                        "fullname": "Zeina Aldallal",
                        "user": "ZeinaD",
                        "type": "user"
                    },
                    "name": "Zeina Aldallal",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-12T12:21:07.736Z",
                    "hidden": false
                },
                {
                    "_id": "6912c6cba644ba07c499c7e2",
                    "name": "Sara Chrouf",
                    "hidden": false
                },
                {
                    "_id": "6912c6cba644ba07c499c7e3",
                    "name": "Safwan AlModhayan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T13:10:31.000Z",
            "submittedOnDailyAt": "2025-11-12T04:46:28.866Z",
            "title": "Wasm: A Pipeline for Constructing Structured Arabic Interleaved\n  Multimodal Corpora",
            "submittedOnDailyBy": {
                "_id": "65276c7911a8a521c91bc10f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
                "isPro": false,
                "fullname": "Khalil Hennara",
                "user": "Hennara",
                "type": "user"
            },
            "summary": "The performance of large language models (LLMs) and large multimodal models\n(LMMs) depends heavily on the quality and scale of their pre-training datasets.\nRecent research shows that large multimodal models trained on natural documents\nwhere images and text are interleaved outperform those trained only on\nimage-text pairs across a wide range of benchmarks, leveraging advanced pre-\ntrained models to enforce semantic alignment, image-sequence consistency, and\ntextual coherence. For Arabic, however, the lack of high-quality multimodal\ndatasets that preserve document structure has limited progress. In this paper,\nwe present our pipeline Wasm for processing the Common Crawl dataset to create\na new Arabic multimodal dataset that uniquely provides markdown output. Unlike\nexisting Arabic corpora that focus solely on text extraction, our approach\npreserves the structural integrity of web content while maintaining flexibility\nfor both text-only and multimodal pre-training scenarios. We provide a\ncomprehensive comparative analysis of our data processing pipeline against\nthose used for major existing datasets, highlighting the convergences in\nfiltering strategies and justifying our specific design choices. To support\nfuture research, we publicly release a representative dataset dump along with\nthe multimodal processing pipeline for Arabic.",
            "upvotes": 30,
            "discussionId": "6912c6cba644ba07c499c7e4",
            "ai_summary": "A pipeline for processing the Common Crawl dataset to create a new Arabic multimodal dataset that preserves document structure and supports both text-only and multimodal pre-training.",
            "ai_keywords": [
                "large language models",
                "large multimodal models",
                "pre-training datasets",
                "natural documents",
                "semantic alignment",
                "image-sequence consistency",
                "textual coherence",
                "Arabic",
                "multimodal datasets",
                "markdown output",
                "web content",
                "data processing pipeline",
                "comparative analysis",
                "dataset dump"
            ]
        },
        "publishedAt": "2025-11-10T08:10:31.000Z",
        "title": "Wasm: A Pipeline for Constructing Structured Arabic Interleaved\n  Multimodal Corpora",
        "summary": "The performance of large language models (LLMs) and large multimodal models\n(LMMs) depends heavily on the quality and scale of their pre-training datasets.\nRecent research shows that large multimodal models trained on natural documents\nwhere images and text are interleaved outperform those trained only on\nimage-text pairs across a wide range of benchmarks, leveraging advanced pre-\ntrained models to enforce semantic alignment, image-sequence consistency, and\ntextual coherence. For Arabic, however, the lack of high-quality multimodal\ndatasets that preserve document structure has limited progress. In this paper,\nwe present our pipeline Wasm for processing the Common Crawl dataset to create\na new Arabic multimodal dataset that uniquely provides markdown output. Unlike\nexisting Arabic corpora that focus solely on text extraction, our approach\npreserves the structural integrity of web content while maintaining flexibility\nfor both text-only and multimodal pre-training scenarios. We provide a\ncomprehensive comparative analysis of our data processing pipeline against\nthose used for major existing datasets, highlighting the convergences in\nfiltering strategies and justifying our specific design choices. To support\nfuture research, we publicly release a representative dataset dump along with\nthe multimodal processing pipeline for Arabic.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07080.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65276c7911a8a521c91bc10f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
            "fullname": "Khalil Hennara",
            "name": "Hennara",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 21
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.05664",
            "authors": [
                {
                    "_id": "6912e1b6a644ba07c499c82d",
                    "user": {
                        "_id": "62e167813eb0730f6217c95e",
                        "avatarUrl": "/avatars/95a18009f725832646643d6169c2bc08.svg",
                        "isPro": false,
                        "fullname": "Seo Hyun Kim",
                        "user": "shkim0116",
                        "type": "user"
                    },
                    "name": "Seo Hyun Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-12T12:21:05.693Z",
                    "hidden": false
                },
                {
                    "_id": "6912e1b6a644ba07c499c82e",
                    "user": {
                        "_id": "6487cd9d6d93bc7eb47ff207",
                        "avatarUrl": "/avatars/f0b33fafd3bed5e49fd4f6630be5ddf2.svg",
                        "isPro": false,
                        "fullname": "H",
                        "user": "SunwooHong",
                        "type": "user"
                    },
                    "name": "Sunwoo Hong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-12T12:21:03.709Z",
                    "hidden": false
                },
                {
                    "_id": "6912e1b6a644ba07c499c82f",
                    "name": "Hojung Jung",
                    "hidden": false
                },
                {
                    "_id": "6912e1b6a644ba07c499c830",
                    "name": "Youngrok Park",
                    "hidden": false
                },
                {
                    "_id": "6912e1b6a644ba07c499c831",
                    "name": "Se-Young Yun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-07T19:05:36.000Z",
            "submittedOnDailyAt": "2025-11-12T03:18:40.045Z",
            "title": "KLASS: KL-Guided Fast Inference in Masked Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "62e167813eb0730f6217c95e",
                "avatarUrl": "/avatars/95a18009f725832646643d6169c2bc08.svg",
                "isPro": false,
                "fullname": "Seo Hyun Kim",
                "user": "shkim0116",
                "type": "user"
            },
            "summary": "Masked diffusion models have demonstrated competitive results on various\ntasks including language generation. However, due to its iterative refinement\nprocess, the inference is often bottlenecked by slow and static sampling speed.\nTo overcome this problem, we introduce `KL-Adaptive Stability Sampling'\n(KLASS), a fast yet effective sampling method that exploits token-level KL\ndivergence to identify stable, high-confidence predictions. By unmasking\nmultiple tokens in each iteration without any additional model training, our\napproach speeds up generation significantly while maintaining sample quality.\nOn reasoning benchmarks, KLASS achieves up to 2.78times wall-clock speedups\nwhile improving performance over standard greedy decoding, attaining\nstate-of-the-art results among diffusion-based samplers. We further validate\nKLASS across diverse domains, including text, image, and molecular generation,\nshowing its effectiveness as a broadly applicable sampler across different\nmodels.",
            "upvotes": 29,
            "discussionId": "6912e1b6a644ba07c499c832",
            "githubRepo": "https://github.com/shkim0116/KLASS",
            "ai_summary": "KL-Adaptive Stability Sampling (KLASS) accelerates diffusion-based generation by identifying stable predictions, achieving significant speedups and quality improvements across various domains.",
            "ai_keywords": [
                "masked diffusion models",
                "language generation",
                "KL divergence",
                "token-level",
                "sampling speed",
                "greedy decoding",
                "text generation",
                "image generation",
                "molecular generation"
            ],
            "githubStars": 7,
            "organization": {
                "_id": "6475760c33192631bad2bb38",
                "name": "kaist-ai",
                "fullname": "KAIST AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
            }
        },
        "publishedAt": "2025-11-07T14:05:36.000Z",
        "title": "KLASS: KL-Guided Fast Inference in Masked Diffusion Models",
        "summary": "Masked diffusion models have demonstrated competitive results on various\ntasks including language generation. However, due to its iterative refinement\nprocess, the inference is often bottlenecked by slow and static sampling speed.\nTo overcome this problem, we introduce `KL-Adaptive Stability Sampling'\n(KLASS), a fast yet effective sampling method that exploits token-level KL\ndivergence to identify stable, high-confidence predictions. By unmasking\nmultiple tokens in each iteration without any additional model training, our\napproach speeds up generation significantly while maintaining sample quality.\nOn reasoning benchmarks, KLASS achieves up to 2.78times wall-clock speedups\nwhile improving performance over standard greedy decoding, attaining\nstate-of-the-art results among diffusion-based samplers. We further validate\nKLASS across diverse domains, including text, image, and molecular generation,\nshowing its effectiveness as a broadly applicable sampler across different\nmodels.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.05664.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62e167813eb0730f6217c95e",
            "avatarUrl": "/avatars/95a18009f725832646643d6169c2bc08.svg",
            "fullname": "Seo Hyun Kim",
            "name": "shkim0116",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "6475760c33192631bad2bb38",
            "name": "kaist-ai",
            "fullname": "KAIST AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.06281",
            "authors": [
                {
                    "_id": "6912a658a644ba07c499c6f5",
                    "name": "Zefeng He",
                    "hidden": false
                },
                {
                    "_id": "6912a658a644ba07c499c6f6",
                    "name": "Xiaoye Qu",
                    "hidden": false
                },
                {
                    "_id": "6912a658a644ba07c499c6f7",
                    "name": "Yafu Li",
                    "hidden": false
                },
                {
                    "_id": "6912a658a644ba07c499c6f8",
                    "name": "Siyuan Huang",
                    "hidden": false
                },
                {
                    "_id": "6912a658a644ba07c499c6f9",
                    "name": "Daizong Liu",
                    "hidden": false
                },
                {
                    "_id": "6912a658a644ba07c499c6fa",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-09T08:36:40.000Z",
            "submittedOnDailyAt": "2025-11-12T00:47:54.808Z",
            "title": "VideoSSR: Video Self-Supervised Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "64cb54da1af278541d663708",
                "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
                "isPro": false,
                "fullname": "Xiaoye Qu",
                "user": "Xiaoye08",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has substantially\nadvanced the video understanding capabilities of Multimodal Large Language\nModels (MLLMs). However, the rapid progress of MLLMs is outpacing the\ncomplexity of existing video datasets, while the manual annotation of new,\nhigh-quality data remains prohibitively expensive. This work investigates a\npivotal question: Can the rich, intrinsic information within videos be\nharnessed to self-generate high-quality, verifiable training data? To\ninvestigate this, we introduce three self-supervised pretext tasks: Anomaly\nGrounding, Object Counting, and Temporal Jigsaw. We construct the Video\nIntrinsic Understanding Benchmark (VIUBench) to validate their difficulty,\nrevealing that current state-of-the-art MLLMs struggle significantly on these\ntasks. Building upon these pretext tasks, we develop the VideoSSR-30K dataset\nand propose VideoSSR, a novel video self-supervised reinforcement learning\nframework for RLVR. Extensive experiments across 17 benchmarks, spanning four\nmajor video domains (General Video QA, Long Video QA, Temporal Grounding, and\nComplex Reasoning), demonstrate that VideoSSR consistently enhances model\nperformance, yielding an average improvement of over 5\\%. These results\nestablish VideoSSR as a potent foundational framework for developing more\nadvanced video understanding in MLLMs. The code is available at\nhttps://github.com/lcqysl/VideoSSR.",
            "upvotes": 14,
            "discussionId": "6912a659a644ba07c499c6fb",
            "projectPage": "https://github.com/lcqysl/VideoSSR",
            "githubRepo": "https://github.com/lcqysl/VideoSSR",
            "ai_summary": "A novel video self-supervised reinforcement learning framework, VideoSSR, enhances MLLM performance across various video understanding tasks by leveraging intrinsic video information.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards",
                "Multimodal Large Language Models",
                "self-supervised pretext tasks",
                "Anomaly Grounding",
                "Object Counting",
                "Temporal Jigsaw",
                "Video Intrinsic Understanding Benchmark",
                "VideoSSR-30K dataset",
                "VideoSSR",
                "General Video QA",
                "Long Video QA",
                "Temporal Grounding",
                "Complex Reasoning"
            ],
            "githubStars": 16,
            "organization": {
                "_id": "6747ee5decec679eafb90450",
                "name": "ShanghaiAiLab",
                "fullname": "shanghai ailab "
            }
        },
        "publishedAt": "2025-11-09T03:36:40.000Z",
        "title": "VideoSSR: Video Self-Supervised Reinforcement Learning",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has substantially\nadvanced the video understanding capabilities of Multimodal Large Language\nModels (MLLMs). However, the rapid progress of MLLMs is outpacing the\ncomplexity of existing video datasets, while the manual annotation of new,\nhigh-quality data remains prohibitively expensive. This work investigates a\npivotal question: Can the rich, intrinsic information within videos be\nharnessed to self-generate high-quality, verifiable training data? To\ninvestigate this, we introduce three self-supervised pretext tasks: Anomaly\nGrounding, Object Counting, and Temporal Jigsaw. We construct the Video\nIntrinsic Understanding Benchmark (VIUBench) to validate their difficulty,\nrevealing that current state-of-the-art MLLMs struggle significantly on these\ntasks. Building upon these pretext tasks, we develop the VideoSSR-30K dataset\nand propose VideoSSR, a novel video self-supervised reinforcement learning\nframework for RLVR. Extensive experiments across 17 benchmarks, spanning four\nmajor video domains (General Video QA, Long Video QA, Temporal Grounding, and\nComplex Reasoning), demonstrate that VideoSSR consistently enhances model\nperformance, yielding an average improvement of over 5\\%. These results\nestablish VideoSSR as a potent foundational framework for developing more\nadvanced video understanding in MLLMs. The code is available at\nhttps://github.com/lcqysl/VideoSSR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06281.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64cb54da1af278541d663708",
            "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
            "fullname": "Xiaoye Qu",
            "name": "Xiaoye08",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "6747ee5decec679eafb90450",
            "name": "ShanghaiAiLab",
            "fullname": "shanghai ailab "
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.07003",
            "authors": [
                {
                    "_id": "69134d59a644ba07c499c8f8",
                    "user": {
                        "_id": "6440b38d3e0374802e1acc5e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6440b38d3e0374802e1acc5e/w-ZpW_9gCSHUeDKyGSeMt.jpeg",
                        "isPro": false,
                        "fullname": "luoyingfeng",
                        "user": "luoyingfeng",
                        "type": "user"
                    },
                    "name": "Yingfeng Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-12T12:19:09.432Z",
                    "hidden": false
                },
                {
                    "_id": "69134d59a644ba07c499c8f9",
                    "user": {
                        "_id": "6705da8fee4c6f44cfb83d3f",
                        "avatarUrl": "/avatars/53cad64bb8acc40a070060a5c7a7dc2d.svg",
                        "isPro": false,
                        "fullname": "Xu Ziqiang",
                        "user": "sleepyhead111",
                        "type": "user"
                    },
                    "name": "Ziqiang Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-12T12:19:07.336Z",
                    "hidden": false
                },
                {
                    "_id": "69134d59a644ba07c499c8fa",
                    "name": "Yuxuan Ouyang",
                    "hidden": false
                },
                {
                    "_id": "69134d59a644ba07c499c8fb",
                    "name": "Murun Yang",
                    "hidden": false
                },
                {
                    "_id": "69134d59a644ba07c499c8fc",
                    "name": "Dingyang Lin",
                    "hidden": false
                },
                {
                    "_id": "69134d59a644ba07c499c8fd",
                    "name": "Kaiyan Chang",
                    "hidden": false
                },
                {
                    "_id": "69134d59a644ba07c499c8fe",
                    "name": "Tong Zheng",
                    "hidden": false
                },
                {
                    "_id": "69134d59a644ba07c499c8ff",
                    "name": "Bei Li",
                    "hidden": false
                },
                {
                    "_id": "69134d59a644ba07c499c900",
                    "name": "Peinan Feng",
                    "hidden": false
                },
                {
                    "_id": "69134d59a644ba07c499c901",
                    "name": "Quan Du",
                    "hidden": false
                },
                {
                    "_id": "69134d59a644ba07c499c902",
                    "name": "Tong Xiao",
                    "hidden": false
                },
                {
                    "_id": "69134d59a644ba07c499c903",
                    "name": "Jingbo Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T11:54:53.000Z",
            "submittedOnDailyAt": "2025-11-12T09:06:16.639Z",
            "title": "Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs",
            "submittedOnDailyBy": {
                "_id": "6440b38d3e0374802e1acc5e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6440b38d3e0374802e1acc5e/w-ZpW_9gCSHUeDKyGSeMt.jpeg",
                "isPro": false,
                "fullname": "luoyingfeng",
                "user": "luoyingfeng",
                "type": "user"
            },
            "summary": "Large language models have significantly advanced Multilingual Machine Translation (MMT), yet the broad language coverage, consistent translation quality, and English-centric bias remain open challenges. To address these challenges, we introduce LMT, a suite of Large-scale Multilingual Translation models centered on both Chinese and English, covering 60 languages and 234 translation directions. During development, we identify a previously overlooked phenomenon of directional degeneration, where symmetric multi-way fine-tuning data overemphasize reverse directions (X to En/Zh), leading to excessive many-to-one mappings and degraded translation quality. We propose Strategic Downsampling, a simple yet effective method to mitigate this degeneration. In addition, we design Parallel Multilingual Prompting (PMP), which leverages typologically related auxiliary languages to enhance cross-lingual transfer. Through rigorous data curation and refined adaptation strategies, LMT achieves SOTA performance among models of comparable language coverage, with our 4B model (LMT-60-4B) surpassing the much larger Aya-101-13B and NLLB-54B models by a substantial margin. We release LMT in four sizes (0.6B/1.7B/4B/8B) to catalyze future research and provide strong baselines for inclusive, scalable, and high-quality MMT \\href{https://github.com/NiuTrans/LMT{https://github.com/NiuTrans/LMT}}.",
            "upvotes": 10,
            "discussionId": "69134d5aa644ba07c499c904",
            "projectPage": "https://github.com/NiuTrans/LMT",
            "githubRepo": "https://github.com/NiuTrans/LMT",
            "ai_summary": "LMT, a suite of large-scale multilingual translation models, addresses challenges in multilingual machine translation through strategic downsampling and parallel multilingual prompting, achieving state-of-the-art performance across 60 languages.",
            "ai_keywords": [
                "LMT",
                "Large-scale Multilingual Translation",
                "directional degeneration",
                "Strategic Downsampling",
                "Parallel Multilingual Prompting",
                "cross-lingual transfer",
                "multilingual machine translation"
            ],
            "githubStars": 66,
            "organization": {
                "_id": "67c534987ae712812d5ef7b4",
                "name": "NiuTrans",
                "fullname": "NiuTrans",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67c5249a64910c03522af447/oSF0H0wFCV6XGfyN3jffn.png"
            }
        },
        "publishedAt": "2025-11-10T06:54:53.000Z",
        "title": "Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs",
        "summary": "Large language models have significantly advanced Multilingual Machine Translation (MMT), yet the broad language coverage, consistent translation quality, and English-centric bias remain open challenges. To address these challenges, we introduce LMT, a suite of Large-scale Multilingual Translation models centered on both Chinese and English, covering 60 languages and 234 translation directions. During development, we identify a previously overlooked phenomenon of directional degeneration, where symmetric multi-way fine-tuning data overemphasize reverse directions (X to En/Zh), leading to excessive many-to-one mappings and degraded translation quality. We propose Strategic Downsampling, a simple yet effective method to mitigate this degeneration. In addition, we design Parallel Multilingual Prompting (PMP), which leverages typologically related auxiliary languages to enhance cross-lingual transfer. Through rigorous data curation and refined adaptation strategies, LMT achieves SOTA performance among models of comparable language coverage, with our 4B model (LMT-60-4B) surpassing the much larger Aya-101-13B and NLLB-54B models by a substantial margin. We release LMT in four sizes (0.6B/1.7B/4B/8B) to catalyze future research and provide strong baselines for inclusive, scalable, and high-quality MMT \\href{https://github.com/NiuTrans/LMT{https://github.com/NiuTrans/LMT}}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07003.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6440b38d3e0374802e1acc5e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6440b38d3e0374802e1acc5e/w-ZpW_9gCSHUeDKyGSeMt.jpeg",
            "fullname": "luoyingfeng",
            "name": "luoyingfeng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "67c534987ae712812d5ef7b4",
            "name": "NiuTrans",
            "fullname": "NiuTrans",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67c5249a64910c03522af447/oSF0H0wFCV6XGfyN3jffn.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.08567",
            "authors": [
                {
                    "_id": "6914114aac231a572657201a",
                    "name": "Hanqing Zhu",
                    "hidden": false
                },
                {
                    "_id": "6914114aac231a572657201b",
                    "name": "Zhenyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6914114aac231a572657201c",
                    "name": "Hanxian Huang",
                    "hidden": false
                },
                {
                    "_id": "6914114aac231a572657201d",
                    "name": "DiJia Su",
                    "hidden": false
                },
                {
                    "_id": "6914114aac231a572657201e",
                    "name": "Zechun Liu",
                    "hidden": false
                },
                {
                    "_id": "6914114aac231a572657201f",
                    "name": "Jiawei Zhao",
                    "hidden": false
                },
                {
                    "_id": "6914114aac231a5726572020",
                    "name": "Igor Fedorov",
                    "hidden": false
                },
                {
                    "_id": "6914114aac231a5726572021",
                    "name": "Hamed Pirsiavash",
                    "hidden": false
                },
                {
                    "_id": "6914114aac231a5726572022",
                    "name": "Zhizhou Sha",
                    "hidden": false
                },
                {
                    "_id": "6914114aac231a5726572023",
                    "name": "Jinwon Lee",
                    "hidden": false
                },
                {
                    "_id": "6914114aac231a5726572024",
                    "name": "David Z. Pan",
                    "hidden": false
                },
                {
                    "_id": "6914114aac231a5726572025",
                    "name": "Zhangyang Wang",
                    "hidden": false
                },
                {
                    "_id": "6914114aac231a5726572026",
                    "name": "Yuandong Tian",
                    "hidden": false
                },
                {
                    "_id": "6914114aac231a5726572027",
                    "name": "Kai Sheng Tai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-11T18:49:45.000Z",
            "submittedOnDailyAt": "2025-11-12T03:56:19.786Z",
            "title": "The Path Not Taken: RLVR Provably Learns Off the Principals",
            "submittedOnDailyBy": {
                "_id": "64adab87692be9d12f81957d",
                "avatarUrl": "/avatars/dbb738c8e78769513d8d699e62d6e49b.svg",
                "isPro": false,
                "fullname": "Hanqing Zhu",
                "user": "hanqing666",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) reliably improves the reasoning performance of large language models, yet it appears to modify only a small fraction of parameters. We revisit this paradox and show that sparsity is a surface artifact of a model-conditioned optimization bias: for a fixed pretrained model, updates consistently localize to preferred parameter regions, highly consistent across runs and largely invariant to datasets and RL recipes. We mechanistically explain these dynamics with a Three-Gate Theory: Gate I (KL Anchor) imposes a KL-constrained update; Gate II (Model Geometry) steers the step off principal directions into low-curvature, spectrum-preserving subspaces; and Gate III (Precision) hides micro-updates in non-preferred regions, making the off-principal bias appear as sparsity. We then validate this theory and, for the first time, provide a parameter-level characterization of RLVR's learning dynamics: RLVR learns off principal directions in weight space, achieving gains via minimal spectral drift, reduced principal-subspace rotation, and off-principal update alignment. In contrast, SFT targets principal weights, distorts the spectrum, and even lags RLVR.\n  Together, these results provide the first parameter-space account of RLVR's training dynamics, revealing clear regularities in how parameters evolve. Crucially, we show that RL operates in a distinct optimization regime from SFT, so directly adapting SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, as evidenced by our case studies on advanced sparse fine-tuning and LoRA variants. We hope this work charts a path toward a white-box understanding of RLVR and the design of geometry-aware, RLVR-native learning algorithms, rather than repurposed SFT-era heuristics.",
            "upvotes": 8,
            "discussionId": "6914114aac231a5726572028",
            "ai_summary": "Reinforcement Learning with Verifiable Rewards (RLVR) improves large language models by modifying a small fraction of parameters through a mechanism involving KL-constrained updates, steering into low-curvature subspaces, and hiding updates in non-preferred regions, differing from supervised fine-tuning methods.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards",
                "RLVR",
                "KL-constrained update",
                "low-curvature subspaces",
                "parameter-level characterization",
                "learning dynamics",
                "principal directions",
                "weight space",
                "spectral drift",
                "principal-subspace rotation",
                "off-principal update alignment",
                "supervised fine-tuning",
                "parameter-efficient fine-tuning",
                "PEFT",
                "sparse fine-tuning",
                "LoRA"
            ],
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "publishedAt": "2025-11-11T13:49:45.000Z",
        "title": "The Path Not Taken: RLVR Provably Learns Off the Principals",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) reliably improves the reasoning performance of large language models, yet it appears to modify only a small fraction of parameters. We revisit this paradox and show that sparsity is a surface artifact of a model-conditioned optimization bias: for a fixed pretrained model, updates consistently localize to preferred parameter regions, highly consistent across runs and largely invariant to datasets and RL recipes. We mechanistically explain these dynamics with a Three-Gate Theory: Gate I (KL Anchor) imposes a KL-constrained update; Gate II (Model Geometry) steers the step off principal directions into low-curvature, spectrum-preserving subspaces; and Gate III (Precision) hides micro-updates in non-preferred regions, making the off-principal bias appear as sparsity. We then validate this theory and, for the first time, provide a parameter-level characterization of RLVR's learning dynamics: RLVR learns off principal directions in weight space, achieving gains via minimal spectral drift, reduced principal-subspace rotation, and off-principal update alignment. In contrast, SFT targets principal weights, distorts the spectrum, and even lags RLVR.\n  Together, these results provide the first parameter-space account of RLVR's training dynamics, revealing clear regularities in how parameters evolve. Crucially, we show that RL operates in a distinct optimization regime from SFT, so directly adapting SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, as evidenced by our case studies on advanced sparse fine-tuning and LoRA variants. We hope this work charts a path toward a white-box understanding of RLVR and the design of geometry-aware, RLVR-native learning algorithms, rather than repurposed SFT-era heuristics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.08567.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64adab87692be9d12f81957d",
            "avatarUrl": "/avatars/dbb738c8e78769513d8d699e62d6e49b.svg",
            "fullname": "Hanqing Zhu",
            "name": "hanqing666",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.07587",
            "authors": [
                {
                    "_id": "6914ecfea1b06ca3cc81344e",
                    "name": "Shreyas Rajesh",
                    "hidden": false
                },
                {
                    "_id": "6914ecfea1b06ca3cc81344f",
                    "name": "Pavan Holur",
                    "hidden": false
                },
                {
                    "_id": "6914ecfea1b06ca3cc813450",
                    "name": "Chenda Duan",
                    "hidden": false
                },
                {
                    "_id": "6914ecfea1b06ca3cc813451",
                    "name": "David Chong",
                    "hidden": false
                },
                {
                    "_id": "6914ecfea1b06ca3cc813452",
                    "name": "Vwani Roychowdhury",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T19:59:34.000Z",
            "submittedOnDailyAt": "2025-11-12T17:55:39.010Z",
            "title": "Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces",
            "submittedOnDailyBy": {
                "_id": "64641e79823c7f8805508de9",
                "avatarUrl": "/avatars/f2e1c785c9d20dabb3a373290f73a9c2.svg",
                "isPro": false,
                "fullname": "Shreyas Rajesh",
                "user": "shreyasrajesh",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, we propose the Generative Semantic Workspace (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. Our framework comprises an Operator, which maps incoming observations to intermediate semantic structures, and a Reconciler, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) huet_episodic_2025 comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to 20\\%. Furthermore, GSW is highly efficient, reducing query-time context tokens by 51\\% compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.",
            "upvotes": 6,
            "discussionId": "6914ecffa1b06ca3cc813453",
            "ai_summary": "The Generative Semantic Workspace (GSW) enhances Large Language Models (LLMs) with a neuro-inspired generative memory framework to handle long-context reasoning and episodic memory tasks more effectively than existing methods.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "long-context reasoning",
                "context windows",
                "external memory frameworks",
                "semantic embeddings",
                "structured knowledge graphs",
                "Generative Semantic Workspace (GSW)",
                "neuro-inspired generative memory framework",
                "Operator",
                "Reconciler",
                "Episodic Memory Benchmark (EpBench)",
                "episodic memory",
                "human-like episodic memory"
            ]
        },
        "publishedAt": "2025-11-10T14:59:34.000Z",
        "title": "Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces",
        "summary": "Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, we propose the Generative Semantic Workspace (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. Our framework comprises an Operator, which maps incoming observations to intermediate semantic structures, and a Reconciler, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) huet_episodic_2025 comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to 20\\%. Furthermore, GSW is highly efficient, reducing query-time context tokens by 51\\% compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07587.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64641e79823c7f8805508de9",
            "avatarUrl": "/avatars/f2e1c785c9d20dabb3a373290f73a9c2.svg",
            "fullname": "Shreyas Rajesh",
            "name": "shreyasrajesh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.08043",
            "authors": [
                {
                    "_id": "6914b626a1b06ca3cc8133bf",
                    "name": "Xueliang Zhao",
                    "hidden": false
                },
                {
                    "_id": "6914b626a1b06ca3cc8133c0",
                    "name": "Wei Wu",
                    "hidden": false
                },
                {
                    "_id": "6914b626a1b06ca3cc8133c1",
                    "name": "Jian Guan",
                    "hidden": false
                },
                {
                    "_id": "6914b626a1b06ca3cc8133c2",
                    "name": "Qintong Li",
                    "hidden": false
                },
                {
                    "_id": "6914b626a1b06ca3cc8133c3",
                    "name": "Lingpeng Kong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-11T09:47:13.000Z",
            "submittedOnDailyAt": "2025-11-12T14:08:32.227Z",
            "title": "DynaAct: Large Language Model Reasoning with Dynamic Action Spaces",
            "submittedOnDailyBy": {
                "_id": "66b291a2d4db4314da3f7664",
                "avatarUrl": "/avatars/3944b62f0be9e1894603bb082c1efc7f.svg",
                "isPro": false,
                "fullname": "Xueliang Zhao",
                "user": "xl-zhao",
                "type": "user"
            },
            "summary": "In modern sequential decision-making systems, the construction of an optimal candidate action space is critical to efficient inference. However, existing approaches either rely on manually defined action spaces that lack scalability or utilize unstructured spaces that render exhaustive search computationally prohibitive. In this paper, we propose a novel framework named DynaAct for automatically constructing a compact action space to enhance sequential reasoning in complex problem-solving scenarios. Our method first estimates a proxy for the complete action space by extracting general sketches observed in a corpus covering diverse complex reasoning problems using large language models. We then formulate a submodular function that jointly evaluates candidate actions based on their utility to the current state and their diversity, and employ a greedy algorithm to select an optimal candidate set. Extensive experiments on six diverse standard benchmarks demonstrate that our approach significantly improves overall performance, while maintaining efficient inference without introducing substantial latency. The implementation is available at https://github.com/zhaoxlpku/DynaAct.",
            "upvotes": 3,
            "discussionId": "6914b626a1b06ca3cc8133c4",
            "ai_summary": "A framework named DynaAct uses large language models to construct a compact action space for sequential decision-making, improving performance and efficiency.",
            "ai_keywords": [
                "sequential decision-making",
                "action space",
                "large language models",
                "submodular function",
                "greedy algorithm",
                "inference",
                "latency"
            ]
        },
        "publishedAt": "2025-11-11T04:47:13.000Z",
        "title": "DynaAct: Large Language Model Reasoning with Dynamic Action Spaces",
        "summary": "In modern sequential decision-making systems, the construction of an optimal candidate action space is critical to efficient inference. However, existing approaches either rely on manually defined action spaces that lack scalability or utilize unstructured spaces that render exhaustive search computationally prohibitive. In this paper, we propose a novel framework named DynaAct for automatically constructing a compact action space to enhance sequential reasoning in complex problem-solving scenarios. Our method first estimates a proxy for the complete action space by extracting general sketches observed in a corpus covering diverse complex reasoning problems using large language models. We then formulate a submodular function that jointly evaluates candidate actions based on their utility to the current state and their diversity, and employ a greedy algorithm to select an optimal candidate set. Extensive experiments on six diverse standard benchmarks demonstrate that our approach significantly improves overall performance, while maintaining efficient inference without introducing substantial latency. The implementation is available at https://github.com/zhaoxlpku/DynaAct.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.08043.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66b291a2d4db4314da3f7664",
            "avatarUrl": "/avatars/3944b62f0be9e1894603bb082c1efc7f.svg",
            "fullname": "Xueliang Zhao",
            "name": "xl-zhao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.07885",
            "authors": [
                {
                    "_id": "69149acea1b06ca3cc8133a2",
                    "name": "Jon Saad-Falcon",
                    "hidden": false
                },
                {
                    "_id": "69149acea1b06ca3cc8133a3",
                    "name": "Avanika Narayan",
                    "hidden": false
                },
                {
                    "_id": "69149acea1b06ca3cc8133a4",
                    "name": "Hakki Orhun Akengin",
                    "hidden": false
                },
                {
                    "_id": "69149acea1b06ca3cc8133a5",
                    "name": "J. Wes Griffin",
                    "hidden": false
                },
                {
                    "_id": "69149acea1b06ca3cc8133a6",
                    "name": "Herumb Shandilya",
                    "hidden": false
                },
                {
                    "_id": "69149acea1b06ca3cc8133a7",
                    "name": "Adrian Gamarra Lafuente",
                    "hidden": false
                },
                {
                    "_id": "69149acea1b06ca3cc8133a8",
                    "name": "Medhya Goel",
                    "hidden": false
                },
                {
                    "_id": "69149acea1b06ca3cc8133a9",
                    "name": "Rebecca Joseph",
                    "hidden": false
                },
                {
                    "_id": "69149acea1b06ca3cc8133aa",
                    "name": "Shlok Natarajan",
                    "hidden": false
                },
                {
                    "_id": "69149acea1b06ca3cc8133ab",
                    "name": "Etash Kumar Guha",
                    "hidden": false
                },
                {
                    "_id": "69149acea1b06ca3cc8133ac",
                    "name": "Shang Zhu",
                    "hidden": false
                },
                {
                    "_id": "69149acea1b06ca3cc8133ad",
                    "name": "Ben Athiwaratkun",
                    "hidden": false
                },
                {
                    "_id": "69149acea1b06ca3cc8133ae",
                    "name": "John Hennessy",
                    "hidden": false
                },
                {
                    "_id": "69149acea1b06ca3cc8133af",
                    "name": "Azalia Mirhoseini",
                    "hidden": false
                },
                {
                    "_id": "69149acea1b06ca3cc8133b0",
                    "name": "Christopher Ré",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-11T06:33:30.000Z",
            "submittedOnDailyAt": "2025-11-12T12:12:34.831Z",
            "title": "Intelligence per Watt: Measuring Intelligence Efficiency of Local AI",
            "submittedOnDailyBy": {
                "_id": "601f5c1a728038834316e6ec",
                "avatarUrl": "/avatars/a9040d7e3e93b738078bccbdd42002e9.svg",
                "isPro": false,
                "fullname": "Narayan",
                "user": "Avanika",
                "type": "user"
            },
            "summary": "Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals 3 findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.",
            "upvotes": 3,
            "discussionId": "69149acea1b06ca3cc8133b1",
            "ai_summary": "Local inference using small language models can efficiently handle a significant portion of real-world queries, reducing demand on centralized cloud infrastructure, with intelligence per watt as a key metric for evaluation.",
            "ai_keywords": [
                "large language model",
                "small LMs",
                "local inference",
                "local accelerators",
                "intelligence per watt",
                "IPW",
                "task accuracy",
                "energy",
                "latency",
                "power",
                "local query coverage",
                "cloud accelerators"
            ],
            "organization": {
                "_id": "636025b83605bd411c1889d9",
                "name": "Stanford",
                "fullname": "Stanford AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/604d2f473050a33ebb17ef51/Z-vDTyG_6-yZhzfXklqAK.jpeg"
            }
        },
        "publishedAt": "2025-11-11T01:33:30.000Z",
        "title": "Intelligence per Watt: Measuring Intelligence Efficiency of Local AI",
        "summary": "Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals 3 findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07885.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "601f5c1a728038834316e6ec",
            "avatarUrl": "/avatars/a9040d7e3e93b738078bccbdd42002e9.svg",
            "fullname": "Narayan",
            "name": "Avanika",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "636025b83605bd411c1889d9",
            "name": "Stanford",
            "fullname": "Stanford AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/604d2f473050a33ebb17ef51/Z-vDTyG_6-yZhzfXklqAK.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.08029",
            "authors": [
                {
                    "_id": "69144007ac231a57265720af",
                    "user": {
                        "_id": "64dca00a5f21fb7f85989628",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64dca00a5f21fb7f85989628/rJx4lxhTr4vkmdSkDuSYi.jpeg",
                        "isPro": false,
                        "fullname": "Aarush",
                        "user": "chungimungi",
                        "type": "user"
                    },
                    "name": "Aarush Sinha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-12T12:18:44.066Z",
                    "hidden": false
                },
                {
                    "_id": "69144007ac231a57265720b0",
                    "user": {
                        "_id": "625c3facf14e262dbb440f53",
                        "avatarUrl": "/avatars/e59e3c28d88dee86ccded3a145672a0e.svg",
                        "isPro": false,
                        "fullname": "Pavan Kumar",
                        "user": "rxpavan",
                        "type": "user"
                    },
                    "name": "Pavan Kumar S",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-12T12:18:42.146Z",
                    "hidden": false
                },
                {
                    "_id": "69144007ac231a57265720b1",
                    "name": "Roshan Balaji",
                    "hidden": false
                },
                {
                    "_id": "69144007ac231a57265720b2",
                    "name": "Nirav Pravinbhai Bhatt",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-11T09:31:37.000Z",
            "submittedOnDailyAt": "2025-11-12T05:38:43.517Z",
            "title": "BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives",
            "submittedOnDailyBy": {
                "_id": "64dca00a5f21fb7f85989628",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64dca00a5f21fb7f85989628/rJx4lxhTr4vkmdSkDuSYi.jpeg",
                "isPro": false,
                "fullname": "Aarush",
                "user": "chungimungi",
                "type": "user"
            },
            "summary": "Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.",
            "upvotes": 2,
            "discussionId": "69144007ac231a57265720b3",
            "githubRepo": "https://github.com/NiravBhattLab/BiCA",
            "ai_summary": "BiCA, a biomedical dense retrieval method using citation-aware hard negatives, enhances retrieval performance with minimal fine-tuning by leveraging document link structure.",
            "ai_keywords": [
                "hard negatives",
                "retrieval models",
                "cross-encoders",
                "static embedding models",
                "cosine distance",
                "biomedical domains",
                "scientific domains",
                "contextual relevance",
                "citation links",
                "PubMed articles",
                "GTE_small",
                "GTE_Base",
                "zero-shot dense retrieval",
                "nDCG@10",
                "BEIR",
                "Success@5",
                "long-tailed topics",
                "domain adaptation"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "6479e4d4ffe1b559f5405f3a",
                "name": "bisectgroup",
                "fullname": "Biosystems Engineering and Control Group"
            }
        },
        "publishedAt": "2025-11-11T04:31:37.000Z",
        "title": "BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives",
        "summary": "Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.08029.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64dca00a5f21fb7f85989628",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64dca00a5f21fb7f85989628/rJx4lxhTr4vkmdSkDuSYi.jpeg",
            "fullname": "Aarush",
            "name": "chungimungi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "6479e4d4ffe1b559f5405f3a",
            "name": "bisectgroup",
            "fullname": "Biosystems Engineering and Control Group"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.06428",
            "authors": [
                {
                    "_id": "69140e95ac231a5726572014",
                    "name": "Samuel Ferino",
                    "hidden": false
                },
                {
                    "_id": "69140e95ac231a5726572015",
                    "name": "Rashina Hoda",
                    "hidden": false
                },
                {
                    "_id": "69140e95ac231a5726572016",
                    "name": "John Grundy",
                    "hidden": false
                },
                {
                    "_id": "69140e95ac231a5726572017",
                    "name": "Christoph Treude",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-09T15:49:55.000Z",
            "submittedOnDailyAt": "2025-11-12T02:06:36.224Z",
            "title": "Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective",
            "submittedOnDailyBy": {
                "_id": "665be0100a6a819676f073d3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665be0100a6a819676f073d3/moAmuTzDuYu45ti2Ob9wR.png",
                "isPro": false,
                "fullname": "Samuel Ferino",
                "user": "samuellucas97",
                "type": "user"
            },
            "summary": "Background: Large Language Models emerged with the potential of provoking a revolution in software development (e.g., automating processes, workforce transformation). Although studies have started to investigate the perceived impact of LLMs for software development, there is a need for empirical studies to comprehend how to balance forward and backward effects of using LLMs. Objective: We investigated how LLMs impact software development and how to manage the impact from a software developer's perspective. Method: We conducted 22 interviews with software practitioners across 3 rounds of data collection and analysis, between October (2024) and September (2025). We employed socio-technical grounded theory (STGT) for data analysis to rigorously analyse interview participants' responses. Results: We identified the benefits (e.g., maintain software development flow, improve developers' mental model, and foster entrepreneurship) and disadvantages (e.g., negative impact on developers' personality and damage to developers' reputation) of using LLMs at individual, team, organisation, and society levels; as well as best practices on how to adopt LLMs. Conclusion: Critically, we present the trade-offs that software practitioners, teams, and organisations face in working with LLMs. Our findings are particularly useful for software team leaders and IT managers to assess the viability of LLMs within their specific context.",
            "upvotes": 2,
            "discussionId": "69140e95ac231a5726572018",
            "ai_summary": "LLMs impact software development by offering benefits like maintaining workflow and fostering entrepreneurship, but also pose risks to developers' well-being and reputation; best practices for adoption are identified.",
            "ai_keywords": [
                ""
            ]
        },
        "publishedAt": "2025-11-09T10:49:55.000Z",
        "title": "Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective",
        "summary": "Background: Large Language Models emerged with the potential of provoking a revolution in software development (e.g., automating processes, workforce transformation). Although studies have started to investigate the perceived impact of LLMs for software development, there is a need for empirical studies to comprehend how to balance forward and backward effects of using LLMs. Objective: We investigated how LLMs impact software development and how to manage the impact from a software developer's perspective. Method: We conducted 22 interviews with software practitioners across 3 rounds of data collection and analysis, between October (2024) and September (2025). We employed socio-technical grounded theory (STGT) for data analysis to rigorously analyse interview participants' responses. Results: We identified the benefits (e.g., maintain software development flow, improve developers' mental model, and foster entrepreneurship) and disadvantages (e.g., negative impact on developers' personality and damage to developers' reputation) of using LLMs at individual, team, organisation, and society levels; as well as best practices on how to adopt LLMs. Conclusion: Critically, we present the trade-offs that software practitioners, teams, and organisations face in working with LLMs. Our findings are particularly useful for software team leaders and IT managers to assess the viability of LLMs within their specific context.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06428.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "665be0100a6a819676f073d3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665be0100a6a819676f073d3/moAmuTzDuYu45ti2Ob9wR.png",
            "fullname": "Samuel Ferino",
            "name": "samuellucas97",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.05650",
            "authors": [
                {
                    "_id": "6914e682a1b06ca3cc81343d",
                    "name": "Yichen Wang",
                    "hidden": false
                },
                {
                    "_id": "6914e682a1b06ca3cc81343e",
                    "name": "Chenghao Yang",
                    "hidden": false
                },
                {
                    "_id": "6914e682a1b06ca3cc81343f",
                    "name": "Tenghao Huang",
                    "hidden": false
                },
                {
                    "_id": "6914e682a1b06ca3cc813440",
                    "name": "Muhao Chen",
                    "hidden": false
                },
                {
                    "_id": "6914e682a1b06ca3cc813441",
                    "name": "Jonathan May",
                    "hidden": false
                },
                {
                    "_id": "6914e682a1b06ca3cc813442",
                    "name": "Mina Lee",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62fb49bafcce44435d7e079a/p85tGsU7eZrtqnltV47MW.mp4"
            ],
            "publishedAt": "2025-11-07T19:00:01.000Z",
            "submittedOnDailyAt": "2025-11-12T17:29:43.921Z",
            "title": "Optimizing Diversity and Quality through Base-Aligned Model Collaboration",
            "submittedOnDailyBy": {
                "_id": "62fb49bafcce44435d7e079a",
                "avatarUrl": "/avatars/116cb4371206ee7010e161c986b09e85.svg",
                "isPro": false,
                "fullname": "Chenghao Yang",
                "user": "chromeNLP",
                "type": "user"
            },
            "summary": "Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality.",
            "upvotes": 2,
            "discussionId": "6914e682a1b06ca3cc813443",
            "ai_summary": "BACo, a token-level collaboration framework, enhances diversity and quality in large language model outputs by dynamically routing between a base model and its aligned counterpart.",
            "ai_keywords": [
                "Base-Aligned Model Collaboration (BACo)",
                "token-level model collaboration",
                "next-token prediction uncertainty",
                "semantic role",
                "diversity-promoting methods",
                "retraining",
                "prompt engineering",
                "multi-sampling methods",
                "open-ended generation tasks",
                "diversity metrics",
                "quality metrics",
                "human evaluations"
            ]
        },
        "publishedAt": "2025-11-07T14:00:01.000Z",
        "title": "Optimizing Diversity and Quality through Base-Aligned Model Collaboration",
        "summary": "Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62fb49bafcce44435d7e079a/p85tGsU7eZrtqnltV47MW.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.05650.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62fb49bafcce44435d7e079a",
            "avatarUrl": "/avatars/116cb4371206ee7010e161c986b09e85.svg",
            "fullname": "Chenghao Yang",
            "name": "chromeNLP",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.05489",
            "authors": [
                {
                    "_id": "69140b6eac231a572657200a",
                    "name": "Junwen Pan",
                    "hidden": false
                },
                {
                    "_id": "69140b6eac231a572657200b",
                    "user": {
                        "_id": "65745b0db238c76bbabdd384",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wdMyuO7OIcOOc3DvADVF6.jpeg",
                        "isPro": false,
                        "fullname": "Qizhe Zhang",
                        "user": "Theia-4869",
                        "type": "user"
                    },
                    "name": "Qizhe Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-12T12:18:55.518Z",
                    "hidden": false
                },
                {
                    "_id": "69140b6eac231a572657200c",
                    "name": "Rui Zhang",
                    "hidden": false
                },
                {
                    "_id": "69140b6eac231a572657200d",
                    "name": "Ming Lu",
                    "hidden": false
                },
                {
                    "_id": "69140b6eac231a572657200e",
                    "name": "Xin Wan",
                    "hidden": false
                },
                {
                    "_id": "69140b6eac231a572657200f",
                    "name": "Yuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "69140b6eac231a5726572010",
                    "name": "Chang Liu",
                    "hidden": false
                },
                {
                    "_id": "69140b6eac231a5726572011",
                    "name": "Qi She",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-07T18:58:25.000Z",
            "submittedOnDailyAt": "2025-11-12T23:02:27.259Z",
            "title": "TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "65745b0db238c76bbabdd384",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wdMyuO7OIcOOc3DvADVF6.jpeg",
                "isPro": false,
                "fullname": "Qizhe Zhang",
                "user": "Theia-4869",
                "type": "user"
            },
            "summary": "Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at https://github.com/Time-Search/TimeSearch-R.",
            "upvotes": 1,
            "discussionId": "69140b6eac231a5726572012",
            "githubRepo": "https://github.com/Time-Search/TimeSearch-R",
            "ai_summary": "TimeSearch-R uses reinforcement learning with GRPO-CSV to improve temporal search and long-form video understanding by verifying the completeness of searched video frames.",
            "ai_keywords": [
                "temporal search",
                "reinforcement learning",
                "GRPO",
                "GRPO-CSV",
                "interleaved text-video thinking",
                "video reasoning",
                "video frames",
                "video content",
                "temporal dependencies",
                "long-form video understanding",
                "Haystack-LVBench",
                "Haystack-Ego4D",
                "VideoMME",
                "MLVU",
                "LongVideoBench",
                "Qwen2.5-VL",
                "Video-R1"
            ],
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "publishedAt": "2025-11-07T13:58:25.000Z",
        "title": "TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning",
        "summary": "Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at https://github.com/Time-Search/TimeSearch-R.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.05489.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65745b0db238c76bbabdd384",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wdMyuO7OIcOOc3DvADVF6.jpeg",
            "fullname": "Qizhe Zhang",
            "name": "Theia-4869",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": true
    }
]