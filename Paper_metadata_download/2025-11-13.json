[
    {
        "paper": {
            "id": "2511.08892",
            "authors": [
                {
                    "_id": "69154dffa1b06ca3cc81351e",
                    "name": "Weihao Tan",
                    "hidden": false
                },
                {
                    "_id": "69154dffa1b06ca3cc81351f",
                    "name": "Xiangyang Li",
                    "hidden": false
                },
                {
                    "_id": "69154dffa1b06ca3cc813520",
                    "name": "Yunhao Fang",
                    "hidden": false
                },
                {
                    "_id": "69154dffa1b06ca3cc813521",
                    "name": "Heyuan Yao",
                    "hidden": false
                },
                {
                    "_id": "69154dffa1b06ca3cc813522",
                    "name": "Shi Yan",
                    "hidden": false
                },
                {
                    "_id": "69154dffa1b06ca3cc813523",
                    "name": "Hao Luo",
                    "hidden": false
                },
                {
                    "_id": "69154dffa1b06ca3cc813524",
                    "name": "Tenglong Ao",
                    "hidden": false
                },
                {
                    "_id": "69154dffa1b06ca3cc813525",
                    "name": "Huihui Li",
                    "hidden": false
                },
                {
                    "_id": "69154dffa1b06ca3cc813526",
                    "name": "Hongbin Ren",
                    "hidden": false
                },
                {
                    "_id": "69154dffa1b06ca3cc813527",
                    "name": "Bairen Yi",
                    "hidden": false
                },
                {
                    "_id": "69154dffa1b06ca3cc813528",
                    "name": "Yujia Qin",
                    "hidden": false
                },
                {
                    "_id": "69154dffa1b06ca3cc813529",
                    "name": "Bo An",
                    "hidden": false
                },
                {
                    "_id": "69154dffa1b06ca3cc81352a",
                    "name": "Libin Liu",
                    "hidden": false
                },
                {
                    "_id": "69154dffa1b06ca3cc81352b",
                    "name": "Guang Shi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/FVxpP05KrXQ1HkQ1G1uNl.mp4"
            ],
            "publishedAt": "2025-11-12T02:01:26.000Z",
            "submittedOnDailyAt": "2025-11-13T00:49:21.639Z",
            "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.",
            "upvotes": 92,
            "discussionId": "69154dffa1b06ca3cc81352c",
            "projectPage": "https://www.lumine-ai.org/",
            "ai_summary": "Lumine, a vision-language model-based agent, completes complex missions in real-time across different 3D open-world environments with human-like efficiency and zero-shot cross-game generalization.",
            "ai_keywords": [
                "vision-language model",
                "end-to-end",
                "real-time",
                "3D open-world environments",
                "zero-shot cross-game generalization"
            ],
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-11-11T21:01:26.000Z",
        "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds",
        "summary": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/FVxpP05KrXQ1HkQ1G1uNl.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.08892.png",
        "numComments": 8,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 160
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.08217",
            "authors": [
                {
                    "_id": "69148856a1b06ca3cc81337f",
                    "user": {
                        "_id": "63f46d83cfd7ba6e26f291ff",
                        "avatarUrl": "/avatars/1c1bdcaa17e00a8ecff6b2e94e9cc4bb.svg",
                        "isPro": false,
                        "fullname": "Frank",
                        "user": "SoloWayG",
                        "type": "user"
                    },
                    "name": "Gleb V. Solovev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-13T13:06:52.809Z",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc813380",
                    "name": "Alina B. Zhidkovskaya",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc813381",
                    "name": "Anastasia Orlova",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc813382",
                    "name": "Nina Gubina",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc813383",
                    "name": "Anastasia Vepreva",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc813384",
                    "name": "Rodion Golovinskii",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc813385",
                    "name": "Ilya Tonkii",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc813386",
                    "name": "Ivan Dubrovsky",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc813387",
                    "name": "Ivan Gurev",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc813388",
                    "name": "Dmitry Gilemkhanov",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc813389",
                    "name": "Denis Chistiakov",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc81338a",
                    "name": "Timur A. Aliev",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc81338b",
                    "name": "Ivan Poddiakov",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc81338c",
                    "name": "Galina Zubkova",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc81338d",
                    "name": "Ekaterina V. Skorb",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc81338e",
                    "name": "Vladimir Vinogradov",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc81338f",
                    "name": "Alexander Boukhanovsky",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc813390",
                    "name": "Nikolay Nikitin",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc813391",
                    "name": "Andrei Dmitrenko",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc813392",
                    "name": "Anna Kalyuzhnaya",
                    "hidden": false
                },
                {
                    "_id": "69148856a1b06ca3cc813393",
                    "name": "Andrey Savchenko",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63f46d83cfd7ba6e26f291ff/M7qC8qH0nCvPNbgVryDQm.png"
            ],
            "publishedAt": "2025-11-11T13:20:35.000Z",
            "submittedOnDailyAt": "2025-11-13T11:30:44.564Z",
            "title": "MADD: Multi-Agent Drug Discovery Orchestra",
            "submittedOnDailyBy": {
                "_id": "63f46d83cfd7ba6e26f291ff",
                "avatarUrl": "/avatars/1c1bdcaa17e00a8ecff6b2e94e9cc4bb.svg",
                "isPro": false,
                "fullname": "Frank",
                "user": "SoloWayG",
                "type": "user"
            },
            "summary": "Hit identification is a central challenge in early drug discovery, traditionally requiring substantial experimental resources. Recent advances in artificial intelligence, particularly large language models (LLMs), have enabled virtual screening methods that reduce costs and improve efficiency. However, the growing complexity of these tools has limited their accessibility to wet-lab researchers. Multi-agent systems offer a promising solution by combining the interpretability of LLMs with the precision of specialized models and tools. In this work, we present MADD, a multi-agent system that builds and executes customized hit identification pipelines from natural language queries. MADD employs four coordinated agents to handle key subtasks in de novo compound generation and screening. We evaluate MADD across seven drug discovery cases and demonstrate its superior performance compared to existing LLM-based solutions. Using MADD, we pioneer the application of AI-first drug design to five biological targets and release the identified hit molecules. Finally, we introduce a new benchmark of query-molecule pairs and docking scores for over three million compounds to contribute to the agentic future of drug design.",
            "upvotes": 46,
            "discussionId": "69148856a1b06ca3cc813394",
            "githubRepo": "https://github.com/sb-ai-lab/MADD",
            "ai_summary": "MADD, a multi-agent system, enhances hit identification in drug discovery by integrating LLMs and specialized models, demonstrating superior performance and accessibility.",
            "ai_keywords": [
                "large language models",
                "multi-agent systems",
                "de novo compound generation",
                "screening",
                "AI-first drug design",
                "benchmark",
                "docking scores"
            ],
            "githubStars": 10,
            "organization": {
                "_id": "68cd79aefd75d93a07d50de3",
                "name": "ITMO-NSS",
                "fullname": "ITMO NSS lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f46d83cfd7ba6e26f291ff/45ZAjuGNy6TCDQiYNCCR1.png"
            }
        },
        "publishedAt": "2025-11-11T08:20:35.000Z",
        "title": "MADD: Multi-Agent Drug Discovery Orchestra",
        "summary": "Hit identification is a central challenge in early drug discovery, traditionally requiring substantial experimental resources. Recent advances in artificial intelligence, particularly large language models (LLMs), have enabled virtual screening methods that reduce costs and improve efficiency. However, the growing complexity of these tools has limited their accessibility to wet-lab researchers. Multi-agent systems offer a promising solution by combining the interpretability of LLMs with the precision of specialized models and tools. In this work, we present MADD, a multi-agent system that builds and executes customized hit identification pipelines from natural language queries. MADD employs four coordinated agents to handle key subtasks in de novo compound generation and screening. We evaluate MADD across seven drug discovery cases and demonstrate its superior performance compared to existing LLM-based solutions. Using MADD, we pioneer the application of AI-first drug design to five biological targets and release the identified hit molecules. Finally, we introduce a new benchmark of query-molecule pairs and docking scores for over three million compounds to contribute to the agentic future of drug design.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63f46d83cfd7ba6e26f291ff/M7qC8qH0nCvPNbgVryDQm.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.08217.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "63f46d83cfd7ba6e26f291ff",
            "avatarUrl": "/avatars/1c1bdcaa17e00a8ecff6b2e94e9cc4bb.svg",
            "fullname": "Frank",
            "name": "SoloWayG",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "68cd79aefd75d93a07d50de3",
            "name": "ITMO-NSS",
            "fullname": "ITMO NSS lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f46d83cfd7ba6e26f291ff/45ZAjuGNy6TCDQiYNCCR1.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.08633",
            "authors": [
                {
                    "_id": "69154cdaa1b06ca3cc8134e4",
                    "name": "Assaf Singer",
                    "hidden": false
                },
                {
                    "_id": "69154cdaa1b06ca3cc8134e5",
                    "user": {
                        "_id": "62b3e85bcbd2a402fc7804b1",
                        "avatarUrl": "/avatars/63125ce8a1e20b8c6e836f223d24284f.svg",
                        "isPro": false,
                        "fullname": "noam rotstein",
                        "user": "noamrot",
                        "type": "user"
                    },
                    "name": "Noam Rotstein",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-13T13:05:49.994Z",
                    "hidden": false
                },
                {
                    "_id": "69154cdaa1b06ca3cc8134e6",
                    "name": "Amir Mann",
                    "hidden": false
                },
                {
                    "_id": "69154cdaa1b06ca3cc8134e7",
                    "name": "Ron Kimmel",
                    "hidden": false
                },
                {
                    "_id": "69154cdaa1b06ca3cc8134e8",
                    "name": "Or Litany",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hZidchKEknO4BdFehrNgb.mp4"
            ],
            "publishedAt": "2025-11-09T22:47:50.000Z",
            "submittedOnDailyAt": "2025-11-13T00:43:43.116Z",
            "title": "Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit's use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: https://time-to-move.github.io/.",
            "upvotes": 39,
            "discussionId": "69154cdaa1b06ca3cc8134e9",
            "projectPage": "https://time-to-move.github.io/",
            "githubRepo": "https://github.com/time-to-move/TTM",
            "ai_summary": "Time-to-Move (TTM) is a training-free framework for motion- and appearance-controlled video generation using image-to-video diffusion models, offering precise motion and appearance control without additional training costs.",
            "ai_keywords": [
                "diffusion-based video generation",
                "image-to-video diffusion models",
                "motion-conditioned synthesis",
                "model-specific fine-tuning",
                "Time-to-Move (TTM)",
                "crude reference animations",
                "cut-and-drag",
                "depth-based reprojection",
                "coarse motion cues",
                "image conditioning",
                "dual-clock denoising",
                "region-dependent strategy",
                "pixel-level conditioning"
            ],
            "githubStars": 17,
            "organization": {
                "_id": "6393322be2364bc1eea56e45",
                "name": "Technion",
                "fullname": "Technion Israel institute of technology",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
            }
        },
        "publishedAt": "2025-11-09T17:47:50.000Z",
        "title": "Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising",
        "summary": "Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit's use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: https://time-to-move.github.io/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hZidchKEknO4BdFehrNgb.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.08633.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 160
        },
        "organization": {
            "_id": "6393322be2364bc1eea56e45",
            "name": "Technion",
            "fullname": "Technion Israel institute of technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.08923",
            "authors": [
                {
                    "_id": "69154d4ba1b06ca3cc813501",
                    "user": {
                        "_id": "6278fbf13503c4f835c7ff70",
                        "avatarUrl": "/avatars/e19960973132ccb0fc0b60a76328b5d8.svg",
                        "isPro": false,
                        "fullname": "Jingyu Liu",
                        "user": "Jingyu6",
                        "type": "user"
                    },
                    "name": "Jingyu Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-13T13:05:42.885Z",
                    "hidden": false
                },
                {
                    "_id": "69154d4ba1b06ca3cc813502",
                    "name": "Xin Dong",
                    "hidden": false
                },
                {
                    "_id": "69154d4ba1b06ca3cc813503",
                    "name": "Zhifan Ye",
                    "hidden": false
                },
                {
                    "_id": "69154d4ba1b06ca3cc813504",
                    "name": "Rishabh Mehta",
                    "hidden": false
                },
                {
                    "_id": "69154d4ba1b06ca3cc813505",
                    "name": "Yonggan Fu",
                    "hidden": false
                },
                {
                    "_id": "69154d4ba1b06ca3cc813506",
                    "name": "Vartika Singh",
                    "hidden": false
                },
                {
                    "_id": "69154d4ba1b06ca3cc813507",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "69154d4ba1b06ca3cc813508",
                    "name": "Ce Zhang",
                    "hidden": false
                },
                {
                    "_id": "69154d4ba1b06ca3cc813509",
                    "name": "Pavlo Molchanov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-12T02:59:33.000Z",
            "submittedOnDailyAt": "2025-11-13T00:45:40.619Z",
            "title": "TiDAR: Think in Diffusion, Talk in Autoregression",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second.",
            "upvotes": 34,
            "discussionId": "69154d4ba1b06ca3cc81350a",
            "ai_summary": "TiDAR, a hybrid diffusion-autoregressive model, achieves high throughput and quality by drafting tokens with diffusion and sampling outputs autoregressively within a single forward pass.",
            "ai_keywords": [
                "diffusion language models",
                "autoregressive models",
                "speculative decoding",
                "left-to-right decoding",
                "TiDAR",
                "sequence-level hybrid architecture",
                "structured attention masks",
                "GPU compute density",
                "KV cache support",
                "Dream",
                "Llada"
            ],
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-11-11T21:59:33.000Z",
        "title": "TiDAR: Think in Diffusion, Talk in Autoregression",
        "summary": "Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.08923.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 160
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.09148",
            "authors": [
                {
                    "_id": "691555e2a1b06ca3cc813576",
                    "name": "Kangning Zhang",
                    "hidden": false
                },
                {
                    "_id": "691555e2a1b06ca3cc813577",
                    "name": "Wenxiang Jiao",
                    "hidden": false
                },
                {
                    "_id": "691555e2a1b06ca3cc813578",
                    "name": "Kounianhua Du",
                    "hidden": false
                },
                {
                    "_id": "691555e2a1b06ca3cc813579",
                    "name": "Yuan Lu",
                    "hidden": false
                },
                {
                    "_id": "691555e2a1b06ca3cc81357a",
                    "name": "Weiwen Liu",
                    "hidden": false
                },
                {
                    "_id": "691555e2a1b06ca3cc81357b",
                    "name": "Weinan Zhang",
                    "hidden": false
                },
                {
                    "_id": "691555e2a1b06ca3cc81357c",
                    "name": "Lei Zhang",
                    "hidden": false
                },
                {
                    "_id": "691555e2a1b06ca3cc81357d",
                    "name": "Yong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-12T09:34:39.000Z",
            "submittedOnDailyAt": "2025-11-13T01:33:44.409Z",
            "title": "LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls",
            "submittedOnDailyBy": {
                "_id": "63b6def76fca9d2a1902fa14",
                "avatarUrl": "/avatars/c7f2487450ea954e2bca4fc5a6db8eb3.svg",
                "isPro": false,
                "fullname": "张康宁",
                "user": "zhuiguang-ning",
                "type": "user"
            },
            "summary": "Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs.",
            "upvotes": 10,
            "discussionId": "691555e2a1b06ca3cc81357e",
            "githubRepo": "https://github.com/Rednote-ExperienceAI-Lab/LoopTool",
            "ai_summary": "LoopTool, an automated data evolution framework, enhances LLMs by iteratively refining data and models, leading to improved tool-use capabilities and state-of-the-art performance.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "tool learning",
                "data synthesis",
                "model training",
                "Greedy Capability Probing (GCP)",
                "Judgement-Guided Label Verification (JGLV)",
                "Error-Driven Data Expansion (EDDE)",
                "BFCL-v3",
                "ACEBench"
            ],
            "githubStars": 12,
            "organization": {
                "_id": "63e5ef7bf2e9a8f22c515654",
                "name": "SJTU",
                "fullname": "Shanghai Jiao Tong University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
            }
        },
        "publishedAt": "2025-11-12T04:34:39.000Z",
        "title": "LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls",
        "summary": "Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.09148.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63b6def76fca9d2a1902fa14",
            "avatarUrl": "/avatars/c7f2487450ea954e2bca4fc5a6db8eb3.svg",
            "fullname": "张康宁",
            "name": "zhuiguang-ning",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "63e5ef7bf2e9a8f22c515654",
            "name": "SJTU",
            "fullname": "Shanghai Jiao Tong University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.09515",
            "authors": [
                {
                    "_id": "69154c35a1b06ca3cc8134dc",
                    "name": "Fangqi Zhu",
                    "hidden": false
                },
                {
                    "_id": "69154c35a1b06ca3cc8134dd",
                    "name": "Zhengyang Yan",
                    "hidden": false
                },
                {
                    "_id": "69154c35a1b06ca3cc8134de",
                    "name": "Zicong Hong",
                    "hidden": false
                },
                {
                    "_id": "69154c35a1b06ca3cc8134df",
                    "name": "Quanxin Shou",
                    "hidden": false
                },
                {
                    "_id": "69154c35a1b06ca3cc8134e0",
                    "name": "Xiao Ma",
                    "hidden": false
                },
                {
                    "_id": "69154c35a1b06ca3cc8134e1",
                    "name": "Song Guo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/MK7Hg3_wVNkeRmYINeMLr.png"
            ],
            "publishedAt": "2025-11-12T17:54:09.000Z",
            "submittedOnDailyAt": "2025-11-13T00:41:08.529Z",
            "title": "WMPO: World Model-based Policy Optimization for Vision-Language-Action Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the \"imagined\" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.",
            "upvotes": 8,
            "discussionId": "69154c36a1b06ca3cc8134e2",
            "projectPage": "https://wm-po.github.io/",
            "githubRepo": "https://github.com/WM-PO/WMPO",
            "ai_summary": "WMPO, a pixel-based world-model framework for on-policy VLA RL, enhances sample efficiency, performance, self-correction, and generalization in robotic manipulation.",
            "ai_keywords": [
                "World-Model-based Policy Optimization",
                "WMPO",
                "on-policy VLA RL",
                "pixel-based predictions",
                "latent world models",
                "GRPO",
                "sample efficiency",
                "self-correction",
                "generalization",
                "lifelong learning"
            ],
            "githubStars": 12,
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-11-12T12:54:09.000Z",
        "title": "WMPO: World Model-based Policy Optimization for Vision-Language-Action Models",
        "summary": "Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the \"imagined\" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/MK7Hg3_wVNkeRmYINeMLr.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.09515.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 160
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.06805",
            "authors": [
                {
                    "_id": "69155a2aa1b06ca3cc813584",
                    "name": "Jinhao Chen",
                    "hidden": false
                },
                {
                    "_id": "69155a2aa1b06ca3cc813585",
                    "name": "Zhen Yang",
                    "hidden": false
                },
                {
                    "_id": "69155a2aa1b06ca3cc813586",
                    "name": "Jianxin Shi",
                    "hidden": false
                },
                {
                    "_id": "69155a2aa1b06ca3cc813587",
                    "name": "Tianyu Wo",
                    "hidden": false
                },
                {
                    "_id": "69155a2aa1b06ca3cc813588",
                    "name": "Jie Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T07:46:19.000Z",
            "submittedOnDailyAt": "2025-11-13T01:41:37.354Z",
            "title": "MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning",
            "submittedOnDailyBy": {
                "_id": "650c0f7fdab7deefd4631109",
                "avatarUrl": "/avatars/3d0683d1113c3bd530b5e8b1499b17f6.svg",
                "isPro": false,
                "fullname": "ZhenYang21",
                "user": "ZhenYang21",
                "type": "user"
            },
            "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language answering tasks. Despite their strengths, these models often encounter challenges in achieving complex reasoning tasks such as mathematical problem-solving. Previous works have focused on fine-tuning on specialized mathematical datasets. However, these datasets are typically distilled directly from teacher models, which capture only static reasoning patterns and leaving substantial gaps compared to student models. This reliance on fixed teacher-derived datasets not only restricts the model's ability to adapt to novel or more intricate questions that extend beyond the confines of the training data, but also lacks the iterative depth needed for robust generalization. To overcome these limitations, we propose \\method, a Mathematical Self-Evolving framework for MLLMs. In contrast to traditional one-shot fine-tuning paradigms, \\method iteratively refines the model through cycles of inference, reflection, and reward-based feedback. Specifically, we leverage iterative fine-tuning by incorporating correct reasoning paths derived from previous-stage inference and integrating reflections from a specialized Outcome Reward Model (ORM). To verify the effectiveness of \\method, we evaluate it on a suite of challenging benchmarks, demonstrating significant performance gains over backbone models. Notably, our experimental results on MathVL-test surpass the leading open-source multimodal mathematical reasoning model QVQ. Our code and models are available at https://zheny2751\\allowbreak-dotcom.github.io/\\allowbreak MathSE.github.io/.",
            "upvotes": 5,
            "discussionId": "69155a2ba1b06ca3cc813589",
            "projectPage": "https://zheny2751-dotcom.github.io/MathSE.github.io/",
            "githubRepo": "https://github.com/zheny2751-dotcom/MathSE",
            "ai_summary": "A Mathematical Self-Evolving framework iteratively refines multimodal large language models for mathematical problem-solving, outperforming existing models on challenging benchmarks.",
            "ai_keywords": [
                "multimodal large language models",
                "vision-language answering tasks",
                "mathematical problem-solving",
                "fine-tuning",
                "specialized mathematical datasets",
                "teacher models",
                "iterative refinement",
                "inference",
                "reflection",
                "reward-based feedback",
                "Outcome Reward Model",
                "MathVL-test",
                "QVQ"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "628735cbc83a2d6ab8d14a66",
                "name": "Tsinghua",
                "fullname": "Tsinghua University"
            }
        },
        "publishedAt": "2025-11-10T02:46:19.000Z",
        "title": "MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning",
        "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language answering tasks. Despite their strengths, these models often encounter challenges in achieving complex reasoning tasks such as mathematical problem-solving. Previous works have focused on fine-tuning on specialized mathematical datasets. However, these datasets are typically distilled directly from teacher models, which capture only static reasoning patterns and leaving substantial gaps compared to student models. This reliance on fixed teacher-derived datasets not only restricts the model's ability to adapt to novel or more intricate questions that extend beyond the confines of the training data, but also lacks the iterative depth needed for robust generalization. To overcome these limitations, we propose \\method, a Mathematical Self-Evolving framework for MLLMs. In contrast to traditional one-shot fine-tuning paradigms, \\method iteratively refines the model through cycles of inference, reflection, and reward-based feedback. Specifically, we leverage iterative fine-tuning by incorporating correct reasoning paths derived from previous-stage inference and integrating reflections from a specialized Outcome Reward Model (ORM). To verify the effectiveness of \\method, we evaluate it on a suite of challenging benchmarks, demonstrating significant performance gains over backbone models. Notably, our experimental results on MathVL-test surpass the leading open-source multimodal mathematical reasoning model QVQ. Our code and models are available at https://zheny2751\\allowbreak-dotcom.github.io/\\allowbreak MathSE.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06805.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650c0f7fdab7deefd4631109",
            "avatarUrl": "/avatars/3d0683d1113c3bd530b5e8b1499b17f6.svg",
            "fullname": "ZhenYang21",
            "name": "ZhenYang21",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "628735cbc83a2d6ab8d14a66",
            "name": "Tsinghua",
            "fullname": "Tsinghua University"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.06251",
            "authors": [
                {
                    "_id": "69141b9aac231a5726572030",
                    "name": "Mingde Xu",
                    "hidden": false
                },
                {
                    "_id": "69141b9aac231a5726572031",
                    "name": "Zhen Yang",
                    "hidden": false
                },
                {
                    "_id": "69141b9aac231a5726572032",
                    "name": "Wenyi Hong",
                    "hidden": false
                },
                {
                    "_id": "69141b9aac231a5726572033",
                    "name": "Lihang Pan",
                    "hidden": false
                },
                {
                    "_id": "69141b9aac231a5726572034",
                    "name": "Xinyue Fan",
                    "hidden": false
                },
                {
                    "_id": "69141b9aac231a5726572035",
                    "name": "Yan Wang",
                    "hidden": false
                },
                {
                    "_id": "69141b9aac231a5726572036",
                    "name": "Xiaotao Gu",
                    "hidden": false
                },
                {
                    "_id": "69141b9aac231a5726572037",
                    "name": "Bin Xu",
                    "hidden": false
                },
                {
                    "_id": "69141b9aac231a5726572038",
                    "name": "Jie Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-09T06:58:52.000Z",
            "submittedOnDailyAt": "2025-11-13T01:32:36.813Z",
            "title": "WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation",
            "submittedOnDailyBy": {
                "_id": "650c0f7fdab7deefd4631109",
                "avatarUrl": "/avatars/3d0683d1113c3bd530b5e8b1499b17f6.svg",
                "isPro": false,
                "fullname": "ZhenYang21",
                "user": "ZhenYang21",
                "type": "user"
            },
            "summary": "User interface (UI) development requires translating design mockups into functional code, a process that remains repetitive and labor-intensive. While recent Vision-Language Models (VLMs) automate UI-to-Code generation, they generate only static HTML/CSS/JavaScript layouts lacking interactivity. To address this, we propose WebVIA, the first agentic framework for interactive UI-to-Code generation and validation. The framework comprises three components: 1) an exploration agent to capture multi-state UI screenshots; 2) a UI2Code model that generates executable interactive code; 3) a validation module that verifies the interactivity. Experiments demonstrate that WebVIA-Agent achieves more stable and accurate UI exploration than general-purpose agents (e.g., Gemini-2.5-Pro). In addition, our fine-tuned WebVIA-UI2Code models exhibit substantial improvements in generating executable and interactive HTML/CSS/JavaScript code, outperforming their base counterparts across both interactive and static UI2Code benchmarks. Our code and models are available at https://zheny2751-dotcom.github.io/webvia.github.io/{https://webvia.github.io}.",
            "upvotes": 5,
            "discussionId": "69141b9aac231a5726572039",
            "projectPage": "https://zheny2751-dotcom.github.io/webvia.github.io/",
            "githubRepo": "https://github.com/zheny2751-dotcom/WebVIA",
            "ai_summary": "WebVIA is an agentic framework that automates the generation and validation of interactive UI code, improving upon existing static UI-to-Code models.",
            "ai_keywords": [
                "Vision-Language Models",
                "UI-to-Code",
                "exploration agent",
                "UI2Code model",
                "validation module",
                "WebVIA-Agent",
                "WebVIA-UI2Code",
                "interactive HTML/CSS/JavaScript",
                "UI2Code benchmarks"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "628735cbc83a2d6ab8d14a66",
                "name": "Tsinghua",
                "fullname": "Tsinghua University"
            }
        },
        "publishedAt": "2025-11-09T01:58:52.000Z",
        "title": "WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation",
        "summary": "User interface (UI) development requires translating design mockups into functional code, a process that remains repetitive and labor-intensive. While recent Vision-Language Models (VLMs) automate UI-to-Code generation, they generate only static HTML/CSS/JavaScript layouts lacking interactivity. To address this, we propose WebVIA, the first agentic framework for interactive UI-to-Code generation and validation. The framework comprises three components: 1) an exploration agent to capture multi-state UI screenshots; 2) a UI2Code model that generates executable interactive code; 3) a validation module that verifies the interactivity. Experiments demonstrate that WebVIA-Agent achieves more stable and accurate UI exploration than general-purpose agents (e.g., Gemini-2.5-Pro). In addition, our fine-tuned WebVIA-UI2Code models exhibit substantial improvements in generating executable and interactive HTML/CSS/JavaScript code, outperforming their base counterparts across both interactive and static UI2Code benchmarks. Our code and models are available at https://zheny2751-dotcom.github.io/webvia.github.io/{https://webvia.github.io}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06251.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650c0f7fdab7deefd4631109",
            "avatarUrl": "/avatars/3d0683d1113c3bd530b5e8b1499b17f6.svg",
            "fullname": "ZhenYang21",
            "name": "ZhenYang21",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "628735cbc83a2d6ab8d14a66",
            "name": "Tsinghua",
            "fullname": "Tsinghua University"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.07499",
            "authors": [
                {
                    "_id": "69156e36a1b06ca3cc8135a8",
                    "name": "Kwanyoung Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T15:52:53.000Z",
            "submittedOnDailyAt": "2025-11-13T03:06:18.813Z",
            "title": "Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance",
            "submittedOnDailyBy": {
                "_id": "63973ee44e7b4959dc98028f",
                "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
                "isPro": false,
                "fullname": "Kwanyoung",
                "user": "kwanyoung",
                "type": "user"
            },
            "summary": "Diffusion models have demonstrated strong generative performance when using guidance methods such as classifier-free guidance (CFG), which enhance output quality by modifying the sampling trajectory. These methods typically improve a target output by intentionally degrading another, often the unconditional output, using heuristic perturbation functions such as identity mixing or blurred conditions. However, these approaches lack a principled foundation and rely on manually designed distortions. In this work, we propose Adversarial Sinkhorn Attention Guidance (ASAG), a novel method that reinterprets attention scores in diffusion models through the lens of optimal transport and intentionally disrupt the transport cost via Sinkhorn algorithm. Instead of naively corrupting the attention mechanism, ASAG injects an adversarial cost within self-attention layers to reduce pixel-wise similarity between queries and keys. This deliberate degradation weakens misleading attention alignments and leads to improved conditional and unconditional sample quality. ASAG shows consistent improvements in text-to-image diffusion, and enhances controllability and fidelity in downstream applications such as IP-Adapter and ControlNet. The method is lightweight, plug-and-play, and improves reliability without requiring any model retraining.",
            "upvotes": 3,
            "discussionId": "69156e36a1b06ca3cc8135a9",
            "ai_summary": "ASAG, a novel guidance method for diffusion models, enhances generative performance by using optimal transport and the Sinkhorn algorithm to disrupt attention scores, improving sample quality and controllability.",
            "ai_keywords": [
                "diffusion models",
                "classifier-free guidance",
                "CFG",
                "heuristic perturbation functions",
                "identity mixing",
                "blurred conditions",
                "Adversarial Sinkhorn Attention Guidance",
                "ASAG",
                "attention scores",
                "optimal transport",
                "Sinkhorn algorithm",
                "pixel-wise similarity",
                "queries",
                "keys",
                "text-to-image diffusion",
                "IP-Adapter",
                "ControlNet",
                "plug-and-play"
            ]
        },
        "publishedAt": "2025-11-10T10:52:53.000Z",
        "title": "Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance",
        "summary": "Diffusion models have demonstrated strong generative performance when using guidance methods such as classifier-free guidance (CFG), which enhance output quality by modifying the sampling trajectory. These methods typically improve a target output by intentionally degrading another, often the unconditional output, using heuristic perturbation functions such as identity mixing or blurred conditions. However, these approaches lack a principled foundation and rely on manually designed distortions. In this work, we propose Adversarial Sinkhorn Attention Guidance (ASAG), a novel method that reinterprets attention scores in diffusion models through the lens of optimal transport and intentionally disrupt the transport cost via Sinkhorn algorithm. Instead of naively corrupting the attention mechanism, ASAG injects an adversarial cost within self-attention layers to reduce pixel-wise similarity between queries and keys. This deliberate degradation weakens misleading attention alignments and leads to improved conditional and unconditional sample quality. ASAG shows consistent improvements in text-to-image diffusion, and enhances controllability and fidelity in downstream applications such as IP-Adapter and ControlNet. The method is lightweight, plug-and-play, and improves reliability without requiring any model retraining.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07499.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63973ee44e7b4959dc98028f",
            "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
            "fullname": "Kwanyoung",
            "name": "kwanyoung",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.06101",
            "authors": [
                {
                    "_id": "6915fa7121e02bff6aac7a3a",
                    "name": "Zhaoyang Wang",
                    "hidden": false
                },
                {
                    "_id": "6915fa7121e02bff6aac7a3b",
                    "name": "Yiming Liang",
                    "hidden": false
                },
                {
                    "_id": "6915fa7121e02bff6aac7a3c",
                    "name": "Xuchao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6915fa7121e02bff6aac7a3d",
                    "name": "Qianhui Wu",
                    "hidden": false
                },
                {
                    "_id": "6915fa7121e02bff6aac7a3e",
                    "name": "Siwei Han",
                    "hidden": false
                },
                {
                    "_id": "6915fa7121e02bff6aac7a3f",
                    "name": "Anson Bastos",
                    "hidden": false
                },
                {
                    "_id": "6915fa7121e02bff6aac7a40",
                    "name": "Rujia Wang",
                    "hidden": false
                },
                {
                    "_id": "6915fa7121e02bff6aac7a41",
                    "name": "Chetan Bansal",
                    "hidden": false
                },
                {
                    "_id": "6915fa7121e02bff6aac7a42",
                    "name": "Baolin Peng",
                    "hidden": false
                },
                {
                    "_id": "6915fa7121e02bff6aac7a43",
                    "name": "Jianfeng Gao",
                    "hidden": false
                },
                {
                    "_id": "6915fa7121e02bff6aac7a44",
                    "name": "Saravan Rajmohan",
                    "hidden": false
                },
                {
                    "_id": "6915fa7121e02bff6aac7a45",
                    "name": "Huaxiu Yao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/CNrV_vKdKsrQ9mSMiV2ry.png"
            ],
            "publishedAt": "2025-11-08T18:45:33.000Z",
            "submittedOnDailyAt": "2025-11-13T13:08:02.218Z",
            "title": "Adapting Web Agents with Synthetic Supervision",
            "submittedOnDailyBy": {
                "_id": "633122d3f242a8532b7a928d",
                "avatarUrl": "/avatars/2158ffff0882a8fb4588e273fd60dea7.svg",
                "isPro": true,
                "fullname": "Chi",
                "user": "ChilleD",
                "type": "user"
            },
            "summary": "Web agents struggle to adapt to new websites due to the scarcity of environment specific tasks and demonstrations. Recent works have explored synthetic data generation to address this challenge, however, they suffer from data quality issues where synthesized tasks contain hallucinations that cannot be executed, and collected trajectories are noisy with redundant or misaligned actions. In this paper, we propose SynthAgent, a fully synthetic supervision framework that aims at improving synthetic data quality via dual refinement of both tasks and trajectories. Our approach begins by synthesizing diverse tasks through categorized exploration of web elements, ensuring efficient coverage of the target environment. During trajectory collection, we refine tasks when conflicts with actual observations are detected, mitigating hallucinations while maintaining task consistency. After collection, we conduct trajectory refinement with a global context to mitigate potential noise or misalignments. Finally, we fine-tune open-source web agents on the refined synthetic data to adapt them to the target environment. Experimental results demonstrate that SynthAgent outperforms existing synthetic data methods, validating the importance of high-quality synthetic supervision. The code will be publicly available at https://github.com/aiming-lab/SynthAgent.",
            "upvotes": 2,
            "discussionId": "6915fa7221e02bff6aac7a46",
            "projectPage": "https://github.com/aiming-lab/SynthAgent",
            "githubRepo": "https://github.com/aiming-lab/SynthAgent",
            "ai_summary": "SynthAgent improves synthetic data quality for web agents by refining both tasks and trajectories, leading to better adaptation to new websites.",
            "ai_keywords": [
                "synthetic data generation",
                "hallucinations",
                "trajectory refinement",
                "task refinement",
                "web agents",
                "categorized exploration",
                "global context",
                "fine-tuning"
            ],
            "githubStars": 14
        },
        "publishedAt": "2025-11-08T13:45:33.000Z",
        "title": "Adapting Web Agents with Synthetic Supervision",
        "summary": "Web agents struggle to adapt to new websites due to the scarcity of environment specific tasks and demonstrations. Recent works have explored synthetic data generation to address this challenge, however, they suffer from data quality issues where synthesized tasks contain hallucinations that cannot be executed, and collected trajectories are noisy with redundant or misaligned actions. In this paper, we propose SynthAgent, a fully synthetic supervision framework that aims at improving synthetic data quality via dual refinement of both tasks and trajectories. Our approach begins by synthesizing diverse tasks through categorized exploration of web elements, ensuring efficient coverage of the target environment. During trajectory collection, we refine tasks when conflicts with actual observations are detected, mitigating hallucinations while maintaining task consistency. After collection, we conduct trajectory refinement with a global context to mitigate potential noise or misalignments. Finally, we fine-tune open-source web agents on the refined synthetic data to adapt them to the target environment. Experimental results demonstrate that SynthAgent outperforms existing synthetic data methods, validating the importance of high-quality synthetic supervision. The code will be publicly available at https://github.com/aiming-lab/SynthAgent.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/CNrV_vKdKsrQ9mSMiV2ry.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06101.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "633122d3f242a8532b7a928d",
            "avatarUrl": "/avatars/2158ffff0882a8fb4588e273fd60dea7.svg",
            "fullname": "Chi",
            "name": "ChilleD",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.04824",
            "authors": [
                {
                    "_id": "6914d0bea1b06ca3cc813415",
                    "name": "Kosei Horikawa",
                    "hidden": false
                },
                {
                    "_id": "6914d0bea1b06ca3cc813416",
                    "user": {
                        "_id": "62b4f3b7464e664268bf4e85",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b4f3b7464e664268bf4e85/atFIscmB37aur0a1zuQ2o.jpeg",
                        "isPro": false,
                        "fullname": "Leo",
                        "user": "hao-li",
                        "type": "user"
                    },
                    "name": "Hao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-13T13:06:05.657Z",
                    "hidden": false
                },
                {
                    "_id": "6914d0bea1b06ca3cc813417",
                    "name": "Yutaro Kashiwa",
                    "hidden": false
                },
                {
                    "_id": "6914d0bea1b06ca3cc813418",
                    "name": "Bram Adams",
                    "hidden": false
                },
                {
                    "_id": "6914d0bea1b06ca3cc813419",
                    "name": "Hajimu Iida",
                    "hidden": false
                },
                {
                    "_id": "6914d0bea1b06ca3cc81341a",
                    "name": "Ahmed E. Hassan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-06T21:24:38.000Z",
            "submittedOnDailyAt": "2025-11-13T01:25:36.659Z",
            "title": "Agentic Refactoring: An Empirical Study of AI Coding Agents",
            "submittedOnDailyBy": {
                "_id": "62b4f3b7464e664268bf4e85",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b4f3b7464e664268bf4e85/atFIscmB37aur0a1zuQ2o.jpeg",
                "isPro": false,
                "fullname": "Leo",
                "user": "hao-li",
                "type": "user"
            },
            "summary": "Agentic coding tools, such as OpenAI Codex, Claude Code, and Cursor, are transforming the software engineering landscape. These AI-powered systems function as autonomous teammates capable of planning and executing complex development tasks. Agents have become active participants in refactoring, a cornerstone of sustainable software development aimed at improving internal code quality without altering observable behavior. Despite their increasing adoption, there is a critical lack of empirical understanding regarding how agentic refactoring is utilized in practice, how it compares to human-driven refactoring, and what impact it has on code quality. To address this empirical gap, we present a large-scale study of AI agent-generated refactorings in real-world open-source Java projects, analyzing 15,451 refactoring instances across 12,256 pull requests and 14,988 commits derived from the AIDev dataset. Our empirical analysis shows that refactoring is a common and intentional activity in this development paradigm, with agents explicitly targeting refactoring in 26.1% of commits. Analysis of refactoring types reveals that agentic efforts are dominated by low-level, consistency-oriented edits, such as Change Variable Type (11.8%), Rename Parameter (10.4%), and Rename Variable (8.5%), reflecting a preference for localized improvements over the high-level design changes common in human refactoring. Additionally, the motivations behind agentic refactoring focus overwhelmingly on internal quality concerns, with maintainability (52.5%) and readability (28.1%). Furthermore, quantitative evaluation of code quality metrics shows that agentic refactoring yields small but statistically significant improvements in structural metrics, particularly for medium-level changes, reducing class size and complexity (e.g., Class LOC median Δ = -15.25).",
            "upvotes": 2,
            "discussionId": "6914d0bfa1b06ca3cc81341b",
            "ai_summary": "AI agents frequently perform refactoring in open-source Java projects, focusing on low-level consistency edits and improving code quality metrics like class size and complexity.",
            "ai_keywords": [
                "agentic coding tools",
                "OpenAI Codex",
                "Claude Code",
                "Cursor",
                "autonomous teammates",
                "refactoring",
                "software development",
                "AIDev dataset",
                "Change Variable Type",
                "Rename Parameter",
                "Rename Variable",
                "maintainability",
                "readability",
                "structural metrics",
                "class size",
                "complexity"
            ]
        },
        "publishedAt": "2025-11-06T16:24:38.000Z",
        "title": "Agentic Refactoring: An Empirical Study of AI Coding Agents",
        "summary": "Agentic coding tools, such as OpenAI Codex, Claude Code, and Cursor, are transforming the software engineering landscape. These AI-powered systems function as autonomous teammates capable of planning and executing complex development tasks. Agents have become active participants in refactoring, a cornerstone of sustainable software development aimed at improving internal code quality without altering observable behavior. Despite their increasing adoption, there is a critical lack of empirical understanding regarding how agentic refactoring is utilized in practice, how it compares to human-driven refactoring, and what impact it has on code quality. To address this empirical gap, we present a large-scale study of AI agent-generated refactorings in real-world open-source Java projects, analyzing 15,451 refactoring instances across 12,256 pull requests and 14,988 commits derived from the AIDev dataset. Our empirical analysis shows that refactoring is a common and intentional activity in this development paradigm, with agents explicitly targeting refactoring in 26.1% of commits. Analysis of refactoring types reveals that agentic efforts are dominated by low-level, consistency-oriented edits, such as Change Variable Type (11.8%), Rename Parameter (10.4%), and Rename Variable (8.5%), reflecting a preference for localized improvements over the high-level design changes common in human refactoring. Additionally, the motivations behind agentic refactoring focus overwhelmingly on internal quality concerns, with maintainability (52.5%) and readability (28.1%). Furthermore, quantitative evaluation of code quality metrics shows that agentic refactoring yields small but statistically significant improvements in structural metrics, particularly for medium-level changes, reducing class size and complexity (e.g., Class LOC median Δ = -15.25).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04824.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62b4f3b7464e664268bf4e85",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b4f3b7464e664268bf4e85/atFIscmB37aur0a1zuQ2o.jpeg",
            "fullname": "Leo",
            "name": "hao-li",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.07464",
            "authors": [
                {
                    "_id": "691623a021e02bff6aac7a95",
                    "name": "Junghwan Lim",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7a96",
                    "name": "Sungmin Lee",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7a97",
                    "name": "Dongseok Kim",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7a98",
                    "name": "Taehyun Kim",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7a99",
                    "name": "Eunhwan Park",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7a9a",
                    "name": "Jeesoo Lee",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7a9b",
                    "name": "Jeongdoo Lee",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7a9c",
                    "name": "Junhyeok Lee",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7a9d",
                    "name": "Wai Ting Cheung",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7a9e",
                    "name": "Dahye Choi",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7a9f",
                    "name": "Jaeheui Her",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7aa0",
                    "name": "Jaeyeon Huh",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7aa1",
                    "name": "Hanbin Jung",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7aa2",
                    "name": "Changjin Kang",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7aa3",
                    "name": "Beomgyu Kim",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7aa4",
                    "name": "Minjae Kim",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7aa5",
                    "name": "Taewhan Kim",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7aa6",
                    "name": "Youngrok Kim",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7aa7",
                    "name": "Hyukjin Kweon",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7aa8",
                    "name": "Haesol Lee",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7aa9",
                    "name": "Kungyu Lee",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7aaa",
                    "name": "Dongpin Oh",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7aab",
                    "name": "Yeongjae Park",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7aac",
                    "name": "Bokki Ryu",
                    "hidden": false
                },
                {
                    "_id": "691623a021e02bff6aac7aad",
                    "name": "Dongjoo Weon",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-07T10:32:16.000Z",
            "submittedOnDailyAt": "2025-11-13T16:02:08.115Z",
            "title": "Motif 2 12.7B technical report",
            "submittedOnDailyBy": {
                "_id": "651e96991b97c9f33d26bde6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg",
                "isPro": true,
                "fullname": "Elie Bakouch",
                "user": "eliebak",
                "type": "user"
            },
            "summary": "We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.",
            "upvotes": 1,
            "discussionId": "691623a121e02bff6aac7aae",
            "ai_summary": "Motif-2-12.7B, a large language model, enhances efficiency through architectural innovations like Grouped Differential Attention and system-level optimizations, achieving competitive performance with a smaller model size.",
            "ai_keywords": [
                "Grouped Differential Attention",
                "MuonClip optimizer",
                "fused PolyNorm activations",
                "Parallel Muon algorithm",
                "curriculum-driven data scheduler",
                "supervised fine-tuning pipeline"
            ],
            "organization": {
                "_id": "6836952136f9e6c2d2493660",
                "name": "Motif-Technologies",
                "fullname": "Motif Technologies",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6836935d054aee793ffd78f1/L3Rw_g8vkvD8dqhOYGZhl.png"
            }
        },
        "publishedAt": "2025-11-07T05:32:16.000Z",
        "title": "Motif 2 12.7B technical report",
        "summary": "We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07464.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "651e96991b97c9f33d26bde6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg",
            "fullname": "Elie Bakouch",
            "name": "eliebak",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 352
        },
        "organization": {
            "_id": "6836952136f9e6c2d2493660",
            "name": "Motif-Technologies",
            "fullname": "Motif Technologies",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6836935d054aee793ffd78f1/L3Rw_g8vkvD8dqhOYGZhl.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.06073",
            "authors": [
                {
                    "_id": "6912d50ba644ba07c499c7f6",
                    "user": {
                        "_id": "645dbaa6f5760d1530d7580d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg",
                        "isPro": true,
                        "fullname": "Simeon Emanuilov",
                        "user": "s-emanuilov",
                        "type": "user"
                    },
                    "name": "Simeon Emanuilov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-11T19:43:28.643Z",
                    "hidden": false
                },
                {
                    "_id": "6912d50ba644ba07c499c7f7",
                    "name": "Richard Ackermann",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-08T17:07:57.000Z",
            "submittedOnDailyAt": "2025-11-13T04:59:37.830Z",
            "title": "Stemming Hallucination in Language Models Using a Licensing Oracle",
            "submittedOnDailyBy": {
                "_id": "645dbaa6f5760d1530d7580d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg",
                "isPro": true,
                "fullname": "Simeon Emanuilov",
                "user": "s-emanuilov",
                "type": "user"
            },
            "summary": "Language models exhibit remarkable natural language generation capabilities\nbut remain prone to hallucinations, generating factually incorrect information\ndespite producing syntactically coherent responses. This study introduces the\nLicensing Oracle, an architectural solution designed to stem hallucinations in\nLMs by enforcing truth constraints through formal validation against structured\nknowledge graphs. Unlike statistical approaches that rely on data scaling or\nfine-tuning, the Licensing Oracle embeds a deterministic validation step into\nthe model's generative process, ensuring that only factually accurate claims\nare made. We evaluated the effectiveness of the Licensing Oracle through\nexperiments comparing it with several state-of-the-art methods, including\nbaseline language model generation, fine-tuning for factual recall, fine-tuning\nfor abstention behavior, and retrieval-augmented generation (RAG). Our results\ndemonstrate that although RAG and fine-tuning improve performance, they fail to\neliminate hallucinations. In contrast, the Licensing Oracle achieved perfect\nabstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring\nthat only valid claims were generated with 89.1% accuracy in factual responses.\nThis work shows that architectural innovations, such as the Licensing Oracle,\noffer a necessary and sufficient solution for hallucinations in domains with\nstructured knowledge representations, offering guarantees that statistical\nmethods cannot match. Although the Licensing Oracle is specifically designed to\naddress hallucinations in fact-based domains, its framework lays the groundwork\nfor truth-constrained generation in future AI systems, providing a new path\ntoward reliable, epistemically grounded models.",
            "upvotes": 0,
            "discussionId": "6912d50ba644ba07c499c7f8",
            "ai_summary": "The Licensing Oracle, an architectural solution, eliminates hallucinations in language models by enforcing truth constraints through formal validation against structured knowledge graphs, achieving perfect abstention precision and zero false answers.",
            "ai_keywords": [
                "Licensing Oracle",
                "hallucinations",
                "language models",
                "truth constraints",
                "structured knowledge graphs",
                "deterministic validation",
                "factual recall",
                "abstention behavior",
                "retrieval-augmented generation (RAG)",
                "epistemically grounded models"
            ]
        },
        "publishedAt": "2025-11-08T12:07:57.000Z",
        "title": "Stemming Hallucination in Language Models Using a Licensing Oracle",
        "summary": "Language models exhibit remarkable natural language generation capabilities\nbut remain prone to hallucinations, generating factually incorrect information\ndespite producing syntactically coherent responses. This study introduces the\nLicensing Oracle, an architectural solution designed to stem hallucinations in\nLMs by enforcing truth constraints through formal validation against structured\nknowledge graphs. Unlike statistical approaches that rely on data scaling or\nfine-tuning, the Licensing Oracle embeds a deterministic validation step into\nthe model's generative process, ensuring that only factually accurate claims\nare made. We evaluated the effectiveness of the Licensing Oracle through\nexperiments comparing it with several state-of-the-art methods, including\nbaseline language model generation, fine-tuning for factual recall, fine-tuning\nfor abstention behavior, and retrieval-augmented generation (RAG). Our results\ndemonstrate that although RAG and fine-tuning improve performance, they fail to\neliminate hallucinations. In contrast, the Licensing Oracle achieved perfect\nabstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring\nthat only valid claims were generated with 89.1% accuracy in factual responses.\nThis work shows that architectural innovations, such as the Licensing Oracle,\noffer a necessary and sufficient solution for hallucinations in domains with\nstructured knowledge representations, offering guarantees that statistical\nmethods cannot match. Although the Licensing Oracle is specifically designed to\naddress hallucinations in fact-based domains, its framework lays the groundwork\nfor truth-constrained generation in future AI systems, providing a new path\ntoward reliable, epistemically grounded models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06073.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645dbaa6f5760d1530d7580d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg",
            "fullname": "Simeon Emanuilov",
            "name": "s-emanuilov",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 44
        },
        "isAuthorParticipating": true
    }
]