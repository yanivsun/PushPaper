[
    {
        "paper": {
            "id": "2511.10629",
            "authors": [
                {
                    "_id": "691708aab63bfc66e049879e",
                    "user": {
                        "_id": "64bfced7dd71b24b9626a949",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bfced7dd71b24b9626a949/tNvOP44A3hPRu--Y-04Xn.png",
                        "isPro": false,
                        "fullname": "Aleksandr Razin",
                        "user": "RazinAleks",
                        "type": "user"
                    },
                    "name": "Aleksandr Razin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-14T14:22:11.716Z",
                    "hidden": false
                },
                {
                    "_id": "691708aab63bfc66e049879f",
                    "user": {
                        "_id": "628e38f1c64b0daa908e8709",
                        "avatarUrl": "/avatars/00ccf0b0286c931532dd720013d95f37.svg",
                        "isPro": false,
                        "fullname": "Danil Kazancev",
                        "user": "vaskers5",
                        "type": "user"
                    },
                    "name": "Danil Kazantsev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-14T14:22:08.073Z",
                    "hidden": false
                },
                {
                    "_id": "691708aab63bfc66e04987a0",
                    "name": "Ilya Makarov",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64bfced7dd71b24b9626a949/5XomVDoz3P--HeR-uZ9IJ.png"
            ],
            "publishedAt": "2025-11-13T18:54:18.000Z",
            "submittedOnDailyAt": "2025-11-14T11:57:59.881Z",
            "title": "One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "64bfced7dd71b24b9626a949",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bfced7dd71b24b9626a949/tNvOP44A3hPRu--Y-04Xn.png",
                "isPro": false,
                "fullname": "Aleksandr Razin",
                "user": "RazinAleks",
                "type": "user"
            },
            "summary": "Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.",
            "upvotes": 54,
            "discussionId": "691708aab63bfc66e04987a1",
            "ai_summary": "LUA is a lightweight module that performs super-resolution directly in the latent space of diffusion models, improving efficiency without compromising image quality.",
            "ai_keywords": [
                "diffusion models",
                "latent code",
                "VAE decoding",
                "Swin-style backbone",
                "pixel-shuffle heads",
                "image-space SR",
                "SwinIR architecture",
                "perceptual quality",
                "decoding and upscaling time",
                "latent spaces",
                "VAEs",
                "high-resolution generation",
                "scalable",
                "high-fidelity image synthesis"
            ]
        },
        "publishedAt": "2025-11-13T13:54:18.000Z",
        "title": "One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models",
        "summary": "Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64bfced7dd71b24b9626a949/5XomVDoz3P--HeR-uZ9IJ.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.10629.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "64bfced7dd71b24b9626a949",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bfced7dd71b24b9626a949/tNvOP44A3hPRu--Y-04Xn.png",
            "fullname": "Aleksandr Razin",
            "name": "RazinAleks",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.09057",
            "authors": [
                {
                    "_id": "6917090db63bfc66e0498981",
                    "name": "PAN Team",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498982",
                    "name": "Jiannan Xiang",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498983",
                    "name": "Yi Gu",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498984",
                    "name": "Zihan Liu",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498985",
                    "name": "Zeyu Feng",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498986",
                    "name": "Qiyue Gao",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498987",
                    "name": "Yiyan Hu",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498988",
                    "name": "Benhao Huang",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498989",
                    "name": "Guangyi Liu",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e049898a",
                    "name": "Yichi Yang",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e049898b",
                    "name": "Kun Zhou",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e049898c",
                    "name": "Davit Abrahamyan",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e049898d",
                    "name": "Arif Ahmad",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e049898e",
                    "name": "Ganesh Bannur",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e049898f",
                    "name": "Junrong Chen",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498990",
                    "name": "Kimi Chen",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498991",
                    "name": "Mingkai Deng",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498992",
                    "name": "Ruobing Han",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498993",
                    "name": "Xinqi Huang",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498994",
                    "name": "Haoqiang Kang",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498995",
                    "name": "Zheqi Li",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498996",
                    "name": "Enze Ma",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498997",
                    "name": "Hector Ren",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498998",
                    "name": "Yashowardhan Shinde",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e0498999",
                    "name": "Rohan Shingre",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e049899a",
                    "name": "Ramsundar Tanikella",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e049899b",
                    "name": "Kaiming Tao",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e049899c",
                    "name": "Dequan Yang",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e049899d",
                    "name": "Xinle Yu",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e049899e",
                    "name": "Cong Zeng",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e049899f",
                    "name": "Binglin Zhou",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e04989a0",
                    "name": "Zhengzhong Liu",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e04989a1",
                    "name": "Zhiting Hu",
                    "hidden": false
                },
                {
                    "_id": "6917090db63bfc66e04989a2",
                    "name": "Eric P. Xing",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-12T07:20:35.000Z",
            "submittedOnDailyAt": "2025-11-14T12:33:57.271Z",
            "title": "PAN: A World Model for General, Interactable, and Long-Horizon World Simulation",
            "submittedOnDailyBy": {
                "_id": "61c4b5c871a107e9d80e33fb",
                "avatarUrl": "/avatars/128a2a9393b0f7921323d0d94db4ecad.svg",
                "isPro": false,
                "fullname": "Jiannan Xiang",
                "user": "jiannanx",
                "type": "user"
            },
            "summary": "A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.",
            "upvotes": 31,
            "discussionId": "6917090db63bfc66e04989a5",
            "ai_summary": "PAN, a general, interactable, and long-horizon world model, predicts future world states using a Generative Latent Prediction (GLP) architecture that combines autoregressive latent dynamics with a video diffusion decoder, enabling detailed, long-term, and coherent video simulation.",
            "ai_keywords": [
                "world model",
                "video generation",
                "causal control",
                "interactivity",
                "long-horizon consistency",
                "prompt-to-full-video",
                "Generative Latent Prediction (GLP)",
                "autoregressive latent dynamics",
                "large language model (LLM)",
                "video diffusion decoder",
                "perceptually detailed",
                "temporally coherent",
                "open-domain",
                "action-conditioned simulation",
                "long-horizon forecasting",
                "simulative reasoning"
            ]
        },
        "publishedAt": "2025-11-12T02:20:35.000Z",
        "title": "PAN: A World Model for General, Interactable, and Long-Horizon World Simulation",
        "summary": "A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.09057.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "61c4b5c871a107e9d80e33fb",
            "avatarUrl": "/avatars/128a2a9393b0f7921323d0d94db4ecad.svg",
            "fullname": "Jiannan Xiang",
            "name": "jiannanx",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.08521",
            "authors": [
                {
                    "_id": "691708afb63bfc66e04987ca",
                    "user": {
                        "_id": "642d26c4c5f19fe0da07284a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xJwh9iTDJcHLxaRT0t8OV.png",
                        "isPro": false,
                        "fullname": "Zhengyang Liang",
                        "user": "chr1ce",
                        "type": "user"
                    },
                    "name": "Zhengyang Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-14T14:22:03.942Z",
                    "hidden": false
                },
                {
                    "_id": "691708afb63bfc66e04987cb",
                    "name": "Daoan Zhang",
                    "hidden": false
                },
                {
                    "_id": "691708afb63bfc66e04987cc",
                    "name": "Huichi Zhou",
                    "hidden": false
                },
                {
                    "_id": "691708afb63bfc66e04987cd",
                    "name": "Rui Huang",
                    "hidden": false
                },
                {
                    "_id": "691708afb63bfc66e04987ce",
                    "name": "Bobo Li",
                    "hidden": false
                },
                {
                    "_id": "691708afb63bfc66e04987cf",
                    "name": "Yuechen Zhang",
                    "hidden": false
                },
                {
                    "_id": "691708afb63bfc66e04987d0",
                    "name": "Shengqiong Wu",
                    "hidden": false
                },
                {
                    "_id": "691708afb63bfc66e04987d1",
                    "name": "Xiaohan Wang",
                    "hidden": false
                },
                {
                    "_id": "691708afb63bfc66e04987d2",
                    "name": "Jiebo Luo",
                    "hidden": false
                },
                {
                    "_id": "691708afb63bfc66e04987d3",
                    "name": "Lizi Liao",
                    "hidden": false
                },
                {
                    "_id": "691708afb63bfc66e04987d4",
                    "user": {
                        "_id": "647773a1168cb428e00e9a8f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
                        "isPro": false,
                        "fullname": "Hao Fei",
                        "user": "scofield7419",
                        "type": "user"
                    },
                    "name": "Hao Fei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-14T14:22:06.106Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/87spQgoG8jmSzQqzVR09j.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/RDvGE1r3gWhsd0fFgt-kp.png",
                "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/6NmH0FkOgnIpuk20Sfny5.png",
                "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/oVgbiR5oxZ3lAi7hal44z.png",
                "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/i3EAUVF0RRLChjwixQ4tN.png",
                "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/530GBdoT6mls8JDJOsefD.png"
            ],
            "publishedAt": "2025-11-11T17:58:13.000Z",
            "submittedOnDailyAt": "2025-11-14T08:20:20.734Z",
            "title": "UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist",
            "submittedOnDailyBy": {
                "_id": "647773a1168cb428e00e9a8f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
                "isPro": false,
                "fullname": "Hao Fei",
                "user": "scofield7419",
                "type": "user"
            },
            "summary": "While specialized AI models excel at isolated video tasks like generation or understanding, real-world applications demand complex, iterative workflows that combine these capabilities. To bridge this gap, we introduce UniVA, an open-source, omni-capable multi-agent framework for next-generation video generalists that unifies video understanding, segmentation, editing, and generation into cohesive workflows. UniVA employs a Plan-and-Act dual-agent architecture that drives a highly automated and proactive workflow: a planner agent interprets user intentions and decomposes them into structured video-processing steps, while executor agents execute these through modular, MCP-based tool servers (for analysis, generation, editing, tracking, etc.). Through a hierarchical multi-level memory (global knowledge, task context, and user-specific preferences), UniVA sustains long-horizon reasoning, contextual continuity, and inter-agent communication, enabling interactive and self-reflective video creation with full traceability. This design enables iterative and any-conditioned video workflows (e.g., text/image/video-conditioned generation rightarrow multi-round editing rightarrow object segmentation rightarrow compositional synthesis) that were previously cumbersome to achieve with single-purpose models or monolithic video-language models. We also introduce UniVA-Bench, a benchmark suite of multi-step video tasks spanning understanding, editing, segmentation, and generation, to rigorously evaluate such agentic video systems. Both UniVA and UniVA-Bench are fully open-sourced, aiming to catalyze research on interactive, agentic, and general-purpose video intelligence for the next generation of multimodal AI systems. (https://univa.online/)",
            "upvotes": 25,
            "discussionId": "691708afb63bfc66e04987e0",
            "projectPage": "https://univa.online/",
            "githubRepo": "https://github.com/univa-agent/univa",
            "ai_summary": "UniVA is an open-source multi-agent framework that integrates video understanding, segmentation, editing, and generation into cohesive workflows using a Plan-and-Act architecture and hierarchical memory.",
            "ai_keywords": [
                "UniVA",
                "multi-agent framework",
                "Plan-and-Act",
                "planner agent",
                "executor agents",
                "MCP-based tool servers",
                "hierarchical multi-level memory",
                "long-horizon reasoning",
                "contextual continuity",
                "inter-agent communication",
                "UniVA-Bench",
                "multi-step video tasks"
            ],
            "githubStars": 10,
            "organization": {
                "_id": "690b4d26463a29d7e84f9e19",
                "name": "UniVA-Agent",
                "fullname": "UniVA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642d26c4c5f19fe0da07284a/qBXCIO24-uc55pVzDH-Yv.png"
            }
        },
        "publishedAt": "2025-11-11T12:58:13.000Z",
        "title": "UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist",
        "summary": "While specialized AI models excel at isolated video tasks like generation or understanding, real-world applications demand complex, iterative workflows that combine these capabilities. To bridge this gap, we introduce UniVA, an open-source, omni-capable multi-agent framework for next-generation video generalists that unifies video understanding, segmentation, editing, and generation into cohesive workflows. UniVA employs a Plan-and-Act dual-agent architecture that drives a highly automated and proactive workflow: a planner agent interprets user intentions and decomposes them into structured video-processing steps, while executor agents execute these through modular, MCP-based tool servers (for analysis, generation, editing, tracking, etc.). Through a hierarchical multi-level memory (global knowledge, task context, and user-specific preferences), UniVA sustains long-horizon reasoning, contextual continuity, and inter-agent communication, enabling interactive and self-reflective video creation with full traceability. This design enables iterative and any-conditioned video workflows (e.g., text/image/video-conditioned generation rightarrow multi-round editing rightarrow object segmentation rightarrow compositional synthesis) that were previously cumbersome to achieve with single-purpose models or monolithic video-language models. We also introduce UniVA-Bench, a benchmark suite of multi-step video tasks spanning understanding, editing, segmentation, and generation, to rigorously evaluate such agentic video systems. Both UniVA and UniVA-Bench are fully open-sourced, aiming to catalyze research on interactive, agentic, and general-purpose video intelligence for the next generation of multimodal AI systems. (https://univa.online/)",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/87spQgoG8jmSzQqzVR09j.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/RDvGE1r3gWhsd0fFgt-kp.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/6NmH0FkOgnIpuk20Sfny5.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/oVgbiR5oxZ3lAi7hal44z.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/i3EAUVF0RRLChjwixQ4tN.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/530GBdoT6mls8JDJOsefD.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.08521.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647773a1168cb428e00e9a8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
            "fullname": "Hao Fei",
            "name": "scofield7419",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "690b4d26463a29d7e84f9e19",
            "name": "UniVA-Agent",
            "fullname": "UniVA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642d26c4c5f19fe0da07284a/qBXCIO24-uc55pVzDH-Yv.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.10643",
            "authors": [
                {
                    "_id": "691708aab63bfc66e0498796",
                    "name": "Tianzhu Ye",
                    "hidden": false
                },
                {
                    "_id": "691708aab63bfc66e0498797",
                    "user": {
                        "_id": "5df85abada6d0311fd3d5408",
                        "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
                        "isPro": false,
                        "fullname": "Li Dong",
                        "user": "unilm",
                        "type": "user"
                    },
                    "name": "Li Dong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-14T14:22:15.496Z",
                    "hidden": false
                },
                {
                    "_id": "691708aab63bfc66e0498798",
                    "name": "Zewen Chi",
                    "hidden": false
                },
                {
                    "_id": "691708aab63bfc66e0498799",
                    "name": "Xun Wu",
                    "hidden": false
                },
                {
                    "_id": "691708aab63bfc66e049879a",
                    "name": "Shaohan Huang",
                    "hidden": false
                },
                {
                    "_id": "691708aab63bfc66e049879b",
                    "name": "Furu Wei",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64ac0605091ff88865352b44/PMTyQps-TkEKvIpSRJ22H.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64ac0605091ff88865352b44/NlW6JrLeKZMxjyZhJrx2d.png"
            ],
            "publishedAt": "2025-11-13T18:58:37.000Z",
            "submittedOnDailyAt": "2025-11-14T08:37:12.638Z",
            "title": "Black-Box On-Policy Distillation of Large Language Models",
            "submittedOnDailyBy": {
                "_id": "64ac0605091ff88865352b44",
                "avatarUrl": "/avatars/c28acb08a1fdeab899afd1961d3dd94a.svg",
                "isPro": false,
                "fullname": "ytz",
                "user": "ytz20",
                "type": "user"
            },
            "summary": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.",
            "upvotes": 19,
            "discussionId": "691708aab63bfc66e049879c",
            "projectPage": "https://aka.ms/GAD-project",
            "githubRepo": "https://github.com/microsoft/LMOps/tree/main/gad",
            "ai_summary": "Generative Adversarial Distillation (GAD) enhances black-box distillation by framing the student model as a generator and using a discriminator to provide adaptive feedback, surpassing traditional sequence-level knowledge distillation.",
            "ai_keywords": [
                "black-box distillation",
                "large language models (LLMs)",
                "Generative Adversarial Distillation (GAD)",
                "generator",
                "discriminator",
                "minimax game",
                "on-policy reward model",
                "sequence-level knowledge distillation",
                "LMSYS-Chat automatic evaluation"
            ],
            "githubStars": 4171,
            "organization": {
                "_id": "68151d0f51add3813f3f7d1b",
                "name": "MicrosoftResearch",
                "fullname": "Microsoft Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
            }
        },
        "publishedAt": "2025-11-13T13:58:37.000Z",
        "title": "Black-Box On-Policy Distillation of Large Language Models",
        "summary": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64ac0605091ff88865352b44/PMTyQps-TkEKvIpSRJ22H.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64ac0605091ff88865352b44/NlW6JrLeKZMxjyZhJrx2d.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.10643.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ac0605091ff88865352b44",
            "avatarUrl": "/avatars/c28acb08a1fdeab899afd1961d3dd94a.svg",
            "fullname": "ytz",
            "name": "ytz20",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "68151d0f51add3813f3f7d1b",
            "name": "MicrosoftResearch",
            "fullname": "Microsoft Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.09780",
            "authors": [
                {
                    "_id": "69175497a6cea0a2e76c976c",
                    "name": "Nikolay Blagoev",
                    "hidden": false
                },
                {
                    "_id": "69175497a6cea0a2e76c976d",
                    "name": "OÄŸuzhan Ersoy",
                    "hidden": false
                },
                {
                    "_id": "69175497a6cea0a2e76c976e",
                    "name": "Lydia Yiyu Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-12T22:29:07.000Z",
            "submittedOnDailyAt": "2025-11-14T13:46:52.953Z",
            "title": "Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO",
            "submittedOnDailyBy": {
                "_id": "673c712c8a9ba30a57741a8c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c712c8a9ba30a57741a8c/khBSYmO5V1BM8Q_uPLLWB.jpeg",
                "isPro": false,
                "fullname": "Oguzhan Ersoy",
                "user": "oguzer",
                "type": "user"
            },
            "summary": "Group Relative Policy Optimization (GRPO) has demonstrated great utilization in post-training of Large Language Models (LLMs). In GRPO, prompts are answered by the model and, through reinforcement learning, preferred completions are learnt. Owing to the small communication volume, GRPO is inherently suitable for decentralised training as the prompts can be concurrently answered by multiple nodes and then exchanged in the forms of strings. In this work, we present the first adversarial attack in decentralised GRPO. We demonstrate that malicious parties can poison such systems by injecting arbitrary malicious tokens in benign models in both out-of-context and in-context attacks. Using empirical examples of math and coding tasks, we show that adversarial attacks can easily poison the benign nodes, polluting their local LLM post-training, achieving attack success rates up to 100% in as few as 50 iterations. We propose two ways to defend against these attacks, depending on whether all users train the same model or different models. We show that these defenses can achieve stop rates of up to 100%, making the attack impossible.",
            "upvotes": 18,
            "discussionId": "69175498a6cea0a2e76c976f",
            "ai_summary": "The study identifies and defends against adversarial attacks in decentralized Group Relative Policy Optimization (GRPO) for Large Language Models (LLMs), demonstrating attack success rates of up to 100% and proposing effective defense mechanisms.",
            "ai_keywords": [
                "Group Relative Policy Optimization",
                "GRPO",
                "Large Language Models",
                "LLMs",
                "reinforcement learning",
                "decentralised training",
                "adversarial attacks",
                "malicious tokens",
                "out-of-context attacks",
                "in-context attacks",
                "attack success rates",
                "defense mechanisms"
            ],
            "organization": {
                "_id": "66d2530bb26010e571f0ea9b",
                "name": "Gensyn",
                "fullname": "Gensyn",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66d252ec8a438492b0d6e4ce/KD6rJavI2N74-2aT7NiZc.jpeg"
            }
        },
        "publishedAt": "2025-11-12T17:29:07.000Z",
        "title": "Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO",
        "summary": "Group Relative Policy Optimization (GRPO) has demonstrated great utilization in post-training of Large Language Models (LLMs). In GRPO, prompts are answered by the model and, through reinforcement learning, preferred completions are learnt. Owing to the small communication volume, GRPO is inherently suitable for decentralised training as the prompts can be concurrently answered by multiple nodes and then exchanged in the forms of strings. In this work, we present the first adversarial attack in decentralised GRPO. We demonstrate that malicious parties can poison such systems by injecting arbitrary malicious tokens in benign models in both out-of-context and in-context attacks. Using empirical examples of math and coding tasks, we show that adversarial attacks can easily poison the benign nodes, polluting their local LLM post-training, achieving attack success rates up to 100% in as few as 50 iterations. We propose two ways to defend against these attacks, depending on whether all users train the same model or different models. We show that these defenses can achieve stop rates of up to 100%, making the attack impossible.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.09780.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "673c712c8a9ba30a57741a8c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c712c8a9ba30a57741a8c/khBSYmO5V1BM8Q_uPLLWB.jpeg",
            "fullname": "Oguzhan Ersoy",
            "name": "oguzer",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "66d2530bb26010e571f0ea9b",
            "name": "Gensyn",
            "fullname": "Gensyn",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66d252ec8a438492b0d6e4ce/KD6rJavI2N74-2aT7NiZc.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.10647",
            "authors": [
                {
                    "_id": "6916d5069a1d7af6ca2f01d5",
                    "name": "Haotong Lin",
                    "hidden": false
                },
                {
                    "_id": "6916d5069a1d7af6ca2f01d6",
                    "name": "Sili Chen",
                    "hidden": false
                },
                {
                    "_id": "6916d5069a1d7af6ca2f01d7",
                    "name": "Junhao Liew",
                    "hidden": false
                },
                {
                    "_id": "6916d5069a1d7af6ca2f01d8",
                    "name": "Donny Y. Chen",
                    "hidden": false
                },
                {
                    "_id": "6916d5069a1d7af6ca2f01d9",
                    "name": "Zhenyu Li",
                    "hidden": false
                },
                {
                    "_id": "6916d5069a1d7af6ca2f01da",
                    "name": "Guang Shi",
                    "hidden": false
                },
                {
                    "_id": "6916d5069a1d7af6ca2f01db",
                    "name": "Jiashi Feng",
                    "hidden": false
                },
                {
                    "_id": "6916d5069a1d7af6ca2f01dc",
                    "name": "Bingyi Kang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-13T18:59:53.000Z",
            "submittedOnDailyAt": "2025-11-14T06:16:02.269Z",
            "title": "Depth Anything 3: Recovering the Visual Space from Any Views",
            "submittedOnDailyBy": {
                "_id": "63a369d98c0c89dcae3b8329",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
                "isPro": true,
                "fullname": "Adina Yakefu",
                "user": "AdinaY",
                "type": "user"
            },
            "summary": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.",
            "upvotes": 15,
            "discussionId": "6916d5069a1d7af6ca2f01dd",
            "projectPage": "https://depth-anything-3.github.io/",
            "githubRepo": "https://github.com/ByteDance-Seed/depth-anything-3",
            "ai_summary": "Depth Anything 3 (DA3) uses a plain transformer for geometry prediction from visual inputs, achieving state-of-the-art results in camera pose estimation, any-view geometry, visual rendering, and monocular depth estimation.",
            "ai_keywords": [
                "plain transformer",
                "vanilla DINO encoder",
                "depth-ray prediction",
                "teacher-student training paradigm",
                "visual geometry benchmark",
                "camera pose estimation",
                "any-view geometry",
                "visual rendering",
                "monocular depth estimation",
                "state-of-the-art (SOTA)",
                "VGGT"
            ],
            "githubStars": 528,
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-11-13T13:59:53.000Z",
        "title": "Depth Anything 3: Recovering the Visual Space from Any Views",
        "summary": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.10647.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a369d98c0c89dcae3b8329",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
            "fullname": "Adina Yakefu",
            "name": "AdinaY",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1177
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.01918",
            "authors": [
                {
                    "_id": "690ae7b8d70e173c8452907b",
                    "user": {
                        "_id": "66a607f8d187b6539c2be8df",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a607f8d187b6539c2be8df/GHM23ueGwH9pjSXPjK6We.png",
                        "isPro": false,
                        "fullname": "Ahmet Erdem Pamuk",
                        "user": "ahmeterdempmk",
                        "type": "user"
                    },
                    "name": "Ahmet Erdem Pamuk",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-10T09:32:19.680Z",
                    "hidden": false
                },
                {
                    "_id": "690ae7b8d70e173c8452907c",
                    "user": {
                        "_id": "65f6b3223a0e5e2d20c0bc7e",
                        "avatarUrl": "/avatars/f21307e39bdddf0c94464db970586541.svg",
                        "isPro": false,
                        "fullname": "Emir Kaan Ã–zdemir",
                        "user": "emirkaanozdemr",
                        "type": "user"
                    },
                    "name": "Emir Kaan Ã–zdemir",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-10T21:03:17.117Z",
                    "hidden": false
                },
                {
                    "_id": "690ae7b8d70e173c8452907d",
                    "user": {
                        "_id": "668c299c95a7493f14fe3bcc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nMuuypcbWFQcDD5xNo4Zt.jpeg",
                        "isPro": true,
                        "fullname": "Åžuayp Talha Kocabay",
                        "user": "suayptalha",
                        "type": "user"
                    },
                    "name": "Åžuayp Talha Kocabay",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-05T16:24:26.781Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-01T16:37:55.000Z",
            "submittedOnDailyAt": "2025-11-14T10:08:44.216Z",
            "title": "Superpositional Gradient Descent: Harnessing Quantum Principles for\n  Model Training",
            "submittedOnDailyBy": {
                "_id": "668c299c95a7493f14fe3bcc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nMuuypcbWFQcDD5xNo4Zt.jpeg",
                "isPro": true,
                "fullname": "Åžuayp Talha Kocabay",
                "user": "suayptalha",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are increasingly trained with classical\noptimization techniques like AdamW to improve convergence and generalization.\nHowever, the mechanisms by which quantum-inspired methods enhance classical\ntraining remain underexplored. We introduce Superpositional Gradient Descent\n(SGD), a novel optimizer linking gradient updates with quantum superposition by\ninjecting quantum circuit perturbations. We present a mathematical framework\nand implement hybrid quantum-classical circuits in PyTorch and Qiskit. On\nsynthetic sequence classification and large-scale LLM fine-tuning, SGD\nconverges faster and yields lower final loss than AdamW. Despite promising\nresults, scalability and hardware constraints limit adoption. Overall, this\nwork provides new insights into the intersection of quantum computing and deep\nlearning, suggesting practical pathways for leveraging quantum principles to\ncontrol and enhance model behavior.",
            "upvotes": 9,
            "discussionId": "690ae7b9d70e173c8452907e",
            "githubRepo": "https://github.com/The-Aqua-Labs/Superpositional-Gradient-Descent",
            "ai_summary": "Superpositional Gradient Descent, a quantum-inspired optimizer, improves convergence and reduces final loss in large language model training compared to AdamW.",
            "ai_keywords": [
                "Superpositional Gradient Descent",
                "SGD",
                "quantum superposition",
                "quantum circuit perturbations",
                "hybrid quantum-classical circuits",
                "PyTorch",
                "Qiskit",
                "sequence classification",
                "large-scale LLM fine-tuning"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-11-01T12:37:55.000Z",
        "title": "Superpositional Gradient Descent: Harnessing Quantum Principles for\n  Model Training",
        "summary": "Large language models (LLMs) are increasingly trained with classical\noptimization techniques like AdamW to improve convergence and generalization.\nHowever, the mechanisms by which quantum-inspired methods enhance classical\ntraining remain underexplored. We introduce Superpositional Gradient Descent\n(SGD), a novel optimizer linking gradient updates with quantum superposition by\ninjecting quantum circuit perturbations. We present a mathematical framework\nand implement hybrid quantum-classical circuits in PyTorch and Qiskit. On\nsynthetic sequence classification and large-scale LLM fine-tuning, SGD\nconverges faster and yields lower final loss than AdamW. Despite promising\nresults, scalability and hardware constraints limit adoption. Overall, this\nwork provides new insights into the intersection of quantum computing and deep\nlearning, suggesting practical pathways for leveraging quantum principles to\ncontrol and enhance model behavior.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01918.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "668c299c95a7493f14fe3bcc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nMuuypcbWFQcDD5xNo4Zt.jpeg",
            "fullname": "Åžuayp Talha Kocabay",
            "name": "suayptalha",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 123
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.09030",
            "authors": [
                {
                    "_id": "691769e2a6cea0a2e76c977d",
                    "name": "Elliot Meyerson",
                    "hidden": false
                },
                {
                    "_id": "691769e2a6cea0a2e76c977e",
                    "name": "Giuseppe Paolo",
                    "hidden": false
                },
                {
                    "_id": "691769e2a6cea0a2e76c977f",
                    "name": "Roberto Dailey",
                    "hidden": false
                },
                {
                    "_id": "691769e2a6cea0a2e76c9780",
                    "name": "Hormoz Shahrzad",
                    "hidden": false
                },
                {
                    "_id": "691769e2a6cea0a2e76c9781",
                    "name": "Olivier Francon",
                    "hidden": false
                },
                {
                    "_id": "691769e2a6cea0a2e76c9782",
                    "name": "Conor F. Hayes",
                    "hidden": false
                },
                {
                    "_id": "691769e2a6cea0a2e76c9783",
                    "name": "Xin Qiu",
                    "hidden": false
                },
                {
                    "_id": "691769e2a6cea0a2e76c9784",
                    "name": "Babak Hodjat",
                    "hidden": false
                },
                {
                    "_id": "691769e2a6cea0a2e76c9785",
                    "name": "Risto Miikkulainen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6514b7fde1273c28705142cc/lBBb-Yt_Dh9v57FHHLyGm.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6514b7fde1273c28705142cc/PW55Oov18C91W_Sp8YWaC.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6514b7fde1273c28705142cc/V9YZt8ugEwUDPplCgkgEp.png"
            ],
            "publishedAt": "2025-11-12T06:27:55.000Z",
            "submittedOnDailyAt": "2025-11-14T15:31:56.859Z",
            "title": "Solving a Million-Step LLM Task with Zero Errors",
            "submittedOnDailyBy": {
                "_id": "6514b7fde1273c28705142cc",
                "avatarUrl": "/avatars/072bf14abd8ef17d9393338a20157cc2.svg",
                "isPro": false,
                "fullname": "Elliot Meyerson",
                "user": "ekmeyerson",
                "type": "user"
            },
            "summary": "LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.",
            "upvotes": 8,
            "discussionId": "691769e3a6cea0a2e76c9786",
            "ai_summary": "MAKER, a system using microagents with error correction, successfully solves tasks with over a million LLM steps, suggesting a new approach for scaling LLM capabilities.",
            "ai_keywords": [
                "LLMs",
                "Towers of Hanoi",
                "MAKER",
                "microagents",
                "extreme decomposition",
                "multi-agent voting scheme",
                "MDAPs"
            ],
            "organization": {
                "_id": "640a094096aae6497413e0b6",
                "name": "CognizantAI",
                "fullname": "Cognizant",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/4nUmMEInO3B4_jJT8MEk4.jpeg"
            }
        },
        "publishedAt": "2025-11-12T01:27:55.000Z",
        "title": "Solving a Million-Step LLM Task with Zero Errors",
        "summary": "LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6514b7fde1273c28705142cc/lBBb-Yt_Dh9v57FHHLyGm.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6514b7fde1273c28705142cc/PW55Oov18C91W_Sp8YWaC.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6514b7fde1273c28705142cc/V9YZt8ugEwUDPplCgkgEp.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.09030.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6514b7fde1273c28705142cc",
            "avatarUrl": "/avatars/072bf14abd8ef17d9393338a20157cc2.svg",
            "fullname": "Elliot Meyerson",
            "name": "ekmeyerson",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "640a094096aae6497413e0b6",
            "name": "CognizantAI",
            "fullname": "Cognizant",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/4nUmMEInO3B4_jJT8MEk4.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.08522",
            "authors": [
                {
                    "_id": "691708acb63bfc66e04987b7",
                    "name": "Zhaojian Yu",
                    "hidden": false
                },
                {
                    "_id": "691708acb63bfc66e04987b8",
                    "name": "Kaiyue Feng",
                    "hidden": false
                },
                {
                    "_id": "691708acb63bfc66e04987b9",
                    "name": "Yilun Zhao",
                    "hidden": false
                },
                {
                    "_id": "691708acb63bfc66e04987ba",
                    "name": "Shilin He",
                    "hidden": false
                },
                {
                    "_id": "691708acb63bfc66e04987bb",
                    "name": "Xiao-Ping Zhang",
                    "hidden": false
                },
                {
                    "_id": "691708acb63bfc66e04987bc",
                    "name": "Arman Cohan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-11T18:03:22.000Z",
            "submittedOnDailyAt": "2025-11-14T08:45:31.747Z",
            "title": "AlphaResearch: Accelerating New Algorithm Discovery with Language Models",
            "submittedOnDailyBy": {
                "_id": "646f3443c261dc413383b8a4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f3443c261dc413383b8a4/7R2XX3VqQ6DmgQoMU7Un4.jpeg",
                "isPro": false,
                "fullname": "Zhaojian Yu",
                "user": "zjy2001",
                "type": "user"
            },
            "summary": "Large language models have made significant progress in complex but easy-to-verify problems, yet they still struggle with discovering the unknown. In this paper, we present AlphaResearch, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct a novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote a transparent evaluation process, we construct AlphaResearchComp, a new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the ``packing circles'' problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct a comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research.",
            "upvotes": 5,
            "discussionId": "691708adb63bfc66e04987bd",
            "githubRepo": "https://github.com/answers111/alpha-research",
            "ai_summary": "AlphaResearch, an autonomous research agent, discovers new algorithms in open-ended problems with a dual research environment, achieving competitive performance against human researchers in a benchmark competition.",
            "ai_keywords": [
                "AlphaResearch",
                "dual research environment",
                "execution-based verification",
                "simulated real-world peer review",
                "algorithm discovery",
                "AlphaResearchComp",
                "open-ended algorithmic problems",
                "packing circles",
                "AlphaEvolve"
            ],
            "githubStars": 18
        },
        "publishedAt": "2025-11-11T13:03:22.000Z",
        "title": "AlphaResearch: Accelerating New Algorithm Discovery with Language Models",
        "summary": "Large language models have made significant progress in complex but easy-to-verify problems, yet they still struggle with discovering the unknown. In this paper, we present AlphaResearch, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct a novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote a transparent evaluation process, we construct AlphaResearchComp, a new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the ``packing circles'' problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct a comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.08522.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646f3443c261dc413383b8a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f3443c261dc413383b8a4/7R2XX3VqQ6DmgQoMU7Un4.jpeg",
            "fullname": "Zhaojian Yu",
            "name": "zjy2001",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.10507",
            "authors": [
                {
                    "_id": "691739c1a6cea0a2e76c970d",
                    "name": "Yun He",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c970e",
                    "name": "Wenzhe Li",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c970f",
                    "name": "Hejia Zhang",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c9710",
                    "name": "Songlin Li",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c9711",
                    "name": "Karishma Mandyam",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c9712",
                    "name": "Sopan Khosla",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c9713",
                    "name": "Yuanhao Xiong",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c9714",
                    "name": "Nanshu Wang",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c9715",
                    "name": "Selina Peng",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c9716",
                    "name": "Beibin Li",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c9717",
                    "name": "Shengjie Bi",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c9718",
                    "name": "Shishir G. Patil",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c9719",
                    "name": "Qi Qi",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c971a",
                    "name": "Shengyu Feng",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c971b",
                    "name": "Julian Katz-Samuels",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c971c",
                    "name": "Richard Yuanzhe Pang",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c971d",
                    "name": "Sujan Gonugondla",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c971e",
                    "name": "Hunter Lang",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c971f",
                    "name": "Yue Yu",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c9720",
                    "name": "Yundi Qian",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c9721",
                    "name": "Maryam Fazel-Zarandi",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c9722",
                    "name": "Licheng Yu",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c9723",
                    "name": "Amine Benhalloum",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c9724",
                    "name": "Hany Awadalla",
                    "hidden": false
                },
                {
                    "_id": "691739c1a6cea0a2e76c9725",
                    "name": "Manaal Faruqui",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-13T17:14:01.000Z",
            "submittedOnDailyAt": "2025-11-14T11:46:59.644Z",
            "title": "Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent progress in large language models (LLMs) has led to impressive performance on a range of tasks, yet advanced instruction following (IF)-especially for complex, multi-turn, and system-prompted instructions-remains a significant challenge. Rigorous evaluation and effective training for such capabilities are hindered by the lack of high-quality, human-annotated benchmarks and reliable, interpretable reward signals. In this work, we introduce AdvancedIF (we will release this benchmark soon), a comprehensive benchmark featuring over 1,600 prompts and expert-curated rubrics that assess LLMs ability to follow complex, multi-turn, and system-level instructions. We further propose RIFL (Rubric-based Instruction-Following Learning), a novel post-training pipeline that leverages rubric generation, a finetuned rubric verifier, and reward shaping to enable effective reinforcement learning for instruction following. Extensive experiments demonstrate that RIFL substantially improves the instruction-following abilities of LLMs, achieving a 6.7% absolute gain on AdvancedIF and strong results on public benchmarks. Our ablation studies confirm the effectiveness of each component in RIFL. This work establishes rubrics as a powerful tool for both training and evaluating advanced IF in LLMs, paving the way for more capable and reliable AI systems.",
            "upvotes": 3,
            "discussionId": "691739c1a6cea0a2e76c9726",
            "ai_summary": "AdvancedIF benchmark and RIFL pipeline improve instruction-following capabilities in large language models by using expert-curated rubrics and reinforcement learning techniques.",
            "ai_keywords": [
                "large language models (LLMs)",
                "instruction following (IF)",
                "multi-turn instructions",
                "system-prompted instructions",
                "human-annotated benchmarks",
                "rubrics",
                "reinforcement learning",
                "rubric generation",
                "rubric verifier",
                "reward shaping"
            ]
        },
        "publishedAt": "2025-11-13T12:14:01.000Z",
        "title": "Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following",
        "summary": "Recent progress in large language models (LLMs) has led to impressive performance on a range of tasks, yet advanced instruction following (IF)-especially for complex, multi-turn, and system-prompted instructions-remains a significant challenge. Rigorous evaluation and effective training for such capabilities are hindered by the lack of high-quality, human-annotated benchmarks and reliable, interpretable reward signals. In this work, we introduce AdvancedIF (we will release this benchmark soon), a comprehensive benchmark featuring over 1,600 prompts and expert-curated rubrics that assess LLMs ability to follow complex, multi-turn, and system-level instructions. We further propose RIFL (Rubric-based Instruction-Following Learning), a novel post-training pipeline that leverages rubric generation, a finetuned rubric verifier, and reward shaping to enable effective reinforcement learning for instruction following. Extensive experiments demonstrate that RIFL substantially improves the instruction-following abilities of LLMs, achieving a 6.7% absolute gain on AdvancedIF and strong results on public benchmarks. Our ablation studies confirm the effectiveness of each component in RIFL. This work establishes rubrics as a powerful tool for both training and evaluating advanced IF in LLMs, paving the way for more capable and reliable AI systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.10507.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 162
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.10289",
            "authors": [
                {
                    "_id": "6916aa949a1d7af6ca2f019e",
                    "user": {
                        "_id": "62c9664eb34e600d7eaa4beb",
                        "avatarUrl": "/avatars/ca23ecdec2d31c99ecce97d9b180ae0c.svg",
                        "isPro": false,
                        "fullname": "Ghosh",
                        "user": "Sreyan88",
                        "type": "user"
                    },
                    "name": "Sreyan Ghosh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-14T14:22:19.454Z",
                    "hidden": false
                },
                {
                    "_id": "6916aa949a1d7af6ca2f019f",
                    "name": "Arushi Goel",
                    "hidden": false
                },
                {
                    "_id": "6916aa949a1d7af6ca2f01a0",
                    "name": "Lasha Koroshinadze",
                    "hidden": false
                },
                {
                    "_id": "6916aa949a1d7af6ca2f01a1",
                    "name": "Sang-gil Lee",
                    "hidden": false
                },
                {
                    "_id": "6916aa949a1d7af6ca2f01a2",
                    "name": "Zhifeng Kong",
                    "hidden": false
                },
                {
                    "_id": "6916aa949a1d7af6ca2f01a3",
                    "name": "Joao Felipe Santos",
                    "hidden": false
                },
                {
                    "_id": "6916aa949a1d7af6ca2f01a4",
                    "name": "Ramani Duraiswami",
                    "hidden": false
                },
                {
                    "_id": "6916aa949a1d7af6ca2f01a5",
                    "name": "Dinesh Manocha",
                    "hidden": false
                },
                {
                    "_id": "6916aa949a1d7af6ca2f01a6",
                    "name": "Wei Ping",
                    "hidden": false
                },
                {
                    "_id": "6916aa949a1d7af6ca2f01a7",
                    "name": "Mohammad Shoeybi",
                    "hidden": false
                },
                {
                    "_id": "6916aa949a1d7af6ca2f01a8",
                    "name": "Bryan Catanzaro",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-13T13:21:09.000Z",
            "submittedOnDailyAt": "2025-11-14T09:21:17.028Z",
            "title": "Music Flamingo: Scaling Music Understanding in Audio Language Models",
            "submittedOnDailyBy": {
                "_id": "62c9664eb34e600d7eaa4beb",
                "avatarUrl": "/avatars/ca23ecdec2d31c99ecce97d9b180ae0c.svg",
                "isPro": false,
                "fullname": "Ghosh",
                "user": "Sreyan88",
                "type": "user"
            },
            "summary": "We introduce Music Flamingo, a novel large audio-language model designed to advance music (including song) understanding in foundational audio models. While audio-language research has progressed rapidly, music remains challenging due to its dynamic, layered, and information-dense nature. Progress has been further limited by the difficulty of scaling open audio understanding models, primarily because of the scarcity of high-quality music data and annotations. As a result, prior models are restricted to producing short, high-level captions, answering only surface-level questions, and showing limited generalization across diverse musical cultures. To address these challenges, we curate MF-Skills, a large-scale dataset labeled through a multi-stage pipeline that yields rich captions and question-answer pairs covering harmony, structure, timbre, lyrics, and cultural context. We fine-tune an enhanced Audio Flamingo 3 backbone on MF-Skills and further strengthen multiple skills relevant to music understanding. To improve the model's reasoning abilities, we introduce a post-training recipe: we first cold-start with MF-Think, a novel chain-of-thought dataset grounded in music theory, followed by GRPO-based reinforcement learning with custom rewards. Music Flamingo achieves state-of-the-art results across 10+ benchmarks for music understanding and reasoning, establishing itself as a generalist and musically intelligent audio-language model. Beyond strong empirical results, Music Flamingo sets a new standard for advanced music understanding by demonstrating how models can move from surface-level recognition toward layered, human-like perception of songs. We believe this work provides both a benchmark and a foundation for the community to build the next generation of models that engage with music as meaningfully as humans do.",
            "upvotes": 3,
            "discussionId": "6916aa949a1d7af6ca2f01cd",
            "projectPage": "https://research.nvidia.com/labs/adlr/MF/",
            "ai_summary": "Music Flamingo, a large audio-language model, advances music understanding through fine-tuning on a rich dataset and post-training with novel methods, achieving state-of-the-art results across various benchmarks.",
            "ai_keywords": [
                "audio-language model",
                "Music Flamingo",
                "MF-Skills",
                "multi-stage pipeline",
                "Audio Flamingo 3",
                "fine-tuning",
                "MF-Think",
                "chain-of-thought dataset",
                "GRPO-based reinforcement learning",
                "custom rewards",
                "music understanding",
                "reasoning",
                "benchmarks",
                "human-like perception"
            ],
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-11-13T08:21:09.000Z",
        "title": "Music Flamingo: Scaling Music Understanding in Audio Language Models",
        "summary": "We introduce Music Flamingo, a novel large audio-language model designed to advance music (including song) understanding in foundational audio models. While audio-language research has progressed rapidly, music remains challenging due to its dynamic, layered, and information-dense nature. Progress has been further limited by the difficulty of scaling open audio understanding models, primarily because of the scarcity of high-quality music data and annotations. As a result, prior models are restricted to producing short, high-level captions, answering only surface-level questions, and showing limited generalization across diverse musical cultures. To address these challenges, we curate MF-Skills, a large-scale dataset labeled through a multi-stage pipeline that yields rich captions and question-answer pairs covering harmony, structure, timbre, lyrics, and cultural context. We fine-tune an enhanced Audio Flamingo 3 backbone on MF-Skills and further strengthen multiple skills relevant to music understanding. To improve the model's reasoning abilities, we introduce a post-training recipe: we first cold-start with MF-Think, a novel chain-of-thought dataset grounded in music theory, followed by GRPO-based reinforcement learning with custom rewards. Music Flamingo achieves state-of-the-art results across 10+ benchmarks for music understanding and reasoning, establishing itself as a generalist and musically intelligent audio-language model. Beyond strong empirical results, Music Flamingo sets a new standard for advanced music understanding by demonstrating how models can move from surface-level recognition toward layered, human-like perception of songs. We believe this work provides both a benchmark and a foundation for the community to build the next generation of models that engage with music as meaningfully as humans do.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.10289.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c9664eb34e600d7eaa4beb",
            "avatarUrl": "/avatars/ca23ecdec2d31c99ecce97d9b180ae0c.svg",
            "fullname": "Ghosh",
            "name": "Sreyan88",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.10547",
            "authors": [
                {
                    "_id": "691739eba6cea0a2e76c9728",
                    "name": "Isabela Albuquerque",
                    "hidden": false
                },
                {
                    "_id": "691739eba6cea0a2e76c9729",
                    "name": "Ira Ktena",
                    "hidden": false
                },
                {
                    "_id": "691739eba6cea0a2e76c972a",
                    "name": "Olivia Wiles",
                    "hidden": false
                },
                {
                    "_id": "691739eba6cea0a2e76c972b",
                    "name": "Ivana KajiÄ‡",
                    "hidden": false
                },
                {
                    "_id": "691739eba6cea0a2e76c972c",
                    "name": "Amal Rannen-Triki",
                    "hidden": false
                },
                {
                    "_id": "691739eba6cea0a2e76c972d",
                    "name": "Cristina Vasconcelos",
                    "hidden": false
                },
                {
                    "_id": "691739eba6cea0a2e76c972e",
                    "name": "Aida Nematzadeh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-13T17:48:38.000Z",
            "submittedOnDailyAt": "2025-11-14T11:47:31.321Z",
            "title": "Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests.\n  Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development.",
            "upvotes": 2,
            "discussionId": "691739eba6cea0a2e76c972f",
            "ai_summary": "A framework for evaluating diversity in text-to-image models through human assessment and systematic analysis of image embeddings.",
            "ai_keywords": [
                "text-to-image (T2I) models",
                "diversity evaluation",
                "human evaluation template",
                "prompt set",
                "factors of variation",
                "image embeddings",
                "binomial tests"
            ],
            "organization": {
                "_id": "5e6aca39878b8b2bf9806447",
                "name": "google",
                "fullname": "Google",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
            }
        },
        "publishedAt": "2025-11-13T12:48:38.000Z",
        "title": "Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation",
        "summary": "Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests.\n  Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.10547.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 162
        },
        "organization": {
            "_id": "5e6aca39878b8b2bf9806447",
            "name": "google",
            "fullname": "Google",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.07685",
            "authors": [
                {
                    "_id": "691708a2b63bfc66e0498739",
                    "name": "Manasi Sharma",
                    "hidden": false
                },
                {
                    "_id": "691708a2b63bfc66e049873a",
                    "name": "Chen Bo Calvin Zhang",
                    "hidden": false
                },
                {
                    "_id": "691708a2b63bfc66e049873b",
                    "name": "Chaithanya Bandi",
                    "hidden": false
                },
                {
                    "_id": "691708a2b63bfc66e049873c",
                    "name": "Clinton Wang",
                    "hidden": false
                },
                {
                    "_id": "691708a2b63bfc66e049873d",
                    "name": "Ankit Aich",
                    "hidden": false
                },
                {
                    "_id": "691708a2b63bfc66e049873e",
                    "name": "Huy Nghiem",
                    "hidden": false
                },
                {
                    "_id": "691708a2b63bfc66e049873f",
                    "name": "Tahseen Rabbani",
                    "hidden": false
                },
                {
                    "_id": "691708a2b63bfc66e0498740",
                    "name": "Ye Htet",
                    "hidden": false
                },
                {
                    "_id": "691708a2b63bfc66e0498741",
                    "name": "Brian Jang",
                    "hidden": false
                },
                {
                    "_id": "691708a2b63bfc66e0498742",
                    "name": "Sumana Basu",
                    "hidden": false
                },
                {
                    "_id": "691708a2b63bfc66e0498743",
                    "name": "Aishwarya Balwani",
                    "hidden": false
                },
                {
                    "_id": "691708a2b63bfc66e0498744",
                    "name": "Denis Peskoff",
                    "hidden": false
                },
                {
                    "_id": "691708a2b63bfc66e0498745",
                    "name": "Marcos Ayestaran",
                    "hidden": false
                },
                {
                    "_id": "691708a2b63bfc66e0498746",
                    "name": "Sean M. Hendryx",
                    "hidden": false
                },
                {
                    "_id": "691708a2b63bfc66e0498747",
                    "name": "Brad Kenstler",
                    "hidden": false
                },
                {
                    "_id": "691708a2b63bfc66e0498748",
                    "name": "Bing Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-10T23:07:14.000Z",
            "submittedOnDailyAt": "2025-11-14T11:48:39.799Z",
            "title": "ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.",
            "upvotes": 2,
            "discussionId": "691708a2b63bfc66e0498749",
            "ai_summary": "ResearchRubrics is a benchmark for evaluating deep research agents, using expert rubrics to assess their factual grounding, reasoning, and clarity across diverse, complex tasks.",
            "ai_keywords": [
                "large language models",
                "multi-step reasoning",
                "cross-document synthesis",
                "evidence-backed",
                "long-form answers",
                "ResearchRubrics",
                "factual grounding",
                "reasoning soundness",
                "clarity",
                "complexity framework",
                "conceptual breadth",
                "logical nesting",
                "exploration",
                "rubric adherence",
                "Gemini's DR",
                "OpenAI's DR"
            ],
            "organization": {
                "_id": "6677220f8a4064c02bc81217",
                "name": "ScaleAI",
                "fullname": "Scale AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65d6a5f94c28026a003581b4/uqHyTuNQ8fX7LheVhzPeO.png"
            }
        },
        "publishedAt": "2025-11-10T18:07:14.000Z",
        "title": "ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents",
        "summary": "Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07685.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 162
        },
        "organization": {
            "_id": "6677220f8a4064c02bc81217",
            "name": "ScaleAI",
            "fullname": "Scale AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65d6a5f94c28026a003581b4/uqHyTuNQ8fX7LheVhzPeO.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.10047",
            "authors": [
                {
                    "_id": "69170a64b63bfc66e0498a36",
                    "name": "Xurui Li",
                    "hidden": false
                },
                {
                    "_id": "69170a64b63bfc66e0498a37",
                    "name": "Feng Xue",
                    "hidden": false
                },
                {
                    "_id": "69170a64b63bfc66e0498a38",
                    "name": "Yu Zhou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65f9a3628d0b9e1104e599cb/nv6pnNBw8crrscWcaIDJX.png"
            ],
            "publishedAt": "2025-11-13T07:47:37.000Z",
            "submittedOnDailyAt": "2025-11-14T08:30:34.800Z",
            "title": "MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples",
            "submittedOnDailyBy": {
                "_id": "65f9a3628d0b9e1104e599cb",
                "avatarUrl": "/avatars/95a35197220c45f4f22a966c9f2b3386.svg",
                "isPro": false,
                "fullname": "xuruili",
                "user": "xrli-U",
                "type": "user"
            },
            "summary": "Zero-shot anomaly classification (AC) and segmentation (AS) methods aim to identify and outline defects without using any labeled samples. In this paper, we reveal a key property that is overlooked by existing methods: normal image patches across industrial products typically find many other similar patches, not only in 2D appearance but also in 3D shapes, while anomalies remain diverse and isolated. To explicitly leverage this discriminative property, we propose a Mutual Scoring framework (MuSc-V2) for zero-shot AC/AS, which flexibly supports single 2D/3D or multimodality. Specifically, our method begins by improving 3D representation through Iterative Point Grouping (IPG), which reduces false positives from discontinuous surfaces. Then we use Similarity Neighborhood Aggregation with Multi-Degrees (SNAMD) to fuse 2D/3D neighborhood cues into more discriminative multi-scale patch features for mutual scoring. The core comprises a Mutual Scoring Mechanism (MSM) that lets samples within each modality to assign score to each other, and Cross-modal Anomaly Enhancement (CAE) that fuses 2D and 3D scores to recover modality-specific missing anomalies. Finally, Re-scoring with Constrained Neighborhood (RsCon) suppresses false classification based on similarity to more representative samples. Our framework flexibly works on both the full dataset and smaller subsets with consistently robust performance, ensuring seamless adaptability across diverse product lines. In aid of the novel framework, MuSc-V2 achieves significant performance improvements: a +23.7% AP gain on the MVTec 3D-AD dataset and a +19.3% boost on the Eyecandies dataset, surpassing previous zero-shot benchmarks and even outperforming most few-shot methods. The code will be available at The code will be available at https://github.com/HUST-SLOW/MuSc-V2{https://github.com/HUST-SLOW/MuSc-V2}.",
            "upvotes": 1,
            "discussionId": "69170a64b63bfc66e0498a39",
            "ai_summary": "MuSc-V2 framework improves zero-shot anomaly detection by leveraging mutual scoring and similarity aggregation in both 2D and 3D data, achieving significant performance gains over existing benchmarks.",
            "ai_keywords": [
                "Mutual Scoring framework",
                "MuSc-V2",
                "zero-shot anomaly classification",
                "zero-shot anomaly segmentation",
                "Iterative Point Grouping",
                "IPG",
                "Similarity Neighborhood Aggregation with Multi-Degrees",
                "SNAMD",
                "Mutual Scoring Mechanism",
                "MSM",
                "Cross-modal Anomaly Enhancement",
                "CAE",
                "Re-scoring with Constrained Neighborhood",
                "RsCon",
                "MVTec 3D-AD dataset",
                "Eyecandies dataset"
            ],
            "organization": {
                "_id": "63ea5d1ee7f148f46015ebb3",
                "name": "Huster",
                "fullname": "Huazhong University of Science and Technology",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676303638801-63ea5a04e7f148f46015c770.jpeg"
            }
        },
        "publishedAt": "2025-11-13T02:47:37.000Z",
        "title": "MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples",
        "summary": "Zero-shot anomaly classification (AC) and segmentation (AS) methods aim to identify and outline defects without using any labeled samples. In this paper, we reveal a key property that is overlooked by existing methods: normal image patches across industrial products typically find many other similar patches, not only in 2D appearance but also in 3D shapes, while anomalies remain diverse and isolated. To explicitly leverage this discriminative property, we propose a Mutual Scoring framework (MuSc-V2) for zero-shot AC/AS, which flexibly supports single 2D/3D or multimodality. Specifically, our method begins by improving 3D representation through Iterative Point Grouping (IPG), which reduces false positives from discontinuous surfaces. Then we use Similarity Neighborhood Aggregation with Multi-Degrees (SNAMD) to fuse 2D/3D neighborhood cues into more discriminative multi-scale patch features for mutual scoring. The core comprises a Mutual Scoring Mechanism (MSM) that lets samples within each modality to assign score to each other, and Cross-modal Anomaly Enhancement (CAE) that fuses 2D and 3D scores to recover modality-specific missing anomalies. Finally, Re-scoring with Constrained Neighborhood (RsCon) suppresses false classification based on similarity to more representative samples. Our framework flexibly works on both the full dataset and smaller subsets with consistently robust performance, ensuring seamless adaptability across diverse product lines. In aid of the novel framework, MuSc-V2 achieves significant performance improvements: a +23.7% AP gain on the MVTec 3D-AD dataset and a +19.3% boost on the Eyecandies dataset, surpassing previous zero-shot benchmarks and even outperforming most few-shot methods. The code will be available at The code will be available at https://github.com/HUST-SLOW/MuSc-V2{https://github.com/HUST-SLOW/MuSc-V2}.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65f9a3628d0b9e1104e599cb/nv6pnNBw8crrscWcaIDJX.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.10047.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f9a3628d0b9e1104e599cb",
            "avatarUrl": "/avatars/95a35197220c45f4f22a966c9f2b3386.svg",
            "fullname": "xuruili",
            "name": "xrli-U",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "63ea5d1ee7f148f46015ebb3",
            "name": "Huster",
            "fullname": "Huazhong University of Science and Technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676303638801-63ea5a04e7f148f46015c770.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.10017",
            "authors": [
                {
                    "_id": "69173a80a6cea0a2e76c9739",
                    "name": "Xinyi Wang",
                    "hidden": false
                },
                {
                    "_id": "69173a80a6cea0a2e76c973a",
                    "name": "Xun Yang",
                    "hidden": false
                },
                {
                    "_id": "69173a80a6cea0a2e76c973b",
                    "name": "Yanlong Xu",
                    "hidden": false
                },
                {
                    "_id": "69173a80a6cea0a2e76c973c",
                    "name": "Yuchen Wu",
                    "hidden": false
                },
                {
                    "_id": "69173a80a6cea0a2e76c973d",
                    "name": "Zhen Li",
                    "hidden": false
                },
                {
                    "_id": "69173a80a6cea0a2e76c973e",
                    "name": "Na Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-13T06:43:00.000Z",
            "submittedOnDailyAt": "2025-11-14T11:50:14.635Z",
            "title": "AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs.",
            "upvotes": 1,
            "discussionId": "69173a81a6cea0a2e76c973f",
            "ai_summary": "AffordBot, a framework combining Multimodal Large Language Models with chain-of-thought reasoning, achieves state-of-the-art performance in predicting affordance elements' spatial locations, motion types, and axes in 3D scenes based on task instructions.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "chain-of-thought reasoning",
                "Fine-grained 3D Embodied Reasoning",
                "surround-view images",
                "viewpoint selection",
                "active perception",
                "affordance elements",
                "SceneFun3D dataset"
            ]
        },
        "publishedAt": "2025-11-13T01:43:00.000Z",
        "title": "AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models",
        "summary": "Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.10017.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 162
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.09715",
            "authors": [
                {
                    "_id": "69173a08a6cea0a2e76c9731",
                    "name": "Arman Zarei",
                    "hidden": false
                },
                {
                    "_id": "69173a08a6cea0a2e76c9732",
                    "name": "Samyadeep Basu",
                    "hidden": false
                },
                {
                    "_id": "69173a08a6cea0a2e76c9733",
                    "name": "Mobina Pournemat",
                    "hidden": false
                },
                {
                    "_id": "69173a08a6cea0a2e76c9734",
                    "name": "Sayan Nag",
                    "hidden": false
                },
                {
                    "_id": "69173a08a6cea0a2e76c9735",
                    "name": "Ryan Rossi",
                    "hidden": false
                },
                {
                    "_id": "69173a08a6cea0a2e76c9736",
                    "name": "Soheil Feizi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-12T20:21:37.000Z",
            "submittedOnDailyAt": "2025-11-14T11:48:08.865Z",
            "title": "SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the user's ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control.",
            "upvotes": 0,
            "discussionId": "69173a08a6cea0a2e76c9737",
            "ai_summary": "SliderEdit enables continuous, fine-grained control over image editing instructions by using low-rank adaptation matrices, improving edit controllability, visual consistency, and user steerability.",
            "ai_keywords": [
                "SliderEdit",
                "fine-grained instruction control",
                "low-rank adaptation matrices",
                "FLUX-Kontext",
                "Qwen-Image-Edit",
                "continuous interpolation",
                "spatial locality",
                "global semantic consistency"
            ]
        },
        "publishedAt": "2025-11-12T15:21:37.000Z",
        "title": "SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control",
        "summary": "Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the user's ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.09715.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 162
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.09067",
            "authors": [
                {
                    "_id": "69170105a0c1cdddcec37283",
                    "name": "Gailun Zeng",
                    "hidden": false
                },
                {
                    "_id": "69170105a0c1cdddcec37284",
                    "name": "Ziyang Luo",
                    "hidden": false
                },
                {
                    "_id": "69170105a0c1cdddcec37285",
                    "name": "Hongzhan Lin",
                    "hidden": false
                },
                {
                    "_id": "69170105a0c1cdddcec37286",
                    "name": "Yuchen Tian",
                    "hidden": false
                },
                {
                    "_id": "69170105a0c1cdddcec37287",
                    "name": "Kaixin Li",
                    "hidden": false
                },
                {
                    "_id": "69170105a0c1cdddcec37288",
                    "name": "Ziyang Gong",
                    "hidden": false
                },
                {
                    "_id": "69170105a0c1cdddcec37289",
                    "name": "Jianxiong Guo",
                    "hidden": false
                },
                {
                    "_id": "69170105a0c1cdddcec3728a",
                    "name": "Jing Ma",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-12T07:43:26.000Z",
            "submittedOnDailyAt": "2025-11-14T23:02:57.975Z",
            "title": "MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique",
            "submittedOnDailyBy": {
                "_id": "6499466c7d1edf7cb612a9a6",
                "avatarUrl": "/avatars/c2e18594aa0879db8226f2a04496fb0b.svg",
                "isPro": false,
                "fullname": "Hongzhan Lin",
                "user": "danielhzlin",
                "type": "user"
            },
            "summary": "The ability of critique is vital for models to self-improve and serve as reliable AI assistants. While extensively studied in language-only settings, multimodal critique of Large Multimodal Models (LMMs) remains underexplored despite their growing capabilities in tasks like captioning and visual reasoning. In this work, we introduce MM-CRITIC, a holistic benchmark for evaluating the critique ability of LMMs across multiple dimensions: basic, correction, and comparison. Covering 8 main task types and over 500 tasks, MM-CRITIC collects responses from various LMMs with different model sizes and is composed of 4471 samples. To enhance the evaluation reliability, we integrate expert-informed ground answers into scoring rubrics that guide GPT-4o in annotating responses and generating reference critiques, which serve as anchors for trustworthy judgments. Extensive experiments validate the effectiveness of MM-CRITIC and provide a comprehensive assessment of leading LMMs' critique capabilities under multiple dimensions. Further analysis reveals some key insights, including the correlation between response quality and critique, and varying critique difficulty across evaluation dimensions. Our code is available at https://github.com/MichealZeng0420/MM-Critic.",
            "upvotes": 0,
            "discussionId": "69170105a0c1cdddcec372a3",
            "ai_summary": "MM-CRITIC is a benchmark for evaluating the critique abilities of Large Multimodal Models across multiple dimensions and tasks, using expert-informed ground answers for reliable scoring.",
            "ai_keywords": [
                "Large Multimodal Models",
                "LMMs",
                "MM-CRITIC",
                "critique ability",
                "evaluation dimensions",
                "task types",
                "expert-informed ground answers",
                "scoring rubrics",
                "GPT-4o",
                "response quality",
                "critique difficulty"
            ],
            "organization": {
                "_id": "620ef4fa9291e41cab585a65",
                "name": "HKBU-NLP",
                "fullname": "HKBU NLP Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6090ff099a8bcaa437b234a4/O9slXTk_HHSMqEOpFiAkw.png"
            }
        },
        "publishedAt": "2025-11-12T02:43:26.000Z",
        "title": "MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique",
        "summary": "The ability of critique is vital for models to self-improve and serve as reliable AI assistants. While extensively studied in language-only settings, multimodal critique of Large Multimodal Models (LMMs) remains underexplored despite their growing capabilities in tasks like captioning and visual reasoning. In this work, we introduce MM-CRITIC, a holistic benchmark for evaluating the critique ability of LMMs across multiple dimensions: basic, correction, and comparison. Covering 8 main task types and over 500 tasks, MM-CRITIC collects responses from various LMMs with different model sizes and is composed of 4471 samples. To enhance the evaluation reliability, we integrate expert-informed ground answers into scoring rubrics that guide GPT-4o in annotating responses and generating reference critiques, which serve as anchors for trustworthy judgments. Extensive experiments validate the effectiveness of MM-CRITIC and provide a comprehensive assessment of leading LMMs' critique capabilities under multiple dimensions. Further analysis reveals some key insights, including the correlation between response quality and critique, and varying critique difficulty across evaluation dimensions. Our code is available at https://github.com/MichealZeng0420/MM-Critic.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.09067.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6499466c7d1edf7cb612a9a6",
            "avatarUrl": "/avatars/c2e18594aa0879db8226f2a04496fb0b.svg",
            "fullname": "Hongzhan Lin",
            "name": "danielhzlin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "620ef4fa9291e41cab585a65",
            "name": "HKBU-NLP",
            "fullname": "HKBU NLP Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6090ff099a8bcaa437b234a4/O9slXTk_HHSMqEOpFiAkw.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.07790",
            "authors": [
                {
                    "_id": "691778dba6cea0a2e76c9788",
                    "name": "Rochana R. Obadage",
                    "hidden": false
                },
                {
                    "_id": "691778dba6cea0a2e76c9789",
                    "name": "Sarah M. Rajtmajer",
                    "hidden": false
                },
                {
                    "_id": "691778dba6cea0a2e76c978a",
                    "name": "Jian Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-11T03:13:17.000Z",
            "submittedOnDailyAt": "2025-11-14T16:19:58.847Z",
            "title": "CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis",
            "submittedOnDailyBy": {
                "_id": "68c20ff2ecd360f9e78325af",
                "avatarUrl": "/avatars/41451fe7d43e0803db82479cbfc03450.svg",
                "isPro": false,
                "fullname": "Rochana R. Obadage",
                "user": "rochanaro",
                "type": "user"
            },
            "summary": "Sentiments about the reproducibility of cited papers in downstream literature offer community perspectives and have shown as a promising signal of the actual reproducibility of published findings. To train effective models to effectively predict reproducibility-oriented sentiments and further systematically study their correlation with reproducibility, we introduce the CC30k dataset, comprising a total of 30,734 citation contexts in machine learning papers. Each citation context is labeled with one of three reproducibility-oriented sentiment labels: Positive, Negative, or Neutral, reflecting the cited paper's perceived reproducibility or replicability. Of these, 25,829 are labeled through crowdsourcing, supplemented with negatives generated through a controlled pipeline to counter the scarcity of negative labels. Unlike traditional sentiment analysis datasets, CC30k focuses on reproducibility-oriented sentiments, addressing a research gap in resources for computational reproducibility studies. The dataset was created through a pipeline that includes robust data cleansing, careful crowd selection, and thorough validation. The resulting dataset achieves a labeling accuracy of 94%. We then demonstrated that the performance of three large language models significantly improves on the reproducibility-oriented sentiment classification after fine-tuning using our dataset. The dataset lays the foundation for large-scale assessments of the reproducibility of machine learning papers. The CC30k dataset and the Jupyter notebooks used to produce and analyze the dataset are publicly available at https://github.com/lamps-lab/CC30k .",
            "upvotes": 0,
            "discussionId": "691778dba6cea0a2e76c978b",
            "ai_summary": "The CC30k dataset, comprising citation contexts labeled with reproducibility-oriented sentiments, enhances the accuracy of large language models in predicting the reproducibility of machine learning papers.",
            "ai_keywords": [
                "CC30k dataset",
                "citation contexts",
                "reproducibility-oriented sentiments",
                "sentiment classification",
                "large language models",
                "fine-tuning",
                "computational reproducibility",
                "data cleansing",
                "crowd selection",
                "validation"
            ]
        },
        "publishedAt": "2025-11-10T22:13:17.000Z",
        "title": "CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis",
        "summary": "Sentiments about the reproducibility of cited papers in downstream literature offer community perspectives and have shown as a promising signal of the actual reproducibility of published findings. To train effective models to effectively predict reproducibility-oriented sentiments and further systematically study their correlation with reproducibility, we introduce the CC30k dataset, comprising a total of 30,734 citation contexts in machine learning papers. Each citation context is labeled with one of three reproducibility-oriented sentiment labels: Positive, Negative, or Neutral, reflecting the cited paper's perceived reproducibility or replicability. Of these, 25,829 are labeled through crowdsourcing, supplemented with negatives generated through a controlled pipeline to counter the scarcity of negative labels. Unlike traditional sentiment analysis datasets, CC30k focuses on reproducibility-oriented sentiments, addressing a research gap in resources for computational reproducibility studies. The dataset was created through a pipeline that includes robust data cleansing, careful crowd selection, and thorough validation. The resulting dataset achieves a labeling accuracy of 94%. We then demonstrated that the performance of three large language models significantly improves on the reproducibility-oriented sentiment classification after fine-tuning using our dataset. The dataset lays the foundation for large-scale assessments of the reproducibility of machine learning papers. The CC30k dataset and the Jupyter notebooks used to produce and analyze the dataset are publicly available at https://github.com/lamps-lab/CC30k .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07790.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68c20ff2ecd360f9e78325af",
            "avatarUrl": "/avatars/41451fe7d43e0803db82479cbfc03450.svg",
            "fullname": "Rochana R. Obadage",
            "name": "rochanaro",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]