[
    {
        "paper": {
            "id": "2511.14993",
            "authors": [
                {
                    "_id": "691e819a3c64d32b036458c0",
                    "name": "Vladimir Arkhipkin",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c1",
                    "name": "Vladimir Korviakov",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c2",
                    "name": "Nikolai Gerasimenko",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c3",
                    "name": "Denis Parkhomenko",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c4",
                    "user": {
                        "_id": "64e4c7764af6c29a0697f57b",
                        "avatarUrl": "/avatars/efc4e9f9b105586fd090b22a1bc7dbb7.svg",
                        "isPro": false,
                        "fullname": "Viacheslav Vasilev",
                        "user": "vvasilev",
                        "type": "user"
                    },
                    "name": "Viacheslav Vasilev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:49:10.246Z",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c5",
                    "user": {
                        "_id": "68838d809080cc7010edf5e2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68838d809080cc7010edf5e2/xBqg5ggt_PfLkiDLmsZxx.jpeg",
                        "isPro": false,
                        "fullname": "Alexey Letunovskiy",
                        "user": "AlexeyLetunovskiy",
                        "type": "user"
                    },
                    "name": "Alexey Letunovskiy",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:49:55.594Z",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c6",
                    "name": "Maria Kovaleva",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c7",
                    "name": "Nikolai Vaulin",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c8",
                    "name": "Ivan Kirillov",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c9",
                    "name": "Lev Novitskiy",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458ca",
                    "name": "Denis Koposov",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458cb",
                    "user": {
                        "_id": "6628b73c35d27082500034f2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628b73c35d27082500034f2/CznOeIbjzJ9DmJaGzlWPD.jpeg",
                        "isPro": false,
                        "fullname": "Nikita Kiselev",
                        "user": "kisnikser",
                        "type": "user"
                    },
                    "name": "Nikita Kiselev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:49:11.927Z",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458cc",
                    "name": "Alexander Varlamov",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458cd",
                    "name": "Dmitrii Mikhailov",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458ce",
                    "name": "Vladimir Polovnikov",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458cf",
                    "name": "Andrey Shutkin",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d0",
                    "name": "Ilya Vasiliev",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d1",
                    "name": "Julia Agafonova",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d2",
                    "name": "Anastasiia Kargapoltseva",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d3",
                    "name": "Anna Dmitrienko",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d4",
                    "name": "Anastasia Maltseva",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d5",
                    "name": "Anna Averchenkova",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d6",
                    "name": "Olga Kim",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d7",
                    "name": "Tatiana Nikulina",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d8",
                    "user": {
                        "_id": "6669a678465d1d802181e456",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6669a678465d1d802181e456/ZCthBBhDFQnh0bBkgUQUU.png",
                        "isPro": false,
                        "fullname": "Denis Dimitrov",
                        "user": "dendimitrov",
                        "type": "user"
                    },
                    "name": "Denis Dimitrov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:49:08.661Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-19T00:23:22.000Z",
            "submittedOnDailyAt": "2025-11-20T00:19:10.078Z",
            "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.",
            "upvotes": 126,
            "discussionId": "691e819b3c64d32b036458d9",
            "projectPage": "https://kandinskylab.ai/",
            "githubRepo": "https://github.com/kandinskylab/kandinsky-5",
            "ai_summary": "Kandinsky 5.0 is a family of state-of-the-art generative models for high-resolution images and short videos, featuring model lineups with varying parameters and enhanced training techniques to achieve superior quality and performance.",
            "ai_keywords": [
                "foundation models",
                "high-resolution image synthesis",
                "10-second video synthesis",
                "image generation models",
                "text-to-video models",
                "image-to-video models",
                "multi-stage training pipeline",
                "self-supervised fine-tuning",
                "reinforcement learning",
                "pre-training",
                "quality-enhancement techniques",
                "architectural optimizations",
                "training optimizations",
                "inference optimizations",
                "human evaluation",
                "generative framework",
                "open-source code",
                "training checkpoints"
            ],
            "githubStars": 260
        },
        "publishedAt": "2025-11-18T19:23:22.000Z",
        "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation",
        "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.14993.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 169
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.15065",
            "authors": [
                {
                    "_id": "691e828b3c64d32b036458e4",
                    "user": {
                        "_id": "67c443afb753bd020f9c97d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xbACBNLSopWmN5G1K8h_Y.png",
                        "isPro": false,
                        "fullname": "Cheng",
                        "user": "YangC777",
                        "type": "user"
                    },
                    "name": "Cheng Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:49:06.809Z",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458e5",
                    "name": "Haiyuan Wan",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458e6",
                    "name": "Yiran Peng",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458e7",
                    "name": "Xin Cheng",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458e8",
                    "user": {
                        "_id": "640dc84b474aa6f89554d518",
                        "avatarUrl": "/avatars/9fcee1023ed5c6cddb3c342e19f18295.svg",
                        "isPro": false,
                        "fullname": "Zhaoyang Yu",
                        "user": "MoshiQAQ",
                        "type": "user"
                    },
                    "name": "Zhaoyang Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:49:02.234Z",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458e9",
                    "name": "Jiayi Zhang",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458ea",
                    "name": "Junchi Yu",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458eb",
                    "user": {
                        "_id": "67d63e228d5c7a132cbcf39b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ynwA3Sya5irwMRCmSeLiC.png",
                        "isPro": false,
                        "fullname": "neil yu",
                        "user": "yxl66666",
                        "type": "user"
                    },
                    "name": "Xinlei Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:49:04.908Z",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458ec",
                    "name": "Xiawu Zheng",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458ed",
                    "name": "Dongzhan Zhou",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458ee",
                    "name": "Chenglin Wu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/syEvNY_b3iyJgYOQZ5cNg.png"
            ],
            "publishedAt": "2025-11-19T03:18:29.000Z",
            "submittedOnDailyAt": "2025-11-20T00:24:18.639Z",
            "title": "Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.",
            "upvotes": 66,
            "discussionId": "691e828b3c64d32b036458ef",
            "projectPage": "https://imyangc7.github.io/VRBench_Web/",
            "githubRepo": "https://github.com/ImYangC7/VR-Bench",
            "ai_summary": "VR-Bench evaluates video models' spatial reasoning capabilities through maze-solving tasks, demonstrating that these models excel in spatial perception and reasoning, outperforming VLMs and benefiting from diverse sampling during inference.",
            "ai_keywords": [
                "video models",
                "video generation",
                "coherent motion dynamics",
                "spatial reasoning",
                "temporal continuity",
                "VR-Bench",
                "maze-solving tasks",
                "procedural generation",
                "spatial planning",
                "multi-step reasoning",
                "SFT",
                "leading VLMs",
                "test-time scaling effect"
            ],
            "githubStars": 10
        },
        "publishedAt": "2025-11-18T22:18:29.000Z",
        "title": "Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks",
        "summary": "Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/syEvNY_b3iyJgYOQZ5cNg.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15065.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 169
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.15593",
            "authors": [
                {
                    "_id": "691efc453c64d32b03645a2d",
                    "name": "Alexis Audran-Reiss",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a2e",
                    "name": "Jordi Armengol Estap√©",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a2f",
                    "name": "Karen Hambardzumyan",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a30",
                    "name": "Amar Budhiraja",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a31",
                    "name": "Martin Josifoski",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a32",
                    "name": "Edan Toledo",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a33",
                    "name": "Rishi Hazra",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a34",
                    "name": "Despoina Magka",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a35",
                    "name": "Michael Shvartsman",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a36",
                    "name": "Parth Pathak",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a37",
                    "name": "Justine T Kao",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a38",
                    "name": "Lucia Cipolina-Kun",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a39",
                    "name": "Bhavul Gauri",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a3a",
                    "name": "Jean-Christophe Gagnon-Audet",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a3b",
                    "name": "Emanuel Tewolde",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a3c",
                    "name": "Jenny Zhang",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a3d",
                    "name": "Taco Cohen",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a3e",
                    "name": "Yossi Adi",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a3f",
                    "name": "Tatiana Shavrina",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a40",
                    "name": "Yoram Bachrach",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-19T16:32:18.000Z",
            "submittedOnDailyAt": "2025-11-20T11:07:52.822Z",
            "title": "What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity",
            "submittedOnDailyBy": {
                "_id": "6687ee79eee600e418404cc9",
                "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg",
                "isPro": false,
                "fullname": "Amar Budhiraja",
                "user": "ambud26",
                "type": "user"
            },
            "summary": "AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.",
            "upvotes": 46,
            "discussionId": "691efc463c64d32b03645a41",
            "ai_summary": "Ideation diversity significantly enhances the performance of AI research agents across various models and scaffolds on the MLE-bench benchmark.",
            "ai_keywords": [
                "ideation diversity",
                "AI research agents",
                "MLE-bench",
                "agent trajectories",
                "agent scaffolds"
            ],
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "publishedAt": "2025-11-19T11:32:18.000Z",
        "title": "What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity",
        "summary": "AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15593.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6687ee79eee600e418404cc9",
            "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg",
            "fullname": "Amar Budhiraja",
            "name": "ambud26",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.15661",
            "authors": [
                {
                    "_id": "691e8b133c64d32b03645915",
                    "name": "Yicheng He",
                    "hidden": false
                },
                {
                    "_id": "691e8b133c64d32b03645916",
                    "user": {
                        "_id": "62ea79dd01ed9b0e8f61ccd3",
                        "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg",
                        "isPro": false,
                        "fullname": "Chengsong Huang",
                        "user": "ChengsongHuang",
                        "type": "user"
                    },
                    "name": "Chengsong Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:48:55.509Z",
                    "hidden": false
                },
                {
                    "_id": "691e8b133c64d32b03645917",
                    "name": "Zongxia Li",
                    "hidden": false
                },
                {
                    "_id": "691e8b133c64d32b03645918",
                    "name": "Jiaxin Huang",
                    "hidden": false
                },
                {
                    "_id": "691e8b133c64d32b03645919",
                    "name": "Yonghui Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-19T17:55:15.000Z",
            "submittedOnDailyAt": "2025-11-20T00:59:58.366Z",
            "title": "VisPlay: Self-Evolving Vision-Language Models from Images",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/",
            "upvotes": 31,
            "discussionId": "691e8b143c64d32b0364591a",
            "projectPage": "https://bruno686.github.io/VisPlay/",
            "githubRepo": "https://github.com/bruno686/VisPlay",
            "ai_summary": "VisPlay, a self-evolving RL framework, uses unlabeled image data to enhance VLMs' reasoning, generalization, and response quality through two interacting roles and GRPO.",
            "ai_keywords": [
                "Reinforcement learning",
                "Vision-Language Models",
                "self-evolving RL framework",
                "Image-Conditioned Questioner",
                "Multimodal Reasoner",
                "Group Relative Policy Optimization",
                "visual reasoning",
                "compositional generalization",
                "hallucination reduction"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "65448bef5b5d9185ba3202b9",
                "name": "UIUC-CS",
                "fullname": "University of Illinois at Urbana-Champaign",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
            }
        },
        "publishedAt": "2025-11-19T12:55:15.000Z",
        "title": "VisPlay: Self-Evolving Vision-Language Models from Images",
        "summary": "Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15661.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 169
        },
        "organization": {
            "_id": "65448bef5b5d9185ba3202b9",
            "name": "UIUC-CS",
            "fullname": "University of Illinois at Urbana-Champaign",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.15186",
            "authors": [
                {
                    "_id": "691e88213c64d32b0364590c",
                    "user": {
                        "_id": "66714579562ee91c16be977c",
                        "avatarUrl": "/avatars/fd1a386c1e06fcbc9c06017d18e4fe5f.svg",
                        "isPro": false,
                        "fullname": "Geon Choi",
                        "user": "checkone",
                        "type": "user"
                    },
                    "name": "Geon Choi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:48:57.580Z",
                    "hidden": false
                },
                {
                    "_id": "691e88213c64d32b0364590d",
                    "user": {
                        "_id": "62a4d58e81a4b10e93064ad6",
                        "avatarUrl": "/avatars/744d5cbc1745a26b816a458260aba050.svg",
                        "isPro": false,
                        "fullname": "hangyulyoon",
                        "user": "hangyulmd",
                        "type": "user"
                    },
                    "name": "Hangyul Yoon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:48:59.480Z",
                    "hidden": false
                },
                {
                    "_id": "691e88213c64d32b0364590e",
                    "name": "Hyunju Shin",
                    "hidden": false
                },
                {
                    "_id": "691e88213c64d32b0364590f",
                    "name": "Hyunki Park",
                    "hidden": false
                },
                {
                    "_id": "691e88213c64d32b03645910",
                    "name": "Sang Hoon Seo",
                    "hidden": false
                },
                {
                    "_id": "691e88213c64d32b03645911",
                    "name": "Eunho Yang",
                    "hidden": false
                },
                {
                    "_id": "691e88213c64d32b03645912",
                    "name": "Edward Choi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-19T07:17:19.000Z",
            "submittedOnDailyAt": "2025-11-20T00:49:39.884Z",
            "title": "Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset",
            "submittedOnDailyBy": {
                "_id": "62a4d58e81a4b10e93064ad6",
                "avatarUrl": "/avatars/744d5cbc1745a26b816a458260aba050.svg",
                "isPro": false,
                "fullname": "hangyulyoon",
                "user": "hangyulmd",
                "type": "user"
            },
            "summary": "The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by a small number of target labels and the reliance on long, detailed expert-level text inputs, creating a barrier to practical use. To address these limitations, we introduce a new paradigm: instruction-guided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level CXR lesion grounding.",
            "upvotes": 22,
            "discussionId": "691e88213c64d32b03645913",
            "ai_summary": "A new instruction-guided lesion segmentation paradigm using a large-scale dataset and a vision-language model enables diverse CXR lesion segmentation with simple instructions.",
            "ai_keywords": [
                "instruction-guided lesion segmentation",
                "MIMIC-ILS",
                "multimodal pipeline",
                "vision-language model",
                "pixel-level CXR lesion grounding"
            ],
            "organization": {
                "_id": "6475760c33192631bad2bb38",
                "name": "kaist-ai",
                "fullname": "KAIST AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
            }
        },
        "publishedAt": "2025-11-19T02:17:19.000Z",
        "title": "Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset",
        "summary": "The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by a small number of target labels and the reliance on long, detailed expert-level text inputs, creating a barrier to practical use. To address these limitations, we introduce a new paradigm: instruction-guided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level CXR lesion grounding.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15186.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62a4d58e81a4b10e93064ad6",
            "avatarUrl": "/avatars/744d5cbc1745a26b816a458260aba050.svg",
            "fullname": "hangyulyoon",
            "name": "hangyulmd",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6475760c33192631bad2bb38",
            "name": "kaist-ai",
            "fullname": "KAIST AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.14349",
            "authors": [
                {
                    "_id": "691e9ebf3c64d32b03645933",
                    "user": {
                        "_id": "646c5fcc97819a8be938ee9b",
                        "avatarUrl": "/avatars/27976d701e88aebf5855418c055ba50e.svg",
                        "isPro": false,
                        "fullname": "Junfu Pu",
                        "user": "Jevin754",
                        "type": "user"
                    },
                    "name": "Junfu Pu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:48:53.327Z",
                    "hidden": false
                },
                {
                    "_id": "691e9ebf3c64d32b03645934",
                    "name": "Teng Wang",
                    "hidden": false
                },
                {
                    "_id": "691e9ebf3c64d32b03645935",
                    "name": "Yixiao Ge",
                    "hidden": false
                },
                {
                    "_id": "691e9ebf3c64d32b03645936",
                    "name": "Yuying Ge",
                    "hidden": false
                },
                {
                    "_id": "691e9ebf3c64d32b03645937",
                    "name": "Chen Li",
                    "hidden": false
                },
                {
                    "_id": "691e9ebf3c64d32b03645938",
                    "name": "Ying Shan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-18T10:53:14.000Z",
            "submittedOnDailyAt": "2025-11-20T02:44:25.232Z",
            "title": "ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries",
            "submittedOnDailyBy": {
                "_id": "646c5fcc97819a8be938ee9b",
                "avatarUrl": "/avatars/27976d701e88aebf5855418c055ba50e.svg",
                "isPro": false,
                "fullname": "Junfu Pu",
                "user": "Jevin754",
                "type": "user"
            },
            "summary": "The proliferation of hour-long videos (e.g., lectures, podcasts, documentaries) has intensified demand for efficient content structuring. However, existing approaches are constrained by small-scale training with annotations that are typical short and coarse, restricting generalization to nuanced transitions in long videos. We introduce ARC-Chapter, the first large-scale video chaptering model trained on over million-level long video chapters, featuring bilingual, temporally grounded, and hierarchical chapter annotations. To achieve this goal, we curated a bilingual English-Chinese chapter dataset via a structured pipeline that unifies ASR transcripts, scene texts, visual captions into multi-level annotations, from short title to long summaries. We demonstrate clear performance improvements with data scaling, both in data volume and label intensity. Moreover, we design a new evaluation metric termed GRACE, which incorporates many-to-one segment overlaps and semantic similarity, better reflecting real-world chaptering flexibility. Extensive experiments demonstrate that ARC-Chapter establishes a new state-of-the-art by a significant margin, outperforming the previous best by 14.0% in F1 score and 11.3% in SODA score. Moreover, ARC-Chapter shows excellent transferability, improving the state-of-the-art on downstream tasks like dense video captioning on YouCook2.",
            "upvotes": 14,
            "discussionId": "691e9ec03c64d32b03645939",
            "projectPage": "https://arcchapter.github.io/index_en.html",
            "githubRepo": "https://github.com/TencentARC/ARC-Chapter",
            "ai_summary": "ARC-Chapter is a large-scale video chaptering model that improves performance through extensive training data and a new evaluation metric, demonstrating superior results and transferability.",
            "ai_keywords": [
                "video chaptering",
                "ASR transcripts",
                "scene texts",
                "visual captions",
                "multi-level annotations",
                "GRACE",
                "F1 score",
                "SODA score",
                "dense video captioning"
            ],
            "githubStars": 19,
            "organization": {
                "_id": "60e3f7f641ca131919975fe5",
                "name": "TencentARC",
                "fullname": "ARC Lab, Tencent PCG",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1625552871844-60e272ca6c78a8c122b12127.png"
            }
        },
        "publishedAt": "2025-11-18T05:53:14.000Z",
        "title": "ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries",
        "summary": "The proliferation of hour-long videos (e.g., lectures, podcasts, documentaries) has intensified demand for efficient content structuring. However, existing approaches are constrained by small-scale training with annotations that are typical short and coarse, restricting generalization to nuanced transitions in long videos. We introduce ARC-Chapter, the first large-scale video chaptering model trained on over million-level long video chapters, featuring bilingual, temporally grounded, and hierarchical chapter annotations. To achieve this goal, we curated a bilingual English-Chinese chapter dataset via a structured pipeline that unifies ASR transcripts, scene texts, visual captions into multi-level annotations, from short title to long summaries. We demonstrate clear performance improvements with data scaling, both in data volume and label intensity. Moreover, we design a new evaluation metric termed GRACE, which incorporates many-to-one segment overlaps and semantic similarity, better reflecting real-world chaptering flexibility. Extensive experiments demonstrate that ARC-Chapter establishes a new state-of-the-art by a significant margin, outperforming the previous best by 14.0% in F1 score and 11.3% in SODA score. Moreover, ARC-Chapter shows excellent transferability, improving the state-of-the-art on downstream tasks like dense video captioning on YouCook2.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.14349.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646c5fcc97819a8be938ee9b",
            "avatarUrl": "/avatars/27976d701e88aebf5855418c055ba50e.svg",
            "fullname": "Junfu Pu",
            "name": "Jevin754",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "60e3f7f641ca131919975fe5",
            "name": "TencentARC",
            "fullname": "ARC Lab, Tencent PCG",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1625552871844-60e272ca6c78a8c122b12127.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.15586",
            "authors": [
                {
                    "_id": "691e81283c64d32b03645895",
                    "name": "Aaron Ferguson",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b03645896",
                    "name": "Ahmed A. A. Osman",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b03645897",
                    "name": "Berta Bescos",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b03645898",
                    "name": "Carsten Stoll",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b03645899",
                    "name": "Chris Twigg",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b0364589a",
                    "name": "Christoph Lassner",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b0364589b",
                    "name": "David Otte",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b0364589c",
                    "name": "Eric Vignola",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b0364589d",
                    "name": "Federica Bogo",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b0364589e",
                    "name": "Igor Santesteban",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b0364589f",
                    "name": "Javier Romero",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458a0",
                    "name": "Jenna Zarate",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458a1",
                    "name": "Jeongseok Lee",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458a2",
                    "name": "Jinhyung Park",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458a3",
                    "name": "Jinlong Yang",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458a4",
                    "name": "John Doublestein",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458a5",
                    "name": "Kishore Venkateshan",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458a6",
                    "name": "Kris Kitani",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458a7",
                    "name": "Ladislav Kavan",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458a8",
                    "name": "Marco Dal Farra",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458a9",
                    "name": "Matthew Hu",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458aa",
                    "name": "Matthew Cioffi",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458ab",
                    "name": "Michael Fabris",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458ac",
                    "name": "Michael Ranieri",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458ad",
                    "name": "Mohammad Modarres",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458ae",
                    "name": "Petr Kadlecek",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458af",
                    "name": "Rinat Abdrashitov",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458b0",
                    "name": "Romain Pr√©vost",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458b1",
                    "name": "Roman Rajbhandari",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458b2",
                    "name": "Ronald Mallet",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458b3",
                    "name": "Russel Pearsall",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458b4",
                    "name": "Sandy Kao",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458b5",
                    "name": "Sanjeev Kumar",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458b6",
                    "name": "Scott Parrish",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458b7",
                    "name": "Te-Li Wang",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458b8",
                    "name": "Tony Tung",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458b9",
                    "name": "Yuan Dong",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458ba",
                    "name": "Yuhua Chen",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458bb",
                    "name": "Yuanlu Xu",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458bc",
                    "name": "Yuting Ye",
                    "hidden": false
                },
                {
                    "_id": "691e81283c64d32b036458bd",
                    "name": "Zhongshi Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-19T16:18:02.000Z",
            "submittedOnDailyAt": "2025-11-20T00:17:24.940Z",
            "title": "MHR: Momentum Human Rig",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present MHR, a parametric human body model that combines the decoupled skeleton/shape paradigm of ATLAS with a flexible, modern rig and pose corrective system inspired by the Momentum library. Our model enables expressive, anatomically plausible human animation, supporting non-linear pose correctives, and is designed for robust integration in AR/VR and graphics pipelines.",
            "upvotes": 7,
            "discussionId": "691e81293c64d32b036458be",
            "githubRepo": "https://github.com/facebookresearch/MHR",
            "ai_summary": "MHR combines ATLAS's skeleton/shape paradigm with a modern rig to provide expressive, anatomically plausible human animation for AR/VR and graphics.",
            "ai_keywords": [
                "parametric human body model",
                "decoupled skeleton/shape",
                "ATLAS",
                "flexible rig",
                "pose corrective system",
                "Momentum library",
                "non-linear pose correctives",
                "AR/VR",
                "graphics pipelines"
            ],
            "githubStars": 191
        },
        "publishedAt": "2025-11-19T11:18:02.000Z",
        "title": "MHR: Momentum Human Rig",
        "summary": "We present MHR, a parametric human body model that combines the decoupled skeleton/shape paradigm of ATLAS with a flexible, modern rig and pose corrective system inspired by the Momentum library. Our model enables expressive, anatomically plausible human animation, supporting non-linear pose correctives, and is designed for robust integration in AR/VR and graphics pipelines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15586.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 169
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.13524",
            "authors": [
                {
                    "_id": "691c98e6fecc9dfeafbf4760",
                    "user": {
                        "_id": "6755741b9450b23696a5b743",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/yA_TSkSykN_-xkXXAPG1f.png",
                        "isPro": false,
                        "fullname": "Astronaut-PENG",
                        "user": "doraemonILoveYou",
                        "type": "user"
                    },
                    "name": "Yuhang Peng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-19T08:39:45.321Z",
                    "hidden": false
                },
                {
                    "_id": "691c98e6fecc9dfeafbf4761",
                    "name": "Yizhou Pan",
                    "hidden": false
                },
                {
                    "_id": "691c98e6fecc9dfeafbf4762",
                    "name": "Xinning He",
                    "hidden": false
                },
                {
                    "_id": "691c98e6fecc9dfeafbf4763",
                    "name": "Jihaoyu Yang",
                    "hidden": false
                },
                {
                    "_id": "691c98e6fecc9dfeafbf4764",
                    "name": "Xinyu Yin",
                    "hidden": false
                },
                {
                    "_id": "691c98e6fecc9dfeafbf4765",
                    "name": "Han Wang",
                    "hidden": false
                },
                {
                    "_id": "691c98e6fecc9dfeafbf4766",
                    "name": "Xiaoji Zheng",
                    "hidden": false
                },
                {
                    "_id": "691c98e6fecc9dfeafbf4767",
                    "name": "Chao Gao",
                    "hidden": false
                },
                {
                    "_id": "691c98e6fecc9dfeafbf4768",
                    "name": "Jiangtao Gong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-17T15:58:46.000Z",
            "submittedOnDailyAt": "2025-11-20T00:19:29.996Z",
            "title": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI",
            "submittedOnDailyBy": {
                "_id": "6755741b9450b23696a5b743",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/yA_TSkSykN_-xkXXAPG1f.png",
                "isPro": false,
                "fullname": "Astronaut-PENG",
                "user": "doraemonILoveYou",
                "type": "user"
            },
            "summary": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.",
            "upvotes": 6,
            "discussionId": "691c98e6fecc9dfeafbf4769",
            "githubRepo": "https://github.com/AIR-DISCOVER/FreeAskWorld",
            "githubStars": 19
        },
        "publishedAt": "2025-11-17T10:58:46.000Z",
        "title": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI",
        "summary": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13524.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6755741b9450b23696a5b743",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/yA_TSkSykN_-xkXXAPG1f.png",
            "fullname": "Astronaut-PENG",
            "name": "doraemonILoveYou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.15038",
            "authors": [
                {
                    "_id": "691ebbb53c64d32b036459cf",
                    "user": {
                        "_id": "655431b2997379e9b0999d23",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
                        "isPro": false,
                        "fullname": "Dorien Herremans",
                        "user": "dorienh",
                        "type": "user"
                    },
                    "name": "Dorien Herremans",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:48:40.235Z",
                    "hidden": false
                },
                {
                    "_id": "691ebbb53c64d32b036459d0",
                    "name": "Abhinaba Roy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-19T02:12:27.000Z",
            "submittedOnDailyAt": "2025-11-20T04:29:37.742Z",
            "title": "Aligning Generative Music AI with Human Preferences: Methods and Challenges",
            "submittedOnDailyBy": {
                "_id": "655431b2997379e9b0999d23",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
                "isPro": false,
                "fullname": "Dorien Herremans",
                "user": "dorienh",
                "type": "user"
            },
            "summary": "Recent advances in generative AI for music have achieved remarkable fidelity and stylistic diversity, yet these systems often fail to align with nuanced human preferences due to the specific loss functions they use. This paper advocates for the systematic application of preference alignment techniques to music generation, addressing the fundamental gap between computational optimization and human musical appreciation. Drawing on recent breakthroughs including MusicRL's large-scale preference learning, multi-preference alignment frameworks like diffusion-based preference optimization in DiffRhythm+, and inference-time optimization techniques like Text2midi-InferAlign, we discuss how these techniques can address music's unique challenges: temporal coherence, harmonic consistency, and subjective quality assessment. We identify key research challenges including scalability to long-form compositions, reliability amongst others in preference modelling. Looking forward, we envision preference-aligned music generation enabling transformative applications in interactive composition tools and personalized music services. This work calls for sustained interdisciplinary research combining advances in machine learning, music-theory to create music AI systems that truly serve human creative and experiential needs.",
            "upvotes": 2,
            "discussionId": "691ebbb53c64d32b036459d1",
            "ai_summary": "Preference alignment techniques, such as those in MusicRL and DiffRhythm+, are proposed to enhance music generation by addressing human preferences and unique challenges like temporal coherence and harmonic consistency.",
            "ai_keywords": [
                "MusicRL",
                "preference learning",
                "diffusion-based preference optimization",
                "DiffRhythm+",
                "Text2midi-InferAlign",
                "temporal coherence",
                "harmonic consistency",
                "preference-aligned music generation",
                "interactive composition tools",
                "personalized music services"
            ]
        },
        "publishedAt": "2025-11-18T21:12:27.000Z",
        "title": "Aligning Generative Music AI with Human Preferences: Methods and Challenges",
        "summary": "Recent advances in generative AI for music have achieved remarkable fidelity and stylistic diversity, yet these systems often fail to align with nuanced human preferences due to the specific loss functions they use. This paper advocates for the systematic application of preference alignment techniques to music generation, addressing the fundamental gap between computational optimization and human musical appreciation. Drawing on recent breakthroughs including MusicRL's large-scale preference learning, multi-preference alignment frameworks like diffusion-based preference optimization in DiffRhythm+, and inference-time optimization techniques like Text2midi-InferAlign, we discuss how these techniques can address music's unique challenges: temporal coherence, harmonic consistency, and subjective quality assessment. We identify key research challenges including scalability to long-form compositions, reliability amongst others in preference modelling. Looking forward, we envision preference-aligned music generation enabling transformative applications in interactive composition tools and personalized music services. This work calls for sustained interdisciplinary research combining advances in machine learning, music-theory to create music AI systems that truly serve human creative and experiential needs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15038.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655431b2997379e9b0999d23",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
            "fullname": "Dorien Herremans",
            "name": "dorienh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.12207",
            "authors": [
                {
                    "_id": "691ef72c3c64d32b03645a18",
                    "name": "Haozhe Liu",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a19",
                    "name": "Ding Liu",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a1a",
                    "name": "Mingchen Zhuge",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a1b",
                    "name": "Zijian Zhou",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a1c",
                    "name": "Tian Xie",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a1d",
                    "name": "Sen He",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a1e",
                    "name": "Yukang Yang",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a1f",
                    "name": "Shuming Liu",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a20",
                    "name": "Yuren Cong",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a21",
                    "name": "Jiadong Guo",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a22",
                    "name": "Hongyu Xu",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a23",
                    "name": "Ke Xu",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a24",
                    "name": "Kam-Woh Ng",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a25",
                    "name": "Juan C. P√©rez",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a26",
                    "name": "Juan-Manuel~P√©rez-R√∫a",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a27",
                    "name": "Tao Xiang",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a28",
                    "name": "Wei Liu",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a29",
                    "name": "Shikun Liu",
                    "hidden": false
                },
                {
                    "_id": "691ef72c3c64d32b03645a2a",
                    "name": "J√ºrgen Schmidhuber",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/644e24ced6001776ed74b0fb/ry4ktZsB5LELg26BBx6yH.png"
            ],
            "publishedAt": "2025-11-15T13:24:57.000Z",
            "submittedOnDailyAt": "2025-11-20T08:53:32.333Z",
            "title": "Mixture of States: Routing Token-Level Dynamics for Multimodal Generation",
            "submittedOnDailyBy": {
                "_id": "644e24ced6001776ed74b0fb",
                "avatarUrl": "/avatars/1833d681e1c63266f85e4c3697221b70.svg",
                "isPro": false,
                "fullname": "L",
                "user": "Hao-Zhe",
                "type": "user"
            },
            "summary": "We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-k hidden states and is trained with an Œµ-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to 4times larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.",
            "upvotes": 2,
            "discussionId": "691ef72d3c64d32b03645a2b",
            "ai_summary": "MoS, a novel multimodal diffusion model fusion paradigm, achieves state-of-the-art results in text-to-image generation and editing with minimal parameters and computational overhead by using a learnable, token-wise router for modality interaction.",
            "ai_keywords": [
                "MoS",
                "Mixture of States",
                "multimodal diffusion models",
                "token-wise router",
                "denoising timestep",
                "input-dependent interactions",
                "hidden states",
                "diffusion trajectory",
                "top-k hidden states",
                "Œµ-greedy strategy",
                "contextual features",
                "compute-efficient paradigm"
            ],
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "publishedAt": "2025-11-15T08:24:57.000Z",
        "title": "Mixture of States: Routing Token-Level Dynamics for Multimodal Generation",
        "summary": "We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-k hidden states and is trained with an Œµ-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to 4times larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/644e24ced6001776ed74b0fb/ry4ktZsB5LELg26BBx6yH.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.12207.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644e24ced6001776ed74b0fb",
            "avatarUrl": "/avatars/1833d681e1c63266f85e4c3697221b70.svg",
            "fullname": "L",
            "name": "Hao-Zhe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.15706",
            "authors": [
                {
                    "_id": "691fc146cce0eb9b6387aebc",
                    "name": "Johan Edstedt",
                    "hidden": false
                },
                {
                    "_id": "691fc146cce0eb9b6387aebd",
                    "name": "David Nordstr√∂m",
                    "hidden": false
                },
                {
                    "_id": "691fc146cce0eb9b6387aebe",
                    "name": "Yushan Zhang",
                    "hidden": false
                },
                {
                    "_id": "691fc146cce0eb9b6387aebf",
                    "name": "Georg B√∂kman",
                    "hidden": false
                },
                {
                    "_id": "691fc146cce0eb9b6387aec0",
                    "name": "Jonathan Astermark",
                    "hidden": false
                },
                {
                    "_id": "691fc146cce0eb9b6387aec1",
                    "name": "Viktor Larsson",
                    "hidden": false
                },
                {
                    "_id": "691fc146cce0eb9b6387aec2",
                    "name": "Anders Heyden",
                    "hidden": false
                },
                {
                    "_id": "691fc146cce0eb9b6387aec3",
                    "name": "Fredrik Kahl",
                    "hidden": false
                },
                {
                    "_id": "691fc146cce0eb9b6387aec4",
                    "name": "M√•rten Wadenb√§ck",
                    "hidden": false
                },
                {
                    "_id": "691fc146cce0eb9b6387aec5",
                    "name": "Michael Felsberg",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-19T18:59:38.000Z",
            "submittedOnDailyAt": "2025-11-20T23:03:59.674Z",
            "title": "RoMa v2: Harder Better Faster Denser Feature Matching",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": true,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability. In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model. In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through a decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through a custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2",
            "upvotes": 0,
            "discussionId": "691fc146cce0eb9b6387aec6",
            "ai_summary": "A novel dense feature matching model using a custom architecture and loss, combined with DINOv3, achieves state-of-the-art accuracy and efficiency.",
            "ai_keywords": [
                "dense feature matching",
                "matching architecture",
                "matching loss",
                "decoupled two-stage pipeline",
                "CUDA kernel",
                "DINOv3"
            ]
        },
        "publishedAt": "2025-11-19T13:59:38.000Z",
        "title": "RoMa v2: Harder Better Faster Denser Feature Matching",
        "summary": "Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability. In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model. In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through a decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through a custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15706.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1038
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.13001",
            "authors": [
                {
                    "_id": "691d7eb60247f3d258ef331f",
                    "user": {
                        "_id": "62583c070efac682e4df2bbf",
                        "avatarUrl": "/avatars/ae1c0c75946f7d12cdeb823c846ad06f.svg",
                        "isPro": false,
                        "fullname": "Pengcheng Shi",
                        "user": "spc819",
                        "type": "user"
                    },
                    "name": "Pengcheng Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:49:48.890Z",
                    "hidden": false
                },
                {
                    "_id": "691d7eb60247f3d258ef3320",
                    "name": "Jiawei Chen",
                    "hidden": false
                },
                {
                    "_id": "691d7eb60247f3d258ef3321",
                    "name": "Jiaqi Liu",
                    "hidden": false
                },
                {
                    "_id": "691d7eb60247f3d258ef3322",
                    "name": "Xinglin Zhang",
                    "hidden": false
                },
                {
                    "_id": "691d7eb60247f3d258ef3323",
                    "name": "Tao Chen",
                    "hidden": false
                },
                {
                    "_id": "691d7eb60247f3d258ef3324",
                    "name": "Lei Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-17T05:44:19.000Z",
            "submittedOnDailyAt": "2025-11-20T06:23:22.847Z",
            "title": "Medal S: Spatio-Textual Prompt Model for Medical Segmentation",
            "submittedOnDailyBy": {
                "_id": "62583c070efac682e4df2bbf",
                "avatarUrl": "/avatars/ae1c0c75946f7d12cdeb823c846ad06f.svg",
                "isPro": false,
                "fullname": "Pengcheng Shi",
                "user": "spc819",
                "type": "user"
            },
            "summary": "We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.",
            "upvotes": 0,
            "discussionId": "691d7eb60247f3d258ef3325",
            "githubRepo": "https://github.com/yinghemedical/Medal-S",
            "ai_summary": "Medal S is a medical segmentation foundation model that integrates spatial and textual prompts for efficient, high-accuracy multi-class segmentation across various imaging modalities.",
            "ai_keywords": [
                "medical segmentation foundation model",
                "spatial prompts",
                "textual prompts",
                "end-to-end trainable framework",
                "channel-wise alignment",
                "volumetric prompts",
                "text embeddings",
                "3D convolutional module",
                "voxel-space refinement",
                "multi-class segmentation",
                "native-resolution masks",
                "parallel processing",
                "hybrid mode",
                "text-only mode",
                "self-refinement",
                "dynamic resampling",
                "SAT",
                "nnU-Net",
                "data augmentation",
                "text preprocessing",
                "two-stage inference",
                "post-processing techniques",
                "DSC",
                "NSD",
                "F1",
                "DSC TP",
                "multi-class medical segmentation"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-11-17T00:44:19.000Z",
        "title": "Medal S: Spatio-Textual Prompt Model for Medical Segmentation",
        "summary": "We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13001.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62583c070efac682e4df2bbf",
            "avatarUrl": "/avatars/ae1c0c75946f7d12cdeb823c846ad06f.svg",
            "fullname": "Pengcheng Shi",
            "name": "spc819",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    }
]