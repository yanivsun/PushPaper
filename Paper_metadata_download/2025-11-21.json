[
    {
        "paper": {
            "id": "2511.16668",
            "authors": [
                {
                    "_id": "691fcf88cce0eb9b6387af2c",
                    "name": "Yang Luo",
                    "hidden": false
                },
                {
                    "_id": "691fcf88cce0eb9b6387af2d",
                    "name": "Xuanlei Zhao",
                    "hidden": false
                },
                {
                    "_id": "691fcf88cce0eb9b6387af2e",
                    "name": "Baijiong Lin",
                    "hidden": false
                },
                {
                    "_id": "691fcf88cce0eb9b6387af2f",
                    "user": {
                        "_id": "6423f5e6774cc34079730f31",
                        "avatarUrl": "/avatars/7ebeb1f623c86b1a676c95bb67572f8b.svg",
                        "isPro": false,
                        "fullname": "Lingting Zhu",
                        "user": "ltzhu",
                        "type": "user"
                    },
                    "name": "Lingting Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T08:59:57.803Z",
                    "hidden": false
                },
                {
                    "_id": "691fcf88cce0eb9b6387af30",
                    "name": "Liyao Tang",
                    "hidden": false
                },
                {
                    "_id": "691fcf88cce0eb9b6387af31",
                    "name": "Yuqi Liu",
                    "hidden": false
                },
                {
                    "_id": "691fcf88cce0eb9b6387af32",
                    "name": "Ying-Cong Chen",
                    "hidden": false
                },
                {
                    "_id": "691fcf88cce0eb9b6387af33",
                    "user": {
                        "_id": "6380580f42cedbc20c7bef71",
                        "avatarUrl": "/avatars/8d710e0de551cd2bf545cc31fcaf099d.svg",
                        "isPro": false,
                        "fullname": "Shengju Qian",
                        "user": "thesouthfrog",
                        "type": "user"
                    },
                    "name": "Shengju Qian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T08:59:59.823Z",
                    "hidden": false
                },
                {
                    "_id": "691fcf88cce0eb9b6387af34",
                    "name": "Xin Wang",
                    "hidden": false
                },
                {
                    "_id": "691fcf88cce0eb9b6387af35",
                    "name": "Yang You",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T18:59:42.000Z",
            "submittedOnDailyAt": "2025-11-21T00:03:52.875Z",
            "title": "V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.",
            "upvotes": 40,
            "discussionId": "691fcf88cce0eb9b6387af36",
            "projectPage": "https://oahzxl.github.io/VReasonBench/",
            "githubRepo": "https://github.com/yangluo7/V-ReasonBench",
            "ai_summary": "V-ReasonBench evaluates generative video models across structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics using a diverse set of tasks.",
            "ai_keywords": [
                "generative video models",
                "Veo-3",
                "zero-shot reasoning",
                "benchmark",
                "structured problem-solving",
                "spatial cognition",
                "pattern-based inference",
                "physical dynamics",
                "synthetic image sequences",
                "real-world image sequences",
                "answer-verifiable tasks",
                "reproducible",
                "scalable",
                "unambiguous",
                "Chain-of-Frames reasoning",
                "human-aligned reasoning skills"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-11-20T13:59:42.000Z",
        "title": "V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models",
        "summary": "Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16668.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 169
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.15700",
            "authors": [
                {
                    "_id": "691fd565cce0eb9b6387af82",
                    "user": {
                        "_id": "65766a36e09de6aa74649e40",
                        "avatarUrl": "/avatars/0841aba1b62942122544b68c8421cae2.svg",
                        "isPro": false,
                        "fullname": "Jingxi Chen",
                        "user": "jxAIbot",
                        "type": "user"
                    },
                    "name": "Jingxi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T16:49:05.614Z",
                    "hidden": false
                },
                {
                    "_id": "691fd565cce0eb9b6387af83",
                    "user": {
                        "_id": "642447e873f7a0d40b30d677",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642447e873f7a0d40b30d677/hbUnfIhKZCeSSPv9wGAzk.png",
                        "isPro": false,
                        "fullname": "LZX",
                        "user": "zli12321",
                        "type": "user"
                    },
                    "name": "Zongxia Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-21T10:09:00.987Z",
                    "hidden": false
                },
                {
                    "_id": "691fd565cce0eb9b6387af84",
                    "name": "Zhichao Liu",
                    "hidden": false
                },
                {
                    "_id": "691fd565cce0eb9b6387af85",
                    "name": "Guangyao Shi",
                    "hidden": false
                },
                {
                    "_id": "691fd565cce0eb9b6387af86",
                    "name": "Xiyang Wu",
                    "hidden": false
                },
                {
                    "_id": "691fd565cce0eb9b6387af87",
                    "name": "Fuxiao Liu",
                    "hidden": false
                },
                {
                    "_id": "691fd565cce0eb9b6387af88",
                    "name": "Cornelia Fermuller",
                    "hidden": false
                },
                {
                    "_id": "691fd565cce0eb9b6387af89",
                    "name": "Brandon Y. Feng",
                    "hidden": false
                },
                {
                    "_id": "691fd565cce0eb9b6387af8a",
                    "name": "Yiannis Aloimonos",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/642447e873f7a0d40b30d677/vlbD6Bxk_YNiKzB6TGjOV.gif"
            ],
            "publishedAt": "2025-11-19T18:56:50.000Z",
            "submittedOnDailyAt": "2025-11-21T00:36:01.876Z",
            "title": "First Frame Is the Place to Go for Video Content Customization",
            "submittedOnDailyBy": {
                "_id": "642447e873f7a0d40b30d677",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642447e873f7a0d40b30d677/hbUnfIhKZCeSSPv9wGAzk.png",
                "isPro": false,
                "fullname": "LZX",
                "user": "zli12321",
                "type": "user"
            },
            "summary": "What role does the first frame play in video generation models? Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation. In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation. Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning. This unveils a powerful, overlooked capability of video generation models for reference-based video customization.",
            "upvotes": 40,
            "discussionId": "691fd565cce0eb9b6387af8b",
            "projectPage": "https://firstframego.github.io",
            "githubRepo": "https://github.com/zli12321/FFGO-Video-Customization?tab=readme-ov-file",
            "ai_summary": "Video generation models use the first frame as a conceptual memory buffer, enabling robust customization with minimal training examples.",
            "ai_keywords": [
                "conceptual memory buffer",
                "video generation models",
                "reference-based video customization"
            ],
            "githubStars": 29,
            "organization": {
                "_id": "68b3c3bbc375e05b059370b2",
                "name": "UMCP",
                "fullname": "University of Maryland College Park",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"
            }
        },
        "publishedAt": "2025-11-19T13:56:50.000Z",
        "title": "First Frame Is the Place to Go for Video Content Customization",
        "summary": "What role does the first frame play in video generation models? Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation. In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation. Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning. This unveils a powerful, overlooked capability of video generation models for reference-based video customization.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/642447e873f7a0d40b30d677/vlbD6Bxk_YNiKzB6TGjOV.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15700.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "642447e873f7a0d40b30d677",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642447e873f7a0d40b30d677/hbUnfIhKZCeSSPv9wGAzk.png",
            "fullname": "LZX",
            "name": "zli12321",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "68b3c3bbc375e05b059370b2",
            "name": "UMCP",
            "fullname": "University of Maryland College Park",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.15848",
            "authors": [
                {
                    "_id": "691fd89fcce0eb9b6387af99",
                    "name": "Fei Tian",
                    "hidden": false
                },
                {
                    "_id": "691fd89fcce0eb9b6387af9a",
                    "user": {
                        "_id": "6524e60c5edccefd13125fd3",
                        "avatarUrl": "/avatars/6daae919255d94f6aa761fb45349650a.svg",
                        "isPro": false,
                        "fullname": "Xiangyu",
                        "user": "Tonyyouyou1",
                        "type": "user"
                    },
                    "name": "Xiangyu Tony Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T08:59:45.995Z",
                    "hidden": false
                },
                {
                    "_id": "691fd89fcce0eb9b6387af9b",
                    "name": "Yuxin Zhang",
                    "hidden": false
                },
                {
                    "_id": "691fd89fcce0eb9b6387af9c",
                    "name": "Haoyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "691fd89fcce0eb9b6387af9d",
                    "name": "Yuxin Li",
                    "hidden": false
                },
                {
                    "_id": "691fd89fcce0eb9b6387af9e",
                    "name": "Daijiao Liu",
                    "hidden": false
                },
                {
                    "_id": "691fd89fcce0eb9b6387af9f",
                    "name": "Yayue Deng",
                    "hidden": false
                },
                {
                    "_id": "691fd89fcce0eb9b6387afa0",
                    "name": "Donghang Wu",
                    "hidden": false
                },
                {
                    "_id": "691fd89fcce0eb9b6387afa1",
                    "name": "Jun Chen",
                    "hidden": false
                },
                {
                    "_id": "691fd89fcce0eb9b6387afa2",
                    "name": "Liang Zhao",
                    "hidden": false
                },
                {
                    "_id": "691fd89fcce0eb9b6387afa3",
                    "name": "Chengyuan Yao",
                    "hidden": false
                },
                {
                    "_id": "691fd89fcce0eb9b6387afa4",
                    "name": "Hexin Liu",
                    "hidden": false
                },
                {
                    "_id": "691fd89fcce0eb9b6387afa5",
                    "name": "Eng Siong Chng",
                    "hidden": false
                },
                {
                    "_id": "691fd89fcce0eb9b6387afa6",
                    "name": "Xuerui Yang",
                    "hidden": false
                },
                {
                    "_id": "691fd89fcce0eb9b6387afa7",
                    "name": "Xiangyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "691fd89fcce0eb9b6387afa8",
                    "name": "Daxin Jiang",
                    "hidden": false
                },
                {
                    "_id": "691fd89fcce0eb9b6387afa9",
                    "user": {
                        "_id": "63417332c5565a4b8d43a0d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63417332c5565a4b8d43a0d8/MmnYG7Wu2Z_lqCBvTfJmy.png",
                        "isPro": false,
                        "fullname": "Gang Yu",
                        "user": "skicy",
                        "type": "user"
                    },
                    "name": "Gang Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T08:59:43.455Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-19T20:12:50.000Z",
            "submittedOnDailyAt": "2025-11-21T00:43:02.457Z",
            "title": "Step-Audio-R1 Technical Report",
            "submittedOnDailyBy": {
                "_id": "66518fd07d8cb2629a514c18",
                "avatarUrl": "/avatars/6280b33a6b1532ee938afd4aa303f709.svg",
                "isPro": false,
                "fullname": "Yang",
                "user": "giantPanda0906",
                "type": "user"
            },
            "summary": "Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.",
            "upvotes": 38,
            "discussionId": "691fd89fcce0eb9b6387afaa",
            "projectPage": "https://stepaudiollm.github.io/step-audio-r1/",
            "githubRepo": "https://github.com/stepfun-ai/Step-Audio-R1",
            "ai_summary": "Step-Audio-R1, using the Modality-Grounded Reasoning Distillation framework, achieves strong reasoning capabilities in audio, outperforming previous models and demonstrating the transferability of reasoning across modalities.",
            "ai_keywords": [
                "reasoning models",
                "chain-of-thought deliberation",
                "audio language models",
                "Step-Audio-R1",
                "Modality-Grounded Reasoning Distillation",
                "acoustic features",
                "audio reasoning",
                "audio understanding",
                "reasoning benchmarks",
                "speech",
                "environmental sounds",
                "music",
                "multimodal reasoning systems"
            ],
            "githubStars": 70,
            "organization": {
                "_id": "66e43eae9d477f566f937935",
                "name": "stepfun-ai",
                "fullname": "StepFun",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
            }
        },
        "publishedAt": "2025-11-19T15:12:50.000Z",
        "title": "Step-Audio-R1 Technical Report",
        "summary": "Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15848.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "66518fd07d8cb2629a514c18",
            "avatarUrl": "/avatars/6280b33a6b1532ee938afd4aa303f709.svg",
            "fullname": "Yang",
            "name": "giantPanda0906",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "66e43eae9d477f566f937935",
            "name": "stepfun-ai",
            "fullname": "StepFun",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.13719",
            "authors": [
                {
                    "_id": "691f541c6d58f71c91487f52",
                    "user": {
                        "_id": "652d06833b5997ed71ce5c46",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg",
                        "isPro": false,
                        "fullname": "Zhongang Cai",
                        "user": "caizhongang",
                        "type": "user"
                    },
                    "name": "Zhongang Cai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T09:00:34.042Z",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f53",
                    "name": "Ruisi Wang",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f54",
                    "name": "Chenyang Gu",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f55",
                    "user": {
                        "_id": "646e1ef5075bbcc48ddf21e8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/_vJC0zeVOIvaNV2R6toqg.jpeg",
                        "isPro": false,
                        "fullname": "Pu Fanyi",
                        "user": "pufanyi",
                        "type": "user"
                    },
                    "name": "Fanyi Pu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T09:00:39.316Z",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f56",
                    "name": "Junxiang Xu",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f57",
                    "name": "Yubo Wang",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f58",
                    "name": "Wanqi Yin",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f59",
                    "name": "Zhitao Yang",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f5a",
                    "name": "Chen Wei",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f5b",
                    "name": "Qingping Sun",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f5c",
                    "name": "Tongxi Zhou",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f5d",
                    "name": "Jiaqi Li",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f5e",
                    "name": "Hui En Pang",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f5f",
                    "user": {
                        "_id": "647ae5462d27d3541deb70ff",
                        "avatarUrl": "/avatars/2258bc8cde543ba27e555fa00f7eca00.svg",
                        "isPro": false,
                        "fullname": "Oscar J Qian",
                        "user": "oscarqjh",
                        "type": "user"
                    },
                    "name": "Oscar Qian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T09:00:37.084Z",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f60",
                    "name": "Yukun Wei",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f61",
                    "name": "Zhiqian Lin",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f62",
                    "name": "Xuanke Shi",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f63",
                    "name": "Kewang Deng",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f64",
                    "user": {
                        "_id": "65d201b32383296176d7cc6b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/mdYa9UxX8yAogrtHA3olj.jpeg",
                        "isPro": false,
                        "fullname": "Xiaoyang Han",
                        "user": "robinhanxy",
                        "type": "user"
                    },
                    "name": "Xiaoyang Han",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T09:00:30.589Z",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f65",
                    "name": "Zukai Chen",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f66",
                    "name": "Xiangyu Fan",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f67",
                    "name": "Hanming Deng",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f68",
                    "name": "Lewei Lu",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f69",
                    "name": "Liang Pan",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f6a",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f6b",
                    "name": "Ziwei Liu",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f6c",
                    "name": "Quan Wang",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f6d",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "691f541c6d58f71c91487f6e",
                    "user": {
                        "_id": "6626a471430a124253f197c8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6626a471430a124253f197c8/uVEk5nnW-bS6-no0rQ7Wh.png",
                        "isPro": false,
                        "fullname": "yl-1993",
                        "user": "yl-1993",
                        "type": "user"
                    },
                    "name": "Lei Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T09:00:32.441Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-17T18:59:33.000Z",
            "submittedOnDailyAt": "2025-11-21T00:54:15.478Z",
            "title": "Scaling Spatial Intelligence with Multimodal Foundation Models",
            "submittedOnDailyBy": {
                "_id": "652d06833b5997ed71ce5c46",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg",
                "isPro": false,
                "fullname": "Zhongang Cai",
                "user": "caizhongang",
                "type": "user"
            },
            "summary": "Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.",
            "upvotes": 33,
            "discussionId": "691f541d6d58f71c91487f6f",
            "projectPage": "https://huggingface.co/sensenova/SenseNova-SI-1.1-InternVL3-8B",
            "githubRepo": "https://github.com/OpenSenseNova/SenseNova-SI",
            "ai_summary": "Multimodal foundation models like SenseNova-SI improve spatial intelligence through diverse data scaling and demonstrate strong performance across various spatial benchmarks while maintaining general multimodal understanding.",
            "ai_keywords": [
                "multimodal foundation models",
                "spatial intelligence",
                "SenseNova-SI",
                "SenseNova-SI-8M",
                "visual understanding models",
                "unified understanding and generation models",
                "spatial capabilities",
                "VSI-Bench",
                "MMSI",
                "MindCube",
                "ViewSpatial",
                "SITE",
                "MMBench-En",
                "spatial chain-of-thought reasoning"
            ],
            "githubStars": 101,
            "organization": {
                "_id": "64f0405f8a4cf3e5e6b38f9c",
                "name": "sensenova",
                "fullname": "SenseNova",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652d06833b5997ed71ce5c46/k66xcOMf4NVbMSFulUjHY.png"
            }
        },
        "publishedAt": "2025-11-17T13:59:33.000Z",
        "title": "Scaling Spatial Intelligence with Multimodal Foundation Models",
        "summary": "Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13719.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652d06833b5997ed71ce5c46",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg",
            "fullname": "Zhongang Cai",
            "name": "caizhongang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 23
        },
        "organization": {
            "_id": "64f0405f8a4cf3e5e6b38f9c",
            "name": "sensenova",
            "fullname": "SenseNova",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652d06833b5997ed71ce5c46/k66xcOMf4NVbMSFulUjHY.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.16624",
            "authors": [
                {
                    "_id": "691fcf51cce0eb9b6387af0b",
                    "name": "SAM 3D Team",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af0c",
                    "name": "Xingyu Chen",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af0d",
                    "name": "Fu-Jen Chu",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af0e",
                    "user": {
                        "_id": "65f22480f5cf26fe060cb558",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/2XwjrbcFiBJoO9hqcsniI.jpeg",
                        "isPro": false,
                        "fullname": "Pierre Gleize",
                        "user": "pgleize",
                        "type": "user"
                    },
                    "name": "Pierre Gleize",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-21T10:18:05.047Z",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af0f",
                    "user": {
                        "_id": "65f3227730d1859ef2ab9c3f",
                        "avatarUrl": "/avatars/fea5047893056a8bc73882b7fa1d9b5e.svg",
                        "isPro": false,
                        "fullname": "Kevin J Liang",
                        "user": "kevinjliang",
                        "type": "user"
                    },
                    "name": "Kevin J Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-21T10:18:10.792Z",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af10",
                    "name": "Alexander Sax",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af11",
                    "name": "Hao Tang",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af12",
                    "name": "Weiyao Wang",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af13",
                    "name": "Michelle Guo",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af14",
                    "user": {
                        "_id": "65d13ed475318341f2fe10a6",
                        "avatarUrl": "/avatars/c040d3b9089f77de317e0f200e394ad0.svg",
                        "isPro": false,
                        "fullname": "Thibaut Hardin",
                        "user": "thibauth",
                        "type": "user"
                    },
                    "name": "Thibaut Hardin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-21T10:21:54.769Z",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af15",
                    "name": "Xiang Li",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af16",
                    "name": "Aohan Lin",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af17",
                    "name": "Jiawei Liu",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af18",
                    "name": "Ziqi Ma",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af19",
                    "user": {
                        "_id": "687742db2451daed8e16457d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ipbCcNRHLay60LCuIzZWM.png",
                        "isPro": false,
                        "fullname": "Anushka Sagar",
                        "user": "anushkasagar302",
                        "type": "user"
                    },
                    "name": "Anushka Sagar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-21T10:17:50.058Z",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af1a",
                    "name": "Bowen Song",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af1b",
                    "name": "Xiaodong Wang",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af1c",
                    "name": "Jianing Yang",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af1d",
                    "name": "Bowen Zhang",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af1e",
                    "name": "Piotr Doll√°r",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af1f",
                    "name": "Georgia Gkioxari",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af20",
                    "user": {
                        "_id": "65a91cda241e1c6c489d53bf",
                        "avatarUrl": "/avatars/eb864f738db56b7e00f40a66d2e3af6c.svg",
                        "isPro": false,
                        "fullname": "Matt Feiszli",
                        "user": "mattfeiszli",
                        "type": "user"
                    },
                    "name": "Matt Feiszli",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-21T10:22:23.652Z",
                    "hidden": false
                },
                {
                    "_id": "691fcf51cce0eb9b6387af21",
                    "user": {
                        "_id": "65369a95605a07338de78ab0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sGFjOjLT2akN-sn5beVWL.jpeg",
                        "isPro": false,
                        "fullname": "Jitendra Malik ",
                        "user": "jitendra1995",
                        "type": "user"
                    },
                    "name": "Jitendra Malik",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-21T10:22:30.555Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T18:31:46.000Z",
            "submittedOnDailyAt": "2025-11-21T00:03:03.895Z",
            "title": "SAM 3D: 3Dfy Anything in Images",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D \"data barrier\". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.",
            "upvotes": 27,
            "discussionId": "691fcf52cce0eb9b6387af22",
            "projectPage": "https://ai.meta.com/sam3d/",
            "githubRepo": "https://github.com/facebookresearch/sam-3d-objects",
            "ai_summary": "SAM 3D is a generative model that reconstructs 3D objects from single images using a multi-stage training framework that includes synthetic pretraining and real-world alignment, achieving high performance in human preference tests.",
            "ai_keywords": [
                "generative model",
                "3D object reconstruction",
                "geometry",
                "texture",
                "layout",
                "human-in-the-loop",
                "object shape",
                "object pose",
                "multi-stage training",
                "synthetic pretraining",
                "real-world alignment",
                "3D data barrier",
                "human preference tests",
                "benchmark"
            ],
            "githubStars": 2782,
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "publishedAt": "2025-11-20T13:31:46.000Z",
        "title": "SAM 3D: 3Dfy Anything in Images",
        "summary": "We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D \"data barrier\". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16624.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 169
        },
        "organization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.16669",
            "authors": [
                {
                    "_id": "691fd2a9cce0eb9b6387af66",
                    "user": {
                        "_id": "6506b77a773ceaa8d52ecea1",
                        "avatarUrl": "/avatars/0e769a0795063e1491c44760a4a83097.svg",
                        "isPro": false,
                        "fullname": "CJH",
                        "user": "Howe666",
                        "type": "user"
                    },
                    "name": "Junhao Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-21T10:17:16.376Z",
                    "hidden": false
                },
                {
                    "_id": "691fd2a9cce0eb9b6387af67",
                    "name": "Liang Hou",
                    "hidden": false
                },
                {
                    "_id": "691fd2a9cce0eb9b6387af68",
                    "name": "Xin Tao",
                    "hidden": false
                },
                {
                    "_id": "691fd2a9cce0eb9b6387af69",
                    "name": "Jing Liao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T18:59:44.000Z",
            "submittedOnDailyAt": "2025-11-21T00:18:06.892Z",
            "title": "Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO",
            "submittedOnDailyBy": {
                "_id": "6506b77a773ceaa8d52ecea1",
                "avatarUrl": "/avatars/0e769a0795063e1491c44760a4a83097.svg",
                "isPro": false,
                "fullname": "CJH",
                "user": "Howe666",
                "type": "user"
            },
            "summary": "While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.",
            "upvotes": 24,
            "discussionId": "691fd2a9cce0eb9b6387af6a",
            "projectPage": "https://video-as-answer.github.io/",
            "githubRepo": "https://github.com/KlingTeam/VANS",
            "ai_summary": "VANS, a model combining reinforcement learning, a Vision-Language Model, and a Video Diffusion Model, achieves state-of-the-art performance in Video-Next-Event Prediction by generating visually consistent and semantically accurate videos.",
            "ai_keywords": [
                "Vision-Language Model",
                "Video Diffusion Model",
                "Joint-GRPO",
                "Video-Next-Event Prediction",
                "procedural learning",
                "creative exploration",
                "multimodal input",
                "instruction-conditioned reasoning"
            ],
            "githubStars": 29,
            "organization": {
                "_id": "662c559b322afcbae51b3c8b",
                "name": "KlingTeam",
                "fullname": "Kling Team",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
            }
        },
        "publishedAt": "2025-11-20T13:59:44.000Z",
        "title": "Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO",
        "summary": "While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16669.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6506b77a773ceaa8d52ecea1",
            "avatarUrl": "/avatars/0e769a0795063e1491c44760a4a83097.svg",
            "fullname": "CJH",
            "name": "Howe666",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "662c559b322afcbae51b3c8b",
            "name": "KlingTeam",
            "fullname": "Kling Team",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.16518",
            "authors": [
                {
                    "_id": "691fd013cce0eb9b6387af38",
                    "name": "Xiaoshuai Hao",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af39",
                    "name": "Lei Zhou",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af3a",
                    "user": {
                        "_id": "634145754c9a81858b2abdba",
                        "avatarUrl": "/avatars/38974eec21dcd2a44c4cc7122239c17b.svg",
                        "isPro": false,
                        "fullname": "zhijian Huang",
                        "user": "a1778850",
                        "type": "user"
                    },
                    "name": "Zhijian Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-21T10:41:08.553Z",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af3b",
                    "name": "Zhiwen Hou",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af3c",
                    "user": {
                        "_id": "67a453a9180dc3f1db708d69",
                        "avatarUrl": "/avatars/beaf31f2b1eaa537fe3bedb4fd8055a0.svg",
                        "isPro": false,
                        "fullname": "Yingbo Tang",
                        "user": "tyb197",
                        "type": "user"
                    },
                    "name": "Yingbo Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-21T10:41:16.528Z",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af3d",
                    "name": "Lingfeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af3e",
                    "name": "Guang Li",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af3f",
                    "name": "Zheng Lu",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af40",
                    "name": "Shuhuai Ren",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af41",
                    "name": "Xianhui Meng",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af42",
                    "name": "Yuchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af43",
                    "name": "Jing Wu",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af44",
                    "name": "Jinghui Lu",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af45",
                    "name": "Chenxu Dang",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af46",
                    "name": "Jiayi Guan",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af47",
                    "name": "Jianhua Wu",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af48",
                    "name": "Zhiyi Hou",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af49",
                    "name": "Hanbing Li",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af4a",
                    "name": "Shumeng Xia",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af4b",
                    "name": "Mingliang Zhou",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af4c",
                    "name": "Yinan Zheng",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af4d",
                    "name": "Zihao Yue",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af4e",
                    "name": "Shuhao Gu",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af4f",
                    "name": "Hao Tian",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af50",
                    "name": "Yuannan Shen",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af51",
                    "name": "Jianwei Cui",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af52",
                    "name": "Wen Zhang",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af53",
                    "name": "Shaoqing Xu",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af54",
                    "name": "Bing Wang",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af55",
                    "name": "Haiyang Sun",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af56",
                    "name": "Zeyu Zhu",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af57",
                    "name": "Yuncheng Jiang",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af58",
                    "name": "Zibin Guo",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af59",
                    "name": "Chuhong Gong",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af5a",
                    "name": "Chaofan Zhang",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af5b",
                    "name": "Wenbo Ding",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af5c",
                    "name": "Kun Ma",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af5d",
                    "name": "Guang Chen",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af5e",
                    "name": "Rui Cai",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af5f",
                    "name": "Diyun Xiang",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af60",
                    "name": "Heng Qu",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af61",
                    "name": "Fuli Luo",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af62",
                    "user": {
                        "_id": "644a6081405bdff86cb7594c",
                        "avatarUrl": "/avatars/f26b4af57ef9650cf219cca81e711e48.svg",
                        "isPro": false,
                        "fullname": "Hangjun Ye",
                        "user": "yehangjun",
                        "type": "user"
                    },
                    "name": "Hangjun Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-21T10:41:39.833Z",
                    "hidden": false
                },
                {
                    "_id": "691fd013cce0eb9b6387af63",
                    "user": {
                        "_id": "63f2cb3af4e30ffd2bd7f933",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f2cb3af4e30ffd2bd7f933/7ye89WAUWrtd3Wbc_unjR.png",
                        "isPro": false,
                        "fullname": "Long Chen",
                        "user": "longchen",
                        "type": "user"
                    },
                    "name": "Long Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T08:59:55.363Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T16:34:55.000Z",
            "submittedOnDailyAt": "2025-11-21T00:06:15.717Z",
            "title": "MiMo-Embodied: X-Embodied Foundation Model Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.",
            "upvotes": 18,
            "discussionId": "691fd013cce0eb9b6387af64",
            "githubRepo": "https://github.com/XiaomiMiMo/MiMo-Embodied",
            "ai_summary": "MiMo-Embodied, a cross-embodied foundation model, achieves state-of-the-art performance in both autonomous driving and embodied AI through multi-stage learning, curated data, and CoT/RL fine-tuning.",
            "ai_keywords": [
                "cross-embodied foundation model",
                "Task Planning",
                "Affordance Prediction",
                "Spatial Understanding",
                "Environmental Perception",
                "Status Prediction",
                "Driving Planning",
                "multi-stage learning",
                "curated data construction",
                "CoT/RL fine-tuning",
                "positive transfer"
            ],
            "githubStars": 62,
            "organization": {
                "_id": "680cb4c37f289defb2210940",
                "name": "XiaomiMiMo",
                "fullname": "Xiaomi MiMo",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/680cb7d1233834890a64acee/6w2qX9u28SPLL11os2Y9C.png"
            }
        },
        "publishedAt": "2025-11-20T11:34:55.000Z",
        "title": "MiMo-Embodied: X-Embodied Foundation Model Technical Report",
        "summary": "We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16518.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 169
        },
        "organization": {
            "_id": "680cb4c37f289defb2210940",
            "name": "XiaomiMiMo",
            "fullname": "Xiaomi MiMo",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/680cb7d1233834890a64acee/6w2qX9u28SPLL11os2Y9C.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.16043",
            "authors": [
                {
                    "_id": "691fc9a6cce0eb9b6387aed0",
                    "user": {
                        "_id": "643e9ee6f6bb3c31a26e7bc4",
                        "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
                        "isPro": false,
                        "fullname": "Peng Xia",
                        "user": "richardxp888",
                        "type": "user"
                    },
                    "name": "Peng Xia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T09:00:02.182Z",
                    "hidden": false
                },
                {
                    "_id": "691fc9a6cce0eb9b6387aed1",
                    "name": "Kaide Zeng",
                    "hidden": false
                },
                {
                    "_id": "691fc9a6cce0eb9b6387aed2",
                    "name": "Jiaqi Liu",
                    "hidden": false
                },
                {
                    "_id": "691fc9a6cce0eb9b6387aed3",
                    "name": "Can Qin",
                    "hidden": false
                },
                {
                    "_id": "691fc9a6cce0eb9b6387aed4",
                    "name": "Fang Wu",
                    "hidden": false
                },
                {
                    "_id": "691fc9a6cce0eb9b6387aed5",
                    "name": "Yiyang Zhou",
                    "hidden": false
                },
                {
                    "_id": "691fc9a6cce0eb9b6387aed6",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "691fc9a6cce0eb9b6387aed7",
                    "name": "Huaxiu Yao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/1xb6ySMJ14XwCfAOHaN3U.png"
            ],
            "publishedAt": "2025-11-20T05:01:57.000Z",
            "submittedOnDailyAt": "2025-11-21T21:34:32.547Z",
            "title": "Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning",
            "submittedOnDailyBy": {
                "_id": "643e9ee6f6bb3c31a26e7bc4",
                "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
                "isPro": false,
                "fullname": "Peng Xia",
                "user": "richardxp888",
                "type": "user"
            },
            "summary": "Large Language Model (LLM) Agents, often trained with Reinforcement Learning (RL), are constrained by a dependency on human-curated data, limiting scalability and tethering AI to human knowledge. Existing self-evolution frameworks offer an alternative but are typically restricted by the model's inherent capabilities and single-round interactions, hindering the development of complex curricula involving tool use or dynamic reasoning. We introduce Agent0, a fully autonomous framework that evolves high-performing agents without external data through multi-step co-evolution and seamless tool integration. Agent0 establishes a symbiotic competition between two agents initialized from the same base LLM: a curriculum agent that proposes increasingly challenging frontier tasks, and an executor agent that learns to solve them. We integrate external tools to enhance the executor's problem-solving capacity; this improvement, in turn, pressures the curriculum agent to construct more complex, tool-aware tasks. Through this iterative process, Agent0 establishes a self-reinforcing cycle that continuously produces high-quality curricula. Empirically, Agent0 substantially boosts reasoning capabilities, improving the Qwen3-8B-Base model by 18% on mathematical reasoning and 24% on general reasoning benchmarks. Code is available at https://github.com/aiming-lab/Agent0.",
            "upvotes": 16,
            "discussionId": "691fc9a6cce0eb9b6387aed8",
            "githubRepo": "https://github.com/aiming-lab/Agent0",
            "ai_summary": "Agent0, a self-evolving framework utilizing multi-step co-evolution and tool integration, enhances LLM reasoning capabilities without human-curated data.",
            "ai_keywords": [
                "Large Language Model (LLM)",
                "Reinforcement Learning (RL)",
                "self-evolution frameworks",
                "multi-step co-evolution",
                "tool integration",
                "curriculum agent",
                "executor agent",
                "mathematical reasoning",
                "general reasoning benchmarks"
            ],
            "githubStars": 4,
            "organization": {
                "_id": "669f9d1fec8789263c0e355a",
                "name": "UNC-ChapelHill",
                "fullname": "University of North Carolina at Chapel Hill",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"
            }
        },
        "publishedAt": "2025-11-20T00:01:57.000Z",
        "title": "Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning",
        "summary": "Large Language Model (LLM) Agents, often trained with Reinforcement Learning (RL), are constrained by a dependency on human-curated data, limiting scalability and tethering AI to human knowledge. Existing self-evolution frameworks offer an alternative but are typically restricted by the model's inherent capabilities and single-round interactions, hindering the development of complex curricula involving tool use or dynamic reasoning. We introduce Agent0, a fully autonomous framework that evolves high-performing agents without external data through multi-step co-evolution and seamless tool integration. Agent0 establishes a symbiotic competition between two agents initialized from the same base LLM: a curriculum agent that proposes increasingly challenging frontier tasks, and an executor agent that learns to solve them. We integrate external tools to enhance the executor's problem-solving capacity; this improvement, in turn, pressures the curriculum agent to construct more complex, tool-aware tasks. Through this iterative process, Agent0 establishes a self-reinforcing cycle that continuously produces high-quality curricula. Empirically, Agent0 substantially boosts reasoning capabilities, improving the Qwen3-8B-Base model by 18% on mathematical reasoning and 24% on general reasoning benchmarks. Code is available at https://github.com/aiming-lab/Agent0.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/1xb6ySMJ14XwCfAOHaN3U.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16043.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643e9ee6f6bb3c31a26e7bc4",
            "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
            "fullname": "Peng Xia",
            "name": "richardxp888",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "669f9d1fec8789263c0e355a",
            "name": "UNC-ChapelHill",
            "fullname": "University of North Carolina at Chapel Hill",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.16664",
            "authors": [
                {
                    "_id": "691fcf18cce0eb9b6387aef9",
                    "name": "Ali Taghibakhshi",
                    "hidden": false
                },
                {
                    "_id": "691fcf18cce0eb9b6387aefa",
                    "name": "Sharath Turuvekere Sreenivas",
                    "hidden": false
                },
                {
                    "_id": "691fcf18cce0eb9b6387aefb",
                    "name": "Saurav Muralidharan",
                    "hidden": false
                },
                {
                    "_id": "691fcf18cce0eb9b6387aefc",
                    "name": "Ruisi Cai",
                    "hidden": false
                },
                {
                    "_id": "691fcf18cce0eb9b6387aefd",
                    "name": "Marcin Chochowski",
                    "hidden": false
                },
                {
                    "_id": "691fcf18cce0eb9b6387aefe",
                    "name": "Ameya Sunil Mahabaleshwarkar",
                    "hidden": false
                },
                {
                    "_id": "691fcf18cce0eb9b6387aeff",
                    "name": "Yoshi Suhara",
                    "hidden": false
                },
                {
                    "_id": "691fcf18cce0eb9b6387af00",
                    "name": "Oluwatobi Olabiyi",
                    "hidden": false
                },
                {
                    "_id": "691fcf18cce0eb9b6387af01",
                    "name": "Daniel Korzekwa",
                    "hidden": false
                },
                {
                    "_id": "691fcf18cce0eb9b6387af02",
                    "name": "Mostofa Patwary",
                    "hidden": false
                },
                {
                    "_id": "691fcf18cce0eb9b6387af03",
                    "name": "Mohammad Shoeybi",
                    "hidden": false
                },
                {
                    "_id": "691fcf18cce0eb9b6387af04",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "691fcf18cce0eb9b6387af05",
                    "name": "Bryan Catanzaro",
                    "hidden": false
                },
                {
                    "_id": "691fcf18cce0eb9b6387af06",
                    "name": "Ashwath Aithal",
                    "hidden": false
                },
                {
                    "_id": "691fcf18cce0eb9b6387af07",
                    "name": "Nima Tajbakhsh",
                    "hidden": false
                },
                {
                    "_id": "691fcf18cce0eb9b6387af08",
                    "name": "Pavlo Molchanov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T18:59:21.000Z",
            "submittedOnDailyAt": "2025-11-21T00:02:06.467Z",
            "title": "Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family.",
            "upvotes": 13,
            "discussionId": "691fcf18cce0eb9b6387af09",
            "projectPage": "https://huggingface.co/nvidia/Nemotron-Elastic-12B",
            "ai_summary": "Nemotron Elastic reduces training costs and memory usage by embedding multiple submodels within a single large language model, optimized for various deployment configurations and budgets without additional training or fine-tuning.",
            "ai_keywords": [
                "large language models",
                "model compression",
                "pruning",
                "knowledge distillation",
                "hybrid Mamba-Attention architectures",
                "nested submodels",
                "parent model",
                "zero-shot extraction",
                "end-to-end trained router",
                "two-stage training curriculum",
                "group-aware SSM elastification",
                "heterogeneous MLP elastification",
                "normalized MSE-based layer importance",
                "multi-budget optimization",
                "Nemotron Nano V2 12B model"
            ],
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-11-20T13:59:21.000Z",
        "title": "Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs",
        "summary": "Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16664.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 169
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.13703",
            "authors": [
                {
                    "_id": "692066f38c38b39d6a482dbd",
                    "user": {
                        "_id": "6143be23a05498d58df55450",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6143be23a05498d58df55450/IexeJ6WaNFuFX4-b3Gxv4.jpeg",
                        "isPro": true,
                        "fullname": "Lavender Jiang",
                        "user": "LavenderJ",
                        "type": "user"
                    },
                    "name": "Lavender Y. Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T16:25:09.686Z",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dbe",
                    "user": {
                        "_id": "6296171e91760de5e5482894",
                        "avatarUrl": "/avatars/f6209e822f78933f4ee6869f7f4dc704.svg",
                        "isPro": false,
                        "fullname": "Angelica Chen",
                        "user": "angie-chen55",
                        "type": "user"
                    },
                    "name": "Angelica Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T16:25:04.382Z",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dbf",
                    "name": "Xu Han",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dc0",
                    "name": "Xujin Chris Liu",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dc1",
                    "user": {
                        "_id": "67649cd08989871cde263811",
                        "avatarUrl": "/avatars/57eb503ddde82b1e44f3795781c2f954.svg",
                        "isPro": false,
                        "fullname": "Radhika Dua",
                        "user": "Radhikadua123",
                        "type": "user"
                    },
                    "name": "Radhika Dua",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T16:49:01.926Z",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dc2",
                    "name": "Kevin Eaton",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dc3",
                    "name": "Frederick Wolff",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dc4",
                    "user": {
                        "_id": "67c7f2a3252114c63bfea097",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/75MAkb33OFBXNwkFWIY69.png",
                        "isPro": false,
                        "fullname": "Robert Steele",
                        "user": "re-bort",
                        "type": "user"
                    },
                    "name": "Robert Steele",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T16:49:03.972Z",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dc5",
                    "name": "Jeff Zhang",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dc6",
                    "name": "Anton Alyakin",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dc7",
                    "name": "Qingkai Pan",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dc8",
                    "name": "Yanbing Chen",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dc9",
                    "name": "Karl L. Sangwon",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dca",
                    "user": {
                        "_id": "644a848244b75fd958062ba3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a848244b75fd958062ba3/d7vrsMSnVaBmpw3TfBVGO.jpeg",
                        "isPro": false,
                        "fullname": "Daniel Alber",
                        "user": "dalber",
                        "type": "user"
                    },
                    "name": "Daniel A. Alber",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T16:25:06.313Z",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dcb",
                    "name": "Jaden Stryker",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dcc",
                    "name": "Jin Vivian Lee",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dcd",
                    "name": "Yindalon Aphinyanaphongs",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dce",
                    "name": "Kyunghyun Cho",
                    "hidden": false
                },
                {
                    "_id": "692066f38c38b39d6a482dcf",
                    "name": "Eric Karl Oermann",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6143be23a05498d58df55450/MNq5Lvf054RSyB_xxrgVZ.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6143be23a05498d58df55450/X5HbIWmBL8m2fv3ocwlEv.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6143be23a05498d58df55450/p51j-5dacPKqaNSCQLdNu.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6143be23a05498d58df55450/c6wtX4Or1JQOfBv8RP_D2.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6143be23a05498d58df55450/ymHxG221-Bos72gE3bUcT.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6143be23a05498d58df55450/K4jnqhaM2n4QqSDhYLGrL.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6143be23a05498d58df55450/gub4rnqsMG8pTN6qWT7oM.png"
            ],
            "publishedAt": "2025-11-17T18:52:22.000Z",
            "submittedOnDailyAt": "2025-11-21T14:11:55.849Z",
            "title": "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations",
            "submittedOnDailyBy": {
                "_id": "6143be23a05498d58df55450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6143be23a05498d58df55450/IexeJ6WaNFuFX4-b3Gxv4.jpeg",
                "isPro": true,
                "fullname": "Lavender Jiang",
                "user": "LavenderJ",
                "type": "user"
            },
            "summary": "Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.",
            "upvotes": 12,
            "discussionId": "692066f48c38b39d6a482dd0",
            "ai_summary": "Lang1, a specialized language model pretrained on clinical data, outperforms generalist models in predicting hospital operational metrics through supervised finetuning and real-world evaluation.",
            "ai_keywords": [
                "foundation models",
                "Lang1",
                "pretrained models",
                "clinical tokens",
                "EHRs",
                "REalistic Medical Evaluation",
                "ReMedE",
                "30-day readmission prediction",
                "30-day mortality prediction",
                "length of stay",
                "comorbidity coding",
                "insurance claims denial",
                "AUROC",
                "zero-shot settings",
                "finetuning",
                "cross-task scaling",
                "in-domain pretraining",
                "specialized LLMs",
                "supervised finetuning",
                "real-world evaluation"
            ],
            "organization": {
                "_id": "691d8e884bbe8df0d99462e2",
                "name": "newyorkuniversity",
                "fullname": "New York University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/orNHmPzOQf2_F5UgXPsu5.png"
            }
        },
        "publishedAt": "2025-11-17T13:52:22.000Z",
        "title": "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations",
        "summary": "Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6143be23a05498d58df55450/MNq5Lvf054RSyB_xxrgVZ.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6143be23a05498d58df55450/X5HbIWmBL8m2fv3ocwlEv.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6143be23a05498d58df55450/p51j-5dacPKqaNSCQLdNu.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6143be23a05498d58df55450/c6wtX4Or1JQOfBv8RP_D2.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6143be23a05498d58df55450/ymHxG221-Bos72gE3bUcT.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6143be23a05498d58df55450/K4jnqhaM2n4QqSDhYLGrL.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6143be23a05498d58df55450/gub4rnqsMG8pTN6qWT7oM.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13703.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6143be23a05498d58df55450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6143be23a05498d58df55450/IexeJ6WaNFuFX4-b3Gxv4.jpeg",
            "fullname": "Lavender Jiang",
            "name": "LavenderJ",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "691d8e884bbe8df0d99462e2",
            "name": "newyorkuniversity",
            "fullname": "New York University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/orNHmPzOQf2_F5UgXPsu5.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.16671",
            "authors": [
                {
                    "_id": "691fd939cce0eb9b6387afac",
                    "name": "Ziyu Guo",
                    "hidden": false
                },
                {
                    "_id": "691fd939cce0eb9b6387afad",
                    "user": {
                        "_id": "645b8b2687c79b6ec0bb3b7a",
                        "avatarUrl": "/avatars/00a9db32a42dc950112bf2593bb109cb.svg",
                        "isPro": false,
                        "fullname": "Renrui",
                        "user": "ZrrSkywalker",
                        "type": "user"
                    },
                    "name": "Renrui Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-21T10:42:45.049Z",
                    "hidden": false
                },
                {
                    "_id": "691fd939cce0eb9b6387afae",
                    "name": "Hongyu Li",
                    "hidden": false
                },
                {
                    "_id": "691fd939cce0eb9b6387afaf",
                    "name": "Manyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "691fd939cce0eb9b6387afb0",
                    "name": "Xinyan Chen",
                    "hidden": false
                },
                {
                    "_id": "691fd939cce0eb9b6387afb1",
                    "name": "Sifan Wang",
                    "hidden": false
                },
                {
                    "_id": "691fd939cce0eb9b6387afb2",
                    "name": "Yan Feng",
                    "hidden": false
                },
                {
                    "_id": "691fd939cce0eb9b6387afb3",
                    "name": "Peng Pei",
                    "hidden": false
                },
                {
                    "_id": "691fd939cce0eb9b6387afb4",
                    "name": "Pheng-Ann Heng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T18:59:52.000Z",
            "submittedOnDailyAt": "2025-11-21T00:45:52.234Z",
            "title": "Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation",
            "submittedOnDailyBy": {
                "_id": "645b8b2687c79b6ec0bb3b7a",
                "avatarUrl": "/avatars/00a9db32a42dc950112bf2593bb109cb.svg",
                "isPro": false,
                "fullname": "Renrui",
                "user": "ZrrSkywalker",
                "type": "user"
            },
            "summary": "Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.",
            "upvotes": 11,
            "discussionId": "691fd939cce0eb9b6387afb5",
            "projectPage": "https://think-while-gen.github.io/",
            "githubRepo": "https://github.com/ZiyuGuo99/Thinking-while-Generating",
            "ai_summary": "A new framework, Thinking-while-Generating (TwiG), integrates interleaved textual reasoning during visual generation, enhancing context-awareness and semantic richness through zero-shot prompting, supervised fine-tuning, and reinforcement learning.",
            "ai_keywords": [
                "Thinking-while-Generating",
                "TwiG",
                "textual reasoning",
                "pre-planning",
                "post-refinement",
                "visual generation",
                "zero-shot prompting",
                "supervised fine-tuning",
                "reinforcement learning",
                "TwiG-50K dataset",
                "TwiG-GRPO"
            ],
            "githubStars": 26
        },
        "publishedAt": "2025-11-20T13:59:52.000Z",
        "title": "Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation",
        "summary": "Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16671.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645b8b2687c79b6ec0bb3b7a",
            "avatarUrl": "/avatars/00a9db32a42dc950112bf2593bb109cb.svg",
            "fullname": "Renrui",
            "name": "ZrrSkywalker",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.16528",
            "authors": [
                {
                    "_id": "691fda11cce0eb9b6387afb7",
                    "user": {
                        "_id": "6464e76894327a238f56d7ff",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464e76894327a238f56d7ff/U-3iova5yEQgigmmtDBPY.png",
                        "isPro": true,
                        "fullname": "√ñzay Ezerceli",
                        "user": "ozayezerceli",
                        "type": "user"
                    },
                    "name": "√ñzay Ezerceli",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T08:59:33.793Z",
                    "hidden": false
                },
                {
                    "_id": "691fda11cce0eb9b6387afb8",
                    "user": {
                        "_id": "6422eab8e2029ade06eeee2c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
                        "isPro": false,
                        "fullname": "Mahmud ElHuseyni üáµüá∏",
                        "user": "MElHuseyni",
                        "type": "user"
                    },
                    "name": "Mahmoud El Hussieni",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T08:59:40.868Z",
                    "hidden": false
                },
                {
                    "_id": "691fda11cce0eb9b6387afb9",
                    "user": {
                        "_id": "63241e8f0fc33e3d14dd5277",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63241e8f0fc33e3d14dd5277/QUF4j7L3xZdKRnLRCrA12.jpeg",
                        "isPro": false,
                        "fullname": "SELVA TA≈û",
                        "user": "selvatas",
                        "type": "user"
                    },
                    "name": "Selva Ta≈ü",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T08:59:38.921Z",
                    "hidden": false
                },
                {
                    "_id": "691fda11cce0eb9b6387afba",
                    "user": {
                        "_id": "64a52413fa840dd18c2f76fe",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a52413fa840dd18c2f76fe/5PD-BHnKm2aQWFZhE7P2I.jpeg",
                        "isPro": false,
                        "fullname": "Reyhan Bayraktar",
                        "user": "byrayhana",
                        "type": "user"
                    },
                    "name": "Reyhan Bayraktar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T08:59:35.611Z",
                    "hidden": false
                },
                {
                    "_id": "691fda11cce0eb9b6387afbb",
                    "name": "Fatma Bet√ºl Terzioƒülu",
                    "hidden": false
                },
                {
                    "_id": "691fda11cce0eb9b6387afbc",
                    "name": "Yusuf √áelebi",
                    "hidden": false
                },
                {
                    "_id": "691fda11cce0eb9b6387afbd",
                    "user": {
                        "_id": "68c1577793c24dc99747a7b0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FIg_MYFd24wf0uEjHnS1C.png",
                        "isPro": false,
                        "fullname": "Yaƒüƒ±z Asker",
                        "user": "yasker00",
                        "type": "user"
                    },
                    "name": "Yaƒüƒ±z Asker",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T16:48:57.702Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T16:42:21.000Z",
            "submittedOnDailyAt": "2025-11-21T00:59:24.449Z",
            "title": "TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval",
            "submittedOnDailyBy": {
                "_id": "6422eab8e2029ade06eeee2c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
                "isPro": false,
                "fullname": "Mahmud ElHuseyni üáµüá∏",
                "user": "MElHuseyni",
                "type": "user"
            },
            "summary": "Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600times smaller than the 600M turkish-e5-large dense encoder while preserving over 71\\% of its average mAP. Late-interaction models that are 3--5times smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33times faster than PLAID and offers +1.7\\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets (leq50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.",
            "upvotes": 9,
            "discussionId": "691fda11cce0eb9b6387afbe",
            "ai_summary": "TurkColBERT evaluates dense encoders and late-interaction models for Turkish information retrieval, demonstrating parameter efficiency and superior performance of late-interaction models with faster indexing.",
            "ai_keywords": [
                "dense bi-encoders",
                "late-interaction models",
                "token-level representations",
                "fine-grained matching",
                "TurkColBERT",
                "Turkish NLI/STS tasks",
                "ColBERT-style retrievers",
                "PyLate",
                "MS MARCO-TR",
                "Turkish BEIR datasets",
                "average mAP",
                "parameter efficiency",
                "colbert-hash-nano-tr",
                "turkish-e5-large",
                "ColmmBERT-base-TR",
                "indexing algorithms",
                "MUVERA+Rerank",
                "PLAID",
                "query times",
                "MUVERA"
            ],
            "organization": {
                "_id": "660c060a1b2939953134fb67",
                "name": "newmindai",
                "fullname": "NewMind AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660c05be387b035b0fc56bae/QzKfgwGy3O-IhWV8lH_RC.png"
            }
        },
        "publishedAt": "2025-11-20T11:42:21.000Z",
        "title": "TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval",
        "summary": "Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600times smaller than the 600M turkish-e5-large dense encoder while preserving over 71\\% of its average mAP. Late-interaction models that are 3--5times smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33times faster than PLAID and offers +1.7\\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets (leq50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16528.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6422eab8e2029ade06eeee2c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
            "fullname": "Mahmud ElHuseyni üáµüá∏",
            "name": "MElHuseyni",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 21
        },
        "organization": {
            "_id": "660c060a1b2939953134fb67",
            "name": "newmindai",
            "fullname": "NewMind AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660c05be387b035b0fc56bae/QzKfgwGy3O-IhWV8lH_RC.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.15605",
            "authors": [
                {
                    "_id": "691fdd84cce0eb9b6387afdf",
                    "name": "Senyu Fei",
                    "hidden": false
                },
                {
                    "_id": "691fdd84cce0eb9b6387afe0",
                    "user": {
                        "_id": "64c3c631e77ea9f28111172a",
                        "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
                        "isPro": false,
                        "fullname": "Siyin Wang (SII)",
                        "user": "sinwang",
                        "type": "user"
                    },
                    "name": "Siyin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T08:59:27.656Z",
                    "hidden": false
                },
                {
                    "_id": "691fdd84cce0eb9b6387afe1",
                    "name": "Li Ji",
                    "hidden": false
                },
                {
                    "_id": "691fdd84cce0eb9b6387afe2",
                    "name": "Ao Li",
                    "hidden": false
                },
                {
                    "_id": "691fdd84cce0eb9b6387afe3",
                    "name": "Shiduo Zhang",
                    "hidden": false
                },
                {
                    "_id": "691fdd84cce0eb9b6387afe4",
                    "name": "Liming Liu",
                    "hidden": false
                },
                {
                    "_id": "691fdd84cce0eb9b6387afe5",
                    "name": "Jinlong Hou",
                    "hidden": false
                },
                {
                    "_id": "691fdd84cce0eb9b6387afe6",
                    "name": "Jingjing Gong",
                    "hidden": false
                },
                {
                    "_id": "691fdd84cce0eb9b6387afe7",
                    "name": "Xianzhong Zhao",
                    "hidden": false
                },
                {
                    "_id": "691fdd84cce0eb9b6387afe8",
                    "name": "Xipeng Qiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-19T16:52:23.000Z",
            "submittedOnDailyAt": "2025-11-21T01:49:30.528Z",
            "title": "SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models",
            "submittedOnDailyBy": {
                "_id": "64c3c631e77ea9f28111172a",
                "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
                "isPro": false,
                "fullname": "Siyin Wang (SII)",
                "user": "sinwang",
                "type": "user"
            },
            "summary": "Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.",
            "upvotes": 8,
            "discussionId": "691fdd84cce0eb9b6387afe9",
            "ai_summary": "Self-Referential Policy Optimization (SRPO) uses latent world representations to assign progress-wise rewards to failed trajectories, improving efficiency and effectiveness in vision-language-action robotic manipulation tasks without external demonstrations.",
            "ai_keywords": [
                "Reinforcement learning",
                "Self-Referential Policy Optimization",
                "SRPO",
                "latent world representations",
                "LIBERO benchmark",
                "LIBERO-Plus benchmark"
            ],
            "organization": {
                "_id": "613b0dee83ec35d460684607",
                "name": "OpenMOSS-Team",
                "fullname": "OpenMOSS",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
            }
        },
        "publishedAt": "2025-11-19T11:52:23.000Z",
        "title": "SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models",
        "summary": "Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15605.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c3c631e77ea9f28111172a",
            "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
            "fullname": "Siyin Wang (SII)",
            "name": "sinwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "organization": {
            "_id": "613b0dee83ec35d460684607",
            "name": "OpenMOSS-Team",
            "fullname": "OpenMOSS",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.16618",
            "authors": [
                {
                    "_id": "691fd2cdcce0eb9b6387af75",
                    "user": {
                        "_id": "66bf75432777c05070bf49dc",
                        "avatarUrl": "/avatars/e18c2f6f5b1c37873dc181de4f255a8b.svg",
                        "isPro": false,
                        "fullname": "Haofeng Liu",
                        "user": "HeverLaw",
                        "type": "user"
                    },
                    "name": "Haofeng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T08:59:51.125Z",
                    "hidden": false
                },
                {
                    "_id": "691fd2cdcce0eb9b6387af76",
                    "name": "Ziyue Wang",
                    "hidden": false
                },
                {
                    "_id": "691fd2cdcce0eb9b6387af77",
                    "name": "Sudhanshu Mishra",
                    "hidden": false
                },
                {
                    "_id": "691fd2cdcce0eb9b6387af78",
                    "name": "Mingqi Gao",
                    "hidden": false
                },
                {
                    "_id": "691fd2cdcce0eb9b6387af79",
                    "name": "Guanyi Qin",
                    "hidden": false
                },
                {
                    "_id": "691fd2cdcce0eb9b6387af7a",
                    "name": "Chang Han Low",
                    "hidden": false
                },
                {
                    "_id": "691fd2cdcce0eb9b6387af7b",
                    "user": {
                        "_id": "691fd8847620f914ecec559f",
                        "avatarUrl": "/avatars/0d1688c65fbcb77de749ef0ff7e43eba.svg",
                        "isPro": false,
                        "fullname": "Alex Kong",
                        "user": "Awsdude",
                        "type": "user"
                    },
                    "name": "Alex Y. W. Kong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T08:59:47.726Z",
                    "hidden": false
                },
                {
                    "_id": "691fd2cdcce0eb9b6387af7c",
                    "name": "Yueming Jin",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66bf75432777c05070bf49dc/6p9AWIJ9wPFAdXc-QSy9V.mp4"
            ],
            "publishedAt": "2025-11-20T18:18:49.000Z",
            "submittedOnDailyAt": "2025-11-21T00:35:55.273Z",
            "title": "SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking",
            "submittedOnDailyBy": {
                "_id": "66bf75432777c05070bf49dc",
                "avatarUrl": "/avatars/e18c2f6f5b1c37873dc181de4f255a8b.svg",
                "isPro": false,
                "fullname": "Haofeng Liu",
                "user": "HeverLaw",
                "type": "user"
            },
            "summary": "Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing SAM2 for Surgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average J\\&F over vanilla SAM2. SAM2S further advances performance to 80.42 average J\\&F, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.",
            "upvotes": 5,
            "discussionId": "691fd2cecce0eb9b6387af7d",
            "projectPage": "https://jinlab-imvr.github.io/SAM2S",
            "githubRepo": "https://github.com/jinlab-imvr/SAM2S",
            "ai_summary": "A surgical video segmentation model SAM2S enhances interactive video object segmentation through robust memory, temporal learning, and ambiguity handling, achieving high performance and real-time inference on a comprehensive surgical benchmark.",
            "ai_keywords": [
                "Interactive Video Object Segmentation",
                "iVOS",
                "Segment Anything Model 2",
                "SAM2",
                "SA-SV",
                "DiveMem",
                "temporal semantic learning",
                "ambiguity-resilient learning",
                "$\\mathcal{J}$\\&$\\mathcal{F}$",
                "real-time inference",
                "zero-shot generalization"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "6508ab2b349930913196378b",
                "name": "NationalUniversityofSingapore",
                "fullname": "National University of Singapore",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
            }
        },
        "publishedAt": "2025-11-20T13:18:49.000Z",
        "title": "SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking",
        "summary": "Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing SAM2 for Surgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average J\\&F over vanilla SAM2. SAM2S further advances performance to 80.42 average J\\&F, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66bf75432777c05070bf49dc/6p9AWIJ9wPFAdXc-QSy9V.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16618.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66bf75432777c05070bf49dc",
            "avatarUrl": "/avatars/e18c2f6f5b1c37873dc181de4f255a8b.svg",
            "fullname": "Haofeng Liu",
            "name": "HeverLaw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "6508ab2b349930913196378b",
            "name": "NationalUniversityofSingapore",
            "fullname": "National University of Singapore",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.16595",
            "authors": [
                {
                    "_id": "691fd2bccce0eb9b6387af6c",
                    "user": {
                        "_id": "67c9728a30792f0621bb2c3c",
                        "avatarUrl": "/avatars/0d70477cbd046823d4abb7b4ba85b0a1.svg",
                        "isPro": false,
                        "fullname": "Xu",
                        "user": "Boshenxx",
                        "type": "user"
                    },
                    "name": "Boshen Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T08:59:53.327Z",
                    "hidden": false
                },
                {
                    "_id": "691fd2bccce0eb9b6387af6d",
                    "name": "Zihan Xiao",
                    "hidden": false
                },
                {
                    "_id": "691fd2bccce0eb9b6387af6e",
                    "name": "Jiaze Li",
                    "hidden": false
                },
                {
                    "_id": "691fd2bccce0eb9b6387af6f",
                    "name": "Jianzhong Ju",
                    "hidden": false
                },
                {
                    "_id": "691fd2bccce0eb9b6387af70",
                    "name": "Zhenbo Luo",
                    "hidden": false
                },
                {
                    "_id": "691fd2bccce0eb9b6387af71",
                    "name": "Jian Luan",
                    "hidden": false
                },
                {
                    "_id": "691fd2bccce0eb9b6387af72",
                    "name": "Qin Jin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T17:48:21.000Z",
            "submittedOnDailyAt": "2025-11-21T00:18:29.862Z",
            "title": "TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding",
            "submittedOnDailyBy": {
                "_id": "67c9728a30792f0621bb2c3c",
                "avatarUrl": "/avatars/0d70477cbd046823d4abb7b4ba85b0a1.svg",
                "isPro": false,
                "fullname": "Xu",
                "user": "Boshenxx",
                "type": "user"
            },
            "summary": "We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.",
            "upvotes": 4,
            "discussionId": "691fd2bccce0eb9b6387af73",
            "projectPage": "https://xuboshen.github.io/TimeViper/",
            "githubRepo": "https://github.com/xiaomi-research/timeviper",
            "ai_summary": "TimeViper, a hybrid vision-language model using Mamba-Transformer architecture, efficiently processes long videos by transferring and compressing vision tokens to text tokens, achieving state-of-the-art performance while handling over 10,000 frames.",
            "ai_keywords": [
                "hybrid vision-language model",
                "TimeViper",
                "Mamba-Transformer",
                "state-space models",
                "attention mechanisms",
                "vision-to-text information aggregation",
                "vision tokens",
                "text tokens",
                "LLM depth",
                "vision token redundancy",
                "TransV",
                "token information transfer module",
                "multimodal understanding",
                "hybrid model interpretability"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "622177ac43826d6f261f8208",
                "name": "RUC",
                "fullname": "Renmin University of China",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
            }
        },
        "publishedAt": "2025-11-20T12:48:21.000Z",
        "title": "TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding",
        "summary": "We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16595.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67c9728a30792f0621bb2c3c",
            "avatarUrl": "/avatars/0d70477cbd046823d4abb7b4ba85b0a1.svg",
            "fullname": "Xu",
            "name": "Boshenxx",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "622177ac43826d6f261f8208",
            "name": "RUC",
            "fullname": "Renmin University of China",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.16317",
            "authors": [
                {
                    "_id": "691fda75cce0eb9b6387afc0",
                    "user": {
                        "_id": "63044b89eedc089484c995ad",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
                        "isPro": false,
                        "fullname": "Zeqiang Lai",
                        "user": "ZeqiangLai",
                        "type": "user"
                    },
                    "name": "Zeqiang Lai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-21T10:52:23.638Z",
                    "hidden": false
                },
                {
                    "_id": "691fda75cce0eb9b6387afc1",
                    "name": "Yunfei Zhao",
                    "hidden": false
                },
                {
                    "_id": "691fda75cce0eb9b6387afc2",
                    "user": {
                        "_id": "62d8ce11c60d1450a1ed8795",
                        "avatarUrl": "/avatars/26f1ca693ad7106be0f2f469070d8500.svg",
                        "isPro": false,
                        "fullname": "zibo.zhao",
                        "user": "cocacola",
                        "type": "user"
                    },
                    "name": "Zibo Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-21T10:52:58.130Z",
                    "hidden": false
                },
                {
                    "_id": "691fda75cce0eb9b6387afc3",
                    "name": "Xin Yang",
                    "hidden": false
                },
                {
                    "_id": "691fda75cce0eb9b6387afc4",
                    "name": "Xin Huang",
                    "hidden": false
                },
                {
                    "_id": "691fda75cce0eb9b6387afc5",
                    "name": "Jingwei Huang",
                    "hidden": false
                },
                {
                    "_id": "691fda75cce0eb9b6387afc6",
                    "user": {
                        "_id": "666a8f24e2990b0cb16b7bf9",
                        "avatarUrl": "/avatars/fcbaf8f1e3e53a2a4a819b7cb2c53aa4.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Yue",
                        "user": "xyyue",
                        "type": "user"
                    },
                    "name": "Xiangyu Yue",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-21T10:52:43.076Z",
                    "hidden": false
                },
                {
                    "_id": "691fda75cce0eb9b6387afc7",
                    "name": "Chunchao Guo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T12:47:22.000Z",
            "submittedOnDailyAt": "2025-11-21T00:51:26.132Z",
            "title": "NaTex: Seamless Texture Generation as Latent Color Diffusion",
            "submittedOnDailyBy": {
                "_id": "63044b89eedc089484c995ad",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
                "isPro": false,
                "fullname": "Zeqiang Lai",
                "user": "ZeqiangLai",
                "type": "user"
            },
            "summary": "We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing.",
            "upvotes": 4,
            "discussionId": "691fda76cce0eb9b6387afc8",
            "projectPage": "https://natex-ldm.github.io/",
            "githubRepo": "https://github.com/Zeqiang-Lai/NaTex",
            "ai_summary": "NaTex generates 3D textures directly using latent color diffusion and geometry-aware models, outperforming previous methods in coherence and alignment.",
            "ai_keywords": [
                "native texture generation",
                "3D space",
                "Multi-View Diffusion models",
                "latent color diffusion",
                "geometry-awared color point cloud VAE",
                "multi-control diffusion transformer",
                "positional embeddings",
                "geometry latents",
                "texture coherence",
                "alignment",
                "material generation",
                "texture refinement",
                "part segmentation and texturing"
            ],
            "githubStars": 46,
            "organization": {
                "_id": "6645f953c39288df638dbdd5",
                "name": "Tencent-Hunyuan",
                "fullname": "Tencent Hunyuan",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
            }
        },
        "publishedAt": "2025-11-20T07:47:22.000Z",
        "title": "NaTex: Seamless Texture Generation as Latent Color Diffusion",
        "summary": "We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16317.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63044b89eedc089484c995ad",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
            "fullname": "Zeqiang Lai",
            "name": "ZeqiangLai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "organization": {
            "_id": "6645f953c39288df638dbdd5",
            "name": "Tencent-Hunyuan",
            "fullname": "Tencent Hunyuan",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.16659",
            "authors": [
                {
                    "_id": "691fcf6ccce0eb9b6387af24",
                    "name": "Zhaoning Wang",
                    "hidden": false
                },
                {
                    "_id": "691fcf6ccce0eb9b6387af25",
                    "user": {
                        "_id": "646d6615da8e99940b7143cf",
                        "avatarUrl": "/avatars/e6e24e993fc4f3e9c7026b4c34bd1a25.svg",
                        "isPro": false,
                        "fullname": "Xinyue Wei",
                        "user": "sarahwei0210",
                        "type": "user"
                    },
                    "name": "Xinyue Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-21T10:46:11.563Z",
                    "hidden": false
                },
                {
                    "_id": "691fcf6ccce0eb9b6387af26",
                    "name": "Ruoxi Shi",
                    "hidden": false
                },
                {
                    "_id": "691fcf6ccce0eb9b6387af27",
                    "user": {
                        "_id": "65f0bae3118cb0547bc2cf75",
                        "avatarUrl": "/avatars/3c13bf493298a2e052125328ec42c9ad.svg",
                        "isPro": false,
                        "fullname": "Xiaoshuai Zhang",
                        "user": "jetd",
                        "type": "user"
                    },
                    "name": "Xiaoshuai Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-21T10:46:22.220Z",
                    "hidden": false
                },
                {
                    "_id": "691fcf6ccce0eb9b6387af28",
                    "name": "Hao Su",
                    "hidden": false
                },
                {
                    "_id": "691fcf6ccce0eb9b6387af29",
                    "name": "Minghua Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T18:58:39.000Z",
            "submittedOnDailyAt": "2025-11-21T00:03:26.903Z",
            "title": "PartUV: Part-Based UV Unwrapping of 3D Meshes",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing artifacts and hindering downstream tasks. We introduce PartUV, a part-based UV unwrapping pipeline that generates significantly fewer, part-aligned charts while maintaining low distortion. Built on top of a recent learning-based part decomposition method PartField, PartUV combines high-level semantic part decomposition with novel geometric heuristics in a top-down recursive framework. It ensures each chart's distortion remains below a user-specified threshold while minimizing the total number of charts. The pipeline integrates and extends parameterization and packing algorithms, incorporates dedicated handling of non-manifold and degenerate meshes, and is extensively parallelized for efficiency. Evaluated across four diverse datasets, including man-made, CAD, AI-generated, and Common Shapes, PartUV outperforms existing tools and recent neural methods in chart count and seam length, achieves comparable distortion, exhibits high success rates on challenging meshes, and enables new applications like part-specific multi-tiles packing. Our project page is at https://www.zhaoningwang.com/PartUV.",
            "upvotes": 3,
            "discussionId": "691fcf6dcce0eb9b6387af2a",
            "projectPage": "https://www.zhaoningwang.com/PartUV/",
            "ai_summary": "PartUV, a part-based UV unwrapping pipeline, improves chart quality and reduces fragmentation for AI-generated meshes using part decomposition and geometric heuristics.",
            "ai_keywords": [
                "UV unwrapping",
                "part-based UV unwrapping",
                "PartUV",
                "PartField",
                "learning-based part decomposition",
                "geometric heuristics",
                "parameterization",
                "packing algorithms",
                "non-manifold",
                "degenerate meshes",
                "parallelization"
            ]
        },
        "publishedAt": "2025-11-20T13:58:39.000Z",
        "title": "PartUV: Part-Based UV Unwrapping of 3D Meshes",
        "summary": "UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing artifacts and hindering downstream tasks. We introduce PartUV, a part-based UV unwrapping pipeline that generates significantly fewer, part-aligned charts while maintaining low distortion. Built on top of a recent learning-based part decomposition method PartField, PartUV combines high-level semantic part decomposition with novel geometric heuristics in a top-down recursive framework. It ensures each chart's distortion remains below a user-specified threshold while minimizing the total number of charts. The pipeline integrates and extends parameterization and packing algorithms, incorporates dedicated handling of non-manifold and degenerate meshes, and is extensively parallelized for efficiency. Evaluated across four diverse datasets, including man-made, CAD, AI-generated, and Common Shapes, PartUV outperforms existing tools and recent neural methods in chart count and seam length, achieves comparable distortion, exhibits high success rates on challenging meshes, and enables new applications like part-specific multi-tiles packing. Our project page is at https://www.zhaoningwang.com/PartUV.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16659.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 169
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.15248",
            "authors": [
                {
                    "_id": "691e7f8e3c64d32b03645884",
                    "user": {
                        "_id": "64930ae54ab21070dc8a42db",
                        "avatarUrl": "/avatars/b341feaf8779b39c225c5b044f29e671.svg",
                        "isPro": false,
                        "fullname": "Kai Yang",
                        "user": "yangkaiSIGS",
                        "type": "user"
                    },
                    "name": "Kai Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T16:49:12.944Z",
                    "hidden": false
                },
                {
                    "_id": "691e7f8e3c64d32b03645885",
                    "name": "Xin Xu",
                    "hidden": false
                },
                {
                    "_id": "691e7f8e3c64d32b03645886",
                    "name": "Yangkun Chen",
                    "hidden": false
                },
                {
                    "_id": "691e7f8e3c64d32b03645887",
                    "name": "Weijie Liu",
                    "hidden": false
                },
                {
                    "_id": "691e7f8e3c64d32b03645888",
                    "name": "Jiafei Lyu",
                    "hidden": false
                },
                {
                    "_id": "691e7f8e3c64d32b03645889",
                    "name": "Zichuan Lin",
                    "hidden": false
                },
                {
                    "_id": "691e7f8e3c64d32b0364588a",
                    "name": "Deheng Ye",
                    "hidden": false
                },
                {
                    "_id": "691e7f8e3c64d32b0364588b",
                    "name": "Saiyong Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-19T09:06:42.000Z",
            "submittedOnDailyAt": "2025-11-21T12:48:06.171Z",
            "title": "EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control",
            "submittedOnDailyBy": {
                "_id": "64930ae54ab21070dc8a42db",
                "avatarUrl": "/avatars/b341feaf8779b39c225c5b044f29e671.svg",
                "isPro": false,
                "fullname": "Kai Yang",
                "user": "yangkaiSIGS",
                "type": "user"
            },
            "summary": "Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.",
            "upvotes": 3,
            "discussionId": "691e7f8e3c64d32b0364588c",
            "projectPage": "https://huggingface.co/spaces/yangkaiSIGS/entropic",
            "githubRepo": "https://github.com/yk7333/EntroPIC",
            "ai_summary": "EntroPIC, a novel reinforcement learning method, adaptsively tunes loss coefficients to stabilize entropy during long-term training of large language models, ensuring efficient exploration and optimal training.",
            "ai_keywords": [
                "large language models",
                "entropy",
                "reinforcement learning",
                "positive and negative samples",
                "proportional-integral control",
                "on-policy learning",
                "off-policy learning"
            ],
            "githubStars": 4,
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-11-19T04:06:42.000Z",
        "title": "EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control",
        "summary": "Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15248.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64930ae54ab21070dc8a42db",
            "avatarUrl": "/avatars/b341feaf8779b39c225c5b044f29e671.svg",
            "fullname": "Kai Yang",
            "name": "yangkaiSIGS",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.14865",
            "authors": [
                {
                    "_id": "691f45cb6d58f71c91487f3a",
                    "user": {
                        "_id": "64b58a4cb84198afd5cf7e32",
                        "avatarUrl": "/avatars/679217a4fb03b5df70a7678989182483.svg",
                        "isPro": false,
                        "fullname": "Dwipam Katariya",
                        "user": "dwipamc1",
                        "type": "user"
                    },
                    "name": "Dwipam Katariya",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T09:00:45.731Z",
                    "hidden": false
                },
                {
                    "_id": "691f45cb6d58f71c91487f3b",
                    "name": "Snehita Varma",
                    "hidden": false
                },
                {
                    "_id": "691f45cb6d58f71c91487f3c",
                    "name": "Akshat Shreemali",
                    "hidden": false
                },
                {
                    "_id": "691f45cb6d58f71c91487f3d",
                    "name": "Benjamin Wu",
                    "hidden": false
                },
                {
                    "_id": "691f45cb6d58f71c91487f3e",
                    "name": "Kalanand Mishra",
                    "hidden": false
                },
                {
                    "_id": "691f45cb6d58f71c91487f3f",
                    "name": "Pranab Mohanty",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-18T19:32:38.000Z",
            "submittedOnDailyAt": "2025-11-21T12:50:03.126Z",
            "title": "FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications",
            "submittedOnDailyBy": {
                "_id": "64b58a4cb84198afd5cf7e32",
                "avatarUrl": "/avatars/679217a4fb03b5df70a7678989182483.svg",
                "isPro": false,
                "fullname": "Dwipam Katariya",
                "user": "dwipamc1",
                "type": "user"
            },
            "summary": "Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.",
            "upvotes": 3,
            "discussionId": "691f45cb6d58f71c91487f40",
            "ai_summary": "FinTRec, a transformer-based framework, addresses challenges in financial services recommendation systems by handling long-range interactions and multiple products, outperforming traditional tree-based models.",
            "ai_keywords": [
                "transformer-based architectures",
                "sequential recommendation systems",
                "long-range user interactions",
                "temporally heterogeneous context",
                "interrelated products",
                "coordinated models",
                "FinTRec",
                "tree-based models",
                "explainability",
                "regulatory requirements",
                "product adaptation",
                "cross-product signal sharing",
                "training cost",
                "technical debt",
                "offline performance"
            ],
            "organization": {
                "_id": "64b705f22f5a966b97251222",
                "name": "capitalone",
                "fullname": "Capital One",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68d2f735945ddf734dbd0c47/e89rBuApdzDHVe120Tp98.png"
            }
        },
        "publishedAt": "2025-11-18T14:32:38.000Z",
        "title": "FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications",
        "summary": "Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.14865.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b58a4cb84198afd5cf7e32",
            "avatarUrl": "/avatars/679217a4fb03b5df70a7678989182483.svg",
            "fullname": "Dwipam Katariya",
            "name": "dwipamc1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "64b705f22f5a966b97251222",
            "name": "capitalone",
            "fullname": "Capital One",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68d2f735945ddf734dbd0c47/e89rBuApdzDHVe120Tp98.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.16315",
            "authors": [
                {
                    "_id": "69207688b5612535ed95545e",
                    "name": "Samuel Stevens",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64df8592d27135dd568380b5/coGdATJxpCqUcAv0-PTOT.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64df8592d27135dd568380b5/2bHOt_2SxyOy-7gqa8NY1.png"
            ],
            "publishedAt": "2025-11-20T12:46:33.000Z",
            "submittedOnDailyAt": "2025-11-21T11:59:29.892Z",
            "title": "BioBench: A Blueprint to Move Beyond ImageNet for Scientific ML Benchmarks",
            "submittedOnDailyBy": {
                "_id": "64df8592d27135dd568380b5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64df8592d27135dd568380b5/1-YkiTAkI11QBbZnVRjJu.jpeg",
                "isPro": false,
                "fullname": "Samuel Stevens",
                "user": "samuelstevens",
                "type": "user"
            },
            "summary": "ImageNet-1K linear-probe transfer accuracy remains the default proxy for visual representation quality, yet it no longer predicts performance on scientific imagery. Across 46 modern vision model checkpoints, ImageNet top-1 accuracy explains only 34% of variance on ecology tasks and mis-ranks 30% of models above 75% accuracy. We present BioBench, an open ecology vision benchmark that captures what ImageNet misses. BioBench unifies 9 publicly released, application-driven tasks, 4 taxonomic kingdoms, and 6 acquisition modalities (drone RGB, web video, micrographs, in-situ and specimen photos, camera-trap frames), totaling 3.1M images. A single Python API downloads data, fits lightweight classifiers to frozen backbones, and reports class-balanced macro-F1 (plus domain metrics for FishNet and FungiCLEF); ViT-L models evaluate in 6 hours on an A6000 GPU. BioBench provides new signal for computer vision in ecology and a template recipe for building reliable AI-for-science benchmarks in any domain. Code and predictions are available at https://github.com/samuelstevens/biobench and results at https://samuelstevens.me/biobench.",
            "upvotes": 1,
            "discussionId": "69207688b5612535ed95545f",
            "projectPage": "https://samuelstevens.me/biobench",
            "githubRepo": "https://github.com/samuelstevens/biobench",
            "ai_summary": "BioBench is an open ecology vision benchmark that addresses the limitations of ImageNet-1K accuracy for scientific imagery by evaluating models on a diverse set of ecological tasks and modalities.",
            "ai_keywords": [
                "ImageNet-1K",
                "linear-probe transfer accuracy",
                "visual representation quality",
                "ecology tasks",
                "BioBench",
                "application-driven tasks",
                "taxonomic kingdoms",
                "acquisition modalities",
                "drone RGB",
                "web video",
                "micrographs",
                "in-situ",
                "specimen photos",
                "camera-trap frames",
                "class-balanced macro-F1",
                "ViT-L models"
            ],
            "githubStars": 8,
            "organization": {
                "_id": "6330c5f48ef21f473089e0c4",
                "name": "imageomics",
                "fullname": "HDR Imageomics Institute",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664140778418-6330c479c375586dc3b2aa2c.jpeg"
            }
        },
        "publishedAt": "2025-11-20T07:46:33.000Z",
        "title": "BioBench: A Blueprint to Move Beyond ImageNet for Scientific ML Benchmarks",
        "summary": "ImageNet-1K linear-probe transfer accuracy remains the default proxy for visual representation quality, yet it no longer predicts performance on scientific imagery. Across 46 modern vision model checkpoints, ImageNet top-1 accuracy explains only 34% of variance on ecology tasks and mis-ranks 30% of models above 75% accuracy. We present BioBench, an open ecology vision benchmark that captures what ImageNet misses. BioBench unifies 9 publicly released, application-driven tasks, 4 taxonomic kingdoms, and 6 acquisition modalities (drone RGB, web video, micrographs, in-situ and specimen photos, camera-trap frames), totaling 3.1M images. A single Python API downloads data, fits lightweight classifiers to frozen backbones, and reports class-balanced macro-F1 (plus domain metrics for FishNet and FungiCLEF); ViT-L models evaluate in 6 hours on an A6000 GPU. BioBench provides new signal for computer vision in ecology and a template recipe for building reliable AI-for-science benchmarks in any domain. Code and predictions are available at https://github.com/samuelstevens/biobench and results at https://samuelstevens.me/biobench.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64df8592d27135dd568380b5/coGdATJxpCqUcAv0-PTOT.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64df8592d27135dd568380b5/2bHOt_2SxyOy-7gqa8NY1.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16315.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64df8592d27135dd568380b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64df8592d27135dd568380b5/1-YkiTAkI11QBbZnVRjJu.jpeg",
            "fullname": "Samuel Stevens",
            "name": "samuelstevens",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "6330c5f48ef21f473089e0c4",
            "name": "imageomics",
            "fullname": "HDR Imageomics Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664140778418-6330c479c375586dc3b2aa2c.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.15943",
            "authors": [
                {
                    "_id": "692023d0cce0eb9b6387b069",
                    "user": {
                        "_id": "64176d65b03817ada642b6e2",
                        "avatarUrl": "/avatars/27fe82eaf9db2894c0bf86acb59543fe.svg",
                        "isPro": false,
                        "fullname": "Zihan Li",
                        "user": "zl111",
                        "type": "user"
                    },
                    "name": "Zihan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T08:59:16.511Z",
                    "hidden": false
                },
                {
                    "_id": "692023d0cce0eb9b6387b06a",
                    "name": "Yiqing Wang",
                    "hidden": false
                },
                {
                    "_id": "692023d0cce0eb9b6387b06b",
                    "name": "Sina Farsiu",
                    "hidden": false
                },
                {
                    "_id": "692023d0cce0eb9b6387b06c",
                    "name": "Paul Kinahan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T00:24:26.000Z",
            "submittedOnDailyAt": "2025-11-21T14:14:21.252Z",
            "title": "Boosting Medical Visual Understanding From Multi-Granular Language Learning",
            "submittedOnDailyBy": {
                "_id": "64176d65b03817ada642b6e2",
                "avatarUrl": "/avatars/27fe82eaf9db2894c0bf86acb59543fe.svg",
                "isPro": false,
                "fullname": "Zihan Li",
                "user": "zl111",
                "type": "user"
            },
            "summary": "Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at https://github.com/HUANGLIZI/MGLL{https://github.com/HUANGLIZI/MGLL}.",
            "upvotes": 1,
            "discussionId": "692023d1cce0eb9b6387b06d",
            "projectPage": "https://github.com/HUANGLIZI/MGLL",
            "githubRepo": "https://github.com/HUANGLIZI/MGLL",
            "ai_summary": "Multi-Granular Language Learning (MGLL) enhances visual understanding by improving multi-label and cross-granularity alignment in image-text pretraining, outperforming existing methods in complex domains like medical imaging.",
            "ai_keywords": [
                "Contrastive Language-Image Pretraining (CLIP)",
                "multi-granular alignment",
                "structured multi-label supervision",
                "soft-label supervision",
                "smooth Kullback-Leibler (KL) divergence",
                "vision-language models"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-11-19T19:24:26.000Z",
        "title": "Boosting Medical Visual Understanding From Multi-Granular Language Learning",
        "summary": "Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at https://github.com/HUANGLIZI/MGLL{https://github.com/HUANGLIZI/MGLL}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15943.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64176d65b03817ada642b6e2",
            "avatarUrl": "/avatars/27fe82eaf9db2894c0bf86acb59543fe.svg",
            "fullname": "Zihan Li",
            "name": "zl111",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.11005",
            "authors": [
                {
                    "_id": "69200ed5cce0eb9b6387b03e",
                    "name": "Sungheon Jeong",
                    "hidden": false
                },
                {
                    "_id": "69200ed5cce0eb9b6387b03f",
                    "name": "Ryozo Masukawa",
                    "hidden": false
                },
                {
                    "_id": "69200ed5cce0eb9b6387b040",
                    "name": "Jihong Park",
                    "hidden": false
                },
                {
                    "_id": "69200ed5cce0eb9b6387b041",
                    "name": "Sanggeon Yun",
                    "hidden": false
                },
                {
                    "_id": "69200ed5cce0eb9b6387b042",
                    "name": "Wenjun Huang",
                    "hidden": false
                },
                {
                    "_id": "69200ed5cce0eb9b6387b043",
                    "name": "Hanning Chen",
                    "hidden": false
                },
                {
                    "_id": "69200ed5cce0eb9b6387b044",
                    "name": "Mahdi Imani",
                    "hidden": false
                },
                {
                    "_id": "69200ed5cce0eb9b6387b045",
                    "name": "Mohsen Imani",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-14T06:50:14.000Z",
            "submittedOnDailyAt": "2025-11-21T04:36:20.389Z",
            "title": "Draft and Refine with Visual Experts",
            "submittedOnDailyBy": {
                "_id": "6445e9bd1cfc9ae6bb40985c",
                "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
                "isPro": false,
                "fullname": "Evan Jeong",
                "user": "Eavn",
                "type": "user"
            },
            "summary": "While recent Large Vision-Language Models (LVLMs) exhibit strong multimodal reasoning abilities, they often produce ungrounded or hallucinated responses because they rely too heavily on linguistic priors instead of visual evidence. This limitation highlights the absence of a quantitative measure of how much these models actually use visual information during reasoning. We propose Draft and Refine (DnR), an agent framework driven by a question-conditioned utilization metric. The metric quantifies the model's reliance on visual evidence by first constructing a query-conditioned relevance map to localize question-specific cues and then measuring dependence through relevance-guided probabilistic masking. Guided by this metric, the DnR agent refines its initial draft using targeted feedback from external visual experts. Each expert's output (such as boxes or masks) is rendered as visual cues on the image, and the model is re-queried to select the response that yields the largest improvement in utilization. This process strengthens visual grounding without retraining or architectural changes. Experiments across VQA and captioning benchmarks show consistent accuracy gains and reduced hallucination, demonstrating that measuring visual utilization provides a principled path toward more interpretable and evidence-driven multimodal agent systems.",
            "upvotes": 1,
            "discussionId": "69200ed6cce0eb9b6387b046",
            "githubRepo": "https://github.com/EavnJeong/Draft-and-Refine-with-Visual-Experts",
            "ai_summary": "The Draft and Refine (DnR) agent framework uses a question-conditioned utilization metric to improve visual grounding in large vision-language models, reducing hallucinations and increasing accuracy.",
            "ai_keywords": [
                "Large Vision-Language Models (LVLMs)",
                "multimodal reasoning",
                "hallucinations",
                "linguistic priors",
                "visual evidence",
                "question-conditioned utilization metric",
                "relevance map",
                "probabilistic masking",
                "visual experts",
                "visual cues",
                "VQA",
                "captioning benchmarks",
                "visual grounding",
                "interpretable",
                "evidence-driven multimodal agent systems"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "6833a989f68e4b263ce97be3",
                "name": "UCI6055",
                "fullname": "University of California, Irvine ",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6833a7ae231225ee2016e2bc/s-bfZozrM7zQZKXl65W9P.png"
            }
        },
        "publishedAt": "2025-11-14T01:50:14.000Z",
        "title": "Draft and Refine with Visual Experts",
        "summary": "While recent Large Vision-Language Models (LVLMs) exhibit strong multimodal reasoning abilities, they often produce ungrounded or hallucinated responses because they rely too heavily on linguistic priors instead of visual evidence. This limitation highlights the absence of a quantitative measure of how much these models actually use visual information during reasoning. We propose Draft and Refine (DnR), an agent framework driven by a question-conditioned utilization metric. The metric quantifies the model's reliance on visual evidence by first constructing a query-conditioned relevance map to localize question-specific cues and then measuring dependence through relevance-guided probabilistic masking. Guided by this metric, the DnR agent refines its initial draft using targeted feedback from external visual experts. Each expert's output (such as boxes or masks) is rendered as visual cues on the image, and the model is re-queried to select the response that yields the largest improvement in utilization. This process strengthens visual grounding without retraining or architectural changes. Experiments across VQA and captioning benchmarks show consistent accuracy gains and reduced hallucination, demonstrating that measuring visual utilization provides a principled path toward more interpretable and evidence-driven multimodal agent systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11005.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6445e9bd1cfc9ae6bb40985c",
            "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
            "fullname": "Evan Jeong",
            "name": "Eavn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6833a989f68e4b263ce97be3",
            "name": "UCI6055",
            "fullname": "University of California, Irvine ",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6833a7ae231225ee2016e2bc/s-bfZozrM7zQZKXl65W9P.png"
        },
        "isAuthorParticipating": false
    }
]