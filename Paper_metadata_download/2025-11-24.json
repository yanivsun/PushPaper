[
    {
        "paper": {
            "id": "2511.16334",
            "authors": [
                {
                    "_id": "691fcb8fcce0eb9b6387aeda",
                    "user": {
                        "_id": "64bb77e786e7fb5b8a317a43",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png",
                        "isPro": false,
                        "fullname": "kcz",
                        "user": "kcz358",
                        "type": "user"
                    },
                    "name": "Kaichen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-24T07:58:42.998Z",
                    "hidden": false
                },
                {
                    "_id": "691fcb8fcce0eb9b6387aedb",
                    "user": {
                        "_id": "66bf00ca5b4e241fe266059d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png",
                        "isPro": false,
                        "fullname": "Keming Wu",
                        "user": "wukeming11",
                        "type": "user"
                    },
                    "name": "Keming Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-24T12:27:05.035Z",
                    "hidden": false
                },
                {
                    "_id": "691fcb8fcce0eb9b6387aedc",
                    "name": "Zuhao Yang",
                    "hidden": false
                },
                {
                    "_id": "691fcb8fcce0eb9b6387aedd",
                    "name": "Kairui Hu",
                    "hidden": false
                },
                {
                    "_id": "691fcb8fcce0eb9b6387aede",
                    "name": "Bin Wang",
                    "hidden": false
                },
                {
                    "_id": "691fcb8fcce0eb9b6387aedf",
                    "name": "Ziwei Liu",
                    "hidden": false
                },
                {
                    "_id": "691fcb8fcce0eb9b6387aee0",
                    "name": "Xingxuan Li",
                    "hidden": false
                },
                {
                    "_id": "691fcb8fcce0eb9b6387aee1",
                    "name": "Lidong Bing",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T13:11:45.000Z",
            "submittedOnDailyAt": "2025-11-24T00:11:13.178Z",
            "title": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe",
            "submittedOnDailyBy": {
                "_id": "64bb77e786e7fb5b8a317a43",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png",
                "isPro": false,
                "fullname": "kcz",
                "user": "kcz358",
                "type": "user"
            },
            "summary": "Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.",
            "upvotes": 76,
            "discussionId": "691fcb8fcce0eb9b6387aee2",
            "projectPage": "https://evolvinglmms-lab.github.io/OpenMMReasoner/",
            "githubRepo": "https://github.com/EvolvingLMMs-Lab/OpenMMReasoner",
            "ai_summary": "OpenMMReasoner, a two-stage training approach combining supervised fine-tuning and reinforcement learning, enhances multimodal reasoning performance through rigorous data curation and improved training strategies.",
            "ai_keywords": [
                "multimodal reasoning",
                "supervised fine-tuning",
                "reinforcement learning",
                "cold-start dataset",
                "step-by-step validation",
                "robust learning process",
                "multimodal reasoning benchmarks",
                "large-scale multimodal reasoning"
            ],
            "githubStars": 68,
            "organization": {
                "_id": "6583eb89bed3689928f5d845",
                "name": "lmms-lab",
                "fullname": "LMMs-Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"
            }
        },
        "publishedAt": "2025-11-20T08:11:45.000Z",
        "title": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe",
        "summary": "Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16334.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64bb77e786e7fb5b8a317a43",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png",
            "fullname": "kcz",
            "name": "kcz358",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "organization": {
            "_id": "6583eb89bed3689928f5d845",
            "name": "lmms-lab",
            "fullname": "LMMs-Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.15210",
            "authors": [
                {
                    "_id": "6923ff30b5612535ed955a20",
                    "name": "Vladislav Pedashenko",
                    "hidden": false
                },
                {
                    "_id": "6923ff30b5612535ed955a21",
                    "name": "Laida Kushnareva",
                    "hidden": false
                },
                {
                    "_id": "6923ff30b5612535ed955a22",
                    "name": "Yana Khassan Nibal",
                    "hidden": false
                },
                {
                    "_id": "6923ff30b5612535ed955a23",
                    "name": "Eduard Tulchinskii",
                    "hidden": false
                },
                {
                    "_id": "6923ff30b5612535ed955a24",
                    "name": "Kristian Kuznetsov",
                    "hidden": false
                },
                {
                    "_id": "6923ff30b5612535ed955a25",
                    "name": "Vladislav Zharchinskii",
                    "hidden": false
                },
                {
                    "_id": "6923ff30b5612535ed955a26",
                    "name": "Yury Maximov",
                    "hidden": false
                },
                {
                    "_id": "6923ff30b5612535ed955a27",
                    "name": "Irina Piontkovskaya",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/636254dc2691058b19d9276a/5yDeYqs7yyBgD7hoKeAP1.png",
                "https://cdn-uploads.huggingface.co/production/uploads/636254dc2691058b19d9276a/qwi_UW2deSSfccjsrV1V8.png",
                "https://cdn-uploads.huggingface.co/production/uploads/636254dc2691058b19d9276a/baU0ZNQ4OfKtZCT5b6QGJ.png"
            ],
            "publishedAt": "2025-11-19T08:00:40.000Z",
            "submittedOnDailyAt": "2025-11-24T04:28:35.717Z",
            "title": "Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story",
            "submittedOnDailyBy": {
                "_id": "636254dc2691058b19d9276a",
                "avatarUrl": "/avatars/36eb0e27e0e321fb0ac513f0d4d67c95.svg",
                "isPro": false,
                "fullname": "Kushnareva",
                "user": "Kushnareva",
                "type": "user"
            },
            "summary": "Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text \"representationally simple\" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively \"easy\", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.",
            "upvotes": 72,
            "discussionId": "6923ff30b5612535ed955a28",
            "ai_summary": "The study explores intrinsic dimension in large language models through cross-encoder analysis, linguistic features, and sparse autoencoders, revealing its independence from entropy, genre-specific stratification, and causal features related to text type.",
            "ai_keywords": [
                "intrinsic dimension",
                "cross-encoder analysis",
                "sparse autoencoders",
                "entropy-based metrics",
                "genre stratification",
                "scientific prose",
                "encyclopedic content",
                "creative/opinion writing",
                "causal features",
                "formal tone",
                "report templates",
                "statistics",
                "humanized signals",
                "personalization",
                "emotion",
                "narrative"
            ]
        },
        "publishedAt": "2025-11-19T03:00:40.000Z",
        "title": "Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story",
        "summary": "Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text \"representationally simple\" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively \"easy\", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/636254dc2691058b19d9276a/5yDeYqs7yyBgD7hoKeAP1.png",
            "https://cdn-uploads.huggingface.co/production/uploads/636254dc2691058b19d9276a/qwi_UW2deSSfccjsrV1V8.png",
            "https://cdn-uploads.huggingface.co/production/uploads/636254dc2691058b19d9276a/baU0ZNQ4OfKtZCT5b6QGJ.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15210.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636254dc2691058b19d9276a",
            "avatarUrl": "/avatars/36eb0e27e0e321fb0ac513f0d4d67c95.svg",
            "fullname": "Kushnareva",
            "name": "Kushnareva",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.15705",
            "authors": [
                {
                    "_id": "692066fd8c38b39d6a482df1",
                    "user": {
                        "_id": "627b73728b6ecd7ece822825",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
                        "isPro": false,
                        "fullname": "Yikun Wang (SII)",
                        "user": "LibraTree",
                        "type": "user"
                    },
                    "name": "Yikun Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-24T07:58:14.951Z",
                    "hidden": false
                },
                {
                    "_id": "692066fd8c38b39d6a482df2",
                    "name": "Zuyan Liu",
                    "hidden": false
                },
                {
                    "_id": "692066fd8c38b39d6a482df3",
                    "name": "Ziyi Wang",
                    "hidden": false
                },
                {
                    "_id": "692066fd8c38b39d6a482df4",
                    "name": "Pengfei Liu",
                    "hidden": false
                },
                {
                    "_id": "692066fd8c38b39d6a482df5",
                    "name": "Han Hu",
                    "hidden": false
                },
                {
                    "_id": "692066fd8c38b39d6a482df6",
                    "name": "Yongming Rao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/627b73728b6ecd7ece822825/iNgtSB9Pwu7odhzjGpPS3.qt"
            ],
            "publishedAt": "2025-11-19T18:59:22.000Z",
            "submittedOnDailyAt": "2025-11-24T00:04:22.841Z",
            "title": "GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization",
            "submittedOnDailyBy": {
                "_id": "627b73728b6ecd7ece822825",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
                "isPro": false,
                "fullname": "Yikun Wang (SII)",
                "user": "LibraTree",
                "type": "user"
            },
            "summary": "Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.",
            "upvotes": 60,
            "discussionId": "692066fd8c38b39d6a482df7",
            "projectPage": "https://ekonwang.github.io/geo-vista/",
            "githubRepo": "https://github.com/ekonwang/GeoVista",
            "ai_summary": "GeoVista, an agentic model integrating tool invocation and reinforcement learning, achieves high geolocalization performance on GeoBench, outperforming open-source models and matching closed-source models.",
            "ai_keywords": [
                "geolocalization",
                "GeoBench",
                "GeoVista",
                "image-zoom-in tool",
                "web-search tool",
                "cold-start supervised fine-tuning",
                "reinforcement learning",
                "hierarchical reward",
                "Gemini-2.5-flash",
                "GPT-5"
            ],
            "githubStars": 108,
            "organization": {
                "_id": "6645f953c39288df638dbdd5",
                "name": "Tencent-Hunyuan",
                "fullname": "Tencent Hunyuan",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
            }
        },
        "publishedAt": "2025-11-19T13:59:22.000Z",
        "title": "GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization",
        "summary": "Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/627b73728b6ecd7ece822825/iNgtSB9Pwu7odhzjGpPS3.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15705.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "627b73728b6ecd7ece822825",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
            "fullname": "Yikun Wang (SII)",
            "name": "LibraTree",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "6645f953c39288df638dbdd5",
            "name": "Tencent-Hunyuan",
            "fullname": "Tencent Hunyuan",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.16719",
            "authors": [
                {
                    "_id": "6923c54bb5612535ed9558c9",
                    "name": "Nicolas Carion",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558ca",
                    "name": "Laura Gustafson",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558cb",
                    "name": "Yuan-Ting Hu",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558cc",
                    "name": "Shoubhik Debnath",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558cd",
                    "name": "Ronghang Hu",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558ce",
                    "name": "Didac Suris",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558cf",
                    "name": "Chaitanya Ryali",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558d0",
                    "name": "Kalyan Vasudev Alwala",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558d1",
                    "name": "Haitham Khedr",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558d2",
                    "name": "Andrew Huang",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558d3",
                    "name": "Jie Lei",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558d4",
                    "name": "Tengyu Ma",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558d5",
                    "name": "Baishan Guo",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558d6",
                    "name": "Arpit Kalla",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558d7",
                    "name": "Markus Marks",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558d8",
                    "name": "Joseph Greer",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558d9",
                    "name": "Meng Wang",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558da",
                    "name": "Peize Sun",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558db",
                    "name": "Roman Rädle",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558dc",
                    "name": "Triantafyllos Afouras",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558dd",
                    "name": "Effrosyni Mavroudi",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558de",
                    "name": "Katherine Xu",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558df",
                    "name": "Tsung-Han Wu",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558e0",
                    "name": "Yu Zhou",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558e1",
                    "name": "Liliane Momeni",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558e2",
                    "name": "Rishi Hazra",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558e3",
                    "name": "Shuangrui Ding",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558e4",
                    "name": "Sagar Vaze",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558e5",
                    "name": "Francois Porcher",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558e6",
                    "name": "Feng Li",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558e7",
                    "name": "Siyuan Li",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558e8",
                    "name": "Aishwarya Kamath",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558e9",
                    "name": "Ho Kei Cheng",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558ea",
                    "name": "Piotr Dollár",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558eb",
                    "name": "Nikhila Ravi",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558ec",
                    "name": "Kate Saenko",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558ed",
                    "name": "Pengchuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6923c54bb5612535ed9558ee",
                    "name": "Christoph Feichtenhofer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T18:59:56.000Z",
            "submittedOnDailyAt": "2025-11-24T00:09:18.247Z",
            "title": "SAM 3: Segment Anything with Concepts",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.",
            "upvotes": 49,
            "discussionId": "6923c54bb5612535ed9558ef",
            "projectPage": "https://ai.meta.com/sam3/",
            "githubRepo": "https://github.com/facebookresearch/sam3",
            "ai_summary": "Segment Anything Model 3 achieves state-of-the-art performance in promptable concept segmentation and tracking by leveraging a unified model architecture with decoupled recognition and localization.",
            "ai_keywords": [
                "Segment Anything Model",
                "Promptable Concept Segmentation",
                "concept prompts",
                "segmentation masks",
                "unique identities",
                "scalable data engine",
                "image-level detector",
                "memory-based video tracker",
                "presence head",
                "visual segmentation tasks",
                "Segment Anything with Concepts benchmark"
            ],
            "githubStars": 3797,
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "publishedAt": "2025-11-20T13:59:56.000Z",
        "title": "SAM 3: Segment Anything with Concepts",
        "summary": "We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16719.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 170
        },
        "organization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.13593",
            "authors": [
                {
                    "_id": "6923bd09b5612535ed95586d",
                    "name": "Piaohong Wang",
                    "hidden": false
                },
                {
                    "_id": "6923bd09b5612535ed95586e",
                    "user": {
                        "_id": "690f8f1364c5f58f09c08312",
                        "avatarUrl": "/avatars/7f38fb15cc5a63fdeab7c949a3ec0433.svg",
                        "isPro": false,
                        "fullname": "Motong Tian",
                        "user": "motongt2",
                        "type": "user"
                    },
                    "name": "Motong Tian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-24T07:55:06.380Z",
                    "hidden": false
                },
                {
                    "_id": "6923bd09b5612535ed95586f",
                    "name": "Jiaxian Li",
                    "hidden": false
                },
                {
                    "_id": "6923bd09b5612535ed955870",
                    "name": "Yuan Liang",
                    "hidden": false
                },
                {
                    "_id": "6923bd09b5612535ed955871",
                    "name": "Yuqing Wang",
                    "hidden": false
                },
                {
                    "_id": "6923bd09b5612535ed955872",
                    "name": "Qianben Chen",
                    "hidden": false
                },
                {
                    "_id": "6923bd09b5612535ed955873",
                    "name": "Tiannan Wang",
                    "hidden": false
                },
                {
                    "_id": "6923bd09b5612535ed955874",
                    "name": "Zhicong Lu",
                    "hidden": false
                },
                {
                    "_id": "6923bd09b5612535ed955875",
                    "name": "Jiawei Ma",
                    "hidden": false
                },
                {
                    "_id": "6923bd09b5612535ed955876",
                    "name": "Yuchen Eleanor Jiang",
                    "hidden": false
                },
                {
                    "_id": "6923bd09b5612535ed955877",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-17T16:55:19.000Z",
            "submittedOnDailyAt": "2025-11-24T00:13:31.976Z",
            "title": "O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents",
            "submittedOnDailyBy": {
                "_id": "632bfaebea6e62428ab0e9c2",
                "avatarUrl": "/avatars/344aaf371bbba9aea091b12741c451e5.svg",
                "isPro": false,
                "fullname": "Tiannan Wang",
                "user": "WTNswaggy",
                "type": "user"
            },
            "summary": "Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.67% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.",
            "upvotes": 19,
            "discussionId": "6923bd09b5612535ed955878",
            "ai_summary": "O-Mem, an active user profiling memory framework, enhances contextual consistency and dynamic personalization in LLM-powered agents, improving performance on benchmarks and response efficiency.",
            "ai_keywords": [
                "LLM-powered agents",
                "contextual consistency",
                "dynamic personalization",
                "memory framework",
                "active user profiling",
                "hierarchical retrieval",
                "persona attributes",
                "topic-related context",
                "LoCoMo benchmark",
                "LangMem",
                "PERSONAMEM",
                "A-Mem",
                "token response time",
                "interaction response time"
            ],
            "organization": {
                "_id": "684a463d17db6e9f271a0b66",
                "name": "PersonalAILab",
                "fullname": "OPPO-Personal-AI-Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632bfaebea6e62428ab0e9c2/P5L4TzWg2d4NXntonI-fK.png"
            }
        },
        "publishedAt": "2025-11-17T11:55:19.000Z",
        "title": "O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents",
        "summary": "Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.67% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13593.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "632bfaebea6e62428ab0e9c2",
            "avatarUrl": "/avatars/344aaf371bbba9aea091b12741c451e5.svg",
            "fullname": "Tiannan Wang",
            "name": "WTNswaggy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "684a463d17db6e9f271a0b66",
            "name": "PersonalAILab",
            "fullname": "OPPO-Personal-AI-Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632bfaebea6e62428ab0e9c2/P5L4TzWg2d4NXntonI-fK.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.17502",
            "authors": [
                {
                    "_id": "6923c792b5612535ed955929",
                    "name": "Jun Cen",
                    "hidden": false
                },
                {
                    "_id": "6923c792b5612535ed95592a",
                    "user": {
                        "_id": "65fd82762bf2cd20ddaa193f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
                        "isPro": false,
                        "fullname": "Siteng Huang",
                        "user": "huangsiteng",
                        "type": "user"
                    },
                    "name": "Siteng Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-24T12:27:31.337Z",
                    "hidden": false
                },
                {
                    "_id": "6923c792b5612535ed95592b",
                    "name": "Yuqian Yuan",
                    "hidden": false
                },
                {
                    "_id": "6923c792b5612535ed95592c",
                    "user": {
                        "_id": "649d54b314afbb10ce2a9eeb",
                        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
                        "isPro": false,
                        "fullname": "Hangjie Yuan",
                        "user": "JacobYuan",
                        "type": "user"
                    },
                    "name": "Hangjie Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-24T12:27:45.960Z",
                    "hidden": false
                },
                {
                    "_id": "6923c792b5612535ed95592d",
                    "user": {
                        "_id": "6805e0a6a0ec02e5d8bb31f8",
                        "avatarUrl": "/avatars/dec2296e1f2d39399a8b99777ff4c5ad.svg",
                        "isPro": false,
                        "fullname": "CHAO-HUI-YU",
                        "user": "Alex-snow",
                        "type": "user"
                    },
                    "name": "Chaohui Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-24T12:27:52.601Z",
                    "hidden": false
                },
                {
                    "_id": "6923c792b5612535ed95592e",
                    "user": {
                        "_id": "629c95b7a5d6f5fe10e6ed45",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629c95b7a5d6f5fe10e6ed45/Sy0Ype5snsRookID-gsSm.jpeg",
                        "isPro": false,
                        "fullname": "Yuming Jiang",
                        "user": "yumingj",
                        "type": "user"
                    },
                    "name": "Yuming Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-24T12:27:59.203Z",
                    "hidden": false
                },
                {
                    "_id": "6923c792b5612535ed95592f",
                    "user": {
                        "_id": "66224557c61c7fbd98099079",
                        "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
                        "isPro": false,
                        "fullname": "Jiayan Guo",
                        "user": "SpaceProduct",
                        "type": "user"
                    },
                    "name": "Jiayan Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-24T12:28:06.395Z",
                    "hidden": false
                },
                {
                    "_id": "6923c792b5612535ed955930",
                    "name": "Kehan Li",
                    "hidden": false
                },
                {
                    "_id": "6923c792b5612535ed955931",
                    "name": "Hao Luo",
                    "hidden": false
                },
                {
                    "_id": "6923c792b5612535ed955932",
                    "name": "Fan Wang",
                    "hidden": false
                },
                {
                    "_id": "6923c792b5612535ed955933",
                    "name": "Xin Li",
                    "hidden": false
                },
                {
                    "_id": "6923c792b5612535ed955934",
                    "name": "Deli Zhao",
                    "hidden": false
                },
                {
                    "_id": "6923c792b5612535ed955935",
                    "name": "Hao Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-21T18:59:32.000Z",
            "submittedOnDailyAt": "2025-11-24T00:19:35.570Z",
            "title": "RynnVLA-002: A Unified Vision-Language-Action and World Model",
            "submittedOnDailyBy": {
                "_id": "65fd82762bf2cd20ddaa193f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
                "isPro": false,
                "fullname": "Siteng Huang",
                "user": "huangsiteng",
                "type": "user"
            },
            "summary": "We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%.",
            "upvotes": 14,
            "discussionId": "6923c792b5612535ed955936",
            "githubRepo": "https://github.com/alibaba-damo-academy/RynnVLA-002",
            "ai_summary": "A unified Vision-Language-Action (VLA) and world model, RynnVLA-002, jointly learns environmental dynamics and action planning, outperforming individual models in both simulation and real-world tasks.",
            "ai_keywords": [
                "Vision-Language-Action (VLA)",
                "world model",
                "image states",
                "underlying physics",
                "action generation",
                "visual understanding",
                "environmental dynamics",
                "action planning",
                "LIBERO simulation benchmark",
                "LeRobot experiments"
            ],
            "githubStars": 570,
            "organization": {
                "_id": "6808e7522a4d69d5111da55f",
                "name": "Alibaba-DAMO-Academy",
                "fullname": "DAMO Academy",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
            }
        },
        "publishedAt": "2025-11-21T13:59:32.000Z",
        "title": "RynnVLA-002: A Unified Vision-Language-Action and World Model",
        "summary": "We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17502.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65fd82762bf2cd20ddaa193f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
            "fullname": "Siteng Huang",
            "name": "huangsiteng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "organization": {
            "_id": "6808e7522a4d69d5111da55f",
            "name": "Alibaba-DAMO-Academy",
            "fullname": "DAMO Academy",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.17220",
            "authors": [
                {
                    "_id": "6923df39b5612535ed9559d9",
                    "user": {
                        "_id": "6899a2c00b1627a0954e728e",
                        "avatarUrl": "/avatars/9324fd3da80a878e08fe54b209efaeba.svg",
                        "isPro": false,
                        "fullname": "Yusuf Çelebi",
                        "user": "yusufcelebi",
                        "type": "user"
                    },
                    "name": "Yusuf Çelebi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-24T07:54:36.800Z",
                    "hidden": false
                },
                {
                    "_id": "6923df39b5612535ed9559da",
                    "user": {
                        "_id": "6422eab8e2029ade06eeee2c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
                        "isPro": false,
                        "fullname": "Mahmud ElHuseyni 🇵🇸",
                        "user": "MElHuseyni",
                        "type": "user"
                    },
                    "name": "Mahmoud El Hussieni",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-24T12:26:55.651Z",
                    "hidden": false
                },
                {
                    "_id": "6923df39b5612535ed9559db",
                    "user": {
                        "_id": "6464e76894327a238f56d7ff",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464e76894327a238f56d7ff/U-3iova5yEQgigmmtDBPY.png",
                        "isPro": true,
                        "fullname": "Özay Ezerceli",
                        "user": "ozayezerceli",
                        "type": "user"
                    },
                    "name": "Özay Ezerceli",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-24T07:54:34.862Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-21T13:01:28.000Z",
            "submittedOnDailyAt": "2025-11-24T02:02:07.486Z",
            "title": "Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs",
            "submittedOnDailyBy": {
                "_id": "6422eab8e2029ade06eeee2c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
                "isPro": false,
                "fullname": "Mahmud ElHuseyni 🇵🇸",
                "user": "MElHuseyni",
                "type": "user"
            },
            "summary": "This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low \"follow rates\" (leq 11%, GPT-5: 4\\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\\%, Qwen 2.5-1.5B: 94\\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of \"resistance to overfitting pressure\" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world.",
            "upvotes": 14,
            "discussionId": "6923df3ab5612535ed9559dc",
            "githubRepo": "https://github.com/YusufCelebii/PARROT",
            "ai_summary": "PARROT evaluates the robustness of large language models against social pressure and sycophancy, revealing significant variability in model behavior and confidence shifts across different domains and authority templates.",
            "ai_keywords": [
                "Persuasion and Agreement Robustness Rating of Output Truth",
                "PARROT",
                "LLMs",
                "sycophancy",
                "double-blind evaluation",
                "log-likelihood-based calibration",
                "eight-state behavioral taxonomy",
                "MMLU-style multiple-choice questions",
                "follow rates",
                "epistemic collapse",
                "resistance to overfitting pressure"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "660c060a1b2939953134fb67",
                "name": "newmindai",
                "fullname": "NewMind AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660c05be387b035b0fc56bae/QzKfgwGy3O-IhWV8lH_RC.png"
            }
        },
        "publishedAt": "2025-11-21T08:01:28.000Z",
        "title": "Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs",
        "summary": "This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low \"follow rates\" (leq 11%, GPT-5: 4\\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\\%, Qwen 2.5-1.5B: 94\\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of \"resistance to overfitting pressure\" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17220.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6422eab8e2029ade06eeee2c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
            "fullname": "Mahmud ElHuseyni 🇵🇸",
            "name": "MElHuseyni",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 21
        },
        "organization": {
            "_id": "660c060a1b2939953134fb67",
            "name": "newmindai",
            "fullname": "NewMind AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660c05be387b035b0fc56bae/QzKfgwGy3O-IhWV8lH_RC.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.17344",
            "authors": [
                {
                    "_id": "69240abab5612535ed955a34",
                    "user": {
                        "_id": "677d4c16da00f8f3698a501d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/V_QVrZRgIhTZSz9eCG0Om.png",
                        "isPro": false,
                        "fullname": "Markus",
                        "user": "Markus-Pobitzer",
                        "type": "user"
                    },
                    "name": "Markus Pobitzer",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-24T12:26:53.036Z",
                    "hidden": false
                },
                {
                    "_id": "69240abab5612535ed955a35",
                    "name": "Chang Liu",
                    "hidden": false
                },
                {
                    "_id": "69240abab5612535ed955a36",
                    "name": "Chenyi Zhuang",
                    "hidden": false
                },
                {
                    "_id": "69240abab5612535ed955a37",
                    "name": "Teng Long",
                    "hidden": false
                },
                {
                    "_id": "69240abab5612535ed955a38",
                    "name": "Bin Ren",
                    "hidden": false
                },
                {
                    "_id": "69240abab5612535ed955a39",
                    "name": "Nicu Sebe",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/677d4c16da00f8f3698a501d/Qnfpc4qnW8vPPVPELO9uY.mp4"
            ],
            "publishedAt": "2025-11-21T16:06:32.000Z",
            "submittedOnDailyAt": "2025-11-24T07:22:38.719Z",
            "title": "Loomis Painter: Reconstructing the Painting Process",
            "submittedOnDailyBy": {
                "_id": "677d4c16da00f8f3698a501d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/V_QVrZRgIhTZSz9eCG0Om.png",
                "isPro": false,
                "fullname": "Markus",
                "user": "Markus-Pobitzer",
                "type": "user"
            },
            "summary": "Step-by-step painting tutorials are vital for learning artistic techniques, but existing video resources (e.g., YouTube) lack interactivity and personalization. While recent generative models have advanced artistic image synthesis, they struggle to generalize across media and often show temporal or structural inconsistencies, hindering faithful reproduction of human creative workflows. To address this, we propose a unified framework for multi-media painting process generation with a semantics-driven style control mechanism that embeds multiple media into a diffusion models conditional space and uses cross-medium style augmentation. This enables consistent texture evolution and process transfer across styles. A reverse-painting training strategy further ensures smooth, human-aligned generation. We also build a large-scale dataset of real painting processes and evaluate cross-media consistency, temporal coherence, and final-image fidelity, achieving strong results on LPIPS, DINO, and CLIP metrics. Finally, our Perceptual Distance Profile (PDP) curve quantitatively models the creative sequence, i.e., composition, color blocking, and detail refinement, mirroring human artistic progression.",
            "upvotes": 13,
            "discussionId": "69240abab5612535ed955a3a",
            "projectPage": "https://markus-pobitzer.github.io/lplp/",
            "githubRepo": "https://github.com/Markus-Pobitzer/wlp",
            "ai_summary": "A unified framework using diffusion models with semantic control and cross-medium style augmentation generates consistent and high-fidelity multi-media painting processes, supported by a comprehensive dataset and evaluation metrics.",
            "ai_keywords": [
                "diffusion models",
                "semantics-driven style control",
                "cross-medium style augmentation",
                "texture evolution",
                "process transfer",
                "reverse-painting training strategy",
                "LPIPS",
                "DINO",
                "CLIP metrics",
                "Perceptual Distance Profile (PDP)"
            ],
            "githubStars": 20,
            "organization": {
                "_id": "647ef9ce45baf21ad707cc59",
                "name": "MHUGLab",
                "fullname": "Multimedia and Human Understanding Group",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ef81a45baf21ad707bfb1/VfcRM08QNOTXAqIUcf-WO.png"
            }
        },
        "publishedAt": "2025-11-21T11:06:32.000Z",
        "title": "Loomis Painter: Reconstructing the Painting Process",
        "summary": "Step-by-step painting tutorials are vital for learning artistic techniques, but existing video resources (e.g., YouTube) lack interactivity and personalization. While recent generative models have advanced artistic image synthesis, they struggle to generalize across media and often show temporal or structural inconsistencies, hindering faithful reproduction of human creative workflows. To address this, we propose a unified framework for multi-media painting process generation with a semantics-driven style control mechanism that embeds multiple media into a diffusion models conditional space and uses cross-medium style augmentation. This enables consistent texture evolution and process transfer across styles. A reverse-painting training strategy further ensures smooth, human-aligned generation. We also build a large-scale dataset of real painting processes and evaluate cross-media consistency, temporal coherence, and final-image fidelity, achieving strong results on LPIPS, DINO, and CLIP metrics. Finally, our Perceptual Distance Profile (PDP) curve quantitatively models the creative sequence, i.e., composition, color blocking, and detail refinement, mirroring human artistic progression.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/677d4c16da00f8f3698a501d/Qnfpc4qnW8vPPVPELO9uY.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17344.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "677d4c16da00f8f3698a501d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/V_QVrZRgIhTZSz9eCG0Om.png",
            "fullname": "Markus",
            "name": "Markus-Pobitzer",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 0
        },
        "organization": {
            "_id": "647ef9ce45baf21ad707cc59",
            "name": "MHUGLab",
            "fullname": "Multimedia and Human Understanding Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ef81a45baf21ad707bfb1/VfcRM08QNOTXAqIUcf-WO.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.16825",
            "authors": [
                {
                    "_id": "6923c56fb5612535ed9558f1",
                    "name": "Dilin Wang",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed9558f2",
                    "name": "Hyunyoung Jung",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed9558f3",
                    "name": "Tom Monnier",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed9558f4",
                    "name": "Kihyuk Sohn",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed9558f5",
                    "name": "Chuhang Zou",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed9558f6",
                    "name": "Xiaoyu Xiang",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed9558f7",
                    "name": "Yu-Ying Yeh",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed9558f8",
                    "name": "Di Liu",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed9558f9",
                    "name": "Zixuan Huang",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed9558fa",
                    "name": "Thu Nguyen-Phuoc",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed9558fb",
                    "name": "Yuchen Fan",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed9558fc",
                    "name": "Sergiu Oprea",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed9558fd",
                    "name": "Ziyan Wang",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed9558fe",
                    "name": "Roman Shapovalov",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed9558ff",
                    "name": "Nikolaos Sarafianos",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed955900",
                    "name": "Thibault Groueix",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed955901",
                    "name": "Antoine Toisoul",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed955902",
                    "name": "Prithviraj Dhar",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed955903",
                    "name": "Xiao Chu",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed955904",
                    "name": "Minghao Chen",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed955905",
                    "name": "Geon Yeong Park",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed955906",
                    "name": "Mahima Gupta",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed955907",
                    "name": "Yassir Azziz",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed955908",
                    "name": "Rakesh Ranjan",
                    "hidden": false
                },
                {
                    "_id": "6923c56fb5612535ed955909",
                    "name": "Andrea Vedaldi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T22:13:18.000Z",
            "submittedOnDailyAt": "2025-11-24T00:09:56.668Z",
            "title": "WorldGen: From Text to Traversable and Interactive 3D Worlds",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce WorldGen, a system that enables the automatic creation of large-scale, interactive 3D worlds directly from text prompts. Our approach transforms natural language descriptions into traversable, fully textured environments that can be immediately explored or edited within standard game engines. By combining LLM-driven scene layout reasoning, procedural generation, diffusion-based 3D generation, and object-aware scene decomposition, WorldGen bridges the gap between creative intent and functional virtual spaces, allowing creators to design coherent, navigable worlds without manual modeling or specialized 3D expertise. The system is fully modular and supports fine-grained control over layout, scale, and style, producing worlds that are geometrically consistent, visually rich, and efficient to render in real time. This work represents a step towards accessible, generative world-building at scale, advancing the frontier of 3D generative AI for applications in gaming, simulation, and immersive social environments.",
            "upvotes": 12,
            "discussionId": "6923c56fb5612535ed95590a",
            "projectPage": "https://www.meta.com/blog/worldgen-3d-world-generation-reality-labs-generative-ai-research/",
            "ai_summary": "WorldGen converts text prompts into interactive 3D environments using LLM-driven reasoning, procedural generation, diffusion-based 3D generation, and object-aware decomposition, enabling creators to build coherent, navigable worlds efficiently.",
            "ai_keywords": [
                "LLM-driven scene layout reasoning",
                "procedural generation",
                "diffusion-based 3D generation",
                "object-aware scene decomposition",
                "3D generative AI"
            ],
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "publishedAt": "2025-11-20T17:13:18.000Z",
        "title": "WorldGen: From Text to Traversable and Interactive 3D Worlds",
        "summary": "We introduce WorldGen, a system that enables the automatic creation of large-scale, interactive 3D worlds directly from text prompts. Our approach transforms natural language descriptions into traversable, fully textured environments that can be immediately explored or edited within standard game engines. By combining LLM-driven scene layout reasoning, procedural generation, diffusion-based 3D generation, and object-aware scene decomposition, WorldGen bridges the gap between creative intent and functional virtual spaces, allowing creators to design coherent, navigable worlds without manual modeling or specialized 3D expertise. The system is fully modular and supports fine-grained control over layout, scale, and style, producing worlds that are geometrically consistent, visually rich, and efficient to render in real time. This work represents a step towards accessible, generative world-building at scale, advancing the frontier of 3D generative AI for applications in gaming, simulation, and immersive social environments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16825.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 170
        },
        "organization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.11007",
            "authors": [
                {
                    "_id": "6923e4d1b5612535ed9559de",
                    "name": "Xinlei Yu",
                    "hidden": false
                },
                {
                    "_id": "6923e4d1b5612535ed9559df",
                    "name": "Chengming Xu",
                    "hidden": false
                },
                {
                    "_id": "6923e4d1b5612535ed9559e0",
                    "name": "Guibin Zhang",
                    "hidden": false
                },
                {
                    "_id": "6923e4d1b5612535ed9559e1",
                    "name": "Zhangquan Chen",
                    "hidden": false
                },
                {
                    "_id": "6923e4d1b5612535ed9559e2",
                    "name": "Yudong Zhang",
                    "hidden": false
                },
                {
                    "_id": "6923e4d1b5612535ed9559e3",
                    "name": "Yongbo He",
                    "hidden": false
                },
                {
                    "_id": "6923e4d1b5612535ed9559e4",
                    "name": "Peng-Tao Jiang",
                    "hidden": false
                },
                {
                    "_id": "6923e4d1b5612535ed9559e5",
                    "name": "Jiangning Zhang",
                    "hidden": false
                },
                {
                    "_id": "6923e4d1b5612535ed9559e6",
                    "name": "Xiaobin Hu",
                    "hidden": false
                },
                {
                    "_id": "6923e4d1b5612535ed9559e7",
                    "name": "Shuicheng Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-14T06:51:34.000Z",
            "submittedOnDailyAt": "2025-11-24T02:24:46.250Z",
            "title": "VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "67d63e228d5c7a132cbcf39b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ynwA3Sya5irwMRCmSeLiC.png",
                "isPro": false,
                "fullname": "neil yu",
                "user": "yxl66666",
                "type": "user"
            },
            "summary": "Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a \"visual processing bottleneck\": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.8% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.",
            "upvotes": 10,
            "discussionId": "6923e4d1b5612535ed9559e8",
            "githubRepo": "https://github.com/YU-deep/VisMem.git",
            "ai_summary": "VisMem enhances Vision-Language Models by incorporating dynamic latent vision memories, improving performance on complex visual tasks through perceptual fidelity and semantic consistency.",
            "ai_keywords": [
                "Visual-Language Models",
                "visual processing bottleneck",
                "short-term visually-dominant memory",
                "long-term semantically-dominant memory",
                "dynamic latent vision memories",
                "perceptual retention",
                "semantic consolidation",
                "visual understanding",
                "visual reasoning",
                "visual generation",
                "latent-space memory enhancement"
            ],
            "githubStars": 18
        },
        "publishedAt": "2025-11-14T01:51:34.000Z",
        "title": "VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models",
        "summary": "Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a \"visual processing bottleneck\": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.8% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11007.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67d63e228d5c7a132cbcf39b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ynwA3Sya5irwMRCmSeLiC.png",
            "fullname": "neil yu",
            "name": "yxl66666",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.16175",
            "authors": [
                {
                    "_id": "6923d532b5612535ed9559b9",
                    "name": "Yi Yang",
                    "hidden": false
                },
                {
                    "_id": "6923d532b5612535ed9559ba",
                    "name": "Xueqi Li",
                    "hidden": false
                },
                {
                    "_id": "6923d532b5612535ed9559bb",
                    "name": "Yiyang Chen",
                    "hidden": false
                },
                {
                    "_id": "6923d532b5612535ed9559bc",
                    "name": "Jin Song",
                    "hidden": false
                },
                {
                    "_id": "6923d532b5612535ed9559bd",
                    "name": "Yihan Wang",
                    "hidden": false
                },
                {
                    "_id": "6923d532b5612535ed9559be",
                    "name": "Zipeng Xiao",
                    "hidden": false
                },
                {
                    "_id": "6923d532b5612535ed9559bf",
                    "name": "Jiadi Su",
                    "hidden": false
                },
                {
                    "_id": "6923d532b5612535ed9559c0",
                    "name": "You Qiaoben",
                    "hidden": false
                },
                {
                    "_id": "6923d532b5612535ed9559c1",
                    "name": "Pengfei Liu",
                    "hidden": false
                },
                {
                    "_id": "6923d532b5612535ed9559c2",
                    "name": "Zhijie Deng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T09:30:23.000Z",
            "submittedOnDailyAt": "2025-11-24T01:22:25.464Z",
            "title": "Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight",
            "submittedOnDailyBy": {
                "_id": "6707eaaf5f50b17754ff9cbc",
                "avatarUrl": "/avatars/f08ca6228b124a8955787d0662a52bbd.svg",
                "isPro": false,
                "fullname": "Yi Yang (SII)",
                "user": "Yysrc",
                "type": "user"
            },
            "summary": "Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms π_{0.5}, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.",
            "upvotes": 9,
            "discussionId": "6923d532b5612535ed9559c3",
            "githubRepo": "https://github.com/zhijie-group/Mantis",
            "ai_summary": "Mantis, a VLA framework with Disentangled Visual Foresight and a diffusion Transformer, improves action prediction, comprehension, and reasoning while reducing training complexity.",
            "ai_keywords": [
                "Disentangled Visual Foresight",
                "diffusion Transformer",
                "meta queries",
                "residual connection",
                "latent actions",
                "LIBERO benchmark",
                "instruction-following capability",
                "reasoning ability"
            ],
            "githubStars": 17,
            "organization": {
                "_id": "673d5fe8d031224e947dc235",
                "name": "SJTU-Deng-Lab",
                "fullname": "DENG Lab @ SJTU",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64bba541da140e461924dfed/_WPqM9jCqIIkS73aTeZP-.png"
            }
        },
        "publishedAt": "2025-11-20T04:30:23.000Z",
        "title": "Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight",
        "summary": "Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms π_{0.5}, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16175.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6707eaaf5f50b17754ff9cbc",
            "avatarUrl": "/avatars/f08ca6228b124a8955787d0662a52bbd.svg",
            "fullname": "Yi Yang (SII)",
            "name": "Yysrc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 0
        },
        "organization": {
            "_id": "673d5fe8d031224e947dc235",
            "name": "SJTU-Deng-Lab",
            "fullname": "DENG Lab @ SJTU",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64bba541da140e461924dfed/_WPqM9jCqIIkS73aTeZP-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.14899",
            "authors": [
                {
                    "_id": "69217a26b5612535ed955689",
                    "user": {
                        "_id": "672b303918685a38713dd8f1",
                        "avatarUrl": "/avatars/800802127af17be6370bf7f65b4a0cc9.svg",
                        "isPro": false,
                        "fullname": "Daniel Gilo",
                        "user": "danielgilo",
                        "type": "user"
                    },
                    "name": "Daniel Gilo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-24T07:57:59.618Z",
                    "hidden": false
                },
                {
                    "_id": "69217a26b5612535ed95568a",
                    "name": "Or Litany",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-18T20:37:52.000Z",
            "submittedOnDailyAt": "2025-11-24T10:04:20.120Z",
            "title": "InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization",
            "submittedOnDailyBy": {
                "_id": "672b303918685a38713dd8f1",
                "avatarUrl": "/avatars/800802127af17be6370bf7f65b4a0cc9.svg",
                "isPro": false,
                "fullname": "Daniel Gilo",
                "user": "danielgilo",
                "type": "user"
            },
            "summary": "We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.",
            "upvotes": 8,
            "discussionId": "69217a26b5612535ed95568b",
            "projectPage": "https://danielgilo.github.io/instruct-mix2mix/",
            "githubRepo": "https://github.com/DanielGilo/instruct-mix2mix/tree/main",
            "ai_summary": "A framework called InstructMix2Mix uses a 2D diffusion model to improve multi-view image editing by leveraging a 3D prior for cross-view consistency.",
            "ai_keywords": [
                "multi-view image editing",
                "textual instruction",
                "neural fields",
                "temporal attention mechanisms",
                "diffusion model",
                "2D diffusion model",
                "multi-view diffusion model",
                "3D prior",
                "Score Distillation Sampling",
                "SDS",
                "student updates",
                "teacher noise scheduler",
                "attention modification",
                "cross-view coherence"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-11-18T15:37:52.000Z",
        "title": "InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization",
        "summary": "We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.14899.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "672b303918685a38713dd8f1",
            "avatarUrl": "/avatars/800802127af17be6370bf7f65b4a0cc9.svg",
            "fullname": "Daniel Gilo",
            "name": "danielgilo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.14806",
            "authors": [
                {
                    "_id": "6920670c8c38b39d6a482e4e",
                    "user": {
                        "_id": "640f7083208821a59b74c757",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678735253848-640f7083208821a59b74c757.jpeg",
                        "isPro": false,
                        "fullname": "Siyuan Li",
                        "user": "Lupin1998",
                        "type": "user"
                    },
                    "name": "Siyuan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-24T12:26:57.695Z",
                    "hidden": false
                },
                {
                    "_id": "6920670c8c38b39d6a482e4f",
                    "name": "Kai Yu",
                    "hidden": false
                },
                {
                    "_id": "6920670c8c38b39d6a482e50",
                    "name": "Anna Wang",
                    "hidden": false
                },
                {
                    "_id": "6920670c8c38b39d6a482e51",
                    "name": "Zicheng Liu",
                    "hidden": false
                },
                {
                    "_id": "6920670c8c38b39d6a482e52",
                    "name": "Chang Yu",
                    "hidden": false
                },
                {
                    "_id": "6920670c8c38b39d6a482e53",
                    "name": "Jingbo Zhou",
                    "hidden": false
                },
                {
                    "_id": "6920670c8c38b39d6a482e54",
                    "name": "Qirong Yang",
                    "hidden": false
                },
                {
                    "_id": "6920670c8c38b39d6a482e55",
                    "name": "Yucheng Guo",
                    "hidden": false
                },
                {
                    "_id": "6920670c8c38b39d6a482e56",
                    "name": "Xiaoming Zhang",
                    "hidden": false
                },
                {
                    "_id": "6920670c8c38b39d6a482e57",
                    "name": "Stan Z. Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-17T19:27:41.000Z",
            "submittedOnDailyAt": "2025-11-24T07:54:13.334Z",
            "title": "MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging",
            "submittedOnDailyBy": {
                "_id": "640f7083208821a59b74c757",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678735253848-640f7083208821a59b74c757.jpeg",
                "isPro": false,
                "fullname": "Siyuan Li",
                "user": "Lupin1998",
                "type": "user"
            },
            "summary": "Modeling genomic sequences faces two unsolved challenges: the information density varies widely across different regions, while there is no clearly defined minimum vocabulary unit. Relying on either four primitive bases or independently designed DNA tokenizers, existing approaches with naive masked language modeling pre-training often fail to adapt to the varying complexities of genomic sequences. Leveraging Token Merging techniques, this paper introduces a hierarchical architecture that jointly optimizes a dynamic genomic tokenizer and latent Transformers with context-aware pre-training tasks. As for network structures, the tokenization module automatically chunks adjacent bases into words by stacking multiple layers of the differentiable token merging blocks with local-window constraints, then a Latent Encoder captures the global context of these merged words by full-attention blocks. Symmetrically employing a Latent Decoder and a Local Decoder, MergeDNA learns with two pre-training tasks: Merged Token Reconstruction simultaneously trains the dynamic tokenization module and adaptively filters important tokens, while Adaptive Masked Token Modeling learns to predict these filtered tokens to capture informative contents. Extensive experiments show that MergeDNA achieves superior performance on three popular DNA benchmarks and several multi-omics tasks with fine-tuning or zero-shot evaluation, outperforming typical tokenization methods and large-scale DNA foundation models.",
            "upvotes": 7,
            "discussionId": "6920670c8c38b39d6a482e58",
            "ai_summary": "MergeDNA uses a hierarchical architecture with Token Merging and latent Transformers to model genomic sequences, achieving superior performance on DNA benchmarks and multi-omics tasks.",
            "ai_keywords": [
                "Token Merging",
                "dynamic genomic tokenizer",
                "latent Transformers",
                "context-aware pre-training",
                "differentiable token merging blocks",
                "full-attention blocks",
                "Latent Encoder",
                "Latent Decoder",
                "Local Decoder",
                "Merged Token Reconstruction",
                "Adaptive Masked Token Modeling",
                "DNA benchmarks",
                "multi-omics tasks"
            ],
            "organization": {
                "_id": "643cb10025681c3afab0f1a6",
                "name": "Westlake-University",
                "fullname": "Westlake University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/SQRCHUyjPRyqdtV3um42X.jpeg"
            }
        },
        "publishedAt": "2025-11-17T14:27:41.000Z",
        "title": "MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging",
        "summary": "Modeling genomic sequences faces two unsolved challenges: the information density varies widely across different regions, while there is no clearly defined minimum vocabulary unit. Relying on either four primitive bases or independently designed DNA tokenizers, existing approaches with naive masked language modeling pre-training often fail to adapt to the varying complexities of genomic sequences. Leveraging Token Merging techniques, this paper introduces a hierarchical architecture that jointly optimizes a dynamic genomic tokenizer and latent Transformers with context-aware pre-training tasks. As for network structures, the tokenization module automatically chunks adjacent bases into words by stacking multiple layers of the differentiable token merging blocks with local-window constraints, then a Latent Encoder captures the global context of these merged words by full-attention blocks. Symmetrically employing a Latent Decoder and a Local Decoder, MergeDNA learns with two pre-training tasks: Merged Token Reconstruction simultaneously trains the dynamic tokenization module and adaptively filters important tokens, while Adaptive Masked Token Modeling learns to predict these filtered tokens to capture informative contents. Extensive experiments show that MergeDNA achieves superior performance on three popular DNA benchmarks and several multi-omics tasks with fine-tuning or zero-shot evaluation, outperforming typical tokenization methods and large-scale DNA foundation models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.14806.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640f7083208821a59b74c757",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678735253848-640f7083208821a59b74c757.jpeg",
            "fullname": "Siyuan Li",
            "name": "Lupin1998",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "643cb10025681c3afab0f1a6",
            "name": "Westlake-University",
            "fullname": "Westlake University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/SQRCHUyjPRyqdtV3um42X.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.16931",
            "authors": [
                {
                    "_id": "6923c453b5612535ed9558a5",
                    "name": "Chenyang Shao",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558a6",
                    "name": "Dehao Huang",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558a7",
                    "name": "Yu Li",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558a8",
                    "name": "Keyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558a9",
                    "name": "Weiquan Lin",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558aa",
                    "name": "Yining Zhang",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558ab",
                    "name": "Qingbin Zeng",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558ac",
                    "name": "Zhiyu Chen",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558ad",
                    "name": "Tianxing Li",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558ae",
                    "name": "Yifei Huang",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558af",
                    "name": "Taozhong Wu",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558b0",
                    "name": "Xinyang Liu",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558b1",
                    "name": "Ruotong Zhao",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558b2",
                    "name": "Mengsheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558b3",
                    "name": "Xuhua Zhang",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558b4",
                    "name": "Yue Wang",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558b5",
                    "name": "Yuanyi Zhen",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558b6",
                    "name": "Fengli Xu",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558b7",
                    "name": "Yong Li",
                    "hidden": false
                },
                {
                    "_id": "6923c453b5612535ed9558b8",
                    "name": "Tie-Yan Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-21T03:55:19.000Z",
            "submittedOnDailyAt": "2025-11-24T00:06:27.655Z",
            "title": "OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "With the rapid development of Large Language Models (LLMs), AI agents have demonstrated increasing proficiency in scientific tasks, ranging from hypothesis generation and experimental design to manuscript writing. Such agent systems are commonly referred to as \"AI Scientists.\" However, existing AI Scientists predominantly formulate scientific discovery as a standalone search or optimization problem, overlooking the fact that scientific research is inherently a social and collaborative endeavor. Real-world science relies on a complex scientific infrastructure composed of collaborative mechanisms, contribution attribution, peer review, and structured scientific knowledge networks. Due to the lack of modeling for these critical dimensions, current systems struggle to establish a genuine research ecosystem or interact deeply with the human scientific community. To bridge this gap, we introduce OmniScientist, a framework that explicitly encodes the underlying mechanisms of human research into the AI scientific workflow. OmniScientist not only achieves end-to-end automation across data foundation, literature review, research ideation, experiment automation, scientific writing, and peer review, but also provides comprehensive infrastructural support by simulating the human scientific system, comprising: (1) a structured knowledge system built upon citation networks and conceptual correlations; (2) a collaborative research protocol (OSP), which enables seamless multi-agent collaboration and human researcher participation; and (3) an open evaluation platform (ScienceArena) based on blind pairwise user voting and Elo rankings. This infrastructure empowers agents to not only comprehend and leverage human knowledge systems but also to collaborate and co-evolve, fostering a sustainable and scalable innovation ecosystem.",
            "upvotes": 6,
            "discussionId": "6923c454b5612535ed9558b9",
            "projectPage": "https://omniscientist.ai/chat",
            "ai_summary": "OmniScientist is a framework that simulates human scientific processes to automate and enhance AI-driven scientific research through structured knowledge systems, collaborative protocols, and evaluation platforms.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "AI Scientists",
                "scientific discovery",
                "optimization problem",
                "collaborative mechanisms",
                "contribution attribution",
                "peer review",
                "structured scientific knowledge networks",
                "OmniScientist",
                "data foundation",
                "literature review",
                "research ideation",
                "experiment automation",
                "scientific writing",
                "structured knowledge system",
                "citation networks",
                "conceptual correlations",
                "collaborative research protocol",
                "OSP",
                "open evaluation platform",
                "ScienceArena",
                "blind pairwise user voting",
                "Elo rankings"
            ]
        },
        "publishedAt": "2025-11-20T22:55:19.000Z",
        "title": "OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists",
        "summary": "With the rapid development of Large Language Models (LLMs), AI agents have demonstrated increasing proficiency in scientific tasks, ranging from hypothesis generation and experimental design to manuscript writing. Such agent systems are commonly referred to as \"AI Scientists.\" However, existing AI Scientists predominantly formulate scientific discovery as a standalone search or optimization problem, overlooking the fact that scientific research is inherently a social and collaborative endeavor. Real-world science relies on a complex scientific infrastructure composed of collaborative mechanisms, contribution attribution, peer review, and structured scientific knowledge networks. Due to the lack of modeling for these critical dimensions, current systems struggle to establish a genuine research ecosystem or interact deeply with the human scientific community. To bridge this gap, we introduce OmniScientist, a framework that explicitly encodes the underlying mechanisms of human research into the AI scientific workflow. OmniScientist not only achieves end-to-end automation across data foundation, literature review, research ideation, experiment automation, scientific writing, and peer review, but also provides comprehensive infrastructural support by simulating the human scientific system, comprising: (1) a structured knowledge system built upon citation networks and conceptual correlations; (2) a collaborative research protocol (OSP), which enables seamless multi-agent collaboration and human researcher participation; and (3) an open evaluation platform (ScienceArena) based on blind pairwise user voting and Elo rankings. This infrastructure empowers agents to not only comprehend and leverage human knowledge systems but also to collaborate and co-evolve, fostering a sustainable and scalable innovation ecosystem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16931.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 170
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.17487",
            "authors": [
                {
                    "_id": "69240fbcb5612535ed955a5e",
                    "user": {
                        "_id": "65c4287b6b3715a9cf28ded9",
                        "avatarUrl": "/avatars/2af7d2c64665cb5283398084628c1701.svg",
                        "isPro": false,
                        "fullname": "Mark Endo",
                        "user": "markendo",
                        "type": "user"
                    },
                    "name": "Mark Endo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-24T12:26:50.910Z",
                    "hidden": false
                },
                {
                    "_id": "69240fbcb5612535ed955a5f",
                    "name": "Serena Yeung-Levy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-21T18:43:01.000Z",
            "submittedOnDailyAt": "2025-11-24T05:33:07.327Z",
            "title": "Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models",
            "submittedOnDailyBy": {
                "_id": "65c4287b6b3715a9cf28ded9",
                "avatarUrl": "/avatars/2af7d2c64665cb5283398084628c1701.svg",
                "isPro": false,
                "fullname": "Mark Endo",
                "user": "markendo",
                "type": "user"
            },
            "summary": "Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.",
            "upvotes": 5,
            "discussionId": "69240fbcb5612535ed955a60",
            "projectPage": "https://web.stanford.edu/~markendo/projects/downscaling_intelligence",
            "githubRepo": "https://github.com/markendo/downscaling_intelligence",
            "ai_summary": "Reducing the capacity of large language models disproportionately impacts visual capabilities in multimodal systems, but visual extraction tuning combined with step-by-step reasoning improves efficiency and performance.",
            "ai_keywords": [
                "multimodal models",
                "large language model (LLM)",
                "downscaling",
                "visual capabilities",
                "visual extraction tuning",
                "Extract+Think approach"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-11-21T13:43:01.000Z",
        "title": "Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models",
        "summary": "Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17487.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65c4287b6b3715a9cf28ded9",
            "avatarUrl": "/avatars/2af7d2c64665cb5283398084628c1701.svg",
            "fullname": "Mark Endo",
            "name": "markendo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.17490",
            "authors": [
                {
                    "_id": "6923c52fb5612535ed9558c0",
                    "name": "Yolo Yunlong Tang",
                    "hidden": false
                },
                {
                    "_id": "6923c52fb5612535ed9558c1",
                    "name": "Daiki Shimada",
                    "hidden": false
                },
                {
                    "_id": "6923c52fb5612535ed9558c2",
                    "name": "Hang Hua",
                    "hidden": false
                },
                {
                    "_id": "6923c52fb5612535ed9558c3",
                    "name": "Chao Huang",
                    "hidden": false
                },
                {
                    "_id": "6923c52fb5612535ed9558c4",
                    "name": "Jing Bi",
                    "hidden": false
                },
                {
                    "_id": "6923c52fb5612535ed9558c5",
                    "name": "Rogerio Feris",
                    "hidden": false
                },
                {
                    "_id": "6923c52fb5612535ed9558c6",
                    "name": "Chenliang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-21T18:47:09.000Z",
            "submittedOnDailyAt": "2025-11-24T00:09:06.782Z",
            "title": "Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination",
            "submittedOnDailyBy": {
                "_id": "6344c87f0f69ad8aa61dfcf6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344c87f0f69ad8aa61dfcf6/tTVHu2l2aiAnK160vgT6u.jpeg",
                "isPro": false,
                "fullname": "Yolo Y. Tang",
                "user": "yunlong10",
                "type": "user"
            },
            "summary": "Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.",
            "upvotes": 4,
            "discussionId": "6923c52fb5612535ed9558c7",
            "projectPage": "https://yunlong10.github.io/Video-R4/",
            "githubRepo": "https://github.com/yunlong10/Video-R4",
            "ai_summary": "Video-R4, a video reasoning LMM, uses iterative visual rumination to improve text-rich video QA by selecting, zooming, and re-encoding frames, achieving state-of-the-art results on various QA tasks.",
            "ai_keywords": [
                "video QA models",
                "visual rumination",
                "Video-R4",
                "LMM",
                "frames",
                "re-encoding",
                "reasoning state",
                "Video-R4-CoT-17k",
                "Video-R4-RL-30k",
                "multi-stage rumination learning",
                "SFT",
                "GRPO-based RL",
                "M4-ViteVQA",
                "multi-page document QA",
                "slides QA",
                "generic video QA"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-11-21T13:47:09.000Z",
        "title": "Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination",
        "summary": "Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17490.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6344c87f0f69ad8aa61dfcf6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344c87f0f69ad8aa61dfcf6/tTVHu2l2aiAnK160vgT6u.jpeg",
            "fullname": "Yolo Y. Tang",
            "name": "yunlong10",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.17074",
            "authors": [
                {
                    "_id": "6923f826b5612535ed955a06",
                    "user": {
                        "_id": "66d16a015ab9ab8cb4ee4f94",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d16a015ab9ab8cb4ee4f94/6ISRsgVL9u3qJ96c0qDDx.jpeg",
                        "isPro": false,
                        "fullname": "TongWang",
                        "user": "TongWang-NJ",
                        "type": "user"
                    },
                    "name": "Tong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-24T07:54:29.468Z",
                    "hidden": false
                },
                {
                    "_id": "6923f826b5612535ed955a07",
                    "name": "Guanyu Yang",
                    "hidden": false
                },
                {
                    "_id": "6923f826b5612535ed955a08",
                    "name": "Nian Liu",
                    "hidden": false
                },
                {
                    "_id": "6923f826b5612535ed955a09",
                    "name": "Kai Wang",
                    "hidden": false
                },
                {
                    "_id": "6923f826b5612535ed955a0a",
                    "name": "Yaxing Wang",
                    "hidden": false
                },
                {
                    "_id": "6923f826b5612535ed955a0b",
                    "name": "Abdelrahman M Shaker",
                    "hidden": false
                },
                {
                    "_id": "6923f826b5612535ed955a0c",
                    "name": "Salman Khan",
                    "hidden": false
                },
                {
                    "_id": "6923f826b5612535ed955a0d",
                    "name": "Fahad Shahbaz Khan",
                    "hidden": false
                },
                {
                    "_id": "6923f826b5612535ed955a0e",
                    "name": "Senmao Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-21T09:24:09.000Z",
            "submittedOnDailyAt": "2025-11-24T05:44:39.380Z",
            "title": "Diversity Has Always Been There in Your Visual Autoregressive Models",
            "submittedOnDailyBy": {
                "_id": "637e1cf4f09bf2498c543a73",
                "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
                "isPro": false,
                "fullname": "Senmao Li",
                "user": "senmaonk",
                "type": "user"
            },
            "summary": "Visual Autoregressive (VAR) models have recently garnered significant attention for their innovative next-scale prediction paradigm, offering notable advantages in both inference efficiency and image quality compared to traditional multi-step autoregressive (AR) and diffusion models. However, despite their efficiency, VAR models often suffer from the diversity collapse i.e., a reduction in output variability, analogous to that observed in few-step distilled diffusion models. In this paper, we introduce DiverseVAR, a simple yet effective approach that restores the generative diversity of VAR models without requiring any additional training. Our analysis reveals the pivotal component of the feature map as a key factor governing diversity formation at early scales. By suppressing the pivotal component in the model input and amplifying it in the model output, DiverseVAR effectively unlocks the inherent generative potential of VAR models while preserving high-fidelity synthesis. Empirical results demonstrate that our approach substantially enhances generative diversity with only neglectable performance influences. Our code will be publicly released at https://github.com/wangtong627/DiverseVAR.",
            "upvotes": 4,
            "discussionId": "6923f826b5612535ed955a0f",
            "githubRepo": "https://github.com/wangtong627/DiverseVAR",
            "ai_summary": "DiverseVAR enhances generative diversity in Visual Autoregressive models by modifying the pivotal component of feature maps without additional training, improving synthesis quality.",
            "ai_keywords": [
                "Visual Autoregressive (VAR) models",
                "next-scale prediction",
                "inference efficiency",
                "image quality",
                "multi-step autoregressive (AR) models",
                "diffusion models",
                "diversity collapse",
                "DiverseVAR",
                "feature map",
                "generative diversity",
                "high-fidelity synthesis"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-11-21T04:24:09.000Z",
        "title": "Diversity Has Always Been There in Your Visual Autoregressive Models",
        "summary": "Visual Autoregressive (VAR) models have recently garnered significant attention for their innovative next-scale prediction paradigm, offering notable advantages in both inference efficiency and image quality compared to traditional multi-step autoregressive (AR) and diffusion models. However, despite their efficiency, VAR models often suffer from the diversity collapse i.e., a reduction in output variability, analogous to that observed in few-step distilled diffusion models. In this paper, we introduce DiverseVAR, a simple yet effective approach that restores the generative diversity of VAR models without requiring any additional training. Our analysis reveals the pivotal component of the feature map as a key factor governing diversity formation at early scales. By suppressing the pivotal component in the model input and amplifying it in the model output, DiverseVAR effectively unlocks the inherent generative potential of VAR models while preserving high-fidelity synthesis. Empirical results demonstrate that our approach substantially enhances generative diversity with only neglectable performance influences. Our code will be publicly released at https://github.com/wangtong627/DiverseVAR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17074.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637e1cf4f09bf2498c543a73",
            "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
            "fullname": "Senmao Li",
            "name": "senmaonk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.15462",
            "authors": [
                {
                    "_id": "69242f90b5612535ed955a8e",
                    "user": {
                        "_id": "61bf84c8ca59d6d196a1b4e8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61bf84c8ca59d6d196a1b4e8/L_NvUwlMYcye9X35z6f7e.jpeg",
                        "isPro": false,
                        "fullname": "Amir Hossein Kargaran",
                        "user": "kargaranamir",
                        "type": "user"
                    },
                    "name": "Amir Hossein Kargaran",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-24T12:26:48.490Z",
                    "hidden": false
                },
                {
                    "_id": "69242f90b5612535ed955a8f",
                    "name": "Nafiseh Nikeghbal",
                    "hidden": false
                },
                {
                    "_id": "69242f90b5612535ed955a90",
                    "name": "Jing Yang",
                    "hidden": false
                },
                {
                    "_id": "69242f90b5612535ed955a91",
                    "name": "Nedjma Ousidhoum",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/61bf84c8ca59d6d196a1b4e8/Yf0nkDqmEC4JPVrskxysK.png"
            ],
            "publishedAt": "2025-11-19T14:21:52.000Z",
            "submittedOnDailyAt": "2025-11-24T07:44:47.211Z",
            "title": "Insights from the ICLR Peer Review and Rebuttal Process",
            "submittedOnDailyBy": {
                "_id": "61bf84c8ca59d6d196a1b4e8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61bf84c8ca59d6d196a1b4e8/L_NvUwlMYcye9X35z6f7e.jpeg",
                "isPro": false,
                "fullname": "Amir Hossein Kargaran",
                "user": "kargaranamir",
                "type": "user"
            },
            "summary": "Peer review is a cornerstone of scientific publishing, including at premier machine learning conferences such as ICLR. As submission volumes increase, understanding the nature and dynamics of the review process is crucial for improving its efficiency, effectiveness, and the quality of published papers. We present a large-scale analysis of the ICLR 2024 and 2025 peer review processes, focusing on before- and after-rebuttal scores and reviewer-author interactions. We examine review scores, author-reviewer engagement, temporal patterns in review submissions, and co-reviewer influence effects. Combining quantitative analyses with LLM-based categorization of review texts and rebuttal discussions, we identify common strengths and weaknesses for each rating group, as well as trends in rebuttal strategies that are most strongly associated with score changes. Our findings show that initial scores and the ratings of co-reviewers are the strongest predictors of score changes during the rebuttal, pointing to a degree of reviewer influence. Rebuttals play a valuable role in improving outcomes for borderline papers, where thoughtful author responses can meaningfully shift reviewer perspectives. More broadly, our study offers evidence-based insights to improve the peer review process, guiding authors on effective rebuttal strategies and helping the community design fairer and more efficient review processes. Our code and score changes data are available at https://github.com/papercopilot/iclr-insights.",
            "upvotes": 3,
            "discussionId": "69242f90b5612535ed955a92",
            "projectPage": "https://github.com/papercopilot/iclr-insights.",
            "ai_summary": "The study analyzes the ICLR 2024 and 2025 peer review processes, focusing on score dynamics and reviewer interactions, using LLM-based text categorization to identify trends and factors influencing score changes.",
            "ai_keywords": [
                "peer review",
                "ICLR",
                "review scores",
                "author-reviewer engagement",
                "temporal patterns",
                "co-reviewer influence",
                "LLM-based categorization",
                "rebuttal strategies",
                "score changes",
                "reviewer influence"
            ]
        },
        "publishedAt": "2025-11-19T09:21:52.000Z",
        "title": "Insights from the ICLR Peer Review and Rebuttal Process",
        "summary": "Peer review is a cornerstone of scientific publishing, including at premier machine learning conferences such as ICLR. As submission volumes increase, understanding the nature and dynamics of the review process is crucial for improving its efficiency, effectiveness, and the quality of published papers. We present a large-scale analysis of the ICLR 2024 and 2025 peer review processes, focusing on before- and after-rebuttal scores and reviewer-author interactions. We examine review scores, author-reviewer engagement, temporal patterns in review submissions, and co-reviewer influence effects. Combining quantitative analyses with LLM-based categorization of review texts and rebuttal discussions, we identify common strengths and weaknesses for each rating group, as well as trends in rebuttal strategies that are most strongly associated with score changes. Our findings show that initial scores and the ratings of co-reviewers are the strongest predictors of score changes during the rebuttal, pointing to a degree of reviewer influence. Rebuttals play a valuable role in improving outcomes for borderline papers, where thoughtful author responses can meaningfully shift reviewer perspectives. More broadly, our study offers evidence-based insights to improve the peer review process, guiding authors on effective rebuttal strategies and helping the community design fairer and more efficient review processes. Our code and score changes data are available at https://github.com/papercopilot/iclr-insights.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/61bf84c8ca59d6d196a1b4e8/Yf0nkDqmEC4JPVrskxysK.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15462.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61bf84c8ca59d6d196a1b4e8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61bf84c8ca59d6d196a1b4e8/L_NvUwlMYcye9X35z6f7e.jpeg",
            "fullname": "Amir Hossein Kargaran",
            "name": "kargaranamir",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 72
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.17450",
            "authors": [
                {
                    "_id": "6923c372b5612535ed95589b",
                    "name": "Yidong Huang",
                    "hidden": false
                },
                {
                    "_id": "6923c372b5612535ed95589c",
                    "name": "Zun Wang",
                    "hidden": false
                },
                {
                    "_id": "6923c372b5612535ed95589d",
                    "name": "Han Lin",
                    "hidden": false
                },
                {
                    "_id": "6923c372b5612535ed95589e",
                    "name": "Dong-Ki Kim",
                    "hidden": false
                },
                {
                    "_id": "6923c372b5612535ed95589f",
                    "name": "Shayegan Omidshafiei",
                    "hidden": false
                },
                {
                    "_id": "6923c372b5612535ed9558a0",
                    "name": "Jaehong Yoon",
                    "hidden": false
                },
                {
                    "_id": "6923c372b5612535ed9558a1",
                    "name": "Yue Zhang",
                    "hidden": false
                },
                {
                    "_id": "6923c372b5612535ed9558a2",
                    "name": "Mohit Bansal",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/CGshL3U2GYW-ogMvRF3jO.mp4"
            ],
            "publishedAt": "2025-11-21T17:48:02.000Z",
            "submittedOnDailyAt": "2025-11-24T00:02:15.529Z",
            "title": "Planning with Sketch-Guided Verification for Physics-Aware Video Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.",
            "upvotes": 1,
            "discussionId": "6923c373b5612535ed9558a3",
            "projectPage": "https://sketchverify.github.io/",
            "ai_summary": "SketchVerify enhances video motion planning and generation by iteratively refining candidate motion plans using a lightweight sketch-based verification process, improving both quality and efficiency.",
            "ai_keywords": [
                "planning intermediate control signals",
                "object trajectories",
                "temporal coherence",
                "motion fidelity",
                "single-shot plans",
                "iterative refinement",
                "training-free",
                "sketch-verification-based planning",
                "motion planning quality",
                "dynamically coherent trajectories",
                "physically plausible",
                "instruction-consistent motions",
                "test-time sampling and verification loop",
                "prompt",
                "reference image",
                "candidate motion plans",
                "vision-language verifier",
                "semantic alignment",
                "physical plausibility",
                "trajectory-conditioned generator",
                "WorldModelBench",
                "PhyWorldBench",
                "motion quality",
                "physical realism",
                "long-term consistency",
                "trajectory candidates"
            ]
        },
        "publishedAt": "2025-11-21T12:48:02.000Z",
        "title": "Planning with Sketch-Guided Verification for Physics-Aware Video Generation",
        "summary": "Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/CGshL3U2GYW-ogMvRF3jO.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17450.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 170
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.17199",
            "authors": [
                {
                    "_id": "6923c486b5612535ed9558bb",
                    "name": "Hanyu Zhou",
                    "hidden": false
                },
                {
                    "_id": "6923c486b5612535ed9558bc",
                    "name": "Chuanhao Ma",
                    "hidden": false
                },
                {
                    "_id": "6923c486b5612535ed9558bd",
                    "name": "Gim Hee Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-21T12:26:30.000Z",
            "submittedOnDailyAt": "2025-11-24T00:05:59.176Z",
            "title": "VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation.",
            "upvotes": 1,
            "discussionId": "6923c487b5612535ed9558be",
            "ai_summary": "VLA-4D enhances robotic manipulation by integrating 4D spatial-temporal awareness into visual and action representations, achieved through cross-attention and temporal extension.",
            "ai_keywords": [
                "4D-aware visual representation",
                "cross-attention mechanism",
                "spatiotemporal action representation",
                "temporal information",
                "spatiotemporal planning",
                "multimodal representations",
                "LLM",
                "spatiotemporally coherent robotic manipulation",
                "temporal action annotations"
            ]
        },
        "publishedAt": "2025-11-21T07:26:30.000Z",
        "title": "VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation",
        "summary": "Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17199.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 170
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.16110",
            "authors": [
                {
                    "_id": "6923ca57b5612535ed95593e",
                    "name": "Yijun Yang",
                    "hidden": false
                },
                {
                    "_id": "6923ca57b5612535ed95593f",
                    "name": "Lichao Wang",
                    "hidden": false
                },
                {
                    "_id": "6923ca57b5612535ed955940",
                    "name": "Jianping Zhang",
                    "hidden": false
                },
                {
                    "_id": "6923ca57b5612535ed955941",
                    "name": "Chi Harold Liu",
                    "hidden": false
                },
                {
                    "_id": "6923ca57b5612535ed955942",
                    "name": "Lanqing Hong",
                    "hidden": false
                },
                {
                    "_id": "6923ca57b5612535ed955943",
                    "name": "Qiang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T07:12:54.000Z",
            "submittedOnDailyAt": "2025-11-24T00:33:26.249Z",
            "title": "Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "6437eb0b819f3ab20d16215b",
                "avatarUrl": "/avatars/0c22e6d78fbcb5859b8724c9d3c05796.svg",
                "isPro": false,
                "fullname": "YijunYang",
                "user": "YijunYang280",
                "type": "user"
            },
            "summary": "The growing misuse of Vision-Language Models (VLMs) has led providers to deploy multiple safeguards, including alignment tuning, system prompts, and content moderation. However, the real-world robustness of these defenses against adversarial attacks remains underexplored. We introduce Multi-Faceted Attack (MFA), a framework that systematically exposes general safety vulnerabilities in leading defense-equipped VLMs such as GPT-4o, Gemini-Pro, and Llama-4. The core component of MFA is the Attention-Transfer Attack (ATA), which hides harmful instructions inside a meta task with competing objectives. We provide a theoretical perspective based on reward hacking to explain why this attack succeeds. To improve cross-model transferability, we further introduce a lightweight transfer-enhancement algorithm combined with a simple repetition strategy that jointly bypasses both input-level and output-level filters without model-specific fine-tuning. Empirically, we show that adversarial images optimized for one vision encoder transfer broadly to unseen VLMs, indicating that shared visual representations create a cross-model safety vulnerability. Overall, MFA achieves a 58.5% success rate and consistently outperforms existing methods. On state-of-the-art commercial models, MFA reaches a 52.8% success rate, surpassing the second-best attack by 34%. These results challenge the perceived robustness of current defense mechanisms and highlight persistent safety weaknesses in modern VLMs. Code: https://github.com/cure-lab/MultiFacetedAttack",
            "upvotes": 1,
            "discussionId": "6923ca58b5612535ed955944",
            "ai_summary": "Multi-Faceted Attack (MFA) framework reveals vulnerabilities in VLMs by transferring adversarial attacks across models, achieving high success rates even against state-of-the-art defenses.",
            "ai_keywords": [
                "Vision-Language Models",
                "Multi-Faceted Attack",
                "Attention-Transfer Attack",
                "reward hacking",
                "input-level filters",
                "output-level filters",
                "cross-model transferability",
                "adversarial images",
                "visual representations"
            ],
            "organization": {
                "_id": "6390c6fdd00f25601f445cd4",
                "name": "CUHK-CSE",
                "fullname": "The Chinese University of Hong Kong",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/621f2eb36e152b56a7cf0248/o8RRAczRjfNEzq70GzUwQ.png"
            }
        },
        "publishedAt": "2025-11-20T02:12:54.000Z",
        "title": "Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models",
        "summary": "The growing misuse of Vision-Language Models (VLMs) has led providers to deploy multiple safeguards, including alignment tuning, system prompts, and content moderation. However, the real-world robustness of these defenses against adversarial attacks remains underexplored. We introduce Multi-Faceted Attack (MFA), a framework that systematically exposes general safety vulnerabilities in leading defense-equipped VLMs such as GPT-4o, Gemini-Pro, and Llama-4. The core component of MFA is the Attention-Transfer Attack (ATA), which hides harmful instructions inside a meta task with competing objectives. We provide a theoretical perspective based on reward hacking to explain why this attack succeeds. To improve cross-model transferability, we further introduce a lightweight transfer-enhancement algorithm combined with a simple repetition strategy that jointly bypasses both input-level and output-level filters without model-specific fine-tuning. Empirically, we show that adversarial images optimized for one vision encoder transfer broadly to unseen VLMs, indicating that shared visual representations create a cross-model safety vulnerability. Overall, MFA achieves a 58.5% success rate and consistently outperforms existing methods. On state-of-the-art commercial models, MFA reaches a 52.8% success rate, surpassing the second-best attack by 34%. These results challenge the perceived robustness of current defense mechanisms and highlight persistent safety weaknesses in modern VLMs. Code: https://github.com/cure-lab/MultiFacetedAttack",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16110.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6437eb0b819f3ab20d16215b",
            "avatarUrl": "/avatars/0c22e6d78fbcb5859b8724c9d3c05796.svg",
            "fullname": "YijunYang",
            "name": "YijunYang280",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "6390c6fdd00f25601f445cd4",
            "name": "CUHK-CSE",
            "fullname": "The Chinese University of Hong Kong",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/621f2eb36e152b56a7cf0248/o8RRAczRjfNEzq70GzUwQ.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.15299",
            "authors": [
                {
                    "_id": "691f14e53c64d32b03645a55",
                    "user": {
                        "_id": "65d9be67be18bfea69c63830",
                        "avatarUrl": "/avatars/fe68775d214b76f8812db0d066d5be63.svg",
                        "isPro": false,
                        "fullname": "Jialong Sun",
                        "user": "Pillow-1",
                        "type": "user"
                    },
                    "name": "Jialong Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-21T09:01:23.852Z",
                    "hidden": false
                },
                {
                    "_id": "691f14e53c64d32b03645a56",
                    "name": "Hongguang Zhu",
                    "hidden": false
                },
                {
                    "_id": "691f14e53c64d32b03645a57",
                    "name": "Weizhe Liu",
                    "hidden": false
                },
                {
                    "_id": "691f14e53c64d32b03645a58",
                    "name": "Yunda Sun",
                    "hidden": false
                },
                {
                    "_id": "691f14e53c64d32b03645a59",
                    "name": "Renshuai Tao",
                    "hidden": false
                },
                {
                    "_id": "691f14e53c64d32b03645a5a",
                    "name": "Yunchao Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-19T10:07:11.000Z",
            "submittedOnDailyAt": "2025-11-24T03:35:42.863Z",
            "title": "Taming Generative Synthetic Data for X-ray Prohibited Item Detection",
            "submittedOnDailyBy": {
                "_id": "65d9be67be18bfea69c63830",
                "avatarUrl": "/avatars/fe68775d214b76f8812db0d066d5be63.svg",
                "isPro": false,
                "fullname": "Jialong Sun",
                "user": "Pillow-1",
                "type": "user"
            },
            "summary": "Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/.",
            "upvotes": 1,
            "discussionId": "691f14e53c64d32b03645a5b",
            "githubRepo": "https://github.com/pILLOW-1/Xsyn/",
            "ai_summary": "A one-stage text-to-image synthesis pipeline for X-ray security images improves efficiency and quality without extra labor cost, enhancing prohibited item detection performance.",
            "ai_keywords": [
                "text-to-image generation",
                "diffusion model",
                "cross-attention map",
                "bounding box annotation",
                "latent space",
                "background occlusion modeling",
                "mAP",
                "prohibited item detection"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-11-19T05:07:11.000Z",
        "title": "Taming Generative Synthetic Data for X-ray Prohibited Item Detection",
        "summary": "Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15299.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d9be67be18bfea69c63830",
            "avatarUrl": "/avatars/fe68775d214b76f8812db0d066d5be63.svg",
            "fullname": "Jialong Sun",
            "name": "Pillow-1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.13081",
            "authors": [
                {
                    "_id": "6923f65db5612535ed955a00",
                    "user": {
                        "_id": "662a80358a721ebd0b4f358b",
                        "avatarUrl": "/avatars/5be5a3c8b13f6f663206a19d0525c18e.svg",
                        "isPro": false,
                        "fullname": "Yehonatan Elisha",
                        "user": "Yoniel",
                        "type": "user"
                    },
                    "name": "Yehonatan Elisha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-24T07:54:32.633Z",
                    "hidden": false
                },
                {
                    "_id": "6923f65db5612535ed955a01",
                    "name": "Seffi Cohen",
                    "hidden": false
                },
                {
                    "_id": "6923f65db5612535ed955a02",
                    "name": "Oren Barkan",
                    "hidden": false
                },
                {
                    "_id": "6923f65db5612535ed955a03",
                    "name": "Noam Koenigstein",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-17T07:29:25.000Z",
            "submittedOnDailyAt": "2025-11-24T03:40:39.348Z",
            "title": "Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations",
            "submittedOnDailyBy": {
                "_id": "662a80358a721ebd0b4f358b",
                "avatarUrl": "/avatars/5be5a3c8b13f6f663206a19d0525c18e.svg",
                "isPro": false,
                "fullname": "Yehonatan Elisha",
                "user": "Yoniel",
                "type": "user"
            },
            "summary": "Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods. We address this gap by introducing the Reference-Frame times Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise (\"Why this prediction?\") and contrastive (\"Why this and not an alternative?\") explanations. Granularity: Ranging from fine-grained class-level (e.g., \"Why Husky?\") to coarse-grained group-level (e.g., \"Why Dog?\") interpretations. Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets. By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.",
            "upvotes": 1,
            "discussionId": "6923f65db5612535ed955a04",
            "ai_summary": "The Reference-Frame × Granularity (RFxG) taxonomy and novel faithfulness metrics improve the evaluation and alignment of saliency explanations with user intent in deep learning.",
            "ai_keywords": [
                "Saliency maps",
                "RFxG taxonomy",
                "Reference-Frame",
                "Granularity",
                "pointwise explanations",
                "contrastive explanations",
                "class-level interpretations",
                "group-level interpretations",
                "faithfulness metrics",
                "saliency methods",
                "model architectures",
                "datasets",
                "user-intent-driven evaluation"
            ]
        },
        "publishedAt": "2025-11-17T02:29:25.000Z",
        "title": "Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations",
        "summary": "Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods. We address this gap by introducing the Reference-Frame times Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise (\"Why this prediction?\") and contrastive (\"Why this and not an alternative?\") explanations. Granularity: Ranging from fine-grained class-level (e.g., \"Why Husky?\") to coarse-grained group-level (e.g., \"Why Dog?\") interpretations. Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets. By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13081.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "662a80358a721ebd0b4f358b",
            "avatarUrl": "/avatars/5be5a3c8b13f6f663206a19d0525c18e.svg",
            "fullname": "Yehonatan Elisha",
            "name": "Yoniel",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]