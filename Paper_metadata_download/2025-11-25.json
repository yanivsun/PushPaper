[
    {
        "paper": {
            "id": "2511.18423",
            "authors": [
                {
                    "_id": "692518ff16eb3a9f1310391c",
                    "name": "B. Y. Yan",
                    "hidden": false
                },
                {
                    "_id": "692518ff16eb3a9f1310391d",
                    "name": "Chaofan Li",
                    "hidden": false
                },
                {
                    "_id": "692518ff16eb3a9f1310391e",
                    "name": "Hongjin Qian",
                    "hidden": false
                },
                {
                    "_id": "692518ff16eb3a9f1310391f",
                    "user": {
                        "_id": "6145b3fd35135ec7e8d4ca45",
                        "avatarUrl": "/avatars/5dc25d18d6a8418c9b1a29ece9a48f5a.svg",
                        "isPro": false,
                        "fullname": "Shuqi Lu",
                        "user": "shuqi",
                        "type": "user"
                    },
                    "name": "Shuqi Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T12:18:11.163Z",
                    "hidden": false
                },
                {
                    "_id": "692518ff16eb3a9f13103920",
                    "user": {
                        "_id": "64a38c590111d5ff6c3d5f2b",
                        "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg",
                        "isPro": false,
                        "fullname": "zhengliu",
                        "user": "lz1001",
                        "type": "user"
                    },
                    "name": "Zheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T12:17:59.618Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-23T12:29:33.000Z",
            "submittedOnDailyAt": "2025-11-25T00:25:04.757Z",
            "title": "General Agentic Memory Via Deep Research",
            "submittedOnDailyBy": {
                "_id": "64a38c590111d5ff6c3d5f2b",
                "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg",
                "isPro": false,
                "fullname": "zhengliu",
                "user": "lz1001",
                "type": "user"
            },
            "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.",
            "upvotes": 120,
            "discussionId": "692518ff16eb3a9f13103921",
            "projectPage": "https://github.com/VectorSpaceLab/general-agentic-memory",
            "githubRepo": "https://github.com/VectorSpaceLab/general-agentic-memory",
            "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.",
            "ai_keywords": [
                "general agentic memory",
                "GAM",
                "just-in time compilation",
                "JIT compilation",
                "memorizer",
                "researcher",
                "universal page-store",
                "large language models",
                "LLMs",
                "reinforcement learning"
            ],
            "githubStars": 99,
            "organization": {
                "_id": "61be9739d2f9358e24ca0a4f",
                "name": "BAAI",
                "fullname": "Beijing Academy of Artificial Intelligence",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
            }
        },
        "publishedAt": "2025-11-23T07:29:33.000Z",
        "title": "General Agentic Memory Via Deep Research",
        "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18423.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a38c590111d5ff6c3d5f2b",
            "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg",
            "fullname": "zhengliu",
            "name": "lz1001",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "organization": {
            "_id": "61be9739d2f9358e24ca0a4f",
            "name": "BAAI",
            "fullname": "Beijing Academy of Artificial Intelligence",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.19304",
            "authors": [
                {
                    "_id": "6925274e16eb3a9f13103998",
                    "user": {
                        "_id": "65f40e83653c231cbaf7defe",
                        "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg",
                        "isPro": false,
                        "fullname": "Jiayi Zhang",
                        "user": "didiforhugface",
                        "type": "user"
                    },
                    "name": "Jiayi Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T09:05:29.887Z",
                    "hidden": false
                },
                {
                    "_id": "6925274e16eb3a9f13103999",
                    "name": "Yiran Peng",
                    "hidden": false
                },
                {
                    "_id": "6925274e16eb3a9f1310399a",
                    "user": {
                        "_id": "6621e02cf34ab6caed18e9c6",
                        "avatarUrl": "/avatars/15888b2060d1cc56be9fa55fd4b34005.svg",
                        "isPro": false,
                        "fullname": "Fanqi Kong",
                        "user": "Fancylalala",
                        "type": "user"
                    },
                    "name": "Fanqi Kong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T09:05:25.655Z",
                    "hidden": false
                },
                {
                    "_id": "6925274e16eb3a9f1310399b",
                    "user": {
                        "_id": "67c443afb753bd020f9c97d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xbACBNLSopWmN5G1K8h_Y.png",
                        "isPro": false,
                        "fullname": "Cheng",
                        "user": "YangC777",
                        "type": "user"
                    },
                    "name": "Yang Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T09:05:18.820Z",
                    "hidden": false
                },
                {
                    "_id": "6925274e16eb3a9f1310399c",
                    "name": "Yifan Wu",
                    "hidden": false
                },
                {
                    "_id": "6925274e16eb3a9f1310399d",
                    "name": "Zhaoyang Yu",
                    "hidden": false
                },
                {
                    "_id": "6925274e16eb3a9f1310399e",
                    "user": {
                        "_id": "649ea7106282cb41e77760bc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649ea7106282cb41e77760bc/HlWjaqxr03ob93vdKg_LQ.jpeg",
                        "isPro": false,
                        "fullname": "Isaac",
                        "user": "XiangJinYu",
                        "type": "user"
                    },
                    "name": "Jinyu Xiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T09:05:23.607Z",
                    "hidden": false
                },
                {
                    "_id": "6925274e16eb3a9f1310399f",
                    "user": {
                        "_id": "68a435cc22fdf7356962ccb9",
                        "avatarUrl": "/avatars/467f4732ade5f47b42433ff354acdeef.svg",
                        "isPro": false,
                        "fullname": "jianhao ruan",
                        "user": "Aurorra1123",
                        "type": "user"
                    },
                    "name": "Jianhao Ruan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T15:53:17.306Z",
                    "hidden": false
                },
                {
                    "_id": "6925274e16eb3a9f131039a0",
                    "name": "Jinlin Wang",
                    "hidden": false
                },
                {
                    "_id": "6925274e16eb3a9f131039a1",
                    "name": "Maojia Song",
                    "hidden": false
                },
                {
                    "_id": "6925274e16eb3a9f131039a2",
                    "user": {
                        "_id": "6632160088f75d987d1a156f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6632160088f75d987d1a156f/mYlMQfK1BGWeEbOSMmeSb.jpeg",
                        "isPro": false,
                        "fullname": "Hongzhang Liu",
                        "user": "Alphamasterliu",
                        "type": "user"
                    },
                    "name": "HongZhang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T09:05:27.575Z",
                    "hidden": false
                },
                {
                    "_id": "6925274e16eb3a9f131039a3",
                    "name": "Xiangru Tang",
                    "hidden": false
                },
                {
                    "_id": "6925274e16eb3a9f131039a4",
                    "name": "Bang Liu",
                    "hidden": false
                },
                {
                    "_id": "6925274e16eb3a9f131039a5",
                    "name": "Chenglin Wu",
                    "hidden": false
                },
                {
                    "_id": "6925274e16eb3a9f131039a6",
                    "name": "Yuyu Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-24T16:54:23.000Z",
            "submittedOnDailyAt": "2025-11-25T01:26:13.029Z",
            "title": "AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning",
            "submittedOnDailyBy": {
                "_id": "65f40e83653c231cbaf7defe",
                "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg",
                "isPro": false,
                "fullname": "Jiayi Zhang",
                "user": "didiforhugface",
                "type": "user"
            },
            "summary": "Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.",
            "upvotes": 79,
            "discussionId": "6925274f16eb3a9f131039a7",
            "ai_summary": "AutoEnv and AutoEnv-36 provide a standardized framework and dataset for evaluating cross-environment learning in agents, highlighting the challenges and limitations of existing learning methods.",
            "ai_keywords": [
                "AutoEnv",
                "factorizable distributions",
                "heterogeneous environments",
                "AutoEnv-36",
                "language models",
                "normalized reward",
                "component-centric process",
                "Selection",
                "Optimization",
                "Evaluation",
                "learning methods",
                "environment-adaptive selection"
            ]
        },
        "publishedAt": "2025-11-24T11:54:23.000Z",
        "title": "AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning",
        "summary": "Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19304.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65f40e83653c231cbaf7defe",
            "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg",
            "fullname": "Jiayi Zhang",
            "name": "didiforhugface",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.15567",
            "authors": [
                {
                    "_id": "692067008c38b39d6a482df9",
                    "user": {
                        "_id": "64440be5af034cdfd69ca3a7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
                        "isPro": false,
                        "fullname": "Qinghong (Kevin) Lin",
                        "user": "KevinQHLin",
                        "type": "user"
                    },
                    "name": "Kevin Qinghong Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T09:06:05.314Z",
                    "hidden": false
                },
                {
                    "_id": "692067008c38b39d6a482dfa",
                    "name": "Siyuan Hu",
                    "hidden": false
                },
                {
                    "_id": "692067008c38b39d6a482dfb",
                    "name": "Linjie Li",
                    "hidden": false
                },
                {
                    "_id": "692067008c38b39d6a482dfc",
                    "user": {
                        "_id": "630713411801ecc7d2592a7c",
                        "avatarUrl": "/avatars/fb36f69f03421c3a2a7f72ba0858fa60.svg",
                        "isPro": true,
                        "fullname": "Zhengyuan Yang",
                        "user": "zyang39",
                        "type": "user"
                    },
                    "name": "Zhengyuan Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:36:56.053Z",
                    "hidden": false
                },
                {
                    "_id": "692067008c38b39d6a482dfd",
                    "name": "Lijuan Wang",
                    "hidden": false
                },
                {
                    "_id": "692067008c38b39d6a482dfe",
                    "user": {
                        "_id": "6565ed28a5ec0231cb07225f",
                        "avatarUrl": "/avatars/7f95bba9aa7811d56eecb380827abfac.svg",
                        "isPro": false,
                        "fullname": "prof philip torr",
                        "user": "philiptorr",
                        "type": "user"
                    },
                    "name": "Philip Torr",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:36:50.554Z",
                    "hidden": false
                },
                {
                    "_id": "692067008c38b39d6a482dff",
                    "user": {
                        "_id": "661ab3da2b14565c7acccf5c",
                        "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
                        "isPro": false,
                        "fullname": "Mike Zheng Shou",
                        "user": "AnalMom",
                        "type": "user"
                    },
                    "name": "Mike Zheng Shou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:36:42.407Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/46SJDQ6RRkIZYeUC4jmvo.mp4"
            ],
            "publishedAt": "2025-11-19T16:00:02.000Z",
            "submittedOnDailyAt": "2025-11-25T00:11:39.230Z",
            "title": "Computer-Use Agents as Judges for Generative User Interface",
            "submittedOnDailyBy": {
                "_id": "64440be5af034cdfd69ca3a7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
                "isPro": false,
                "fullname": "Qinghong (Kevin) Lin",
                "user": "KevinQHLin",
                "type": "user"
            },
            "summary": "Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.",
            "upvotes": 46,
            "discussionId": "692067008c38b39d6a482e00",
            "projectPage": "https://showlab.github.io/AUI/",
            "githubRepo": "https://github.com/showlab/AUI",
            "ai_summary": "A framework leveraging Computer-Use Agents as judges to assist coding-oriented language models in designing efficient and functional GUIs.",
            "ai_keywords": [
                "Computer-Use Agents",
                "CUA",
                "Graphical User Interfaces",
                "GUI",
                "language models",
                "Coder",
                "AUI-Gym",
                "task reliability",
                "Coder-CUA collaboration",
                "CUA Dashboard",
                "task solvability",
                "navigation success rate"
            ],
            "githubStars": 23,
            "organization": {
                "_id": "63a553c4ce5763e06f78669c",
                "name": "showlab",
                "fullname": "Show Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"
            }
        },
        "publishedAt": "2025-11-19T11:00:02.000Z",
        "title": "Computer-Use Agents as Judges for Generative User Interface",
        "summary": "Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/46SJDQ6RRkIZYeUC4jmvo.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15567.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64440be5af034cdfd69ca3a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
            "fullname": "Qinghong (Kevin) Lin",
            "name": "KevinQHLin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 40
        },
        "organization": {
            "_id": "63a553c4ce5763e06f78669c",
            "name": "showlab",
            "fullname": "Show Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.19365",
            "authors": [
                {
                    "_id": "69251d6b16eb3a9f13103933",
                    "user": {
                        "_id": "65d851096769b3a9c9376134",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d851096769b3a9c9376134/j_d2RSa-3rRmSAmRICSk0.jpeg",
                        "isPro": false,
                        "fullname": "ZehongMa",
                        "user": "zehongma",
                        "type": "user"
                    },
                    "name": "Zehong Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:37:20.953Z",
                    "hidden": false
                },
                {
                    "_id": "69251d6b16eb3a9f13103934",
                    "name": "Longhui Wei",
                    "hidden": false
                },
                {
                    "_id": "69251d6b16eb3a9f13103935",
                    "user": {
                        "_id": "66615c855fd9d736e670e0a9",
                        "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
                        "isPro": false,
                        "fullname": "wangshuai",
                        "user": "wangsssssss",
                        "type": "user"
                    },
                    "name": "Shuai Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T15:53:18.956Z",
                    "hidden": false
                },
                {
                    "_id": "69251d6b16eb3a9f13103936",
                    "user": {
                        "_id": "681c126ad15c979cc4c8cad1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/rpuwzjv2qXzrA4hfbdXJ8.png",
                        "isPro": false,
                        "fullname": "shiliang zhang",
                        "user": "slade96",
                        "type": "user"
                    },
                    "name": "Shiliang Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:37:37.350Z",
                    "hidden": false
                },
                {
                    "_id": "69251d6b16eb3a9f13103937",
                    "name": "Qi Tian",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-24T17:59:06.000Z",
            "submittedOnDailyAt": "2025-11-25T01:03:11.081Z",
            "title": "DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation",
            "submittedOnDailyBy": {
                "_id": "65d851096769b3a9c9376134",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d851096769b3a9c9376134/j_d2RSa-3rRmSAmRICSk0.jpeg",
                "isPro": false,
                "fullname": "ZehongMa",
                "user": "zehongma",
                "type": "user"
            },
            "summary": "Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.",
            "upvotes": 43,
            "discussionId": "69251d6b16eb3a9f13103938",
            "projectPage": "https://zehong-ma.github.io/DeCo/",
            "githubRepo": "https://github.com/Zehong-Ma/DeCo",
            "ai_summary": "The frequency-DeCoupled pixel diffusion framework improves image generation efficiency and quality by separating high-frequency details and low-frequency semantics, achieving superior performance compared to existing pixel diffusion models.",
            "ai_keywords": [
                "pixel diffusion",
                "pixel space",
                "VAE",
                "latent diffusion",
                "diffusion transformer (DiT)",
                "frequency-DeCoupled pixel diffusion",
                "lightweight pixel decoder",
                "semantic guidance",
                "frequency-aware flow-matching loss",
                "FID",
                "ImageNet",
                "GenEval"
            ],
            "githubStars": 33,
            "organization": {
                "_id": "61dcd8e344f59573371b5cb6",
                "name": "PekingUniversity",
                "fullname": "Peking University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
            }
        },
        "publishedAt": "2025-11-24T12:59:06.000Z",
        "title": "DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation",
        "summary": "Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19365.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65d851096769b3a9c9376134",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d851096769b3a9c9376134/j_d2RSa-3rRmSAmRICSk0.jpeg",
            "fullname": "ZehongMa",
            "name": "zehongma",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.19399",
            "authors": [
                {
                    "_id": "692531b316eb3a9f13103a46",
                    "name": "Rulin Shao",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a47",
                    "user": {
                        "_id": "6266e0cb7a1f5a1562c4e86e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6266e0cb7a1f5a1562c4e86e/kkJZPssa76mdRPnKH-q3e.jpeg",
                        "isPro": false,
                        "fullname": "Akari Asai",
                        "user": "akariasai",
                        "type": "user"
                    },
                    "name": "Akari Asai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T15:53:14.992Z",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a48",
                    "user": {
                        "_id": "613f897ffbfd59f147a88c81",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659455519119-613f897ffbfd59f147a88c81.jpeg",
                        "isPro": false,
                        "fullname": "Shannon Shen",
                        "user": "shannons",
                        "type": "user"
                    },
                    "name": "Shannon Zejiang Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T09:05:11.372Z",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a49",
                    "user": {
                        "_id": "62608fc2ffe8827cb1d89f9f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png",
                        "isPro": false,
                        "fullname": "Hamish Ivison",
                        "user": "hamishivi",
                        "type": "user"
                    },
                    "name": "Hamish Ivison",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T09:05:13.345Z",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a4a",
                    "name": "Varsha Kishore",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a4b",
                    "user": {
                        "_id": "6530c7263976e5f4412ba737",
                        "avatarUrl": "/avatars/8b4d9d847a9e115c3da8cad629bd0a41.svg",
                        "isPro": false,
                        "fullname": "Jingming Zhuo",
                        "user": "JingmingZ",
                        "type": "user"
                    },
                    "name": "Jingming Zhuo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T09:34:32.721Z",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a4c",
                    "name": "Xinran Zhao",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a4d",
                    "name": "Molly Park",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a4e",
                    "name": "Samuel G. Finlayson",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a4f",
                    "name": "David Sontag",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a50",
                    "user": {
                        "_id": "65e5fefbe8cbae176d9ca005",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e5fefbe8cbae176d9ca005/5SLREDsVzycEwVsPNv765.jpeg",
                        "isPro": false,
                        "fullname": "Tyler Murray",
                        "user": "undfined",
                        "type": "user"
                    },
                    "name": "Tyler Murray",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:41:12.772Z",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a51",
                    "user": {
                        "_id": "63a76d0de27a6dbd485fe863",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a76d0de27a6dbd485fe863/qJJwHOuvyQGq1o0KscOF_.jpeg",
                        "isPro": false,
                        "fullname": "Sewon Min",
                        "user": "sewon",
                        "type": "user"
                    },
                    "name": "Sewon Min",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:41:07.253Z",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a52",
                    "user": {
                        "_id": "6408fcc93461c51cf735a61e",
                        "avatarUrl": "/avatars/619f3653911d111f046a5a6c30fc8319.svg",
                        "isPro": false,
                        "fullname": "Pradeep Dasigi",
                        "user": "pradeepd",
                        "type": "user"
                    },
                    "name": "Pradeep Dasigi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:41:00.782Z",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a53",
                    "name": "Luca Soldaini",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a54",
                    "user": {
                        "_id": "65282b8d578679aac7888aec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65282b8d578679aac7888aec/dibBkhH-z1c70mJZZxJ7u.jpeg",
                        "isPro": false,
                        "fullname": "Faeze Brahman",
                        "user": "faezeb",
                        "type": "user"
                    },
                    "name": "Faeze Brahman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:41:17.953Z",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a55",
                    "name": "Wen-tau Yih",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a56",
                    "name": "Tongshuang Wu",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a57",
                    "name": "Luke Zettlemoyer",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a58",
                    "name": "Yoon Kim",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a59",
                    "name": "Hannaneh Hajishirzi",
                    "hidden": false
                },
                {
                    "_id": "692531b316eb3a9f13103a5a",
                    "user": {
                        "_id": "641b4263abfce26bcf7b27de",
                        "avatarUrl": "/avatars/e91b4205e4f74b0dd8c333c23203a924.svg",
                        "isPro": false,
                        "fullname": "Pang Wei Koh",
                        "user": "pangwei",
                        "type": "user"
                    },
                    "name": "Pang Wei Koh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:38:49.292Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-24T18:35:54.000Z",
            "submittedOnDailyAt": "2025-11-25T02:20:40.847Z",
            "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research",
            "submittedOnDailyBy": {
                "_id": "62608fc2ffe8827cb1d89f9f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png",
                "isPro": false,
                "fullname": "Hamish Ivison",
                "user": "hamishivi",
                "type": "user"
            },
            "summary": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.",
            "upvotes": 40,
            "discussionId": "692531b316eb3a9f13103a5b",
            "projectPage": "https://github.com/rlresearch/dr-tulu",
            "githubRepo": "https://github.com/rlresearch/dr-tulu",
            "ai_summary": "Reinforcement Learning with Evolving Rubrics (RLER) enables training of deep research models for long-form tasks, outperforming existing models and proprietary systems while being more cost-effective.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards (RLVR)",
                "Reinforcement Learning with Evolving Rubrics (RLER)",
                "Deep Research Tulu (DR Tulu-8B)",
                "long-form deep research",
                "deep research models",
                "open-ended tasks",
                "science benchmarks",
                "healthcare benchmarks",
                "general domain benchmarks",
                "MCP-based agent infrastructure"
            ],
            "githubStars": 328,
            "organization": {
                "_id": "6916602cb89e7abe20e1da38",
                "name": "rl-research",
                "fullname": "RL ReSearch",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6334a0bd31a2be3938c59537/iySh9RAgtqs7m77mwfFOi.png"
            }
        },
        "publishedAt": "2025-11-24T13:35:54.000Z",
        "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research",
        "summary": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19399.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62608fc2ffe8827cb1d89f9f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png",
            "fullname": "Hamish Ivison",
            "name": "hamishivi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "organization": {
            "_id": "6916602cb89e7abe20e1da38",
            "name": "rl-research",
            "fullname": "RL ReSearch",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6334a0bd31a2be3938c59537/iySh9RAgtqs7m77mwfFOi.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.18050",
            "authors": [
                {
                    "_id": "692525c216eb3a9f1310396f",
                    "name": "Tian Ye",
                    "hidden": false
                },
                {
                    "_id": "692525c216eb3a9f13103970",
                    "name": "Song Fei",
                    "hidden": false
                },
                {
                    "_id": "692525c216eb3a9f13103971",
                    "name": "Lei Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-22T13:07:21.000Z",
            "submittedOnDailyAt": "2025-11-25T01:29:51.780Z",
            "title": "UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios",
            "submittedOnDailyBy": {
                "_id": "66015e8aa4d296af07de538e",
                "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
                "isPro": false,
                "fullname": "Owen",
                "user": "Owen777",
                "type": "user"
            },
            "summary": "Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0.",
            "upvotes": 31,
            "discussionId": "692525c216eb3a9f13103972",
            "projectPage": "https://github.com/W2GenAI-Lab/UltraFlux",
            "githubRepo": "https://github.com/W2GenAI-Lab/UltraFlux",
            "ai_summary": "UltraFlux, a Flux-based DiT trained on a 4K dataset, addresses failures in diffusion transformers at 4K resolution through enhanced positional encoding, improved VAE compression, gradient rebalancing, and aesthetic curriculum learning, achieving superior performance compared to existing models.",
            "ai_keywords": [
                "diffusion transformers",
                "text-to-image generation",
                "4K resolution",
                "positional encoding",
                "VAE compression",
                "Resonance 2D RoPE",
                "YaRN",
                "VAE post-training",
                "SNR-Aware Huber Wavelet",
                "Stage-wise Aesthetic Curriculum Learning",
                "aesthetic evaluation",
                "Aesthetic-Eval",
                "Seedream 4.0"
            ],
            "githubStars": 40,
            "organization": {
                "_id": "68b2b2167f881fc640bb9d21",
                "name": "W2GenAI",
                "fullname": "W2GenAI Lab"
            }
        },
        "publishedAt": "2025-11-22T08:07:21.000Z",
        "title": "UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios",
        "summary": "Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18050.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66015e8aa4d296af07de538e",
            "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
            "fullname": "Owen",
            "name": "Owen777",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 39
        },
        "organization": {
            "_id": "68b2b2167f881fc640bb9d21",
            "name": "W2GenAI",
            "fullname": "W2GenAI Lab"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.19401",
            "authors": [
                {
                    "_id": "692523c616eb3a9f1310395f",
                    "name": "Gongfan Fang",
                    "hidden": false
                },
                {
                    "_id": "692523c616eb3a9f13103960",
                    "user": {
                        "_id": "64396ebc21221ac7411852b3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg",
                        "isPro": false,
                        "fullname": "Xinyin Ma",
                        "user": "horseee",
                        "type": "user"
                    },
                    "name": "Xinyin Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:38:13.400Z",
                    "hidden": false
                },
                {
                    "_id": "692523c616eb3a9f13103961",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-24T18:38:45.000Z",
            "submittedOnDailyAt": "2025-11-25T01:10:58.624Z",
            "title": "In-Video Instructions: Visual Signals as Generative Control",
            "submittedOnDailyBy": {
                "_id": "64396ebc21221ac7411852b3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg",
                "isPro": false,
                "fullname": "Xinyin Ma",
                "user": "horseee",
                "type": "user"
            },
            "summary": "Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.",
            "upvotes": 25,
            "discussionId": "692523c716eb3a9f13103962",
            "projectPage": "https://fangggf.github.io/In-Video/",
            "ai_summary": "Video generative models can interpret and execute visual instructions embedded within frames, enhancing controllability in image-to-video generation.",
            "ai_keywords": [
                "video generative models",
                "In-Video Instruction",
                "prompt-based control",
                "Veo 3.1",
                "Kling 2.5",
                "Wan 2.2",
                "visual instructions",
                "overlaid text",
                "arrows",
                "trajectories",
                "spatial-aware",
                "multi-object scenarios"
            ],
            "organization": {
                "_id": "6508ab2b349930913196378b",
                "name": "NationalUniversityofSingapore",
                "fullname": "National University of Singapore",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
            }
        },
        "publishedAt": "2025-11-24T13:38:45.000Z",
        "title": "In-Video Instructions: Visual Signals as Generative Control",
        "summary": "Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19401.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64396ebc21221ac7411852b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg",
            "fullname": "Xinyin Ma",
            "name": "horseee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "6508ab2b349930913196378b",
            "name": "NationalUniversityofSingapore",
            "fullname": "National University of Singapore",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.17006",
            "authors": [
                {
                    "_id": "6925218816eb3a9f13103946",
                    "name": "Tengxiao Liu",
                    "hidden": false
                },
                {
                    "_id": "6925218816eb3a9f13103947",
                    "name": "Zifeng Wang",
                    "hidden": false
                },
                {
                    "_id": "6925218816eb3a9f13103948",
                    "name": "Jin Miao",
                    "hidden": false
                },
                {
                    "_id": "6925218816eb3a9f13103949",
                    "name": "I-Hung Hsu",
                    "hidden": false
                },
                {
                    "_id": "6925218816eb3a9f1310394a",
                    "name": "Jun Yan",
                    "hidden": false
                },
                {
                    "_id": "6925218816eb3a9f1310394b",
                    "name": "Jiefeng Chen",
                    "hidden": false
                },
                {
                    "_id": "6925218816eb3a9f1310394c",
                    "name": "Rujun Han",
                    "hidden": false
                },
                {
                    "_id": "6925218816eb3a9f1310394d",
                    "name": "Fangyuan Xu",
                    "hidden": false
                },
                {
                    "_id": "6925218816eb3a9f1310394e",
                    "name": "Yanfei Chen",
                    "hidden": false
                },
                {
                    "_id": "6925218816eb3a9f1310394f",
                    "name": "Ke Jiang",
                    "hidden": false
                },
                {
                    "_id": "6925218816eb3a9f13103950",
                    "name": "Samira Daruki",
                    "hidden": false
                },
                {
                    "_id": "6925218816eb3a9f13103951",
                    "name": "Yi Liang",
                    "hidden": false
                },
                {
                    "_id": "6925218816eb3a9f13103952",
                    "name": "William Yang Wang",
                    "hidden": false
                },
                {
                    "_id": "6925218816eb3a9f13103953",
                    "name": "Tomas Pfister",
                    "hidden": false
                },
                {
                    "_id": "6925218816eb3a9f13103954",
                    "name": "Chen-Yu Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-21T07:18:55.000Z",
            "submittedOnDailyAt": "2025-11-25T00:59:36.249Z",
            "title": "Budget-Aware Tool-Use Enables Effective Agent Scaling",
            "submittedOnDailyBy": {
                "_id": "66ab1a31d456f0408b8c97e1",
                "avatarUrl": "/avatars/f47851b19eae288bbdad586b513522b9.svg",
                "isPro": true,
                "fullname": "Tengxiao Liu",
                "user": "JustinTX",
                "type": "user"
            },
            "summary": "Scaling test-time computation improves performance across different tasks on large language models (LLMs), which has also been extended to tool-augmented agents. For these agents, scaling involves not only \"thinking\" in tokens but also \"acting\" via tool calls. The number of tool calls directly bounds the agent's interaction with the external environment. However, we find that simply granting agents a larger tool-call budget fails to improve performance, as they lack \"budget awareness\" and quickly hit a performance ceiling. To address this, we study how to scale such agents effectively under explicit tool-call budgets, focusing on web search agents. We first introduce the Budget Tracker, a lightweight plug-in that provides the agent with continuous budget awareness, enabling simple yet effective scaling. We further develop BATS (Budget Aware Test-time Scaling), an advanced framework that leverages this awareness to dynamically adapt its planning and verification strategy, deciding whether to \"dig deeper\" on a promising lead or \"pivot\" to new paths based on remaining resources. To analyze cost-performance scaling in a controlled manner, we formalize a unified cost metric that jointly accounts for token and tool consumption. We provide the first systematic study on budget-constrained agents, showing that budget-aware methods produce more favorable scaling curves and push the cost-performance Pareto frontier. Our work offers empirical insights toward a more transparent and principled understanding of scaling in tool-augmented agents.",
            "upvotes": 18,
            "discussionId": "6925218816eb3a9f13103955",
            "ai_summary": "Budget-aware methods improve the scaling of tool-augmented agents by providing continuous budget awareness and adaptive planning, leading to better cost-performance trade-offs.",
            "ai_keywords": [
                "Budget Tracker",
                "BATS",
                "Budget Aware Test-time Scaling",
                "token consumption",
                "tool consumption",
                "cost-performance Pareto frontier"
            ],
            "organization": {
                "_id": "5e6aca39878b8b2bf9806447",
                "name": "google",
                "fullname": "Google",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
            }
        },
        "publishedAt": "2025-11-21T02:18:55.000Z",
        "title": "Budget-Aware Tool-Use Enables Effective Agent Scaling",
        "summary": "Scaling test-time computation improves performance across different tasks on large language models (LLMs), which has also been extended to tool-augmented agents. For these agents, scaling involves not only \"thinking\" in tokens but also \"acting\" via tool calls. The number of tool calls directly bounds the agent's interaction with the external environment. However, we find that simply granting agents a larger tool-call budget fails to improve performance, as they lack \"budget awareness\" and quickly hit a performance ceiling. To address this, we study how to scale such agents effectively under explicit tool-call budgets, focusing on web search agents. We first introduce the Budget Tracker, a lightweight plug-in that provides the agent with continuous budget awareness, enabling simple yet effective scaling. We further develop BATS (Budget Aware Test-time Scaling), an advanced framework that leverages this awareness to dynamically adapt its planning and verification strategy, deciding whether to \"dig deeper\" on a promising lead or \"pivot\" to new paths based on remaining resources. To analyze cost-performance scaling in a controlled manner, we formalize a unified cost metric that jointly accounts for token and tool consumption. We provide the first systematic study on budget-constrained agents, showing that budget-aware methods produce more favorable scaling curves and push the cost-performance Pareto frontier. Our work offers empirical insights toward a more transparent and principled understanding of scaling in tool-augmented agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17006.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66ab1a31d456f0408b8c97e1",
            "avatarUrl": "/avatars/f47851b19eae288bbdad586b513522b9.svg",
            "fullname": "Tengxiao Liu",
            "name": "JustinTX",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "5e6aca39878b8b2bf9806447",
            "name": "google",
            "fullname": "Google",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.19418",
            "authors": [
                {
                    "_id": "69252cfc16eb3a9f13103a29",
                    "name": "Yiming Qin",
                    "hidden": false
                },
                {
                    "_id": "69252cfc16eb3a9f13103a2a",
                    "name": "Bomin Wei",
                    "hidden": false
                },
                {
                    "_id": "69252cfc16eb3a9f13103a2b",
                    "name": "Jiaxin Ge",
                    "hidden": false
                },
                {
                    "_id": "69252cfc16eb3a9f13103a2c",
                    "name": "Konstantinos Kallidromitis",
                    "hidden": false
                },
                {
                    "_id": "69252cfc16eb3a9f13103a2d",
                    "name": "Stephanie Fu",
                    "hidden": false
                },
                {
                    "_id": "69252cfc16eb3a9f13103a2e",
                    "name": "Trevor Darrell",
                    "hidden": false
                },
                {
                    "_id": "69252cfc16eb3a9f13103a2f",
                    "name": "Xudong Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/yfpo2h56TTWsr_Aqwsdp5.png"
            ],
            "publishedAt": "2025-11-24T18:55:19.000Z",
            "submittedOnDailyAt": "2025-11-25T01:44:14.396Z",
            "title": "Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.",
            "upvotes": 17,
            "discussionId": "69252cfd16eb3a9f13103a30",
            "projectPage": "https://wakalsprojectpage.github.io/comt-website/",
            "ai_summary": "Chain-of-Visual-Thought (COVT) enables Vision-Language Models to reason through visual tokens, improving their performance on perceptual tasks by capturing dense visual information.",
            "ai_keywords": [
                "Chain-of-Visual-Thought",
                "COVT",
                "continuous visual tokens",
                "lightweight vision experts",
                "dense visual information",
                "spatial dimensions",
                "2D appearance",
                "3D geometry",
                "spatial layout",
                "edge structure",
                "autoregressive prediction",
                "dense supervision signals",
                "depth",
                "segmentation",
                "edges",
                "DINO features",
                "CV-Bench",
                "MMVP",
                "RealWorldQA",
                "MMStar",
                "WorldMedQA",
                "HRBench",
                "Qwen2.5-VL",
                "LLaVA",
                "multimodal intelligence"
            ]
        },
        "publishedAt": "2025-11-24T13:55:19.000Z",
        "title": "Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens",
        "summary": "Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/yfpo2h56TTWsr_Aqwsdp5.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19418.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 170
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.17803",
            "authors": [
                {
                    "_id": "6925197016eb3a9f13103923",
                    "name": "Kumar Krishna Agrawal",
                    "hidden": false
                },
                {
                    "_id": "6925197016eb3a9f13103924",
                    "name": "Longchao Liu",
                    "hidden": false
                },
                {
                    "_id": "6925197016eb3a9f13103925",
                    "name": "Long Lian",
                    "hidden": false
                },
                {
                    "_id": "6925197016eb3a9f13103926",
                    "name": "Michael Nercessian",
                    "hidden": false
                },
                {
                    "_id": "6925197016eb3a9f13103927",
                    "name": "Natalia Harguindeguy",
                    "hidden": false
                },
                {
                    "_id": "6925197016eb3a9f13103928",
                    "name": "Yufu Wu",
                    "hidden": false
                },
                {
                    "_id": "6925197016eb3a9f13103929",
                    "name": "Peter Mikhael",
                    "hidden": false
                },
                {
                    "_id": "6925197016eb3a9f1310392a",
                    "name": "Gigin Lin",
                    "hidden": false
                },
                {
                    "_id": "6925197016eb3a9f1310392b",
                    "name": "Lecia V. Sequist",
                    "hidden": false
                },
                {
                    "_id": "6925197016eb3a9f1310392c",
                    "name": "Florian Fintelmann",
                    "hidden": false
                },
                {
                    "_id": "6925197016eb3a9f1310392d",
                    "name": "Trevor Darrell",
                    "hidden": false
                },
                {
                    "_id": "6925197016eb3a9f1310392e",
                    "name": "Yutong Bai",
                    "hidden": false
                },
                {
                    "_id": "6925197016eb3a9f1310392f",
                    "name": "Maggie Chung",
                    "hidden": false
                },
                {
                    "_id": "6925197016eb3a9f13103930",
                    "name": "Adam Yala",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6254623af74165c7d2df0c18/-cTjmFF64vxY4SyK8UIAa.png"
            ],
            "publishedAt": "2025-11-21T21:50:34.000Z",
            "submittedOnDailyAt": "2025-11-25T01:49:24.068Z",
            "title": "Pillar-0: A New Frontier for Radiology Foundation Models",
            "submittedOnDailyBy": {
                "_id": "6254623af74165c7d2df0c18",
                "avatarUrl": "/avatars/4ddd1aa858723dd9348fc51966f700c3.svg",
                "isPro": false,
                "fullname": "Kumar Krishna Agrawal",
                "user": "kumarkrishna",
                "type": "user"
            },
            "summary": "Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.",
            "upvotes": 16,
            "discussionId": "6925197016eb3a9f13103931",
            "ai_summary": "Pillar-0, a radiology foundation model pretrained on diverse imaging datasets, outperforms existing models across various tasks and extends to new applications using RATE for label extraction.",
            "ai_keywords": [
                "foundation models",
                "volumetric CT",
                "MRI",
                "grayscale contrast",
                "RATE",
                "structured labels",
                "AUROC",
                "MedGemma",
                "MedImageInsight",
                "Lingshu",
                "Merlin",
                "Stanford Abdominal CT dataset",
                "long-horizon lung cancer risk prediction",
                "Sybil",
                "C-index",
                "brain hemorrhage detection",
                "sample efficiency",
                "open",
                "clinically rigorous"
            ],
            "organization": {
                "_id": "66d1308fe801db14250a1cc6",
                "name": "YalaLab",
                "fullname": "Yala Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6333a9195a032dcd095dda13/r-7psCzH30cILTNWRQ60N.png"
            }
        },
        "publishedAt": "2025-11-21T16:50:34.000Z",
        "title": "Pillar-0: A New Frontier for Radiology Foundation Models",
        "summary": "Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6254623af74165c7d2df0c18/-cTjmFF64vxY4SyK8UIAa.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17803.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6254623af74165c7d2df0c18",
            "avatarUrl": "/avatars/4ddd1aa858723dd9348fc51966f700c3.svg",
            "fullname": "Kumar Krishna Agrawal",
            "name": "kumarkrishna",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "66d1308fe801db14250a1cc6",
            "name": "YalaLab",
            "fullname": "Yala Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6333a9195a032dcd095dda13/r-7psCzH30cILTNWRQ60N.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.18870",
            "authors": [
                {
                    "_id": "69252c0d16eb3a9f131039d6",
                    "name": "Bing Wu",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039d7",
                    "name": "Chang Zou",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039d8",
                    "name": "Changlin Li",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039d9",
                    "name": "Duojun Huang",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039da",
                    "name": "Fang Yang",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039db",
                    "name": "Hao Tan",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039dc",
                    "name": "Jack Peng",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039dd",
                    "name": "Jianbing Wu",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039de",
                    "name": "Jiangfeng Xiong",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039df",
                    "name": "Jie Jiang",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039e0",
                    "name": "Linus",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039e1",
                    "name": "Patrol",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039e2",
                    "user": {
                        "_id": "670bc2aac9c7e01d3db61243",
                        "avatarUrl": "/avatars/2bd5fba261e710a4c287fcf2f78b584f.svg",
                        "isPro": false,
                        "fullname": "Peizhen Zhang",
                        "user": "maxpzzhang",
                        "type": "user"
                    },
                    "name": "Peizhen Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T12:19:00.711Z",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039e3",
                    "name": "Peng Chen",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039e4",
                    "name": "Penghao Zhao",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039e5",
                    "name": "Qi Tian",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039e6",
                    "name": "Songtao Liu",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039e7",
                    "user": {
                        "_id": "67f7ab7ef241f0b92ce4b087",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vDhBPKqfWjMI65K-kL2tm.jpeg",
                        "isPro": false,
                        "fullname": "Weijie Kong",
                        "user": "Knightbreeze",
                        "type": "user"
                    },
                    "name": "Weijie Kong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T12:19:44.134Z",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039e8",
                    "name": "Weiyan Wang",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039e9",
                    "name": "Xiao He",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039ea",
                    "name": "Xin Li",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039eb",
                    "name": "Xinchi Deng",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039ec",
                    "name": "Xuefei Zhe",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039ed",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039ee",
                    "name": "Yanxin Long",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039ef",
                    "name": "Yuanbo Peng",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039f0",
                    "name": "Yue Wu",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039f1",
                    "name": "Yuhong Liu",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039f2",
                    "name": "Zhenyu Wang",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039f3",
                    "user": {
                        "_id": "6316fe2a6d322c52490f90e8",
                        "avatarUrl": "/avatars/8c896b3733e183657009a426834a9dab.svg",
                        "isPro": false,
                        "fullname": "Dai Zuozhuo",
                        "user": "zyand",
                        "type": "user"
                    },
                    "name": "Zuozhuo Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T12:19:54.939Z",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039f4",
                    "name": "Bo Peng",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039f5",
                    "name": "Coopers Li",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039f6",
                    "name": "Gu Gong",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039f7",
                    "name": "Guojian Xiao",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039f8",
                    "name": "Jiahe Tian",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039f9",
                    "name": "Jiaxin Lin",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039fa",
                    "name": "Jie Liu",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039fb",
                    "name": "Jihong Zhang",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039fc",
                    "name": "Jiesong Lian",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039fd",
                    "name": "Kaihang Pan",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039fe",
                    "name": "Lei Wang",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f131039ff",
                    "name": "Lin Niu",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a00",
                    "name": "Mingtao Chen",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a01",
                    "name": "Mingyang Chen",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a02",
                    "name": "Mingzhe Zheng",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a03",
                    "name": "Miles Yang",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a04",
                    "name": "Qiangqiang Hu",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a05",
                    "name": "Qi Yang",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a06",
                    "name": "Qiuyong Xiao",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a07",
                    "name": "Runzhou Wu",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a08",
                    "name": "Ryan Xu",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a09",
                    "name": "Rui Yuan",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a0a",
                    "name": "Shanshan Sang",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a0b",
                    "name": "Shisheng Huang",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a0c",
                    "name": "Siruis Gong",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a0d",
                    "name": "Shuo Huang",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a0e",
                    "name": "Weiting Guo",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a0f",
                    "name": "Xiang Yuan",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a10",
                    "name": "Xiaojia Chen",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a11",
                    "name": "Xiawei Hu",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a12",
                    "name": "Wenzhi Sun",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a13",
                    "name": "Xiele Wu",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a14",
                    "name": "Xianshun Ren",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a15",
                    "name": "Xiaoyan Yuan",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a16",
                    "name": "Xiaoyue Mi",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a17",
                    "name": "Yepeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a18",
                    "name": "Yifu Sun",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a19",
                    "name": "Yiting Lu",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a1a",
                    "name": "Yitong Li",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a1b",
                    "name": "You Huang",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a1c",
                    "name": "Yu Tang",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a1d",
                    "name": "Yixuan Li",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a1e",
                    "name": "Yuhang Deng",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a1f",
                    "name": "Yuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a20",
                    "name": "Zhichao Hu",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a21",
                    "user": {
                        "_id": "64556769fbe00f9e73c1bddb",
                        "avatarUrl": "/avatars/dc0a2d9cf91190d722cb6613072a6556.svg",
                        "isPro": false,
                        "fullname": "ZHIGUANG LIU",
                        "user": "lz7fdmu",
                        "type": "user"
                    },
                    "name": "Zhiguang Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T12:23:02.416Z",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a22",
                    "name": "Zhihe Yang",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a23",
                    "name": "Zilin Yang",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a24",
                    "name": "Zhenzhi Lu",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a25",
                    "name": "Zixiang Zhou",
                    "hidden": false
                },
                {
                    "_id": "69252c0d16eb3a9f13103a26",
                    "name": "Zhao Zhong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-24T08:22:07.000Z",
            "submittedOnDailyAt": "2025-11-25T01:39:58.047Z",
            "title": "HunyuanVideo 1.5 Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.",
            "upvotes": 15,
            "discussionId": "69252c0d16eb3a9f13103a27",
            "ai_summary": "HunyuanVideo 1.5 is a lightweight video generation model with state-of-the-art visual quality and motion coherence, using a DiT architecture with SSTA and an efficient video super-resolution network.",
            "ai_keywords": [
                "DiT architecture",
                "selective and sliding tile attention",
                "SSTA",
                "glyph-aware text encoding",
                "progressive pre-training",
                "post-training",
                "video super-resolution network",
                "text-to-video",
                "image-to-video"
            ]
        },
        "publishedAt": "2025-11-24T03:22:07.000Z",
        "title": "HunyuanVideo 1.5 Technical Report",
        "summary": "We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18870.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 170
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.17986",
            "authors": [
                {
                    "_id": "6925296c16eb3a9f131039b2",
                    "name": "Lun Huang",
                    "hidden": false
                },
                {
                    "_id": "6925296c16eb3a9f131039b3",
                    "name": "You Xie",
                    "hidden": false
                },
                {
                    "_id": "6925296c16eb3a9f131039b4",
                    "name": "Hongyi Xu",
                    "hidden": false
                },
                {
                    "_id": "6925296c16eb3a9f131039b5",
                    "name": "Tianpei Gu",
                    "hidden": false
                },
                {
                    "_id": "6925296c16eb3a9f131039b6",
                    "name": "Chenxu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6925296c16eb3a9f131039b7",
                    "name": "Guoxian Song",
                    "hidden": false
                },
                {
                    "_id": "6925296c16eb3a9f131039b8",
                    "name": "Zenan Li",
                    "hidden": false
                },
                {
                    "_id": "6925296c16eb3a9f131039b9",
                    "name": "Xiaochen Zhao",
                    "hidden": false
                },
                {
                    "_id": "6925296c16eb3a9f131039ba",
                    "name": "Linjie Luo",
                    "hidden": false
                },
                {
                    "_id": "6925296c16eb3a9f131039bb",
                    "name": "Guillermo Sapiro",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-22T08:59:09.000Z",
            "submittedOnDailyAt": "2025-11-25T01:28:44.671Z",
            "title": "Plan-X: Instruct Video Generation via Semantic Planning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured \"semantic sketches\" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.",
            "upvotes": 15,
            "discussionId": "6925296c16eb3a9f131039bc",
            "projectPage": "https://byteaigc.github.io/Plan-X/",
            "ai_summary": "Plan-X integrates a Semantic Planner and diffusion models to reduce visual hallucinations and improve instruction-aligned video generation by using multimodal semantic tokens.",
            "ai_keywords": [
                "Diffusion Transformers",
                "visual synthesis",
                "high-level semantic reasoning",
                "long-horizon planning",
                "visual hallucinations",
                "Semantic Planner",
                "multimodal language model",
                "text-grounded spatio-temporal semantic tokens",
                "video diffusion model",
                "photorealistic video synthesis"
            ]
        },
        "publishedAt": "2025-11-22T03:59:09.000Z",
        "title": "Plan-X: Instruct Video Generation via Semantic Planning",
        "summary": "Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured \"semantic sketches\" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17986.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 170
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.17729",
            "authors": [
                {
                    "_id": "6925154e16eb3a9f1310390b",
                    "name": "Yang Zhou",
                    "hidden": false
                },
                {
                    "_id": "6925154e16eb3a9f1310390c",
                    "name": "Mingyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "6925154e16eb3a9f1310390d",
                    "name": "Zhenting Wang",
                    "hidden": false
                },
                {
                    "_id": "6925154e16eb3a9f1310390e",
                    "user": {
                        "_id": "64fd1fa384bf01577e90382c",
                        "avatarUrl": "/avatars/43b39461781b504c08831f2cdc3af432.svg",
                        "isPro": false,
                        "fullname": "Difei Gu",
                        "user": "difeigu",
                        "type": "user"
                    },
                    "name": "Difei Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T09:05:43.437Z",
                    "hidden": false
                },
                {
                    "_id": "6925154e16eb3a9f1310390f",
                    "name": "Bangwei Guo",
                    "hidden": false
                },
                {
                    "_id": "6925154e16eb3a9f13103910",
                    "name": "Ruosong Ye",
                    "hidden": false
                },
                {
                    "_id": "6925154e16eb3a9f13103911",
                    "name": "Ligong Han",
                    "hidden": false
                },
                {
                    "_id": "6925154e16eb3a9f13103912",
                    "name": "Can Jin",
                    "hidden": false
                },
                {
                    "_id": "6925154e16eb3a9f13103913",
                    "name": "Dimitris N. Metaxas",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-21T19:27:02.000Z",
            "submittedOnDailyAt": "2025-11-25T00:05:53.981Z",
            "title": "M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark",
            "submittedOnDailyBy": {
                "_id": "64dfcc62e8b6f3f3baa950e0",
                "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
                "isPro": false,
                "fullname": "Zhenting Wang",
                "user": "ztwang",
                "type": "user"
            },
            "summary": "We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resources across steps. We introduce a similarity-driven alignment that serializes each tool call, embeds signatures with a sentence encoder, and performs similarity-bucketed Hungarian matching to obtain auditable one-to-one correspondences. On top of this alignment, we report interpretable metrics that decouple semantic fidelity from workflow consistency. The benchmark spans 28 servers with 231 tools, and provides standardized trajectories curated through an Executor & Judge pipeline with human verification; an auxiliary four large language models (LLMs) judge ensemble reports end-task Task Completion and information grounding. Evaluations of representative state-of-the-art Multimodal LLMs (MLLMs) reveal persistent gaps in multimodal MCP tool use, particularly in argument fidelity and structure consistency, underscoring the need for methods that jointly reason over images, text, and tool graphs. Our Benchmark's anonymous repository is at https://github.com/EtaYang10th/Open-M3-Bench",
            "upvotes": 13,
            "discussionId": "6925154e16eb3a9f13103914",
            "githubRepo": "https://github.com/EtaYang10th/Open-M3-Bench",
            "ai_summary": "M^3-Bench evaluates multimodal tool use with a focus on visual grounding, textual reasoning, and tool dependencies using a novel similarity-driven alignment method and interpretable metrics.",
            "ai_keywords": [
                "Model Context Protocol",
                "multimodal tool use",
                "multi-hop workflows",
                "multi-threaded workflows",
                "visual grounding",
                "textual reasoning",
                "cross-tool dependencies",
                "intermediate resources",
                "similarity-driven alignment",
                "sentence encoder",
                "Hungarian matching",
                "semantic fidelity",
                "workflow consistency",
                "Multimodal LLMs",
                "argument fidelity",
                "structure consistency",
                "tool graphs"
            ],
            "githubStars": 12
        },
        "publishedAt": "2025-11-21T14:27:02.000Z",
        "title": "M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark",
        "summary": "We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resources across steps. We introduce a similarity-driven alignment that serializes each tool call, embeds signatures with a sentence encoder, and performs similarity-bucketed Hungarian matching to obtain auditable one-to-one correspondences. On top of this alignment, we report interpretable metrics that decouple semantic fidelity from workflow consistency. The benchmark spans 28 servers with 231 tools, and provides standardized trajectories curated through an Executor & Judge pipeline with human verification; an auxiliary four large language models (LLMs) judge ensemble reports end-task Task Completion and information grounding. Evaluations of representative state-of-the-art Multimodal LLMs (MLLMs) reveal persistent gaps in multimodal MCP tool use, particularly in argument fidelity and structure consistency, underscoring the need for methods that jointly reason over images, text, and tool graphs. Our Benchmark's anonymous repository is at https://github.com/EtaYang10th/Open-M3-Bench",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17729.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64dfcc62e8b6f3f3baa950e0",
            "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
            "fullname": "Zhenting Wang",
            "name": "ztwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.13288",
            "authors": [
                {
                    "_id": "69255d6f16eb3a9f13103aa3",
                    "name": "Haoyang Hong",
                    "hidden": false
                },
                {
                    "_id": "69255d6f16eb3a9f13103aa4",
                    "name": "Jiajun Yin",
                    "hidden": false
                },
                {
                    "_id": "69255d6f16eb3a9f13103aa5",
                    "name": "Yuan Wang",
                    "hidden": false
                },
                {
                    "_id": "69255d6f16eb3a9f13103aa6",
                    "name": "Jingnan Liu",
                    "hidden": false
                },
                {
                    "_id": "69255d6f16eb3a9f13103aa7",
                    "name": "Zhe Chen",
                    "hidden": false
                },
                {
                    "_id": "69255d6f16eb3a9f13103aa8",
                    "name": "Ailing Yu",
                    "hidden": false
                },
                {
                    "_id": "69255d6f16eb3a9f13103aa9",
                    "name": "Ji Li",
                    "hidden": false
                },
                {
                    "_id": "69255d6f16eb3a9f13103aaa",
                    "name": "Zhiling Ye",
                    "hidden": false
                },
                {
                    "_id": "69255d6f16eb3a9f13103aab",
                    "name": "Hansong Xiao",
                    "hidden": false
                },
                {
                    "_id": "69255d6f16eb3a9f13103aac",
                    "name": "Yefei Chen",
                    "hidden": false
                },
                {
                    "_id": "69255d6f16eb3a9f13103aad",
                    "name": "Hualei Zhou",
                    "hidden": false
                },
                {
                    "_id": "69255d6f16eb3a9f13103aae",
                    "name": "Yun Yue",
                    "hidden": false
                },
                {
                    "_id": "69255d6f16eb3a9f13103aaf",
                    "name": "Minghui Yang",
                    "hidden": false
                },
                {
                    "_id": "69255d6f16eb3a9f13103ab0",
                    "name": "Chunxiao Guo",
                    "hidden": false
                },
                {
                    "_id": "69255d6f16eb3a9f13103ab1",
                    "name": "Junwei Liu",
                    "hidden": false
                },
                {
                    "_id": "69255d6f16eb3a9f13103ab2",
                    "name": "Peng Wei",
                    "hidden": false
                },
                {
                    "_id": "69255d6f16eb3a9f13103ab3",
                    "name": "Jinjie Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-17T12:06:30.000Z",
            "submittedOnDailyAt": "2025-11-25T05:12:51.765Z",
            "title": "Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO",
            "submittedOnDailyBy": {
                "_id": "6826eb9bf609dcc6d6c6df52",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6826eb9bf609dcc6d6c6df52/L_uK87dUkyuDy7iu1g_6C.jpeg",
                "isPro": false,
                "fullname": "Erinyu",
                "user": "eerrr9",
                "type": "user"
            },
            "summary": "Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.",
            "upvotes": 11,
            "discussionId": "69255d7016eb3a9f13103ab4",
            "ai_summary": "M-GRPO, an extension of Group Relative Policy Optimization for hierarchical multi-agent systems, improves stability and efficiency in tool-augmented reasoning tasks by aligning heterogeneous trajectories and decoupling agent training.",
            "ai_keywords": [
                "multi-agent systems",
                "large language model (LLM)",
                "Group Relative Policy Optimization",
                "hierarchical credit assignment",
                "trajectory-alignment scheme",
                "decoupled training pipeline",
                "shared store",
                "cross-server backpropagation",
                "tool-augmented reasoning tasks"
            ],
            "organization": {
                "_id": "68a4291ddb54c3044cf32600",
                "name": "AQ-MedAI",
                "fullname": "AQ",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6826eb9bf609dcc6d6c6df52/mOPPxrtf_etoO1rlLBtnV.jpeg"
            }
        },
        "publishedAt": "2025-11-17T07:06:30.000Z",
        "title": "Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO",
        "summary": "Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13288.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6826eb9bf609dcc6d6c6df52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6826eb9bf609dcc6d6c6df52/L_uK87dUkyuDy7iu1g_6C.jpeg",
            "fullname": "Erinyu",
            "name": "eerrr9",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "68a4291ddb54c3044cf32600",
            "name": "AQ-MedAI",
            "fullname": "AQ",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6826eb9bf609dcc6d6c6df52/mOPPxrtf_etoO1rlLBtnV.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.17405",
            "authors": [
                {
                    "_id": "692526e316eb3a9f1310398a",
                    "user": {
                        "_id": "657db8c97cf2a47c8cd790f3",
                        "avatarUrl": "/avatars/35d5dfae09b79f37d8c4d08ebc0bcd10.svg",
                        "isPro": false,
                        "fullname": "yes liu",
                        "user": "zhizhou57",
                        "type": "user"
                    },
                    "name": "Yesheng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T09:05:35.186Z",
                    "hidden": false
                },
                {
                    "_id": "692526e316eb3a9f1310398b",
                    "name": "Hao Li",
                    "hidden": false
                },
                {
                    "_id": "692526e316eb3a9f1310398c",
                    "user": {
                        "_id": "68b97ed277d4f089dfded859",
                        "avatarUrl": "/avatars/ff20a41afea3bd5fd310c3e050862843.svg",
                        "isPro": false,
                        "fullname": "Xu Haiyu",
                        "user": "lindaxu525",
                        "type": "user"
                    },
                    "name": "Haiyu Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T09:05:37.241Z",
                    "hidden": false
                },
                {
                    "_id": "692526e316eb3a9f1310398d",
                    "name": "Baoqi Pei",
                    "hidden": false
                },
                {
                    "_id": "692526e316eb3a9f1310398e",
                    "name": "Jiahao Wang",
                    "hidden": false
                },
                {
                    "_id": "692526e316eb3a9f1310398f",
                    "name": "Mingxuan Zhao",
                    "hidden": false
                },
                {
                    "_id": "692526e316eb3a9f13103990",
                    "name": "Jingshu Zheng",
                    "hidden": false
                },
                {
                    "_id": "692526e316eb3a9f13103991",
                    "name": "Zheqi He",
                    "hidden": false
                },
                {
                    "_id": "692526e316eb3a9f13103992",
                    "name": "JG Yao",
                    "hidden": false
                },
                {
                    "_id": "692526e316eb3a9f13103993",
                    "name": "Bowen Qin",
                    "hidden": false
                },
                {
                    "_id": "692526e316eb3a9f13103994",
                    "name": "Xi Yang",
                    "hidden": false
                },
                {
                    "_id": "692526e316eb3a9f13103995",
                    "name": "Jiajun Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-21T17:06:37.000Z",
            "submittedOnDailyAt": "2025-11-25T23:16:45.235Z",
            "title": "Beyond Multiple Choice: Verifiable OpenQA for Robust Vision-Language RFT",
            "submittedOnDailyBy": {
                "_id": "657db8c97cf2a47c8cd790f3",
                "avatarUrl": "/avatars/35d5dfae09b79f37d8c4d08ebc0bcd10.svg",
                "isPro": false,
                "fullname": "yes liu",
                "user": "zhizhou57",
                "type": "user"
            },
            "summary": "Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.",
            "upvotes": 9,
            "discussionId": "692526e316eb3a9f13103996",
            "ai_summary": "ReVeL, a framework that converts multiple-choice questions to open-form questions, improves data efficiency and robustness in fine-tuning multimodal language models and reveals score inflation in MCQA benchmarks.",
            "ai_keywords": [
                "reinforcement fine-tuning",
                "multimodal language models",
                "multiple-choice question answering",
                "ReVeL",
                "automatic verification",
                "open-form questions",
                "answer verification",
                "GRPO",
                "Qwen2.5-VL",
                "OpenQA",
                "score inflation",
                "judging accuracy"
            ]
        },
        "publishedAt": "2025-11-21T12:06:37.000Z",
        "title": "Beyond Multiple Choice: Verifiable OpenQA for Robust Vision-Language RFT",
        "summary": "Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17405.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "657db8c97cf2a47c8cd790f3",
            "avatarUrl": "/avatars/35d5dfae09b79f37d8c4d08ebc0bcd10.svg",
            "fullname": "yes liu",
            "name": "zhizhou57",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.18945",
            "authors": [
                {
                    "_id": "692573ed16eb3a9f13103b05",
                    "user": {
                        "_id": "63e5017caf814a2149ec772d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e5017caf814a2149ec772d/W1Vfd4Na5kTmxeHoChSIB.jpeg",
                        "isPro": false,
                        "fullname": "German G",
                        "user": "grgera",
                        "type": "user"
                    },
                    "name": "German Gritsai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T09:34:27.825Z",
                    "hidden": false
                },
                {
                    "_id": "692573ed16eb3a9f13103b06",
                    "name": "Megan Richards",
                    "hidden": false
                },
                {
                    "_id": "692573ed16eb3a9f13103b07",
                    "user": {
                        "_id": "641c5ae9dad248407395e3ab",
                        "avatarUrl": "/avatars/27c0bcac855c8683017443d6c713f3c8.svg",
                        "isPro": false,
                        "fullname": "Maxime M",
                        "user": "pie3636",
                        "type": "user"
                    },
                    "name": "Maxime Mloux",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T15:53:13.298Z",
                    "hidden": false
                },
                {
                    "_id": "692573ed16eb3a9f13103b08",
                    "name": "Kyunghyun Cho",
                    "hidden": false
                },
                {
                    "_id": "692573ed16eb3a9f13103b09",
                    "user": {
                        "_id": "6369394dd322a76e1ea4bdf6",
                        "avatarUrl": "/avatars/a4e5ab0167025fbbfc970d54630ce754.svg",
                        "isPro": false,
                        "fullname": "Maxime Peyrard",
                        "user": "peyrardm",
                        "type": "user"
                    },
                    "name": "Maxime Peyrard",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T12:10:03.931Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-24T09:55:28.000Z",
            "submittedOnDailyAt": "2025-11-25T06:50:17.276Z",
            "title": "MIST: Mutual Information Via Supervised Training",
            "submittedOnDailyBy": {
                "_id": "63e5017caf814a2149ec772d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e5017caf814a2149ec772d/W1Vfd4Na5kTmxeHoChSIB.jpeg",
                "isPro": false,
                "fullname": "German G",
                "user": "grgera",
                "type": "user"
            },
            "summary": "We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions.",
            "upvotes": 8,
            "discussionId": "692573ed16eb3a9f13103b0a",
            "githubRepo": "https://github.com/grgera/mist",
            "ai_summary": "A data-driven neural network approach estimates mutual information using a meta-dataset of synthetic distributions, offering flexibility, efficiency, and uncertainty quantification.",
            "ai_keywords": [
                "mutual information",
                "MI estimator",
                "neural network",
                "MIST",
                "meta-dataset",
                "two-dimensional attention",
                "permutation invariance",
                "quantile regression",
                "quantile-based intervals",
                "bootstrap-based confidence intervals",
                "normalizing flows"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-11-24T04:55:28.000Z",
        "title": "MIST: Mutual Information Via Supervised Training",
        "summary": "We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18945.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63e5017caf814a2149ec772d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e5017caf814a2149ec772d/W1Vfd4Na5kTmxeHoChSIB.jpeg",
            "fullname": "German G",
            "name": "grgera",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.16249",
            "authors": [
                {
                    "_id": "6925262216eb3a9f13103981",
                    "name": "Zihao Liu",
                    "hidden": false
                },
                {
                    "_id": "6925262216eb3a9f13103982",
                    "name": "Zunnan Xu",
                    "hidden": false
                },
                {
                    "_id": "6925262216eb3a9f13103983",
                    "name": "Shi Shu",
                    "hidden": false
                },
                {
                    "_id": "6925262216eb3a9f13103984",
                    "name": "Jun Zhou",
                    "hidden": false
                },
                {
                    "_id": "6925262216eb3a9f13103985",
                    "name": "Ruicheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6925262216eb3a9f13103986",
                    "name": "Zhenchao Tang",
                    "hidden": false
                },
                {
                    "_id": "6925262216eb3a9f13103987",
                    "name": "Xiu Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T11:27:21.000Z",
            "submittedOnDailyAt": "2025-11-25T01:15:23.165Z",
            "title": "Controllable Layer Decomposition for Reversible Multi-Layer Image Generation",
            "submittedOnDailyBy": {
                "_id": "6481523b3fb124fc9850afed",
                "avatarUrl": "/avatars/ddde178c88713662800aafd2343647a4.svg",
                "isPro": false,
                "fullname": "kkakkkka",
                "user": "kkakkkka",
                "type": "user"
            },
            "summary": "This work presents Controllable Layer Decomposition (CLD), a method for achieving fine-grained and controllable multi-layer separation of raster images. In practical workflows, designers typically generate and edit each RGBA layer independently before compositing them into a final raster image. However, this process is irreversible: once composited, layer-level editing is no longer possible. Existing methods commonly rely on image matting and inpainting, but remain limited in controllability and segmentation precision. To address these challenges, we propose two key modules: LayerDecompose-DiT (LD-DiT), which decouples image elements into distinct layers and enables fine-grained control; and Multi-Layer Conditional Adapter (MLCA), which injects target image information into multi-layer tokens to achieve precise conditional generation. To enable a comprehensive evaluation, we build a new benchmark and introduce tailored evaluation metrics. Experimental results show that CLD consistently outperforms existing methods in both decomposition quality and controllability. Furthermore, the separated layers produced by CLD can be directly manipulated in commonly used design tools such as PowerPoint, highlighting its practical value and applicability in real-world creative workflows.",
            "upvotes": 6,
            "discussionId": "6925262216eb3a9f13103988",
            "projectPage": "https://monkek123king.github.io/CLD_page/",
            "githubRepo": "https://github.com/monkek123King/CLD",
            "ai_summary": "Controllable Layer Decomposition (CLD) enables fine-grained and controllable separation of raster images into RGBA layers, surpassing existing methods in quality and practical use.",
            "ai_keywords": [
                "Controllable Layer Decomposition",
                "CLD",
                "LayerDecompose-DiT",
                "LD-DiT",
                "Multi-Layer Conditional Adapter",
                "MLCA",
                "image matting",
                "inpainting",
                "decomposition quality",
                "conditional generation",
                "tailored evaluation metrics"
            ],
            "githubStars": 8,
            "organization": {
                "_id": "628735cbc83a2d6ab8d14a66",
                "name": "Tsinghua",
                "fullname": "Tsinghua University"
            }
        },
        "publishedAt": "2025-11-20T06:27:21.000Z",
        "title": "Controllable Layer Decomposition for Reversible Multi-Layer Image Generation",
        "summary": "This work presents Controllable Layer Decomposition (CLD), a method for achieving fine-grained and controllable multi-layer separation of raster images. In practical workflows, designers typically generate and edit each RGBA layer independently before compositing them into a final raster image. However, this process is irreversible: once composited, layer-level editing is no longer possible. Existing methods commonly rely on image matting and inpainting, but remain limited in controllability and segmentation precision. To address these challenges, we propose two key modules: LayerDecompose-DiT (LD-DiT), which decouples image elements into distinct layers and enables fine-grained control; and Multi-Layer Conditional Adapter (MLCA), which injects target image information into multi-layer tokens to achieve precise conditional generation. To enable a comprehensive evaluation, we build a new benchmark and introduce tailored evaluation metrics. Experimental results show that CLD consistently outperforms existing methods in both decomposition quality and controllability. Furthermore, the separated layers produced by CLD can be directly manipulated in commonly used design tools such as PowerPoint, highlighting its practical value and applicability in real-world creative workflows.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16249.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6481523b3fb124fc9850afed",
            "avatarUrl": "/avatars/ddde178c88713662800aafd2343647a4.svg",
            "fullname": "kkakkkka",
            "name": "kkakkkka",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "628735cbc83a2d6ab8d14a66",
            "name": "Tsinghua",
            "fullname": "Tsinghua University"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.18373",
            "authors": [
                {
                    "_id": "6925336216eb3a9f13103a5d",
                    "name": "Xiyang Wu",
                    "hidden": false
                },
                {
                    "_id": "6925336216eb3a9f13103a5e",
                    "name": "Zongxia Li",
                    "hidden": false
                },
                {
                    "_id": "6925336216eb3a9f13103a5f",
                    "name": "Jihui Jin",
                    "hidden": false
                },
                {
                    "_id": "6925336216eb3a9f13103a60",
                    "name": "Guangyao Shi",
                    "hidden": false
                },
                {
                    "_id": "6925336216eb3a9f13103a61",
                    "name": "Gouthaman KV",
                    "hidden": false
                },
                {
                    "_id": "6925336216eb3a9f13103a62",
                    "name": "Vishnu Raj",
                    "hidden": false
                },
                {
                    "_id": "6925336216eb3a9f13103a63",
                    "name": "Nilotpal Sinha",
                    "hidden": false
                },
                {
                    "_id": "6925336216eb3a9f13103a64",
                    "name": "Jingxi Chen",
                    "hidden": false
                },
                {
                    "_id": "6925336216eb3a9f13103a65",
                    "name": "Fan Du",
                    "hidden": false
                },
                {
                    "_id": "6925336216eb3a9f13103a66",
                    "name": "Dinesh Manocha",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64ea62f918d79efd533c93fe/4cDs65YDGfa_Q8AFZoryW.png"
            ],
            "publishedAt": "2025-11-23T09:43:44.000Z",
            "submittedOnDailyAt": "2025-11-25T02:13:07.225Z",
            "title": "MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "64ea62f918d79efd533c93fe",
                "avatarUrl": "/avatars/9985a789ce11b788de2cba12adfb72fc.svg",
                "isPro": true,
                "fullname": "Xiyang Wu",
                "user": "wuxiyang",
                "type": "user"
            },
            "summary": "Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach.",
            "upvotes": 5,
            "discussionId": "6925336216eb3a9f13103a67",
            "ai_summary": "A method that enhances vision language models with spatial-temporal signals and motion tracking improves their performance on physics-driven video reasoning tasks.",
            "ai_keywords": [
                "Vision Language Models",
                "VLMs",
                "physics-driven reasoning",
                "motion dynamics",
                "spatial interactions",
                "MASS-Bench",
                "real-world videos",
                "AIGC videos",
                "video question-answering",
                "visual detections",
                "sub-segment grounding",
                "3D motion tracking",
                "MASS",
                "model-agnostic method",
                "depth-based 3D encoding",
                "visual grounding",
                "motion tracker",
                "object dynamics",
                "reinforcement fine-tuning",
                "cross-modal alignment",
                "physics reasoning",
                "comprehension",
                "Gemini-2.5-Flash"
            ],
            "organization": {
                "_id": "68b3c3bbc375e05b059370b2",
                "name": "UMCP",
                "fullname": "University of Maryland College Park",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"
            }
        },
        "publishedAt": "2025-11-23T04:43:44.000Z",
        "title": "MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models",
        "summary": "Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64ea62f918d79efd533c93fe/4cDs65YDGfa_Q8AFZoryW.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18373.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64ea62f918d79efd533c93fe",
            "avatarUrl": "/avatars/9985a789ce11b788de2cba12adfb72fc.svg",
            "fullname": "Xiyang Wu",
            "name": "wuxiyang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "68b3c3bbc375e05b059370b2",
            "name": "UMCP",
            "fullname": "University of Maryland College Park",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.19314",
            "authors": [
                {
                    "_id": "69251dc116eb3a9f1310393a",
                    "name": "Jaewoo Lee",
                    "hidden": false
                },
                {
                    "_id": "69251dc116eb3a9f1310393b",
                    "name": "Archiki Prasad",
                    "hidden": false
                },
                {
                    "_id": "69251dc116eb3a9f1310393c",
                    "name": "Justin Chih-Yao Chen",
                    "hidden": false
                },
                {
                    "_id": "69251dc116eb3a9f1310393d",
                    "name": "Zaid Khan",
                    "hidden": false
                },
                {
                    "_id": "69251dc116eb3a9f1310393e",
                    "name": "Elias Stengel-Eskin",
                    "hidden": false
                },
                {
                    "_id": "69251dc116eb3a9f1310393f",
                    "name": "Mohit Bansal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-24T17:09:43.000Z",
            "submittedOnDailyAt": "2025-11-25T00:40:23.792Z",
            "title": "PRInTS: Reward Modeling for Long-Horizon Information Seeking",
            "submittedOnDailyBy": {
                "_id": "607aeae5d2cd8c150e6ae074",
                "avatarUrl": "/avatars/a087743b98b6fe2181283a9610db4ec4.svg",
                "isPro": false,
                "fullname": "Archiki Prasad",
                "user": "archiki",
                "type": "user"
            },
            "summary": "Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.",
            "upvotes": 4,
            "discussionId": "69251dc116eb3a9f13103940",
            "ai_summary": "PRInTS, a generative process reward model, enhances information-seeking abilities in AI agents by providing dense scoring and trajectory summarization, outperforming existing models with smaller backbones.",
            "ai_keywords": [
                "process reward models",
                "PRMs",
                "dense scoring",
                "tool interactions",
                "tool call informativeness",
                "trajectory summarization",
                "FRAMES",
                "GAIA",
                "WebWalkerQA",
                "best-of-n sampling",
                "open-source models",
                "specialized agents",
                "reward modeling"
            ]
        },
        "publishedAt": "2025-11-24T12:09:43.000Z",
        "title": "PRInTS: Reward Modeling for Long-Horizon Information Seeking",
        "summary": "Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19314.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "607aeae5d2cd8c150e6ae074",
            "avatarUrl": "/avatars/a087743b98b6fe2181283a9610db4ec4.svg",
            "fullname": "Archiki Prasad",
            "name": "archiki",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.16301",
            "authors": [
                {
                    "_id": "6925b8d24b7b0d870b182711",
                    "name": "Minseok Seo",
                    "hidden": false
                },
                {
                    "_id": "6925b8d24b7b0d870b182712",
                    "name": "Mark Hamilton",
                    "hidden": false
                },
                {
                    "_id": "6925b8d24b7b0d870b182713",
                    "name": "Changick Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T12:27:53.000Z",
            "submittedOnDailyAt": "2025-11-25T11:44:25.614Z",
            "title": "Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling",
            "submittedOnDailyBy": {
                "_id": "679c6c454544b95cb91c8aec",
                "avatarUrl": "/avatars/b69f7c6da667cbe739200ce722c6fdbb.svg",
                "isPro": true,
                "fullname": "minseokseo",
                "user": "minseok96",
                "type": "user"
            },
            "summary": "We present Upsample Anything, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only approx0.419 s per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling. Project page: https://seominseok0429.github.io/Upsample-Anything/{https://seominseok0429.github.io/Upsample-Anything/}",
            "upvotes": 3,
            "discussionId": "6925b8d24b7b0d870b182714",
            "ai_summary": "Upsample Anything is a lightweight test-time optimization framework that enhances low-resolution features to high-resolution outputs without training, using an anisotropic Gaussian kernel for precise reconstruction in tasks like semantic segmentation and depth estimation.",
            "ai_keywords": [
                "test-time optimization",
                "Vision Foundation Models",
                "feature upsampling",
                "anisotropic Gaussian kernel",
                "Gaussian Splatting",
                "Joint Bilateral Upsampling",
                "semantic segmentation",
                "depth estimation",
                "probability map upsampling"
            ]
        },
        "publishedAt": "2025-11-20T07:27:53.000Z",
        "title": "Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling",
        "summary": "We present Upsample Anything, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only approx0.419 s per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling. Project page: https://seominseok0429.github.io/Upsample-Anything/{https://seominseok0429.github.io/Upsample-Anything/}",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16301.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "679c6c454544b95cb91c8aec",
            "avatarUrl": "/avatars/b69f7c6da667cbe739200ce722c6fdbb.svg",
            "fullname": "minseokseo",
            "name": "minseok96",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.16166",
            "authors": [
                {
                    "_id": "6925cb4a4b7b0d870b18275f",
                    "name": "Zeting Liu",
                    "hidden": false
                },
                {
                    "_id": "6925cb4a4b7b0d870b182760",
                    "name": "Zida Yang",
                    "hidden": false
                },
                {
                    "_id": "6925cb4a4b7b0d870b182761",
                    "user": {
                        "_id": "64ec877bb93654d4ca5c92e9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                        "isPro": false,
                        "fullname": "Zeyu Zhang",
                        "user": "SteveZeyuZhang",
                        "type": "user"
                    },
                    "name": "Zeyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T15:53:07.061Z",
                    "hidden": false
                },
                {
                    "_id": "6925cb4a4b7b0d870b182762",
                    "name": "Hao Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T09:08:33.000Z",
            "submittedOnDailyAt": "2025-11-25T12:59:58.784Z",
            "title": "EvoVLA: Self-Evolving Vision-Language-Action Model",
            "submittedOnDailyBy": {
                "_id": "64ec877bb93654d4ca5c92e9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                "isPro": false,
                "fullname": "Zeyu Zhang",
                "user": "SteveZeyuZhang",
                "type": "user"
            },
            "summary": "Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.",
            "upvotes": 3,
            "discussionId": "6925cb4a4b7b0d870b182763",
            "projectPage": "https://aigeeksgroup.github.io/EvoVLA/",
            "githubRepo": "https://github.com/AIGeeksGroup/EvoVLA",
            "ai_summary": "EvoVLA, a self-supervised VLA framework, enhances long-horizon robotic manipulation by addressing stage hallucination through triplet contrastive learning, pose-based exploration, and long-horizon memory, achieving improved success rates and sample efficiency on both simulated and real-world tasks.",
            "ai_keywords": [
                "Stage-Aligned Reward",
                "triplet contrastive learning",
                "Gemini-generated hard negatives",
                "Pose-Based Object Exploration",
                "relative object-gripper pose",
                "Long-Horizon Memory",
                "selective context retention",
                "gated fusion",
                "Discoverse-L",
                "OpenVLA-OFT",
                "sim-to-real transfer"
            ],
            "githubStars": 9
        },
        "publishedAt": "2025-11-20T04:08:33.000Z",
        "title": "EvoVLA: Self-Evolving Vision-Language-Action Model",
        "summary": "Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16166.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "fullname": "Zeyu Zhang",
            "name": "SteveZeyuZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.19428",
            "authors": [
                {
                    "_id": "69252a5216eb3a9f131039d0",
                    "name": "Shangyuan Tong",
                    "hidden": false
                },
                {
                    "_id": "69252a5216eb3a9f131039d1",
                    "name": "Nanye Ma",
                    "hidden": false
                },
                {
                    "_id": "69252a5216eb3a9f131039d2",
                    "name": "Saining Xie",
                    "hidden": false
                },
                {
                    "_id": "69252a5216eb3a9f131039d3",
                    "name": "Tommi Jaakkola",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-24T18:58:55.000Z",
            "submittedOnDailyAt": "2025-11-25T01:32:41.763Z",
            "title": "Flow Map Distillation Without Data",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.",
            "upvotes": 2,
            "discussionId": "69252a5216eb3a9f131039d4",
            "ai_summary": "A data-free framework that samples from the prior distribution surpasses data-based alternatives in flow map distillation, achieving state-of-the-art fidelity with minimal sampling steps.",
            "ai_keywords": [
                "flow models",
                "iterative sampling",
                "flow map distillation",
                "Teacher-Data Mismatch",
                "prior distribution",
                "sampling path",
                "FID",
                "ImageNet"
            ]
        },
        "publishedAt": "2025-11-24T13:58:55.000Z",
        "title": "Flow Map Distillation Without Data",
        "summary": "State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19428.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 170
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.18922",
            "authors": [
                {
                    "_id": "6925cee24b7b0d870b182778",
                    "name": "Zhenxing Mi",
                    "hidden": false
                },
                {
                    "_id": "6925cee24b7b0d870b182779",
                    "name": "Yuxin Wang",
                    "hidden": false
                },
                {
                    "_id": "6925cee24b7b0d870b18277a",
                    "name": "Dan Xu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6354bda206d707b33249c4c2/xx0ZGu22MXJB7yx4zoNk-.mp4"
            ],
            "publishedAt": "2025-11-24T09:31:23.000Z",
            "submittedOnDailyAt": "2025-11-25T14:43:07.437Z",
            "title": "One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control",
            "submittedOnDailyBy": {
                "_id": "6354bda206d707b33249c4c2",
                "avatarUrl": "/avatars/bbd9f76274ac52214df92084d50bc7b5.svg",
                "isPro": false,
                "fullname": "Zhenxing Mi",
                "user": "Mifucius",
                "type": "user"
            },
            "summary": "We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D",
            "upvotes": 2,
            "discussionId": "6925cee24b7b0d870b18277b",
            "projectPage": "https://mizhenxing.github.io/One4D",
            "githubRepo": "https://github.com/MiZhenxing/One4D",
            "ai_summary": "One4D is a unified framework for 4D generation and reconstruction that uses a Unified Masked Conditioning mechanism and Decoupled LoRA Control to generate synchronized RGB frames and pointmaps from varying sparsities of input.",
            "ai_keywords": [
                "Unified Masked Conditioning (UMC)",
                "diffusion finetuning",
                "depthmap",
                "pointmap",
                "Decoupled LoRA Control (DLC)",
                "LoRA adapters",
                "RGB frames",
                "pointmaps",
                "4D generation",
                "4D reconstruction",
                "synthetic datasets",
                "real 4D datasets",
                "video diffusion models"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-11-24T04:31:23.000Z",
        "title": "One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control",
        "summary": "We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6354bda206d707b33249c4c2/xx0ZGu22MXJB7yx4zoNk-.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18922.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6354bda206d707b33249c4c2",
            "avatarUrl": "/avatars/bbd9f76274ac52214df92084d50bc7b5.svg",
            "fullname": "Zhenxing Mi",
            "name": "Mifucius",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.17792",
            "authors": [
                {
                    "_id": "692529de16eb3a9f131039be",
                    "name": "Dingrui Wang",
                    "hidden": false
                },
                {
                    "_id": "692529de16eb3a9f131039bf",
                    "name": "Hongyuan Ye",
                    "hidden": false
                },
                {
                    "_id": "692529de16eb3a9f131039c0",
                    "name": "Zhihao Liang",
                    "hidden": false
                },
                {
                    "_id": "692529de16eb3a9f131039c1",
                    "name": "Zhexiao Sun",
                    "hidden": false
                },
                {
                    "_id": "692529de16eb3a9f131039c2",
                    "name": "Zhaowei Lu",
                    "hidden": false
                },
                {
                    "_id": "692529de16eb3a9f131039c3",
                    "name": "Yuchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "692529de16eb3a9f131039c4",
                    "name": "Yuyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "692529de16eb3a9f131039c5",
                    "name": "Yuan Gao",
                    "hidden": false
                },
                {
                    "_id": "692529de16eb3a9f131039c6",
                    "name": "Marvin Seegert",
                    "hidden": false
                },
                {
                    "_id": "692529de16eb3a9f131039c7",
                    "name": "Finn Schfer",
                    "hidden": false
                },
                {
                    "_id": "692529de16eb3a9f131039c8",
                    "name": "Haotong Qin",
                    "hidden": false
                },
                {
                    "_id": "692529de16eb3a9f131039c9",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "692529de16eb3a9f131039ca",
                    "name": "Luigi Palmieri",
                    "hidden": false
                },
                {
                    "_id": "692529de16eb3a9f131039cb",
                    "name": "Felix Jahncke",
                    "hidden": false
                },
                {
                    "_id": "692529de16eb3a9f131039cc",
                    "name": "Mattia Piccinini",
                    "hidden": false
                },
                {
                    "_id": "692529de16eb3a9f131039cd",
                    "name": "Johannes Betz",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Cl6TTW-8t_j4VWw0YNaDD.mp4"
            ],
            "publishedAt": "2025-11-21T21:36:02.000Z",
            "submittedOnDailyAt": "2025-11-25T01:30:39.799Z",
            "title": "Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.",
            "upvotes": 2,
            "discussionId": "692529de16eb3a9f131039ce",
            "projectPage": "https://target-bench.github.io/",
            "githubRepo": "https://github.com/TUM-AVS/target-bench",
            "ai_summary": "Target-Bench evaluates state-of-the-shelf world models on mapless path planning tasks, showing significant improvement through fine-tuning.",
            "ai_keywords": [
                "world models",
                "Target-Bench",
                "robot-collected video sequences",
                "SLAM-based ground truth trajectories",
                "camera motion recovery",
                "trajectory accuracy",
                "directional consistency",
                "Sora 2",
                "Veo 3.1",
                "Wan series",
                "fine-tuning",
                "5B-parameter model"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-11-21T16:36:02.000Z",
        "title": "Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?",
        "summary": "While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Cl6TTW-8t_j4VWw0YNaDD.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17792.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 170
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.16397",
            "authors": [
                {
                    "_id": "6923c67eb5612535ed95590c",
                    "name": "Ren Ma",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed95590d",
                    "name": "Jiantao Qiu",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed95590e",
                    "name": "Chao Xu",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed95590f",
                    "name": "Pei Chu",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed955910",
                    "name": "Kaiwen Liu",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed955911",
                    "name": "Pengli Ren",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed955912",
                    "name": "Yuan Qu",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed955913",
                    "name": "Jiahui Peng",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed955914",
                    "name": "Linfeng Hou",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed955915",
                    "name": "Mengjie Liu",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed955916",
                    "name": "Lindong Lu",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed955917",
                    "name": "Wenchang Ning",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed955918",
                    "name": "Jia Yu",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed955919",
                    "name": "Rui Min",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed95591a",
                    "name": "Jin Shi",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed95591b",
                    "name": "Haojiong Chen",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed95591c",
                    "name": "Peng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed95591d",
                    "name": "Wenjian Zhang",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed95591e",
                    "name": "Qian Jiang",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed95591f",
                    "name": "Zengjie Hu",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed955920",
                    "name": "Guoqiang Yang",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed955921",
                    "name": "Zhenxiang Li",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed955922",
                    "name": "Fukai Shang",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed955923",
                    "name": "Zhongying Tu",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed955924",
                    "name": "Wentao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed955925",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "6923c67eb5612535ed955926",
                    "name": "Conghui He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-20T14:15:23.000Z",
            "submittedOnDailyAt": "2025-11-25T07:09:22.609Z",
            "title": "AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser",
            "submittedOnDailyBy": {
                "_id": "5f17f0a0925b9863e28ad517",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f17f0a0925b9863e28ad517/fXIY5i9RLsIa1v3CCuVtt.jpeg",
                "isPro": true,
                "fullname": "Victor Mustar",
                "user": "victor",
                "type": "user"
            },
            "summary": "While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model. Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\\% ROUGE-N F1 compared to Trafilatura's 63.6\\%, with exceptional structured element preservation (90.9\\% for code blocks, 94.0\\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\\% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction.",
            "upvotes": 2,
            "discussionId": "6923c67eb5612535ed955927",
            "ai_summary": "A novel extraction pipeline using a language model improves web data quality, significantly enhancing the performance of large language models trained on extracted corpora.",
            "ai_keywords": [
                "sequence labeling",
                "language model",
                "semantic understanding",
                "two-stage formatting pipeline",
                "Markdown",
                "ROUGE-N F1",
                "structured element preservation",
                "multilingual corpus",
                "pretraining experiments",
                "benchmarks",
                "AI-ready Common Crawl",
                "RefinedWeb",
                "FineWeb",
                "HTML extraction"
            ],
            "organization": {
                "_id": "66ce9d1f5e180b9b9c8e6f31",
                "name": "opendatalab",
                "fullname": "OpenDataLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"
            }
        },
        "publishedAt": "2025-11-20T09:15:23.000Z",
        "title": "AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser",
        "summary": "While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model. Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\\% ROUGE-N F1 compared to Trafilatura's 63.6\\%, with exceptional structured element preservation (90.9\\% for code blocks, 94.0\\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\\% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16397.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5f17f0a0925b9863e28ad517",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f17f0a0925b9863e28ad517/fXIY5i9RLsIa1v3CCuVtt.jpeg",
            "fullname": "Victor Mustar",
            "name": "victor",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 4885
        },
        "organization": {
            "_id": "66ce9d1f5e180b9b9c8e6f31",
            "name": "opendatalab",
            "fullname": "OpenDataLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.19166",
            "authors": [
                {
                    "_id": "6925b8904b7b0d870b18270b",
                    "name": "Samantha Dies",
                    "hidden": false
                },
                {
                    "_id": "6925b8904b7b0d870b18270c",
                    "name": "Courtney Maynard",
                    "hidden": false
                },
                {
                    "_id": "6925b8904b7b0d870b18270d",
                    "user": {
                        "_id": "667092a1abdd1ea72bbd5beb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5DWPaDwCEo7_l97gcirYe.png",
                        "isPro": false,
                        "fullname": "Germans Savcisens",
                        "user": "carlomarxx",
                        "type": "user"
                    },
                    "name": "Germans Savcisens",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T15:53:11.665Z",
                    "hidden": false
                },
                {
                    "_id": "6925b8904b7b0d870b18270e",
                    "name": "Tina Eliassi-Rad",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-24T14:28:50.000Z",
            "submittedOnDailyAt": "2025-11-25T11:40:35.372Z",
            "title": "Representational Stability of Truth in Large Language Models",
            "submittedOnDailyBy": {
                "_id": "667092a1abdd1ea72bbd5beb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5DWPaDwCEo7_l97gcirYe.png",
                "isPro": false,
                "fullname": "Germans Savcisens",
                "user": "carlomarxx",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are widely used for factual tasks such as \"What treats asthma?\" or \"What is the capital of Latvia?\". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to 40% flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes (leq 8.2%). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.",
            "upvotes": 1,
            "discussionId": "6925b8914b7b0d870b18270f",
            "ai_summary": "LLMs exhibit varying levels of stability in encoding truth representations, influenced more by epistemic familiarity than linguistic form, as assessed through perturbation analysis of their activations.",
            "ai_keywords": [
                "large language models",
                "representational stability",
                "veracity representations",
                "linear probe",
                "activations",
                "truth judgements",
                "unfamiliar neither statements",
                "familiar neither statements",
                "epistemic familiarity",
                "semantic uncertainty"
            ]
        },
        "publishedAt": "2025-11-24T09:28:50.000Z",
        "title": "Representational Stability of Truth in Large Language Models",
        "summary": "Large language models (LLMs) are widely used for factual tasks such as \"What treats asthma?\" or \"What is the capital of Latvia?\". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to 40% flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes (leq 8.2%). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19166.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "667092a1abdd1ea72bbd5beb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5DWPaDwCEo7_l97gcirYe.png",
            "fullname": "Germans Savcisens",
            "name": "carlomarxx",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.19319",
            "authors": [
                {
                    "_id": "6925284016eb3a9f131039a9",
                    "user": {
                        "_id": "62ebd791fee90fca4742ead8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ebd791fee90fca4742ead8/8-iJYS9Wk41l6JTGtJrNf.jpeg",
                        "isPro": false,
                        "fullname": "levon dang",
                        "user": "levondang",
                        "type": "user"
                    },
                    "name": "Lingwei Dang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T09:05:15.579Z",
                    "hidden": false
                },
                {
                    "_id": "6925284016eb3a9f131039aa",
                    "name": "Zonghan Li",
                    "hidden": false
                },
                {
                    "_id": "6925284016eb3a9f131039ab",
                    "user": {
                        "_id": "6654763c965ea394ee475878",
                        "avatarUrl": "/avatars/92e1eb4c88f209646cc03fc269025f0f.svg",
                        "isPro": false,
                        "fullname": "Li Juntong",
                        "user": "qzfm",
                        "type": "user"
                    },
                    "name": "Juntong Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:36:22.719Z",
                    "hidden": false
                },
                {
                    "_id": "6925284016eb3a9f131039ac",
                    "name": "Hongwen Zhang",
                    "hidden": false
                },
                {
                    "_id": "6925284016eb3a9f131039ad",
                    "name": "Liang An",
                    "hidden": false
                },
                {
                    "_id": "6925284016eb3a9f131039ae",
                    "user": {
                        "_id": "62e14dbe4db2175cd2735a80",
                        "avatarUrl": "/avatars/e6385dcedcb97e1b36281b49210321aa.svg",
                        "isPro": false,
                        "fullname": "Yebin Liu",
                        "user": "YebinLiu",
                        "type": "user"
                    },
                    "name": "Yebin Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:36:04.701Z",
                    "hidden": false
                },
                {
                    "_id": "6925284016eb3a9f131039af",
                    "name": "Qingyao Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-24T17:14:19.000Z",
            "submittedOnDailyAt": "2025-11-25T01:30:42.586Z",
            "title": "SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis",
            "submittedOnDailyBy": {
                "_id": "62ebd791fee90fca4742ead8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ebd791fee90fca4742ead8/8-iJYS9Wk41l6JTGtJrNf.jpeg",
                "isPro": false,
                "fullname": "levon dang",
                "user": "levondang",
                "type": "user"
            },
            "summary": "Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.",
            "upvotes": 0,
            "discussionId": "6925284016eb3a9f131039b0",
            "projectPage": "https://droliven.github.io/SyncMV4D/",
            "ai_summary": "SyncMV4D generates realistic and consistent multi-view 3D Hand-Object Interaction videos and 4D motions by integrating visual priors, motion dynamics, and multi-view geometry.",
            "ai_keywords": [
                "Hand-Object Interaction",
                "HOI",
                "SyncMV4D",
                "Multi-view Joint Diffusion",
                "MJD",
                "Diffusion Points Aligner",
                "DPA",
                "diffusion denoising",
                "4D motions",
                "multi-view consistency",
                "visual realism",
                "motion plausibility"
            ]
        },
        "publishedAt": "2025-11-24T12:14:19.000Z",
        "title": "SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis",
        "summary": "Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19319.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62ebd791fee90fca4742ead8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ebd791fee90fca4742ead8/8-iJYS9Wk41l6JTGtJrNf.jpeg",
            "fullname": "levon dang",
            "name": "levondang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.18047",
            "authors": [
                {
                    "_id": "692521e016eb3a9f13103957",
                    "name": "Oren Barkan",
                    "hidden": false
                },
                {
                    "_id": "692521e016eb3a9f13103958",
                    "user": {
                        "_id": "67150b7f1798d13be9550412",
                        "avatarUrl": "/avatars/bfb5f1b2f1b2355155b491cd7f0af825.svg",
                        "isPro": false,
                        "fullname": "Yahlly Schein",
                        "user": "Yahlly21",
                        "type": "user"
                    },
                    "name": "Yahlly Schein",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:35:29.071Z",
                    "hidden": false
                },
                {
                    "_id": "692521e016eb3a9f13103959",
                    "user": {
                        "_id": "662a80358a721ebd0b4f358b",
                        "avatarUrl": "/avatars/5be5a3c8b13f6f663206a19d0525c18e.svg",
                        "isPro": false,
                        "fullname": "Yehonatan Elisha",
                        "user": "Yoniel",
                        "type": "user"
                    },
                    "name": "Yehonatan Elisha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:34:58.953Z",
                    "hidden": false
                },
                {
                    "_id": "692521e016eb3a9f1310395a",
                    "name": "Veronika Bogina",
                    "hidden": false
                },
                {
                    "_id": "692521e016eb3a9f1310395b",
                    "user": {
                        "_id": "6655c15ba57d0c383610e3e6",
                        "avatarUrl": "/avatars/457f8b8731fef7abfdc3555768b038e7.svg",
                        "isPro": false,
                        "fullname": "Mikhail Baklanov",
                        "user": "mikhailbaklanov",
                        "type": "user"
                    },
                    "name": "Mikhail Baklanov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:35:17.142Z",
                    "hidden": false
                },
                {
                    "_id": "692521e016eb3a9f1310395c",
                    "user": {
                        "_id": "669536cad4ca2767b9a9da91",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/itE8uwV6pji4yR38F3zCP.png",
                        "isPro": false,
                        "fullname": "Noam Koenigstein",
                        "user": "deltalab-noam",
                        "type": "user"
                    },
                    "name": "Noam Koenigstein",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:35:09.571Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-22T12:59:04.000Z",
            "submittedOnDailyAt": "2025-11-25T05:25:33.165Z",
            "title": "Fidelity-Aware Recommendation Explanations via Stochastic Path Integration",
            "submittedOnDailyBy": {
                "_id": "662a80358a721ebd0b4f358b",
                "avatarUrl": "/avatars/5be5a3c8b13f6f663206a19d0525c18e.svg",
                "isPro": false,
                "fullname": "Yehonatan Elisha",
                "user": "Yoniel",
                "type": "user"
            },
            "summary": "Explanation fidelity, which measures how accurately an explanation reflects a model's true reasoning, remains critically underexplored in recommender systems. We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), a model-agnostic approach that adapts path-integration techniques to the sparse and implicit nature of recommendation data. To overcome the limitations of prior methods, SPINRec employs stochastic baseline sampling: instead of integrating from a fixed or unrealistic baseline, it samples multiple plausible user profiles from the empirical data distribution and selects the most faithful attribution path. This design captures the influence of both observed and unobserved interactions, yielding more stable and personalized explanations. We conduct the most comprehensive fidelity evaluation to date across three models (MF, VAE, NCF), three datasets (ML1M, Yahoo! Music, Pinterest), and a suite of counterfactual metrics, including AUC-based perturbation curves and fixed-length diagnostics. SPINRec consistently outperforms all baselines, establishing a new benchmark for faithful explainability in recommendation. Code and evaluation tools are publicly available at https://github.com/DeltaLabTLV/SPINRec.",
            "upvotes": 0,
            "discussionId": "692521e016eb3a9f1310395d",
            "ai_summary": "SPINRec, a model-agnostic approach, enhances explanation fidelity in recommender systems by using stochastic baseline sampling and path-integration techniques to capture both observed and unobserved interactions.",
            "ai_keywords": [
                "explanation fidelity",
                "path-integration",
                "stochastic baseline sampling",
                "user profiles",
                "empirical data distribution",
                "attribution path",
                "counterfactual metrics",
                "AUC-based perturbation curves",
                "fixed-length diagnostics",
                "MF",
                "VAE",
                "NCF",
                "ML1M",
                "Yahoo! Music",
                "Pinterest"
            ]
        },
        "publishedAt": "2025-11-22T07:59:04.000Z",
        "title": "Fidelity-Aware Recommendation Explanations via Stochastic Path Integration",
        "summary": "Explanation fidelity, which measures how accurately an explanation reflects a model's true reasoning, remains critically underexplored in recommender systems. We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), a model-agnostic approach that adapts path-integration techniques to the sparse and implicit nature of recommendation data. To overcome the limitations of prior methods, SPINRec employs stochastic baseline sampling: instead of integrating from a fixed or unrealistic baseline, it samples multiple plausible user profiles from the empirical data distribution and selects the most faithful attribution path. This design captures the influence of both observed and unobserved interactions, yielding more stable and personalized explanations. We conduct the most comprehensive fidelity evaluation to date across three models (MF, VAE, NCF), three datasets (ML1M, Yahoo! Music, Pinterest), and a suite of counterfactual metrics, including AUC-based perturbation curves and fixed-length diagnostics. SPINRec consistently outperforms all baselines, establishing a new benchmark for faithful explainability in recommendation. Code and evaluation tools are publicly available at https://github.com/DeltaLabTLV/SPINRec.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18047.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "662a80358a721ebd0b4f358b",
            "avatarUrl": "/avatars/5be5a3c8b13f6f663206a19d0525c18e.svg",
            "fullname": "Yehonatan Elisha",
            "name": "Yoniel",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.18024",
            "authors": [
                {
                    "_id": "6925312216eb3a9f13103a40",
                    "name": "Dor Arviv",
                    "hidden": false
                },
                {
                    "_id": "6925312216eb3a9f13103a41",
                    "user": {
                        "_id": "662a80358a721ebd0b4f358b",
                        "avatarUrl": "/avatars/5be5a3c8b13f6f663206a19d0525c18e.svg",
                        "isPro": false,
                        "fullname": "Yehonatan Elisha",
                        "user": "Yoniel",
                        "type": "user"
                    },
                    "name": "Yehonatan Elisha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:34:23.580Z",
                    "hidden": false
                },
                {
                    "_id": "6925312216eb3a9f13103a42",
                    "name": "Oren Barkan",
                    "hidden": false
                },
                {
                    "_id": "6925312216eb3a9f13103a43",
                    "user": {
                        "_id": "669536cad4ca2767b9a9da91",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/itE8uwV6pji4yR38F3zCP.png",
                        "isPro": false,
                        "fullname": "Noam Koenigstein",
                        "user": "deltalab-noam",
                        "type": "user"
                    },
                    "name": "Noam Koenigstein",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-11-25T09:34:43.704Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-22T11:27:32.000Z",
            "submittedOnDailyAt": "2025-11-25T05:26:49.924Z",
            "title": "Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems",
            "submittedOnDailyBy": {
                "_id": "662a80358a721ebd0b4f358b",
                "avatarUrl": "/avatars/5be5a3c8b13f6f663206a19d0525c18e.svg",
                "isPro": false,
                "fullname": "Yehonatan Elisha",
                "user": "Yoniel",
                "type": "user"
            },
            "summary": "We present a method for extracting monosemantic neurons, defined as latent dimensions that align with coherent and interpretable concepts, from user and item embeddings in recommender systems. Our approach employs a Sparse Autoencoder (SAE) to reveal semantic structure within pretrained representations. In contrast to work on language models, monosemanticity in recommendation must preserve the interactions between separate user and item embeddings. To achieve this, we introduce a prediction aware training objective that backpropagates through a frozen recommender and aligns the learned latent structure with the model's user-item affinity predictions. The resulting neurons capture properties such as genre, popularity, and temporal trends, and support post hoc control operations including targeted filtering and content promotion without modifying the base model. Our method generalizes across different recommendation models and datasets, providing a practical tool for interpretable and controllable personalization. Code and evaluation resources are available at https://github.com/DeltaLabTLV/Monosemanticity4Rec.",
            "upvotes": 0,
            "discussionId": "6925312316eb3a9f13103a44",
            "ai_summary": "A Sparse Autoencoder method extracts interpretable latent dimensions from user and item embeddings in recommender systems, aligning with model predictions and supporting controllable personalization.",
            "ai_keywords": [
                "Sparse Autoencoder",
                "monosemantic neurons",
                "latent dimensions",
                "pretrained representations",
                "prediction aware",
                "user-item affinity predictions",
                "targeted filtering",
                "content promotion"
            ]
        },
        "publishedAt": "2025-11-22T06:27:32.000Z",
        "title": "Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems",
        "summary": "We present a method for extracting monosemantic neurons, defined as latent dimensions that align with coherent and interpretable concepts, from user and item embeddings in recommender systems. Our approach employs a Sparse Autoencoder (SAE) to reveal semantic structure within pretrained representations. In contrast to work on language models, monosemanticity in recommendation must preserve the interactions between separate user and item embeddings. To achieve this, we introduce a prediction aware training objective that backpropagates through a frozen recommender and aligns the learned latent structure with the model's user-item affinity predictions. The resulting neurons capture properties such as genre, popularity, and temporal trends, and support post hoc control operations including targeted filtering and content promotion without modifying the base model. Our method generalizes across different recommendation models and datasets, providing a practical tool for interpretable and controllable personalization. Code and evaluation resources are available at https://github.com/DeltaLabTLV/Monosemanticity4Rec.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18024.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "662a80358a721ebd0b4f358b",
            "avatarUrl": "/avatars/5be5a3c8b13f6f663206a19d0525c18e.svg",
            "fullname": "Yehonatan Elisha",
            "name": "Yoniel",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.12810",
            "authors": [
                {
                    "_id": "69250b1a16eb3a9f131038db",
                    "user": {
                        "_id": "65becdfd443909bf23265819",
                        "avatarUrl": "/avatars/b2f493c1315822a184901440b26ff6e8.svg",
                        "isPro": false,
                        "fullname": "Leena Alghamdi",
                        "user": "linaa98",
                        "type": "user"
                    },
                    "name": "Leena Alghamdi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-25T09:05:47.384Z",
                    "hidden": false
                },
                {
                    "_id": "69250b1a16eb3a9f131038dc",
                    "name": "Muhammad Usman",
                    "hidden": false
                },
                {
                    "_id": "69250b1a16eb3a9f131038dd",
                    "name": "Hafeez Anwar",
                    "hidden": false
                },
                {
                    "_id": "69250b1a16eb3a9f131038de",
                    "name": "Abdul Bais",
                    "hidden": false
                },
                {
                    "_id": "69250b1a16eb3a9f131038df",
                    "name": "Saeed Anwar",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-16T22:29:06.000Z",
            "submittedOnDailyAt": "2025-11-25T11:02:25.829Z",
            "title": "MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection",
            "submittedOnDailyBy": {
                "_id": "65becdfd443909bf23265819",
                "avatarUrl": "/avatars/b2f493c1315822a184901440b26ff6e8.svg",
                "isPro": false,
                "fullname": "Leena Alghamdi",
                "user": "linaa98",
                "type": "user"
            },
            "summary": "Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at https://github.com/linaagh98/MSRNet{https://github.com/linaagh98/MSRNet}.",
            "upvotes": 0,
            "discussionId": "69250b1a16eb3a9f131038e0",
            "ai_summary": "A Multi-Scale Recursive Network using a Pyramid Vision Transformer and specialized units improves camouflaged object detection by enhancing feature extraction and recursive feature refinement.",
            "ai_keywords": [
                "Pyramid Vision Transformer",
                "Attention-Based Scale Integration Units",
                "Multi-Granularity Fusion Units",
                "recursive-feedback decoding strategy",
                "multi-scale learning",
                "recursive feature optimization"
            ],
            "organization": {
                "_id": "6338798e20fc636fd8b178ba",
                "name": "KFUPM",
                "fullname": "KFUPM",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676198977195-6338792d76421c054310cb07.png"
            }
        },
        "publishedAt": "2025-11-16T17:29:06.000Z",
        "title": "MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection",
        "summary": "Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at https://github.com/linaagh98/MSRNet{https://github.com/linaagh98/MSRNet}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.12810.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65becdfd443909bf23265819",
            "avatarUrl": "/avatars/b2f493c1315822a184901440b26ff6e8.svg",
            "fullname": "Leena Alghamdi",
            "name": "linaa98",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6338798e20fc636fd8b178ba",
            "name": "KFUPM",
            "fullname": "KFUPM",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676198977195-6338792d76421c054310cb07.png"
        },
        "isAuthorParticipating": true
    }
]