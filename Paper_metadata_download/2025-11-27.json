[
    {
        "paper": {
            "id": "2511.15552",
            "authors": [
                {
                    "_id": "6920b3b2b5612535ed9554f9",
                    "name": "Artem Chervyakov",
                    "hidden": false
                },
                {
                    "_id": "6920b3b2b5612535ed9554fa",
                    "user": {
                        "_id": "6207bbdb2f97dabc41b3b32b",
                        "avatarUrl": "/avatars/6e568045b0ebaa05ac1d469d941d4c96.svg",
                        "isPro": false,
                        "fullname": "Ulyana",
                        "user": "ulyanaisaeva",
                        "type": "user"
                    },
                    "name": "Ulyana Isaeva",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T20:40:04.347Z",
                    "hidden": false
                },
                {
                    "_id": "6920b3b2b5612535ed9554fb",
                    "name": "Anton Emelyanov",
                    "hidden": false
                },
                {
                    "_id": "6920b3b2b5612535ed9554fc",
                    "name": "Artem Safin",
                    "hidden": false
                },
                {
                    "_id": "6920b3b2b5612535ed9554fd",
                    "name": "Maria Tikhonova",
                    "hidden": false
                },
                {
                    "_id": "6920b3b2b5612535ed9554fe",
                    "name": "Alexander Kharitonov",
                    "hidden": false
                },
                {
                    "_id": "6920b3b2b5612535ed9554ff",
                    "name": "Yulia Lyakh",
                    "hidden": false
                },
                {
                    "_id": "6920b3b2b5612535ed955500",
                    "name": "Petr Surovtsev",
                    "hidden": false
                },
                {
                    "_id": "6920b3b2b5612535ed955501",
                    "name": "Denis Shevelev",
                    "hidden": false
                },
                {
                    "_id": "6920b3b2b5612535ed955502",
                    "name": "Vildan Saburov",
                    "hidden": false
                },
                {
                    "_id": "6920b3b2b5612535ed955503",
                    "user": {
                        "_id": "622b1f6b9f6139daa8e998ce",
                        "avatarUrl": "/avatars/842719c100a5969be75d04da97333675.svg",
                        "isPro": false,
                        "fullname": "Vasily Konovalov",
                        "user": "Vasily",
                        "type": "user"
                    },
                    "name": "Vasily Konovalov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T20:40:06.412Z",
                    "hidden": false
                },
                {
                    "_id": "6920b3b2b5612535ed955504",
                    "user": {
                        "_id": "636b867ecde3707d10999b96",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667991162387-noauth.png",
                        "isPro": false,
                        "fullname": "Rykov Elisei",
                        "user": "lmeribal",
                        "type": "user"
                    },
                    "name": "Elisei Rykov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T20:40:09.480Z",
                    "hidden": false
                },
                {
                    "_id": "6920b3b2b5612535ed955505",
                    "user": {
                        "_id": "61dedb1b2066746d68b63adb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dedb1b2066746d68b63adb/PLBHQxvbcay3qDJjY7HM3.jpeg",
                        "isPro": false,
                        "fullname": "Ivan Sviridov",
                        "user": "univanxx",
                        "type": "user"
                    },
                    "name": "Ivan Sviridov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T10:00:36.928Z",
                    "hidden": false
                },
                {
                    "_id": "6920b3b2b5612535ed955506",
                    "name": "Amina Miftakhova",
                    "hidden": false
                },
                {
                    "_id": "6920b3b2b5612535ed955507",
                    "name": "Ilseyar Alimova",
                    "hidden": false
                },
                {
                    "_id": "6920b3b2b5612535ed955508",
                    "name": "Alexander Panchenko",
                    "hidden": false
                },
                {
                    "_id": "6920b3b2b5612535ed955509",
                    "name": "Alexander Kapitanov",
                    "hidden": false
                },
                {
                    "_id": "6920b3b2b5612535ed95550a",
                    "name": "Alena Fenogenova",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-19T15:43:53.000Z",
            "submittedOnDailyAt": "2025-11-27T10:05:09.849Z",
            "title": "Multimodal Evaluation of Russian-language Architectures",
            "submittedOnDailyBy": {
                "_id": "61dedb1b2066746d68b63adb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dedb1b2066746d68b63adb/PLBHQxvbcay3qDJjY7HM3.jpeg",
                "isPro": false,
                "fullname": "Ivan Sviridov",
                "user": "univanxx",
                "type": "user"
            },
            "summary": "Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.",
            "upvotes": 71,
            "discussionId": "6920b3b2b5612535ed95550b",
            "projectPage": "https://mera.a-ai.ru/en/multi",
            "githubRepo": "https://github.com/MERA-Evaluation/MERA_MULTIMODAL/tree/main",
            "ai_summary": "Mera Multi is an open multimodal evaluation framework for Russian-spoken architectures, addressing the lack of such benchmarks with 18 newly constructed tasks and a methodology to prevent benchmark leakage.",
            "ai_keywords": [
                "multimodal large language models",
                "Mera Multi",
                "multimodal evaluation framework",
                "Russian language",
                "multimodal abilities",
                "benchmark leakage",
                "watermarking"
            ],
            "githubStars": 11
        },
        "publishedAt": "2025-11-19T10:43:53.000Z",
        "title": "Multimodal Evaluation of Russian-language Architectures",
        "summary": "Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15552.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "61dedb1b2066746d68b63adb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dedb1b2066746d68b63adb/PLBHQxvbcay3qDJjY7HM3.jpeg",
            "fullname": "Ivan Sviridov",
            "name": "univanxx",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.20639",
            "authors": [
                {
                    "_id": "6927c504243b2216fb75cd62",
                    "user": {
                        "_id": "65c288280aa2d53135734a42",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg",
                        "isPro": false,
                        "fullname": "Jiaru Zou",
                        "user": "jiaruz2",
                        "type": "user"
                    },
                    "name": "Jiaru Zou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T09:58:44.029Z",
                    "hidden": false
                },
                {
                    "_id": "6927c504243b2216fb75cd63",
                    "name": "Xiyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "6927c504243b2216fb75cd64",
                    "name": "Ruizhong Qiu",
                    "hidden": false
                },
                {
                    "_id": "6927c504243b2216fb75cd65",
                    "name": "Gaotang Li",
                    "hidden": false
                },
                {
                    "_id": "6927c504243b2216fb75cd66",
                    "name": "Katherine Tieu",
                    "hidden": false
                },
                {
                    "_id": "6927c504243b2216fb75cd67",
                    "user": {
                        "_id": "60f5f68fa7fd83d025749234",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
                        "isPro": false,
                        "fullname": "Pan Lu",
                        "user": "lupantech",
                        "type": "user"
                    },
                    "name": "Pan Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T09:58:40.924Z",
                    "hidden": false
                },
                {
                    "_id": "6927c504243b2216fb75cd68",
                    "name": "Ke Shen",
                    "hidden": false
                },
                {
                    "_id": "6927c504243b2216fb75cd69",
                    "name": "Hanghang Tong",
                    "hidden": false
                },
                {
                    "_id": "6927c504243b2216fb75cd6a",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "6927c504243b2216fb75cd6b",
                    "name": "Jingrui He",
                    "hidden": false
                },
                {
                    "_id": "6927c504243b2216fb75cd6c",
                    "name": "James Zou",
                    "hidden": false
                },
                {
                    "_id": "6927c504243b2216fb75cd6d",
                    "name": "Mengdi Wang",
                    "hidden": false
                },
                {
                    "_id": "6927c504243b2216fb75cd6e",
                    "name": "Ling Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-25T18:56:57.000Z",
            "submittedOnDailyAt": "2025-11-27T01:00:26.981Z",
            "title": "Latent Collaboration in Multi-Agent Systems",
            "submittedOnDailyBy": {
                "_id": "65c288280aa2d53135734a42",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg",
                "isPro": false,
                "fullname": "Jiaru Zou",
                "user": "jiaruz2",
                "type": "user"
            },
            "summary": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.",
            "upvotes": 55,
            "discussionId": "6927c504243b2216fb75cd6f",
            "githubRepo": "https://github.com/Gen-Verse/LatentMAS",
            "ai_summary": "LatentMAS enables efficient and effective collaboration among LLM agents using latent space representations, enhancing reasoning quality and reducing computational costs.",
            "ai_keywords": [
                "multi-agent systems",
                "large language models",
                "latent space",
                "LatentMAS",
                "auto-regressive latent thoughts generation",
                "last-layer hidden embeddings",
                "shared latent working memory",
                "expressiveness",
                "information preservation",
                "complexity",
                "math and science reasoning",
                "commonsense understanding",
                "code generation",
                "accuracy",
                "output token usage",
                "end-to-end inference"
            ],
            "githubStars": 79,
            "organization": {
                "_id": "67a21d7efeeacb7707bf40de",
                "name": "Gen-Verse",
                "fullname": "Princeton-AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64fde4e252e82dd432b74ce9/TAEScS71YX5NPRM4TXZc8.png"
            }
        },
        "publishedAt": "2025-11-25T13:56:57.000Z",
        "title": "Latent Collaboration in Multi-Agent Systems",
        "summary": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20639.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "65c288280aa2d53135734a42",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg",
            "fullname": "Jiaru Zou",
            "name": "jiaruz2",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "67a21d7efeeacb7707bf40de",
            "name": "Gen-Verse",
            "fullname": "Princeton-AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64fde4e252e82dd432b74ce9/TAEScS71YX5NPRM4TXZc8.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.20714",
            "authors": [
                {
                    "_id": "6927bc37243b2216fb75cd3c",
                    "name": "Inferix Team",
                    "hidden": false
                },
                {
                    "_id": "6927bc37243b2216fb75cd3d",
                    "name": "Tianyu Feng",
                    "hidden": false
                },
                {
                    "_id": "6927bc37243b2216fb75cd3e",
                    "name": "Yizeng Han",
                    "hidden": false
                },
                {
                    "_id": "6927bc37243b2216fb75cd3f",
                    "name": "Jiahao He",
                    "hidden": false
                },
                {
                    "_id": "6927bc37243b2216fb75cd40",
                    "name": "Yuanyu He",
                    "hidden": false
                },
                {
                    "_id": "6927bc37243b2216fb75cd41",
                    "name": "Xi Lin",
                    "hidden": false
                },
                {
                    "_id": "6927bc37243b2216fb75cd42",
                    "name": "Teng Liu",
                    "hidden": false
                },
                {
                    "_id": "6927bc37243b2216fb75cd43",
                    "name": "Hanfeng Lu",
                    "hidden": false
                },
                {
                    "_id": "6927bc37243b2216fb75cd44",
                    "name": "Jiasheng Tang",
                    "hidden": false
                },
                {
                    "_id": "6927bc37243b2216fb75cd45",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "6927bc37243b2216fb75cd46",
                    "name": "Zhiyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "6927bc37243b2216fb75cd47",
                    "name": "Jichao Wu",
                    "hidden": false
                },
                {
                    "_id": "6927bc37243b2216fb75cd48",
                    "name": "Mingyang Yang",
                    "hidden": false
                },
                {
                    "_id": "6927bc37243b2216fb75cd49",
                    "name": "Yinghao Yu",
                    "hidden": false
                },
                {
                    "_id": "6927bc37243b2216fb75cd4a",
                    "user": {
                        "_id": "64ec877bb93654d4ca5c92e9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                        "isPro": false,
                        "fullname": "Zeyu Zhang",
                        "user": "SteveZeyuZhang",
                        "type": "user"
                    },
                    "name": "Zeyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T09:59:06.036Z",
                    "hidden": false
                },
                {
                    "_id": "6927bc37243b2216fb75cd4b",
                    "name": "Bohan Zhuang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-25T01:45:04.000Z",
            "submittedOnDailyAt": "2025-11-27T00:19:36.886Z",
            "title": "Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.\n  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.",
            "upvotes": 37,
            "discussionId": "6927bc38243b2216fb75cd4c",
            "githubRepo": "https://github.com/alibaba-damo-academy/Inferix",
            "ai_summary": "Inferix is a next-generation inference engine designed for immersive world synthesis using semi-autoregressive decoding, combining diffusion and autoregressive methods for high-quality, real-time video generation and interaction.",
            "ai_keywords": [
                "world models",
                "agentic AI",
                "embodied AI",
                "gaming",
                "visual perception",
                "understanding",
                "reasoning",
                "semi-autoregressive",
                "block-diffusion",
                "diffusion",
                "autoregressive methods",
                "video tokens",
                "KV Cache management",
                "Inferix",
                "vLLM",
                "SGLang",
                "xDiTs",
                "interactive video streaming",
                "profiling",
                "LV-Bench",
                "minute-long video generation"
            ],
            "githubStars": 21
        },
        "publishedAt": "2025-11-24T20:45:04.000Z",
        "title": "Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation",
        "summary": "World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.\n  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20714.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 171
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.21579",
            "authors": [
                {
                    "_id": "69283a32805584b280405089",
                    "name": "Teng Hu",
                    "hidden": false
                },
                {
                    "_id": "69283a32805584b28040508a",
                    "name": "Zhentao Yu",
                    "hidden": false
                },
                {
                    "_id": "69283a32805584b28040508b",
                    "name": "Guozhen Zhang",
                    "hidden": false
                },
                {
                    "_id": "69283a32805584b28040508c",
                    "name": "Zihan Su",
                    "hidden": false
                },
                {
                    "_id": "69283a32805584b28040508d",
                    "name": "Zhengguang Zhou",
                    "hidden": false
                },
                {
                    "_id": "69283a32805584b28040508e",
                    "name": "Youliang Zhang",
                    "hidden": false
                },
                {
                    "_id": "69283a32805584b28040508f",
                    "name": "Yuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "69283a32805584b280405090",
                    "name": "Qinglin Lu",
                    "hidden": false
                },
                {
                    "_id": "69283a32805584b280405091",
                    "name": "Ran Yi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-26T16:53:05.000Z",
            "submittedOnDailyAt": "2025-11-27T09:20:11.686Z",
            "title": "Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy",
            "submittedOnDailyBy": {
                "_id": "6484576450e57f3530ee6432",
                "avatarUrl": "/avatars/16a780f94597e9f5a4d57a84879102d7.svg",
                "isPro": false,
                "fullname": "huteng",
                "user": "JTUplayer",
                "type": "user"
            },
            "summary": "The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.",
            "upvotes": 17,
            "discussionId": "69283a32805584b280405092",
            "projectPage": "https://sjtuplayer.github.io/projects/Harmony/",
            "githubRepo": "https://github.com/sjtuplayer/Harmony",
            "ai_summary": "Harmony addresses audio-visual synchronization in generative AI by introducing a Cross-Task Synergy training paradigm, Global-Local Decoupled Interaction Module, and Synchronization-Enhanced CFG to improve alignment and fidelity.",
            "ai_keywords": [
                "joint diffusion process",
                "Correspondence Drift",
                "global attention mechanisms",
                "temporal cues",
                "intra-modal bias",
                "Classifier-Free Guidance",
                "Cross-Task Synergy",
                "Global-Local Decoupled Interaction Module",
                "Synchronization-Enhanced CFG"
            ],
            "githubStars": 20,
            "organization": {
                "_id": "6645f953c39288df638dbdd5",
                "name": "Tencent-Hunyuan",
                "fullname": "Tencent Hunyuan",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
            }
        },
        "publishedAt": "2025-11-26T11:53:05.000Z",
        "title": "Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy",
        "summary": "The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21579.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6484576450e57f3530ee6432",
            "avatarUrl": "/avatars/16a780f94597e9f5a4d57a84879102d7.svg",
            "fullname": "huteng",
            "name": "JTUplayer",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6645f953c39288df638dbdd5",
            "name": "Tencent-Hunyuan",
            "fullname": "Tencent Hunyuan",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.21692",
            "authors": [
                {
                    "_id": "6927cf95243b2216fb75cd90",
                    "user": {
                        "_id": "626d13b15f7327906f0514e9",
                        "avatarUrl": "/avatars/6fb9b22961b9a283700c60713bc5a61f.svg",
                        "isPro": false,
                        "fullname": "Yeganeh Kordi",
                        "user": "Yeganeh",
                        "type": "user"
                    },
                    "name": "Yeganeh Kordi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T09:58:38.604Z",
                    "hidden": false
                },
                {
                    "_id": "6927cf95243b2216fb75cd91",
                    "name": "Nihal V. Nayak",
                    "hidden": false
                },
                {
                    "_id": "6927cf95243b2216fb75cd92",
                    "name": "Max Zuo",
                    "hidden": false
                },
                {
                    "_id": "6927cf95243b2216fb75cd93",
                    "name": "Ilana Nguyen",
                    "hidden": false
                },
                {
                    "_id": "6927cf95243b2216fb75cd94",
                    "name": "Stephen H. Bach",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-26T18:59:57.000Z",
            "submittedOnDailyAt": "2025-11-27T01:58:38.144Z",
            "title": "Revisiting Generalization Across Difficulty Levels: It's Not So Easy",
            "submittedOnDailyBy": {
                "_id": "64fbb457c7f04f7cee8624e0",
                "avatarUrl": "/avatars/f0512561780625d9be43f00dfd5cd46d.svg",
                "isPro": false,
                "fullname": "Max Zuo",
                "user": "zuom",
                "type": "user"
            },
            "summary": "We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.",
            "upvotes": 10,
            "discussionId": "6927cf95243b2216fb75cd95",
            "githubRepo": "https://github.com/BatsResearch/Cross-Difficulty",
            "ai_summary": "LLMs do not consistently generalize across different task difficulties, indicating the need for a broad range of difficulty levels in both training and evaluation datasets.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "generalization",
                "task difficulties",
                "data curation",
                "evaluation",
                "Item Response Theory",
                "IRT",
                "cross-difficulty generalization"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "650d8bcd5085c0ce1f286c12",
                "name": "BatsResearch",
                "fullname": "Bats Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/608849cadf398c3b285ce95b/oFrkKbxGUKIW2W8FFGUje.png"
            }
        },
        "publishedAt": "2025-11-26T13:59:57.000Z",
        "title": "Revisiting Generalization Across Difficulty Levels: It's Not So Easy",
        "summary": "We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21692.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64fbb457c7f04f7cee8624e0",
            "avatarUrl": "/avatars/f0512561780625d9be43f00dfd5cd46d.svg",
            "fullname": "Max Zuo",
            "name": "zuom",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "650d8bcd5085c0ce1f286c12",
            "name": "BatsResearch",
            "fullname": "Bats Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/608849cadf398c3b285ce95b/oFrkKbxGUKIW2W8FFGUje.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.21395",
            "authors": [
                {
                    "_id": "6927bed6243b2216fb75cd58",
                    "name": "Qixun Wang",
                    "hidden": false
                },
                {
                    "_id": "6927bed6243b2216fb75cd59",
                    "user": {
                        "_id": "673c7319d11b1c2e246ead9c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
                        "isPro": false,
                        "fullname": "Yang Shi",
                        "user": "DogNeverSleep",
                        "type": "user"
                    },
                    "name": "Yang Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T09:59:04.104Z",
                    "hidden": false
                },
                {
                    "_id": "6927bed6243b2216fb75cd5a",
                    "name": "Yifei Wang",
                    "hidden": false
                },
                {
                    "_id": "6927bed6243b2216fb75cd5b",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "6927bed6243b2216fb75cd5c",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "6927bed6243b2216fb75cd5d",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "6927bed6243b2216fb75cd5e",
                    "name": "Xianghua Ying",
                    "hidden": false
                },
                {
                    "_id": "6927bed6243b2216fb75cd5f",
                    "name": "Yisen Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-26T13:46:39.000Z",
            "submittedOnDailyAt": "2025-11-27T00:31:32.938Z",
            "title": "Monet: Reasoning in Latent Visual Space Beyond Images and Language",
            "submittedOnDailyBy": {
                "_id": "673c7319d11b1c2e246ead9c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
                "isPro": false,
                "fullname": "Yang Shi",
                "user": "DogNeverSleep",
                "type": "user"
            },
            "summary": "\"Thinking with images\" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet.",
            "upvotes": 9,
            "discussionId": "6927bed6243b2216fb75cd60",
            "githubRepo": "https://github.com/NOVAglow646/Monet",
            "ai_summary": "Monet, a training framework, enables MLLMs to reason in latent visual space using continuous embeddings, addressing challenges like computational cost and supervision, and outperforms on visual reasoning tasks.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "latent visual space",
                "continuous embeddings",
                "intermediate visual thoughts",
                "three-stage distillation-based supervised fine-tuning",
                "SFT",
                "GRPO",
                "Visual-latent Policy Optimization",
                "VLPO",
                "reinforcement learning",
                "policy gradient updates",
                "CoT dataset",
                "real-world perception",
                "reasoning benchmarks",
                "out-of-distribution generalization"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-11-26T08:46:39.000Z",
        "title": "Monet: Reasoning in Latent Visual Space Beyond Images and Language",
        "summary": "\"Thinking with images\" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21395.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "673c7319d11b1c2e246ead9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
            "fullname": "Yang Shi",
            "name": "DogNeverSleep",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.20478",
            "authors": [
                {
                    "_id": "6926d024243b2216fb75cbca",
                    "name": "Kateryna Chumachenko",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbcb",
                    "name": "Amala Sanjay Deshmukh",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbcc",
                    "name": "Jarno Seppanen",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbcd",
                    "name": "Ilia Karmanov",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbce",
                    "name": "Chia-Chih Chen",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbcf",
                    "name": "Lukas Voegtle",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbd0",
                    "name": "Philipp Fischer",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbd1",
                    "name": "Marek Wawrzos",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbd2",
                    "name": "Saeid Motiian",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbd3",
                    "name": "Roman Ageev",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbd4",
                    "name": "Kedi Wu",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbd5",
                    "name": "Alexandre Milesi",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbd6",
                    "name": "Maryam Moosaei",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbd7",
                    "name": "Krzysztof Pawelec",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbd8",
                    "name": "Padmavathy Subramanian",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbd9",
                    "name": "Mehrzad Samadi",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbda",
                    "name": "Xin Yu",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbdb",
                    "name": "Celina Dear",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbdc",
                    "name": "Sarah Stoddard",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbdd",
                    "name": "Jenna Diamond",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbde",
                    "name": "Jesse Oliver",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbdf",
                    "name": "Leanna Chraghchian",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbe0",
                    "name": "Patrick Skelly",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbe1",
                    "name": "Tom Balough",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbe2",
                    "name": "Yao Xu",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbe3",
                    "name": "Jane Polak Scowcroft",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbe4",
                    "name": "Daniel Korzekwa",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbe5",
                    "name": "Darragh Hanley",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbe6",
                    "name": "Sandip Bhaskar",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbe7",
                    "name": "Timo Roman",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbe8",
                    "name": "Karan Sapra",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbe9",
                    "name": "Andrew Tao",
                    "hidden": false
                },
                {
                    "_id": "6926d024243b2216fb75cbea",
                    "name": "Bryan Catanzaro",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-25T16:41:25.000Z",
            "submittedOnDailyAt": "2025-11-27T00:15:47.055Z",
            "title": "NVIDIA Nemotron Parse 1.1",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.",
            "upvotes": 9,
            "discussionId": "6926d024243b2216fb75cbeb",
            "ai_summary": "Nemotron-Parse-1.1 is a lightweight OCR and document parsing model with improved capabilities in general OCR, markdown formatting, structured table parsing, and text extraction from images, using an encoder-decoder architecture.",
            "ai_keywords": [
                "OCR",
                "document parsing",
                "markdown formatting",
                "structured table parsing",
                "text extraction",
                "encoder-decoder architecture",
                "bounding boxes",
                "semantic classes",
                "Huggingface",
                "NIM container",
                "vision tokens"
            ],
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-11-25T11:41:25.000Z",
        "title": "NVIDIA Nemotron Parse 1.1",
        "summary": "We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20478.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 171
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.19797",
            "authors": [
                {
                    "_id": "6927bd2d243b2216fb75cd52",
                    "name": "Linqi Zhou",
                    "hidden": false
                },
                {
                    "_id": "6927bd2d243b2216fb75cd53",
                    "name": "Mathias Parger",
                    "hidden": false
                },
                {
                    "_id": "6927bd2d243b2216fb75cd54",
                    "name": "Ayaan Haque",
                    "hidden": false
                },
                {
                    "_id": "6927bd2d243b2216fb75cd55",
                    "name": "Jiaming Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-24T23:55:45.000Z",
            "submittedOnDailyAt": "2025-11-27T00:23:41.631Z",
            "title": "Terminal Velocity Matching",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the 2-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.",
            "upvotes": 8,
            "discussionId": "6927bd2d243b2216fb75cd56",
            "ai_summary": "Terminal Velocity Matching (TVM) generalizes flow matching for high-fidelity generative modeling, achieving state-of-the-art performance on ImageNet with minimal computational steps.",
            "ai_keywords": [
                "Terminal Velocity Matching",
                "flow matching",
                "diffusion timesteps",
                "2-Wasserstein distance",
                "Lipschitz continuous",
                "Diffusion Transformers",
                "fused attention kernel",
                "Jacobian-Vector Products",
                "ImageNet-256x256",
                "ImageNet-512x512",
                "FID"
            ]
        },
        "publishedAt": "2025-11-24T18:55:45.000Z",
        "title": "Terminal Velocity Matching",
        "summary": "We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the 2-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19797.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 171
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.19413",
            "authors": [
                {
                    "_id": "69284dd2805584b2804050de",
                    "user": {
                        "_id": "6887a18b7253a2e54262cfb5",
                        "avatarUrl": "/avatars/a1c2df1ac85fa6e940a34f216745645f.svg",
                        "isPro": false,
                        "fullname": "zhaolong su",
                        "user": "rollingsu",
                        "type": "user"
                    },
                    "name": "Zhaolong Su",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T20:39:40.284Z",
                    "hidden": false
                },
                {
                    "_id": "69284dd2805584b2804050df",
                    "name": "Wang Lu",
                    "hidden": false
                },
                {
                    "_id": "69284dd2805584b2804050e0",
                    "name": "Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "69284dd2805584b2804050e1",
                    "name": "Sharon Li",
                    "hidden": false
                },
                {
                    "_id": "69284dd2805584b2804050e2",
                    "name": "Jindong Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-24T18:50:01.000Z",
            "submittedOnDailyAt": "2025-11-27T10:41:07.823Z",
            "title": "UniGame: Turning a Unified Multimodal Model Into Its Own Adversary",
            "submittedOnDailyBy": {
                "_id": "6204cc0d522e40b4a18d86e2",
                "avatarUrl": "/avatars/18daf2de5671e711dc745388dd60569d.svg",
                "isPro": false,
                "fullname": "Jindong Wang",
                "user": "jindongwang",
                "type": "user"
            },
            "summary": "Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame",
            "upvotes": 8,
            "discussionId": "69284dd2805584b2804050e3",
            "githubRepo": "https://github.com/AIFrontierLab/UniGame",
            "ai_summary": "UniGame, a self-adversarial post-training framework, improves consistency, understanding, generation, and robustness in unified multimodal models by introducing a lightweight perturber at the shared token interface.",
            "ai_keywords": [
                "Unified Multimodal Models",
                "self-adversarial",
                "post-training framework",
                "compact embeddings",
                "reconstruction-rich representations",
                "decision boundaries",
                "cross-modal coherence",
                "distributional shifts",
                "adversarial shifts",
                "adversarial self-play",
                "multimodal foundation models"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-11-24T13:50:01.000Z",
        "title": "UniGame: Turning a Unified Multimodal Model Into Its Own Adversary",
        "summary": "Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19413.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6204cc0d522e40b4a18d86e2",
            "avatarUrl": "/avatars/18daf2de5671e711dc745388dd60569d.svg",
            "fullname": "Jindong Wang",
            "name": "jindongwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.21688",
            "authors": [
                {
                    "_id": "6927d285243b2216fb75cdaa",
                    "user": {
                        "_id": "645940c8f1f5263c2ef334ab",
                        "avatarUrl": "/avatars/e908686d0e1b2708adb0e30e596a6f39.svg",
                        "isPro": false,
                        "fullname": "Wenbo Hu",
                        "user": "gordonhu",
                        "type": "user"
                    },
                    "name": "Wenbo Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T09:58:34.612Z",
                    "hidden": false
                },
                {
                    "_id": "6927d285243b2216fb75cdab",
                    "name": "Jingli Lin",
                    "hidden": false
                },
                {
                    "_id": "6927d285243b2216fb75cdac",
                    "name": "Yilin Long",
                    "hidden": false
                },
                {
                    "_id": "6927d285243b2216fb75cdad",
                    "name": "Yunlong Ran",
                    "hidden": false
                },
                {
                    "_id": "6927d285243b2216fb75cdae",
                    "name": "Lihan Jiang",
                    "hidden": false
                },
                {
                    "_id": "6927d285243b2216fb75cdaf",
                    "name": "Yifan Wang",
                    "hidden": false
                },
                {
                    "_id": "6927d285243b2216fb75cdb0",
                    "name": "Chenming Zhu",
                    "hidden": false
                },
                {
                    "_id": "6927d285243b2216fb75cdb1",
                    "name": "Runsen Xu",
                    "hidden": false
                },
                {
                    "_id": "6927d285243b2216fb75cdb2",
                    "name": "Tai Wang",
                    "hidden": false
                },
                {
                    "_id": "6927d285243b2216fb75cdb3",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-26T18:59:39.000Z",
            "submittedOnDailyAt": "2025-11-27T18:52:14.636Z",
            "title": "G^2VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning",
            "submittedOnDailyBy": {
                "_id": "645940c8f1f5263c2ef334ab",
                "avatarUrl": "/avatars/e908686d0e1b2708adb0e30e596a6f39.svg",
                "isPro": false,
                "fullname": "Wenbo Hu",
                "user": "gordonhu",
                "type": "user"
            },
            "summary": "Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G^2VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G^2VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G^2VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G^2VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.",
            "upvotes": 5,
            "discussionId": "6927d286243b2216fb75cdb4",
            "projectPage": "https://gordonhu608.github.io/g2vlm.github.io/",
            "githubRepo": "https://github.com/InternRobotics/G2VLM",
            "ai_summary": "G$^2$VLM integrates 3D geometry learning with vision-language models to enhance spatial understanding and reasoning, outperforming existing models in these tasks.",
            "ai_keywords": [
                "geometry grounded vision-language model",
                "spatial 3D reconstruction",
                "in-context learning",
                "interleaved reasoning",
                "multi-view image",
                "video data",
                "3D visual priors",
                "spatial understanding",
                "spatial reasoning",
                "feed-forward 3D reconstruction models"
            ],
            "githubStars": 43,
            "organization": {
                "_id": "6881c146ff13df8b65153273",
                "name": "InternRobotics",
                "fullname": "Intern Robotics",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65d9f09bbcd15bc5cb255fed/REfA3nEK1_Y-PTfGn_5H1.jpeg"
            }
        },
        "publishedAt": "2025-11-26T13:59:39.000Z",
        "title": "G^2VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning",
        "summary": "Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G^2VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G^2VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G^2VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G^2VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21688.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645940c8f1f5263c2ef334ab",
            "avatarUrl": "/avatars/e908686d0e1b2708adb0e30e596a6f39.svg",
            "fullname": "Wenbo Hu",
            "name": "gordonhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "6881c146ff13df8b65153273",
            "name": "InternRobotics",
            "fullname": "Intern Robotics",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65d9f09bbcd15bc5cb255fed/REfA3nEK1_Y-PTfGn_5H1.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.20426",
            "authors": [
                {
                    "_id": "6928381b805584b280405072",
                    "name": "Hmrishav Bandyopadhyay",
                    "hidden": false
                },
                {
                    "_id": "6928381b805584b280405073",
                    "name": "Nikhil Pinnaparaju",
                    "hidden": false
                },
                {
                    "_id": "6928381b805584b280405074",
                    "name": "Rahim Entezari",
                    "hidden": false
                },
                {
                    "_id": "6928381b805584b280405075",
                    "name": "Jim Scott",
                    "hidden": false
                },
                {
                    "_id": "6928381b805584b280405076",
                    "name": "Yi-Zhe Song",
                    "hidden": false
                },
                {
                    "_id": "6928381b805584b280405077",
                    "name": "Varun Jampani",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/638c81fa61eb51017518fa31/sJ5nwkavPQrYbJrbi3-mo.qt"
            ],
            "publishedAt": "2025-11-25T15:52:58.000Z",
            "submittedOnDailyAt": "2025-11-27T09:09:13.732Z",
            "title": "Block Cascading: Training Free Acceleration of Block-Causal Video Models",
            "submittedOnDailyBy": {
                "_id": "638c81fa61eb51017518fa31",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/f0eCrzBrxz7Y9n25WkZ2v.png",
                "isPro": false,
                "fullname": "Hmrishav Bandyopadhyay",
                "user": "Hmrishav",
                "type": "user"
            },
            "summary": "Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/",
            "upvotes": 4,
            "discussionId": "6928381b805584b280405078",
            "projectPage": "https://hmrishavbandy.github.io/block_cascading_page/",
            "ai_summary": "Block Cascading parallelizes video block generation, achieving significant speed improvements without compromising quality.",
            "ai_keywords": [
                "block-causal video generation",
                "parallelization",
                "denoised current blocks",
                "sequential pipelines",
                "parallel cascades",
                "temporal parallelism",
                "KV-recaching",
                "context switches",
                "interactive generation"
            ],
            "organization": {
                "_id": "62e1573a6fb6e362b4a90690",
                "name": "stabilityai",
                "fullname": "Stability AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png"
            }
        },
        "publishedAt": "2025-11-25T10:52:58.000Z",
        "title": "Block Cascading: Training Free Acceleration of Block-Causal Video Models",
        "summary": "Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/638c81fa61eb51017518fa31/sJ5nwkavPQrYbJrbi3-mo.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20426.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638c81fa61eb51017518fa31",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/f0eCrzBrxz7Y9n25WkZ2v.png",
            "fullname": "Hmrishav Bandyopadhyay",
            "name": "Hmrishav",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "organization": {
            "_id": "62e1573a6fb6e362b4a90690",
            "name": "stabilityai",
            "fullname": "Stability AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.17889",
            "authors": [
                {
                    "_id": "69259d62c97bfa495574df19",
                    "user": {
                        "_id": "675aa0331b310ed01068857f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PtTMPhuHlfdui_CJOjgHV.png",
                        "isPro": false,
                        "fullname": "HuangTing",
                        "user": "Believe0029",
                        "type": "user"
                    },
                    "name": "Ting Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T10:00:30.581Z",
                    "hidden": false
                },
                {
                    "_id": "69259d62c97bfa495574df1a",
                    "name": "Dongjian Li",
                    "hidden": false
                },
                {
                    "_id": "69259d62c97bfa495574df1b",
                    "name": "Rui Yang",
                    "hidden": false
                },
                {
                    "_id": "69259d62c97bfa495574df1c",
                    "user": {
                        "_id": "64ec877bb93654d4ca5c92e9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                        "isPro": false,
                        "fullname": "Zeyu Zhang",
                        "user": "SteveZeyuZhang",
                        "type": "user"
                    },
                    "name": "Zeyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T10:00:26.747Z",
                    "hidden": true
                },
                {
                    "_id": "69259d62c97bfa495574df1d",
                    "user": {
                        "_id": "67e129f772dece12fa3e20d3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ifxojVXbdpxd02IIVCkWX.png",
                        "isPro": false,
                        "fullname": "Yangzida",
                        "user": "PKUYZD",
                        "type": "user"
                    },
                    "name": "Zida Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T10:00:28.652Z",
                    "hidden": false
                },
                {
                    "_id": "69259d62c97bfa495574df1e",
                    "name": "Hao Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-22T02:34:10.000Z",
            "submittedOnDailyAt": "2025-11-27T05:18:13.911Z",
            "title": "MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots",
            "submittedOnDailyBy": {
                "_id": "675aa0331b310ed01068857f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PtTMPhuHlfdui_CJOjgHV.png",
                "isPro": false,
                "fullname": "HuangTing",
                "user": "Believe0029",
                "type": "user"
            },
            "summary": "Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.",
            "upvotes": 4,
            "discussionId": "69259d62c97bfa495574df1f",
            "projectPage": "https://aigeeksgroup.github.io/MobileVLA-R1/",
            "githubRepo": "https://github.com/AIGeeksGroup/MobileVLA-R1",
            "ai_summary": "A unified vision-language-action framework, MobileVLA-R1, enhances reasoning and control for quadruped robots through supervised chain-of-thought alignment and GRPO reinforcement learning, achieving superior performance in complex environments.",
            "ai_keywords": [
                "vision-language-action framework",
                "MobileVLA-R1",
                "chain-of-thought (CoT)",
                "GRPO reinforcement learning",
                "VLN tasks",
                "VLA tasks"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-11-21T21:34:10.000Z",
        "title": "MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots",
        "summary": "Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17889.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "675aa0331b310ed01068857f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PtTMPhuHlfdui_CJOjgHV.png",
            "fullname": "HuangTing",
            "name": "Believe0029",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.20410",
            "authors": [
                {
                    "_id": "6927f3c9243b2216fb75ce24",
                    "user": {
                        "_id": "6927f08d48991e82e392a40b",
                        "avatarUrl": "/avatars/cfc69ddde26d0f32b6bcebff70060944.svg",
                        "isPro": false,
                        "fullname": "bao tang",
                        "user": "Ttt82",
                        "type": "user"
                    },
                    "name": "Bao Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T08:58:05.545Z",
                    "hidden": false
                },
                {
                    "_id": "6927f3c9243b2216fb75ce25",
                    "name": "Shuai Zhang",
                    "hidden": false
                },
                {
                    "_id": "6927f3c9243b2216fb75ce26",
                    "name": "Yueting Zhu",
                    "hidden": false
                },
                {
                    "_id": "6927f3c9243b2216fb75ce27",
                    "name": "Jijun Xiang",
                    "hidden": false
                },
                {
                    "_id": "6927f3c9243b2216fb75ce28",
                    "name": "Xin Yang",
                    "hidden": false
                },
                {
                    "_id": "6927f3c9243b2216fb75ce29",
                    "name": "Li Yu",
                    "hidden": false
                },
                {
                    "_id": "6927f3c9243b2216fb75ce2a",
                    "name": "Wenyu Liu",
                    "hidden": false
                },
                {
                    "_id": "6927f3c9243b2216fb75ce2b",
                    "name": "Xinggang Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6927f08d48991e82e392a40b/Y4-uv8I4XjOFDNZHjP6y4.png"
            ],
            "publishedAt": "2025-11-25T15:36:20.000Z",
            "submittedOnDailyAt": "2025-11-27T09:24:11.781Z",
            "title": "Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs",
            "submittedOnDailyBy": {
                "_id": "6927f08d48991e82e392a40b",
                "avatarUrl": "/avatars/cfc69ddde26d0f32b6bcebff70060944.svg",
                "isPro": false,
                "fullname": "bao tang",
                "user": "Ttt82",
                "type": "user"
            },
            "summary": "Timestep distillation is an effective approach for improving the generation efficiency of diffusion models. The Consistency Model (CM), as a trajectory-based framework, demonstrates significant potential due to its strong theoretical foundation and high-quality few-step generation. Nevertheless, current continuous-time consistency distillation methods still rely heavily on training data and computational resources, hindering their deployment in resource-constrained scenarios and limiting their scalability to diverse domains. To address this issue, we propose Trajectory-Backward Consistency Model (TBCM), which eliminates the dependence on external training data by extracting latent representations directly from the teacher model's generation trajectory. Unlike conventional methods that require VAE encoding and large-scale datasets, our self-contained distillation paradigm significantly improves both efficiency and simplicity. Moreover, the trajectory-extracted samples naturally bridge the distribution gap between training and inference, thereby enabling more effective knowledge transfer. Empirically, TBCM achieves 6.52 FID and 28.08 CLIP scores on MJHQ-30k under one-step generation, while reducing training time by approximately 40% compared to Sana-Sprint and saving a substantial amount of GPU memory, demonstrating superior efficiency without sacrificing quality. We further reveal the diffusion-generation space discrepancy in continuous-time consistency distillation and analyze how sampling strategies affect distillation performance, offering insights for future distillation research. GitHub Link: https://github.com/hustvl/TBCM.",
            "upvotes": 2,
            "discussionId": "6927f3c9243b2216fb75ce2c",
            "githubRepo": "https://github.com/hustvl/TBCM",
            "ai_summary": "TBCM, a self-contained trajectory-based distillation method, enhances diffusion model efficiency by eliminating external data dependency and improving knowledge transfer, achieving high-quality generation with reduced computational resources.",
            "ai_keywords": [
                "timestep distillation",
                "diffusion models",
                "Consistency Model",
                "trajectory-based framework",
                "continuous-time consistency distillation",
                "latent representations",
                "VAE encoding",
                "FID",
                "CLIP scores",
                "one-step generation",
                "diffusion-generation space discrepancy",
                "sampling strategies"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-11-25T10:36:20.000Z",
        "title": "Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs",
        "summary": "Timestep distillation is an effective approach for improving the generation efficiency of diffusion models. The Consistency Model (CM), as a trajectory-based framework, demonstrates significant potential due to its strong theoretical foundation and high-quality few-step generation. Nevertheless, current continuous-time consistency distillation methods still rely heavily on training data and computational resources, hindering their deployment in resource-constrained scenarios and limiting their scalability to diverse domains. To address this issue, we propose Trajectory-Backward Consistency Model (TBCM), which eliminates the dependence on external training data by extracting latent representations directly from the teacher model's generation trajectory. Unlike conventional methods that require VAE encoding and large-scale datasets, our self-contained distillation paradigm significantly improves both efficiency and simplicity. Moreover, the trajectory-extracted samples naturally bridge the distribution gap between training and inference, thereby enabling more effective knowledge transfer. Empirically, TBCM achieves 6.52 FID and 28.08 CLIP scores on MJHQ-30k under one-step generation, while reducing training time by approximately 40% compared to Sana-Sprint and saving a substantial amount of GPU memory, demonstrating superior efficiency without sacrificing quality. We further reveal the diffusion-generation space discrepancy in continuous-time consistency distillation and analyze how sampling strategies affect distillation performance, offering insights for future distillation research. GitHub Link: https://github.com/hustvl/TBCM.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6927f08d48991e82e392a40b/Y4-uv8I4XjOFDNZHjP6y4.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20410.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6927f08d48991e82e392a40b",
            "avatarUrl": "/avatars/cfc69ddde26d0f32b6bcebff70060944.svg",
            "fullname": "bao tang",
            "name": "Ttt82",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.20814",
            "authors": [
                {
                    "_id": "69280993243b2216fb75ce47",
                    "user": {
                        "_id": "620c1a977af55d45f5519914",
                        "avatarUrl": "/avatars/4d2fb7c1a5ad5dabdb8888fa2fe72e65.svg",
                        "isPro": true,
                        "fullname": "Tanvirul Alam",
                        "user": "xashru",
                        "type": "user"
                    },
                    "name": "Md Tanvirul Alam",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T20:39:46.813Z",
                    "hidden": false
                },
                {
                    "_id": "69280993243b2216fb75ce48",
                    "name": "Saksham Aggarwal",
                    "hidden": false
                },
                {
                    "_id": "69280993243b2216fb75ce49",
                    "name": "Justin Yang Chae",
                    "hidden": false
                },
                {
                    "_id": "69280993243b2216fb75ce4a",
                    "name": "Nidhi Rastogi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-25T20:00:47.000Z",
            "submittedOnDailyAt": "2025-11-27T06:03:14.739Z",
            "title": "SPHINX: A Synthetic Environment for Visual Perception and Reasoning",
            "submittedOnDailyBy": {
                "_id": "620c1a977af55d45f5519914",
                "avatarUrl": "/avatars/4d2fb7c1a5ad5dabdb8888fa2fe72e65.svg",
                "isPro": true,
                "fullname": "Tanvirul Alam",
                "user": "xashru",
                "type": "user"
            },
            "summary": "We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.",
            "upvotes": 1,
            "discussionId": "69280994243b2216fb75ce4b",
            "githubRepo": "https://github.com/xashru/sphinx",
            "ai_summary": "Sphinx, a synthetic environment for visual perception and reasoning, evaluates large vision-language models and demonstrates that reinforcement learning with verifiable rewards improves model accuracy on diverse tasks.",
            "ai_keywords": [
                "synthetic environment",
                "visual perception",
                "reasoning",
                "cognitive primitives",
                "procedural generation",
                "motifs",
                "tiles",
                "charts",
                "icons",
                "geometric primitives",
                "ground-truth solutions",
                "dataset construction",
                "symmetry detection",
                "geometric transformations",
                "spatial reasoning",
                "chart interpretation",
                "sequence prediction",
                "large vision-language models",
                "GPT-5",
                "reinforcement learning",
                "verifiable rewards",
                "multimodal reasoning"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-11-25T15:00:47.000Z",
        "title": "SPHINX: A Synthetic Environment for Visual Perception and Reasoning",
        "summary": "We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20814.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620c1a977af55d45f5519914",
            "avatarUrl": "/avatars/4d2fb7c1a5ad5dabdb8888fa2fe72e65.svg",
            "fullname": "Tanvirul Alam",
            "name": "xashru",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.20633",
            "authors": [
                {
                    "_id": "69286c60ab13296a93f862fa",
                    "user": {
                        "_id": "640d8a26b03f4cd29f52acdd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678608917790-noauth.png",
                        "isPro": false,
                        "fullname": "Jiahui Zhang",
                        "user": "jasonzhango",
                        "type": "user"
                    },
                    "name": "Jiahui Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T20:39:30.552Z",
                    "hidden": false
                },
                {
                    "_id": "69286c60ab13296a93f862fb",
                    "name": "Ze Huang",
                    "hidden": false
                },
                {
                    "_id": "69286c60ab13296a93f862fc",
                    "name": "Chun Gu",
                    "hidden": false
                },
                {
                    "_id": "69286c60ab13296a93f862fd",
                    "name": "Zipei Ma",
                    "hidden": false
                },
                {
                    "_id": "69286c60ab13296a93f862fe",
                    "name": "Li Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-25T18:52:56.000Z",
            "submittedOnDailyAt": "2025-11-27T12:53:33.787Z",
            "title": "Reinforcing Action Policies by Prophesying",
            "submittedOnDailyBy": {
                "_id": "640d8a26b03f4cd29f52acdd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678608917790-noauth.png",
                "isPro": false,
                "fullname": "Jiahui Zhang",
                "user": "jasonzhango",
                "type": "user"
            },
            "summary": "Vision-Language-Action (VLA) policies excel in aligning language, perception, and robot control. However, most VLAs are trained purely by imitation, which overfits to demonstrations, and is brittle under distribution shift. Reinforcement learning (RL) directly optimizes task reward and thus addresses this misalignment, but real-robot interaction is expensive and conventional simulators are hard to engineer and transfer. We address both data efficiency and optimization stability in VLA post-training via a learned world model and an RL procedure tailored to flow-based action heads. Specifically, we introduce Prophet, a unified action-to-video robot actuation pretrained across large-scale, heterogeneous robot data to learn reusable action-outcome dynamics. It is able to few-shot adapt to new robots, objects, and environments, yielding a rollout-ready simulator. Upon Prophet, we reinforce action policies with Flow-action-GRPO (FA-GRPO), which adapts Flow-GRPO to operate on VLA actions, and with FlowScale, a stepwise reweighting that rescales per-step gradients in the flow head. Together, Prophet, FA-GRPO, and FlowScale constitute ProphRL, a practical, data- and compute-efficient path to VLA post-training. Experiments show 5-17% success gains on public benchmarks and 24-30% gains on real robots across different VLA variants.",
            "upvotes": 1,
            "discussionId": "69286c61ab13296a93f862ff",
            "projectPage": "https://logosroboticsgroup.github.io/ProphRL/",
            "githubRepo": "https://github.com/LogosRoboticsGroup/ProphRL",
            "ai_summary": "ProphRL enhances Vision-Language-Action policies through a learned world model and reinforcement learning tailored to flow-based action heads, improving data efficiency and optimization stability.",
            "ai_keywords": [
                "Vision-Language-Action",
                "imitation",
                "reinforcement learning",
                "world model",
                "Flow-action-GRPO",
                "FlowScale",
                "Prophet",
                "rollout-ready simulator",
                "ProphRL"
            ],
            "githubStars": 11,
            "organization": {
                "_id": "68fceb6d4be572720b6900d4",
                "name": "LogosRobotics",
                "fullname": "LogosRobotics"
            }
        },
        "publishedAt": "2025-11-25T13:52:56.000Z",
        "title": "Reinforcing Action Policies by Prophesying",
        "summary": "Vision-Language-Action (VLA) policies excel in aligning language, perception, and robot control. However, most VLAs are trained purely by imitation, which overfits to demonstrations, and is brittle under distribution shift. Reinforcement learning (RL) directly optimizes task reward and thus addresses this misalignment, but real-robot interaction is expensive and conventional simulators are hard to engineer and transfer. We address both data efficiency and optimization stability in VLA post-training via a learned world model and an RL procedure tailored to flow-based action heads. Specifically, we introduce Prophet, a unified action-to-video robot actuation pretrained across large-scale, heterogeneous robot data to learn reusable action-outcome dynamics. It is able to few-shot adapt to new robots, objects, and environments, yielding a rollout-ready simulator. Upon Prophet, we reinforce action policies with Flow-action-GRPO (FA-GRPO), which adapts Flow-GRPO to operate on VLA actions, and with FlowScale, a stepwise reweighting that rescales per-step gradients in the flow head. Together, Prophet, FA-GRPO, and FlowScale constitute ProphRL, a practical, data- and compute-efficient path to VLA post-training. Experiments show 5-17% success gains on public benchmarks and 24-30% gains on real robots across different VLA variants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20633.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640d8a26b03f4cd29f52acdd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678608917790-noauth.png",
            "fullname": "Jiahui Zhang",
            "name": "jasonzhango",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "68fceb6d4be572720b6900d4",
            "name": "LogosRobotics",
            "fullname": "LogosRobotics"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.18452",
            "authors": [
                {
                    "_id": "6927339d243b2216fb75cca7",
                    "user": {
                        "_id": "646b8882e96a751c5252bf18",
                        "avatarUrl": "/avatars/14bbe1d53ce00e530b29fa3c6cac7a6b.svg",
                        "isPro": false,
                        "fullname": "Loick ",
                        "user": "LChambon",
                        "type": "user"
                    },
                    "name": "Loick Chambon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T09:59:23.747Z",
                    "hidden": false
                },
                {
                    "_id": "6927339d243b2216fb75cca8",
                    "name": "Paul Couairon",
                    "hidden": false
                },
                {
                    "_id": "6927339d243b2216fb75cca9",
                    "name": "Eloi Zablocki",
                    "hidden": false
                },
                {
                    "_id": "6927339d243b2216fb75ccaa",
                    "name": "Alexandre Boulch",
                    "hidden": false
                },
                {
                    "_id": "6927339d243b2216fb75ccab",
                    "name": "Nicolas Thome",
                    "hidden": false
                },
                {
                    "_id": "6927339d243b2216fb75ccac",
                    "name": "Matthieu Cord",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/646b8882e96a751c5252bf18/qI0Bc9jvexA_xyCP-opqd.mp4"
            ],
            "publishedAt": "2025-11-23T13:43:52.000Z",
            "submittedOnDailyAt": "2025-11-27T18:19:03.334Z",
            "title": "NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering",
            "submittedOnDailyBy": {
                "_id": "646b8882e96a751c5252bf18",
                "avatarUrl": "/avatars/14bbe1d53ce00e530b29fa3c6cac7a6b.svg",
                "isPro": false,
                "fullname": "Loick ",
                "user": "LChambon",
                "type": "user"
            },
            "summary": "Vision Foundation Models (VFMs) extract spatially downsampled representations, posing challenges for pixel-level tasks. Existing upsampling approaches face a fundamental trade-off: classical filters are fast and broadly applicable but rely on fixed forms, while modern upsamplers achieve superior accuracy through learnable, VFM-specific forms at the cost of retraining for each VFM. We introduce Neighborhood Attention Filtering (NAF), which bridges this gap by learning adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings (RoPE), guided solely by the high-resolution input image. NAF operates zero-shot: it upsamples features from any VFM without retraining, making it the first VFM-agnostic architecture to outperform VFM-specific upsamplers and achieve state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Beyond feature upsampling, NAF demonstrates strong performance on image restoration, highlighting its versatility. Code and checkpoints are available at https://github.com/valeoai/NAF.",
            "upvotes": 1,
            "discussionId": "6927339d243b2216fb75ccad",
            "ai_summary": "Neighborhood Attention Filtering (NAF) upsamples features from Vision Foundation Models without retraining, achieving state-of-the-art performance across tasks with high efficiency.",
            "ai_keywords": [
                "Neighborhood Attention Filtering (NAF)",
                "Vision Foundation Models (VFMs)",
                "spatially downsampled representations",
                "upsampling approaches",
                "classical filters",
                "learnable upsamplers",
                "Cross-Scale Neighborhood Attention",
                "Rotary Position Embeddings (RoPE)",
                "zero-shot upsampling",
                "feature upsampling",
                "image restoration"
            ]
        },
        "publishedAt": "2025-11-23T08:43:52.000Z",
        "title": "NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering",
        "summary": "Vision Foundation Models (VFMs) extract spatially downsampled representations, posing challenges for pixel-level tasks. Existing upsampling approaches face a fundamental trade-off: classical filters are fast and broadly applicable but rely on fixed forms, while modern upsamplers achieve superior accuracy through learnable, VFM-specific forms at the cost of retraining for each VFM. We introduce Neighborhood Attention Filtering (NAF), which bridges this gap by learning adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings (RoPE), guided solely by the high-resolution input image. NAF operates zero-shot: it upsamples features from any VFM without retraining, making it the first VFM-agnostic architecture to outperform VFM-specific upsamplers and achieve state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Beyond feature upsampling, NAF demonstrates strong performance on image restoration, highlighting its versatility. Code and checkpoints are available at https://github.com/valeoai/NAF.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/646b8882e96a751c5252bf18/qI0Bc9jvexA_xyCP-opqd.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18452.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "646b8882e96a751c5252bf18",
            "avatarUrl": "/avatars/14bbe1d53ce00e530b29fa3c6cac7a6b.svg",
            "fullname": "Loick ",
            "name": "LChambon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.18005",
            "authors": [
                {
                    "_id": "69283863805584b28040507a",
                    "name": "Shengyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "69283863805584b28040507b",
                    "name": "Zhiheng Zheng",
                    "hidden": false
                },
                {
                    "_id": "69283863805584b28040507c",
                    "name": "Yu Shang",
                    "hidden": false
                },
                {
                    "_id": "69283863805584b28040507d",
                    "name": "Lixuan He",
                    "hidden": false
                },
                {
                    "_id": "69283863805584b28040507e",
                    "name": "Yangcheng Yu",
                    "hidden": false
                },
                {
                    "_id": "69283863805584b28040507f",
                    "name": "Fan Hangyu",
                    "hidden": false
                },
                {
                    "_id": "69283863805584b280405080",
                    "user": {
                        "_id": "6465d3bd63e7e09dd02e95c3",
                        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
                        "isPro": false,
                        "fullname": "Jie Feng",
                        "user": "JJ-TMT",
                        "type": "user"
                    },
                    "name": "Jie Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T20:39:44.514Z",
                    "hidden": false
                },
                {
                    "_id": "69283863805584b280405081",
                    "name": "Qingmin Liao",
                    "hidden": false
                },
                {
                    "_id": "69283863805584b280405082",
                    "name": "Yong Li",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/IU7BNgpDOnNDxRquLjD6N.jpeg"
            ],
            "publishedAt": "2025-11-22T10:09:22.000Z",
            "submittedOnDailyAt": "2025-11-27T09:13:59.930Z",
            "title": "RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale",
            "submittedOnDailyBy": {
                "_id": "6465d3bd63e7e09dd02e95c3",
                "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
                "isPro": false,
                "fullname": "Jie Feng",
                "user": "JJ-TMT",
                "type": "user"
            },
            "summary": "City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECity, a Reality-Aligned Intelligent Synthesis Engine that creates detailed, City-scale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECity in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over a 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECity a promising foundation for applications in immersive media, embodied intelligence, and world models.",
            "upvotes": 1,
            "discussionId": "69283864805584b280405083",
            "githubRepo": "https://github.com/tsinghua-fib-lab/RAISECity",
            "ai_summary": "RAISECity generates high-quality, city-scale 3D worlds with real-world alignment, using an agentic framework with multimodal tools, iterative refinement, and advanced representations.",
            "ai_keywords": [
                "agentic framework",
                "multimodal foundation tools",
                "dynamic data processing",
                "iterative self-reflection",
                "reality-aligned",
                "shape precision",
                "texture fidelity",
                "aesthetics level",
                "perceptual quality",
                "computer graphics pipelines"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-11-22T05:09:22.000Z",
        "title": "RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale",
        "summary": "City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECity, a Reality-Aligned Intelligent Synthesis Engine that creates detailed, City-scale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECity in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over a 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECity a promising foundation for applications in immersive media, embodied intelligence, and world models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/IU7BNgpDOnNDxRquLjD6N.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18005.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6465d3bd63e7e09dd02e95c3",
            "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
            "fullname": "Jie Feng",
            "name": "JJ-TMT",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.21208",
            "authors": [
                {
                    "_id": "692808b8243b2216fb75ce41",
                    "user": {
                        "_id": "63e24a0cf0740bec2bfd7f9e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e24a0cf0740bec2bfd7f9e/VS786oOKIF9IqIm3xnniS.jpeg",
                        "isPro": false,
                        "fullname": "Lucas Thil",
                        "user": "LucasThil",
                        "type": "user"
                    },
                    "name": "Lucas Thil",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T08:58:03.581Z",
                    "hidden": false
                },
                {
                    "_id": "692808b8243b2216fb75ce42",
                    "name": "Jesse Read",
                    "hidden": false
                },
                {
                    "_id": "692808b8243b2216fb75ce43",
                    "name": "Rim Kaddah",
                    "hidden": false
                },
                {
                    "_id": "692808b8243b2216fb75ce44",
                    "name": "Guillaume Doquet",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-26T09:39:35.000Z",
            "submittedOnDailyAt": "2025-11-27T05:47:11.194Z",
            "title": "I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation",
            "submittedOnDailyBy": {
                "_id": "63e24a0cf0740bec2bfd7f9e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e24a0cf0740bec2bfd7f9e/VS786oOKIF9IqIm3xnniS.jpeg",
                "isPro": false,
                "fullname": "Lucas Thil",
                "user": "LucasThil",
                "type": "user"
            },
            "summary": "Accurate remaining useful life (RUL) prediction hinges on the quality of health indicators (HIs), yet existing methods often fail to disentangle complex degradation mechanisms in multi-sensor systems or quantify uncertainty in HI reliability. This paper introduces a novel framework for HI construction, advancing three key contributions. First, we adapt Reconstruction along Projected Pathways (RaPP) as a health indicator (HI) for RUL prediction for the first time, showing that it outperforms traditional reconstruction error metrics. Second, we show that augmenting RaPP-derived HIs with aleatoric and epistemic uncertainty quantification (UQ) via Monte Carlo dropout and probabilistic latent spaces- significantly improves RUL-prediction robustness. Third, and most critically, we propose indicator groups, a paradigm that isolates sensor subsets to model system-specific degradations, giving rise to our novel method, I-GLIDE which enables interpretable, mechanism-specific diagnostics. Evaluated on data sourced from aerospace and manufacturing systems, our approach achieves marked improvements in accuracy and generalizability compared to state-of-the-art HI methods while providing actionable insights into system failure pathways. This work bridges the gap between anomaly detection and prognostics, offering a principled framework for uncertainty-aware degradation modeling in complex systems.",
            "upvotes": 0,
            "discussionId": "692808b8243b2216fb75ce45",
            "projectPage": "https://lucasandrei.com/pages/i_glide.html",
            "githubRepo": "https://github.com/LucasStill/I-GLIDE",
            "ai_summary": "A novel framework using RaPP and uncertainty quantification improves RUL prediction accuracy and interpretability in multi-sensor systems.",
            "ai_keywords": [
                "Reconstruction along Projected Pathways (RaPP)",
                "aleatoric uncertainty",
                "epistemic uncertainty",
                "Monte Carlo dropout",
                "probabilistic latent spaces",
                "indicator groups",
                "I-GLIDE",
                "anomaly detection",
                "prognostics"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "691f39b236fe6b60136afe00",
                "name": "orailix",
                "fullname": "Orailix",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/654e1e137cac9d7b51418f83/EwhwHf5mYSV65VRf-5bXb.png"
            }
        },
        "publishedAt": "2025-11-26T04:39:35.000Z",
        "title": "I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation",
        "summary": "Accurate remaining useful life (RUL) prediction hinges on the quality of health indicators (HIs), yet existing methods often fail to disentangle complex degradation mechanisms in multi-sensor systems or quantify uncertainty in HI reliability. This paper introduces a novel framework for HI construction, advancing three key contributions. First, we adapt Reconstruction along Projected Pathways (RaPP) as a health indicator (HI) for RUL prediction for the first time, showing that it outperforms traditional reconstruction error metrics. Second, we show that augmenting RaPP-derived HIs with aleatoric and epistemic uncertainty quantification (UQ) via Monte Carlo dropout and probabilistic latent spaces- significantly improves RUL-prediction robustness. Third, and most critically, we propose indicator groups, a paradigm that isolates sensor subsets to model system-specific degradations, giving rise to our novel method, I-GLIDE which enables interpretable, mechanism-specific diagnostics. Evaluated on data sourced from aerospace and manufacturing systems, our approach achieves marked improvements in accuracy and generalizability compared to state-of-the-art HI methods while providing actionable insights into system failure pathways. This work bridges the gap between anomaly detection and prognostics, offering a principled framework for uncertainty-aware degradation modeling in complex systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21208.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63e24a0cf0740bec2bfd7f9e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e24a0cf0740bec2bfd7f9e/VS786oOKIF9IqIm3xnniS.jpeg",
            "fullname": "Lucas Thil",
            "name": "LucasThil",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "691f39b236fe6b60136afe00",
            "name": "orailix",
            "fullname": "Orailix",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/654e1e137cac9d7b51418f83/EwhwHf5mYSV65VRf-5bXb.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.19504",
            "authors": [
                {
                    "_id": "69286865805584b280405112",
                    "name": "Subramanyam Sahoo",
                    "hidden": false
                },
                {
                    "_id": "69286865805584b280405113",
                    "user": {
                        "_id": "63a4754927f1f64ed7238dac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                        "isPro": false,
                        "fullname": "Aman Chadha",
                        "user": "amanchadha",
                        "type": "user"
                    },
                    "name": "Aman Chadha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-27T20:39:36.790Z",
                    "hidden": false
                },
                {
                    "_id": "69286865805584b280405114",
                    "name": "Vinija Jain",
                    "hidden": false
                },
                {
                    "_id": "69286865805584b280405115",
                    "name": "Divya Chaudhary",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-23T20:23:23.000Z",
            "submittedOnDailyAt": "2025-11-27T13:18:43.979Z",
            "title": "Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma",
            "submittedOnDailyBy": {
                "_id": "63a4754927f1f64ed7238dac",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                "isPro": false,
                "fullname": "Aman Chadha",
                "user": "amanchadha",
                "type": "user"
            },
            "summary": "Reinforcement Learning from Human Feedback (RLHF) is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as the Alignment Trilemma: no RLHF system can simultaneously achieve (i) epsilon-representativeness across diverse human values, (ii) polynomial tractability in sample and compute complexity, and (iii) delta-robustness against adversarial perturbations and distribution shift. Through a complexity-theoretic analysis integrating statistical learning theory and robust optimization, we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies including preference collapse, sycophancy, and systematic bias amplification. We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements.",
            "upvotes": 0,
            "discussionId": "69286865805584b280405116",
            "ai_summary": "The Alignment Trilemma in RLHF shows that achieving representativeness, tractability, and robustness is computationally infeasible, leading to trade-offs in current implementations.",
            "ai_keywords": [
                "Reinforcement Learning from Human Feedback (RLHF)",
                "Alignment Trilemma",
                "epsilon-representativeness",
                "polynomial tractability",
                "delta-robustness",
                "complexity-theoretic analysis",
                "statistical learning theory",
                "robust optimization",
                "preference collapse",
                "sycophancy",
                "systematic bias amplification"
            ]
        },
        "publishedAt": "2025-11-23T15:23:23.000Z",
        "title": "Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as the Alignment Trilemma: no RLHF system can simultaneously achieve (i) epsilon-representativeness across diverse human values, (ii) polynomial tractability in sample and compute complexity, and (iii) delta-robustness against adversarial perturbations and distribution shift. Through a complexity-theoretic analysis integrating statistical learning theory and robust optimization, we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies including preference collapse, sycophancy, and systematic bias amplification. We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19504.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "fullname": "Aman Chadha",
            "name": "amanchadha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.17918",
            "authors": [
                {
                    "_id": "692680db243b2216fb75cab2",
                    "user": {
                        "_id": "68e34e1f349df589247872e4",
                        "avatarUrl": "/avatars/884f38e7b4370d0661aab7a93dd1cd5d.svg",
                        "isPro": false,
                        "fullname": "Youngsik Yun",
                        "user": "bbangsik13",
                        "type": "user"
                    },
                    "name": "Youngsik Yun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-26T09:27:27.289Z",
                    "hidden": false
                },
                {
                    "_id": "692680db243b2216fb75cab3",
                    "name": "Dongjun Gu",
                    "hidden": false
                },
                {
                    "_id": "692680db243b2216fb75cab4",
                    "name": "Youngjung Uh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-22T05:04:46.000Z",
            "submittedOnDailyAt": "2025-11-27T02:06:03.890Z",
            "title": "Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization",
            "submittedOnDailyBy": {
                "_id": "68e34e1f349df589247872e4",
                "avatarUrl": "/avatars/884f38e7b4370d0661aab7a93dd1cd5d.svg",
                "isPro": false,
                "fullname": "Youngsik Yun",
                "user": "bbangsik13",
                "type": "user"
            },
            "summary": "Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.",
            "upvotes": 0,
            "discussionId": "692680db243b2216fb75cab5",
            "projectPage": "https://bbangsik13.github.io/FASR",
            "githubRepo": "https://github.com/bbangsik13/FASR",
            "ai_summary": "Frequency-Adaptive Sharpness Regularization (FASR) enhances 3D Gaussian Splatting's generalization to novel viewpoints by adaptively adjusting regularization based on local image frequency.",
            "ai_keywords": [
                "3D Gaussian Splatting",
                "3DGS",
                "Frequency-Adaptive Sharpness Regularization",
                "FASR",
                "Sharpness-Aware Minimization",
                "SAM",
                "generalization",
                "novel view synthesis",
                "sharpness regularization",
                "high-frequency details",
                "floater artifacts"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-11-22T00:04:46.000Z",
        "title": "Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization",
        "summary": "Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17918.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68e34e1f349df589247872e4",
            "avatarUrl": "/avatars/884f38e7b4370d0661aab7a93dd1cd5d.svg",
            "fullname": "Youngsik Yun",
            "name": "bbangsik13",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]