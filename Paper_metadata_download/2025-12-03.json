[
    {
        "paper": {
            "id": "2512.02556",
            "authors": [
                {
                    "_id": "692fa6da26742347f61dab24",
                    "name": "DeepSeek-AI",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab25",
                    "name": "Aixin Liu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab26",
                    "name": "Aoxue Mei",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab27",
                    "name": "Bangcai Lin",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab28",
                    "name": "Bing Xue",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab29",
                    "user": {
                        "_id": "6523d81d56fe05f216a559f6",
                        "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg",
                        "isPro": false,
                        "fullname": "Bingxuan Wang",
                        "user": "YellowDoge",
                        "type": "user"
                    },
                    "name": "Bingxuan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-03T09:26:23.047Z",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab2a",
                    "name": "Bingzheng Xu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab2b",
                    "name": "Bochao Wu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab2c",
                    "name": "Bowei Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab2d",
                    "user": {
                        "_id": "644200d95d600fb09520de53",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg",
                        "isPro": false,
                        "fullname": "Chaofan Lin",
                        "user": "siriusneo",
                        "type": "user"
                    },
                    "name": "Chaofan Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-03T09:26:56.864Z",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab2e",
                    "name": "Chen Dong",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab2f",
                    "name": "Chengda Lu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab30",
                    "name": "Chenggang Zhao",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab31",
                    "name": "Chengqi Deng",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab32",
                    "name": "Chenhao Xu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab33",
                    "name": "Chong Ruan",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab34",
                    "name": "Damai Dai",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab35",
                    "name": "Daya Guo",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab36",
                    "name": "Dejian Yang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab37",
                    "name": "Deli Chen",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab38",
                    "name": "Erhang Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab39",
                    "name": "Fangqi Zhou",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab3a",
                    "name": "Fangyun Lin",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab3b",
                    "name": "Fucong Dai",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab3c",
                    "name": "Guangbo Hao",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab3d",
                    "name": "Guanting Chen",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab3e",
                    "name": "Guowei Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab3f",
                    "name": "H. Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab40",
                    "name": "Hanwei Xu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab41",
                    "name": "Hao Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab42",
                    "name": "Haofen Liang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab43",
                    "name": "Haoran Wei",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab44",
                    "name": "Haowei Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab45",
                    "name": "Haowen Luo",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab46",
                    "name": "Haozhe Ji",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab47",
                    "name": "Honghui Ding",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab48",
                    "name": "Hongxuan Tang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab49",
                    "name": "Huanqi Cao",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab4a",
                    "name": "Huazuo Gao",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab4b",
                    "name": "Hui Qu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab4c",
                    "name": "Hui Zeng",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab4d",
                    "name": "Jialiang Huang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab4e",
                    "name": "Jiashi Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab4f",
                    "name": "Jiaxin Xu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab50",
                    "name": "Jiewen Hu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab51",
                    "name": "Jingchang Chen",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab52",
                    "name": "Jingting Xiang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab53",
                    "name": "Jingyang Yuan",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab54",
                    "name": "Jingyuan Cheng",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab55",
                    "name": "Jinhua Zhu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab56",
                    "name": "Jun Ran",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab57",
                    "name": "Junguang Jiang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab58",
                    "name": "Junjie Qiu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab59",
                    "name": "Junlong Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab5a",
                    "name": "Junxiao Song",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab5b",
                    "name": "Kai Dong",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab5c",
                    "name": "Kaige Gao",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab5d",
                    "name": "Kang Guan",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab5e",
                    "name": "Kexin Huang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab5f",
                    "name": "Kexing Zhou",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab60",
                    "name": "Kezhao Huang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab61",
                    "name": "Kuai Yu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab62",
                    "name": "Lean Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab63",
                    "name": "Lecong Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab64",
                    "name": "Lei Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab65",
                    "name": "Liang Zhao",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab66",
                    "name": "Liangsheng Yin",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab67",
                    "name": "Lihua Guo",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab68",
                    "name": "Lingxiao Luo",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab69",
                    "name": "Linwang Ma",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab6a",
                    "name": "Litong Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab6b",
                    "name": "Liyue Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab6c",
                    "name": "M. S. Di",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab6d",
                    "name": "M. Y Xu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab6e",
                    "name": "Mingchuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab6f",
                    "name": "Minghua Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab70",
                    "name": "Minghui Tang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab71",
                    "name": "Mingxu Zhou",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab72",
                    "name": "Panpan Huang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab73",
                    "name": "Peixin Cong",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab74",
                    "name": "Peiyi Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab75",
                    "name": "Qiancheng Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab76",
                    "name": "Qihao Zhu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab77",
                    "name": "Qingyang Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab78",
                    "name": "Qinyu Chen",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab79",
                    "name": "Qiushi Du",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab7a",
                    "name": "Ruiling Xu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab7b",
                    "name": "Ruiqi Ge",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab7c",
                    "name": "Ruisong Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab7d",
                    "name": "Ruizhe Pan",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab7e",
                    "name": "Runji Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab7f",
                    "name": "Runqiu Yin",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab80",
                    "name": "Runxin Xu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab81",
                    "name": "Ruomeng Shen",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab82",
                    "name": "Ruoyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab83",
                    "name": "S. H. Liu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab84",
                    "name": "Shanghao Lu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab85",
                    "name": "Shangyan Zhou",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab86",
                    "name": "Shanhuang Chen",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab87",
                    "name": "Shaofei Cai",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab88",
                    "name": "Shaoyuan Chen",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab89",
                    "name": "Shengding Hu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab8a",
                    "name": "Shengyu Liu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab8b",
                    "name": "Shiqiang Hu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab8c",
                    "name": "Shirong Ma",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab8d",
                    "name": "Shiyu Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab8e",
                    "name": "Shuiping Yu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab8f",
                    "name": "Shunfeng Zhou",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab90",
                    "name": "Shuting Pan",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab91",
                    "name": "Songyang Zhou",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab92",
                    "name": "Tao Ni",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab93",
                    "name": "Tao Yun",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab94",
                    "name": "Tian Pei",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab95",
                    "name": "Tian Ye",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab96",
                    "name": "Tianyuan Yue",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab97",
                    "name": "Wangding Zeng",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab98",
                    "name": "Wen Liu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab99",
                    "name": "Wenfeng Liang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab9a",
                    "name": "Wenjie Pang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab9b",
                    "name": "Wenjing Luo",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab9c",
                    "name": "Wenjun Gao",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab9d",
                    "name": "Wentao Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab9e",
                    "name": "Xi Gao",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dab9f",
                    "name": "Xiangwen Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61daba0",
                    "name": "Xiao Bi",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61daba1",
                    "name": "Xiaodong Liu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61daba2",
                    "name": "Xiaohan Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61daba3",
                    "name": "Xiaokang Chen",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61daba4",
                    "name": "Xiaokang Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61daba5",
                    "name": "Xiaotao Nie",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61daba6",
                    "name": "Xin Cheng",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61daba7",
                    "name": "Xin Liu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61daba8",
                    "name": "Xin Xie",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61daba9",
                    "name": "Xingchao Liu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabaa",
                    "name": "Xingkai Yu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabab",
                    "name": "Xingyou Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabac",
                    "name": "Xinyu Yang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabad",
                    "name": "Xinyuan Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabae",
                    "name": "Xu Chen",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabaf",
                    "name": "Xuecheng Su",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabb0",
                    "name": "Xuehai Pan",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabb1",
                    "name": "Xuheng Lin",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabb2",
                    "name": "Xuwei Fu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabb3",
                    "name": "Y. Q. Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabb4",
                    "name": "Yang Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabb5",
                    "name": "Yanhong Xu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabb6",
                    "name": "Yanru Ma",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabb7",
                    "name": "Yao Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabb8",
                    "name": "Yao Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabb9",
                    "name": "Yao Zhao",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabba",
                    "name": "Yaofeng Sun",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabbb",
                    "name": "Yaohui Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabbc",
                    "name": "Yi Qian",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabbd",
                    "name": "Yi Yu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabbe",
                    "name": "Yichao Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabbf",
                    "name": "Yifan Ding",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabc0",
                    "name": "Yifan Shi",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabc1",
                    "name": "Yiliang Xiong",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabc2",
                    "name": "Ying He",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabc3",
                    "name": "Ying Zhou",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabc4",
                    "name": "Yinmin Zhong",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabc5",
                    "name": "Yishi Piao",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabc6",
                    "name": "Yisong Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabc7",
                    "name": "Yixiao Chen",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabc8",
                    "name": "Yixuan Tan",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabc9",
                    "name": "Yixuan Wei",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabca",
                    "name": "Yiyang Ma",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabcb",
                    "name": "Yiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabcc",
                    "name": "Yonglun Yang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabcd",
                    "name": "Yongqiang Guo",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabce",
                    "name": "Yongtong Wu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabcf",
                    "name": "Yu Wu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabd0",
                    "name": "Yuan Cheng",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabd1",
                    "name": "Yuan Ou",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabd2",
                    "name": "Yuanfan Xu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabd3",
                    "name": "Yuduan Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabd4",
                    "name": "Yue Gong",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabd5",
                    "name": "Yuhan Wu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabd6",
                    "name": "Yuheng Zou",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabd7",
                    "name": "Yukun Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabd8",
                    "name": "Yunfan Xiong",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabd9",
                    "name": "Yuxiang Luo",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabda",
                    "name": "Yuxiang You",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabdb",
                    "name": "Yuxuan Liu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabdc",
                    "name": "Yuyang Zhou",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabdd",
                    "name": "Z. F. Wu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabde",
                    "name": "Z. Z. Ren",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabdf",
                    "name": "Zehua Zhao",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabe0",
                    "name": "Zehui Ren",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabe1",
                    "name": "Zhangli Sha",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabe2",
                    "name": "Zhe Fu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabe3",
                    "name": "Zhean Xu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabe4",
                    "name": "Zhenda Xie",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabe5",
                    "name": "Zhengyan Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabe6",
                    "name": "Zhewen Hao",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabe7",
                    "name": "Zhibin Gou",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabe8",
                    "name": "Zhicheng Ma",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabe9",
                    "name": "Zhigang Yan",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabea",
                    "name": "Zhihong Shao",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabeb",
                    "name": "Zhixian Huang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabec",
                    "name": "Zhiyu Wu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabed",
                    "name": "Zhuoshu Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabee",
                    "name": "Zhuping Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabef",
                    "name": "Zian Xu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabf0",
                    "name": "Zihao Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabf1",
                    "name": "Zihui Gu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabf2",
                    "name": "Zijia Zhu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabf3",
                    "name": "Zilin Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabf4",
                    "name": "Zipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabf5",
                    "name": "Ziwei Xie",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabf6",
                    "name": "Ziyi Gao",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabf7",
                    "name": "Zizheng Pan",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabf8",
                    "name": "Zongqing Yao",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabf9",
                    "name": "Bei Feng",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabfa",
                    "name": "Hui Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabfb",
                    "name": "J. L. Cai",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabfc",
                    "name": "Jiaqi Ni",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabfd",
                    "name": "Lei Xu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabfe",
                    "name": "Meng Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dabff",
                    "name": "Ning Tian",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac00",
                    "name": "R. J. Chen",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac01",
                    "name": "R. L. Jin",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac02",
                    "name": "S. S. Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac03",
                    "name": "Shuang Zhou",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac04",
                    "name": "Tianyu Sun",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac05",
                    "name": "X. Q. Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac06",
                    "name": "Xiangyue Jin",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac07",
                    "name": "Xiaojin Shen",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac08",
                    "name": "Xiaosha Chen",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac09",
                    "name": "Xinnan Song",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac0a",
                    "name": "Xinyi Zhou",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac0b",
                    "name": "Y. X. Zhu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac0c",
                    "name": "Yanping Huang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac0d",
                    "name": "Yaohui Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac0e",
                    "name": "Yi Zheng",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac0f",
                    "name": "Yuchen Zhu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac10",
                    "name": "Yunxian Ma",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac11",
                    "name": "Zhen Huang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac12",
                    "name": "Zhipeng Xu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac13",
                    "name": "Zhongyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac14",
                    "name": "Dongjie Ji",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac15",
                    "name": "Jian Liang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac16",
                    "name": "Jianzhong Guo",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac17",
                    "name": "Jin Chen",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac18",
                    "name": "Leyi Xia",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac19",
                    "name": "Miaojun Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac1a",
                    "name": "Mingming Li",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac1b",
                    "name": "Peng Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac1c",
                    "name": "Ruyi Chen",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac1d",
                    "name": "Shangmian Sun",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac1e",
                    "name": "Shaoqing Wu",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac1f",
                    "name": "Shengfeng Ye",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac20",
                    "name": "T. Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac21",
                    "name": "W. L. Xiao",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac22",
                    "name": "Wei An",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac23",
                    "name": "Xianzu Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac24",
                    "name": "Xiaowen Sun",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac25",
                    "name": "Xiaoxiang Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac26",
                    "name": "Ying Tang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac27",
                    "name": "Yukun Zha",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac28",
                    "name": "Zekai Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac29",
                    "name": "Zhe Ju",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac2a",
                    "name": "Zhen Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa6da26742347f61dac2b",
                    "name": "Zihua Qu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T09:25:14.000Z",
            "submittedOnDailyAt": "2025-12-03T00:26:37.248Z",
            "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.",
            "upvotes": 109,
            "discussionId": "692fa6da26742347f61dac2c",
            "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.",
            "ai_keywords": [
                "DeepSeek Sparse Attention",
                "DSA",
                "reinforcement learning framework",
                "agentic task synthesis pipeline",
                "computational efficiency",
                "long-context scenarios",
                "gold-medal performance",
                "International Mathematical Olympiad",
                "International Olympiad in Informatics",
                "reasoning proficiency"
            ],
            "organization": {
                "_id": "652faff917096ceb6bf53f3f",
                "name": "deepseek-ai",
                "fullname": "DeepSeek",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"
            }
        },
        "publishedAt": "2025-12-02T04:25:14.000Z",
        "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models",
        "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 176
        },
        "organization": {
            "_id": "652faff917096ceb6bf53f3f",
            "name": "deepseek-ai",
            "fullname": "DeepSeek",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.21689",
            "authors": [
                {
                    "_id": "692f11ffbfc6eea1b6fb6900",
                    "name": "Hongjin Su",
                    "hidden": false
                },
                {
                    "_id": "692f11ffbfc6eea1b6fb6901",
                    "name": "Shizhe Diao",
                    "hidden": false
                },
                {
                    "_id": "692f11ffbfc6eea1b6fb6902",
                    "name": "Ximing Lu",
                    "hidden": false
                },
                {
                    "_id": "692f11ffbfc6eea1b6fb6903",
                    "name": "Mingjie Liu",
                    "hidden": false
                },
                {
                    "_id": "692f11ffbfc6eea1b6fb6904",
                    "name": "Jiacheng Xu",
                    "hidden": false
                },
                {
                    "_id": "692f11ffbfc6eea1b6fb6905",
                    "name": "Xin Dong",
                    "hidden": false
                },
                {
                    "_id": "692f11ffbfc6eea1b6fb6906",
                    "name": "Yonggan Fu",
                    "hidden": false
                },
                {
                    "_id": "692f11ffbfc6eea1b6fb6907",
                    "name": "Peter Belcak",
                    "hidden": false
                },
                {
                    "_id": "692f11ffbfc6eea1b6fb6908",
                    "name": "Hanrong Ye",
                    "hidden": false
                },
                {
                    "_id": "692f11ffbfc6eea1b6fb6909",
                    "name": "Hongxu Yin",
                    "hidden": false
                },
                {
                    "_id": "692f11ffbfc6eea1b6fb690a",
                    "name": "Yi Dong",
                    "hidden": false
                },
                {
                    "_id": "692f11ffbfc6eea1b6fb690b",
                    "name": "Evelina Bakhturina",
                    "hidden": false
                },
                {
                    "_id": "692f11ffbfc6eea1b6fb690c",
                    "name": "Tao Yu",
                    "hidden": false
                },
                {
                    "_id": "692f11ffbfc6eea1b6fb690d",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "692f11ffbfc6eea1b6fb690e",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "692f11ffbfc6eea1b6fb690f",
                    "name": "Pavlo Molchanov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-26T18:59:46.000Z",
            "submittedOnDailyAt": "2025-12-03T13:46:43.945Z",
            "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
            "submittedOnDailyBy": {
                "_id": "633bd54b00732349209a18fe",
                "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
                "isPro": false,
                "fullname": "Shizhe Diao",
                "user": "shizhediao",
                "type": "user"
            },
            "summary": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.",
            "upvotes": 59,
            "discussionId": "692f11ffbfc6eea1b6fb6910",
            "projectPage": "https://research.nvidia.com/labs/lpr/ToolOrchestra/",
            "githubRepo": "https://github.com/NVlabs/ToolOrchestra/",
            "ai_summary": "A small orchestrator using ToolOrchestra method coordinates various intelligent tools with reinforcement learning, achieving higher accuracy and efficiency in solving complex tasks like Humanity's Last Exam compared to larger models.",
            "ai_keywords": [
                "large language models",
                "ToolOrchestra",
                "reinforcement learning",
                "outcome-aware rewards",
                "efficiency-aware rewards",
                "user-preference-aware rewards",
                "Orchestrator",
                "tool-use agents",
                "humanity's last exam",
                "tau2-bench",
                "FRAMES",
                "tool-augmented reasoning systems"
            ],
            "githubStars": 230,
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-11-26T13:59:46.000Z",
        "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
        "summary": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21689.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "633bd54b00732349209a18fe",
            "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
            "fullname": "Shizhe Diao",
            "name": "shizhediao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.03041",
            "authors": [
                {
                    "_id": "692fcb3726742347f61dad18",
                    "user": {
                        "_id": "646f3418a6a58aa29505fd30",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f3418a6a58aa29505fd30/1z13rnpb6rsUgQsYumWPg.png",
                        "isPro": false,
                        "fullname": "QINGHE WANG",
                        "user": "DecoderWQH666",
                        "type": "user"
                    },
                    "name": "Qinghe Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:13:33.820Z",
                    "hidden": false
                },
                {
                    "_id": "692fcb3726742347f61dad19",
                    "name": "Xiaoyu Shi",
                    "hidden": false
                },
                {
                    "_id": "692fcb3726742347f61dad1a",
                    "user": {
                        "_id": "652cb3e1c6857682d30d4c2b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JEQW6WZBcmjAWwODiDkYA.jpeg",
                        "isPro": false,
                        "fullname": "Libaolu",
                        "user": "8ruceLi",
                        "type": "user"
                    },
                    "name": "Baolu Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:13:31.176Z",
                    "hidden": false
                },
                {
                    "_id": "692fcb3726742347f61dad1b",
                    "name": "Weikang Bian",
                    "hidden": false
                },
                {
                    "_id": "692fcb3726742347f61dad1c",
                    "name": "Quande Liu",
                    "hidden": false
                },
                {
                    "_id": "692fcb3726742347f61dad1d",
                    "name": "Huchuan Lu",
                    "hidden": false
                },
                {
                    "_id": "692fcb3726742347f61dad1e",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "692fcb3726742347f61dad1f",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "692fcb3726742347f61dad20",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "692fcb3726742347f61dad21",
                    "name": "Xu Jia",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T18:59:48.000Z",
            "submittedOnDailyAt": "2025-12-03T03:10:24.737Z",
            "title": "MultiShotMaster: A Controllable Multi-Shot Video Generation Framework",
            "submittedOnDailyBy": {
                "_id": "646f3418a6a58aa29505fd30",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f3418a6a58aa29505fd30/1z13rnpb6rsUgQsYumWPg.png",
                "isPro": false,
                "fullname": "QINGHE WANG",
                "user": "DecoderWQH666",
                "type": "user"
            },
            "summary": "Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.",
            "upvotes": 50,
            "discussionId": "692fcb3726742347f61dad22",
            "projectPage": "https://qinghew.github.io/MultiShotMaster/",
            "ai_summary": "MultiShotMaster extends a single-shot model with novel RoPE variants for flexible and controllable multi-shot video generation, addressing data scarcity with an automated annotation pipeline.",
            "ai_keywords": [
                "RoPE",
                "Multi-Shot Narrative RoPE",
                "Spatiotemporal Position-Aware RoPE",
                "reference tokens",
                "cross-shot grounding signals",
                "reference images",
                "text-driven inter-shot consistency",
                "motion control",
                "background-driven customized scene"
            ],
            "organization": {
                "_id": "662c559b322afcbae51b3c8b",
                "name": "KlingTeam",
                "fullname": "Kling Team",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
            }
        },
        "publishedAt": "2025-12-02T13:59:48.000Z",
        "title": "MultiShotMaster: A Controllable Multi-Shot Video Generation Framework",
        "summary": "Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03041.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "646f3418a6a58aa29505fd30",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f3418a6a58aa29505fd30/1z13rnpb6rsUgQsYumWPg.png",
            "fullname": "QINGHE WANG",
            "name": "DecoderWQH666",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "organization": {
            "_id": "662c559b322afcbae51b3c8b",
            "name": "KlingTeam",
            "fullname": "Kling Team",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.22609",
            "authors": [
                {
                    "_id": "692fa76826742347f61dac2e",
                    "user": {
                        "_id": "68a32c6e87458cb74ce765a8",
                        "avatarUrl": "/avatars/95fc9bf8a575eed4d083052c6feaf715.svg",
                        "isPro": false,
                        "fullname": "bo",
                        "user": "bogogogogo",
                        "type": "user"
                    },
                    "name": "Bo Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:15:56.436Z",
                    "hidden": false
                },
                {
                    "_id": "692fa76826742347f61dac2f",
                    "name": "Jiehong Lin",
                    "hidden": false
                },
                {
                    "_id": "692fa76826742347f61dac30",
                    "name": "Chenzhi Liu",
                    "hidden": false
                },
                {
                    "_id": "692fa76826742347f61dac31",
                    "name": "Xinting Hu",
                    "hidden": false
                },
                {
                    "_id": "692fa76826742347f61dac32",
                    "name": "Yifei Yu",
                    "hidden": false
                },
                {
                    "_id": "692fa76826742347f61dac33",
                    "name": "Tianjia Liu",
                    "hidden": false
                },
                {
                    "_id": "692fa76826742347f61dac34",
                    "name": "Zhongrui Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa76826742347f61dac35",
                    "name": "Xiaojuan Qi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-27T16:43:21.000Z",
            "submittedOnDailyAt": "2025-12-03T02:54:07.128Z",
            "title": "MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory",
            "submittedOnDailyBy": {
                "_id": "652845524def261e0febbf5b",
                "avatarUrl": "/avatars/92743e595cbce4e37a8fe7400b5051ba.svg",
                "isPro": false,
                "fullname": "Xinting Hu",
                "user": "kaleidudu",
                "type": "user"
            },
            "summary": "We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions.",
            "upvotes": 44,
            "discussionId": "692fa76826742347f61dac36",
            "ai_summary": "MG-Nav, a dual-scale framework for zero-shot visual navigation, combines global memory-guided planning with local geometry-enhanced control using a Sparse Spatial Memory Graph and a VGGT-adapter for robust navigation in unseen environments.",
            "ai_keywords": [
                "Sparse Spatial Memory Graph",
                "SMG",
                "keyframe",
                "object semantics",
                "image-to-instance hybrid retrieval",
                "waypoint",
                "point-goal mode",
                "image-goal mode",
                "obstacle-aware control",
                "VGGT-adapter",
                "3D-aware space",
                "HM3D",
                "MP3D",
                "zero-shot performance",
                "dynamic rearrangements"
            ],
            "organization": {
                "_id": "66deb312fd7d68a29348aa8d",
                "name": "TheHKU",
                "fullname": "Hong Kong University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66dc525add44163a31059cf6/kyqlTADY27mPRTqznqQFL.png"
            }
        },
        "publishedAt": "2025-11-27T11:43:21.000Z",
        "title": "MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory",
        "summary": "We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22609.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652845524def261e0febbf5b",
            "avatarUrl": "/avatars/92743e595cbce4e37a8fe7400b5051ba.svg",
            "fullname": "Xinting Hu",
            "name": "kaleidudu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "66deb312fd7d68a29348aa8d",
            "name": "TheHKU",
            "fullname": "Hong Kong University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66dc525add44163a31059cf6/kyqlTADY27mPRTqznqQFL.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.23127",
            "authors": [
                {
                    "_id": "692ea20837312eaa83fd8988",
                    "user": {
                        "_id": "66864408c1d35ab079488662",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66864408c1d35ab079488662/U-btVsJ65j7m6vFLVIka8.jpeg",
                        "isPro": false,
                        "fullname": "Hongfei (Faye) Zhang",
                        "user": "FayeHongfeiZhang",
                        "type": "user"
                    },
                    "name": "Hongfei Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:17:37.157Z",
                    "hidden": false
                },
                {
                    "_id": "692ea20837312eaa83fd8989",
                    "name": "Kanghao Chen",
                    "hidden": false
                },
                {
                    "_id": "692ea20837312eaa83fd898a",
                    "name": "Zixin Zhang",
                    "hidden": false
                },
                {
                    "_id": "692ea20837312eaa83fd898b",
                    "user": {
                        "_id": "6570450a78d7aca0c361a177",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/MX7jHhTQwLs-BvYIu5rqb.jpeg",
                        "isPro": false,
                        "fullname": "Harold Chen",
                        "user": "Harold328",
                        "type": "user"
                    },
                    "name": "Harold Haodong Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:16:15.369Z",
                    "hidden": false
                },
                {
                    "_id": "692ea20837312eaa83fd898c",
                    "name": "Yuanhuiyi Lyu",
                    "hidden": false
                },
                {
                    "_id": "692ea20837312eaa83fd898d",
                    "user": {
                        "_id": "692b0ee2cc35d0556d48e290",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692b0ee2cc35d0556d48e290/rXH58_Ub7nywDlIDpUcph.png",
                        "isPro": false,
                        "fullname": "YuqiZhang",
                        "user": "murphy0103",
                        "type": "user"
                    },
                    "name": "Yuqi Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:16:13.161Z",
                    "hidden": false
                },
                {
                    "_id": "692ea20837312eaa83fd898e",
                    "name": "Shuai Yang",
                    "hidden": false
                },
                {
                    "_id": "692ea20837312eaa83fd898f",
                    "name": "Kun Zhou",
                    "hidden": false
                },
                {
                    "_id": "692ea20837312eaa83fd8990",
                    "name": "Yingcong Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-28T12:19:57.000Z",
            "submittedOnDailyAt": "2025-12-03T01:09:15.156Z",
            "title": "DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation",
            "submittedOnDailyBy": {
                "_id": "66864408c1d35ab079488662",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66864408c1d35ab079488662/U-btVsJ65j7m6vFLVIka8.jpeg",
                "isPro": false,
                "fullname": "Hongfei (Faye) Zhang",
                "user": "FayeHongfeiZhang",
                "type": "user"
            },
            "summary": "This paper presents DualCamCtrl, a novel end-to-end diffusion model for camera-controlled video generation. Recent works have advanced this field by representing camera poses as ray-based conditions, yet they often lack sufficient scene understanding and geometric awareness. DualCamCtrl specifically targets this limitation by introducing a dual-branch framework that mutually generates camera-consistent RGB and depth sequences. To harmonize these two modalities, we further propose the Semantic Guided Mutual Alignment (SIGMA) mechanism, which performs RGB-depth fusion in a semantics-guided and mutually reinforced manner. These designs collectively enable DualCamCtrl to better disentangle appearance and geometry modeling, generating videos that more faithfully adhere to the specified camera trajectories. Additionally, we analyze and reveal the distinct influence of depth and camera poses across denoising stages and further demonstrate that early and late stages play complementary roles in forming global structure and refining local details. Extensive experiments demonstrate that DualCamCtrl achieves more consistent camera-controlled video generation, with over 40\\% reduction in camera motion errors compared with prior methods. Our project page: https://soyouthinkyoucantell.github.io/dualcamctrl-page/",
            "upvotes": 38,
            "discussionId": "692ea20937312eaa83fd8991",
            "projectPage": "https://soyouthinkyoucantell.github.io/dualcamctrl-page/",
            "githubRepo": "https://github.com/EnVision-Research/DualCamCtrl",
            "ai_summary": "DualCamCtrl is a diffusion model for camera-controlled video generation that uses a dual-branch framework and Semantic Guided Mutual Alignment to improve consistency and disentangle appearance and geometry modeling.",
            "ai_keywords": [
                "diffusion model",
                "camera-controlled video generation",
                "ray-based conditions",
                "dual-branch framework",
                "Semantic Guided Mutual Alignment",
                "RGB",
                "depth sequences",
                "RGB-depth fusion",
                "appearance and geometry modeling",
                "denoising stages",
                "camera motion errors"
            ],
            "githubStars": 32
        },
        "publishedAt": "2025-11-28T07:19:57.000Z",
        "title": "DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation",
        "summary": "This paper presents DualCamCtrl, a novel end-to-end diffusion model for camera-controlled video generation. Recent works have advanced this field by representing camera poses as ray-based conditions, yet they often lack sufficient scene understanding and geometric awareness. DualCamCtrl specifically targets this limitation by introducing a dual-branch framework that mutually generates camera-consistent RGB and depth sequences. To harmonize these two modalities, we further propose the Semantic Guided Mutual Alignment (SIGMA) mechanism, which performs RGB-depth fusion in a semantics-guided and mutually reinforced manner. These designs collectively enable DualCamCtrl to better disentangle appearance and geometry modeling, generating videos that more faithfully adhere to the specified camera trajectories. Additionally, we analyze and reveal the distinct influence of depth and camera poses across denoising stages and further demonstrate that early and late stages play complementary roles in forming global structure and refining local details. Extensive experiments demonstrate that DualCamCtrl achieves more consistent camera-controlled video generation, with over 40\\% reduction in camera motion errors compared with prior methods. Our project page: https://soyouthinkyoucantell.github.io/dualcamctrl-page/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.23127.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66864408c1d35ab079488662",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66864408c1d35ab079488662/U-btVsJ65j7m6vFLVIka8.jpeg",
            "fullname": "Hongfei (Faye) Zhang",
            "name": "FayeHongfeiZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.02472",
            "authors": [
                {
                    "_id": "692fb2c026742347f61dac94",
                    "user": {
                        "_id": "5feab3a28a3201f8e554c969",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660795228685-5feab3a28a3201f8e554c969.png",
                        "isPro": false,
                        "fullname": "Wenhao Yu",
                        "user": "wyu1",
                        "type": "user"
                    },
                    "name": "Wenhao Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:15:34.540Z",
                    "hidden": false
                },
                {
                    "_id": "692fb2c026742347f61dac95",
                    "name": "Zhenwen Liang",
                    "hidden": false
                },
                {
                    "_id": "692fb2c026742347f61dac96",
                    "name": "Chengsong Huang",
                    "hidden": false
                },
                {
                    "_id": "692fb2c026742347f61dac97",
                    "name": "Kishan Panaganti",
                    "hidden": false
                },
                {
                    "_id": "692fb2c026742347f61dac98",
                    "name": "Tianqing Fang",
                    "hidden": false
                },
                {
                    "_id": "692fb2c026742347f61dac99",
                    "name": "Haitao Mi",
                    "hidden": false
                },
                {
                    "_id": "692fb2c026742347f61dac9a",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T07:06:11.000Z",
            "submittedOnDailyAt": "2025-12-03T01:17:53.542Z",
            "title": "Guided Self-Evolving LLMs with Minimal Human Supervision",
            "submittedOnDailyBy": {
                "_id": "62ea79dd01ed9b0e8f61ccd3",
                "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg",
                "isPro": false,
                "fullname": "Chengsong Huang",
                "user": "ChengsongHuang",
                "type": "user"
            },
            "summary": "AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.",
            "upvotes": 37,
            "discussionId": "692fb2c026742347f61dac9b",
            "ai_summary": "R-Few, a guided Self-Play Challenger-Solver framework, enables stable and controllable model self-evolution with minimal human supervision, achieving performance improvements on math and reasoning benchmarks.",
            "ai_keywords": [
                "self-evolution",
                "superintelligence",
                "concept drift",
                "diversity collapse",
                "mis-evolution",
                "R-Few",
                "Self-Play Challenger-Solver",
                "in-context grounding",
                "mixed training",
                "synthetic question generation",
                "online curriculum",
                "Qwen3-8B-Base",
                "R-Zero",
                "General-Reasoner",
                "ablation studies",
                "co-evolutionary dynamics"
            ],
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-12-02T02:06:11.000Z",
        "title": "Guided Self-Evolving LLMs with Minimal Human Supervision",
        "summary": "AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02472.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62ea79dd01ed9b0e8f61ccd3",
            "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg",
            "fullname": "Chengsong Huang",
            "name": "ChengsongHuang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.02395",
            "authors": [
                {
                    "_id": "692ffb3426742347f61db128",
                    "name": "Yifan Zhang",
                    "hidden": false
                },
                {
                    "_id": "692ffb3426742347f61db129",
                    "name": "Liang Hu",
                    "hidden": false
                },
                {
                    "_id": "692ffb3426742347f61db12a",
                    "name": "Haofeng Sun",
                    "hidden": false
                },
                {
                    "_id": "692ffb3426742347f61db12b",
                    "name": "Peiyu Wang",
                    "hidden": false
                },
                {
                    "_id": "692ffb3426742347f61db12c",
                    "name": "Yichen Wei",
                    "hidden": false
                },
                {
                    "_id": "692ffb3426742347f61db12d",
                    "name": "Shukang Yin",
                    "hidden": false
                },
                {
                    "_id": "692ffb3426742347f61db12e",
                    "name": "Jiangbo Pei",
                    "hidden": false
                },
                {
                    "_id": "692ffb3426742347f61db12f",
                    "name": "Wei Shen",
                    "hidden": false
                },
                {
                    "_id": "692ffb3426742347f61db130",
                    "name": "Peng Xia",
                    "hidden": false
                },
                {
                    "_id": "692ffb3426742347f61db131",
                    "name": "Yi Peng",
                    "hidden": false
                },
                {
                    "_id": "692ffb3426742347f61db132",
                    "name": "Tianyidan Xie",
                    "hidden": false
                },
                {
                    "_id": "692ffb3426742347f61db133",
                    "name": "Eric Li",
                    "hidden": false
                },
                {
                    "_id": "692ffb3426742347f61db134",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "692ffb3426742347f61db135",
                    "name": "Xuchen Song",
                    "hidden": false
                },
                {
                    "_id": "692ffb3426742347f61db136",
                    "name": "Yahui Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T04:12:57.000Z",
            "submittedOnDailyAt": "2025-12-03T08:58:17.260Z",
            "title": "Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch",
            "submittedOnDailyBy": {
                "_id": "620f5a1c3f76c50e6458a9b6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
                "isPro": false,
                "fullname": "Peiyu Wang",
                "user": "OrlandoHugBot",
                "type": "user"
            },
            "summary": "Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and lack planning grounded in real tool-execution traces. To address these limitations, we present Skywork-R1V4, a 30B (A3B) parameter multimodal agentic model that unifies multimodal planning, active image manipulation (\"thinking with images\"), deep multimodal search, and, most critically, interleaved reasoning that dynamically alternates between visual operations and external knowledge retrieval. Trained solely via supervised fine-tuning on fewer than 30,000 high-quality, planning-execution-consistent trajectories and validated through stepwise consistency filtering, Skywork-R1V4 achieves state-of-the-art results across perception and multimodal search benchmarks: it scores 66.1 on MMSearch and 67.2 on FVQA, surpassing Gemini 2.5 Flash on all 11 metrics. Skywork-R1V4 exhibits emergent long-horizon reasoning at inference time, successfully orchestrating more than 10 tool calls to solve complex, multi-step tasks. Our results demonstrate that sophisticated agentic multimodal intelligence can be achieved through carefully curated supervised learning alone, without any reliance on reinforcement learning.",
            "upvotes": 34,
            "discussionId": "692ffb3426742347f61db145",
            "projectPage": "https://docs.skyworkmodel.ai/r1v4/api-reference/completions.html",
            "githubRepo": "https://github.com/SkyworkAI/Skywork-R1V/tree/main/r1v4",
            "ai_summary": "Skywork-R1V4, a 30B parameter multimodal agentic model, achieves top performance in perception and multimodal search benchmarks through supervised fine-tuning and interleaved reasoning without reinforcement learning.",
            "ai_keywords": [
                "multimodal agentic systems",
                "image manipulation",
                "web search",
                "reinforcement learning",
                "multimodal planning",
                "active image manipulation",
                "deep multimodal search",
                "interleaved reasoning",
                "supervised fine-tuning",
                "MMSearch",
                "FVQA",
                "Gemini 2.5 Flash",
                "long-horizon reasoning"
            ],
            "githubStars": 3107,
            "organization": {
                "_id": "6522615d9334173c627b0efa",
                "name": "Skywork",
                "fullname": "Skywork",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64535b71bcbd25618f7655da/AvtJ4GuPAyhLxl2-leVt6.jpeg"
            }
        },
        "publishedAt": "2025-12-01T23:12:57.000Z",
        "title": "Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch",
        "summary": "Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and lack planning grounded in real tool-execution traces. To address these limitations, we present Skywork-R1V4, a 30B (A3B) parameter multimodal agentic model that unifies multimodal planning, active image manipulation (\"thinking with images\"), deep multimodal search, and, most critically, interleaved reasoning that dynamically alternates between visual operations and external knowledge retrieval. Trained solely via supervised fine-tuning on fewer than 30,000 high-quality, planning-execution-consistent trajectories and validated through stepwise consistency filtering, Skywork-R1V4 achieves state-of-the-art results across perception and multimodal search benchmarks: it scores 66.1 on MMSearch and 67.2 on FVQA, surpassing Gemini 2.5 Flash on all 11 metrics. Skywork-R1V4 exhibits emergent long-horizon reasoning at inference time, successfully orchestrating more than 10 tool calls to solve complex, multi-step tasks. Our results demonstrate that sophisticated agentic multimodal intelligence can be achieved through carefully curated supervised learning alone, without any reliance on reinforcement learning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02395.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620f5a1c3f76c50e6458a9b6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
            "fullname": "Peiyu Wang",
            "name": "OrlandoHugBot",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "organization": {
            "_id": "6522615d9334173c627b0efa",
            "name": "Skywork",
            "fullname": "Skywork",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64535b71bcbd25618f7655da/AvtJ4GuPAyhLxl2-leVt6.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.23369",
            "authors": [
                {
                    "_id": "692faf1e26742347f61dac5a",
                    "user": {
                        "_id": "6448baa3e780dbfc89058bc3",
                        "avatarUrl": "/avatars/44c243c8eb4714cdcdaa7cd04e5a9716.svg",
                        "isPro": false,
                        "fullname": "Haochen Tian",
                        "user": "StarBurger",
                        "type": "user"
                    },
                    "name": "Haochen Tian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:15:38.824Z",
                    "hidden": false
                },
                {
                    "_id": "692faf1e26742347f61dac5b",
                    "user": {
                        "_id": "64f022aa9e7770db74d98655",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f022aa9e7770db74d98655/4Rp4QMlORynd6O9aDTHdk.jpeg",
                        "isPro": false,
                        "fullname": "Tianyu Li",
                        "user": "sephyli",
                        "type": "user"
                    },
                    "name": "Tianyu Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:15:41.434Z",
                    "hidden": false
                },
                {
                    "_id": "692faf1e26742347f61dac5c",
                    "user": {
                        "_id": "65c1d586d4c3b8dff20ea129",
                        "avatarUrl": "/avatars/bf34d4189142fe48f46aa8351e99ff2d.svg",
                        "isPro": false,
                        "fullname": "HaochenLiu",
                        "user": "georgeliu23333",
                        "type": "user"
                    },
                    "name": "Haochen Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:15:43.590Z",
                    "hidden": false
                },
                {
                    "_id": "692faf1e26742347f61dac5d",
                    "name": "Jiazhi Yang",
                    "hidden": false
                },
                {
                    "_id": "692faf1e26742347f61dac5e",
                    "name": "Yihang Qiu",
                    "hidden": false
                },
                {
                    "_id": "692faf1e26742347f61dac5f",
                    "name": "Guang Li",
                    "hidden": false
                },
                {
                    "_id": "692faf1e26742347f61dac60",
                    "name": "Junli Wang",
                    "hidden": false
                },
                {
                    "_id": "692faf1e26742347f61dac61",
                    "name": "Yinfeng Gao",
                    "hidden": false
                },
                {
                    "_id": "692faf1e26742347f61dac62",
                    "name": "Zhang Zhang",
                    "hidden": false
                },
                {
                    "_id": "692faf1e26742347f61dac63",
                    "name": "Liang Wang",
                    "hidden": false
                },
                {
                    "_id": "692faf1e26742347f61dac64",
                    "name": "Hangjun Ye",
                    "hidden": false
                },
                {
                    "_id": "692faf1e26742347f61dac65",
                    "name": "Tieniu Tan",
                    "hidden": false
                },
                {
                    "_id": "692faf1e26742347f61dac66",
                    "name": "Long Chen",
                    "hidden": false
                },
                {
                    "_id": "692faf1e26742347f61dac67",
                    "name": "Hongyang Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-28T17:17:38.000Z",
            "submittedOnDailyAt": "2025-12-03T01:18:11.530Z",
            "title": "SimScale: Learning to Drive via Real-World Simulation at Scale",
            "submittedOnDailyBy": {
                "_id": "65c1d586d4c3b8dff20ea129",
                "avatarUrl": "/avatars/bf34d4189142fe48f46aa8351e99ff2d.svg",
                "isPro": false,
                "fullname": "HaochenLiu",
                "user": "georgeliu23333",
                "type": "user"
            },
            "summary": "Achieving fully autonomous driving systems requires learning rational decisions in a wide span of scenarios, including safety-critical and out-of-distribution ones. However, such cases are underrepresented in real-world corpus collected by human experts. To complement for the lack of data diversity, we introduce a novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs. Our pipeline utilizes advanced neural rendering with a reactive environment to generate high-fidelity multi-view observations controlled by the perturbed ego trajectory. Furthermore, we develop a pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision. Upon the synthesized data, we find that a simple co-training strategy on both real-world and simulated samples can lead to significant improvements in both robustness and generalization for various planning methods on challenging real-world benchmarks, up to +6.8 EPDMS on navhard and +2.9 on navtest. More importantly, such policy improvement scales smoothly by increasing simulation data only, even without extra real-world data streaming in. We further reveal several crucial findings of such a sim-real learning system, which we term SimScale, including the design of pseudo-experts and the scaling properties for different policy architectures. Our simulation data and code would be released.",
            "upvotes": 33,
            "discussionId": "692faf1e26742347f61dac68",
            "projectPage": "https://opendrivelab.com/SimScale",
            "githubRepo": "https://github.com/OpenDriveLab/SimScale",
            "ai_summary": "A simulation framework improves autonomous driving by generating diverse, high-fidelity driving scenarios, leading to better generalization and robustness in real-world testing.",
            "ai_keywords": [
                "neural rendering",
                "reactive environment",
                "co-training strategy",
                "pseudo-expert trajectory generation",
                "SimScale",
                "EPDMS",
                "navhard",
                "navtest"
            ],
            "githubStars": 51,
            "organization": {
                "_id": "64e5798f5af55fb4d1f171d5",
                "name": "OpenDriveLab",
                "fullname": "OpenDriveLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64e57309b78bc92221ce3b70/dwqGmx-hiCdAikyUuNLkO.png"
            }
        },
        "publishedAt": "2025-11-28T12:17:38.000Z",
        "title": "SimScale: Learning to Drive via Real-World Simulation at Scale",
        "summary": "Achieving fully autonomous driving systems requires learning rational decisions in a wide span of scenarios, including safety-critical and out-of-distribution ones. However, such cases are underrepresented in real-world corpus collected by human experts. To complement for the lack of data diversity, we introduce a novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs. Our pipeline utilizes advanced neural rendering with a reactive environment to generate high-fidelity multi-view observations controlled by the perturbed ego trajectory. Furthermore, we develop a pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision. Upon the synthesized data, we find that a simple co-training strategy on both real-world and simulated samples can lead to significant improvements in both robustness and generalization for various planning methods on challenging real-world benchmarks, up to +6.8 EPDMS on navhard and +2.9 on navtest. More importantly, such policy improvement scales smoothly by increasing simulation data only, even without extra real-world data streaming in. We further reveal several crucial findings of such a sim-real learning system, which we term SimScale, including the design of pseudo-experts and the scaling properties for different policy architectures. Our simulation data and code would be released.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.23369.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65c1d586d4c3b8dff20ea129",
            "avatarUrl": "/avatars/bf34d4189142fe48f46aa8351e99ff2d.svg",
            "fullname": "HaochenLiu",
            "name": "georgeliu23333",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "64e5798f5af55fb4d1f171d5",
            "name": "OpenDriveLab",
            "fullname": "OpenDriveLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64e57309b78bc92221ce3b70/dwqGmx-hiCdAikyUuNLkO.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.01822",
            "authors": [
                {
                    "_id": "692ffb3026742347f61db108",
                    "name": "Jintian Zhang",
                    "hidden": false
                },
                {
                    "_id": "692ffb3026742347f61db109",
                    "name": "Kewei Xu",
                    "hidden": false
                },
                {
                    "_id": "692ffb3026742347f61db10a",
                    "name": "Jingsheng Zheng",
                    "hidden": false
                },
                {
                    "_id": "692ffb3026742347f61db10b",
                    "name": "Zhuoyun Yu",
                    "hidden": false
                },
                {
                    "_id": "692ffb3026742347f61db10c",
                    "name": "Yuqi Zhu",
                    "hidden": false
                },
                {
                    "_id": "692ffb3026742347f61db10d",
                    "name": "Yujie Luo",
                    "hidden": false
                },
                {
                    "_id": "692ffb3026742347f61db10e",
                    "name": "Lanning Wei",
                    "hidden": false
                },
                {
                    "_id": "692ffb3026742347f61db10f",
                    "name": "Shuofei Qiao",
                    "hidden": false
                },
                {
                    "_id": "692ffb3026742347f61db110",
                    "name": "Lun Du",
                    "hidden": false
                },
                {
                    "_id": "692ffb3026742347f61db111",
                    "name": "Da Zheng",
                    "hidden": false
                },
                {
                    "_id": "692ffb3026742347f61db112",
                    "name": "Shumin Deng",
                    "hidden": false
                },
                {
                    "_id": "692ffb3026742347f61db113",
                    "name": "Huajun Chen",
                    "hidden": false
                },
                {
                    "_id": "692ffb3026742347f61db114",
                    "name": "Ningyu Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-01T16:03:04.000Z",
            "submittedOnDailyAt": "2025-12-03T11:09:36.022Z",
            "title": "InnoGym: Benchmarking the Innovation Potential of AI Agents",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": true,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.",
            "upvotes": 28,
            "discussionId": "692ffb3026742347f61db11a",
            "ai_summary": "InnoGym is a benchmark and framework that evaluates the innovation potential of AI agents using performance gain and novelty metrics, highlighting a gap between creativity and effectiveness.",
            "ai_keywords": [
                "InnoGym",
                "performance gain",
                "novelty",
                "iGym",
                "reproducible",
                "long-horizon evaluations"
            ],
            "organization": {
                "_id": "6878f1158a96055b30c4e802",
                "name": "zju-community",
                "fullname": "Zhejiang University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660c2d134ba2fcc848b03e21/Yom0mtdRlos3bpIk7eLcz.png"
            }
        },
        "publishedAt": "2025-12-01T11:03:04.000Z",
        "title": "InnoGym: Benchmarking the Innovation Potential of AI Agents",
        "summary": "LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01822.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 34
        },
        "organization": {
            "_id": "6878f1158a96055b30c4e802",
            "name": "zju-community",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660c2d134ba2fcc848b03e21/Yom0mtdRlos3bpIk7eLcz.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.03036",
            "authors": [
                {
                    "_id": "692fc40826742347f61dacff",
                    "name": "Mengchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fc40826742347f61dad00",
                    "name": "Qi Chen",
                    "hidden": false
                },
                {
                    "_id": "692fc40826742347f61dad01",
                    "name": "Tong Wu",
                    "hidden": false
                },
                {
                    "_id": "692fc40826742347f61dad02",
                    "name": "Zihan Liu",
                    "hidden": false
                },
                {
                    "_id": "692fc40826742347f61dad03",
                    "name": "Dahua Lin",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64de20c5808492ba6e65d124/sM-gbt0oJOGtwEChTx7ZI.mp4"
            ],
            "publishedAt": "2025-12-02T18:56:12.000Z",
            "submittedOnDailyAt": "2025-12-03T02:37:05.086Z",
            "title": "ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation",
            "submittedOnDailyBy": {
                "_id": "64de20c5808492ba6e65d124",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de20c5808492ba6e65d124/58IX_TI5vJw73qS1knw56.jpeg",
                "isPro": false,
                "fullname": "Zhang Mengchen",
                "user": "Dubhe-zmc",
                "type": "user"
            },
            "summary": "Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project.",
            "upvotes": 20,
            "discussionId": "692fc40926742347f61dad04",
            "projectPage": "https://kszpxxzmc.github.io/ViSAudio-project/",
            "githubRepo": "https://github.com/kszpxxzmc/ViSAudio",
            "ai_summary": "ViSAudio, an end-to-end framework using conditional flow matching, generates high-quality binaural audio from silent video, providing spatial immersion and consistency across various acoustic conditions.",
            "ai_keywords": [
                "conditional flow matching",
                "dual-branch audio generation architecture",
                "audio latent flows",
                "conditional spacetime module",
                "binaural spatial audio generation",
                "BiAudio dataset",
                "spatio-temporal alignment"
            ],
            "githubStars": 29,
            "organization": {
                "_id": "61bac2af530e5c78d7b99667",
                "name": "zju",
                "fullname": "Zhejiang University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
            }
        },
        "publishedAt": "2025-12-02T13:56:12.000Z",
        "title": "ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation",
        "summary": "Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64de20c5808492ba6e65d124/sM-gbt0oJOGtwEChTx7ZI.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03036.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64de20c5808492ba6e65d124",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de20c5808492ba6e65d124/58IX_TI5vJw73qS1knw56.jpeg",
            "fullname": "Zhang Mengchen",
            "name": "Dubhe-zmc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.02899",
            "authors": [
                {
                    "_id": "692fad4226742347f61dac42",
                    "name": "Zhuobai Dong",
                    "hidden": false
                },
                {
                    "_id": "692fad4226742347f61dac43",
                    "name": "Rui Zhao",
                    "hidden": false
                },
                {
                    "_id": "692fad4226742347f61dac44",
                    "name": "Songjie Wu",
                    "hidden": false
                },
                {
                    "_id": "692fad4226742347f61dac45",
                    "name": "Junchao Yi",
                    "hidden": false
                },
                {
                    "_id": "692fad4226742347f61dac46",
                    "name": "Linjie Li",
                    "hidden": false
                },
                {
                    "_id": "692fad4226742347f61dac47",
                    "name": "Zhengyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "692fad4226742347f61dac48",
                    "name": "Lijuan Wang",
                    "hidden": false
                },
                {
                    "_id": "692fad4226742347f61dac49",
                    "name": "Alex Jinpeng Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T16:05:21.000Z",
            "submittedOnDailyAt": "2025-12-03T00:54:18.768Z",
            "title": "Glance: Accelerating Diffusion Models with 1 Sample",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.",
            "upvotes": 20,
            "discussionId": "692fad4226742347f61dac4a",
            "ai_summary": "Using phase-aware LoRA adapters, diffusion models achieve efficient acceleration and strong generalization with minimal retraining.",
            "ai_keywords": [
                "diffusion models",
                "computational cost",
                "inference steps",
                "distillation",
                "compact student models",
                "retraining costs",
                "phase-aware strategy",
                "slow and fast denoising phases",
                "lightweight LoRA adapters",
                "Slow-LoRA",
                "Fast-LoRA",
                "acceleration",
                "visual quality",
                "benchmarks",
                "samples",
                "V100",
                "unseen prompts"
            ]
        },
        "publishedAt": "2025-12-02T11:05:21.000Z",
        "title": "Glance: Accelerating Diffusion Models with 1 Sample",
        "summary": "Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02899.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8835
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.02425",
            "authors": [
                {
                    "_id": "692faddf26742347f61dac4c",
                    "user": {
                        "_id": "66d30f5fad293ffc4b7672bc",
                        "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
                        "isPro": false,
                        "fullname": "Woongyeong Yeo",
                        "user": "wgcyeo",
                        "type": "user"
                    },
                    "name": "Woongyeong Yeo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:15:47.690Z",
                    "hidden": false
                },
                {
                    "_id": "692faddf26742347f61dac4d",
                    "name": "Kangsan Kim",
                    "hidden": false
                },
                {
                    "_id": "692faddf26742347f61dac4e",
                    "name": "Jaehong Yoon",
                    "hidden": false
                },
                {
                    "_id": "692faddf26742347f61dac4f",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T05:14:52.000Z",
            "submittedOnDailyAt": "2025-12-03T01:36:49.416Z",
            "title": "WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning",
            "submittedOnDailyBy": {
                "_id": "66d30f5fad293ffc4b7672bc",
                "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
                "isPro": false,
                "fullname": "Woongyeong Yeo",
                "user": "wgcyeo",
                "type": "user"
            },
            "summary": "Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.",
            "upvotes": 19,
            "discussionId": "692faddf26742347f61dac50",
            "projectPage": "https://worldmm.github.io",
            "githubRepo": "https://github.com/wgcyeo/WorldMM",
            "ai_summary": "WorldMM, a novel multimodal memory agent with episodic, semantic, and visual memory, outperforms existing methods in long video question-answering by adaptively retrieving from multiple temporal scales and memory sources.",
            "ai_keywords": [
                "video large language models",
                "memory-augmented methods",
                "textual summaries",
                "visual evidence",
                "multimodal memory agent",
                "episodic memory",
                "semantic memory",
                "visual memory",
                "adaptive retrieval agent",
                "long video question-answering benchmarks"
            ],
            "githubStars": 8,
            "organization": {
                "_id": "6475760c33192631bad2bb38",
                "name": "kaist-ai",
                "fullname": "KAIST AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
            }
        },
        "publishedAt": "2025-12-02T00:14:52.000Z",
        "title": "WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning",
        "summary": "Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02425.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66d30f5fad293ffc4b7672bc",
            "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
            "fullname": "Woongyeong Yeo",
            "name": "wgcyeo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "6475760c33192631bad2bb38",
            "name": "kaist-ai",
            "fullname": "KAIST AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.02038",
            "authors": [
                {
                    "_id": "692ffb3826742347f61db159",
                    "name": "Zhengliang Shi",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db15a",
                    "name": "Yiqun Chen",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db15b",
                    "name": "Haitao Li",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db15c",
                    "name": "Weiwei Sun",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db15d",
                    "name": "Shiyu Ni",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db15e",
                    "name": "Yougang Lyu",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db15f",
                    "name": "Run-Ze Fan",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db160",
                    "name": "Bowen Jin",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db161",
                    "name": "Yixuan Weng",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db162",
                    "name": "Minjun Zhu",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db163",
                    "name": "Qiujie Xie",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db164",
                    "name": "Xinyu Guo",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db165",
                    "name": "Qu Yang",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db166",
                    "name": "Jiayi Wu",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db167",
                    "name": "Jujia Zhao",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db168",
                    "name": "Xiaqiang Tang",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db169",
                    "name": "Xinbei Ma",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db16a",
                    "name": "Cunxiang Wang",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db16b",
                    "name": "Jiaxin Mao",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db16c",
                    "name": "Qingyao Ai",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db16d",
                    "name": "Jen-Tse Huang",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db16e",
                    "name": "Wenxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db16f",
                    "name": "Yue Zhang",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db170",
                    "name": "Yiming Yang",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db171",
                    "name": "Zhaopeng Tu",
                    "hidden": false
                },
                {
                    "_id": "692ffb3826742347f61db172",
                    "name": "Zhaochun Ren",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/616bfc2b40e2f69baa1c7add/spBCRdfHM-n5H18S1l2ih.png",
                "https://cdn-uploads.huggingface.co/production/uploads/616bfc2b40e2f69baa1c7add/aM_NAfSKiYZV8DXTE3dV3.png"
            ],
            "publishedAt": "2025-11-24T15:28:28.000Z",
            "submittedOnDailyAt": "2025-12-03T12:22:35.336Z",
            "title": "Deep Research: A Systematic Survey",
            "submittedOnDailyBy": {
                "_id": "616bfc2b40e2f69baa1c7add",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616bfc2b40e2f69baa1c7add/Os7_qgMei-2lRVelrOG7B.jpeg",
                "isPro": false,
                "fullname": "Run-Ze Fan",
                "user": "Vfrz",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.",
            "upvotes": 16,
            "discussionId": "692ffb3826742347f61db173",
            "githubRepo": "https://github.com/mangopy/Deep-Research-Survey",
            "ai_summary": "Deep Research systems integrate LLMs with external tools to enhance problem-solving capabilities, involving query planning, information acquisition, memory management, and answer generation.",
            "ai_keywords": [
                "Deep Research",
                "Large language models",
                "LLMs",
                "reasoning capabilities",
                "search engines",
                "research agents",
                "three-stage roadmap",
                "query planning",
                "information acquisition",
                "memory management",
                "answer generation",
                "sub-taxonomies",
                "prompting",
                "supervised fine-tuning",
                "agentic reinforcement learning",
                "evaluation criteria",
                "open challenges"
            ],
            "githubStars": 143
        },
        "publishedAt": "2025-11-24T10:28:28.000Z",
        "title": "Deep Research: A Systematic Survey",
        "summary": "Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/616bfc2b40e2f69baa1c7add/spBCRdfHM-n5H18S1l2ih.png",
            "https://cdn-uploads.huggingface.co/production/uploads/616bfc2b40e2f69baa1c7add/aM_NAfSKiYZV8DXTE3dV3.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02038.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "616bfc2b40e2f69baa1c7add",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616bfc2b40e2f69baa1c7add/Os7_qgMei-2lRVelrOG7B.jpeg",
            "fullname": "Run-Ze Fan",
            "name": "Vfrz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.19433",
            "authors": [
                {
                    "_id": "69268d88243b2216fb75caff",
                    "user": {
                        "_id": "642c25e68f90c557f7423eb0",
                        "avatarUrl": "/avatars/6f86a611994590b871e9ecfed61dc06f.svg",
                        "isPro": false,
                        "fullname": "Dong Jing",
                        "user": "Timsty",
                        "type": "user"
                    },
                    "name": "Dong Jing",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-02T08:49:57.902Z",
                    "hidden": false
                },
                {
                    "_id": "69268d88243b2216fb75cb00",
                    "name": "Gang Wang",
                    "hidden": false
                },
                {
                    "_id": "69268d88243b2216fb75cb01",
                    "name": "Jiaqi Liu",
                    "hidden": false
                },
                {
                    "_id": "69268d88243b2216fb75cb02",
                    "name": "Weiliang Tang",
                    "hidden": false
                },
                {
                    "_id": "69268d88243b2216fb75cb03",
                    "name": "Zelong Sun",
                    "hidden": false
                },
                {
                    "_id": "69268d88243b2216fb75cb04",
                    "name": "Yunchao Yao",
                    "hidden": false
                },
                {
                    "_id": "69268d88243b2216fb75cb05",
                    "name": "Zhenyu Wei",
                    "hidden": false
                },
                {
                    "_id": "69268d88243b2216fb75cb06",
                    "name": "Yunhui Liu",
                    "hidden": false
                },
                {
                    "_id": "69268d88243b2216fb75cb07",
                    "name": "Zhiwu Lu",
                    "hidden": false
                },
                {
                    "_id": "69268d88243b2216fb75cb08",
                    "name": "Mingyu Ding",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-24T18:59:51.000Z",
            "submittedOnDailyAt": "2025-12-03T02:19:45.410Z",
            "title": "Mixture of Horizons in Action Chunking",
            "submittedOnDailyBy": {
                "_id": "642c25e68f90c557f7423eb0",
                "avatarUrl": "/avatars/6f86a611994590b871e9ecfed61dc06f.svg",
                "isPro": false,
                "fullname": "Dong Jing",
                "user": "Timsty",
                "type": "user"
            },
            "summary": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the action chunk length used during training, termed horizon. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a mixture of horizons (MoH) strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5times higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies _0, _{0.5}, and one-step regression policy _{reg} demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, _{0.5} with MoH reaches a new state-of-the-art with 99% average success rate on LIBERO after only 30k training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons",
            "upvotes": 15,
            "discussionId": "69268d88243b2216fb75cb09",
            "projectPage": "https://timsty1.github.io/moh/",
            "githubRepo": "https://github.com/Timsty1/MixtureOfHorizons/tree/main",
            "ai_summary": "A mixture of horizons strategy in VLA models improves performance and generalizability by combining long-term foresight and short-term precision, achieving higher throughput and superior performance in robotic manipulation tasks.",
            "ai_keywords": [
                "Vision-language-action models",
                "action chunk length",
                "horizon",
                "mixture of horizons (MoH)",
                "action transformer",
                "cross-horizon consensus",
                "flow-based policies",
                "one-step regression policy",
                "LIBERO"
            ],
            "githubStars": 14
        },
        "publishedAt": "2025-11-24T13:59:51.000Z",
        "title": "Mixture of Horizons in Action Chunking",
        "summary": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the action chunk length used during training, termed horizon. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a mixture of horizons (MoH) strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5times higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies _0, _{0.5}, and one-step regression policy _{reg} demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, _{0.5} with MoH reaches a new state-of-the-art with 99% average success rate on LIBERO after only 30k training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19433.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642c25e68f90c557f7423eb0",
            "avatarUrl": "/avatars/6f86a611994590b871e9ecfed61dc06f.svg",
            "fullname": "Dong Jing",
            "name": "Timsty",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.20645",
            "authors": [
                {
                    "_id": "692ffb0d26742347f61daee9",
                    "name": "Yongsheng Yu",
                    "hidden": false
                },
                {
                    "_id": "692ffb0d26742347f61daeea",
                    "name": "Wei Xiong",
                    "hidden": false
                },
                {
                    "_id": "692ffb0d26742347f61daeeb",
                    "name": "Weili Nie",
                    "hidden": false
                },
                {
                    "_id": "692ffb0d26742347f61daeec",
                    "name": "Yichen Sheng",
                    "hidden": false
                },
                {
                    "_id": "692ffb0d26742347f61daeed",
                    "name": "Shiqiu Liu",
                    "hidden": false
                },
                {
                    "_id": "692ffb0d26742347f61daeee",
                    "name": "Jiebo Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-25T18:59:25.000Z",
            "submittedOnDailyAt": "2025-12-03T14:07:23.980Z",
            "title": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
            "submittedOnDailyBy": {
                "_id": "635b6bdfd63bcb528acefa78",
                "avatarUrl": "/avatars/54c62b41c147d0450bdfc72121860bb4.svg",
                "isPro": true,
                "fullname": "Yongsheng Yu",
                "user": "yeates",
                "type": "user"
            },
            "summary": "Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.",
            "upvotes": 14,
            "discussionId": "692ffb0d26742347f61daeef",
            "ai_summary": "PixelDiT is a single-stage, end-to-end diffusion model that operates directly in pixel space, overcoming the limitations of latent-space modeling by using a dual-level transformer architecture and achieving competitive performance in image and text-to-image generation.",
            "ai_keywords": [
                "Latent-space modeling",
                "Diffusion Transformers",
                "DiTs",
                "PixelDiT",
                "pretrained autoencoder",
                "pixel space",
                "fully transformer-based architecture",
                "patch-level DiT",
                "pixel-level DiT",
                "token modeling",
                "FID",
                "ImageNet",
                "GenEval",
                "DPG-bench"
            ],
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-11-25T13:59:25.000Z",
        "title": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "summary": "Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20645.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635b6bdfd63bcb528acefa78",
            "avatarUrl": "/avatars/54c62b41c147d0450bdfc72121860bb4.svg",
            "fullname": "Yongsheng Yu",
            "name": "yeates",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.00956",
            "authors": [
                {
                    "_id": "692e4d7e37312eaa83fd8736",
                    "user": {
                        "_id": "6298d8dab58e71e2ac9e2967",
                        "avatarUrl": "/avatars/99fec4ba78ab61c5952f51e6ebf03ffa.svg",
                        "isPro": false,
                        "fullname": "Jiale Chen",
                        "user": "softmax",
                        "type": "user"
                    },
                    "name": "Jiale Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-02T08:45:08.031Z",
                    "hidden": false
                },
                {
                    "_id": "692e4d7e37312eaa83fd8737",
                    "name": "Vage Egiazarian",
                    "hidden": false
                },
                {
                    "_id": "692e4d7e37312eaa83fd8738",
                    "name": "Torsten Hoefler",
                    "hidden": false
                },
                {
                    "_id": "692e4d7e37312eaa83fd8739",
                    "name": "Dan Alistarh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-30T16:17:34.000Z",
            "submittedOnDailyAt": "2025-12-03T10:42:06.449Z",
            "title": "WUSH: Near-Optimal Adaptive Transforms for LLM Quantization",
            "submittedOnDailyBy": {
                "_id": "6298d8dab58e71e2ac9e2967",
                "avatarUrl": "/avatars/99fec4ba78ab61c5952f51e6ebf03ffa.svg",
                "isPro": false,
                "fullname": "Jiale Chen",
                "user": "softmax",
                "type": "user"
            },
            "summary": "Quantization to low bitwidth is a standard approach for deploying large language models, however, a few extreme weights and activations stretch the dynamic range and reduce the effective resolution of the quantizer. A common mitigation approach is to apply some fixed orthogonal transforms, such as Hadamard matrices, before quantization, which typically reduces the dynamic range. Yet, these transforms ignore the statistics of the data, and their optimality is currently not understood. In this work, we derive, for the first time, closed-form optimal linear blockwise transforms for joint weight-activation quantization using standard data-free quantizers for common numerical formats. Specifically, we provide derivations of the optimal adaptive (data-aware) transforms for round-to-nearest (RTN), AbsMax-scaled block quantizers for both integer and floating-point formats. The resulting construction, which we call WUSH, combines a Hadamard backbone with a data-dependent component based on second-order moments, yielding a non-orthogonal transform that is provably optimal under mild assumptions and remains structured for efficient implementation. Preliminary experimental results show that our approach consistently improves upon the Hadamard transform for common formats.",
            "upvotes": 13,
            "discussionId": "692e4d7e37312eaa83fd873a",
            "ai_summary": "Optimal linear blockwise transforms for joint weight-activation quantization improve upon standard orthogonal transforms like the Hadamard transform by incorporating data statistics.",
            "ai_keywords": [
                "quantization",
                "low bitwidth",
                "dynamic range",
                "Hadamard matrices",
                "data-free quantizers",
                "round-to-nearest",
                "AbsMax-scaled block quantizers",
                "optimal adaptive transforms",
                "second-order moments",
                "WUSH"
            ],
            "organization": {
                "_id": "64d0ffde9cff738203a50e9b",
                "name": "ISTA-DASLab",
                "fullname": " IST Austria Distributed Algorithms and Systems Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/628e0ce4e53bbd334577fcb0/TRPtgtSavYjDJOK3S1I8M.png"
            }
        },
        "publishedAt": "2025-11-30T11:17:34.000Z",
        "title": "WUSH: Near-Optimal Adaptive Transforms for LLM Quantization",
        "summary": "Quantization to low bitwidth is a standard approach for deploying large language models, however, a few extreme weights and activations stretch the dynamic range and reduce the effective resolution of the quantizer. A common mitigation approach is to apply some fixed orthogonal transforms, such as Hadamard matrices, before quantization, which typically reduces the dynamic range. Yet, these transforms ignore the statistics of the data, and their optimality is currently not understood. In this work, we derive, for the first time, closed-form optimal linear blockwise transforms for joint weight-activation quantization using standard data-free quantizers for common numerical formats. Specifically, we provide derivations of the optimal adaptive (data-aware) transforms for round-to-nearest (RTN), AbsMax-scaled block quantizers for both integer and floating-point formats. The resulting construction, which we call WUSH, combines a Hadamard backbone with a data-dependent component based on second-order moments, yielding a non-orthogonal transform that is provably optimal under mild assumptions and remains structured for efficient implementation. Preliminary experimental results show that our approach consistently improves upon the Hadamard transform for common formats.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.00956.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6298d8dab58e71e2ac9e2967",
            "avatarUrl": "/avatars/99fec4ba78ab61c5952f51e6ebf03ffa.svg",
            "fullname": "Jiale Chen",
            "name": "softmax",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "64d0ffde9cff738203a50e9b",
            "name": "ISTA-DASLab",
            "fullname": " IST Austria Distributed Algorithms and Systems Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/628e0ce4e53bbd334577fcb0/TRPtgtSavYjDJOK3S1I8M.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.02457",
            "authors": [
                {
                    "_id": "692f9ccf26742347f61daad8",
                    "name": "Jianzong Wu",
                    "hidden": false
                },
                {
                    "_id": "692f9ccf26742347f61daad9",
                    "name": "Hao Lian",
                    "hidden": false
                },
                {
                    "_id": "692f9ccf26742347f61daada",
                    "name": "Dachao Hao",
                    "hidden": false
                },
                {
                    "_id": "692f9ccf26742347f61daadb",
                    "name": "Ye Tian",
                    "hidden": false
                },
                {
                    "_id": "692f9ccf26742347f61daadc",
                    "user": {
                        "_id": "656724074f6ec72017754d33",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zLIgt4xvfbYPLZh0E3WWF.png",
                        "isPro": false,
                        "fullname": "QingyuShi",
                        "user": "QingyuShi",
                        "type": "user"
                    },
                    "name": "Qingyu Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:15:58.766Z",
                    "hidden": false
                },
                {
                    "_id": "692f9ccf26742347f61daadd",
                    "name": "Biaolong Chen",
                    "hidden": false
                },
                {
                    "_id": "692f9ccf26742347f61daade",
                    "name": "Hao Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T06:31:38.000Z",
            "submittedOnDailyAt": "2025-12-03T00:20:33.093Z",
            "title": "Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation",
            "submittedOnDailyBy": {
                "_id": "657a6eed1ccc3c2a5ea7b585",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RIQIF-JJdNI0SwJEq_9z7.jpeg",
                "isPro": true,
                "fullname": "Jianzong Wu",
                "user": "jianzongwu",
                "type": "user"
            },
            "summary": "Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision times impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.",
            "upvotes": 10,
            "discussionId": "692f9cd026742347f61daadf",
            "projectPage": "https://jianzongwu.github.io/projects/does-hearing-help-seeing",
            "githubRepo": "https://github.com/jianzongwu/Does-Hearing-Help-Seeing",
            "ai_summary": "Audio-video joint denoising improves video generation quality by regularizing video dynamics through audio as a privileged signal, as demonstrated by the AVFullDiT architecture.",
            "ai_keywords": [
                "Audio-Video Full DiT",
                "AVFullDiT",
                "text-to-video",
                "text-to-audio",
                "T2AV",
                "T2V",
                "joint denoising",
                "cross-modal co-training",
                "world models"
            ],
            "githubStars": 5,
            "organization": {
                "_id": "61dcd8e344f59573371b5cb6",
                "name": "PekingUniversity",
                "fullname": "Peking University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
            }
        },
        "publishedAt": "2025-12-02T01:31:38.000Z",
        "title": "Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation",
        "summary": "Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision times impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02457.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "657a6eed1ccc3c2a5ea7b585",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RIQIF-JJdNI0SwJEq_9z7.jpeg",
            "fullname": "Jianzong Wu",
            "name": "jianzongwu",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.02551",
            "authors": [
                {
                    "_id": "692ffb3526742347f61db147",
                    "name": "Songqiao Su",
                    "hidden": false
                },
                {
                    "_id": "692ffb3526742347f61db148",
                    "name": "Xiaofei Sun",
                    "hidden": false
                },
                {
                    "_id": "692ffb3526742347f61db149",
                    "name": "Xiaoya Li",
                    "hidden": false
                },
                {
                    "_id": "692ffb3526742347f61db14a",
                    "name": "Albert Wang",
                    "hidden": false
                },
                {
                    "_id": "692ffb3526742347f61db14b",
                    "name": "Jiwei Li",
                    "hidden": false
                },
                {
                    "_id": "692ffb3526742347f61db14c",
                    "name": "Chris Shum",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6803ddda5c044d396d02f03a/Qz5NhI_UTCEzno_Fd5ndY.png"
            ],
            "publishedAt": "2025-12-02T09:20:15.000Z",
            "submittedOnDailyAt": "2025-12-03T08:46:35.783Z",
            "title": "CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "6803ddda5c044d396d02f03a",
                "avatarUrl": "/avatars/ec583f6560d3d82fbd3bf95fb83c9356.svg",
                "isPro": true,
                "fullname": "Xiaoya Li",
                "user": "xxiaoyali",
                "type": "user"
            },
            "summary": "In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used {\\it torch.matmul} to state-of-the-art Nvidia's closed-source libraries, i.e., {\\it cuBLAS}, {\\it cuBLASLt}. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0\\% over {\\it torch.matmul} on average; +19.2\\% over {\\it cuBLAS} using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8\\% over {\\it cuBLASLt-heuristic}, which queries {\\it cuBLASLt} library and selects the algorithm based on the heuristic's suggestion; and +11.4\\% over the most competitive {\\it cuBLASLt-AutoTuning} model, which selects the fastest algorithm from up to 100 candidates from {\\it cuBLASLt}'s suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7\\%, +26.0\\%, +22.4\\%, and +15.9\\% for {\\it torch.matmul}, {\\it cuBLAS}, {\\it cuBLASLt-heuristic}, and {\\it cuBLASLt-AutoTuning} respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2",
            "upvotes": 9,
            "discussionId": "692ffb3526742347f61db14d",
            "githubRepo": "https://github.com/deepreinforce-ai/CUDA-L2",
            "ai_summary": "CUDA-L2, a system combining large language models and reinforcement learning, optimizes Half-precision General Matrix Multiply CUDA kernels, achieving significant speedups over existing baselines in both offline and server modes.",
            "ai_keywords": [
                "LARGE LANGUAGE MODELS",
                "REINFORCEMENT LEARNING",
                "HALF-PRECISION GENERAL MATRIX MULTIPLY",
                "CUDA KERNELS",
                "RL REWARD",
                "cuBLAS",
                "cuBLASLt",
                "cuBLASLt-heuristic",
                "cuBLASLt-AutoTuning"
            ],
            "githubStars": 19,
            "organization": {
                "_id": "68898157cad3d316633a440d",
                "name": "deepreinforce-ai",
                "fullname": "DeepReinforce",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68661bce022667cf9c7534fb/9BFOcjICEpj0xvyOHVp1h.png"
            }
        },
        "publishedAt": "2025-12-02T04:20:15.000Z",
        "title": "CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning",
        "summary": "In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used {\\it torch.matmul} to state-of-the-art Nvidia's closed-source libraries, i.e., {\\it cuBLAS}, {\\it cuBLASLt}. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0\\% over {\\it torch.matmul} on average; +19.2\\% over {\\it cuBLAS} using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8\\% over {\\it cuBLASLt-heuristic}, which queries {\\it cuBLASLt} library and selects the algorithm based on the heuristic's suggestion; and +11.4\\% over the most competitive {\\it cuBLASLt-AutoTuning} model, which selects the fastest algorithm from up to 100 candidates from {\\it cuBLASLt}'s suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7\\%, +26.0\\%, +22.4\\%, and +15.9\\% for {\\it torch.matmul}, {\\it cuBLAS}, {\\it cuBLASLt-heuristic}, and {\\it cuBLASLt-AutoTuning} respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6803ddda5c044d396d02f03a/Qz5NhI_UTCEzno_Fd5ndY.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02551.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6803ddda5c044d396d02f03a",
            "avatarUrl": "/avatars/ec583f6560d3d82fbd3bf95fb83c9356.svg",
            "fullname": "Xiaoya Li",
            "name": "xxiaoyali",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "68898157cad3d316633a440d",
            "name": "deepreinforce-ai",
            "fullname": "DeepReinforce",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68661bce022667cf9c7534fb/9BFOcjICEpj0xvyOHVp1h.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.20344",
            "authors": [
                {
                    "_id": "692eaf5537312eaa83fd899b",
                    "user": {
                        "_id": "6225c3ddc201a0ad62fee53d",
                        "avatarUrl": "/avatars/c42fef2c448deb8336d3aebc11bab148.svg",
                        "isPro": false,
                        "fullname": "Taewhoo Lee",
                        "user": "Taewhoo",
                        "type": "user"
                    },
                    "name": "Taewhoo Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-02T16:50:15.761Z",
                    "hidden": false
                },
                {
                    "_id": "692eaf5537312eaa83fd899c",
                    "name": "Minju Song",
                    "hidden": false
                },
                {
                    "_id": "692eaf5537312eaa83fd899d",
                    "name": "Chanwoong Yoon",
                    "hidden": false
                },
                {
                    "_id": "692eaf5537312eaa83fd899e",
                    "name": "Jungwoo Park",
                    "hidden": false
                },
                {
                    "_id": "692eaf5537312eaa83fd899f",
                    "name": "Jaewoo Kang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-25T14:23:58.000Z",
            "submittedOnDailyAt": "2025-12-03T03:44:17.639Z",
            "title": "The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6225c3ddc201a0ad62fee53d",
                "avatarUrl": "/avatars/c42fef2c448deb8336d3aebc11bab148.svg",
                "isPro": false,
                "fullname": "Taewhoo Lee",
                "user": "Taewhoo",
                "type": "user"
            },
            "summary": "Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition.",
            "upvotes": 9,
            "discussionId": "692eaf5537312eaa83fd89a0",
            "githubRepo": "https://github.com/dmis-lab/analogical-reasoning",
            "ai_summary": "LLMs can encode and apply high-level relational concepts in analogical reasoning but face limitations, particularly when relational information is missing or when transferring to new entities.",
            "ai_keywords": [
                "analogical reasoning",
                "LLMs",
                "task patterns",
                "surface-level concepts",
                "high-level relational concepts",
                "proportional and story analogies",
                "attributive information",
                "relational information",
                "mid-upper layers",
                "hidden representations",
                "token positions",
                "structural alignment"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "6621bc39e774284ec1742ab8",
                "name": "KoreaUniversity",
                "fullname": "Korea University"
            }
        },
        "publishedAt": "2025-11-25T09:23:58.000Z",
        "title": "The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models",
        "summary": "Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20344.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6225c3ddc201a0ad62fee53d",
            "avatarUrl": "/avatars/c42fef2c448deb8336d3aebc11bab148.svg",
            "fullname": "Taewhoo Lee",
            "name": "Taewhoo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6621bc39e774284ec1742ab8",
            "name": "KoreaUniversity",
            "fullname": "Korea University"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.01715",
            "authors": [
                {
                    "_id": "692fac3b26742347f61dac38",
                    "user": {
                        "_id": "640dd700fdeaae139081f598",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg",
                        "isPro": false,
                        "fullname": "Wanpeng Zhang",
                        "user": "zawnpn",
                        "type": "user"
                    },
                    "name": "Wanpeng Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:15:49.704Z",
                    "hidden": false
                },
                {
                    "_id": "692fac3b26742347f61dac39",
                    "name": "Ye Wang",
                    "hidden": false
                },
                {
                    "_id": "692fac3b26742347f61dac3a",
                    "name": "Hao Luo",
                    "hidden": false
                },
                {
                    "_id": "692fac3b26742347f61dac3b",
                    "name": "Haoqi Yuan",
                    "hidden": false
                },
                {
                    "_id": "692fac3b26742347f61dac3c",
                    "name": "Yicheng Feng",
                    "hidden": false
                },
                {
                    "_id": "692fac3b26742347f61dac3d",
                    "name": "Sipeng Zheng",
                    "hidden": false
                },
                {
                    "_id": "692fac3b26742347f61dac3e",
                    "name": "Qin Jin",
                    "hidden": false
                },
                {
                    "_id": "692fac3b26742347f61dac3f",
                    "name": "Zongqing Lu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/oemMx2I5b2nw8kp6EjaVc.mp4"
            ],
            "publishedAt": "2025-12-01T14:21:15.000Z",
            "submittedOnDailyAt": "2025-12-03T00:52:47.096Z",
            "title": "DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models",
            "submittedOnDailyBy": {
                "_id": "640dd700fdeaae139081f598",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg",
                "isPro": false,
                "fullname": "Wanpeng Zhang",
                "user": "zawnpn",
                "type": "user"
            },
            "summary": "Vision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tasks. However, their performance often degrades under distribution shift and on complex multi-step tasks, suggesting that the learned representations may not robustly capture task-relevant semantics. We introduce DiG-Flow, a principled framework that enhances VLA robustness through geometric regularization. Our key insight is that the distributional discrepancy between observation and action embeddings provides a meaningful geometric signal: lower transport cost indicates compatible representations, while higher cost suggests potential misalignment. DiG-Flow computes a discrepancy measure between empirical distributions of observation and action embeddings, maps it to a modulation weight via a monotone function, and applies residual updates to the observation embeddings before flow matching. Crucially, this intervention operates at the representation level without modifying the flow matching path or target vector field. We provide theoretical guarantees showing that discrepancy-guided training provably decreases the training objective, and that guided inference refinement converges with contraction. Empirically, DiG-Flow integrates into existing VLA architectures with negligible overhead and consistently improves performance, with particularly pronounced gains on complex multi-step tasks and under limited training data.",
            "upvotes": 8,
            "discussionId": "692fac3b26742347f61dac40",
            "projectPage": "https://beingbeyond.github.io/DiG-Flow/",
            "githubRepo": "https://github.com/BeingBeyond/DiG-Flow",
            "ai_summary": "DiG-Flow enhances VLA models' robustness by using geometric regularization to align observation and action embeddings, improving performance on complex tasks and with limited data.",
            "ai_keywords": [
                "flow matching",
                "distributional discrepancy",
                "transport cost",
                "empirical distributions",
                "observation embeddings",
                "action embeddings",
                "modulation weight",
                "monotone function",
                "residual updates",
                "contraction",
                "training objective",
                "inference refinement"
            ],
            "githubStars": 9,
            "organization": {
                "_id": "687a8ba5aedd77694bc94386",
                "name": "BeingBeyond",
                "fullname": "BeingBeyond",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/YBq5V88ndIv-QcHweJEUk.png"
            }
        },
        "publishedAt": "2025-12-01T09:21:15.000Z",
        "title": "DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models",
        "summary": "Vision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tasks. However, their performance often degrades under distribution shift and on complex multi-step tasks, suggesting that the learned representations may not robustly capture task-relevant semantics. We introduce DiG-Flow, a principled framework that enhances VLA robustness through geometric regularization. Our key insight is that the distributional discrepancy between observation and action embeddings provides a meaningful geometric signal: lower transport cost indicates compatible representations, while higher cost suggests potential misalignment. DiG-Flow computes a discrepancy measure between empirical distributions of observation and action embeddings, maps it to a modulation weight via a monotone function, and applies residual updates to the observation embeddings before flow matching. Crucially, this intervention operates at the representation level without modifying the flow matching path or target vector field. We provide theoretical guarantees showing that discrepancy-guided training provably decreases the training objective, and that guided inference refinement converges with contraction. Empirically, DiG-Flow integrates into existing VLA architectures with negligible overhead and consistently improves performance, with particularly pronounced gains on complex multi-step tasks and under limited training data.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/oemMx2I5b2nw8kp6EjaVc.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01715.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640dd700fdeaae139081f598",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg",
            "fullname": "Wanpeng Zhang",
            "name": "zawnpn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "687a8ba5aedd77694bc94386",
            "name": "BeingBeyond",
            "fullname": "BeingBeyond",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/YBq5V88ndIv-QcHweJEUk.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.02622",
            "authors": [
                {
                    "_id": "692fd68126742347f61dad5d",
                    "user": {
                        "_id": "6684a72f74af0ef94892a3fa",
                        "avatarUrl": "/avatars/69c8bb5696f55a83aab627316a629ba8.svg",
                        "isPro": false,
                        "fullname": "XUMING HE",
                        "user": "hexmSeeU",
                        "type": "user"
                    },
                    "name": "Xuming He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:13:20.641Z",
                    "hidden": false
                },
                {
                    "_id": "692fd68126742347f61dad5e",
                    "name": "Zehao Fan",
                    "hidden": false
                },
                {
                    "_id": "692fd68126742347f61dad5f",
                    "name": "Hengjia Li",
                    "hidden": false
                },
                {
                    "_id": "692fd68126742347f61dad60",
                    "name": "Fan Zhuo",
                    "hidden": false
                },
                {
                    "_id": "692fd68126742347f61dad61",
                    "name": "Hankun Xu",
                    "hidden": false
                },
                {
                    "_id": "692fd68126742347f61dad62",
                    "name": "Senlin Cheng",
                    "hidden": false
                },
                {
                    "_id": "692fd68126742347f61dad63",
                    "name": "Di Weng",
                    "hidden": false
                },
                {
                    "_id": "692fd68126742347f61dad64",
                    "name": "Haifeng Liu",
                    "hidden": false
                },
                {
                    "_id": "692fd68126742347f61dad65",
                    "name": "Can Ye",
                    "hidden": false
                },
                {
                    "_id": "692fd68126742347f61dad66",
                    "name": "Boxi Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T10:29:51.000Z",
            "submittedOnDailyAt": "2025-12-03T10:25:36.654Z",
            "title": "RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence",
            "submittedOnDailyBy": {
                "_id": "6684a72f74af0ef94892a3fa",
                "avatarUrl": "/avatars/69c8bb5696f55a83aab627316a629ba8.svg",
                "isPro": false,
                "fullname": "XUMING HE",
                "user": "hexmSeeU",
                "type": "user"
            },
            "summary": "Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking a crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack a fine-grained decomposition of reasoning capabilities and a comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, a benchmark designed to evaluate the reasoning ability of video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct a checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence.",
            "upvotes": 7,
            "discussionId": "692fd68126742347f61dad67",
            "projectPage": "https://hexmseeu.github.io/RULER-Bench-proj/",
            "githubRepo": "https://github.com/hexmSeeU/RULER-Bench",
            "ai_summary": "RULER-Bench evaluates video generation models' reasoning abilities through 40 tasks across six categories, revealing gaps in their rule coherence and providing a benchmark for future improvements.",
            "ai_keywords": [
                "video generation",
                "temporal consistency",
                "visual quality",
                "vision foundation models",
                "visual aesthetics",
                "instruction adherence",
                "temporal coherence",
                "zero-shot learners",
                "reasoning capabilities",
                "cognitive rules",
                "text-to-video",
                "image-to-video",
                "RULER-Bench",
                "annotated instances",
                "checklist",
                "GPT-o3",
                "rule coherence",
                "reasoning-aware video generation",
                "vision foundation intelligence"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-12-02T05:29:51.000Z",
        "title": "RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence",
        "summary": "Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking a crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack a fine-grained decomposition of reasoning capabilities and a comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, a benchmark designed to evaluate the reasoning ability of video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct a checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02622.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6684a72f74af0ef94892a3fa",
            "avatarUrl": "/avatars/69c8bb5696f55a83aab627316a629ba8.svg",
            "fullname": "XUMING HE",
            "name": "hexmSeeU",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.01248",
            "authors": [
                {
                    "_id": "692e868f37312eaa83fd8910",
                    "name": "Junyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "692e868f37312eaa83fd8911",
                    "name": "Bin Wang",
                    "hidden": false
                },
                {
                    "_id": "692e868f37312eaa83fd8912",
                    "name": "Qintong Zhang",
                    "hidden": false
                },
                {
                    "_id": "692e868f37312eaa83fd8913",
                    "name": "Fan Wu",
                    "hidden": false
                },
                {
                    "_id": "692e868f37312eaa83fd8914",
                    "name": "Zichen Wen",
                    "hidden": false
                },
                {
                    "_id": "692e868f37312eaa83fd8915",
                    "name": "Jialin Lu",
                    "hidden": false
                },
                {
                    "_id": "692e868f37312eaa83fd8916",
                    "name": "Junjie Shan",
                    "hidden": false
                },
                {
                    "_id": "692e868f37312eaa83fd8917",
                    "name": "Ziqi Zhao",
                    "hidden": false
                },
                {
                    "_id": "692e868f37312eaa83fd8918",
                    "name": "Shuya Yang",
                    "hidden": false
                },
                {
                    "_id": "692e868f37312eaa83fd8919",
                    "name": "Ziling Wang",
                    "hidden": false
                },
                {
                    "_id": "692e868f37312eaa83fd891a",
                    "name": "Ziyang Miao",
                    "hidden": false
                },
                {
                    "_id": "692e868f37312eaa83fd891b",
                    "name": "Huaping Zhong",
                    "hidden": false
                },
                {
                    "_id": "692e868f37312eaa83fd891c",
                    "name": "Yuhang Zang",
                    "hidden": false
                },
                {
                    "_id": "692e868f37312eaa83fd891d",
                    "name": "Xiaoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "692e868f37312eaa83fd891e",
                    "name": "Ka-Ho Chow",
                    "hidden": false
                },
                {
                    "_id": "692e868f37312eaa83fd891f",
                    "name": "Conghui He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-01T03:49:00.000Z",
            "submittedOnDailyAt": "2025-12-03T02:12:58.590Z",
            "title": "TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition",
            "submittedOnDailyBy": {
                "_id": "65411de281a8731a1c28293d",
                "avatarUrl": "/avatars/18607dcd65303e1f688fae8015f43314.svg",
                "isPro": false,
                "fullname": "junyuan",
                "user": "Carkham",
                "type": "user"
            },
            "summary": "Table recognition (TR) aims to transform table images into semi-structured representations such as HTML or Markdown. As a core component of document parsing, TR has long relied on supervised learning, with recent efforts dominated by fine-tuning vision-language models (VLMs) using labeled data. While VLMs have brought TR to the next level, pushing performance further demands large-scale labeled data that is costly to obtain. Consequently, although proprietary models have continuously pushed the performance boundary, open-source models, often trained with limited resources and, in practice, the only viable option for many due to privacy regulations, still lag far behind. To bridge this gap, we introduce TRivia, a self-supervised fine-tuning method that enables pretrained VLMs to learn TR directly from unlabeled table images in the wild. Built upon Group Relative Policy Optimization, TRivia automatically identifies unlabeled samples that most effectively facilitate learning and eliminates the need for human annotations through a question-answering-based reward mechanism. An attention-guided module generates diverse questions for each table image, and the ability to interpret the recognition results and answer them correctly provides feedback to optimize the TR model. This closed-loop process allows the TR model to autonomously learn to recognize, structure, and reason over tables without labeled data. Leveraging this pipeline, we present TRivia-3B, an open-sourced, compact, and state-of-the-art TR model that surpasses existing systems (e.g., Gemini 2.5 Pro, MinerU2.5) on three popular benchmarks. Model and code are released at: https://github.com/opendatalab/TRivia",
            "upvotes": 7,
            "discussionId": "692e868f37312eaa83fd8920",
            "githubRepo": "https://github.com/opendatalab/TRivia",
            "ai_summary": "TRivia, a self-supervised fine-tuning method, enables pretrained vision-language models to learn table recognition from unlabeled data using a question-answering-based reward mechanism, outperforming existing models on popular benchmarks.",
            "ai_keywords": [
                "self-supervised fine-tuning",
                "vision-language models",
                "Group Relative Policy Optimization",
                "attention-guided module",
                "question-answering-based reward mechanism",
                "table recognition",
                "TRivia",
                "TRivia-3B",
                "Gemini 2.5 Pro",
                "MinerU2.5"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-11-30T22:49:00.000Z",
        "title": "TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition",
        "summary": "Table recognition (TR) aims to transform table images into semi-structured representations such as HTML or Markdown. As a core component of document parsing, TR has long relied on supervised learning, with recent efforts dominated by fine-tuning vision-language models (VLMs) using labeled data. While VLMs have brought TR to the next level, pushing performance further demands large-scale labeled data that is costly to obtain. Consequently, although proprietary models have continuously pushed the performance boundary, open-source models, often trained with limited resources and, in practice, the only viable option for many due to privacy regulations, still lag far behind. To bridge this gap, we introduce TRivia, a self-supervised fine-tuning method that enables pretrained VLMs to learn TR directly from unlabeled table images in the wild. Built upon Group Relative Policy Optimization, TRivia automatically identifies unlabeled samples that most effectively facilitate learning and eliminates the need for human annotations through a question-answering-based reward mechanism. An attention-guided module generates diverse questions for each table image, and the ability to interpret the recognition results and answer them correctly provides feedback to optimize the TR model. This closed-loop process allows the TR model to autonomously learn to recognize, structure, and reason over tables without labeled data. Leveraging this pipeline, we present TRivia-3B, an open-sourced, compact, and state-of-the-art TR model that surpasses existing systems (e.g., Gemini 2.5 Pro, MinerU2.5) on three popular benchmarks. Model and code are released at: https://github.com/opendatalab/TRivia",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01248.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65411de281a8731a1c28293d",
            "avatarUrl": "/avatars/18607dcd65303e1f688fae8015f43314.svg",
            "fullname": "junyuan",
            "name": "Carkham",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.03046",
            "authors": [
                {
                    "_id": "69303cc62d1e5b0a7d84d75b",
                    "name": "Zichen Liu",
                    "hidden": false
                },
                {
                    "_id": "69303cc62d1e5b0a7d84d75c",
                    "name": "Yue Yu",
                    "hidden": false
                },
                {
                    "_id": "69303cc62d1e5b0a7d84d75d",
                    "name": "Hao Ouyang",
                    "hidden": false
                },
                {
                    "_id": "69303cc62d1e5b0a7d84d75e",
                    "name": "Qiuyu Wang",
                    "hidden": false
                },
                {
                    "_id": "69303cc62d1e5b0a7d84d75f",
                    "name": "Shuailei Ma",
                    "hidden": false
                },
                {
                    "_id": "69303cc62d1e5b0a7d84d760",
                    "name": "Ka Leong Cheng",
                    "hidden": false
                },
                {
                    "_id": "69303cc62d1e5b0a7d84d761",
                    "name": "Wen Wang",
                    "hidden": false
                },
                {
                    "_id": "69303cc62d1e5b0a7d84d762",
                    "name": "Qingyan Bai",
                    "hidden": false
                },
                {
                    "_id": "69303cc62d1e5b0a7d84d763",
                    "name": "Yuxuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "69303cc62d1e5b0a7d84d764",
                    "name": "Yanhong Zeng",
                    "hidden": false
                },
                {
                    "_id": "69303cc62d1e5b0a7d84d765",
                    "name": "Yixuan Li",
                    "hidden": false
                },
                {
                    "_id": "69303cc62d1e5b0a7d84d766",
                    "name": "Xing Zhu",
                    "hidden": false
                },
                {
                    "_id": "69303cc62d1e5b0a7d84d767",
                    "name": "Yujun Shen",
                    "hidden": false
                },
                {
                    "_id": "69303cc62d1e5b0a7d84d768",
                    "name": "Qifeng Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T18:59:58.000Z",
            "submittedOnDailyAt": "2025-12-03T11:28:09.999Z",
            "title": "MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We propose MagicQuill V2, a novel system that introduces a layered composition paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software. While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance. To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette. Our technical contributions include a specialized data generation pipeline for context-aware content integration, a unified control module to process all visual cues, and a fine-tuned spatial branch for precise local editing, including object removal. Extensive experiments validate that this layered approach effectively resolves the user intention gap, granting creators direct, intuitive control over the generative process.",
            "upvotes": 5,
            "discussionId": "69303cc62d1e5b0a7d84d769",
            "ai_summary": "MagicQuill V2 introduces a layered composition paradigm for generative image editing, combining diffusion models with granular control, enabling clear separation and manipulation of user intentions for content, position, shape, and color.",
            "ai_keywords": [
                "diffusion transformers",
                "layered composition",
                "generative image editing",
                "semantic power",
                "diffusion models",
                "granular control",
                "creative intent",
                "content layer",
                "spatial layer",
                "structural layer",
                "color layer",
                "context-aware content integration",
                "unified control module",
                "fine-tuned spatial branch",
                "object removal"
            ]
        },
        "publishedAt": "2025-12-02T13:59:58.000Z",
        "title": "MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues",
        "summary": "We propose MagicQuill V2, a novel system that introduces a layered composition paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software. While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance. To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette. Our technical contributions include a specialized data generation pipeline for context-aware content integration, a unified control module to process all visual cues, and a fine-tuned spatial branch for precise local editing, including object removal. Extensive experiments validate that this layered approach effectively resolves the user intention gap, granting creators direct, intuitive control over the generative process.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03046.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8835
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.22586",
            "authors": [
                {
                    "_id": "692d22f24397b1ec214f690b",
                    "name": "Yifan Du",
                    "hidden": false
                },
                {
                    "_id": "692d22f24397b1ec214f690c",
                    "name": "Kun Zhou",
                    "hidden": false
                },
                {
                    "_id": "692d22f24397b1ec214f690d",
                    "name": "Yingqian Min",
                    "hidden": false
                },
                {
                    "_id": "692d22f24397b1ec214f690e",
                    "name": "Yue Ling",
                    "hidden": false
                },
                {
                    "_id": "692d22f24397b1ec214f690f",
                    "name": "Wayne Xin Zhao",
                    "hidden": false
                },
                {
                    "_id": "692d22f24397b1ec214f6910",
                    "name": "Youbin Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-27T16:19:34.000Z",
            "submittedOnDailyAt": "2025-12-03T00:27:02.051Z",
            "title": "Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization",
            "submittedOnDailyBy": {
                "_id": "61d78857a21a9b49c7e8e4a9",
                "avatarUrl": "/avatars/c7e7f84cad775be2d13fab8530bf21f5.svg",
                "isPro": false,
                "fullname": "Yifan Du",
                "user": "Richard1999",
                "type": "user"
            },
            "summary": "We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as \"think with image\", has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a \"short is long\" effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.",
            "upvotes": 5,
            "discussionId": "692d22f24397b1ec214f6911",
            "ai_summary": "Investigating different Chain-of-Thought designs in vision-language models reveals that concise grounding steps are most effective for improving generalizable visual reasoning across various tasks.",
            "ai_keywords": [
                "Chain-of-Thought",
                "vision-language models",
                "visual reasoning",
                "CoT",
                "Grounding CoT",
                "Visual CoT",
                "maze-solving benchmark",
                "Qwen2.5-VL-7B",
                "SFT-then-RL",
                "visual reasoning tasks"
            ],
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-11-27T11:19:34.000Z",
        "title": "Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization",
        "summary": "We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as \"think with image\", has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a \"short is long\" effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22586.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61d78857a21a9b49c7e8e4a9",
            "avatarUrl": "/avatars/c7e7f84cad775be2d13fab8530bf21f5.svg",
            "fullname": "Yifan Du",
            "name": "Richard1999",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.01989",
            "authors": [
                {
                    "_id": "692e9bc537312eaa83fd896b",
                    "name": "Fengzhe Zhou",
                    "hidden": false
                },
                {
                    "_id": "692e9bc537312eaa83fd896c",
                    "user": {
                        "_id": "63cfa23ceea5651bbb80d981",
                        "avatarUrl": "/avatars/2c6839fc71e91591bc26441b928d4dc5.svg",
                        "isPro": false,
                        "fullname": "Jiannan Huang",
                        "user": "Jiannan2003",
                        "type": "user"
                    },
                    "name": "Jiannan Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:17:39.299Z",
                    "hidden": false
                },
                {
                    "_id": "692e9bc537312eaa83fd896d",
                    "name": "Jialuo Li",
                    "hidden": false
                },
                {
                    "_id": "692e9bc537312eaa83fd896e",
                    "name": "Deva Ramanan",
                    "hidden": false
                },
                {
                    "_id": "692e9bc537312eaa83fd896f",
                    "name": "Humphrey Shi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-01T18:47:39.000Z",
            "submittedOnDailyAt": "2025-12-03T00:15:44.928Z",
            "title": "PAI-Bench: A Comprehensive Benchmark For Physical AI",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.",
            "upvotes": 4,
            "discussionId": "692e9bc637312eaa83fd8970",
            "githubRepo": "https://github.com/SHI-Labs/physical-ai-bench",
            "ai_summary": "PAI-Bench evaluates the perception and prediction capabilities of multi-modal large language models and video generative models, revealing limitations in physical coherence and causal reasoning.",
            "ai_keywords": [
                "multi-modal large language models",
                "video generative models",
                "video generation",
                "conditional video generation",
                "video understanding",
                "physical plausibility",
                "domain-specific reasoning",
                "physically coherent dynamics",
                "forecasting",
                "causal interpretation"
            ],
            "githubStars": 31
        },
        "publishedAt": "2025-12-01T13:47:39.000Z",
        "title": "PAI-Bench: A Comprehensive Benchmark For Physical AI",
        "summary": "Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01989.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 176
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.03040",
            "authors": [
                {
                    "_id": "692fb17b26742347f61dac7e",
                    "name": "Zeqi Xiao",
                    "hidden": false
                },
                {
                    "_id": "692fb17b26742347f61dac7f",
                    "name": "Yiwei Zhao",
                    "hidden": false
                },
                {
                    "_id": "692fb17b26742347f61dac80",
                    "name": "Lingxiao Li",
                    "hidden": false
                },
                {
                    "_id": "692fb17b26742347f61dac81",
                    "name": "Yushi Lan",
                    "hidden": false
                },
                {
                    "_id": "692fb17b26742347f61dac82",
                    "name": "Yu Ning",
                    "hidden": false
                },
                {
                    "_id": "692fb17b26742347f61dac83",
                    "name": "Rahul Garg",
                    "hidden": false
                },
                {
                    "_id": "692fb17b26742347f61dac84",
                    "name": "Roshni Cooper",
                    "hidden": false
                },
                {
                    "_id": "692fb17b26742347f61dac85",
                    "name": "Mohammad H. Taghavi",
                    "hidden": false
                },
                {
                    "_id": "692fb17b26742347f61dac86",
                    "name": "Xingang Pan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T18:59:44.000Z",
            "submittedOnDailyAt": "2025-12-03T01:13:12.275Z",
            "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.",
            "upvotes": 3,
            "discussionId": "692fb17c26742347f61dac87",
            "ai_summary": "Video4Spatial demonstrates that video diffusion models can perform complex spatial tasks using only visual data, achieving strong spatial understanding and generalization.",
            "ai_keywords": [
                "video diffusion models",
                "scene navigation",
                "object grounding",
                "semantic localization",
                "instruction following",
                "planning",
                "3D geometry",
                "video-only inputs",
                "spatial consistency",
                "generalization"
            ]
        },
        "publishedAt": "2025-12-02T13:59:44.000Z",
        "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
        "summary": "We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03040.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8835
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.02492",
            "authors": [
                {
                    "_id": "692faf0326742347f61dac52",
                    "name": "Jiahui Chen",
                    "hidden": false
                },
                {
                    "_id": "692faf0326742347f61dac53",
                    "name": "Weida Wang",
                    "hidden": false
                },
                {
                    "_id": "692faf0326742347f61dac54",
                    "name": "Runhua Shi",
                    "hidden": false
                },
                {
                    "_id": "692faf0326742347f61dac55",
                    "name": "Huan Yang",
                    "hidden": false
                },
                {
                    "_id": "692faf0326742347f61dac56",
                    "name": "Chaofan Ding",
                    "hidden": false
                },
                {
                    "_id": "692faf0326742347f61dac57",
                    "name": "Zihao Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T07:31:19.000Z",
            "submittedOnDailyAt": "2025-12-03T01:02:18.953Z",
            "title": "YingVideo-MV: Music-Driven Multi-Stage Video Generation",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .",
            "upvotes": 3,
            "discussionId": "692faf0326742347f61dac58",
            "ai_summary": "YingVideo-MV generates high-quality music performance videos with synchronized camera motion using cascaded frameworks, audio semantic analysis, and temporal-aware diffusion Transformers.",
            "ai_keywords": [
                "diffusion model",
                "audio-driven avatar video generation",
                "audio semantic analysis",
                "shot planning module",
                "MV-Director",
                "temporal-aware diffusion Transformer architectures",
                "long-sequence consistency modeling",
                "Music-in-the-Wild Dataset",
                "camera adapter module",
                "latent noise",
                "time-aware dynamic window range strategy",
                "denoising ranges",
                "audio embedding",
                "music-motion-camera synchronization"
            ]
        },
        "publishedAt": "2025-12-02T02:31:19.000Z",
        "title": "YingVideo-MV: Music-Driven Multi-Stage Video Generation",
        "summary": "While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02492.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8835
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.01078",
            "authors": [
                {
                    "_id": "692ffba226742347f61db184",
                    "name": "Jiawei Ren",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db185",
                    "name": "Yan Zhuang",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db186",
                    "name": "Xiaokang Ye",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db187",
                    "name": "Lingjun Mao",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db188",
                    "name": "Xuhong He",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db189",
                    "name": "Jianzhi Shen",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db18a",
                    "name": "Mrinaal Dogra",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db18b",
                    "name": "Yiming Liang",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db18c",
                    "name": "Ruixuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db18d",
                    "name": "Tianai Yue",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db18e",
                    "name": "Yiqing Yang",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db18f",
                    "name": "Eric Liu",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db190",
                    "name": "Ryan Wu",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db191",
                    "name": "Kevin Benavente",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db192",
                    "name": "Rajiv Mandya Nagaraju",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db193",
                    "name": "Muhammad Faayez",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db194",
                    "name": "Xiyan Zhang",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db195",
                    "name": "Dhruv Vivek Sharma",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db196",
                    "name": "Xianrui Zhong",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db197",
                    "name": "Ziqiao Ma",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db198",
                    "name": "Tianmin Shu",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db199",
                    "name": "Zhiting Hu",
                    "hidden": false
                },
                {
                    "_id": "692ffba226742347f61db19a",
                    "name": "Lianhui Qin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-30T20:58:13.000Z",
            "submittedOnDailyAt": "2025-12-03T06:35:42.072Z",
            "title": "SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds",
            "submittedOnDailyBy": {
                "_id": "65509836e5f380aca800c408",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65509836e5f380aca800c408/wS6edLhx3YNaWWj0o8kaV.jpeg",
                "isPro": false,
                "fullname": "Lingjun Mao",
                "user": "mao1207",
                "type": "user"
            },
            "summary": "While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.",
            "upvotes": 3,
            "discussionId": "692ffba326742347f61db19b",
            "projectPage": "https://simworld.org/",
            "githubRepo": "https://github.com/SimWorld-AI/SimWorld",
            "ai_summary": "SimWorld, a new Unreal Engine 5-based simulator, enables the development and evaluation of LLM/VLM agents in realistic, real-world-like settings with diverse physical and social reasoning scenarios.",
            "ai_keywords": [
                "LLM",
                "VLM",
                "AI agents",
                "world simulation",
                "Unreal Engine 5",
                "procedural environment generation",
                "multimodal inputs",
                "open-vocabulary actions",
                "reasoning scenarios",
                "multi-agent delivery tasks",
                "strategic cooperation",
                "competition",
                "DeepSeek-Prover-V2",
                "GPT-4o",
                "Gemini-2.5-Flash",
                "Claude-3.5"
            ],
            "githubStars": 141
        },
        "publishedAt": "2025-11-30T15:58:13.000Z",
        "title": "SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds",
        "summary": "While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01078.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65509836e5f380aca800c408",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65509836e5f380aca800c408/wS6edLhx3YNaWWj0o8kaV.jpeg",
            "fullname": "Lingjun Mao",
            "name": "mao1207",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.00903",
            "authors": [
                {
                    "_id": "692fbdef26742347f61dacaf",
                    "name": "Chaojun Ni",
                    "hidden": false
                },
                {
                    "_id": "692fbdef26742347f61dacb0",
                    "name": "Cheng Chen",
                    "hidden": false
                },
                {
                    "_id": "692fbdef26742347f61dacb1",
                    "user": {
                        "_id": "6426616ea5ec4a5cbc535634",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6426616ea5ec4a5cbc535634/hH6JsxnXeakH3mBTeNNmO.jpeg",
                        "isPro": false,
                        "fullname": "JeffWang",
                        "user": "Jeff-Wang",
                        "type": "user"
                    },
                    "name": "Xiaofeng Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:15:30.398Z",
                    "hidden": false
                },
                {
                    "_id": "692fbdef26742347f61dacb2",
                    "user": {
                        "_id": "656e9b562cd7a3e348011d26",
                        "avatarUrl": "/avatars/bcca51bdc27c664f8f132420e6ed99fa.svg",
                        "isPro": false,
                        "fullname": "Zheng Zhu",
                        "user": "ZhengZhu",
                        "type": "user"
                    },
                    "name": "Zheng Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:15:32.398Z",
                    "hidden": false
                },
                {
                    "_id": "692fbdef26742347f61dacb3",
                    "name": "Wenzhao Zheng",
                    "hidden": false
                },
                {
                    "_id": "692fbdef26742347f61dacb4",
                    "name": "Boyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "692fbdef26742347f61dacb5",
                    "name": "Tianrun Chen",
                    "hidden": false
                },
                {
                    "_id": "692fbdef26742347f61dacb6",
                    "name": "Guosheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "692fbdef26742347f61dacb7",
                    "name": "Haoyun Li",
                    "hidden": false
                },
                {
                    "_id": "692fbdef26742347f61dacb8",
                    "name": "Zhehao Dong",
                    "hidden": false
                },
                {
                    "_id": "692fbdef26742347f61dacb9",
                    "name": "Qiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fbdef26742347f61dacba",
                    "name": "Yun Ye",
                    "hidden": false
                },
                {
                    "_id": "692fbdef26742347f61dacbb",
                    "name": "Yang Wang",
                    "hidden": false
                },
                {
                    "_id": "692fbdef26742347f61dacbc",
                    "name": "Guan Huang",
                    "hidden": false
                },
                {
                    "_id": "692fbdef26742347f61dacbd",
                    "name": "Wenjun Mei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-30T14:10:28.000Z",
            "submittedOnDailyAt": "2025-12-03T02:07:18.024Z",
            "title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
            "submittedOnDailyBy": {
                "_id": "656e9b562cd7a3e348011d26",
                "avatarUrl": "/avatars/bcca51bdc27c664f8f132420e6ed99fa.svg",
                "isPro": false,
                "fullname": "Zheng Zhu",
                "user": "ZhengZhu",
                "type": "user"
            },
            "summary": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.",
            "upvotes": 3,
            "discussionId": "692fbdef26742347f61dacbe",
            "projectPage": "https://swiftvla.github.io/",
            "githubRepo": "https://github.com/GigaAI-research/SwiftVLA",
            "ai_summary": "SwiftVLA enhances compact Vision-Language-Action models with 4D understanding using Fusion Tokens and a mask-and-reconstruct strategy, achieving high performance with reduced computational and memory demands.",
            "ai_keywords": [
                "pretrained Vision-Language Models",
                "VLA models",
                "spatiotemporal reasoning",
                "lightweight VLM",
                "3D inputs",
                "4D visual geometry transformer",
                "temporal cache",
                "Fusion Tokens",
                "future prediction objective",
                "mask-and-reconstruct strategy",
                "4D representations",
                "edge devices"
            ],
            "githubStars": 17,
            "organization": {
                "_id": "68f5feb151a9558c3dc84362",
                "name": "GigaAI-Research",
                "fullname": "GigaAI-Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b68f3cdd7f21b758968b8d/8uazzXK5v3edLbCaNvmup.png"
            }
        },
        "publishedAt": "2025-11-30T09:10:28.000Z",
        "title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
        "summary": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.00903.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656e9b562cd7a3e348011d26",
            "avatarUrl": "/avatars/bcca51bdc27c664f8f132420e6ed99fa.svg",
            "fullname": "Zheng Zhu",
            "name": "ZhengZhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "68f5feb151a9558c3dc84362",
            "name": "GigaAI-Research",
            "fullname": "GigaAI-Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b68f3cdd7f21b758968b8d/8uazzXK5v3edLbCaNvmup.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.22982",
            "authors": [
                {
                    "_id": "692ffb2b26742347f61db0d0",
                    "name": "Guo-Hua Wang",
                    "hidden": false
                },
                {
                    "_id": "692ffb2b26742347f61db0d1",
                    "name": "Liangfu Cao",
                    "hidden": false
                },
                {
                    "_id": "692ffb2b26742347f61db0d2",
                    "name": "Tianyu Cui",
                    "hidden": false
                },
                {
                    "_id": "692ffb2b26742347f61db0d3",
                    "name": "Minghao Fu",
                    "hidden": false
                },
                {
                    "_id": "692ffb2b26742347f61db0d4",
                    "name": "Xiaohao Chen",
                    "hidden": false
                },
                {
                    "_id": "692ffb2b26742347f61db0d5",
                    "name": "Pengxin Zhan",
                    "hidden": false
                },
                {
                    "_id": "692ffb2b26742347f61db0d6",
                    "name": "Jianshan Zhao",
                    "hidden": false
                },
                {
                    "_id": "692ffb2b26742347f61db0d7",
                    "name": "Lan Li",
                    "hidden": false
                },
                {
                    "_id": "692ffb2b26742347f61db0d8",
                    "name": "Bowen Fu",
                    "hidden": false
                },
                {
                    "_id": "692ffb2b26742347f61db0d9",
                    "name": "Jiaqi Liu",
                    "hidden": false
                },
                {
                    "_id": "692ffb2b26742347f61db0da",
                    "name": "Qing-Guo Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-28T08:42:31.000Z",
            "submittedOnDailyAt": "2025-12-03T10:32:15.538Z",
            "title": "Ovis-Image Technical Report",
            "submittedOnDailyBy": {
                "_id": "636f4c6b5d2050767e4a1491",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636f4c6b5d2050767e4a1491/qErBLQnYzL35EkbDAhvqN.jpeg",
                "isPro": false,
                "fullname": "Guo-Hua Wang",
                "user": "Flourish",
                "type": "user"
            },
            "summary": "We introduce Ovis-Image, a 7B text-to-image model specifically optimized for high-quality text rendering, designed to operate efficiently under stringent computational constraints. Built upon our previous Ovis-U1 framework, Ovis-Image integrates a diffusion-based visual decoder with the stronger Ovis 2.5 multimodal backbone, leveraging a text-centric training pipeline that combines large-scale pre-training with carefully tailored post-training refinements. Despite its compact architecture, Ovis-Image achieves text rendering performance on par with significantly larger open models such as Qwen-Image and approaches closed-source systems like Seedream and GPT4o. Crucially, the model remains deployable on a single high-end GPU with moderate memory, narrowing the gap between frontier-level text rendering and practical deployment. Our results indicate that combining a strong multimodal backbone with a carefully designed, text-focused training recipe is sufficient to achieve reliable bilingual text rendering without resorting to oversized or proprietary models.",
            "upvotes": 2,
            "discussionId": "692ffb2b26742347f61db0db",
            "githubRepo": "https://github.com/AIDC-AI/Ovis-Image",
            "ai_summary": "Ovis-Image is a 7B text-to-image model optimized for high-quality text rendering under computational constraints, combining a diffusion-based visual decoder with a multimodal backbone and a text-centric training pipeline.",
            "ai_keywords": [
                "text-to-image model",
                "diffusion-based visual decoder",
                "multimodal backbone",
                "text-centric training pipeline",
                "Qwen-Image",
                "Seedream",
                "GPT4o",
                "bilingual text rendering"
            ],
            "githubStars": 171
        },
        "publishedAt": "2025-11-28T03:42:31.000Z",
        "title": "Ovis-Image Technical Report",
        "summary": "We introduce Ovis-Image, a 7B text-to-image model specifically optimized for high-quality text rendering, designed to operate efficiently under stringent computational constraints. Built upon our previous Ovis-U1 framework, Ovis-Image integrates a diffusion-based visual decoder with the stronger Ovis 2.5 multimodal backbone, leveraging a text-centric training pipeline that combines large-scale pre-training with carefully tailored post-training refinements. Despite its compact architecture, Ovis-Image achieves text rendering performance on par with significantly larger open models such as Qwen-Image and approaches closed-source systems like Seedream and GPT4o. Crucially, the model remains deployable on a single high-end GPU with moderate memory, narrowing the gap between frontier-level text rendering and practical deployment. Our results indicate that combining a strong multimodal backbone with a carefully designed, text-focused training recipe is sufficient to achieve reliable bilingual text rendering without resorting to oversized or proprietary models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22982.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636f4c6b5d2050767e4a1491",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636f4c6b5d2050767e4a1491/qErBLQnYzL35EkbDAhvqN.jpeg",
            "fullname": "Guo-Hua Wang",
            "name": "Flourish",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.22973",
            "authors": [
                {
                    "_id": "692fc0c026742347f61dacc0",
                    "user": {
                        "_id": "64ec877bb93654d4ca5c92e9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                        "isPro": false,
                        "fullname": "Zeyu Zhang",
                        "user": "SteveZeyuZhang",
                        "type": "user"
                    },
                    "name": "Zeyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:15:28.241Z",
                    "hidden": false
                },
                {
                    "_id": "692fc0c026742347f61dacc1",
                    "name": "Shuning Chang",
                    "hidden": false
                },
                {
                    "_id": "692fc0c026742347f61dacc2",
                    "name": "Yuanyu He",
                    "hidden": false
                },
                {
                    "_id": "692fc0c026742347f61dacc3",
                    "name": "Yizeng Han",
                    "hidden": false
                },
                {
                    "_id": "692fc0c026742347f61dacc4",
                    "name": "Jiasheng Tang",
                    "hidden": false
                },
                {
                    "_id": "692fc0c026742347f61dacc5",
                    "name": "Fan Wang",
                    "hidden": false
                },
                {
                    "_id": "692fc0c026742347f61dacc6",
                    "name": "Bohan Zhuang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/3Lvr1j1cc28m7caAw-606.mp4"
            ],
            "publishedAt": "2025-11-28T08:25:59.000Z",
            "submittedOnDailyAt": "2025-12-03T02:19:51.264Z",
            "title": "BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation",
            "submittedOnDailyBy": {
                "_id": "64ec877bb93654d4ca5c92e9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                "isPro": false,
                "fullname": "Zeyu Zhang",
                "user": "SteveZeyuZhang",
                "type": "user"
            },
            "summary": "Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.",
            "upvotes": 2,
            "discussionId": "692fc0c126742347f61dacc7",
            "projectPage": "https://ziplab.co/BlockVid/",
            "githubRepo": "https://github.com/alibaba-damo-academy/Inferix/",
            "ai_summary": "BlockVid addresses challenges in block diffusion video generation by employing semantic-aware sparse KV caching, Block Forcing training, and noise scheduling to produce high-quality, coherent minute-long videos.",
            "ai_keywords": [
                "semi-autoregressive",
                "block diffusion",
                "diffusion models",
                "autoregressive models",
                "KV caching",
                "parallel sampling",
                "KV-cache-induced long-horizon error accumulation",
                "BlockVid",
                "semantic-aware sparse KV cache",
                "Block Forcing",
                "chunk-wise noise scheduling",
                "temporal consistency",
                "LV-Bench",
                "long-range coherence",
                "VBench",
                "VDE Subject",
                "VDE Clarity"
            ],
            "githubStars": 65,
            "organization": {
                "_id": "6808e7522a4d69d5111da55f",
                "name": "Alibaba-DAMO-Academy",
                "fullname": "DAMO Academy",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
            }
        },
        "publishedAt": "2025-11-28T03:25:59.000Z",
        "title": "BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation",
        "summary": "Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/3Lvr1j1cc28m7caAw-606.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22973.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "fullname": "Zeyu Zhang",
            "name": "SteveZeyuZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "6808e7522a4d69d5111da55f",
            "name": "Alibaba-DAMO-Academy",
            "fullname": "DAMO Academy",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.03013",
            "authors": [
                {
                    "_id": "693005d726742347f61db1c7",
                    "name": "Sagi Polaczek",
                    "hidden": false
                },
                {
                    "_id": "693005d726742347f61db1c8",
                    "name": "Or Patashnik",
                    "hidden": false
                },
                {
                    "_id": "693005d726742347f61db1c9",
                    "name": "Ali Mahdavi-Amiri",
                    "hidden": false
                },
                {
                    "_id": "693005d726742347f61db1ca",
                    "name": "Daniel Cohen-Or",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T18:40:35.000Z",
            "submittedOnDailyAt": "2025-12-03T13:12:49.559Z",
            "title": "In-Context Sync-LoRA for Portrait Video Editing",
            "submittedOnDailyBy": {
                "_id": "630b37dee67c604e9b79e773",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b37dee67c604e9b79e773/roSaYAsQ4E9rQrahYZitG.png",
                "isPro": false,
                "fullname": "Sagi Polaczek",
                "user": "SagiPolaczek",
                "type": "user"
            },
            "summary": "Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.",
            "upvotes": 1,
            "discussionId": "693005d726742347f61db1cb",
            "projectPage": "https://sagipolaczek.github.io/Sync-LoRA/",
            "githubRepo": "https://github.com/SagiPolaczek/Sync-LoRA",
            "ai_summary": "Sync-LoRA uses an image-to-video diffusion model trained with in-context LoRA to enable precise frame-accurate edits in portrait videos while maintaining identity consistency and temporal coherence.",
            "ai_keywords": [
                "diffusion model",
                "LoRA",
                "image-to-video",
                "synchronization",
                "temporal coherence",
                "visual modifications",
                "identity consistency",
                "edit fidelity",
                "pose variations",
                "expression changes",
                "background changes",
                "object addition"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-12-02T13:40:35.000Z",
        "title": "In-Context Sync-LoRA for Portrait Video Editing",
        "summary": "Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03013.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630b37dee67c604e9b79e773",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b37dee67c604e9b79e773/roSaYAsQ4E9rQrahYZitG.png",
            "fullname": "Sagi Polaczek",
            "name": "SagiPolaczek",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.02942",
            "authors": [
                {
                    "_id": "692fa4dd26742347f61daaf1",
                    "name": "Lanxiang Hu",
                    "hidden": false
                },
                {
                    "_id": "692fa4dd26742347f61daaf2",
                    "name": "Abhilash Shankarampeta",
                    "hidden": false
                },
                {
                    "_id": "692fa4dd26742347f61daaf3",
                    "name": "Yixin Huang",
                    "hidden": false
                },
                {
                    "_id": "692fa4dd26742347f61daaf4",
                    "name": "Zilin Dai",
                    "hidden": false
                },
                {
                    "_id": "692fa4dd26742347f61daaf5",
                    "name": "Haoyang Yu",
                    "hidden": false
                },
                {
                    "_id": "692fa4dd26742347f61daaf6",
                    "name": "Yujie Zhao",
                    "hidden": false
                },
                {
                    "_id": "692fa4dd26742347f61daaf7",
                    "name": "Haoqiang Kang",
                    "hidden": false
                },
                {
                    "_id": "692fa4dd26742347f61daaf8",
                    "name": "Daniel Zhao",
                    "hidden": false
                },
                {
                    "_id": "692fa4dd26742347f61daaf9",
                    "name": "Tajana Rosing",
                    "hidden": false
                },
                {
                    "_id": "692fa4dd26742347f61daafa",
                    "name": "Hao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T17:11:23.000Z",
            "submittedOnDailyAt": "2025-12-03T00:18:36.179Z",
            "title": "Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: https://github.com/hao-ai-lab/VideoScience{github.com/hao-ai-lab/VideoScience}.",
            "upvotes": 1,
            "discussionId": "692fa4dd26742347f61daafb",
            "ai_summary": "VideoScience-Bench evaluates video models' scientific reasoning by assessing their ability to generate phenomena consistent with undergraduate-level physics and chemistry concepts.",
            "ai_keywords": [
                "zero-shot reasoning",
                "scientific reasoning",
                "VideoScience-Bench",
                "prompts",
                "scientific concepts",
                "T2V",
                "I2V",
                "Prompt Consistency",
                "Phenomenon Congruency",
                "Correct Dynamism",
                "Immutability",
                "Spatio-Temporal Continuity",
                "VLM-as-a-Judge"
            ]
        },
        "publishedAt": "2025-12-02T12:11:23.000Z",
        "title": "Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench",
        "summary": "The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: https://github.com/hao-ai-lab/VideoScience{github.com/hao-ai-lab/VideoScience}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02942.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 176
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.02423",
            "authors": [
                {
                    "_id": "692fa63626742347f61dab09",
                    "name": "Haolong Yan",
                    "hidden": false
                },
                {
                    "_id": "692fa63626742347f61dab0a",
                    "name": "Yeqing Shen",
                    "hidden": false
                },
                {
                    "_id": "692fa63626742347f61dab0b",
                    "name": "Xin Huang",
                    "hidden": false
                },
                {
                    "_id": "692fa63626742347f61dab0c",
                    "name": "Jia Wang",
                    "hidden": false
                },
                {
                    "_id": "692fa63626742347f61dab0d",
                    "name": "Kaijun Tan",
                    "hidden": false
                },
                {
                    "_id": "692fa63626742347f61dab0e",
                    "name": "Zhixuan Liang",
                    "hidden": false
                },
                {
                    "_id": "692fa63626742347f61dab0f",
                    "name": "Hongxin Li",
                    "hidden": false
                },
                {
                    "_id": "692fa63626742347f61dab10",
                    "name": "Zheng Ge",
                    "hidden": false
                },
                {
                    "_id": "692fa63626742347f61dab11",
                    "name": "Osamu Yoshie",
                    "hidden": false
                },
                {
                    "_id": "692fa63626742347f61dab12",
                    "name": "Si Li",
                    "hidden": false
                },
                {
                    "_id": "692fa63626742347f61dab13",
                    "name": "Xiangyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fa63626742347f61dab14",
                    "name": "Daxin Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T05:11:23.000Z",
            "submittedOnDailyAt": "2025-12-03T00:24:05.516Z",
            "title": "GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "With the rapid development of Large Vision Language Models, the focus of Graphical User Interface (GUI) agent tasks shifts from single-screen tasks to complex screen navigation challenges. However, real-world GUI environments, such as PC software and mobile Apps, are often complex and proprietary, making it difficult to obtain the comprehensive environment information needed for agent training and evaluation. This limitation hinders systematic investigation and benchmarking of agent navigation capabilities. To address this limitation, we introduce GUI Exploration Lab, a simulation environment engine for GUI agent navigation research that enables flexible definition and composition of screens, icons, and navigation graphs, while providing full access to environment information for comprehensive agent training and evaluation. Through extensive experiments, we find that supervised fine-tuning enables effective memorization of fundamental knowledge, serving as a crucial foundation for subsequent training. Building on this, single-turn reinforcement learning further enhances generalization to unseen scenarios. Finally, multi-turn reinforcement learning encourages the development of exploration strategies through interactive trial and error, leading to further improvements in screen navigation performance. We validate our methods on both static and interactive benchmarks, demonstrating that our findings generalize effectively to real-world scenarios. These findings demonstrate the advantages of reinforcement learning approaches in GUI navigation and offer practical guidance for building more capable and generalizable GUI agents.",
            "upvotes": 1,
            "discussionId": "692fa63726742347f61dab15",
            "ai_summary": "GUI Exploration Lab enables effective training and evaluation of GUI agents through simulation, leveraging supervised and reinforcement learning to enhance navigation capabilities.",
            "ai_keywords": [
                "GUI Exploration Lab",
                "supervised fine-tuning",
                "single-turn reinforcement learning",
                "multi-turn reinforcement learning",
                "GUI agent navigation",
                "screen navigation",
                "static benchmarks",
                "interactive benchmarks"
            ]
        },
        "publishedAt": "2025-12-02T00:11:23.000Z",
        "title": "GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning",
        "summary": "With the rapid development of Large Vision Language Models, the focus of Graphical User Interface (GUI) agent tasks shifts from single-screen tasks to complex screen navigation challenges. However, real-world GUI environments, such as PC software and mobile Apps, are often complex and proprietary, making it difficult to obtain the comprehensive environment information needed for agent training and evaluation. This limitation hinders systematic investigation and benchmarking of agent navigation capabilities. To address this limitation, we introduce GUI Exploration Lab, a simulation environment engine for GUI agent navigation research that enables flexible definition and composition of screens, icons, and navigation graphs, while providing full access to environment information for comprehensive agent training and evaluation. Through extensive experiments, we find that supervised fine-tuning enables effective memorization of fundamental knowledge, serving as a crucial foundation for subsequent training. Building on this, single-turn reinforcement learning further enhances generalization to unseen scenarios. Finally, multi-turn reinforcement learning encourages the development of exploration strategies through interactive trial and error, leading to further improvements in screen navigation performance. We validate our methods on both static and interactive benchmarks, demonstrating that our findings generalize effectively to real-world scenarios. These findings demonstrate the advantages of reinforcement learning approaches in GUI navigation and offer practical guidance for building more capable and generalizable GUI agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02423.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 176
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.02351",
            "authors": [
                {
                    "_id": "693076202d1e5b0a7d84d89e",
                    "name": "Shwai He",
                    "hidden": false
                },
                {
                    "_id": "693076202d1e5b0a7d84d89f",
                    "name": "Chaorui Deng",
                    "hidden": false
                },
                {
                    "_id": "693076202d1e5b0a7d84d8a0",
                    "name": "Ang Li",
                    "hidden": false
                },
                {
                    "_id": "693076202d1e5b0a7d84d8a1",
                    "name": "Shen Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T02:47:29.000Z",
            "submittedOnDailyAt": "2025-12-03T15:11:45.440Z",
            "title": "Understanding and Harnessing Sparsity in Unified Multimodal Models",
            "submittedOnDailyBy": {
                "_id": "64468d7a3b28db22e7f84418",
                "avatarUrl": "/avatars/ff05994acd7da64a2ed6e44f12cf77bc.svg",
                "isPro": false,
                "fullname": "Shwai He",
                "user": "Shwai",
                "type": "user"
            },
            "summary": "Large multimodal models have achieved remarkable progress in both understanding and generation. Recent efforts pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. However, such unification introduces inference inefficiencies, e.g., specific tasks or samples may not require the full knowledge or capacity of the unified model. Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited. In this work, we first conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. Our study reveals that the understanding component exhibits notable compressibility in both understanding and generation tasks, which is more pronounced in the latter. In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate compression ratios. To address this limitation, we propose the Mixture-of-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. We validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that a fully trainable adaptation delivers additional gains. As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters. The code is released at https://github.com/Shwai-He/SparseUnifiedModel{this link}.",
            "upvotes": 1,
            "discussionId": "693076212d1e5b0a7d84d8a2",
            "ai_summary": "Unified multimodal models suffer from inefficiencies in certain tasks, leading to the proposal of Mixture-of-Experts Adaptation to improve compression and maintain performance.",
            "ai_keywords": [
                "unified multimodal models",
                "training-free pruning",
                "depth pruning",
                "width reduction",
                "understanding component",
                "generation components",
                "compressibility",
                "Mixture-of-Experts (MoE) Adaptation",
                "sparse activation",
                "expert-frozen tuning",
                "fully trainable adaptation",
                "BAGEL model"
            ],
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-12-01T21:47:29.000Z",
        "title": "Understanding and Harnessing Sparsity in Unified Multimodal Models",
        "summary": "Large multimodal models have achieved remarkable progress in both understanding and generation. Recent efforts pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. However, such unification introduces inference inefficiencies, e.g., specific tasks or samples may not require the full knowledge or capacity of the unified model. Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited. In this work, we first conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. Our study reveals that the understanding component exhibits notable compressibility in both understanding and generation tasks, which is more pronounced in the latter. In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate compression ratios. To address this limitation, we propose the Mixture-of-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. We validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that a fully trainable adaptation delivers additional gains. As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters. The code is released at https://github.com/Shwai-He/SparseUnifiedModel{this link}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02351.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64468d7a3b28db22e7f84418",
            "avatarUrl": "/avatars/ff05994acd7da64a2ed6e44f12cf77bc.svg",
            "fullname": "Shwai He",
            "name": "Shwai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.02017",
            "authors": [
                {
                    "_id": "6930c2572d1e5b0a7d84d8c6",
                    "name": "Shaowei Liu",
                    "hidden": false
                },
                {
                    "_id": "6930c2572d1e5b0a7d84d8c7",
                    "name": "David Yifan Yao",
                    "hidden": false
                },
                {
                    "_id": "6930c2572d1e5b0a7d84d8c8",
                    "name": "Saurabh Gupta",
                    "hidden": false
                },
                {
                    "_id": "6930c2572d1e5b0a7d84d8c9",
                    "name": "Shenlong Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/670462c6fd5ef6902568d7bd/N7xf653nCimInnri9TCXM.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/670462c6fd5ef6902568d7bd/LNH_wSw82bELyZCPwvEIv.mp4"
            ],
            "publishedAt": "2025-12-01T18:59:57.000Z",
            "submittedOnDailyAt": "2025-12-03T21:00:24.008Z",
            "title": "Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion",
            "submittedOnDailyBy": {
                "_id": "670462c6fd5ef6902568d7bd",
                "avatarUrl": "/avatars/2d6ea275ccc289f6f1b07d0c8e860888.svg",
                "isPro": false,
                "fullname": "Shaowei Liu",
                "user": "shaoweiliu",
                "type": "user"
            },
            "summary": "Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.",
            "upvotes": 1,
            "discussionId": "6930c2572d1e5b0a7d84d8ca",
            "projectPage": "https://stevenlsw.github.io/visualsync/",
            "githubRepo": "https://github.com/stevenlsw/visualsync",
            "ai_summary": "VisualSync aligns unposed, unsynchronized videos using multi-view dynamics and epipolar constraints, outperforming existing methods with millisecond accuracy.",
            "ai_keywords": [
                "multi-view dynamics",
                "epipolar constraints",
                "3D reconstruction",
                "feature matching",
                "dense tracking",
                "tracklets",
                "relative poses",
                "cross-view correspondences"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "65448bef5b5d9185ba3202b9",
                "name": "UIUC-CS",
                "fullname": "University of Illinois at Urbana-Champaign",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
            }
        },
        "publishedAt": "2025-12-01T13:59:57.000Z",
        "title": "Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion",
        "summary": "Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/670462c6fd5ef6902568d7bd/N7xf653nCimInnri9TCXM.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/670462c6fd5ef6902568d7bd/LNH_wSw82bELyZCPwvEIv.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02017.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "670462c6fd5ef6902568d7bd",
            "avatarUrl": "/avatars/2d6ea275ccc289f6f1b07d0c8e860888.svg",
            "fullname": "Shaowei Liu",
            "name": "shaoweiliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "65448bef5b5d9185ba3202b9",
            "name": "UIUC-CS",
            "fullname": "University of Illinois at Urbana-Champaign",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.01988",
            "authors": [
                {
                    "_id": "692fe12026742347f61dadbb",
                    "name": "Wei Tang",
                    "hidden": false
                },
                {
                    "_id": "692fe12026742347f61dadbc",
                    "user": {
                        "_id": "64297212e5f33939cf3a3d9b",
                        "avatarUrl": "/avatars/bd21759ab5d7e526b99fcb7ed813ffb3.svg",
                        "isPro": false,
                        "fullname": "yanpeng_sun",
                        "user": "syp115",
                        "type": "user"
                    },
                    "name": "Yanpeng Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:13:14.051Z",
                    "hidden": false
                },
                {
                    "_id": "692fe12026742347f61dadbd",
                    "name": "Shan Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fe12026742347f61dadbe",
                    "name": "Xiaofan Li",
                    "hidden": false
                },
                {
                    "_id": "692fe12026742347f61dadbf",
                    "name": "Piotr Koniusz",
                    "hidden": false
                },
                {
                    "_id": "692fe12026742347f61dadc0",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "692fe12026742347f61dadc1",
                    "name": "Na Zhao",
                    "hidden": false
                },
                {
                    "_id": "692fe12026742347f61dadc2",
                    "name": "Zechao Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-01T18:45:30.000Z",
            "submittedOnDailyAt": "2025-12-03T04:36:51.749Z",
            "title": "Artemis: Structured Visual Reasoning for Perception Policy Learning",
            "submittedOnDailyBy": {
                "_id": "64297212e5f33939cf3a3d9b",
                "avatarUrl": "/avatars/bd21759ab5d7e526b99fcb7ed813ffb3.svg",
                "isPro": false,
                "fullname": "yanpeng_sun",
                "user": "syp115",
                "type": "user"
            },
            "summary": "Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.",
            "upvotes": 1,
            "discussionId": "692fe12126742347f61dadc3",
            "projectPage": "https://vi-ocean.github.io/projects/artemis/",
            "githubRepo": "https://github.com/WayneTomas/Artemis",
            "ai_summary": "Artemis, a perception-policy learning framework, enhances performance on visual tasks by using structured spatial reasoning with (label, bounding-box) pairs instead of linguistic intermediate reasoning.",
            "ai_keywords": [
                "reinforcement-learning frameworks",
                "visual perception policy",
                "intermediate reasoning chains",
                "natural language",
                "semantic reasoning",
                "spatial reasoning",
                "object-centric space",
                "structured proposal-based reasoning",
                "bounding-box",
                "verifiable visual state",
                "direct supervision",
                "ambiguity",
                "grounding",
                "detection",
                "counting",
                "geometric-perception",
                "MLLM benchmarks",
                "scalable perception policies"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-12-01T13:45:30.000Z",
        "title": "Artemis: Structured Visual Reasoning for Perception Policy Learning",
        "summary": "Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01988.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64297212e5f33939cf3a3d9b",
            "avatarUrl": "/avatars/bd21759ab5d7e526b99fcb7ed813ffb3.svg",
            "fullname": "yanpeng_sun",
            "name": "syp115",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.02790",
            "authors": [
                {
                    "_id": "692fa53126742347f61daafd",
                    "name": "Keming Ye",
                    "hidden": false
                },
                {
                    "_id": "692fa53126742347f61daafe",
                    "name": "Zhipeng Huang",
                    "hidden": false
                },
                {
                    "_id": "692fa53126742347f61daaff",
                    "name": "Canmiao Fu",
                    "hidden": false
                },
                {
                    "_id": "692fa53126742347f61dab00",
                    "name": "Qingyang Liu",
                    "hidden": false
                },
                {
                    "_id": "692fa53126742347f61dab01",
                    "name": "Jiani Cai",
                    "hidden": false
                },
                {
                    "_id": "692fa53126742347f61dab02",
                    "name": "Zheqi Lv",
                    "hidden": false
                },
                {
                    "_id": "692fa53126742347f61dab03",
                    "name": "Chen Li",
                    "hidden": false
                },
                {
                    "_id": "692fa53126742347f61dab04",
                    "name": "Jing Lyu",
                    "hidden": false
                },
                {
                    "_id": "692fa53126742347f61dab05",
                    "name": "Zhou Zhao",
                    "hidden": false
                },
                {
                    "_id": "692fa53126742347f61dab06",
                    "name": "Shengyu Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/DrQsXnJQZzdo8CTSQgen2.png"
            ],
            "publishedAt": "2025-12-01T17:45:44.000Z",
            "submittedOnDailyAt": "2025-12-03T00:19:53.098Z",
            "title": "UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "With the rapid advances of powerful multimodal models such as GPT-4o, Nano Banana, and Seedream 4.0 in Image Editing, the performance gap between closed-source and open-source models is widening, primarily due to the scarcity of large-scale, high-quality training data and comprehensive benchmarks capable of diagnosing model weaknesses across diverse editing behaviors. Existing data construction methods face a scale-quality trade-off: human annotations are high-quality but not scalable, while automated pipelines suffer from error propagation and noise. To address this, we introduce a lightweight data pipeline that replaces multi-toolchains with an end-to-end model and a unified post-verification stage. For scalable quality control, we train a 7B dual-task expert model, Qwen-Verify, for efficient failure detection and instruction recaptioning. This pipeline yields UnicEdit-10M, a 10M-scale dataset spanning diverse basic and complex editing tasks. We also propose UnicBench, a general benchmark that extends beyond basic edits to explicitly assess spatial and knowledge-driven reasoning. To enable fine-grained diagnosis, we introduce novel metrics, including Non-edit Consistency and Reasoning Accuracy. Our analysis of mainstream models on UnicBench reveals their limitations and provides clear directions for future research.",
            "upvotes": 1,
            "discussionId": "692fa53126742347f61dab07",
            "ai_summary": "A lightweight data pipeline and benchmark are introduced to improve the quality and scale of image editing datasets and model evaluations, addressing the performance gap between closed-source and open-source models.",
            "ai_keywords": [
                "GPT-4o",
                "Nano Banana",
                "Seedream 4.0",
                "multimodal models",
                "Qwen-Verify",
                "UnicEdit-10M",
                "UnicBench",
                "Non-edit Consistency",
                "Reasoning Accuracy"
            ]
        },
        "publishedAt": "2025-12-01T12:45:44.000Z",
        "title": "UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits",
        "summary": "With the rapid advances of powerful multimodal models such as GPT-4o, Nano Banana, and Seedream 4.0 in Image Editing, the performance gap between closed-source and open-source models is widening, primarily due to the scarcity of large-scale, high-quality training data and comprehensive benchmarks capable of diagnosing model weaknesses across diverse editing behaviors. Existing data construction methods face a scale-quality trade-off: human annotations are high-quality but not scalable, while automated pipelines suffer from error propagation and noise. To address this, we introduce a lightweight data pipeline that replaces multi-toolchains with an end-to-end model and a unified post-verification stage. For scalable quality control, we train a 7B dual-task expert model, Qwen-Verify, for efficient failure detection and instruction recaptioning. This pipeline yields UnicEdit-10M, a 10M-scale dataset spanning diverse basic and complex editing tasks. We also propose UnicBench, a general benchmark that extends beyond basic edits to explicitly assess spatial and knowledge-driven reasoning. To enable fine-grained diagnosis, we introduce novel metrics, including Non-edit Consistency and Reasoning Accuracy. Our analysis of mainstream models on UnicBench reveals their limitations and provides clear directions for future research.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/DrQsXnJQZzdo8CTSQgen2.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02790.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 176
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.01540",
            "authors": [
                {
                    "_id": "69305d972d1e5b0a7d84d836",
                    "name": "Zipeng Wang",
                    "hidden": false
                },
                {
                    "_id": "69305d972d1e5b0a7d84d837",
                    "name": "Dan Xu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65d31df8e3667040af7bc945/y4Stim5tbqenx3rCO0Csi.png"
            ],
            "publishedAt": "2025-12-01T11:12:37.000Z",
            "submittedOnDailyAt": "2025-12-03T13:36:27.976Z",
            "title": "FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention",
            "submittedOnDailyBy": {
                "_id": "65d31df8e3667040af7bc945",
                "avatarUrl": "/avatars/abe1137fccc45da0e1bd41ef0c172899.svg",
                "isPro": false,
                "fullname": "Zipeng Wang",
                "user": "ZipW",
                "type": "user"
            },
            "summary": "3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.",
            "upvotes": 1,
            "discussionId": "69305d982d1e5b0a7d84d838",
            "projectPage": "https://wzpscott.github.io/flashvggt_page/",
            "githubRepo": "https://github.com/wzpscott/flashvggt",
            "ai_summary": "FlashVGGT uses descriptor-based attention to efficiently perform 3D reconstruction from multi-view images, significantly reducing inference time and improving scalability compared to VGGT.",
            "ai_keywords": [
                "feed-forward methods",
                "Visual Geometry Grounding Transformer (VGGT)",
                "self-attention",
                "descriptor-based attention",
                "descriptor tokens",
                "cross-attention",
                "online inference",
                "chunk-recursive mechanism"
            ],
            "githubStars": 21,
            "organization": {
                "_id": "63355133edc1a61aecf74b0e",
                "name": "HKUST",
                "fullname": "HKUST"
            }
        },
        "publishedAt": "2025-12-01T06:12:37.000Z",
        "title": "FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention",
        "summary": "3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65d31df8e3667040af7bc945/y4Stim5tbqenx3rCO0Csi.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01540.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d31df8e3667040af7bc945",
            "avatarUrl": "/avatars/abe1137fccc45da0e1bd41ef0c172899.svg",
            "fullname": "Zipeng Wang",
            "name": "ZipW",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "63355133edc1a61aecf74b0e",
            "name": "HKUST",
            "fullname": "HKUST"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.22184",
            "authors": [
                {
                    "_id": "6930d3ad2d1e5b0a7d84d900",
                    "name": "Daniel Sungho Jung",
                    "hidden": false
                },
                {
                    "_id": "6930d3ad2d1e5b0a7d84d901",
                    "name": "Kyoung Mu Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-27T07:50:47.000Z",
            "submittedOnDailyAt": "2025-12-03T21:51:10.919Z",
            "title": "Shoe Style-Invariant and Ground-Aware Learning for Dense Foot Contact Estimation",
            "submittedOnDailyBy": {
                "_id": "65601c6ee23401f82005e361",
                "avatarUrl": "/avatars/e9fc24bd8c5afd8b07a2f42765d44a7d.svg",
                "isPro": false,
                "fullname": "Daniel Sungho Jung",
                "user": "dqj5182",
                "type": "user"
            },
            "summary": "Foot contact plays a critical role in human interaction with the world, and thus exploring foot contact can advance our understanding of human movement and physical interaction. Despite its importance, existing methods often approximate foot contact using a zero-velocity constraint and focus on joint-level contact, failing to capture the detailed interaction between the foot and the world. Dense estimation of foot contact is crucial for accurately modeling this interaction, yet predicting dense foot contact from a single RGB image remains largely underexplored. There are two main challenges for learning dense foot contact estimation. First, shoes exhibit highly diverse appearances, making it difficult for models to generalize across different styles. Second, ground often has a monotonous appearance, making it difficult to extract informative features. To tackle these issues, we present a FEet COntact estimation (FECO) framework that learns dense foot contact with shoe style-invariant and ground-aware learning. To overcome the challenge of shoe appearance diversity, our approach incorporates shoe style adversarial training that enforces shoe style-invariant features for contact estimation. To effectively utilize ground information, we introduce a ground feature extractor that captures ground properties based on spatial context. As a result, our proposed method achieves robust foot contact estimation regardless of shoe appearance and effectively leverages ground information. Code will be released.",
            "upvotes": 1,
            "discussionId": "6930d3ad2d1e5b0a7d84d902",
            "ai_summary": "A framework for dense foot contact estimation addresses challenges of shoe appearance diversity and ground feature extraction using adversarial training and spatial context-based feature extraction.",
            "ai_keywords": [
                "foot contact estimation",
                "zero-velocity constraint",
                "shoe style adversarial training",
                "ground feature extractor",
                "spatial context"
            ],
            "organization": {
                "_id": "66d54dc8033492801db2bf5a",
                "name": "SeoulNatlUniv",
                "fullname": "Seoul National University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/659ccc9d18897eb6594e897f/_-0BM-1UyM-d-lRiahFnf.png"
            }
        },
        "publishedAt": "2025-11-27T02:50:47.000Z",
        "title": "Shoe Style-Invariant and Ground-Aware Learning for Dense Foot Contact Estimation",
        "summary": "Foot contact plays a critical role in human interaction with the world, and thus exploring foot contact can advance our understanding of human movement and physical interaction. Despite its importance, existing methods often approximate foot contact using a zero-velocity constraint and focus on joint-level contact, failing to capture the detailed interaction between the foot and the world. Dense estimation of foot contact is crucial for accurately modeling this interaction, yet predicting dense foot contact from a single RGB image remains largely underexplored. There are two main challenges for learning dense foot contact estimation. First, shoes exhibit highly diverse appearances, making it difficult for models to generalize across different styles. Second, ground often has a monotonous appearance, making it difficult to extract informative features. To tackle these issues, we present a FEet COntact estimation (FECO) framework that learns dense foot contact with shoe style-invariant and ground-aware learning. To overcome the challenge of shoe appearance diversity, our approach incorporates shoe style adversarial training that enforces shoe style-invariant features for contact estimation. To effectively utilize ground information, we introduce a ground feature extractor that captures ground properties based on spatial context. As a result, our proposed method achieves robust foot contact estimation regardless of shoe appearance and effectively leverages ground information. Code will be released.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22184.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65601c6ee23401f82005e361",
            "avatarUrl": "/avatars/e9fc24bd8c5afd8b07a2f42765d44a7d.svg",
            "fullname": "Daniel Sungho Jung",
            "name": "dqj5182",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "66d54dc8033492801db2bf5a",
            "name": "SeoulNatlUniv",
            "fullname": "Seoul National University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/659ccc9d18897eb6594e897f/_-0BM-1UyM-d-lRiahFnf.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.22146",
            "authors": [
                {
                    "_id": "692fd6c726742347f61dad69",
                    "name": "Kairong Han",
                    "hidden": false
                },
                {
                    "_id": "692fd6c726742347f61dad6a",
                    "name": "Nuanqiao Shan",
                    "hidden": false
                },
                {
                    "_id": "692fd6c726742347f61dad6b",
                    "name": "Ziyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "692fd6c726742347f61dad6c",
                    "name": "Zijing Hu",
                    "hidden": false
                },
                {
                    "_id": "692fd6c726742347f61dad6d",
                    "name": "Xinpeng Dong",
                    "hidden": false
                },
                {
                    "_id": "692fd6c726742347f61dad6e",
                    "name": "Junjian Ye",
                    "hidden": false
                },
                {
                    "_id": "692fd6c726742347f61dad6f",
                    "name": "Lujia Pan",
                    "hidden": false
                },
                {
                    "_id": "692fd6c726742347f61dad70",
                    "name": "Fei Wu",
                    "hidden": false
                },
                {
                    "_id": "692fd6c726742347f61dad71",
                    "name": "Kun Kuang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-27T06:33:33.000Z",
            "submittedOnDailyAt": "2025-12-03T03:52:11.482Z",
            "title": "C^2DLM: Causal Concept-Guided Diffusion Large Language Models",
            "submittedOnDailyBy": {
                "_id": "68c6b73ad0bef8d72febf45d",
                "avatarUrl": "/avatars/7b65febb59f2524a81b08c9f6bd881f5.svg",
                "isPro": false,
                "fullname": "Kairong Han",
                "user": "Kairong-Han",
                "type": "user"
            },
            "summary": "Autoregressive (AR) language models and Diffusion Language Models (DLMs) constitute the two principal paradigms of large language models. However, both paradigms suffer from insufficient reasoning capabilities. Human reasoning inherently relies on causal knowledge and thought, which are reflected in natural language. But in the AR paradigm, language is modeled as next token prediction (a strictly left-to-right, token-by-token order), whereas natural language itself exhibits more flexible causal structures. In the DLM paradigm, the attention mechanism is fully connected, which entirely disregards causal order. To fill this gap, we propose a \\textbf{C}ausal \\textbf{C}oncept-Guided \\textbf{D}iffusion \\textbf{L}anguage \\textbf{M}odel (C^2DLM). Starting from DLM's fully connected attention, C^2DLM first obtains a concept-level causal graph from the teacher model, and then explicitly guides attention to learn causal relationships between concepts. By focusing on causal relationships and avoiding interference from difficult subgoals involving causal inversion, C^2DLM improves 12\\% with about 3.2 times training speedup in the COT-OrderPerturb task, and achieves an average gain of 1.31\\% across six downstream reasoning tasks. More details in the repository ~https://github.com/Kairong-Han/C-2-DLM{here}.",
            "upvotes": 1,
            "discussionId": "692fd6c726742347f61dad72",
            "ai_summary": "A Causal Concept-Guided Diffusion Language Model (C2DLM) improves reasoning capabilities by learning causal relationships between concepts, enhancing performance and training efficiency in downstream tasks.",
            "ai_keywords": [
                "autoregressive models",
                "diffusion language models",
                "causal knowledge",
                "next token prediction",
                "fully connected attention",
                "causal graph",
                "causal relationships",
                "causal inversion",
                "COT-OrderPerturb task",
                "downstream reasoning tasks"
            ]
        },
        "publishedAt": "2025-11-27T01:33:33.000Z",
        "title": "C^2DLM: Causal Concept-Guided Diffusion Large Language Models",
        "summary": "Autoregressive (AR) language models and Diffusion Language Models (DLMs) constitute the two principal paradigms of large language models. However, both paradigms suffer from insufficient reasoning capabilities. Human reasoning inherently relies on causal knowledge and thought, which are reflected in natural language. But in the AR paradigm, language is modeled as next token prediction (a strictly left-to-right, token-by-token order), whereas natural language itself exhibits more flexible causal structures. In the DLM paradigm, the attention mechanism is fully connected, which entirely disregards causal order. To fill this gap, we propose a \\textbf{C}ausal \\textbf{C}oncept-Guided \\textbf{D}iffusion \\textbf{L}anguage \\textbf{M}odel (C^2DLM). Starting from DLM's fully connected attention, C^2DLM first obtains a concept-level causal graph from the teacher model, and then explicitly guides attention to learn causal relationships between concepts. By focusing on causal relationships and avoiding interference from difficult subgoals involving causal inversion, C^2DLM improves 12\\% with about 3.2 times training speedup in the COT-OrderPerturb task, and achieves an average gain of 1.31\\% across six downstream reasoning tasks. More details in the repository ~https://github.com/Kairong-Han/C-2-DLM{here}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22146.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68c6b73ad0bef8d72febf45d",
            "avatarUrl": "/avatars/7b65febb59f2524a81b08c9f6bd881f5.svg",
            "fullname": "Kairong Han",
            "name": "Kairong-Han",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.00097",
            "authors": [
                {
                    "_id": "692f338d26742347f61daa57",
                    "name": "Boyan Duan",
                    "hidden": false
                },
                {
                    "_id": "692f338d26742347f61daa58",
                    "name": "Xiao Liang",
                    "hidden": false
                },
                {
                    "_id": "692f338d26742347f61daa59",
                    "name": "Shuai Lu",
                    "hidden": false
                },
                {
                    "_id": "692f338d26742347f61daa5a",
                    "name": "Yaoxiang Wang",
                    "hidden": false
                },
                {
                    "_id": "692f338d26742347f61daa5b",
                    "name": "Yelong Shen",
                    "hidden": false
                },
                {
                    "_id": "692f338d26742347f61daa5c",
                    "name": "Kai-Wei Chang",
                    "hidden": false
                },
                {
                    "_id": "692f338d26742347f61daa5d",
                    "name": "Ying Nian Wu",
                    "hidden": false
                },
                {
                    "_id": "692f338d26742347f61daa5e",
                    "name": "Mao Yang",
                    "hidden": false
                },
                {
                    "_id": "692f338d26742347f61daa5f",
                    "name": "Weizhu Chen",
                    "hidden": false
                },
                {
                    "_id": "692f338d26742347f61daa60",
                    "name": "Yeyun Gong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-27T01:05:00.000Z",
            "submittedOnDailyAt": "2025-12-03T15:45:36.440Z",
            "title": "Gold-Medal-Level Olympiad Geometry Solving with Efficient Heuristic Auxiliary Constructions",
            "submittedOnDailyBy": {
                "_id": "6560763e152b659e623865ae",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6560763e152b659e623865ae/cTT2jGnPU_8XMrUTvqZ2h.jpeg",
                "isPro": false,
                "fullname": "Xiao Liang",
                "user": "MasterVito",
                "type": "user"
            },
            "summary": "Automated theorem proving in Euclidean geometry, particularly for International Mathematical Olympiad (IMO) level problems, remains a major challenge and an important research focus in Artificial Intelligence. In this paper, we present a highly efficient method for geometry theorem proving that runs entirely on CPUs without relying on neural network-based inference. Our initial study shows that a simple random strategy for adding auxiliary points can achieve silver-medal level human performance on IMO. Building on this, we propose HAGeo, a Heuristic-based method for adding Auxiliary constructions in Geometric deduction that solves 28 of 30 problems on the IMO-30 benchmark, achieving gold-medal level performance and surpassing AlphaGeometry, a competitive neural network-based approach, by a notable margin. To evaluate our method and existing approaches more comprehensively, we further construct HAGeo-409, a benchmark consisting of 409 geometry problems with human-assessed difficulty levels. Compared with the widely used IMO-30, our benchmark poses greater challenges and provides a more precise evaluation, setting a higher bar for geometry theorem proving.",
            "upvotes": 1,
            "discussionId": "692f338e26742347f61daa61",
            "githubRepo": "https://github.com/boduan1/HAGeo",
            "ai_summary": "HAGeo, a heuristic-based method for adding auxiliary constructions in geometric deduction, achieves gold-medal level performance on IMO geometry problems, surpassing neural network-based approaches.",
            "ai_keywords": [
                "automated theorem proving",
                "Euclidean geometry",
                "International Mathematical Olympiad",
                "random strategy",
                "auxiliary points",
                "heuristic-based method",
                "geometric deduction",
                "HAGeo",
                "AlphaGeometry",
                "HAGeo-409",
                "geometry problems",
                "human-assessed difficulty levels"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "5e6485f787403103f9f1055e",
                "name": "microsoft",
                "fullname": "Microsoft",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
            }
        },
        "publishedAt": "2025-11-26T20:05:00.000Z",
        "title": "Gold-Medal-Level Olympiad Geometry Solving with Efficient Heuristic Auxiliary Constructions",
        "summary": "Automated theorem proving in Euclidean geometry, particularly for International Mathematical Olympiad (IMO) level problems, remains a major challenge and an important research focus in Artificial Intelligence. In this paper, we present a highly efficient method for geometry theorem proving that runs entirely on CPUs without relying on neural network-based inference. Our initial study shows that a simple random strategy for adding auxiliary points can achieve silver-medal level human performance on IMO. Building on this, we propose HAGeo, a Heuristic-based method for adding Auxiliary constructions in Geometric deduction that solves 28 of 30 problems on the IMO-30 benchmark, achieving gold-medal level performance and surpassing AlphaGeometry, a competitive neural network-based approach, by a notable margin. To evaluate our method and existing approaches more comprehensively, we further construct HAGeo-409, a benchmark consisting of 409 geometry problems with human-assessed difficulty levels. Compared with the widely used IMO-30, our benchmark poses greater challenges and provides a more precise evaluation, setting a higher bar for geometry theorem proving.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.00097.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6560763e152b659e623865ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6560763e152b659e623865ae/cTT2jGnPU_8XMrUTvqZ2h.jpeg",
            "fullname": "Xiao Liang",
            "name": "MasterVito",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "5e6485f787403103f9f1055e",
            "name": "microsoft",
            "fullname": "Microsoft",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.21338",
            "authors": [
                {
                    "_id": "692d866d0ce2ec9461c880bb",
                    "user": {
                        "_id": "67d44d099c024761ad196132",
                        "avatarUrl": "/avatars/614c79d727424e427d65fe6ee7b53abd.svg",
                        "isPro": false,
                        "fullname": "Julianna Piskorz",
                        "user": "jpiskorz",
                        "type": "user"
                    },
                    "name": "Julianna Piskorz",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-01T16:24:36.550Z",
                    "hidden": false
                },
                {
                    "_id": "692d866d0ce2ec9461c880bc",
                    "name": "Cristina Pinneri",
                    "hidden": false
                },
                {
                    "_id": "692d866d0ce2ec9461c880bd",
                    "name": "Alvaro Correia",
                    "hidden": false
                },
                {
                    "_id": "692d866d0ce2ec9461c880be",
                    "name": "Motasem Alfarra",
                    "hidden": false
                },
                {
                    "_id": "692d866d0ce2ec9461c880bf",
                    "name": "Risheek Garrepalli",
                    "hidden": false
                },
                {
                    "_id": "692d866d0ce2ec9461c880c0",
                    "name": "Christos Louizos",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-26T12:44:29.000Z",
            "submittedOnDailyAt": "2025-12-03T08:52:08.143Z",
            "title": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models",
            "submittedOnDailyBy": {
                "_id": "67d44d099c024761ad196132",
                "avatarUrl": "/avatars/614c79d727424e427d65fe6ee7b53abd.svg",
                "isPro": false,
                "fullname": "Julianna Piskorz",
                "user": "jpiskorz",
                "type": "user"
            },
            "summary": "Masked Diffusion Language Models (MDLMs) have recently emerged as a promising alternative to Autoregressive Language Models (ARLMs), leveraging a denoising objective that, in principle, should enable more uniform context utilisation. In this work, we examine the context comprehension abilities of MDLMs and uncover two key limitations. First, despite their more global training objective and bidirectional attention mechanism, similarly to ARLMS, MDLMs exhibit a strong locality bias: performance is highly sensitive to the position of relevant information within the input, favouring local over distant context. Second, we show that appending a large number of mask tokens--required for generation--can significantly degrade context comprehension. Through systematic ablations, we find that these masks act as distractors, reducing the model's ability to process relevant information. To address this, we introduce a mask-agnostic loss function that encourages predictions to remain invariant to the number of appended masks. Fine-tuning with this objective substantially mitigates the distracting effect of masks, improving robustness of MDLMs. Overall, our findings reveal critical limitations of the current MDLM training paradigm and provide actionable insights for building diffusion-based language models with stronger context comprehension.",
            "upvotes": 1,
            "discussionId": "692d866d0ce2ec9461c880c1",
            "ai_summary": "Masked Diffusion Language Models (MDLMs) exhibit locality bias and are negatively impacted by appended mask tokens, but a mask-agnostic loss function improves their context comprehension.",
            "ai_keywords": [
                "Masked Diffusion Language Models",
                "MDLMs",
                "Autoregressive Language Models",
                "ARLMs",
                "denoising objective",
                "bidirectional attention",
                "locality bias",
                "mask tokens",
                "mask-agnostic loss function",
                "context comprehension"
            ],
            "organization": {
                "_id": "616851aa840fa49535b3d5b2",
                "name": "qualcomm",
                "fullname": "Qualcomm",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
            }
        },
        "publishedAt": "2025-11-26T07:44:29.000Z",
        "title": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models",
        "summary": "Masked Diffusion Language Models (MDLMs) have recently emerged as a promising alternative to Autoregressive Language Models (ARLMs), leveraging a denoising objective that, in principle, should enable more uniform context utilisation. In this work, we examine the context comprehension abilities of MDLMs and uncover two key limitations. First, despite their more global training objective and bidirectional attention mechanism, similarly to ARLMS, MDLMs exhibit a strong locality bias: performance is highly sensitive to the position of relevant information within the input, favouring local over distant context. Second, we show that appending a large number of mask tokens--required for generation--can significantly degrade context comprehension. Through systematic ablations, we find that these masks act as distractors, reducing the model's ability to process relevant information. To address this, we introduce a mask-agnostic loss function that encourages predictions to remain invariant to the number of appended masks. Fine-tuning with this objective substantially mitigates the distracting effect of masks, improving robustness of MDLMs. Overall, our findings reveal critical limitations of the current MDLM training paradigm and provide actionable insights for building diffusion-based language models with stronger context comprehension.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21338.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67d44d099c024761ad196132",
            "avatarUrl": "/avatars/614c79d727424e427d65fe6ee7b53abd.svg",
            "fullname": "Julianna Piskorz",
            "name": "jpiskorz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.19661",
            "authors": [
                {
                    "_id": "692fe32326742347f61dadd3",
                    "user": {
                        "_id": "6633dedc9c2b8424a93b715c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6633dedc9c2b8424a93b715c/4yosLG5H0DeVw_6lV6IiQ.jpeg",
                        "isPro": false,
                        "fullname": "Xinhai Hou",
                        "user": "RenlyH",
                        "type": "user"
                    },
                    "name": "Xinhai Hou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:13:12.152Z",
                    "hidden": false
                },
                {
                    "_id": "692fe32326742347f61dadd4",
                    "name": "Shaoyuan Xu",
                    "hidden": false
                },
                {
                    "_id": "692fe32326742347f61dadd5",
                    "name": "Manan Biyani",
                    "hidden": false
                },
                {
                    "_id": "692fe32326742347f61dadd6",
                    "name": "Mayan Li",
                    "hidden": false
                },
                {
                    "_id": "692fe32326742347f61dadd7",
                    "name": "Jia Liu",
                    "hidden": false
                },
                {
                    "_id": "692fe32326742347f61dadd8",
                    "name": "Todd C. Hollon",
                    "hidden": false
                },
                {
                    "_id": "692fe32326742347f61dadd9",
                    "name": "Bryan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-24T19:48:46.000Z",
            "submittedOnDailyAt": "2025-12-03T07:34:30.500Z",
            "title": "CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization",
            "submittedOnDailyBy": {
                "_id": "6633dedc9c2b8424a93b715c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6633dedc9c2b8424a93b715c/4yosLG5H0DeVw_6lV6IiQ.jpeg",
                "isPro": false,
                "fullname": "Xinhai Hou",
                "user": "RenlyH",
                "type": "user"
            },
            "summary": "Agentic vision-language models are increasingly trained to \"think with images\" by calling image operations. However, we show that high final-answer accuracy often hides unfaithful visual reasoning: models may invoke tools on irrelevant regions or ignore tool outputs entirely, yet still guess the correct answer. In this work, we first propose a faithfulness evaluation protocol that measures whether intermediate visual tool outputs (e.g., crops) actually contain the queried evidence. This reveals that recent visual agents achieve high final-answer accuracy but exhibit low rates of faithful tool-use on visual search benchmarks. We then introduce CodeV, a code-based visual agent trained with Tool-Aware Policy Optimization (TAPO). TAPO is a process-level RL framework that augments GRPO with dense rewards defined directly on visual tool inputs and outputs, rather than on chain-of-thought tokens, making supervision easier to verify and less susceptible to reward hacking. CodeV represents visual tools as executable Python code, and TAPO assigns step-wise rewards based solely on the question and tool output, encouraging both necessary and evidence-consistent tool use. In a two-stage SFT+RL pipeline, CodeV achieves competitive or superior accuracy while substantially increasing faithful tool-use rates on related visual search benchmarks. Beyond visual search, CodeV attains strong performance on a range of multimodal reasoning and math benchmarks, suggesting that explicitly supervising intermediate tool behavior is crucial for building trustworthy, agentic visual reasoning systems.",
            "upvotes": 1,
            "discussionId": "692fe32426742347f61dadda",
            "ai_summary": "CodeV, a code-based visual agent trained with Tool-Aware Policy Optimization (TAPO), improves faithful tool use and accuracy in visual and multimodal reasoning tasks.",
            "ai_keywords": [
                "Agentic vision-language models",
                "image operations",
                "faithfulness evaluation protocol",
                "intermediate visual tool outputs",
                "Tool-Aware Policy Optimization (TAPO)",
                "process-level RL",
                "GRPO",
                "dense rewards",
                "visual tool inputs",
                "visual tool outputs",
                "step-wise rewards",
                "SFT+RL pipeline",
                "multimodal reasoning",
                "math benchmarks"
            ],
            "organization": {
                "_id": "63df4874e742e86dc925d67c",
                "name": "umich",
                "fullname": "University of Michigan",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1675577443573-63df328115266dd945fc01f4.png"
            }
        },
        "publishedAt": "2025-11-24T14:48:46.000Z",
        "title": "CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization",
        "summary": "Agentic vision-language models are increasingly trained to \"think with images\" by calling image operations. However, we show that high final-answer accuracy often hides unfaithful visual reasoning: models may invoke tools on irrelevant regions or ignore tool outputs entirely, yet still guess the correct answer. In this work, we first propose a faithfulness evaluation protocol that measures whether intermediate visual tool outputs (e.g., crops) actually contain the queried evidence. This reveals that recent visual agents achieve high final-answer accuracy but exhibit low rates of faithful tool-use on visual search benchmarks. We then introduce CodeV, a code-based visual agent trained with Tool-Aware Policy Optimization (TAPO). TAPO is a process-level RL framework that augments GRPO with dense rewards defined directly on visual tool inputs and outputs, rather than on chain-of-thought tokens, making supervision easier to verify and less susceptible to reward hacking. CodeV represents visual tools as executable Python code, and TAPO assigns step-wise rewards based solely on the question and tool output, encouraging both necessary and evidence-consistent tool use. In a two-stage SFT+RL pipeline, CodeV achieves competitive or superior accuracy while substantially increasing faithful tool-use rates on related visual search benchmarks. Beyond visual search, CodeV attains strong performance on a range of multimodal reasoning and math benchmarks, suggesting that explicitly supervising intermediate tool behavior is crucial for building trustworthy, agentic visual reasoning systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19661.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6633dedc9c2b8424a93b715c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6633dedc9c2b8424a93b715c/4yosLG5H0DeVw_6lV6IiQ.jpeg",
            "fullname": "Xinhai Hou",
            "name": "RenlyH",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "63df4874e742e86dc925d67c",
            "name": "umich",
            "fullname": "University of Michigan",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1675577443573-63df328115266dd945fc01f4.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2511.18685",
            "authors": [
                {
                    "_id": "6930e2df2d1e5b0a7d84d937",
                    "name": "Dayong Liu",
                    "hidden": false
                },
                {
                    "_id": "6930e2df2d1e5b0a7d84d938",
                    "name": "Chao Xu",
                    "hidden": false
                },
                {
                    "_id": "6930e2df2d1e5b0a7d84d939",
                    "name": "Weihong Chen",
                    "hidden": false
                },
                {
                    "_id": "6930e2df2d1e5b0a7d84d93a",
                    "name": "Suyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6930e2df2d1e5b0a7d84d93b",
                    "name": "Juncheng Wang",
                    "hidden": false
                },
                {
                    "_id": "6930e2df2d1e5b0a7d84d93c",
                    "name": "Jiankang Deng",
                    "hidden": false
                },
                {
                    "_id": "6930e2df2d1e5b0a7d84d93d",
                    "name": "Baigui Sun",
                    "hidden": false
                },
                {
                    "_id": "6930e2df2d1e5b0a7d84d93e",
                    "name": "Yang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-24T02:02:29.000Z",
            "submittedOnDailyAt": "2025-12-03T23:00:41.660Z",
            "title": "Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents",
            "submittedOnDailyBy": {
                "_id": "691ae6cbc8e384e92365db5a",
                "avatarUrl": "/avatars/5ac2cb6bbf08de52353510fdd1983376.svg",
                "isPro": false,
                "fullname": "Chao Xu",
                "user": "Chaoxu0309",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents.",
            "upvotes": 1,
            "discussionId": "6930e2df2d1e5b0a7d84d93f",
            "ai_summary": "CFG-Bench evaluates multimodal large language models on fine-grained action intelligence and higher-order reasoning in embodied agent tasks, revealing limitations and potential for improvement.",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "embodied agents",
                "CFG-Bench",
                "three-modalities",
                "Physical Interaction",
                "Temporal-Causal Relation",
                "Intentional Understanding",
                "Evaluative Judgment",
                "supervised fine-tuning (SFT)",
                "embodied benchmarks"
            ],
            "organization": {
                "_id": "6345aadf5efccdc07f1365a5",
                "name": "ZhejiangUniversity",
                "fullname": "Zhejiang University"
            }
        },
        "publishedAt": "2025-11-23T21:02:29.000Z",
        "title": "Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents",
        "summary": "Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18685.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "691ae6cbc8e384e92365db5a",
            "avatarUrl": "/avatars/5ac2cb6bbf08de52353510fdd1983376.svg",
            "fullname": "Chao Xu",
            "name": "Chaoxu0309",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6345aadf5efccdc07f1365a5",
            "name": "ZhejiangUniversity",
            "fullname": "Zhejiang University"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.02817",
            "authors": [
                {
                    "_id": "692fedc026742347f61dadeb",
                    "name": "Sai Koneru",
                    "hidden": false
                },
                {
                    "_id": "692fedc026742347f61dadec",
                    "name": "Fabian Retkowski",
                    "hidden": false
                },
                {
                    "_id": "692fedc026742347f61daded",
                    "name": "Christian Huber",
                    "hidden": false
                },
                {
                    "_id": "692fedc026742347f61dadee",
                    "name": "Lukas Hilgert",
                    "hidden": false
                },
                {
                    "_id": "692fedc026742347f61dadef",
                    "name": "Seymanur Akti",
                    "hidden": false
                },
                {
                    "_id": "692fedc026742347f61dadf0",
                    "name": "Enes Yavuz Ugan",
                    "hidden": false
                },
                {
                    "_id": "692fedc026742347f61dadf1",
                    "name": "Alexander Waibel",
                    "hidden": false
                },
                {
                    "_id": "692fedc026742347f61dadf2",
                    "name": "Jan Niehues",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T14:27:26.000Z",
            "submittedOnDailyAt": "2025-12-03T19:33:35.058Z",
            "title": "BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion",
            "submittedOnDailyBy": {
                "_id": "63ea5f96b1d9c4ef71ed4da7",
                "avatarUrl": "/avatars/c7faaba1385d9fb9c8b40792144ea405.svg",
                "isPro": false,
                "fullname": "Sai Koneru",
                "user": "skoneru",
                "type": "user"
            },
            "summary": "The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present BOOM, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. We release our Slide Translation code at https://github.com/saikoneru/image-translator and integrate it in Lecture Translator at https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline}\\footnote{All released code and models are licensed under the MIT License.",
            "upvotes": 0,
            "discussionId": "692fedc026742347f61dadf3",
            "ai_summary": "BOOM is a multimodal multilingual lecture companion that translates audio and slides, producing synchronized outputs across text, images, and speech, enhancing accessibility and preservation of educational content.",
            "ai_keywords": [
                "multimodal",
                "multilingual",
                "lecture companion",
                "slide-aware transcripts",
                "summarization",
                "question answering"
            ],
            "organization": {
                "_id": "68934a384da2588a6df4add7",
                "name": "kit-isl-ai4lt",
                "fullname": "ISL & AI4LT @ KIT",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ea5f96b1d9c4ef71ed4da7/5ge8VAbtZ01OUHl2Aubmw.jpeg"
            }
        },
        "publishedAt": "2025-12-02T09:27:26.000Z",
        "title": "BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion",
        "summary": "The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present BOOM, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. We release our Slide Translation code at https://github.com/saikoneru/image-translator and integrate it in Lecture Translator at https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline}\\footnote{All released code and models are licensed under the MIT License.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02817.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63ea5f96b1d9c4ef71ed4da7",
            "avatarUrl": "/avatars/c7faaba1385d9fb9c8b40792144ea405.svg",
            "fullname": "Sai Koneru",
            "name": "skoneru",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "68934a384da2588a6df4add7",
            "name": "kit-isl-ai4lt",
            "fullname": "ISL & AI4LT @ KIT",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ea5f96b1d9c4ef71ed4da7/5ge8VAbtZ01OUHl2Aubmw.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.15948",
            "authors": [
                {
                    "_id": "692fc10826742347f61dacc9",
                    "name": "Raphael Ruschel",
                    "hidden": false
                },
                {
                    "_id": "692fc10826742347f61dacca",
                    "name": "Hardikkumar Prajapati",
                    "hidden": false
                },
                {
                    "_id": "692fc10826742347f61daccb",
                    "user": {
                        "_id": "60d34de513f774189902f547",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1640713172129-60d34de513f774189902f547.png",
                        "isPro": false,
                        "fullname": "Awsaf",
                        "user": "awsaf49",
                        "type": "user"
                    },
                    "name": "Awsafur Rahman",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-03T09:15:25.796Z",
                    "hidden": false
                },
                {
                    "_id": "692fc10826742347f61daccc",
                    "name": "B. S. Manjunath",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/dW4aVZP_cWfVUb9BrIu5V.png",
                "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/CuhWni1q3xKfIRNK1NEXu.png",
                "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/cy1dWRobfR3zQpcxXqUxV.png",
                "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/vy_6ImEUZKYASZKcSzZoT.png"
            ],
            "publishedAt": "2025-11-20T00:49:25.000Z",
            "submittedOnDailyAt": "2025-12-03T02:29:10.006Z",
            "title": "Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click",
            "submittedOnDailyBy": {
                "_id": "60d34de513f774189902f547",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1640713172129-60d34de513f774189902f547.png",
                "isPro": false,
                "fullname": "Awsaf",
                "user": "awsaf49",
                "type": "user"
            },
            "summary": "State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts <subject, object, predicate> triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding.",
            "upvotes": 0,
            "discussionId": "692fc10826742347f61daccd",
            "ai_summary": "Click2Graph is an interactive framework for panoptic video scene graph generation that combines user cues with dynamic interaction discovery and semantic classification for precise and controllable scene understanding.",
            "ai_keywords": [
                "Video Scene Graph Generation",
                "VSGG",
                "promptable segmentation models",
                "SAM2",
                "Panoptic Video Scene Graph Generation",
                "PVSG",
                "user interaction",
                "visual prompting",
                "Dynamic Interaction Discovery Module",
                "Semantic Classification Head",
                "scene graph",
                "openPVSG benchmark"
            ],
            "organization": {
                "_id": "691d9d63284266ada1eb632a",
                "name": "UCSantaBarbara",
                "fullname": "University of California, Santa Barbara",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/AoPyKoRF9O8PHOoaJOi7h.png"
            }
        },
        "publishedAt": "2025-11-19T19:49:25.000Z",
        "title": "Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click",
        "summary": "State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts <subject, object, predicate> triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/dW4aVZP_cWfVUb9BrIu5V.png",
            "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/CuhWni1q3xKfIRNK1NEXu.png",
            "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/cy1dWRobfR3zQpcxXqUxV.png",
            "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/vy_6ImEUZKYASZKcSzZoT.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15948.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60d34de513f774189902f547",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1640713172129-60d34de513f774189902f547.png",
            "fullname": "Awsaf",
            "name": "awsaf49",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "691d9d63284266ada1eb632a",
            "name": "UCSantaBarbara",
            "fullname": "University of California, Santa Barbara",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/AoPyKoRF9O8PHOoaJOi7h.png"
        },
        "isAuthorParticipating": true
    }
]